<S sid="47" ssid="13">Because of this, we retokenized and lowercased submitted output with our own tokenizer, which was also used to prepare the training and test data.</S>
    <S sid="8" ssid="1">The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.</S>
<S sid="18" ssid="11">In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.</S>
<S sid="144" ssid="37">Our suspicion is that BLEU is very sensitive to jargon, to selecting exactly the right words, and not synonyms that human judges may appreciate as equally good.</S>
<S sid="145" ssid="38">This is can not be the only explanation, since the discrepancy still holds, for instance, for out-of-domain French-English, where Systran receives among the best adequacy and fluency scores, but a worse BLEU score than all but one statistical system.</S>
    <S sid="103" ssid="19">Given a set of n sentences, we can compute the sample mean x&#65533; and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x&#8722;d, x+df can be computed by d = 1.96 &#183;&#65533;n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
<S sid="50" ssid="16">Following this method, we repeatedly &#8212; say, 1000 times &#8212; sample sets of sentences from the output of each system, measure their BLEU score, and use these 1000 BLEU scores as basis for estimating a confidence interval.</S>
<S sid="68" ssid="7">We asked participants to each judge 200&#8211;300 sentences in terms of fluency and adequacy, the most commonly used manual evaluation metrics.</S>
<S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
<S sid="92" ssid="8">The way judgements are collected, human judges tend to use the scores to rank systems against each other.</S>