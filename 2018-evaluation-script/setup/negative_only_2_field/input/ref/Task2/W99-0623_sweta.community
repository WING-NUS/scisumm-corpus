 <S sid="144" ssid="6">Combining multiple highly-accurate independent parsers yields promising results.</S>
 <S sid="125" ssid="54">The constituent voting and na&#239;ve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S>
<S sid="125" ssid="54">The constituent voting and na&#239;ve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S>
    <S sid="48" ssid="34">&#8226; Similarly, when the na&#239;ve Bayes classifier is configured such that the constituents require estimated probabilities strictly larger than 0.5 to be accepted, there is not enough probability mass remaining on crossing brackets for them to be included in the hypothesis.</S>
 <S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
<S sid="134" ssid="63">As seen by the drop in average individual parser performance baseline, the introduced parser does not perform very well.</S>
 <S sid="38" ssid="24">Under certain conditions the constituent voting and na&#239;ve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S>
    <S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
<S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
<S sid="13" ssid="9">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993).</S>
<S sid="108" ssid="37">From this we see that a finer-grained model for parser combination, at least for the features we have examined, will not give us any additional power.</S>
<S sid="98" ssid="27">Adding the isolated constituents to our hypothesis parse could increase our expected recall, but in the cases we investigated it would invariably hurt our precision more than we would gain on recall.</S>
<S sid="27" ssid="13">Another technique for parse hybridization is to use a na&#239;ve Bayes classifier to determine which constituents to include in the parse.</S>
"<S sid=""80"" ssid=""9"">For our experiments we also report the mean of precision and recall, which we denote by (P + R)I2 and F-measure.</S>
    <S sid=""81"" ssid=""10"">F-measure is the harmonic mean of precision and recall, 2PR/(P + R).</S>
    <S sid=""82"" ssid=""11"">It is closer to the smaller value of precision and recall when there is a large skew in their values.</S>"
<S sid="49" ssid="35">In general, the lemma of the previous section does not ensure that all the productions in the combined parse are found in the grammars of the member parsers.</S>
<S sid="11" ssid="7">Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al., 1998).</S>