In this paper the author aims at introducing a polylingual topic model that discovers topics aligned across multiple languages. They explore the modelâ€™s characteristics using two large corpora, each with over ten different languages, and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages. Statistical topic models have emerged as an increasingly useful analysis tool for large text collections. Topic models have been used for analyzing topic trends in research literature, inferring captions for images, social network analysis in email, and expanding queries with topically related words in information retrieval. The author argues that topic modelling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages. By linking topics across languages, polylingual topic models can increase cross-cultural understanding by providing readers with the ability to characterize the contents of collections in unfamiliar languages and identify trends in topic prevalence. When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.