<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
<S sid="17" ssid="5">This paper will examine language modeling for speech recognition from a natural language processing point of view.
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
<S sid="302" ssid="58">In the beam search approach outlined above, we can estimate the string's probability in the same manner, by summing the probabilities of the parses that the algorithm finds.</S>
<S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
"<S sid=""79"" ssid=""37"">This underspecification of the nonterminal predictions (e.g., VP-VBD in the example in Figure 2, as opposed to NP), allows lexical items to become part of the left context, and so be used to condition production probabilities, even the production probabilities of constituents that dominate them in the unfactored tree.</S>
    <S sid=""80"" ssid=""38"">It also brings words further downstream into the look-ahead at the point of specification.</S>"
<S sid="138" ssid="42">Our approach is found to yield very accurate parses efficiently, and, in addition, to lend itself straightforwardly to estimating word probabilities on-line, that is, in a single pass from left to right.</S>
<S sid="291" ssid="47">Also, the parser returns a set of candidate parses, from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence), we find an average labeled precision/recall of 94.1, for sentences of length &lt; 100.</S>
"<S sid=""209"" ssid=""113"">This parser is essentially a stochastic version of the top-down parser described in Aho, Sethi, and Ullman (1986).</S>
    <S sid=""210"" ssid=""114"">It uses a PCFG with a conditional probability model of the sort defined in the previous section.</S>"
NA
"<S sid=""20"" ssid=""8"">Two features of our top-down parsing approach will emerge as key to its success.</S>
    <S sid=""21"" ssid=""9"">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S><S sid=""32"" ssid=""20"">A second key feature of our approach is that top-down guidance improves the efficiency of the search as more and more conditioning events are extracted from the derivation for use in the probabilistic model.</S>"
<S sid="33" ssid="21">Because the rooted partial derivation is fully connected, all of the conditioning information that might be extracted from the top-down left context has already been specified, and a conditional probability model built on this information will not impose any additional burden on the search.</S>