<S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
<S sid="159" ssid="26">To prevent this we &amp;quot;smooth&amp;quot; the confidence by adding a small value, e, to both W+ and W_, giving at = Plugging the value of at from Equ.</S>
<S sid="137" ssid="4">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S>
<S sid="91" ssid="24">There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.</S>
S sid="213" ssid="80">Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.</S>
 <S sid="250" ssid="1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S>
    <S sid="36" ssid="30">Roughly speaking, the new algorithm presented in this paper performs a similar search, but instead minimizes a bound on the number of (unlabeled) examples on which two classifiers disagree.</S>
<S sid="29" ssid="23">Unfortunately, Yarowsky's method is not well understood from a theoretical viewpoint: we would like to formalize the notion of redundancy in unlabeled data, and set up the learning task as optimization of some appropriate objective function.</S>
<S sid="7" ssid="1">Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples.</S>
    <S sid="85" ssid="18">(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.</S>
    <S sid="95" ssid="28">(Specifically, the limit n starts at 5 and increases by 5 at each iteration.)</S>
<S sid="213" ssid="80">Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.</S>