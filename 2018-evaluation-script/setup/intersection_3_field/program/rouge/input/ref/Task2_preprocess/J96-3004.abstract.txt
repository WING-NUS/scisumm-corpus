<S sid ="0">A Stochastic Finite-State Word-Segmentation Algorithm for Chinese</S>
<S sid="">The initial stage of text analysis for any NLP task usually involves the tokenization of the input into words.</S>
<S sid=""> For languages like English one can assume, to a first approximation, that word boundaries are given by whitespace or punctuation.</S>
<S sid=""> In various Asian languages, including Chinese, on the other hand, whitespace is never used to delimit words, so one must resort to lexical information to "reconstruct" the word-boundary information.</S>
<S sid=""> In this paper we present a stochastic finite-state model wherein the basic workhorse is the weighted finite-state transducer.</S>
<S sid=""> The model segments Chinese text into dictionary entries and words derived by various productive lexical processes, and--since the primary intended application of this model is to text-to-speech synthesis--provides pronunciations for these words.</S>
<S sid=""> We evaluate the system's performance by comparing its segmentation 'Tudgments" with the judgments of a pool of human segmenters, and the system is shown to perform quite well.</S>
