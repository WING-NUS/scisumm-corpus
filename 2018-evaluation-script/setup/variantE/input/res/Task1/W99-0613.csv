Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,W99-0613,N01-1023,0,"Collins and Singer, 1999",0,"Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named entity identification (Collins and Singer, 1999)","Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named entity identification (Collins and Singer, 1999)","['142', '228', '0', '75', '77']","<S sid =""142"" ssid = ""9"">The input to AdaBoost is a set of training examples ((xi   yi)    (xâ€ž.â€ž yrn)).</S><S sid =""228"" ssid = ""7"">Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).</S><S sid =""0"" ssid = ""0"">Unsupervised Models for Named Entity Classification Collins</S><S sid =""75"" ssid = ""8"">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S><S sid =""77"" ssid = ""10"">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S>",['Results_Citation']
2,W99-0613,N01-1023,0,"Collins and Singer, 1999",0,"They also discuss an application of classifying web pages by using their method of mutually constrained models. (Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms toAdaBoost which force the classifiers to agree (called Co Boosting)","(Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms to AdaBoost which force the classifiers to agree (called Co Boosting)","['142', '77', '102', '52', '75']","<S sid =""142"" ssid = ""9"">The input to AdaBoost is a set of training examples ((xi   yi)    (xâ€ž.â€ž yrn)).</S><S sid =""77"" ssid = ""10"">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid =""102"" ssid = ""35"">(Blum and Mitchell 98) describe learning in the following situation: X = X1 X X2 where X1 and X2 correspond to two different &quot;views&quot; of an example.</S><S sid =""52"" ssid = ""6"">The NP is a complement to a preposition  which is the head of a PP.</S><S sid =""75"" ssid = ""8"">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S>",['Method_Citation']
3,W99-0613,W03-1509,0,Collins and Singer 1999,0,"Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick, et al 1999] and so on","Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick, et al 1999] and so on","['225', '203', '140', '79', '31']","<S sid =""225"" ssid = ""4"">We again assume a training set of n examples {x1 . xri} where the first m examples have labels {y1 ... yin}  and the last (n â€” m) examples are unlabeled.</S><S sid =""203"" ssid = ""70"">Several extensions of AdaBoost for multiclass problems have been suggested (Freund and Schapire 97; Schapire and Singer 98).</S><S sid =""140"" ssid = ""7"">AdaBoost was first introduced in (Freund and Schapire 97); (Schapire and Singer 98) gave a generalization of AdaBoost which we will use in this paper.</S><S sid =""79"" ssid = ""12"">2 We now introduce a new algorithm for learning from unlabeled examples  which we will call DLCoTrain (DL stands for decision list  the term Cotrain is taken from (Blum and Mitchell 98)).</S><S sid =""31"" ssid = ""25"">Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).</S>",['Method_Citation']
4,W99-0613,C02-1154,0,"Collins and Singer, 1999",0,"DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syn tactically analyzed corpus","DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syntactically analyzed corpus","['142', '102', '77', '52', '228']","<S sid =""142"" ssid = ""9"">The input to AdaBoost is a set of training examples ((xi   yi)    (xâ€ž.â€ž yrn)).</S><S sid =""102"" ssid = ""35"">(Blum and Mitchell 98) describe learning in the following situation: X = X1 X X2 where X1 and X2 correspond to two different &quot;views&quot; of an example.</S><S sid =""77"" ssid = ""10"">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid =""52"" ssid = ""6"">The NP is a complement to a preposition  which is the head of a PP.</S><S sid =""228"" ssid = ""7"">Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).</S>",['Method_Citation']
5,W99-0613,C02-1154,0,"Collins and Singer, 1999",0,"(Collins and Singer, 1999) also makes use of competing categories (person, organization, and location), which cover 96% of all the instances itset out to classify","(Collins and Singer, 1999) also makes use of competing categories (person, organization, and location), which cover 96% of all the instances it set out to classify","['236', '207', '248', '166', '84']","<S sid =""236"" ssid = ""3"">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S><S sid =""207"" ssid = ""74"">Pseudo-labels are formed by taking seed labels on the labeled examples  and the output of the fixed classifier on the unlabeled examples.</S><S sid =""248"" ssid = ""15"">With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.</S><S sid =""166"" ssid = ""33"">The first m pairs have labels yi  whereas for i = m + 1    n the pairs are unlabeled.</S><S sid =""84"" ssid = ""17"">For each label (Per s on  organization and Location)  take the n contextual rules with the highest value of Count' (x) whose unsmoothed3 strength is above some threshold pmin.</S>",['Method_Citation']
6,W99-0613,W06-2204,0,"Collins and Singer, 1999",0,"In (Collins and Singer, 1999) Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification","In (Collins and Singer, 1999) Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification","['228', '142', '75', '77', '0']","<S sid =""228"" ssid = ""7"">Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).</S><S sid =""142"" ssid = ""9"">The input to AdaBoost is a set of training examples ((xi   yi)    (xâ€ž.â€ž yrn)).</S><S sid =""75"" ssid = ""8"">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S><S sid =""77"" ssid = ""10"">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid =""0"" ssid = ""0"">Unsupervised Models for Named Entity Classification Collins</S>",['Method_Citation']
8,W99-0613,W03-1022,0,"Collins and Singer, 1999",0,"Collins and Singer (1999) for example report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999)","Collins and Singer (1999) for example report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999)","['207', '248', '86', '204', '10']","<S sid =""207"" ssid = ""74"">Pseudo-labels are formed by taking seed labels on the labeled examples  and the output of the fixed classifier on the unlabeled examples.</S><S sid =""248"" ssid = ""15"">With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.</S><S sid =""86"" ssid = ""19"">Thus at each iteration the method induces at most n x k rules  where k is the number of possible labels (k = 3 in the experiments in this paper). step 3.</S><S sid =""204"" ssid = ""71"">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid =""10"" ssid = ""4"">The task is to learn a function from an input string (proper name) to its type  which we will assume to be one of the categories Person  Organization  or Location.</S>",['Method_Citation']
9,W99-0613,E09-1018,0,"Collinsand Singer, 1999",0,"While EM has worked quite well for a few tasks, notably ma chine translations (starting with the IBM models 1-5 (Brown et al, 1993), it has not had success inmost others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collinsand Singer, 1999) and context-free-grammar induction (numerous attempts, too many to mention)","While EM has worked quite well for a few tasks, notably machine translations (starting with the IBM models 1-5 (Brown et al, 1993), it has not had success in most others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collinsand Singer, 1999) and context-free-grammar induction (numerous attempts, too many to mention)","['202', '124', '121', '50', '25']","<S sid =""202"" ssid = ""69"">The CoBoost algorithm just described is for the case where there are two labels: for the named entity task there are three labels  and in general it will be useful to generalize the CoBoost algorithm to the multiclass case.</S><S sid =""124"" ssid = ""57"">In particular  it may not be possible to learn functions fi (x f2(x2 t) for i = m + 1...n: either because there is some noise in the data  or because it is just not realistic to expect to learn perfect classifiers given the features used for representation.</S><S sid =""121"" ssid = ""54"">They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page  and other pages pointing to the page).</S><S sid =""50"" ssid = ""4"">It is a sequence of proper nouns within an NP; its last word Cooper is the head of the NP; and the NP has an appositive modifier (a vice president at S.&P.) whose head is a singular noun (president).</S><S sid =""25"" ssid = ""19"">The unlabeled data gives many such &quot;hints&quot; that two features should predict the same label  and these hints turn out to be surprisingly useful when building a classifier.</S>",['Method_Citation']
11,W99-0613,W07-1712,0,"Collins and Singer, 1999",0,"In addition, we would also like to explore the semi-supervised techniques such as co-training and self-training (Collins and Singer, 1999)","In addition, we would also like to explore the semi-supervised techniques such as co-training and self-training (Collins and Singer, 1999)","['157', '52', '75', '12', '150']","<S sid =""157"" ssid = ""24"">Zt can be written as follows Following the derivation of Schapire and Singer  providing that W+ > W_  Equ.</S><S sid =""52"" ssid = ""6"">The NP is a complement to a preposition  which is the head of a PP.</S><S sid =""75"" ssid = ""8"">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S><S sid =""12"" ssid = ""6"">The approach uses both spelling and contextual rules.</S><S sid =""150"" ssid = ""17"">Pseudo-code describing the generalized boosting algorithm of Schapire and Singer is given in Figure 1.</S>",['Method_Citation']
12,W99-0613,W09-2208,0,"Collins and Singer, 1999",0,"Collins et al (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky ?smethod (Yarowsky, 1995) and the framework suggested by (Blum and Mitchell, 1998)","Collins et al (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky's method (Yarowsky, 1995) and the framework suggested by (Blum and Mitchell, 1998)","['63', '52', '157', '77', '47']","<S sid =""63"" ssid = ""17"">In this case nonalpha is the string formed by removing all upper/lower case letters from the spelling (e.g.  for Thomas E. Petry nonalpha= .</S><S sid =""52"" ssid = ""6"">The NP is a complement to a preposition  which is the head of a PP.</S><S sid =""157"" ssid = ""24"">Zt can be written as follows Following the derivation of Schapire and Singer  providing that W+ > W_  Equ.</S><S sid =""77"" ssid = ""10"">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid =""47"" ssid = ""1"">971 746 sentences of New York Times text were parsed using the parser of (Collins 96).1 Word sequences that met the following criteria were then extracted as named entity examples: whose head is a singular noun (tagged NN).</S>",['Method_Citation']
13,W99-0613,W06-2207,0,"Collins and Singer, 1999",0,"This approach was shown to perform well on real-world natural language problems (Collins and Singer, 1999)","This approach was shown to perform well on real-world natural language problems (Collins and Singer, 1999)","['204', '239', '93', '249', '197']","<S sid =""204"" ssid = ""71"">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid =""239"" ssid = ""6"">Of these cases  38 were temporal expressions (either a day of the week or month of the year).</S><S sid =""93"" ssid = ""26"">To measure the contribution of each modification  a third  intermediate algorithm  Yarowsky-cautious was also tested.</S><S sid =""249"" ssid = ""16"">The test accuracy more or less asymptotes.</S><S sid =""197"" ssid = ""64"">Note that in our formalism a weakhypothesis can abstain.</S>",['Method_Citation']
15,W99-0613,W06-2207,0,"Collins and Singer, 1999",0,"(6) Similarly to (Collins and Singer, 1999) we used T= 0.95 for all experiments reported here","(6) Similarly to (Collins and Singer, 1999) we used T= 0.95 for all experiments reported here","['204', '75', '228', '0', '93']","<S sid =""204"" ssid = ""71"">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid =""75"" ssid = ""8"">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S><S sid =""228"" ssid = ""7"">Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).</S><S sid =""0"" ssid = ""0"">Unsupervised Models for Named Entity Classification Collins</S><S sid =""93"" ssid = ""26"">To measure the contribution of each modification  a third  intermediate algorithm  Yarowsky-cautious was also tested.</S>",['Method_Citation']
16,W99-0613,P12-1065,0,1999,0,We use Collins and Singer (1999) for our exact specification of Yarowsky.2 It uses DL rule scores ?fj?| ?fj |+ |? f |+ L (1) where  is a smoothing constant,We use Collins and Singer (1999) for our exact specification of Yarowsky,"['52', '157', '77', '193', '142']","<S sid =""52"" ssid = ""6"">The NP is a complement to a preposition  which is the head of a PP.</S><S sid =""157"" ssid = ""24"">Zt can be written as follows Following the derivation of Schapire and Singer  providing that W+ > W_  Equ.</S><S sid =""77"" ssid = ""10"">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid =""193"" ssid = ""60"">In Input: {(x1 i  Initialize: Vi  j : e(xi) = 0.</S><S sid =""142"" ssid = ""9"">The input to AdaBoost is a set of training examples ((xi   yi)    (xâ€ž.â€ž yrn)).</S>",['Results_Citation']
