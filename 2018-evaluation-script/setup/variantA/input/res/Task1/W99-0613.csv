Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,W99-0613,N01-1023,0,"Collins and Singer, 1999",0,"Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named entity identification (Collins and Singer, 1999)","Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named entity identification (Collins and Singer, 1999)","['222', '214', '23', '13', '132']","<S sid =""222"" ssid = ""1"">The Expectation Maximization (EM) algorithm (Dempster  Laird and Rubin 77) is a common approach for unsupervised training; in this section we describe its application to the named entity problem.</S><S sid =""214"" ssid = ""81"">This modification brings the method closer to the DL-CoTrain algorithm described earlier  and is motivated by the intuition that all three labels should be kept healthily populated in the unlabeled examples  preventing one label from dominating â€” this deserves more theoretical investigation.</S><S sid =""23"" ssid = ""17"">For example  in ..  says Mr. Cooper  a vice president of.. both a spelling feature (that the string contains Mr.) and a contextual feature (that president modifies the string) are strong indications that Mr. Cooper is of type Person.</S><S sid =""13"" ssid = ""7"">A spelling rule might be a simple look-up for the string (e.g.  a rule that Honduras is a location) or a rule that looks at words within a string (e.g.  a rule that any string containing Mr. is a person).</S><S sid =""132"" ssid = ""65"">The algorithm  called CoBoost  has the advantage of being more general than the decision-list learning alInput: (xi   yi)    (xim  ) ; x  E 2x  yi = +1 Initialize Di (i) = 1/m.</S>",['Method_Citation']
2,W99-0613,N01-1023,0,"Collins and Singer, 1999",0,"They also discuss an application of classifying web pages by using their method of mutually constrained models. (Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms toAdaBoost which force the classifiers to agree (called Co Boosting)","(Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms to AdaBoost which force the classifiers to agree (called Co Boosting)","['40', '42', '164', '102', '214']","<S sid =""40"" ssid = ""34"">(Berland and Charniak 99) describe a method for extracting parts of objects from wholes (e.g.  &quot;speedometer&quot; from &quot;car&quot;) from a large corpus using hand-crafted patterns.</S><S sid =""42"" ssid = ""36"">(Riloff and Shepherd 97) describe a bootstrapping approach for acquiring nouns in particular categories (such as &quot;vehicle&quot; or &quot;weapon&quot; categories).</S><S sid =""164"" ssid = ""31"">Following the convention presented in earlier sections  we assume that each example is an instance pair of the from (xi  i  x2 ) where xj   E 2x3   j E 2}.</S><S sid =""102"" ssid = ""35"">(Blum and Mitchell 98) describe learning in the following situation: X = X1 X X2 where X1 and X2 correspond to two different &quot;views&quot; of an example.</S><S sid =""214"" ssid = ""81"">This modification brings the method closer to the DL-CoTrain algorithm described earlier  and is motivated by the intuition that all three labels should be kept healthily populated in the unlabeled examples  preventing one label from dominating â€” this deserves more theoretical investigation.</S>",['Method_Citation']
3,W99-0613,W03-1509,0,Collins and Singer 1999,0,"Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick, et al 1999] and so on","Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick, et al 1999] and so on","['23', '20', '132', '40', '138']","<S sid =""23"" ssid = ""17"">For example  in ..  says Mr. Cooper  a vice president of.. both a spelling feature (that the string contains Mr.) and a contextual feature (that president modifies the string) are strong indications that Mr. Cooper is of type Person.</S><S sid =""20"" ssid = ""14"">The only supervision is in the form of 7 seed rules (namely  that New York  California and U.S. are locations; that any name containing Mr is a person; that any name containing Incorporated is an organization; and that I.B.M. and Microsoft are organizations).</S><S sid =""132"" ssid = ""65"">The algorithm  called CoBoost  has the advantage of being more general than the decision-list learning alInput: (xi   yi)    (xim  ) ; x  E 2x  yi = +1 Initialize Di (i) = 1/m.</S><S sid =""40"" ssid = ""34"">(Berland and Charniak 99) describe a method for extracting parts of objects from wholes (e.g.  &quot;speedometer&quot; from &quot;car&quot;) from a large corpus using hand-crafted patterns.</S><S sid =""138"" ssid = ""5"">(We would like to note though that unlike previous boosting algorithms  the CoBoost algorithm presented here is not a boosting algorithm under Valiant's (Valiant 84) Probably Approximately Correct (PAC) model.)</S>",['Method_Citation']
4,W99-0613,C02-1154,0,"Collins and Singer, 1999",0,"DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syn tactically analyzed corpus","DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syntactically analyzed corpus","['5', '31', '142', '39', '27']","<S sid =""5"" ssid = ""5"">The first method uses a similar algorithm to that of (Yarowsky 95)  with modifications motivated by (Blum and Mitchell 98).</S><S sid =""31"" ssid = ""25"">Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).</S><S sid =""142"" ssid = ""9"">The input to AdaBoost is a set of training examples ((xi   yi)    (xâ€ž.â€ž yrn)).</S><S sid =""39"" ssid = ""33"">(Brin 98)  describes a system for extracting (author  book-title) pairs from the World Wide Web using an approach that bootstraps from an initial seed set of examples.</S><S sid =""27"" ssid = ""21"">The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).</S>",['Method_Citation']
5,W99-0613,C02-1154,0,"Collins and Singer, 1999",0,"(Collins and Singer, 1999) also makes use of competing categories (person, organization, and location), which cover 96% of all the instances itset out to classify","(Collins and Singer, 1999) also makes use of competing categories (person, organization, and location), which cover 96% of all the instances it set out to classify","['39', '5', '28', '30', '40']","<S sid =""39"" ssid = ""33"">(Brin 98)  describes a system for extracting (author  book-title) pairs from the World Wide Web using an approach that bootstraps from an initial seed set of examples.</S><S sid =""5"" ssid = ""5"">The first method uses a similar algorithm to that of (Yarowsky 95)  with modifications motivated by (Blum and Mitchell 98).</S><S sid =""28"" ssid = ""22"">(Yarowsky 95) describes an algorithm for word-sense disambiguation that exploits redundancy in contextual features  and gives impressive performance.</S><S sid =""30"" ssid = ""24"">(Blum and Mitchell 98) offer a promising formulation of redundancy  also prove some results about how the use of unlabeled examples can help classification  and suggest an objective function when training with unlabeled examples.</S><S sid =""40"" ssid = ""34"">(Berland and Charniak 99) describe a method for extracting parts of objects from wholes (e.g.  &quot;speedometer&quot; from &quot;car&quot;) from a large corpus using hand-crafted patterns.</S>",['Method_Citation']
6,W99-0613,W06-2204,0,"Collins and Singer, 1999",0,"In (Collins and Singer, 1999) Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification","In (Collins and Singer, 1999) Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification","['15', '30', '39', '217', '251']","<S sid =""15"" ssid = ""9"">The task can be considered to be one component of the MUC (MUC-6  1995) named entity task (the other task is that of segmentation  i.e.  pulling possible people  places and locations from text before sending them to the classifier).</S><S sid =""30"" ssid = ""24"">(Blum and Mitchell 98) offer a promising formulation of redundancy  also prove some results about how the use of unlabeled examples can help classification  and suggest an objective function when training with unlabeled examples.</S><S sid =""39"" ssid = ""33"">(Brin 98)  describes a system for extracting (author  book-title) pairs from the World Wide Web using an approach that bootstraps from an initial seed set of examples.</S><S sid =""217"" ssid = ""84"">When this feature type was included  CoBoost chose this default feature at an early iteration  thereby giving non-abstaining pseudo-labels for all examples  with eventual convergence to the two classifiers agreeing by assigning the same label to almost all examples.</S><S sid =""251"" ssid = ""2"">In addition to a heuristic based on decision list learning  we also presented a boosting-like framework that builds on ideas from (Blum and Mitchell 98).</S>",['Method_Citation']
8,W99-0613,W03-1022,0,"Collins and Singer, 1999",0,"Collins and Singer (1999) for example report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999)","Collins and Singer (1999) for example report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999)","['120', '35', '30', '5', '44']","<S sid =""120"" ssid = ""53"">(Blum and Mitchell 98) go on to give PAC results for learning in the cotraining case.</S><S sid =""35"" ssid = ""29"">AdaBoost finds a weighted combination of simple (weak) classifiers  where the weights are chosen to minimize a function that bounds the classification error on a set of training examples.</S><S sid =""30"" ssid = ""24"">(Blum and Mitchell 98) offer a promising formulation of redundancy  also prove some results about how the use of unlabeled examples can help classification  and suggest an objective function when training with unlabeled examples.</S><S sid =""5"" ssid = ""5"">The first method uses a similar algorithm to that of (Yarowsky 95)  with modifications motivated by (Blum and Mitchell 98).</S><S sid =""44"" ssid = ""38"">More recently  (Riloff and Jones 99) describe a method they term &quot;mutual bootstrapping&quot; for simultaneously constructing a lexicon and contextual extraction patterns.</S>",['Method_Citation']
9,W99-0613,E09-1018,0,"Collinsand Singer, 1999",0,"While EM has worked quite well for a few tasks, notably ma chine translations (starting with the IBM models 1-5 (Brown et al, 1993), it has not had success inmost others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collinsand Singer, 1999) and context-free-grammar induction (numerous attempts, too many to mention)","While EM has worked quite well for a few tasks, notably machine translations (starting with the IBM models 1-5 (Brown et al, 1993), it has not had success in most others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collinsand Singer, 1999) and context-free-grammar induction (numerous attempts, too many to mention)","['232', '224', '187', '20', '85']","<S sid =""232"" ssid = ""11"">For example  the independence assumptions mean that the model fails to capture the dependence between specific and more general features (for example the fact that the feature full.-string=New_York is always seen with the features contains (New) and The baseline method tags all entities as the most frequent class type (organization). contains (York) and is never seen with a feature such as contains (Group) ).</S><S sid =""224"" ssid = ""3"">The model was parameterized such that the joint probability of a (label  feature-set) pair P(yi  xi) is written as The model assumes that (y  x) pairs are generated by an underlying process where the label is first chosen with some prior probability P(yi); the number of features mi is then chosen with some probability P(mi); finally the features are independently generated with probabilities P(xulyi).</S><S sid =""187"" ssid = ""54"">Using the virtual distribution Di (i) and pseudo-labels&quot;y. â€ž values for Wo  WÂ± and W_ can be calculated for each possible weak hypothesis (i.e.  for each feature x E Xi); the weak hypothesis with minimal value for Wo + 2/WW _ can be chosen as before; and the weight for this weak hypothesis at = ln ww+411:) can be calculated.</S><S sid =""20"" ssid = ""14"">The only supervision is in the form of 7 seed rules (namely  that New York  California and U.S. are locations; that any name containing Mr is a person; that any name containing Incorporated is an organization; and that I.B.M. and Microsoft are organizations).</S><S sid =""85"" ssid = ""18"">(If fewer than n rules have Precision greater than pin  we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events  hence we do not use smoothing  allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm n was fixed at 0.95 in all experiments in this paper.</S>",['Method_Citation']
11,W99-0613,W07-1712,0,"Collins and Singer, 1999",0,"In addition, we would also like to explore the semi-supervised techniques such as co-training and self-training (Collins and Singer, 1999)","In addition, we would also like to explore the semi-supervised techniques such as co-training and self-training (Collins and Singer, 1999)","['214', '43', '6', '141', '68']","<S sid =""214"" ssid = ""81"">This modification brings the method closer to the DL-CoTrain algorithm described earlier  and is motivated by the intuition that all three labels should be kept healthily populated in the unlabeled examples  preventing one label from dominating â€” this deserves more theoretical investigation.</S><S sid =""43"" ssid = ""37"">The approach builds from an initial seed set for a category  and is quite similar to the decision list approach described in (Yarowsky 95).</S><S sid =""6"" ssid = ""6"">The second algorithm extends ideas from boosting algorithms  designed for supervised learning tasks  to the framework suggested by (Blum and Mitchell 98).</S><S sid =""141"" ssid = ""8"">For a description of the application of AdaBoost to various NLP problems see the paper by Abney  Schapire  and Singer in this volume.</S><S sid =""68"" ssid = ""1"">The first unsupervised algorithm we describe is based on the decision list method from (Yarowsky 95).</S>",['Results_Citation']
12,W99-0613,W09-2208,0,"Collins and Singer, 1999",0,"Collins et al (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky ?smethod (Yarowsky, 1995) and the framework suggested by (Blum and Mitchell, 1998)","Collins et al (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky's method (Yarowsky, 1995) and the framework suggested by (Blum and Mitchell, 1998)","['27', '40', '5', '23', '140']","<S sid =""27"" ssid = ""21"">The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).</S><S sid =""40"" ssid = ""34"">(Berland and Charniak 99) describe a method for extracting parts of objects from wholes (e.g.  &quot;speedometer&quot; from &quot;car&quot;) from a large corpus using hand-crafted patterns.</S><S sid =""5"" ssid = ""5"">The first method uses a similar algorithm to that of (Yarowsky 95)  with modifications motivated by (Blum and Mitchell 98).</S><S sid =""23"" ssid = ""17"">For example  in ..  says Mr. Cooper  a vice president of.. both a spelling feature (that the string contains Mr.) and a contextual feature (that president modifies the string) are strong indications that Mr. Cooper is of type Person.</S><S sid =""140"" ssid = ""7"">AdaBoost was first introduced in (Freund and Schapire 97); (Schapire and Singer 98) gave a generalization of AdaBoost which we will use in this paper.</S>",['Method_Citation']
13,W99-0613,W06-2207,0,"Collins and Singer, 1999",0,"This approach was shown to perform well on real-world natural language problems (Collins and Singer, 1999)","This approach was shown to perform well on real-world natural language problems (Collins and Singer, 1999)","['6', '16', '27', '43', '68']","<S sid =""6"" ssid = ""6"">The second algorithm extends ideas from boosting algorithms  designed for supervised learning tasks  to the framework suggested by (Blum and Mitchell 98).</S><S sid =""16"" ssid = ""10"">Supervised methods have been applied quite successfully to the full MUC named-entity task (Bikel et al. 97).</S><S sid =""27"" ssid = ""21"">The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).</S><S sid =""43"" ssid = ""37"">The approach builds from an initial seed set for a category  and is quite similar to the decision list approach described in (Yarowsky 95).</S><S sid =""68"" ssid = ""1"">The first unsupervised algorithm we describe is based on the decision list method from (Yarowsky 95).</S>",['Method_Citation']
15,W99-0613,W06-2207,0,"Collins and Singer, 1999",0,"(6) Similarly to (Collins and Singer, 1999) we used T= 0.95 for all experiments reported here","(6) Similarly to (Collins and Singer, 1999) we used T= 0.95 for all experiments reported here","['157', '88', '186', '86', '120']","<S sid =""157"" ssid = ""24"">Zt can be written as follows Following the derivation of Schapire and Singer  providing that W+ > W_  Equ.</S><S sid =""88"" ssid = ""21"">We can now compare this algorithm to that of (Yarowsky 95).</S><S sid =""186"" ssid = ""53"">(8) can now be rewritten5 as which is of the same form as the function Zt used in AdaBoost.</S><S sid =""86"" ssid = ""19"">Thus at each iteration the method induces at most n x k rules  where k is the number of possible labels (k = 3 in the experiments in this paper). step 3.</S><S sid =""120"" ssid = ""53"">(Blum and Mitchell 98) go on to give PAC results for learning in the cotraining case.</S>",['Method_Citation']
16,W99-0613,P12-1065,0,1999,0,We use Collins and Singer (1999) for our exact specification of Yarowsky.2 It uses DL rule scores ?fj?| ?fj |+ |? f |+ L (1) where  is a smoothing constant,We use Collins and Singer (1999) for our exact specification of Yarowsky,"['163', '88', '136', '141', '79']","<S sid =""163"" ssid = ""30"">We now describe the CoBoost algorithm for the named entity problem.</S><S sid =""88"" ssid = ""21"">We can now compare this algorithm to that of (Yarowsky 95).</S><S sid =""136"" ssid = ""3"">We then discuss how we adapt and generalize a boosting algorithm  AdaBoost  to the problem of named entity classification.</S><S sid =""141"" ssid = ""8"">For a description of the application of AdaBoost to various NLP problems see the paper by Abney  Schapire  and Singer in this volume.</S><S sid =""79"" ssid = ""12"">2 We now introduce a new algorithm for learning from unlabeled examples  which we will call DLCoTrain (DL stands for decision list  the term Cotrain is taken from (Blum and Mitchell 98)).</S>",['Method_Citation']
