<S sid="40" ssid="6">We extend Subramanya et al.&#8217;s intuitions to our bilingual setup.</S>
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
<S sid="15" ssid="11">First, we use a novel graph-based framework for projecting syntactic information across language boundaries.</S>
<S sid="25" ssid="2">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
<S sid="29" ssid="6">To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (&#167;3.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.</S>
<S sid="18" ssid="14">To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
<S sid="158" ssid="1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
<S sid="21" ssid="17">These universal POS categories not only facilitate the transfer of POS information from one language to another, but also relieve us from using controversial evaluation metrics,2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.</S>
<S sid="24" ssid="1">The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.</S>
<S sid="17" ssid="13">Second, we treat the projected labels as features in an unsupervised model (&#167;5), rather than using them directly for supervised training.</S>
<S sid="111" ssid="11">We use the universal POS tagset of Petrov et al. (2011) in our experiments.10 This set C consists of the following 12 coarse-grained tags: NOUN (nouns), VERB (verbs), ADJ (adjectives), ADV (adverbs), PRON (pronouns), DET (determiners), ADP (prepositions or postpositions), NUM (numerals), CONJ (conjunctions), PRT (particles), PUNC (punctuation marks) and X (a catch-all for other categories such as abbreviations or foreign words).</S>
<S sid="161" ssid="4">Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.</S>