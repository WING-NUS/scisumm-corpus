<S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
<S sid="8" ssid="1">The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.</S>
<S sid="9" ssid="2">Training and testing is based on the Europarl corpus.</S>
<S sid="36" ssid="2">The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.</S>
<S sid="140" ssid="33">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>
<S sid="102" ssid="18">Confidence Interval: To estimate confidence intervals for the average mean scores for the systems, we use standard significance testing.</S>
<S sid="84" ssid="23">The human judges were presented with the following definition of adequacy and fluency, but no additional instructions:</S>
<S sid="11" ssid="4">To lower the barrier of entrance to the competition, we provided a complete baseline MT system, along with data resources.</S>
<S sid="126" ssid="19">The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences out-ofdomain test data.</S>
<S sid="15" ssid="8">Out-of-domain test data is from the Project Syndicate web site, a compendium of political commentary.</S>
<S sid="90" ssid="6">Another way to view the judgements is that they are less quality judgements of machine translation systems per se, but rankings of machine translation systems.</S>
"<S sid=""5"" ssid=""3"">&#8226; We evaluated translation from English, in addition to into English.</S>
    <S sid=""6"" ssid=""4"">English was again paired with German, French, and Spanish.</S>"