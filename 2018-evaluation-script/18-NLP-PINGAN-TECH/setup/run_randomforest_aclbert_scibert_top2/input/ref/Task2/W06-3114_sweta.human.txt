In this paper the author evaluates machine translation performance for six European language pairs that participated in a shared task: translating French, German, Spanish texts to English and back. Evaluation was done automatically using the BLEU score and manually on fluency and adequacy. Due to many similarly performing systems, the author was not able to draw strong conclusions on the question of correlation of manual and automatic evaluation metrics. The bias of automatic methods in favour of statistical systems seems to be less pronounced on out-of-domain test data. The manual evaluation of scoring translation on a graded scale from 1â€“5 seems to be very hard to perform. Replacing this with a ranked evaluation seems to be more suitable. Human judges also pointed out difficulties with the evaluation of long sentences.