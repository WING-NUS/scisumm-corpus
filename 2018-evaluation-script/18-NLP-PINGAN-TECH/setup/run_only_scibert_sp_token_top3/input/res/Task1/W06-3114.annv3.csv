Citance Number,Citation Marker,Citation Marker Offset,Citation Offset,Citation Text,Citation Text Clean,Citing Article,Discourse Facet,Reference Article,Reference Offset,Reference Text
1.0,"Koehn and Monz, 2006",0,0,"The official results were slightly better because a lowercase evaluation was used, see (Koehn and Monz, 2006)","The official results were slightly better because a lowercase evaluation was used, see (Koehn and Monz, 2006)",W06-3120,"Results_Citation,Hypothesis_Citation,Implication_Citation,Method_Citation,Aim_Citation",W06-3114,"'170','9','153'","<S sid=""170"" ssid=""1"">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid=""9"" ssid=""2"">Training and testing is based on the Europarl corpus.</S><S sid=""153"" ssid=""46"">This is the first time that we organized a large-scale manual evaluation.</S>"
2.0,"Koehn and Monz, 2006",0,0,"We are further focusing on the shared task of the workshop on Statistical Machine Translation, which took place last year (Koehn and Monz, 2006) and consisted in translating Spanish, German, and French texts from and to English","We are further focusing on the shared task of the workshop on Statistical Machine Translation, which took place last year (Koehn and Monz, 2006) and consisted in translating Spanish, German, and French texts from and to English",D07-1092,"Hypothesis_Citation,Results_Citation,Implication_Citation",W06-3114,"'0','170','6'","<S sid=""0"" ssid=""0"">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid=""170"" ssid=""1"">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid=""6"" ssid=""4"">English was again paired with German, French, and Spanish.</S>"
3.0,"Koehn and Monz, 2006",0,0,"For our training and test data we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference","For our training and test data we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference",C08-1074,"Method_Citation,Hypothesis_Citation,Implication_Citation",W06-3114,"'9','143','0'","<S sid=""9"" ssid=""2"">Training and testing is based on the Europarl corpus.</S><S sid=""143"" ssid=""36"">For instance, for out-ofdomain English-French, Systran has the best BLEU and manual scores.</S><S sid=""0"" ssid=""0"">Manual and Automatic Evaluation of Machine Translation between European Languages</S>"
4.0,"Koehn and Monz, 2006",0,0,"The results of last year? s workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006)","The results of last year's workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006)",W07-0718,"Method_Citation,Hypothesis_Citation,Results_Citation,Implication_Citation",W06-3114,"'62','42','50'","<S sid=""62"" ssid=""1"">While automatic measures are an invaluable tool for the day-to-day development of machine translation systems, they are only a imperfect substitute for human assessment of translation quality, or as the acronym BLEU puts it, a bilingual evaluation understudy.</S><S sid=""42"" ssid=""8"">It was our hope that this competition, which included the manual and automatic evaluation of statistical systems and one rulebased commercial system, will give further insight into the relation between automatic and manual evaluation.</S><S sid=""50"" ssid=""16"">Following this method, we repeatedly — say, 1000 times — sample sets of sentences from the output of each system, measure their BLEU score, and use these 1000 BLEU scores as basis for estimating a confidence interval.</S>"
5.0,"Koehn and Monz, 2006",0,0,"For the bi text-based annotation, we use publicly available word alignments from the Europarl corpus, automatically generated by GIZA++ for FrenchEnglish (Fr), Spanish-English (Es) and GermanEnglish (De) (Koehn and Monz, 2006)","For the bi text-based annotation, we use publicly available word alignments from the Europarl corpus, automatically generated by GIZA++ for FrenchEnglish (Fr), Spanish-English (Es) and German-English (De) (Koehn and Monz, 2006)",P07-1083,"Method_Citation,Hypothesis_Citation,Implication_Citation",W06-3114,"'9','6','132'","<S sid=""9"" ssid=""2"">Training and testing is based on the Europarl corpus.</S><S sid=""6"" ssid=""4"">English was again paired with German, French, and Spanish.</S><S sid=""132"" ssid=""25"">It is well know that language pairs such as EnglishGerman pose more challenges to machine translation systems than language pairs such as FrenchEnglish.</S>"
6.0,2006,0,0,"Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MTquality indicator","Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MTquality indicator",W07-0738,"Method_Citation,Hypothesis_Citation,Results_Citation,Implication_Citation",W06-3114,"'62','50','44'","<S sid=""62"" ssid=""1"">While automatic measures are an invaluable tool for the day-to-day development of machine translation systems, they are only a imperfect substitute for human assessment of translation quality, or as the acronym BLEU puts it, a bilingual evaluation understudy.</S><S sid=""50"" ssid=""16"">Following this method, we repeatedly — say, 1000 times — sample sets of sentences from the output of each system, measure their BLEU score, and use these 1000 BLEU scores as basis for estimating a confidence interval.</S><S sid=""44"" ssid=""10"">We computed BLEU scores for each submission with a single reference translation.</S>"
7.0,2006,0,0,"For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001)","For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001)",W07-0738,"Method_Citation,Hypothesis_Citation,Implication_Citation",W06-3114,"'50','62','139'","<S sid=""50"" ssid=""16"">Following this method, we repeatedly — say, 1000 times — sample sets of sentences from the output of each system, measure their BLEU score, and use these 1000 BLEU scores as basis for estimating a confidence interval.</S><S sid=""62"" ssid=""1"">While automatic measures are an invaluable tool for the day-to-day development of machine translation systems, they are only a imperfect substitute for human assessment of translation quality, or as the acronym BLEU puts it, a bilingual evaluation understudy.</S><S sid=""139"" ssid=""32"">Given the closeness of most systems and the wide over-lapping confidence intervals it is hard to make strong statements about the correlation between human judgements and automatic scoring methods such as BLEU.</S>"
8.0,2006,0,0,Wepresenta comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006) (see Section 3),We present a comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006) (see Section 3),W07-0738,"Method_Citation,Hypothesis_Citation,Results_Citation,Implication_Citation",W06-3114,'71',"<S sid=""71"" ssid=""10"">Presenting the output of several system allows the human judge to make more informed judgements, contrasting the quality of the different systems.</S>"
9.0,2006,0,0,Weanalyze some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006),We analyze some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006),W07-0738,"Method_Citation,Hypothesis_Citation,Results_Citation,Implication_Citation",W06-3114,'140',"<S sid=""140"" ssid=""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>"
10.0,"Koehn and Monz, 2006",0,0,"We use the same method described in (Koehn and Monz, 2006) to perform the significance test","We use the same method described in (Koehn and Monz, 2006) to perform the significance test",D07-1030,"Method_Citation,Hypothesis_Citation,Implication_Citation",W06-3114,"'102','52','113'","<S sid=""102"" ssid=""18"">Confidence Interval: To estimate confidence intervals for the average mean scores for the systems, we use standard significance testing.</S><S sid=""52"" ssid=""18"">Pairwise comparison: We can use the same method to assess the statistical significance of one system outperforming another.</S><S sid=""113"" ssid=""6"">The confidence intervals are computed by bootstrap resampling for BLEU, and by standard significance testing for the manual scores, as described earlier in the paper.</S>"
11.0,"Koehn and Monz, 2016",0,0,"We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006)","We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006)",D07-1030,"Method_Citation,Hypothesis_Citation,Results_Citation,Implication_Citation",W06-3114,"'123','68','130'","<S sid=""123"" ssid=""16"">For the manual scoring, we can distinguish only half of the systems, both in terms of fluency and adequacy.</S><S sid=""68"" ssid=""7"">We asked participants to each judge 200–300 sentences in terms of fluency and adequacy, the most commonly used manual evaluation metrics.</S><S sid=""130"" ssid=""23"">This is demonstrated by average scores over all systems, in terms of BLEU, fluency and adequacy, as displayed in Figure 5.</S>"
12.0,"Koehn and Monz, 2017",0,0,"The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002)","The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002)",W08-0406,"Method_Citation,Hypothesis_Citation,Implication_Citation",W06-3114,"'9','11','12'","<S sid=""9"" ssid=""2"">Training and testing is based on the Europarl corpus.</S><S sid=""11"" ssid=""4"">To lower the barrier of entrance to the competition, we provided a complete baseline MT system, along with data resources.</S><S sid=""12"" ssid=""5"">To summarize, we provided: The performance of the baseline system is similar to the best submissions in last year’s shared task.</S>"
13.0,2006,0,0,"Callison-Burch et al (2006 )andKoehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality","Callison-Burch et al (2006) and Koehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality",W11-1002,"Method_Citation,Hypothesis_Citation,Results_Citation,Implication_Citation",W06-3114,"'62','50','118'","<S sid=""62"" ssid=""1"">While automatic measures are an invaluable tool for the day-to-day development of machine translation systems, they are only a imperfect substitute for human assessment of translation quality, or as the acronym BLEU puts it, a bilingual evaluation understudy.</S><S sid=""50"" ssid=""16"">Following this method, we repeatedly — say, 1000 times — sample sets of sentences from the output of each system, measure their BLEU score, and use these 1000 BLEU scores as basis for estimating a confidence interval.</S><S sid=""118"" ssid=""11"">At first glance, we quickly recognize that many systems are scored very similar, both in terms of manual judgement and BLEU.</S>"
14.0,"Koehn and Monz, 2006",0,0,"The English? German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006)","The English German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006)",D07-1091,"Method_Citation,Hypothesis_Citation,Implication_Citation",W06-3114,"'9','170','0'","<S sid=""9"" ssid=""2"">Training and testing is based on the Europarl corpus.</S><S sid=""170"" ssid=""1"">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid=""0"" ssid=""0"">Manual and Automatic Evaluation of Machine Translation between European Languages</S>"
15.0,"Koehn and Monz, 2006",0,0,"We report results on the development test set, which is also the out-of-domain test set of the WMT06 workshop shared task (Koehn and Monz, 2006)","We report results on the development test set, which is also the out-of-domain test set of the WMT06 workshop shared task (Koehn and Monz, 2006)",D07-1091,"Method_Citation,Hypothesis_Citation,Implication_Citation",W06-3114,"'83','15','13'","<S sid=""83"" ssid=""22"">The number of judgements is additionally fragmented by our breakup of sentences into in-domain and out-of-domain.</S><S sid=""15"" ssid=""8"">Out-of-domain test data is from the Project Syndicate web site, a compendium of political commentary.</S><S sid=""13"" ssid=""6"">We are currently working on a complete open source implementation of a training and decoding system, which should become available over the summer. pus, from which also the in-domain test set is taken.</S>"
16.0,"Koehn and Monz, 2006",0,0,"A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Ma chine Translation (Koehn and Monz, 2006)","A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Machine Translation (Koehn and Monz, 2006)",P07-1108,"Results_Citation,Hypothesis_Citation,Implication_Citation,Method_Citation,Aim_Citation",W06-3114,"'170','0','64'","<S sid=""170"" ssid=""1"">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid=""0"" ssid=""0"">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid=""64"" ssid=""3"">Also, the argument has been made that machine translation performance should be evaluated via task-based evaluation metrics, i.e. how much it assists performing a useful task, such as supporting human translators or aiding the analysis of texts.</S>"
18.0,"Koehn and Monz, 2006",0,0,"For the same reason, human evaluation metrics based on adequacy and fluency were not suitable either (Koehn and Monz, 2006)","For the same reason, human evaluation metrics based on adequacy and fluency were not suitable either (Koehn and Monz, 2006)",E12-3010,"Method_Citation,Hypothesis_Citation,Results_Citation,Implication_Citation",W06-3114,"'68','80','123'","<S sid=""68"" ssid=""7"">We asked participants to each judge 200–300 sentences in terms of fluency and adequacy, the most commonly used manual evaluation metrics.</S><S sid=""80"" ssid=""19"">We collected around 300–400 judgements per judgement type (adequacy or fluency), per system, per language pair.</S><S sid=""123"" ssid=""16"">For the manual scoring, we can distinguish only half of the systems, both in terms of fluency and adequacy.</S>"
19.0,"Koehn and Monz, 2006",0,0,"The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008)","The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008)",W09-0402,"Method_Citation,Hypothesis_Citation,Implication_Citation",W06-3114,"'44','170','0'","<S sid=""44"" ssid=""10"">We computed BLEU scores for each submission with a single reference translation.</S><S sid=""170"" ssid=""1"">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid=""0"" ssid=""0"">Manual and Automatic Evaluation of Machine Translation between European Languages</S>"
