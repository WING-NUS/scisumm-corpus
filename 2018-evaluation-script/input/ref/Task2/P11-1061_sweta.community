 <S sid="9" ssid="5">Unfortunately, the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al., 2010), making its practical usability questionable at best.</S>
<S sid="47" ssid="13">Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).</S>
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
<S sid="70" ssid="1">Given the bilingual graph described in the previous section, we can use label propagation to project the English POS labels to the foreign language.</S>
<S sid="52" ssid="18">This is similar to stacking the different feature instantiations into long (sparse) vectors and computing the cosine similarity between them.</S>
 <S sid="83" ssid="14">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value &#964;: We describe how we choose &#964; in &#167;6.4.</S>
<S sid="113" ssid="13">For each language under consideration, Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S>
<S sid="3" ssid="3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010).</S>
<S sid="18" ssid="14">To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
 <S sid="13" ssid="9">(2009) study related but different multilingual grammar and tagger induction tasks, where it is assumed that no labeled data at all is available.</S>
 <S sid="3" ssid="3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010).</S>
<S sid="120" ssid="20">We were intentionally lenient with our baselines: bilingual information by projecting POS tags directly across alignments in the parallel data.</S>
<S sid="2" ssid="2">Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages.</S>
<S sid="19" ssid="15">Syntactic universals are a well studied concept in linguistics (Carnie, 2002; Newmeyer, 2005), and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S>
<S sid="153" ssid="16">Figure 2 shows an excerpt of a sentence from the Italian test set and the tags assigned by four different models, as well as the gold tags.</S>
<S sid="161" ssid="4">Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.</S>
<S sid="23" ssid="19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>