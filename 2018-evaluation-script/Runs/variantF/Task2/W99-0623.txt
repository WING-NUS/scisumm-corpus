Because we are working with only three parsers  the only situation in which context will help us is when it can indicate we should choose to believe a single parser that disagrees with the majority hypothesis instead of the majority hypothesis itself.The average individual parser accuracy was reduced by more than 5% when we added this new parser  but the precision of the constituent voting technique was the only result that decreased significantly.One side of the decision making process is when we choose to believe a constituent should be in the parse  even though only one parser suggests it.We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.In general  the lemma of the previous section does not ensure that all the productions in the combined parse are found in the grammars of the member parsers.IL+-1Proof: Assume a pair of crossing constituents appears in the output of the constituent voting technique using k parsers.Similarly Figures 1 and 2 show how the isolated constituent precision varies by sentence length and the size of the span of the hypothesized constituent.None of the parsers produce parses with crossing brackets  so none of them votes for both of the assumed constituents.We include a constituent in our hypothesized parse if it appears in the output of a majority of the parsers.The entries in this table can be compared with those of Table 3 to see how the performance of the combining techniques degrades in the presence of an inferior parser.The precision and recall measures (described in more detail in Section 3) used in evaluating Treebank parsing treat each constituent as a separate entity  a minimal unit of correctness.The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser  and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.In this case we are interested in finding' the maximum probability parse  ri  and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers.It was then tested on section 22 of the Treebank in conjunction with the other parsers.Another technique for parse hybridization is to use a naïve Bayes classifier to determine which constituents to include in the parse.We have presented two general approaches to studying parser combination: parser switching and parse hybridization.The first shows how constituent features and context do not help in deciding which parser to trust.We would like to thank Eugene Charniak  Michael Collins  and Adwait Ratnaparkhi for enabling all of this research by providing us with their parsers and helpful comments.This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.There are simply not enough votes remaining to allow any of the crossing structures to enter the hypothesized constituent set.This technique has the advantage of requiring no training  but it has the disadvantage of treating all parsers equally even though they may have differing accuracies or may specialize in modeling different phenomena.• Similarly  when the naïve Bayes classifier is configured such that the constituents require estimated probabilities strictly larger than 0.5 to be accepted  there is not enough probability mass remaining on crossing brackets for them to be included in the hypothesis.The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank  leaving only sections 22 and 23 completely untouched during the development of any of the parsers.The set of candidate constituents comes from the union of all the constituents suggested by the member parsers.Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.Furthermore  we know one of the original parses will be the hypothesized parse  so the direct method of determining which one is best is to compute the probability of each of the candidate parses using the probabilistic model we developed in Section 2.1.Another way to interpret this is that less than 5% of the correct constituents are missing from the hypotheses generated by the union of the three parsers.It is possible one could produce better models by introducing features describing constituents and their contexts because one parser could be much better than the majority of the others in particular situations.For our experiments we also report the mean of precision and recall  which we denote by (P + R)I2 and F-measure.We model each parse as the decisions made to create it  and model those decisions as independent events.The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.We used section 23 as the development set for our combining techniques  and section 22 only for final testing.Once again we present both a non-parametric and a parametric technique for this task.This is the parse that is closest to the centroid of the observed parses under the similarity metric.The hypothesized parse is then the set of constituents that are likely (P > 0.5) to be in the parse according to this model.