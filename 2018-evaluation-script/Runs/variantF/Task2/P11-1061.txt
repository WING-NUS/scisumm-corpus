We would also like to thank Amarnag Subramanya for helping us with the implementation of label propagation and Shankar Kumar for access to the parallel data.Supervised part-of-speech (POS) taggers  for example  approach the level of inter-annotator agreement (Shen et al.  2007  97.3% accuracy for English).The feature-HMM model works better for all languages  generalizing the results achieved for English by Berg-Kirkpatrick et al. (2010).This scenario is applicable to a large set of languages and has been considered by a number of authors in the past (Alshawi et al.  2000; Xi and Hwa  2005; Ganchev et al.  2009).For comparison  the completely unsupervised feature-HMM baseline accuracy on the universal POS tags for English is 79.4%  and goes up to 88.7% with a treebank dictionary.While there might be some controversy about the exact definition of such a tagset  these 12 categories cover the most frequent part-of-speech and exist in one form or another in all of the languages that we studied.The focus of this work is on building POS taggers for foreign languages  assuming that we have an English POS tagger and some parallel text between the two languages.It would have therefore also been possible to use the integer programming (IP) based approach of Ravi and Knight (2009) instead of the feature-HMM for POS induction on the foreign side.Therefore  the number of fine tags varied across languages for our experiments; however  one could as well have fixed the set of HMM states to be a constant across languages  and created one mapping to the universal POS tagset.Given this similarity function  we define a nearest neighbor graph  where the edge weight for the n most similar vertices is set to the value of the similarity function and to 0 for all other vertices.10) and trained both EM and L-BFGS for 1000 iterations.We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections  and bridge the gap between purely supervised and unsupervised POS tagging models.As indicated by bolding  for seven out of eight languages the improvements of the “With LP” setting are statistically significant with respect to the other models  including the “No LP” setting.11 Overall  it performs 10.4% better than the hitherto state-of-the-art feature-HMM baseline  and 4.6% better than direct projection  when we macro-average the accuracy over all languages.Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).For each trigram type x2 x3 x4 in a sequence x1 x2 x3 x4 x5  we count how many times that trigram type co-occurs with the different instantiations of each concept  and compute the point-wise mutual information (PMI) between the two.5 The similarity between two trigram types is given by summing over the PMI values over feature instantiations that they have in common.We use two different similarity functions to define the edge weights among the foreign vertices and between vertices from different languages.Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.Since our graph is built from a parallel corpus  we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g.  when the trigram type and the feature instantiation don’t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall  and we therefore extract only intersected high-confidence (> 0.9) alignments De�f.This stage of label propagation results in a tag distribution ri over labels y  which encodes the proportion of times the middle word of ui E Vf aligns to English words vy tagged with label y: The second stage consists of running traditional label propagation to propagate labels from these peripheral vertices Vf� to all foreign language vertices in the graph  optimizing the following objective: 5 POS Induction After running label propagation (LP)  we compute tag probabilities for foreign word types x by marginalizing the POS tag distributions of foreign trigrams ui = x− x x+ over the left and right context words: where the qi (i = 1  ...   |Vf|) are the label distributions over the foreign language vertices and µ and ν are hyperparameters that we discuss in §6.4.Because we are interested in applying our techniques to languages for which no labeled resources are available  we paid particular attention to minimize the number of free parameters and used the same hyperparameters for all language pairs.Graph construction for structured prediction problems such as POS tagging is non-trivial: on the one hand  using individual words as the vertices throws away the context necessary for disambiguation; on the other hand  it is unclear how to define (sequence) similarity if the vertices correspond to entire sentences.These universal POS categories not only facilitate the transfer of POS information from one language to another  but also relieve us from using controversial evaluation metrics 2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.Our “Projection” baseline is able to benefit from the bilingual information and greatly improves upon the monolingual baselines  but falls short of the “No LP” model by 2.5% on an average.This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.In the first stage  we run a single step of label propagation  which transfers the label distributions from the English vertices to the connected foreign language vertices (say  Vf�) at the periphery of the graph.Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.For each language  we took the same number of sentences from the bitext as there are in its treebank  and trained a supervised feature-HMM.The number of latent HMM states for each language in our experiments was set to the number of fine tags in the language’s treebank.We use label propagation in two stages to generate soft labels on all the vertices in the graph.To this end  we construct a bilingual graph over word types to establish a connection between the two languages (§3)  and then use graph label propagation to project syntactic information from English to the foreign language (§4).Label propagation is used to propagate these tags inwards and results in tag distributions for the middle word of each Italian trigram.For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.To bridge this gap  we consider a practically motivated scenario  in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest  but that we have access to parallel data with a resource-rich language.We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.For a sentence x and a state sequence z  a first order Markov model defines a distribution: (9) where Val(X) corresponds to the entire vocabulary.To establish a soft correspondence between the two languages  we use a second similarity function  which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data  our goal is to project syntactic information from the English side to the foreign side.Supervised learning approaches have advanced the state-of-the-art on a variety of tasks in natural language processing  resulting in highly accurate systems.To initialize the graph for label propagation we use a supervised English tagger to label the English side of the bitext.7 We then simply count the individual labels of the English tokens and normalize the counts to produce tag distributions over English word types.However  we needed to restrict ourselves to these languages in order to be able to evaluate the performance of our approach.Given the bilingual graph described in the previous section  we can use label propagation to project the English POS labels to the foreign language.