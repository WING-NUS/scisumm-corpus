As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.While we could have smoothed in the same fashion  we choose instead to use standard deleted interpolation.We created a parser based upon the maximumentropy-inspired model of the last section  smoothed using standard deleted interpolation.In this notation the above equation takes the following form: Next we describe how we assign a probability to the expansion e of a constituent.In the previous sections we have concentrated on the relation of the parser to a maximumentropy approach  the aspect of the parser that is most novel.One of the first and without doubt the most significant change we made in the current parser is to move from two stages of probabilistic decisions at each node to three.That the previous three best parsers on this test [5 9 17] all perform within a percentage point of each other  despite quite different basic mechanisms  led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training  and to conjecture that perhaps we were at it.The major problem confronting the author of a generative parser is what information to use to condition the probabilities required in the model  and how to smooth the empirically obtained probabilities to take the sting out of the sparse data problems that are inevitable with even the most modest conditioning.Note that the As on both ends of the expansion in Expression 2 are conditioned just like any other label in the expansion.The author would like to thank Mark Johnson and all the rest of the Brown Laboratory for Linguistic Information Processing. is s. Then for any s the parser returns the parse ir that maximizes this probability.Note that the definitions of labeled precision and recall are those given in [9] and used in all of the previous work.As we discuss in more detail in Section 5  several different features in the context surrounding c are useful to include in H: the label  head pre-terminal and head of the parent of c (denoted as lp  tp  hp)  the label of c's left sibling (lb for &quot;before&quot;)  and the label of the grandparent of c (la).Now consider computing a conditional probability p(a H) with a set of features h that connect a to the history H. In a log-linear model the probability function takes the following form: Here the Ai are weights between negative and positive infinity that indicate the relative importance of a feature: the more relevant the feature to the value of the probability  the higher the absolute value of the associated A.This is indicated in Figure 2  where the model labeled &quot;Best&quot; has precision of 89.8% and recall of 89.6% for an average of 89.7%  0.4% lower than the results on the official test corpus.We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.Indeed  we initiated this line of work in an attempt to create a parser that would be flexible enough to allow modifications for parsing down to more semantic levels of detail.This quantity is a relatively intuitive one (as  for example  it is the quantity used in a PCFG to relate words to their pre-terminals) and it seems particularly good to condition upon here since we use it  in effect  as the unsmoothed probability upon which all smoothing of p(h) is based.To compute a probability in a log-linear model one first defines a set of &quot;features&quot;  functions from the space of configurations over which one is trying to compute probabilities to integers that denote the number of times some pattern occurs in the input.The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c  t(c) (t for &quot;tag&quot;)  then the lexical head of c  h(c)  and then the expansion of c into further constituents e(c).The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).This requires finding the appropriate Ais for Equation 3  which is accomplished using an algorithm such as iterative scaling [II] in which values for the Ai are initially &quot;guessed&quot; and then modified until they converge on stable values.The results for the new parser as well as for the previous top-three individual parsers on this corpus are given in Figure 1.Also  the earlier parser uses two techniques not employed in the current parser.In contrast  the current parser first guesses the head's pre-terminal  then the head  and then the expansion.Maximum-entropy models have two benefits for a parser builder.Now we observe that if we were to use a maximum-entropy approach but run iterative scaling zero times  we would  in fact  just have Equation 7.Rather than conditioning each term on the previous ones  they are now conditioned only on those aspects of the history that seem most relevant.This is consistent with the average precision/recall of 86.6% for [5] mentioned above  as the latter was on the test corpus and the former on the development corpus.For example  in the Penn Treebank a vp with both main and auxiliary verbs has the structure shown in Figure 3.One way to do this is to modify the simple version shown in Equation 6 to allow this: Note the changes to the last three terms in Equation 7.As already noted  Char97 first guesses the lexical head of a constituent and then  given the head  guesses the PCFG rule used to expand the constituent in question.In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / â€” that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar  and a right-hand side that is a sequence of one or more such symbols.Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.Though in some respects not quite as flexible as true maximum entropy  it is much simpler and  in our estimation  has benefits when it comes to smoothing.A vp coordinate structure is defined here as a constituent with two or more vp children  one or more of the constituents comma  cc  conjp (conjunctive phrase)  and nothing else; coordinate np phrases are defined similarly.(Our experience is that rather than requiring 50 or so iterations  three suffice.)In the more interesting version  Equation 7  this is not true in general  but one would not expect it to differ much from one  and we assume that as long as we are not publishing the raw probabilities (as we would be doing  for example  in publishing perplexity results) the difference from one should be unimportant.The hope is that by doing this we will have less difficulty with the splitting of conditioning events  and thus somewhat less difficulty with sparse data.In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.The results of [13] achieved by combining the aforementioned three-best parsers also suggest that the limit on tree-bank trained parsers is much higher than previously thought.From our perspective  perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head.So  e.g.  even if the word &quot;conflating&quot; does not appear in the training corpus (and it does not)  the &quot;ng&quot; ending allows our program to guess with relative security that the word has the vbg pre-terminal  and thus the probability of various rule expansions can be considerable sharpened.