Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,W06-3114,W06-3120,0,"Koehn and Monz, 2006",0,"The official results were slightly better because a lowercase evaluation was used, see (Koehn and Monz, 2006)","The official results were slightly better because a lowercase evaluation was used, see (Koehn and Monz, 2006)","['161', '59', '145', '94', '47']","<S sid =""161"" ssid = ""54"">Since all long sentence translation are somewhat muddled  even a contrastive evaluation between systems was difficult.</S><S sid =""59"" ssid = ""25"">The sign test checks  how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.</S><S sid =""145"" ssid = ""38"">This is can not be the only explanation  since the discrepancy still holds  for instance  for out-of-domain French-English  where Systran receives among the best adequacy and fluency scores  but a worse BLEU score than all but one statistical system.</S><S sid =""94"" ssid = ""10"">On the other hand  when all systems produce muddled output  but one is better  and one is worse  but not completely wrong  a judge is inclined to hand out judgements of 4  3  and 2.</S><S sid =""47"" ssid = ""13"">Because of this  we retokenized and lowercased submitted output with our own tokenizer  which was also used to prepare the training and test data.</S>",['Method_Citation']
2,W06-3114,D07-1092,0,"Koehn and Monz, 2006",0,"We are further focusing on the shared task of the workshop on Statistical Machine Translation, which took place last year (Koehn and Monz, 2006) and consisted in translating Spanish, German, and French texts from and to English","We are further focusing on the shared task of the workshop on Statistical Machine Translation, which took place last year (Koehn and Monz, 2006) and consisted in translating Spanish, German, and French texts from and to English","['163', '46', '109', '123', '18']","<S sid =""163"" ssid = ""56"">Not every annotator was fluent in both the source and the target language.</S><S sid =""46"" ssid = ""12"">By taking the ratio of matching n-grams to the total number of n-grams in the system output  we obtain the precision pn for each n-gram order n. These values for n-gram precision are combined into a BLEU score: The formula for the BLEU metric also includes a brevity penalty for too short output  which is based on the total number of words in the system output c and in the reference r. BLEU is sensitive to tokenization.</S><S sid =""109"" ssid = ""2"">The scores and confidence intervals are detailed first in the Figures 7–10 in table form (including ranks)  and then in graphical form in Figures 11–16.</S><S sid =""123"" ssid = ""16"">For the manual scoring  we can distinguish only half of the systems  both in terms of fluency and adequacy.</S><S sid =""18"" ssid = ""11"">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S>",['Method_Citation']
3,W06-3114,C08-1074,0,"Koehn and Monz, 2006",0,"For our training and test data we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference","For our training and test data we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference","['108', '154', '18', '34', '46']","<S sid =""108"" ssid = ""1"">The results of the manual and automatic evaluation of the participating system translations is detailed in the figures at the end of this paper.</S><S sid =""154"" ssid = ""47"">While we used the standard metrics of the community  the we way presented translations and prompted for assessment differed from other evaluation campaigns.</S><S sid =""18"" ssid = ""11"">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S><S sid =""34"" ssid = ""27"">For more on the participating systems  please refer to the respective system description in the proceedings of the workshop.</S><S sid =""46"" ssid = ""12"">By taking the ratio of matching n-grams to the total number of n-grams in the system output  we obtain the precision pn for each n-gram order n. These values for n-gram precision are combined into a BLEU score: The formula for the BLEU metric also includes a brevity penalty for too short output  which is based on the total number of words in the system output c and in the reference r. BLEU is sensitive to tokenization.</S>",['Method_Citation']
4,W06-3114,W07-0718,0,"Koehn and Monz, 2006",0,"The results of last year? s workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006)","The results of last year's workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006)","['108', '90', '123', '138', '71']","<S sid =""108"" ssid = ""1"">The results of the manual and automatic evaluation of the participating system translations is detailed in the figures at the end of this paper.</S><S sid =""90"" ssid = ""6"">Another way to view the judgements is that they are less quality judgements of machine translation systems per se  but rankings of machine translation systems.</S><S sid =""123"" ssid = ""16"">For the manual scoring  we can distinguish only half of the systems  both in terms of fluency and adequacy.</S><S sid =""138"" ssid = ""31"">Hence  the different averages of manual scores for the different language pairs reflect the behaviour of the judges  not the quality of the systems on different language pairs.</S><S sid =""71"" ssid = ""10"">Presenting the output of several system allows the human judge to make more informed judgements  contrasting the quality of the different systems.</S>",['Implication_Citation']
5,W06-3114,P07-1083,0,"Koehn and Monz, 2006",0,"For the bi text-based annotation, we use publicly available word alignments from the Europarl corpus, automatically generated by GIZA++ for FrenchEnglish (Fr), Spanish-English (Es) and GermanEnglish (De) (Koehn and Monz, 2006)","For the bi text-based annotation, we use publicly available word alignments from the Europarl corpus, automatically generated by GIZA++ for FrenchEnglish (Fr), Spanish-English (Es) and German-English (De) (Koehn and Monz, 2006)","['113', '154', '46', '18', '47']","<S sid =""113"" ssid = ""6"">The confidence intervals are computed by bootstrap resampling for BLEU  and by standard significance testing for the manual scores  as described earlier in the paper.</S><S sid =""154"" ssid = ""47"">While we used the standard metrics of the community  the we way presented translations and prompted for assessment differed from other evaluation campaigns.</S><S sid =""46"" ssid = ""12"">By taking the ratio of matching n-grams to the total number of n-grams in the system output  we obtain the precision pn for each n-gram order n. These values for n-gram precision are combined into a BLEU score: The formula for the BLEU metric also includes a brevity penalty for too short output  which is based on the total number of words in the system output c and in the reference r. BLEU is sensitive to tokenization.</S><S sid =""18"" ssid = ""11"">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S><S sid =""47"" ssid = ""13"">Because of this  we retokenized and lowercased submitted output with our own tokenizer  which was also used to prepare the training and test data.</S>",['Method_Citation']
6,W06-3114,W07-0738,0,2006,0,"Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MTquality indicator","Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MTquality indicator","['145', '113', '140', '130', '156']","<S sid =""145"" ssid = ""38"">This is can not be the only explanation  since the discrepancy still holds  for instance  for out-of-domain French-English  where Systran receives among the best adequacy and fluency scores  but a worse BLEU score than all but one statistical system.</S><S sid =""113"" ssid = ""6"">The confidence intervals are computed by bootstrap resampling for BLEU  and by standard significance testing for the manual scores  as described earlier in the paper.</S><S sid =""140"" ssid = ""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S><S sid =""130"" ssid = ""23"">This is demonstrated by average scores over all systems  in terms of BLEU  fluency and adequacy  as displayed in Figure 5.</S><S sid =""156"" ssid = ""49"">Almost all annotators reported difficulties in maintaining a consistent standard for fluency and adequacy judgements  but nevertheless most did not explicitly move towards a ranking-based evaluation.</S>",['Method_Citation']
7,W06-3114,W07-0738,0,2006,0,"For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001)","For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001)","['113', '130', '56', '154', '84']","<S sid =""113"" ssid = ""6"">The confidence intervals are computed by bootstrap resampling for BLEU  and by standard significance testing for the manual scores  as described earlier in the paper.</S><S sid =""130"" ssid = ""23"">This is demonstrated by average scores over all systems  in terms of BLEU  fluency and adequacy  as displayed in Figure 5.</S><S sid =""56"" ssid = ""22"">The bootstrap method has been critized by Riezler and Maxwell (2005) and Collins et al. (2005)  as being too optimistic in deciding for statistical significant difference between systems.</S><S sid =""154"" ssid = ""47"">While we used the standard metrics of the community  the we way presented translations and prompted for assessment differed from other evaluation campaigns.</S><S sid =""84"" ssid = ""23"">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S>",['Method_Citation']
8,W06-3114,W07-0738,0,2006,0,Wepresenta comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006) (see Section 3),We present a comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006) (see Section 3),"['138', '108', '123', '103', '46']","<S sid =""138"" ssid = ""31"">Hence  the different averages of manual scores for the different language pairs reflect the behaviour of the judges  not the quality of the systems on different language pairs.</S><S sid =""108"" ssid = ""1"">The results of the manual and automatic evaluation of the participating system translations is detailed in the figures at the end of this paper.</S><S sid =""123"" ssid = ""16"">For the manual scoring  we can distinguish only half of the systems  both in terms of fluency and adequacy.</S><S sid =""103"" ssid = ""19"">Given a set of n sentences  we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d  x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric  we want to be able to rank different systems against each other  for which we need assessments of statistical significance on the differences between a pair of systems.</S><S sid =""46"" ssid = ""12"">By taking the ratio of matching n-grams to the total number of n-grams in the system output  we obtain the precision pn for each n-gram order n. These values for n-gram precision are combined into a BLEU score: The formula for the BLEU metric also includes a brevity penalty for too short output  which is based on the total number of words in the system output c and in the reference r. BLEU is sensitive to tokenization.</S>",['Method_Citation']
9,W06-3114,W07-0738,0,2006,0,Weanalyze some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006),We analyze some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006),"['140', '123', '154', '113', '54']","<S sid =""140"" ssid = ""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S><S sid =""123"" ssid = ""16"">For the manual scoring  we can distinguish only half of the systems  both in terms of fluency and adequacy.</S><S sid =""154"" ssid = ""47"">While we used the standard metrics of the community  the we way presented translations and prompted for assessment differed from other evaluation campaigns.</S><S sid =""113"" ssid = ""6"">The confidence intervals are computed by bootstrap resampling for BLEU  and by standard significance testing for the manual scores  as described earlier in the paper.</S><S sid =""54"" ssid = ""20"">To check for this  we do pairwise bootstrap resampling: Again  we repeatedly sample sets of sentences  this time from both systems  and compare their BLEU scores on these sets.</S>",['Method_Citation']
10,W06-3114,D07-1030,0,"Koehn and Monz, 2006",0,"We use the same method described in (Koehn and Monz, 2006) to perform the significance test","We use the same method described in (Koehn and Monz, 2006) to perform the significance test","['52', '34', '58', '46', '154']","<S sid =""52"" ssid = ""18"">Pairwise comparison: We can use the same method to assess the statistical significance of one system outperforming another.</S><S sid =""34"" ssid = ""27"">For more on the participating systems  please refer to the respective system description in the proceedings of the workshop.</S><S sid =""58"" ssid = ""24"">We divide up each test set into blocks of 20 sentences (100 blocks for the in-domain test set  53 blocks for the out-of-domain test set)  check for each block  if one system has a higher BLEU score than the other  and then use the sign test.</S><S sid =""46"" ssid = ""12"">By taking the ratio of matching n-grams to the total number of n-grams in the system output  we obtain the precision pn for each n-gram order n. These values for n-gram precision are combined into a BLEU score: The formula for the BLEU metric also includes a brevity penalty for too short output  which is based on the total number of words in the system output c and in the reference r. BLEU is sensitive to tokenization.</S><S sid =""154"" ssid = ""47"">While we used the standard metrics of the community  the we way presented translations and prompted for assessment differed from other evaluation campaigns.</S>",['Implication_Citation']
11,W06-3114,D07-1030,0,"Koehn and Monz, 2016",0,"We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006)","We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006)","['130', '123', '118', '109', '42']","<S sid =""130"" ssid = ""23"">This is demonstrated by average scores over all systems  in terms of BLEU  fluency and adequacy  as displayed in Figure 5.</S><S sid =""123"" ssid = ""16"">For the manual scoring  we can distinguish only half of the systems  both in terms of fluency and adequacy.</S><S sid =""118"" ssid = ""11"">At first glance  we quickly recognize that many systems are scored very similar  both in terms of manual judgement and BLEU.</S><S sid =""109"" ssid = ""2"">The scores and confidence intervals are detailed first in the Figures 7–10 in table form (including ranks)  and then in graphical form in Figures 11–16.</S><S sid =""42"" ssid = ""8"">It was our hope that this competition  which included the manual and automatic evaluation of statistical systems and one rulebased commercial system  will give further insight into the relation between automatic and manual evaluation.</S>",['Method_Citation']
12,W06-3114,W08-0406,0,"Koehn and Monz, 2017",0,"The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002)","The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002)","['46', '113', '117', '163', '108']","<S sid =""46"" ssid = ""12"">By taking the ratio of matching n-grams to the total number of n-grams in the system output  we obtain the precision pn for each n-gram order n. These values for n-gram precision are combined into a BLEU score: The formula for the BLEU metric also includes a brevity penalty for too short output  which is based on the total number of words in the system output c and in the reference r. BLEU is sensitive to tokenization.</S><S sid =""113"" ssid = ""6"">The confidence intervals are computed by bootstrap resampling for BLEU  and by standard significance testing for the manual scores  as described earlier in the paper.</S><S sid =""117"" ssid = ""10"">For instance: if 10 systems participate  and one system does better than 3 others  worse then 2  and is not significant different from the remaining 4  its rank is in the interval 3–7.</S><S sid =""163"" ssid = ""56"">Not every annotator was fluent in both the source and the target language.</S><S sid =""108"" ssid = ""1"">The results of the manual and automatic evaluation of the participating system translations is detailed in the figures at the end of this paper.</S>",['Implication_Citation']
13,W06-3114,W11-1002,0,2006,0,"Callison-Burch et al (2006 )andKoehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality","Callison-Burch et al (2006) and Koehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality","['130', '139', '62', '168', '113']","<S sid =""130"" ssid = ""23"">This is demonstrated by average scores over all systems  in terms of BLEU  fluency and adequacy  as displayed in Figure 5.</S><S sid =""139"" ssid = ""32"">Given the closeness of most systems and the wide over-lapping confidence intervals it is hard to make strong statements about the correlation between human judgements and automatic scoring methods such as BLEU.</S><S sid =""62"" ssid = ""1"">While automatic measures are an invaluable tool for the day-to-day development of machine translation systems  they are only a imperfect substitute for human assessment of translation quality  or as the acronym BLEU puts it  a bilingual evaluation understudy.</S><S sid =""168"" ssid = ""61"">Annotators argued for the importance of having correct and even multiple references.</S><S sid =""113"" ssid = ""6"">The confidence intervals are computed by bootstrap resampling for BLEU  and by standard significance testing for the manual scores  as described earlier in the paper.</S>",['Method_Citation']
14,W06-3114,D07-1091,0,"Koehn and Monz, 2006",0,"The English? German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006)","The English German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006)","['58', '163', '113', '46', '9']","<S sid =""58"" ssid = ""24"">We divide up each test set into blocks of 20 sentences (100 blocks for the in-domain test set  53 blocks for the out-of-domain test set)  check for each block  if one system has a higher BLEU score than the other  and then use the sign test.</S><S sid =""163"" ssid = ""56"">Not every annotator was fluent in both the source and the target language.</S><S sid =""113"" ssid = ""6"">The confidence intervals are computed by bootstrap resampling for BLEU  and by standard significance testing for the manual scores  as described earlier in the paper.</S><S sid =""46"" ssid = ""12"">By taking the ratio of matching n-grams to the total number of n-grams in the system output  we obtain the precision pn for each n-gram order n. These values for n-gram precision are combined into a BLEU score: The formula for the BLEU metric also includes a brevity penalty for too short output  which is based on the total number of words in the system output c and in the reference r. BLEU is sensitive to tokenization.</S><S sid =""9"" ssid = ""2"">Training and testing is based on the Europarl corpus.</S>",['Method_Citation']
15,W06-3114,D07-1091,0,"Koehn and Monz, 2006",0,"We report results on the development test set, which is also the out-of-domain test set of the WMT06 workshop shared task (Koehn and Monz, 2006)","We report results on the development test set, which is also the out-of-domain test set of the WMT06 workshop shared task (Koehn and Monz, 2006)","['18', '58', '108', '13', '163']","<S sid =""18"" ssid = ""11"">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S><S sid =""58"" ssid = ""24"">We divide up each test set into blocks of 20 sentences (100 blocks for the in-domain test set  53 blocks for the out-of-domain test set)  check for each block  if one system has a higher BLEU score than the other  and then use the sign test.</S><S sid =""108"" ssid = ""1"">The results of the manual and automatic evaluation of the participating system translations is detailed in the figures at the end of this paper.</S><S sid =""13"" ssid = ""6"">We are currently working on a complete open source implementation of a training and decoding system  which should become available over the summer. pus  from which also the in-domain test set is taken.</S><S sid =""163"" ssid = ""56"">Not every annotator was fluent in both the source and the target language.</S>",['Implication_Citation']
16,W06-3114,P07-1108,0,"Koehn and Monz, 2006",0,"A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Ma chine Translation (Koehn and Monz, 2006)","A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Machine Translation (Koehn and Monz, 2006)","['8', '33', '46', '62', '12']","<S sid =""8"" ssid = ""1"">The evaluation framework for the shared task is similar to the one used in last year’s shared task.</S><S sid =""33"" ssid = ""26"">While building a machine translation system is a serious undertaking  in future we hope to attract more newcomers to the field by keeping the barrier of entry as low as possible.</S><S sid =""46"" ssid = ""12"">By taking the ratio of matching n-grams to the total number of n-grams in the system output  we obtain the precision pn for each n-gram order n. These values for n-gram precision are combined into a BLEU score: The formula for the BLEU metric also includes a brevity penalty for too short output  which is based on the total number of words in the system output c and in the reference r. BLEU is sensitive to tokenization.</S><S sid =""62"" ssid = ""1"">While automatic measures are an invaluable tool for the day-to-day development of machine translation systems  they are only a imperfect substitute for human assessment of translation quality  or as the acronym BLEU puts it  a bilingual evaluation understudy.</S><S sid =""12"" ssid = ""5"">To summarize  we provided: The performance of the baseline system is similar to the best submissions in last year’s shared task.</S>",['Method_Citation']
18,W06-3114,E12-3010,0,"Koehn and Monz, 2006",0,"For the same reason, human evaluation metrics based on adequacy and fluency were not suitable either (Koehn and Monz, 2006)","For the same reason, human evaluation metrics based on adequacy and fluency were not suitable either (Koehn and Monz, 2006)","['155', '163', '145', '84', '9']","<S sid =""155"" ssid = ""48"">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S><S sid =""163"" ssid = ""56"">Not every annotator was fluent in both the source and the target language.</S><S sid =""145"" ssid = ""38"">This is can not be the only explanation  since the discrepancy still holds  for instance  for out-of-domain French-English  where Systran receives among the best adequacy and fluency scores  but a worse BLEU score than all but one statistical system.</S><S sid =""84"" ssid = ""23"">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid =""9"" ssid = ""2"">Training and testing is based on the Europarl corpus.</S>",['Aim_Citation']
19,W06-3114,W09-0402,0,"Koehn and Monz, 2006",0,"The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008)","The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008)","['108', '34', '46', '163', '8']","<S sid =""108"" ssid = ""1"">The results of the manual and automatic evaluation of the participating system translations is detailed in the figures at the end of this paper.</S><S sid =""34"" ssid = ""27"">For more on the participating systems  please refer to the respective system description in the proceedings of the workshop.</S><S sid =""46"" ssid = ""12"">By taking the ratio of matching n-grams to the total number of n-grams in the system output  we obtain the precision pn for each n-gram order n. These values for n-gram precision are combined into a BLEU score: The formula for the BLEU metric also includes a brevity penalty for too short output  which is based on the total number of words in the system output c and in the reference r. BLEU is sensitive to tokenization.</S><S sid =""163"" ssid = ""56"">Not every annotator was fluent in both the source and the target language.</S><S sid =""8"" ssid = ""1"">The evaluation framework for the shared task is similar to the one used in last year’s shared task.</S>",['Aim_Citation']
