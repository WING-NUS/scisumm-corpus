Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P08-1102,C08-1049,0,2008,0,"Following Jiang et al (2008), we describe segmentation and Joint S& amp; T as below: For a given Chinese sentence appearing as a character sequence: C 1: n= C 1 C 2.","Following Jiang et al (2008), we describe segmentation and Joint S& amp; T as below: For a given Chinese sentence appearing as a character sequence: C 1: n= C 1 C 2.","['94', '0', '71', '46', '1']","<S sid =""94"" ssid = ""5"">With precision P and recall R  the balance F-measure is defined as: F = 2PR/(P + R).</S><S sid =""0"" ssid = ""0"">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S sid =""71"" ssid = ""22"">Using W = w1:m to denote the word sequence  T = t1:m to denote the corresponding POS sequence  P (T |W) to denote the probability that W is labelled as T  and P(W|T) to denote the probability that T generates W  we can define the cooccurrence model as follows: λwt and λtw denote the corresponding weights of the two components.</S><S sid =""46"" ssid = ""18"">Following Collins  we use a function GEN(x) generating all candidate results of an input x   a representation 4) mapping each training example (x  y) ∈ X × Y to a feature vector 4)(x  y) ∈ Rd  and a parameter vector α� ∈ Rd corresponding to the feature vector. d means the dimension of the vector space  it equals to the amount of features in the model.</S><S sid =""1"" ssid = ""1"">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S>",['Method_Citation']
2,P08-1102,C08-1049,0,2008,0,"As described in Ng and Low (2004 )andJiang et al (2008), we use s indicating a single character word, while b, m and e indicating the be gin, middle and end of a word respectively","As described in Ng and Low (2004) and Jiang et al (2008), we use s indicating a single character word, while b, m and e indicating the be gin, middle and end of a word respectively","['25', '10', '81', '114', '31']","<S sid =""25"" ssid = ""21"">According to Ng and Low (2004)  the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S><S sid =""10"" ssid = ""6"">Since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a boundary tag (Xue and Shen  2003)  Joint S&T can be conducted in a labelling fashion by expanding boundary tags to include POS information (Ng and Low  2004).</S><S sid =""81"" ssid = ""6"">When we derive a candidate result from a word-POS pair p and a candidate q at prior position of p  we calculate the scores of the word LM  the POS LM  the labelling probability and the generating probability  Algorithm 2 Decoding algorithm. as well as the score of the perceptron model.</S><S sid =""114"" ssid = ""25"">We used SRI Language Modelling Toolkit (Stolcke and Andreas  2002) to train a 3gram word LM with modified Kneser-Ney smoothing (Chen and Goodman  1998)  and a 4-gram POS LM with Witten-Bell smoothing  and we trained a word-POS co-occurrence model simply by MLE without smoothing.</S><S sid =""31"" ssid = ""3"">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S>",['Method_Citation']
3,P08-1102,C08-1049,0,2008,0,plates called lexical-target in the column below areintroduced by Jiang et al (2008),plates called lexical-target in the column below are introduced by Jiang et al (2008),"['40', '38', '13', '55', '7']","<S sid =""40"" ssid = ""12"">Templates in the column below are expanded from the upper ones.</S><S sid =""38"" ssid = ""10"">Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target.</S><S sid =""13"" ssid = ""9"">However  as such features are generated dynamically during the decoding procedure  two limitation arise: on the one hand  the amount of parameters increases rapidly  which is apt to overfit on training corpus; on the other hand  exact inference by dynamic programming is intractable because the current predication relies on the results of prior predications.</S><S sid =""55"" ssid = ""6"">In addition  even though these higher grams were managed to be used  there still remains another problem: as the current predication relies on the results of prior ones  the decoding procedure has to resort to approximate inference by maintaining a list of N-best candidates at each predication position  which evokes a potential risk to depress the training.</S><S sid =""7"" ssid = ""3"">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S>",['Method_Citation']
4,P08-1102,P12-1110,0,2008,0,"For CTB-5, we refer to the split by Duan et al (2007) as CTB-5d, and to the split by Jiang et al (2008) as CTB-5j","For CTB-5, we refer to the split by Duan et al (2007) as CTB-5d, and to the split by Jiang et al (2008) as CTB-5j","['54', '93', '141', '65', '2']","<S sid =""54"" ssid = ""5"">We noticed that the templates related to word unigrams and bigrams bring to the feature space an enlargement much rapider than the character-base ones  not to mention the higher-order grams such as trigrams or 4-grams.</S><S sid =""93"" ssid = ""4"">In all experiments  we use the averaged parameters for the perceptrons  and F-measure as the accuracy measure.</S><S sid =""141"" ssid = ""4"">We would also like to Hwee-Tou Ng for sharing his code  and Yang Liu and Yun Huang for suggestions.</S><S sid =""65"" ssid = ""16"">As shown in Figure 1  the character-based perceptron is used as the inside-layer linear model and sends its output to the outside-layer.</S><S sid =""2"" ssid = ""2"">With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S>",['Implication_Citation']
5,P08-1102,D12-1126,0,2008,0,Jiang et al (2008) proposes a cascaded linear model for joint Chinese word segmentation and POS tagging,Jiang et al (2008) proposes a cascaded linear model for joint Chinese word segmentation and POS tagging,"['1', '130', '0', '9', '24']","<S sid =""1"" ssid = ""1"">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S sid =""130"" ssid = ""1"">We proposed a cascaded linear model for Chinese Joint S&T.</S><S sid =""0"" ssid = ""0"">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S sid =""9"" ssid = ""5"">To segment and tag a character sequence  there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T).</S><S sid =""24"" ssid = ""20"">It is a better idea to perform segmentation and POS tagging jointly in a uniform framework.</S>",['Method_Citation']
6,P08-1102,C10-1135,0,2008,0,"We use the feature templates the same as Jiang et al, (2008) to extract features form E model","We use the feature templates the same as Jiang et al, (2008) to extract features form E model","['116', '54', '53', '2', '82']","<S sid =""116"" ssid = ""27"">In order to inspect how much improvement each feature brings into the cascaded model  every time we removed a feature while retaining others  then retrained the model and tested its performance on the test set.</S><S sid =""54"" ssid = ""5"">We noticed that the templates related to word unigrams and bigrams bring to the feature space an enlargement much rapider than the character-base ones  not to mention the higher-order grams such as trigrams or 4-grams.</S><S sid =""53"" ssid = ""4"">Figure 2 shows the growing tendency of feature space with the introduction of these features as well as the character-based ones.</S><S sid =""2"" ssid = ""2"">With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S><S sid =""82"" ssid = ""7"">In addition  we add the score of the word count penalty as another feature to alleviate the tendency of LMs to favor shorter candidates.</S>",['Method_Citation']
8,P08-1102,P12-1025,0,"Jiangetal., 2008a",0,"approach, where basic processing units are characters which compose words (Jiangetal., 2008a)","basic processing units are characters which compose words (Jiangetal., 2008a)","['5', '12', '132', '14', '25']","<S sid =""5"" ssid = ""1"">Word segmentation and part-of-speech (POS) tagging are important tasks in computer processing of Chinese and other Asian languages.</S><S sid =""12"" ssid = ""8"">Besides the usual character-based features  additional features dependent on POS’s or words can also be employed to improve the performance.</S><S sid =""132"" ssid = ""3"">This is a substitute method to use both local and non-local features  and it would be especially useful when the training corpus is very large.</S><S sid =""14"" ssid = ""10"">As a result  many theoretically useful features such as higherorder word or POS n-grams are difficult to be incorporated in the model efficiently.</S><S sid =""25"" ssid = ""21"">According to Ng and Low (2004)  the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S>",['Method_Citation']
9,P08-1102,C10-2096,0,2008b,0,"The solid lines show the 1-best result, which is wrong. Jiang et al (2008b) stress the problems in re ranking phase","The solid lines show the 1-best result, which is wrong. Jiang et al (2008b) stress the problems in re ranking phase","['66', '13', '105', '77', '92']","<S sid =""66"" ssid = ""17"">Besides the output of the perceptron  the outside-layer also receive the outputs of the word LM  the POS LM  the co-occurrence model and a word count penalty which is similar to the translation length penalty in SMT.</S><S sid =""13"" ssid = ""9"">However  as such features are generated dynamically during the decoding procedure  two limitation arise: on the one hand  the amount of parameters increases rapidly  which is apt to overfit on training corpus; on the other hand  exact inference by dynamic programming is intractable because the current predication relies on the results of prior predications.</S><S sid =""105"" ssid = ""16"">At the first step  we conducted a group of contrasting experiments on the core perceptron  the first concentrated on the segmentation regardless of the POS information and reported the F-measure on segmentation only  while the second performed Joint S&T using POS information and reported the F-measure both on segmentation and on Joint S&T.</S><S sid =""77"" ssid = ""2"">In Chinese Joint S&T  the mission of the decoder is to find the boundary-POS labelled sequence with the highest score.</S><S sid =""92"" ssid = ""3"">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.</S>",['Method_Citation']
10,P08-1102,C10-2096,0,"Jiang et al, 2008a",0,"6.1.1 Baseline Forest-based System We first segment the Chinese sentences into the1-best segmentations using a state-of-the-art system (Jiang et al, 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results","We first segment the Chinese sentences into the 1-best segmentations using a state-of-the-art system (Jiang et al, 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results","['41', '55', '105', '26', '13']","<S sid =""41"" ssid = ""13"">We add a field C0 to each template in the upper column  so that it can carry out predication according to not only the context but also the current character itself.</S><S sid =""55"" ssid = ""6"">In addition  even though these higher grams were managed to be used  there still remains another problem: as the current predication relies on the results of prior ones  the decoding procedure has to resort to approximate inference by maintaining a list of N-best candidates at each predication position  which evokes a potential risk to depress the training.</S><S sid =""105"" ssid = ""16"">At the first step  we conducted a group of contrasting experiments on the core perceptron  the first concentrated on the segmentation regardless of the POS information and reported the F-measure on segmentation only  while the second performed Joint S&T using POS information and reported the F-measure both on segmentation and on Joint S&T.</S><S sid =""26"" ssid = ""22"">In order to perform POS tagging at the same time  we expand boundary tags to include POS information by attaching a POS to the tail of a boundary tag as a postfix following Ng and Low (2004).</S><S sid =""13"" ssid = ""9"">However  as such features are generated dynamically during the decoding procedure  two limitation arise: on the one hand  the amount of parameters increases rapidly  which is apt to overfit on training corpus; on the other hand  exact inference by dynamic programming is intractable because the current predication relies on the results of prior predications.</S>",['Method_Citation']
11,P08-1102,C10-2096,0,"Jiang et al, 2008a",0,"6.1.2 Lattice-forest SystemWe first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al, 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm","We first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al, 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm","['105', '96', '66', '58', '81']","<S sid =""105"" ssid = ""16"">At the first step  we conducted a group of contrasting experiments on the core perceptron  the first concentrated on the segmentation regardless of the POS information and reported the F-measure on segmentation only  while the second performed Joint S&T using POS information and reported the F-measure both on segmentation and on Joint S&T.</S><S sid =""96"" ssid = ""7"">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus  we randomly chosen 2  000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84  294 sentences)  then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S><S sid =""66"" ssid = ""17"">Besides the output of the perceptron  the outside-layer also receive the outputs of the word LM  the POS LM  the co-occurrence model and a word count penalty which is similar to the translation length penalty in SMT.</S><S sid =""58"" ssid = ""9"">Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.</S><S sid =""81"" ssid = ""6"">When we derive a candidate result from a word-POS pair p and a candidate q at prior position of p  we calculate the scores of the word LM  the POS LM  the labelling probability and the generating probability  Algorithm 2 Decoding algorithm. as well as the score of the perceptron model.</S>",['Implication_Citation']
12,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"However, when we repeat the work of (Jiang et al, 2008), which reports to achieve the state-of-art performance in the data-sets that we adopt, it has been found that some features (e.g., C0) are unnoticeably trained several times in their model (which are implicitly generated from different feature templates used in the paper)","However, when we repeat the work of (Jiang et al, 2008), which reports to achieve the state-of-art performance in the data-sets that we adopt, it has been found that some features (e.g., C0) are unnoticeably trained several times in their model (which are implicitly generated from different feature templates used in the paper)","['13', '55', '66', '7', '96']","<S sid =""13"" ssid = ""9"">However  as such features are generated dynamically during the decoding procedure  two limitation arise: on the one hand  the amount of parameters increases rapidly  which is apt to overfit on training corpus; on the other hand  exact inference by dynamic programming is intractable because the current predication relies on the results of prior predications.</S><S sid =""55"" ssid = ""6"">In addition  even though these higher grams were managed to be used  there still remains another problem: as the current predication relies on the results of prior ones  the decoding procedure has to resort to approximate inference by maintaining a list of N-best candidates at each predication position  which evokes a potential risk to depress the training.</S><S sid =""66"" ssid = ""17"">Besides the output of the perceptron  the outside-layer also receive the outputs of the word LM  the POS LM  the co-occurrence model and a word count penalty which is similar to the translation length penalty in SMT.</S><S sid =""7"" ssid = ""3"">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S><S sid =""96"" ssid = ""7"">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus  we randomly chosen 2  000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84  294 sentences)  then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S>",['Method_Citation']
13,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"Unicode/CP936 1.1M/55K 104K/13K 0.035 Table 3: Corpus statistics for the second SIGHAN Bakeoff appears twice, which is generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al, 2008), with n=0, generates [C0C0])","Table 3: Corpus statistics for the second SIGHAN Bakeoff appears twice, which is generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al, 2008), with n=0, generates [C0C0])","['96', '13', '91', '7', '105']","<S sid =""96"" ssid = ""7"">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus  we randomly chosen 2  000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84  294 sentences)  then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S><S sid =""13"" ssid = ""9"">However  as such features are generated dynamically during the decoding procedure  two limitation arise: on the one hand  the amount of parameters increases rapidly  which is apt to overfit on training corpus; on the other hand  exact inference by dynamic programming is intractable because the current predication relies on the results of prior predications.</S><S sid =""91"" ssid = ""2"">The first was conducted to test the performance of the perceptron on segmentation on the corpus from SIGHAN Bakeoff 2  including the Academia Sinica Corpus (AS)  the Hong Kong City University Corpus (CityU)  the Peking University Corpus (PKU) and the Microsoft Research Corpus (MSR).</S><S sid =""7"" ssid = ""3"">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S><S sid =""105"" ssid = ""16"">At the first step  we conducted a group of contrasting experiments on the core perceptron  the first concentrated on the segmentation regardless of the POS information and reported the F-measure on segmentation only  while the second performed Joint S&T using POS information and reported the F-measure both on segmentation and on Joint S&T.</S>",['Implication_Citation']
14,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"As all the features adopted in (Jiang et al, 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be? n?, at least in principle","As all the features adopted in (Jiang et al, 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be n, at least in principle","['126', '46', '62', '132', '41']","<S sid =""126"" ssid = ""37"">We suppose that the character-based features used in the perceptron play a similar role as the lowerorder word LM  and it would be helpful if we train a higher-order word LM on a larger scale corpus.</S><S sid =""46"" ssid = ""18"">Following Collins  we use a function GEN(x) generating all candidate results of an input x   a representation 4) mapping each training example (x  y) ∈ X × Y to a feature vector 4)(x  y) ∈ Rd  and a parameter vector α� ∈ Rd corresponding to the feature vector. d means the dimension of the vector space  it equals to the amount of features in the model.</S><S sid =""62"" ssid = ""13"">Suppose we have n features gj (j = 1..n) coupled with n corresponding weights wj (j = 1..n)  each feature gj gives a score gj(r) to a candidate r  then the total score of r is given by: The decoding procedure aims to find the candidate r* with the highest score: While the mission of the training procedure is to tune the weights wj(j = 1..n) to guarantee that the candidate r with the highest score happens to be the best result with a high probability.</S><S sid =""132"" ssid = ""3"">This is a substitute method to use both local and non-local features  and it would be especially useful when the training corpus is very large.</S><S sid =""41"" ssid = ""13"">We add a field C0 to each template in the upper column  so that it can carry out predication according to not only the context but also the current character itself.</S>",['Method_Citation']
15,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"Inspired by (Jiang et al, 2008), we set the real d Although Table 5 has shown that the proposed all the value of C0 to be 2.0, the value of C-1C0anC0C1 to be 3.0, and the values of all other features to be 1.0 for the character-based discriminative-plus model","Inspired by (Jiang et al, 2008), we set the real d Although Table 5 has shown that the proposed all the value of C0 to be 2.0, the value of C-1C0anC0C1 to be 3.0, and the values of all other features to be 1.0 for the character-based discriminative-plus model","['66', '13', '96', '55', '7']","<S sid =""66"" ssid = ""17"">Besides the output of the perceptron  the outside-layer also receive the outputs of the word LM  the POS LM  the co-occurrence model and a word count penalty which is similar to the translation length penalty in SMT.</S><S sid =""13"" ssid = ""9"">However  as such features are generated dynamically during the decoding procedure  two limitation arise: on the one hand  the amount of parameters increases rapidly  which is apt to overfit on training corpus; on the other hand  exact inference by dynamic programming is intractable because the current predication relies on the results of prior predications.</S><S sid =""96"" ssid = ""7"">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus  we randomly chosen 2  000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84  294 sentences)  then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S><S sid =""55"" ssid = ""6"">In addition  even though these higher grams were managed to be used  there still remains another problem: as the current predication relies on the results of prior ones  the decoding procedure has to resort to approximate inference by maintaining a list of N-best candidates at each predication position  which evokes a potential risk to depress the training.</S><S sid =""7"" ssid = ""3"">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S>",['Method_Citation']
17,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"Last, (Jiang et al, 2008) 5 adds repeated features implicitly based on (Ng and Low, 2004)","Last, (Jiang et al, 2008) 5 adds repeated features implicitly based on (Ng and Low, 2004)","['110', '11', '116', '129', '12']","<S sid =""110"" ssid = ""21"">Similar trend appeared in experiments of Ng and Low (2004)  where they conducted experiments on CTB 3.0 and achieved Fmeasure 0.919 on Joint S&T  a ratio of 96% to the F-measure 0.952 on segmentation.</S><S sid =""11"" ssid = ""7"">Compared to performing segmentation and POS tagging one at a time  Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging (Ng and Low  2004).</S><S sid =""116"" ssid = ""27"">In order to inspect how much improvement each feature brings into the cascaded model  every time we removed a feature while retaining others  then retrained the model and tested its performance on the test set.</S><S sid =""129"" ssid = ""40"">Experimental results show that  it achieves obvious improvement over the perceptron-only model  about from 0.973 to 0.978 on segmentation  and from 0.925 to 0.934 on Joint S&T  with error reductions of 18.5% and 12% respectively.</S><S sid =""12"" ssid = ""8"">Besides the usual character-based features  additional features dependent on POS’s or words can also be employed to improve the performance.</S>",['Implication_Citation']
20,P08-1102,D12-1046,0,Jiang et al2008a,0,"Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a) ,perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b)","Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a), perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b)","['3', '1', '113', '120', '57']","<S sid =""3"" ssid = ""3"">Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.</S><S sid =""1"" ssid = ""1"">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S sid =""113"" ssid = ""24"">Besides this perceptron  other sub-models are trained and used as additional features of the outside-layer linear model.</S><S sid =""120"" ssid = ""31"">Without the perceptron  the cascaded model (if we can still call it “cascaded”) performs poorly on both segmentation and Joint S&T.</S><S sid =""57"" ssid = ""8"">It has a two-layer architecture  with a perceptron as the core and another linear model as the outside-layer.</S>",['Method_Citation']
