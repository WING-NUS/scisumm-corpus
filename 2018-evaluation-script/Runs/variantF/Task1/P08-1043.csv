Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P08-1043,C10-1045,0,2008,0,"Finally, we note that simple weighting gives nearly a 2% F1 improvement, whereas Goldberg and Tsarfaty (2008) found that unweighted lattices were more effective for Hebrew","Finally, we note that simple weighting gives nearly a 2% F1 improvement, whereas Goldberg and Tsarfaty (2008) found that unweighted lattices were more effective for Hebrew","['118', '33', '134', '56', '97']","<S sid =""118"" ssid = ""50"">For the remaining arcs  if the segment is in fact a known lexeme it is tagged as usual  but for the OOV arcs which are valid Hebrew entries lacking tags assignment  we assign all possible tags and then simulate a grammatical constraint.</S><S sid =""33"" ssid = ""12"">The current work treats both segmental and super-segmental phenomena  yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg  2008).</S><S sid =""134"" ssid = ""12"">Such resources exist for Hebrew (Itai et al.  2006)  but unfortunately use a tagging scheme which is incompatible with the one of the Hebrew Treebank.s For this reason  we use a data-driven morphological analyzer derived from the training data similar to (Cohen and Smith  2007).</S><S sid =""56"" ssid = ""3"">We refer to a segment and its assigned PoS tag as a lexeme  and so analyses are in fact sequences of lexemes.</S><S sid =""97"" ssid = ""29"">Thus our proposed model is a proper model assigning probability mass to all (7r  L) pairs  where 7r is a parse tree and L is the one and only lattice that a sequence of characters (and spaces) W over our alpha-beth gives rise to.</S>",['Method_Citation']
2,P08-1043,P11-1141,0,2008,0,Goldberg and Tsarfaty (2008) showed that a single model for morphological segmentation and syntactic parsing of Hebrew yielded an error reduction of 12% over the best pipelined models,Goldberg and Tsarfaty (2008) showed that a single model for morphological segmentation and syntactic parsing of Hebrew yielded an error reduction of 12% over the best pipelined models,"['4', '17', '3', '16', '189']","<S sid =""4"" ssid = ""4"">Using a treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined  integrated or factorized systems for Hebrew morphological and syntactic processing  yielding an error reduction of 12% over the best published results so far.</S><S sid =""17"" ssid = ""13"">Tsarfaty (2006) argues that for Semitic languages determining the correct morphological segmentation is dependent on syntactic context and shows that increasing information sharing between the morphological and the syntactic components leads to improved performance on the joint task.</S><S sid =""3"" ssid = ""3"">Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.</S><S sid =""16"" ssid = ""12"">The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens  and the expected number of leaves in the syntactic analysis in not known in advance.</S><S sid =""189"" ssid = ""3"">Better grammars are shown here to improve performance on both morphological and syntactic tasks  providing support for the advantage of a joint framework over pipelined or factorized ones.</S>",['Method_Citation']
3,P08-1043,P10-1074,0,2008,0,"Zhang and Clark (2008) built a perceptron-based joint segmenter and part-of-speech (POS) tagger for Chinese, and Toutanova and Cherry (2009) learned a joint model of lemmatization and POS tagging which outperformed a pipelined model. Adler and Elhadad (2006) presented an HMMbased approach for unsupervised joint morphological segmentation and tagging of Hebrew, and Goldberg and Tsarfaty (2008) developed a joint model of segmentation, tagging and parsing of He brew, based on lattice parsing","Zhang and Clark (2008) built a perceptron-based joint segmenter and part-of-speech (POS) tagger for Chinese, and Toutanova and Cherry (2009) learned a joint model of lemmatization and POS tagging which outperformed a pipelined model. Adler and Elhadad (2006) presented an HMM-based approach for unsupervised joint morphological segmentation and tagging of Hebrew, and Goldberg and Tsarfaty (2008) developed a joint model of segmentation, tagging and parsing of Hebrew, based on lattice parsing","['18', '21', '52', '188', '3']","<S sid =""18"" ssid = ""14"">Cohen and Smith (2007) followed up on these results and proposed a system for joint inference of morphological and syntactic structures using factored models each designed and trained on its own.</S><S sid =""21"" ssid = ""17"">Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty  2006) and (Cohen and Smith  2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2</S><S sid =""52"" ssid = ""10"">Cohen and Smith (2007) later on based a system for joint inference on factored  independent  morphological and syntactic components of which scores are combined to cater for the joint inference task.</S><S sid =""188"" ssid = ""2"">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S><S sid =""3"" ssid = ""3"">Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.</S>",['Method_Citation']
4,P08-1043,P11-1089,0,2008,0,Goldberg and Tsarfaty (2008) pro pose a generative joint model,Goldberg and Tsarfaty (2008) propose a generative joint model,"['21', '3', '188', '18', '53']","<S sid =""21"" ssid = ""17"">Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty  2006) and (Cohen and Smith  2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2</S><S sid =""3"" ssid = ""3"">Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.</S><S sid =""188"" ssid = ""2"">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S><S sid =""18"" ssid = ""14"">Cohen and Smith (2007) followed up on these results and proposed a system for joint inference of morphological and syntactic structures using factored models each designed and trained on its own.</S><S sid =""53"" ssid = ""11"">Both (Tsarfaty  2006; Cohen and Smith  2007) have shown that a single integrated framework outperforms a completely streamlined implementation  yet neither has shown a single generative model which handles both tasks.</S>",['Implication_Citation']
5,P08-1043,W10-1404,0,2008,0,Goldberg and Tsarfaty (2008) concluded that an integrated model of morphological disambiguation and syntactic parsing in Hebrew Treebank parsing improves the results of a pipelined approach,Goldberg and Tsarfaty (2008) concluded that an integrated model of morphological disambiguation and syntactic parsing in Hebrew Treebank parsing improves the results of a pipelined approach,"['16', '4', '5', '17', '176']","<S sid =""16"" ssid = ""12"">The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens  and the expected number of leaves in the syntactic analysis in not known in advance.</S><S sid =""4"" ssid = ""4"">Using a treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined  integrated or factorized systems for Hebrew morphological and syntactic processing  yielding an error reduction of 12% over the best published results so far.</S><S sid =""5"" ssid = ""1"">Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth  ‘tokens’) that constitute the unanalyzed surface forms (utterances).</S><S sid =""17"" ssid = ""13"">Tsarfaty (2006) argues that for Semitic languages determining the correct morphological segmentation is dependent on syntactic context and shows that increasing information sharing between the morphological and the syntactic components leads to improved performance on the joint task.</S><S sid =""176"" ssid = ""14"">Furthermore  the combination of pruning and vertical markovization of the grammar outperforms the Oracle results reported by Cohen and Smith.</S>",['Method_Citation']
6,P08-1043,P11-2124,0,2008,0,Goldberg and Tsarfaty (2008) demonstrated the effectiveness of lattice parsing for jointly performing segmentation and parsing of Hebrewtext,Goldberg and Tsarfaty (2008) demonstrated the effectiveness of lattice parsing for jointly performing segmentation and parsing of Hebrew text,"['16', '75', '156', '176', '169']","<S sid =""16"" ssid = ""12"">The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens  and the expected number of leaves in the syntactic analysis in not known in advance.</S><S sid =""75"" ssid = ""7"">Every token is independent of the others  and the sentence lattice is in fact a concatenation of smaller lattices  one for each token.</S><S sid =""156"" ssid = ""34"">To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S><S sid =""176"" ssid = ""14"">Furthermore  the combination of pruning and vertical markovization of the grammar outperforms the Oracle results reported by Cohen and Smith.</S><S sid =""169"" ssid = ""7"">Table 2 compares the performance of our system on the setup of Cohen and Smith (2007) to the best results reported by them for the same tasks.</S>",['Method_Citation']
7,P08-1043,P11-2124,0,"Goldberg and Tsarfaty, 2008",0,"Following (Goldberg and Tsarfaty, 2008) we deal with the ambiguous affixation patterns in Hebrew by encoding the input sentence as a segmentation lattice","Following (Goldberg and Tsarfaty, 2008) we deal with the ambiguous affixation patterns in Hebrew by encoding the input sentence as a segmentation lattice","['156', '67', '164', '90', '70']","<S sid =""156"" ssid = ""34"">To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S><S sid =""67"" ssid = ""14"">Hence  we take the probability of the event fmnh analyzed as REL VB to be This means that we generate f and mnh independently depending on their corresponding PoS tags  and the context (as well as the syntactic relation between the two) is modeled via the derivation resulting in a sequence REL VB spanning the form fmnh. based on linear context.</S><S sid =""164"" ssid = ""2"">In addition we report for each model its performance on goldsegmented input (GS) to indicate the upper bound 11Overt definiteness errors may be seen as a wrong feature rather than as wrong constituent and it is by now an accepted standard to report accuracy with and without such errors. for the grammars’ performance on the parsing task.</S><S sid =""90"" ssid = ""22"">Since the lattice L for a given sentence W is determined by the morphological analyzer M we have which is precisely the formula corresponding to the so-called lattice parsing familiar from speech recognition.</S><S sid =""70"" ssid = ""2"">Each lattice arc corresponds to a segment and its corresponding PoS tag  and a path through the lattice corresponds to a specific morphological segmentation of the utterance.</S>",['Method_Citation']
8,P08-1043,P12-2002,0,2008,0,2The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008),The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008),"['16', '85', '75', '2', '164']","<S sid =""16"" ssid = ""12"">The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens  and the expected number of leaves in the syntactic analysis in not known in advance.</S><S sid =""85"" ssid = ""17"">The Input The set of analyses for a token is thus represented as a lattice in which every arc corresponds to a specific lexeme l  as shown in Figure 1.</S><S sid =""75"" ssid = ""7"">Every token is independent of the others  and the sentence lattice is in fact a concatenation of smaller lattices  one for each token.</S><S sid =""2"" ssid = ""2"">These words are in turn highly ambiguous  breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance.</S><S sid =""164"" ssid = ""2"">In addition we report for each model its performance on goldsegmented input (GS) to indicate the upper bound 11Overt definiteness errors may be seen as a wrong feature rather than as wrong constituent and it is by now an accepted standard to report accuracy with and without such errors. for the grammars’ performance on the parsing task.</S>",['Method_Citation']
9,P08-1043,D12-1046,0,"Goldberg and Tsarfaty, 2008",0,"A study that is closely related toours is (Goldberg and Tsarfaty, 2008), where a single generative model was proposed for joint morphological segmentation and syntactic parsing for Hebrew","A study that is closely related to ours is (Goldberg and Tsarfaty, 2008), where a single generative model was proposed for joint morphological segmentation and syntactic parsing for Hebrew","['3', '97', '141', '118', '187']","<S sid =""3"" ssid = ""3"">Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.</S><S sid =""97"" ssid = ""29"">Thus our proposed model is a proper model assigning probability mass to all (7r  L) pairs  where 7r is a parse tree and L is the one and only lattice that a sequence of characters (and spaces) W over our alpha-beth gives rise to.</S><S sid =""141"" ssid = ""19"">This analyzer setting is similar to that of (Cohen and Smith  2007)  and models using it are denoted nohsp  Parser and Grammar We used BitPar (Schmid  2004)  an efficient general purpose parser 10 together with various treebank grammars to parse the input sentences and propose compatible morphological segmentation and syntactic analysis.</S><S sid =""118"" ssid = ""50"">For the remaining arcs  if the segment is in fact a known lexeme it is tagged as usual  but for the OOV arcs which are valid Hebrew entries lacking tags assignment  we assign all possible tags and then simulate a grammatical constraint.</S><S sid =""187"" ssid = ""1"">Employing a PCFG-based generative framework to make both syntactic and morphological disambiguation decisions is not only theoretically clean and linguistically justified and but also probabilistically apropriate and empirically sound.</S>",['Method_Citation']
10,P08-1043,D12-1133,0,2008,0,"Models that in addition incorporate morphological analysis and segmentation have been explored by Tsarfaty (2006), Cohen and Smith (2007), and Goldberg and Tsarfaty (2008) with special reference to Hebrew parsing","Models that in addition incorporate morphological analysis and segmentation have been explored by Tsarfaty (2006), Cohen and Smith (2007), and Goldberg and Tsarfaty (2008) with special reference to Hebrew parsing","['141', '43', '50', '18', '187']","<S sid =""141"" ssid = ""19"">This analyzer setting is similar to that of (Cohen and Smith  2007)  and models using it are denoted nohsp  Parser and Grammar We used BitPar (Schmid  2004)  an efficient general purpose parser 10 together with various treebank grammars to parse the input sentences and propose compatible morphological segmentation and syntactic analysis.</S><S sid =""43"" ssid = ""1"">Morphological analyzers for Hebrew that analyze a surface form in isolation have been proposed by Segal (2000)  Yona and Wintner (2005)  and recently by the knowledge center for processing Hebrew (Itai et al.  2006).</S><S sid =""50"" ssid = ""8"">The joint morphological and syntactic hypothesis was first discussed in (Tsarfaty  2006; Tsarfaty and Sima’an  2004) and empirically explored in (Tsarfaty  2006).</S><S sid =""18"" ssid = ""14"">Cohen and Smith (2007) followed up on these results and proposed a system for joint inference of morphological and syntactic structures using factored models each designed and trained on its own.</S><S sid =""187"" ssid = ""1"">Employing a PCFG-based generative framework to make both syntactic and morphological disambiguation decisions is not only theoretically clean and linguistically justified and but also probabilistically apropriate and empirically sound.</S>",['Implication_Citation']
11,P08-1043,E09-1038,0,"Goldberg and Tsarfaty, 2008",0,"4), and in a more realistic one in which parsing and segmentation are handled jointly by the parser (Goldberg and Tsarfaty, 2008) (Sec","Parsing and segmentation are handled jointly by the parser (Goldberg and Tsarfaty, 2008)","['141', '163', '144', '156', '21']","<S sid =""141"" ssid = ""19"">This analyzer setting is similar to that of (Cohen and Smith  2007)  and models using it are denoted nohsp  Parser and Grammar We used BitPar (Schmid  2004)  an efficient general purpose parser 10 together with various treebank grammars to parse the input sentences and propose compatible morphological segmentation and syntactic analysis.</S><S sid =""163"" ssid = ""1"">The accuracy results for segmentation  tagging and parsing using our different models and our standard data split are summarized in Table 1.</S><S sid =""144"" ssid = ""22"">In our second model GTvpi we also distinguished finite and non-finite verbs and VPs as 10Lattice parsing can be performed by special initialization of the chart in a CKY parser (Chappelier et al.  1999).</S><S sid =""156"" ssid = ""34"">To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S><S sid =""21"" ssid = ""17"">Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty  2006) and (Cohen and Smith  2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2</S>",['Method_Citation']
12,P08-1043,E09-1038,0,"Goldberg and Tsarfaty, 2008",0,"It is the same grammar as described in (Goldberg and Tsarfaty, 2008)","It is the same grammar as described in (Goldberg and Tsarfaty, 2008)","['67', '62', '118', '177', '182']","<S sid =""67"" ssid = ""14"">Hence  we take the probability of the event fmnh analyzed as REL VB to be This means that we generate f and mnh independently depending on their corresponding PoS tags  and the context (as well as the syntactic relation between the two) is modeled via the derivation resulting in a sequence REL VB spanning the form fmnh. based on linear context.</S><S sid =""62"" ssid = ""9"">When a token fmnh is to be interpreted as the lexeme sequence f/REL mnh/VB  the analysis introduces two distinct entities  the relativizer f (“that”) and the verb mnh (“counted”)  and not as the complex entity “that counted”.</S><S sid =""118"" ssid = ""50"">For the remaining arcs  if the segment is in fact a known lexeme it is tagged as usual  but for the OOV arcs which are valid Hebrew entries lacking tags assignment  we assign all possible tags and then simulate a grammatical constraint.</S><S sid =""177"" ssid = ""15"">This essentially means that a better grammar tunes the joint model for optimized syntactic disambiguation at least in as much as their hyper parameters do.</S><S sid =""182"" ssid = ""20"">In addition  as the CRF and PCFG look at similar sorts of information from within two inherently different models  they are far from independent and optimizing their product is meaningless.</S>",['Implication_Citation']
14,P08-1043,E09-1038,0,2008,0,"Several studies followed this line, (Cohen and Smith, 2007) the most recent of which is Goldberg and Tsarfaty (2008), who presented a model based on unweighted lattice parsing for performing the joint task","The most recent of which is Goldberg and Tsarfaty (2008), who presented a model based on unweighted lattice parsing for performing the joint task","['52', '18', '189', '75', '21']","<S sid =""52"" ssid = ""10"">Cohen and Smith (2007) later on based a system for joint inference on factored  independent  morphological and syntactic components of which scores are combined to cater for the joint inference task.</S><S sid =""18"" ssid = ""14"">Cohen and Smith (2007) followed up on these results and proposed a system for joint inference of morphological and syntactic structures using factored models each designed and trained on its own.</S><S sid =""189"" ssid = ""3"">Better grammars are shown here to improve performance on both morphological and syntactic tasks  providing support for the advantage of a joint framework over pipelined or factorized ones.</S><S sid =""75"" ssid = ""7"">Every token is independent of the others  and the sentence lattice is in fact a concatenation of smaller lattices  one for each token.</S><S sid =""21"" ssid = ""17"">Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty  2006) and (Cohen and Smith  2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2</S>",['Method_Citation']
15,P08-1043,E09-1038,0,2008,0,Goldberg and Tsarfaty (2008) use a data-driven morphological analyzer derived from the tree bank,Goldberg and Tsarfaty (2008) use a data-driven morphological analyzer derived from the tree bank,"['134', '51', '21', '191', '70']","<S sid =""134"" ssid = ""12"">Such resources exist for Hebrew (Itai et al.  2006)  but unfortunately use a tagging scheme which is incompatible with the one of the Hebrew Treebank.s For this reason  we use a data-driven morphological analyzer derived from the training data similar to (Cohen and Smith  2007).</S><S sid =""51"" ssid = ""9"">Tsarfaty (2006) used a morphological analyzer (Segal  2000)  a PoS tagger (Bar-Haim et al.  2005)  and a general purpose parser (Schmid  2000) in an integrated framework in which morphological and syntactic components interact to share information  leading to improved performance on the joint task.</S><S sid =""21"" ssid = ""17"">Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty  2006) and (Cohen and Smith  2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2</S><S sid =""191"" ssid = ""5"">In the current work morphological analyses and lexical probabilities are derived from a small Treebank  which is by no means the best way to go.</S><S sid =""70"" ssid = ""2"">Each lattice arc corresponds to a segment and its corresponding PoS tag  and a path through the lattice corresponds to a specific morphological segmentation of the utterance.</S>",['Method_Citation']
16,P08-1043,E09-1038,0,2008,0,The model of Goldberg and Tsarfaty (2008) uses a morphological analyzer to constructs a lattice for each input token,The model of Goldberg and Tsarfaty (2008) uses a morphological analyzer to constructs a lattice for each input token,"['85', '51', '173', '70', '69']","<S sid =""85"" ssid = ""17"">The Input The set of analyses for a token is thus represented as a lattice in which every arc corresponds to a specific lexeme l  as shown in Figure 1.</S><S sid =""51"" ssid = ""9"">Tsarfaty (2006) used a morphological analyzer (Segal  2000)  a PoS tagger (Bar-Haim et al.  2005)  and a general purpose parser (Schmid  2000) in an integrated framework in which morphological and syntactic components interact to share information  leading to improved performance on the joint task.</S><S sid =""173"" ssid = ""11"">The addition of vertical markovization enables non-pruned models to outperform all previously reported re12Cohen and Smith (2007) make use of a parameter (α) which is tuned separately for each of the tasks.</S><S sid =""70"" ssid = ""2"">Each lattice arc corresponds to a segment and its corresponding PoS tag  and a path through the lattice corresponds to a specific morphological segmentation of the utterance.</S><S sid =""69"" ssid = ""1"">We represent all morphological analyses of a given utterance using a lattice structure.</S>",['Implication_Citation']
17,P08-1043,E09-1038,0,"Goldberg and Tsarfaty, 2008",0,"Instead, we use the evaluation measure of (Tsarfaty, 2006), also used in (Goldberg and Tsarfaty, 2008), which is an adaptation of parse val to use characters instead of space-delimited tokens as its basic units","Instead, we use the evaluation measure of (Tsarfaty, 2006), also used in (Goldberg and Tsarfaty, 2008), which is an adaptation of parseval to use characters instead of space-delimited tokens as its basic units","['16', '164', '94', '173', '134']","<S sid =""16"" ssid = ""12"">The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens  and the expected number of leaves in the syntactic analysis in not known in advance.</S><S sid =""164"" ssid = ""2"">In addition we report for each model its performance on goldsegmented input (GS) to indicate the upper bound 11Overt definiteness errors may be seen as a wrong feature rather than as wrong constituent and it is by now an accepted standard to report accuracy with and without such errors. for the grammars’ performance on the parsing task.</S><S sid =""94"" ssid = ""26"">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S><S sid =""173"" ssid = ""11"">The addition of vertical markovization enables non-pruned models to outperform all previously reported re12Cohen and Smith (2007) make use of a parameter (α) which is tuned separately for each of the tasks.</S><S sid =""134"" ssid = ""12"">Such resources exist for Hebrew (Itai et al.  2006)  but unfortunately use a tagging scheme which is incompatible with the one of the Hebrew Treebank.s For this reason  we use a data-driven morphological analyzer derived from the training data similar to (Cohen and Smith  2007).</S>",['Method_Citation']
