Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,W11-2123,W11-2138,0,"Heafield, 2011",0,"We used common tools for phrase-based translation? Moses (Koehn et al, 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modelling and GIZA++ (Och and Ney, 2000) for word alignments","We used common tools for phrase-based translation Moses (Koehn et al, 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modelling and GIZA++ (Och and Ney, 2000) for word alignments","['124', '23', '244', '279', '76']","<S sid =""124"" ssid = ""28"">Lossy compressed models RandLM (Talbot and Osborne  2007) and Sheffield (Guthrie and Hepple  2010) offer better memory consumption at the expense of CPU and accuracy.</S><S sid =""23"" ssid = ""1"">We implement two data structures: PROBING  designed for speed  and TRIE  optimized for memory.</S><S sid =""244"" ssid = ""63"">Time for Moses itself to load  including loading the language model and phrase table  is included.</S><S sid =""279"" ssid = ""6"">The code is opensource  has minimal dependencies  and offers both C++ and Java interfaces for integration.</S><S sid =""76"" ssid = ""54"">This technique was introduced by Clarkson and Rosenfeld (1997) and is also implemented by IRSTLM and BerkeleyLM’s compressed option.</S>",['Method_Citation']
2,W11-2123,P14-2022,0,"Heafield, 2011",0,"The language model was com piled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run","The language model was compiled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run","['223', '206', '51', '227', '269']","<S sid =""223"" ssid = ""42"">The binary language model from Section 5.2 and text phrase table were forced into disk cache before each run.</S><S sid =""206"" ssid = ""25"">Tokens were converted to vocabulary identifiers in advance and state was carried from each query to the next.</S><S sid =""51"" ssid = ""29"">This differs from other implementations (Stolcke  2002; Pauls and Klein  2011) that use hash tables as nodes in a trie  as explained in the next section.</S><S sid =""227"" ssid = ""46"">The first value reports use immediately after loading while the second reports the increase during scoring. dBerkeleyLM is written in Java which requires memory be specified in advance.</S><S sid =""269"" ssid = ""11"">If they knew that the first four words in a hypergraph node would never extend to the left and form a 5-gram  then three or even fewer words could be kept in the backward state.</S>",['Method_Citation']
3,W11-2123,W12-3145,0,"Heafield, 2011",0,"Thus given afragment tf consisting of a sequence of target to kens, we compute LM scores for (i) < s& gt ;tf, (ii )tf and (iii )tf < /s& gt; and use the best score (only) for pruning.2 While this increases the number ofLM queries, we exploit the language model state in formation in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states","Thus given a fragment tf consisting of a sequence of target tokens, we compute LM scores for (i) < s& gt ;tf, (ii )tf and (iii )tf < /s& gt; and use the best score (only) for pruning. While this increases the number of LM queries, we exploit the language model state in formation in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states","['218', '192', '241', '180', '39']","<S sid =""218"" ssid = ""37"">We run the baseline Moses system for the French-English track of the 2011 Workshop on Machine Translation 9 translating the 3003-sentence test set.</S><S sid =""192"" ssid = ""11"">For the PROBING implementation  hash table sizes are in the millions  so the most relevant values are on the right size of the graph  where linear probing wins.</S><S sid =""241"" ssid = ""60"">We incur some additional memory cost due to storing state in each hypothesis  though this is minimal compared with the size of the model itself.</S><S sid =""180"" ssid = ""52"">In all of our experiments  the binary file (whether mapped or  in the case of most other packages  interpreted) is loaded into the disk cache in advance so that lazy mapping will never fault to disk.</S><S sid =""39"" ssid = ""17"">As the name implies  space is O(m) and linear in the number of entries.</S>",['Method_Citation']
4,W11-2123,W12-3131,0,"Heafield, 2011",0,"Our translation system uses cdec (Dyer et al,2010), an implementation of the hierarchical phrase based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference","Our translation system uses cdec (Dyer et al,2010), an implementation of the hierarchical phrase based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference","['218', '26', '242', '1', '69']","<S sid =""218"" ssid = ""37"">We run the baseline Moses system for the French-English track of the 2011 Workshop on Machine Translation 9 translating the 3003-sentence test set.</S><S sid =""26"" ssid = ""4"">We use two common techniques  hash tables and sorted arrays  describing each before the model that uses the technique.</S><S sid =""242"" ssid = ""61"">The TRIE model continues to use the least memory of ing (-P) with MAP POPULATE  the default.</S><S sid =""1"" ssid = ""1"">We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.</S><S sid =""69"" ssid = ""47"">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S>",['Implication_Citation']
5,W11-2123,W12-3154,0,"Heafield, 2011",0,"The three data sets in use in this paper are summarised in Table 1.The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al, 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime","The three data sets in use in this paper are summarised in Table 1.The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al, 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime","['51', '240', '199', '102', '233']","<S sid =""51"" ssid = ""29"">This differs from other implementations (Stolcke  2002; Pauls and Klein  2011) that use hash tables as nodes in a trie  as explained in the next section.</S><S sid =""240"" ssid = ""59"">In line with perplexity results from Table 1  the PROBING model is the fastest followed by TRIE  and subsequently other packages.</S><S sid =""199"" ssid = ""18"">For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid =""102"" ssid = ""6"">The PROBING model was designed to improve upon SRILM by using linear probing hash tables (though not arranged in a trie)  allocating memory all at once (eliminating the need for full pointers)  and being easy to compile.</S><S sid =""233"" ssid = ""52"">The authors provided us with a ratio between TPT and SRI under different conditions. aLossy compression with the same weights. bLossy compression with retuned weights. ditions make the value appropriate for estimating repeated run times  such as in parameter tuning.</S>",['Method_Citation']
6,W11-2123,P12-2058,0,"Heafield, 2011",0,"The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011)","The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011)","['68', '27', '120', '2', '0']","<S sid =""68"" ssid = ""46"">The trie data structure is commonly used for language modeling.</S><S sid =""27"" ssid = ""5"">Hash tables are a common sparse mapping technique used by SRILM’s default and BerkeleyLM’s hashed variant.</S><S sid =""120"" ssid = ""24"">The compressed variant uses block compression and is rather slow as a result.</S><S sid =""2"" ssid = ""2"">The structure uses linear probing hash tables and is designed for speed.</S><S sid =""0"" ssid = ""0"">KenLM: Faster and Smaller Language Model Queries</S>",['Method_Citation']
7,W11-2123,W11-2139,0,2011,0,Inference was carried out using the language modeling library described by Heafield (2011),Inference was carried out using the language modeling library described by Heafield (2011),"['206', '240', '199', '102', '71']","<S sid =""206"" ssid = ""25"">Tokens were converted to vocabulary identifiers in advance and state was carried from each query to the next.</S><S sid =""240"" ssid = ""59"">In line with perplexity results from Table 1  the PROBING model is the fastest followed by TRIE  and subsequently other packages.</S><S sid =""199"" ssid = ""18"">For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid =""102"" ssid = ""6"">The PROBING model was designed to improve upon SRILM by using linear probing hash tables (though not arranged in a trie)  allocating memory all at once (eliminating the need for full pointers)  and being easy to compile.</S><S sid =""71"" ssid = ""49"">Nodes in the trie are based on arrays sorted by vocabulary identifier.</S>",['Method_Citation']
8,W11-2123,P13-2003,0,"Heafield, 2011",0,"We used the MADA ATB segmentation for Arabic (Roth et al, 2008) and true casing for English, phrases of maximal length 7, KneserNey smoothing, and lexicalized reordering (Koehn et al, 2005), and a 5-gram language model, trained on GigaWordv.5 using KenLM (Heafield, 2011)","We used the MADA ATB segmentation for Arabic (Roth et al, 2008) and true casing for English, phrases of maximal length 7, KneserNey smoothing, and lexicalized reordering (Koehn et al, 2005), and a 5-gram language model, trained on GigaWordv.5 using KenLM (Heafield, 2011)","['199', '253', '233', '89', '51']","<S sid =""199"" ssid = ""18"">For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid =""253"" ssid = ""72"">Using RandLM and the documented settings (8-bit values and 1 256 false-positive probability)  we built a stupid backoff model on the same data as in Section 5.2.</S><S sid =""233"" ssid = ""52"">The authors provided us with a ratio between TPT and SRI under different conditions. aLossy compression with the same weights. bLossy compression with retuned weights. ditions make the value appropriate for estimating repeated run times  such as in parameter tuning.</S><S sid =""89"" ssid = ""67"">Floating point values may be stored in the trie exactly  using 31 bits for non-positive log probability and 32 bits for backoff5.</S><S sid =""51"" ssid = ""29"">This differs from other implementations (Stolcke  2002; Pauls and Klein  2011) that use hash tables as nodes in a trie  as explained in the next section.</S>",['Method_Citation']
9,W11-2123,W12-3134,0,2011,0,The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language mod els by Heafield (2011) and Pauls and Klein (2011)? both language model implementations are now integrated with Joshua,The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language models by Heafield (2011) and Pauls and Klein (2011)? both language model implementations are now integrated with Joshua,"['76', '244', '24', '102', '237']","<S sid =""76"" ssid = ""54"">This technique was introduced by Clarkson and Rosenfeld (1997) and is also implemented by IRSTLM and BerkeleyLM’s compressed option.</S><S sid =""244"" ssid = ""63"">Time for Moses itself to load  including loading the language model and phrase table  is included.</S><S sid =""24"" ssid = ""2"">The set of n-grams appearing in a model is sparse  and we want to efficiently find their associated probabilities and backoff penalties.</S><S sid =""102"" ssid = ""6"">The PROBING model was designed to improve upon SRILM by using linear probing hash tables (though not arranged in a trie)  allocating memory all at once (eliminating the need for full pointers)  and being easy to compile.</S><S sid =""237"" ssid = ""56"">Moses keeps language models and many other resources in static variables  so these are still resident in memory.</S>",['Method_Citation']
10,W11-2123,W12-3134,0,2011,0,Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets,Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets,"['92', '253', '4', '51', '69']","<S sid =""92"" ssid = ""70"">To quantize  we use the binning method (Federico and Bertoldi  2006) that sorts values  divides into equally sized bins  and averages within each bin.</S><S sid =""253"" ssid = ""72"">Using RandLM and the documented settings (8-bit values and 1 256 false-positive probability)  we built a stupid backoff model on the same data as in Section 5.2.</S><S sid =""4"" ssid = ""4"">Our code is thread-safe  and integrated into the Moses  cdec  and Joshua translation systems.</S><S sid =""51"" ssid = ""29"">This differs from other implementations (Stolcke  2002; Pauls and Klein  2011) that use hash tables as nodes in a trie  as explained in the next section.</S><S sid =""69"" ssid = ""47"">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S>",['Implication_Citation']
11,W11-2123,W12-3134,0,2011,0,"With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult to compile SRILM toolkit (Stolcke, 2002)","With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult to compile SRILM toolkit (Stolcke, 2002)","['158', '69', '240', '51', '192']","<S sid =""158"" ssid = ""30"">These are written to the state s(wn1) and returned so that they can be used for the following query.</S><S sid =""69"" ssid = ""47"">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid =""240"" ssid = ""59"">In line with perplexity results from Table 1  the PROBING model is the fastest followed by TRIE  and subsequently other packages.</S><S sid =""51"" ssid = ""29"">This differs from other implementations (Stolcke  2002; Pauls and Klein  2011) that use hash tables as nodes in a trie  as explained in the next section.</S><S sid =""192"" ssid = ""11"">For the PROBING implementation  hash table sizes are in the millions  so the most relevant values are on the right size of the graph  where linear probing wins.</S>",['Method_Citation']
12,W11-2123,W12-3160,0,"Heafield, 2011",0,"This was used to create a KenLM (Heafield, 2011)","This was used to create a KenLM (Heafield, 2011)","['109', '206', '102', '175', '99']","<S sid =""109"" ssid = ""13"">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search  bit level packing  and stateful queries.</S><S sid =""206"" ssid = ""25"">Tokens were converted to vocabulary identifiers in advance and state was carried from each query to the next.</S><S sid =""102"" ssid = ""6"">The PROBING model was designed to improve upon SRILM by using linear probing hash tables (though not arranged in a trie)  allocating memory all at once (eliminating the need for full pointers)  and being easy to compile.</S><S sid =""175"" ssid = ""47"">This is especially bad with PROBING because it is based on hashing and performs random lookups  but it is not intended to be used in low-memory scenarios.</S><S sid =""99"" ssid = ""3"">Each trie node is individually allocated and full 64-bit pointers are used to find them  wasting memory.</S>",['Implication_Citation']
13,W11-2123,W12-3706,0,"Heafield, 2011",0,"In the Opinum system we query the M p, M n mod els with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable fora web application","In the Opinum system we query the M p, M n models with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable for a web application","['137', '178', '240', '156', '245']","<S sid =""137"" ssid = ""9"">The state function is integrated into the query process so that  in lieu of the query p(wnjwn−1 1 )  the application issues query p(wnjs(wn−1 1 )) which also returns s(wn1 ).</S><S sid =""178"" ssid = ""50"">TPT has theoretically better locality because it stores ngrams near their suffixes  thereby placing reads for a single query in the same or adjacent pages.</S><S sid =""240"" ssid = ""59"">In line with perplexity results from Table 1  the PROBING model is the fastest followed by TRIE  and subsequently other packages.</S><S sid =""156"" ssid = ""28"">As noted in Section 1  our code finds the longest matching entry wnf for query p(wn|s(wn−1 f ) The probability p(wn|wn−1 f ) is stored with wnf and the backoffs are immediately accessible in the provided state s(wn−1 When our code walks the data structure to find wnf   it visits wnn  wnn−1  ...   wnf .</S><S sid =""245"" ssid = ""64"">Along with locking and background kernel operations such as prefaulting  this explains why wall time is not one-eighth that of the single-threaded case. aLossy compression with the same weights. bLossy compression with retuned weights. the non-lossy options.</S>",['Method_Citation']
14,W11-2123,W11-2147,0,"Heafield, 2011",0,"Our base line is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al, 2007) for translation model training and decoding, GIZA++ (Ochand Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights","Our base line is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al, 2007) for translation model training and decoding, GIZA++ (Ochand Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights","['244', '130', '102', '199', '69']","<S sid =""244"" ssid = ""63"">Time for Moses itself to load  including loading the language model and phrase table  is included.</S><S sid =""130"" ssid = ""2"">Applications such as machine translation use language model probability as a feature to assist in choosing between hypotheses.</S><S sid =""102"" ssid = ""6"">The PROBING model was designed to improve upon SRILM by using linear probing hash tables (though not arranged in a trie)  allocating memory all at once (eliminating the need for full pointers)  and being easy to compile.</S><S sid =""199"" ssid = ""18"">For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid =""69"" ssid = ""47"">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S>",['Method_Citation']
15,W11-2123,E12-1083,0,"Heafield, 2011",0,"For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011)","For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011)","['199', '1', '103', '214', '240']","<S sid =""199"" ssid = ""18"">For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid =""1"" ssid = ""1"">We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.</S><S sid =""103"" ssid = ""7"">IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.</S><S sid =""214"" ssid = ""33"">For even larger models  we recommend RandLM; the memory consumption of the cache is not expected to grow with model size  and it has been reported to scale well.</S><S sid =""240"" ssid = ""59"">In line with perplexity results from Table 1  the PROBING model is the fastest followed by TRIE  and subsequently other packages.</S>",['Implication_Citation']
16,W11-2123,P12-1002,0,"Heafield, 2011",0,"Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Sima ?an,2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well.3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using kenlm (Heafield, 2011)","Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Simaan,2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well.3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using kenlm (Heafield, 2011)","['192', '69', '158', '180', '39']","<S sid =""192"" ssid = ""11"">For the PROBING implementation  hash table sizes are in the millions  so the most relevant values are on the right size of the graph  where linear probing wins.</S><S sid =""69"" ssid = ""47"">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid =""158"" ssid = ""30"">These are written to the state s(wn1) and returned so that they can be used for the following query.</S><S sid =""180"" ssid = ""52"">In all of our experiments  the binary file (whether mapped or  in the case of most other packages  interpreted) is loaded into the disk cache in advance so that lazy mapping will never fault to disk.</S><S sid =""39"" ssid = ""17"">As the name implies  space is O(m) and linear in the number of entries.</S>",['Method_Citation']
17,W11-2123,D12-1108,0,"Heafield, 2011",0,"n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3","n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3","['131', '240', '199', '52', '218']","<S sid =""131"" ssid = ""3"">Dynamic programming efficiently scores many hypotheses by exploiting the fact that an N-gram language model conditions on at most N − 1 preceding words.</S><S sid =""240"" ssid = ""59"">In line with perplexity results from Table 1  the PROBING model is the fastest followed by TRIE  and subsequently other packages.</S><S sid =""199"" ssid = ""18"">For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid =""52"" ssid = ""30"">Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.</S><S sid =""218"" ssid = ""37"">We run the baseline Moses system for the French-English track of the 2011 Workshop on Machine Translation 9 translating the 3003-sentence test set.</S>",['Aim_Citation']
18,W11-2123,P12-2006,0,"Heafield, 2011",0,"Research efforts to increase search efficiency for phrase-based MT (Koehn et al, 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al, 2006) to additional early pruning techniques (Delaney et al, 2006), (Moore and Quirk, 2007) and more efficient language model (LM) querying (Heafield, 2011)","Research efforts to increase search efficiency for phrase-based MT (Koehn et al, 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al, 2006) to additional early pruning techniques (Delaney et al, 2006), (Moore and Quirk, 2007) and more efficient language model (LM) querying (Heafield, 2011)","['21', '102', '206', '268', '51']","<S sid =""21"" ssid = ""16"">Performance improvements transfer to the Moses (Koehn et al.  2007)  cdec (Dyer et al.  2010)  and Joshua (Li et al.  2009) translation systems where our code has been integrated.</S><S sid =""102"" ssid = ""6"">The PROBING model was designed to improve upon SRILM by using linear probing hash tables (though not arranged in a trie)  allocating memory all at once (eliminating the need for full pointers)  and being easy to compile.</S><S sid =""206"" ssid = ""25"">Tokens were converted to vocabulary identifiers in advance and state was carried from each query to the next.</S><S sid =""268"" ssid = ""10"">For example  syntactic decoders (Koehn et al.  2007; Dyer et al.  2010; Li et al.  2009) perform dynamic programming parametrized by both backward- and forward-looking state.</S><S sid =""51"" ssid = ""29"">This differs from other implementations (Stolcke  2002; Pauls and Klein  2011) that use hash tables as nodes in a trie  as explained in the next section.</S>",['Aim_Citation']
19,W11-2123,P13-2073,0,"Heafield, 2011",0,"For English language modeling, we use English Giga word Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011)","For English language modeling, we use English Giga word Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011)","['199', '152', '205', '244', '242']","<S sid =""199"" ssid = ""18"">For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid =""152"" ssid = ""24"">All language model queries issued by machine translation decoders follow a left-to-right pattern  starting with either the begin of sentence token or null context for mid-sentence fragments.</S><S sid =""205"" ssid = ""24"">We evaluate the time and memory consumption of each data structure by computing perplexity on 4 billion tokens from the English Gigaword corpus (Parker et al.  2009).</S><S sid =""244"" ssid = ""63"">Time for Moses itself to load  including loading the language model and phrase table  is included.</S><S sid =""242"" ssid = ""61"">The TRIE model continues to use the least memory of ing (-P) with MAP POPULATE  the default.</S>","['Implication_Citation', 'Aim_Citation']"
20,W11-2123,P13-1109,0,"Heafield, 2011",0,"For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with KneserNey smoothing","For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with KneserNey smoothing","['199', '192', '218', '205', '241']","<S sid =""199"" ssid = ""18"">For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid =""192"" ssid = ""11"">For the PROBING implementation  hash table sizes are in the millions  so the most relevant values are on the right size of the graph  where linear probing wins.</S><S sid =""218"" ssid = ""37"">We run the baseline Moses system for the French-English track of the 2011 Workshop on Machine Translation 9 translating the 3003-sentence test set.</S><S sid =""205"" ssid = ""24"">We evaluate the time and memory consumption of each data structure by computing perplexity on 4 billion tokens from the English Gigaword corpus (Parker et al.  2009).</S><S sid =""241"" ssid = ""60"">We incur some additional memory cost due to storing state in each hypothesis  though this is minimal compared with the size of the model itself.</S>",['Results_Citation']
