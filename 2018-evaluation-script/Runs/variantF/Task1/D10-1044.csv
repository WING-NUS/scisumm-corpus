Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,D10-1044,P11-2074,0,"Foster et al, 2010",0,"Another popular task in SMT is domain adaptation (Foster et al, 2010)","Another popular task in SMT is domain adaptation (Foster et al, 2010)","['10', '5', '48', '86', '6']","<S sid =""10"" ssid = ""7"">This is a standard adaptation problem for SMT.</S><S sid =""5"" ssid = ""2"">Even when there is training data available in the domain of interest  there is often additional data from other domains that could in principle be used to improve performance.</S><S sid =""48"" ssid = ""12"">This is appropriate in cases where it is sanctioned by Bayes’ law  such as multiplying LM and TM probabilities  but for adaptation a more suitable framework is often a mixture model in which each event may be generated from some domain.</S><S sid =""86"" ssid = ""23"">This variant is tested in the experiments below.</S><S sid =""6"" ssid = ""3"">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S>",['Method_Citation']
2,D10-1044,P12-1048,0,"Foster et al, 2010",0,"In addition, discriminative weighting methods were proposed to assign appropriate weights to the sentences from training corpus (Matsoukas et al, 2009) or the phrase pairs of phrase table (Foster et al, 2010)","In addition, discriminative weighting methods were proposed to assign appropriate weights to the sentences from training corpus (Matsoukas et al, 2009) or the phrase pairs of phrase table (Foster et al, 2010)","['152', '96', '95', '62', '113']","<S sid =""152"" ssid = ""9"">We will also directly compare with a baseline similar to the Matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences.</S><S sid =""96"" ssid = ""33"">We used it to score all phrase pairs in the OUT table  in order to provide a feature for the instance-weighting model.</S><S sid =""95"" ssid = ""32"">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models)  and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples  with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.</S><S sid =""62"" ssid = ""26"">To approximate these baselines  we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S><S sid =""113"" ssid = ""17"">The corpus was wordaligned using both HMM and IBM2 models  and the phrase table was the union of phrases extracted from these separate alignments  with a length limit of 7.</S>",['Method_Citation']
3,D10-1044,D12-1129,0,"Foster et al., 2010",0,"Domain knowledge also has the potential to improve open-text applications such as summarization (Ceylan et al 2010) and machine translation (Foster et al., 2010) .Research in Word Sense Disambiguation (Navigli, 2009, WSD), the task aimed at the automatic labeling of text with word senses, has been oriented towards domain text understanding for several years now","Domain knowledge also has the potential to improve open-text applications such as summarization (Ceylan et al 2010) and machine translation (Foster et al., 2010)","['141', '142', '59', '67', '132']","<S sid =""141"" ssid = ""10"">Moving beyond directly related work  major themes in SMT adaptation include the IR (Hildebrand et al.  2005; L¨u et al.  2007; Zhao et al.  2004) and mixture (Finch and Sumita  2008; Foster and Kuhn  2007; Koehn and Schroeder  2007; L¨u et al.  2007) approaches for LMs and TMs described above  as well as methods for exploiting monolingual in-domain text  typically by translating it automatically and then performing self training (Bertoldi and Federico  2009; Ueffing et al.  2007; Schwenk and Senellart  2009).</S><S sid =""142"" ssid = ""11"">There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan  2007; Wu et al.  2005)  and on dynamically choosing a dev set (Xu et al.  2007).</S><S sid =""59"" ssid = ""23"">To set β  we used the same criterion as for α  over a dev corpus: The MAP combination was used for TM probabilities only  in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney  1995).3 Motivated by information retrieval  a number of approaches choose “relevant” sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al.  2005; L¨u et al.  2007)  or individual target hypotheses (Zhao et al.  2004).</S><S sid =""67"" ssid = ""4"">We extend the Matsoukas et al approach in several ways.</S><S sid =""132"" ssid = ""1"">We have already mentioned the closely related work by Matsoukas et al (2009) on discriminative corpus weighting  and Jiang and Zhai (2007) on (nondiscriminative) instance weighting.</S>",['Method_Citation']
4,D10-1044,P14-2093,0,2010,0,"Yasuda et al (2008) and Foster et al (2010) ranked the sentence pairs in the general-domain corpus according to the perplexity scores of sentences, which are computed with respect to in-domain language models","Yasuda et al (2008) and Foster et al (2010) ranked the sentence pairs in the general-domain corpus according to the perplexity scores of sentences, which are computed with respect to in-domain language models","['62', '119', '152', '96', '59']","<S sid =""62"" ssid = ""26"">To approximate these baselines  we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S><S sid =""119"" ssid = ""23"">The 2nd block contains the IR system  which was tuned by selecting text in multiples of the size of the EMEA training corpus  according to dev set performance.</S><S sid =""152"" ssid = ""9"">We will also directly compare with a baseline similar to the Matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences.</S><S sid =""96"" ssid = ""33"">We used it to score all phrase pairs in the OUT table  in order to provide a feature for the instance-weighting model.</S><S sid =""59"" ssid = ""23"">To set β  we used the same criterion as for α  over a dev corpus: The MAP combination was used for TM probabilities only  in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney  1995).3 Motivated by information retrieval  a number of approaches choose “relevant” sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al.  2005; L¨u et al.  2007)  or individual target hypotheses (Zhao et al.  2004).</S>",['Implication_Citation']
5,D10-1044,E12-1055,0,2010,0,"However, such confounding factors do not affect the optimization algorithm, which works with a fixed set of phrase pairs, and merely varies? .Our main technical contributions are as fol lows: Additionally to perplexity optimization for linear interpolation, which was first applied by Foster et al (2010), we propose perplexity optimization for weighted counts (equation 3), and a modified implementation of linear interpolation. Also, we independently perform perplexity minimization for all four features of the standard SMTtranslation model: the phrase translation probabilities p (t|s) and p (s|t), and the lexical weights lex (t|s) and lex (s|t)","Our main technical contributions are as follows: Additionally to perplexity optimization for linear interpolation, which was first applied by Foster et al (2010), we propose perplexity optimization for weighted counts (equation 3), and a modified implementation of linear interpolation","['59', '123', '50', '95', '72']","<S sid =""59"" ssid = ""23"">To set β  we used the same criterion as for α  over a dev corpus: The MAP combination was used for TM probabilities only  in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney  1995).3 Motivated by information retrieval  a number of approaches choose “relevant” sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al.  2005; L¨u et al.  2007)  or individual target hypotheses (Zhao et al.  2004).</S><S sid =""123"" ssid = ""27"">However  when the linear LM is combined with a linear TM (lm+lin tm) or MAP TM (lm+map TM)  the results are much better than a log-linear combination for the EMEA setting  and on a par for NIST.</S><S sid =""50"" ssid = ""14"">Linear weights are difficult to incorporate into the standard MERT procedure because they are “hidden” within a top-level probability that represents the linear combination.1 Following previous work (Foster and Kuhn  2007)  we circumvent this problem by choosing weights to optimize corpus loglikelihood  which is roughly speaking the training criterion used by the LM and TM themselves.</S><S sid =""95"" ssid = ""32"">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models)  and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples  with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.</S><S sid =""72"" ssid = ""9"">The original OUT counts co(s  t) are weighted by a logistic function wλ(s  t): To motivate weighting joint OUT counts as in (6)  we begin with the “ideal” objective for setting multinomial phrase probabilities 0 = {p(s|t)  dst}  which is the likelihood with respect to the true IN distribution pi(s  t).</S>",['Method_Citation']
6,D10-1044,E12-1055,0,2010,0,"Matsoukas et al (2009) propose an approach where each sentence is weighted according to a classifier, and Foster et al (2010) ex tend this approach by weighting individual phrase pairs","Matsoukas et al (2009) propose an approach where each sentence is weighted according to a classifier, and Foster et al (2010) extend this approach by weighting individual phrase pairs","['67', '65', '152', '59', '26']","<S sid =""67"" ssid = ""4"">We extend the Matsoukas et al approach in several ways.</S><S sid =""65"" ssid = ""2"">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S><S sid =""152"" ssid = ""9"">We will also directly compare with a baseline similar to the Matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences.</S><S sid =""59"" ssid = ""23"">To set β  we used the same criterion as for α  over a dev corpus: The MAP combination was used for TM probabilities only  in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney  1995).3 Motivated by information retrieval  a number of approaches choose “relevant” sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al.  2005; L¨u et al.  2007)  or individual target hypotheses (Zhao et al.  2004).</S><S sid =""26"" ssid = ""23"">Phrase-level granularity distinguishes our work from previous work by Matsoukas et al (2009)  who weight sentences according to sub-corpus and genre membership.</S>",['Method_Citation']
7,D10-1044,E12-1055,0,2010,0,"These more fine-grained methods need not be seen as alternatives to coarse-grained ones. Foster et al (2010) combine the two, applying linear interpolation to combine the instance 542 weighted out-of-domain model with an in-domain model","Foster et al (2010) combine the two, applying linear interpolation to combine the instance weighted out-of-domain model with an in-domain model","['45', '152', '114', '67', '69']","<S sid =""45"" ssid = ""9"">An easy way to achieve this is to put the domain-specific LMs and TMs into the top-level log-linear model and learn optimal weights with MERT (Och  2003).</S><S sid =""152"" ssid = ""9"">We will also directly compare with a baseline similar to the Matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences.</S><S sid =""114"" ssid = ""18"">It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.</S><S sid =""67"" ssid = ""4"">We extend the Matsoukas et al approach in several ways.</S><S sid =""69"" ssid = ""6"">Intuitively  as suggested by the example in the introduction  this is the right granularity to capture domain effects.</S>",['Method_Citation']
8,D10-1044,E12-1055,0,Foster et al 2010,0,Note that both data sets have a relatively high ratio of in-domain to out-of-domain parallel training data (1:20 for DE? EN and 1:5 for HT? EN) Previous research has been performed with ratios of 1:100 (Foster et al 2010) or 1:400 (Axelrod et al 2011),Note that both data sets have a relatively high ratio of in-domain to out-of-domain parallel training data (1:20 for DE? EN and 1:5 for HT? EN); Previous research has been performed with ratios of 1:100 (Foster et al 2010) or 1:400 (Axelrod et al 2011),"['59', '95', '142', '113', '19']","<S sid =""59"" ssid = ""23"">To set β  we used the same criterion as for α  over a dev corpus: The MAP combination was used for TM probabilities only  in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney  1995).3 Motivated by information retrieval  a number of approaches choose “relevant” sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al.  2005; L¨u et al.  2007)  or individual target hypotheses (Zhao et al.  2004).</S><S sid =""95"" ssid = ""32"">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models)  and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples  with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.</S><S sid =""142"" ssid = ""11"">There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan  2007; Wu et al.  2005)  and on dynamically choosing a dev set (Xu et al.  2007).</S><S sid =""113"" ssid = ""17"">The corpus was wordaligned using both HMM and IBM2 models  and the phrase table was the union of phrases extracted from these separate alignments  with a length limit of 7.</S><S sid =""19"" ssid = ""16"">The idea of distinguishing between general and domain-specific examples is due to Daum´e and Marcu (2006)  who used a maximum-entropy model with latent variables to capture the degree of specificity.</S>",['Method_Citation']
9,D10-1044,E12-1055,0,Foster et al 2010,0,"We expand on work by (Foster et al 2010) in establishing translation model perplexity minimization as a robust baseline for a weighted combination of translationmodels.15 We demonstrate perplexity optimization for weighted counts, which are a natural extension of unadapted MLE training, but are of little prominence in domain adaptation research",We expand on work by (Foster et al 2010) in establishing translation model perplexity minimization as a robust baseline for a weighted combination of translation models,"['59', '9', '149', '96', '152']","<S sid =""59"" ssid = ""23"">To set β  we used the same criterion as for α  over a dev corpus: The MAP combination was used for TM probabilities only  in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney  1995).3 Motivated by information retrieval  a number of approaches choose “relevant” sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al.  2005; L¨u et al.  2007)  or individual target hypotheses (Zhao et al.  2004).</S><S sid =""9"" ssid = ""6"">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material—though adequate for reasonable performance—is also available.</S><S sid =""149"" ssid = ""6"">We obtained positive results using a very simple phrase-based system in two different adaptation settings: using English/French Europarl to improve a performance on a small  specialized medical domain; and using non-news portions of the NIST09 training material to improve performance on the news-related corpora.</S><S sid =""96"" ssid = ""33"">We used it to score all phrase pairs in the OUT table  in order to provide a feature for the instance-weighting model.</S><S sid =""152"" ssid = ""9"">We will also directly compare with a baseline similar to the Matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences.</S>",['Method_Citation']
10,D10-1044,P12-1099,0,"Foster et al, 2010",0,"In addition to the basic approach of concatenation of in-domain and out-of-domain data, we also trained a log-linear mixture model (Foster and Kuhn, 2007) 940 as well as the linear mixture model of (Foster et al, 2010) for conditional phrase-pair probabilities over IN and OUT","In addition to the basic approach of concatenation of in-domain and out-of-domain data, we also trained a log-linear mixture model (Foster and Kuhn, 2007) as well as the linear mixture model of (Foster et al, 2010) for conditional phrase-pair probabilities over IN and OUT","['28', '95', '124', '48', '98']","<S sid =""28"" ssid = ""25"">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S><S sid =""95"" ssid = ""32"">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models)  and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples  with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.</S><S sid =""124"" ssid = ""28"">This is consistent with the nature of these two settings: log-linear combination  which effectively takes the intersection of IN and OUT  does relatively better on NIST  where the domains are broader and closer together.</S><S sid =""48"" ssid = ""12"">This is appropriate in cases where it is sanctioned by Bayes’ law  such as multiplying LM and TM probabilities  but for adaptation a more suitable framework is often a mixture model in which each event may be generated from some domain.</S><S sid =""98"" ssid = ""2"">The first setting uses the European Medicines Agency (EMEA) corpus (Tiedemann  2009) as IN  and the Europarl (EP) corpus (www.statmt.org/europarl) as OUT  for English/French translation in both directions.</S>",['Implication_Citation']
11,D10-1044,P12-1099,0,2010,0,m ?mpm (e? |f?) Our technique for setting? m is similar to that outlined in Foster et al (2010),Our technique for setting ? m is similar to that outlined in Foster et al (2010),"['85', '59', '105', '152', '133']","<S sid =""85"" ssid = ""22"">With the additional assumption that (s  t) can be restricted to the support of co(s  t)  this is equivalent to a “flat” alternative to (6) in which each non-zero co(s  t) is set to one.</S><S sid =""59"" ssid = ""23"">To set β  we used the same criterion as for α  over a dev corpus: The MAP combination was used for TM probabilities only  in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney  1995).3 Motivated by information retrieval  a number of approaches choose “relevant” sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al.  2005; L¨u et al.  2007)  or individual target hypotheses (Zhao et al.  2004).</S><S sid =""105"" ssid = ""9"">Compared to the EMEA/EP setting  the two domains in the NIST setting are less homogeneous and more similar to each other; there is also considerably more IN text available.</S><S sid =""152"" ssid = ""9"">We will also directly compare with a baseline similar to the Matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences.</S><S sid =""133"" ssid = ""2"">It is difficult to directly compare the Matsoukas et al results with ours  since our out-of-domain corpus is homogeneous; given heterogeneous training data  however  it would be trivial to include Matsoukas-style identity features in our instance-weighting model.</S>",['Method_Citation']
12,D10-1044,P12-1099,0,"Foster et al., 2010",0,"m ?mpm (e? |f?) For efficiency and stability, we use the EMalgorithm to find??, rather than L-BFGS as in (Foster et al., 2010)","For efficiency and stability, we use the EM algorithm to find ?, rather than L-BFGS as in (Foster et al., 2010)","['75', '151', '59', '87', '96']","<S sid =""75"" ssid = ""12"">However  it is robust  efficient  and easy to implement.4 To perform the maximization in (7)  we used the popular L-BFGS algorithm (Liu and Nocedal  1989)  which requires gradient information.</S><S sid =""151"" ssid = ""8"">In future work we plan to try this approach with more competitive SMT systems  and to extend instance weighting to other standard SMT components such as the LM  lexical phrase weights  and lexicalized distortion.</S><S sid =""59"" ssid = ""23"">To set β  we used the same criterion as for α  over a dev corpus: The MAP combination was used for TM probabilities only  in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney  1995).3 Motivated by information retrieval  a number of approaches choose “relevant” sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al.  2005; L¨u et al.  2007)  or individual target hypotheses (Zhao et al.  2004).</S><S sid =""87"" ssid = ""24"">A final alternate approach would be to combine weighted joint frequencies rather than conditional estimates  ie: cI(s  t) + w \(s  t)co(  s  t)  suitably normalized.5 Such an approach could be simulated by a MAP-style combination in which separate 0(t) values were maintained for each t. This would make the model more powerful  but at the cost of having to learn to downweight OUT separately for each t  which we suspect would require more training data for reliable performance.</S><S sid =""96"" ssid = ""33"">We used it to score all phrase pairs in the OUT table  in order to provide a feature for the instance-weighting model.</S>",['Implication_Citation']
13,D10-1044,P12-1099,0,2010,0,"Foster et al (2010), however, uses a different approach to select related sentences from OUT","Foster et al (2010), however, uses a different approach to select related sentences from OUT","['31', '152', '59', '67', '143']","<S sid =""31"" ssid = ""28"">For comparison to information-retrieval inspired baselines  eg (L¨u et al.  2007)  we select sentences from OUT using language model perplexities from IN.</S><S sid =""152"" ssid = ""9"">We will also directly compare with a baseline similar to the Matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences.</S><S sid =""59"" ssid = ""23"">To set β  we used the same criterion as for α  over a dev corpus: The MAP combination was used for TM probabilities only  in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney  1995).3 Motivated by information retrieval  a number of approaches choose “relevant” sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al.  2005; L¨u et al.  2007)  or individual target hypotheses (Zhao et al.  2004).</S><S sid =""67"" ssid = ""4"">We extend the Matsoukas et al approach in several ways.</S><S sid =""143"" ssid = ""12"">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S>",['Method_Citation']
14,D10-1044,P12-1099,0,2010,0,Foster et al (2010) propose asimilar method for machine translation that uses features to capture degrees of generality,Foster et al (2010) propose a similar method for machine translation that uses features to capture degrees of generality,"['153', '22', '70', '89', '96']","<S sid =""153"" ssid = ""10"">Finally  we intend to explore more sophisticated instanceweighting features for capturing the degree of generality of phrase pairs.</S><S sid =""22"" ssid = ""19"">Within this framework  we use features intended to capture degree of generality  including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.</S><S sid =""70"" ssid = ""7"">Second  rather than relying on a division of the corpus into manually-assigned portions  we use features intended to capture the usefulness of each phrase pair.</S><S sid =""89"" ssid = ""26"">We used 22 features for the logistic weighting model  divided into two groups: one intended to reflect the degree to which a phrase pair belongs to general language  and one intended to capture similarity to the IN domain.</S><S sid =""96"" ssid = ""33"">We used it to score all phrase pairs in the OUT table  in order to provide a feature for the instance-weighting model.</S>",['Method_Citation']
15,D10-1044,P13-1126,0,"Foster et al, 2010",0,"As in (Foster et al, 2010), this approach works at the level of phrase pairs","As in (Foster et al, 2010), this approach works at the level of phrase pairs","['56', '96', '69', '23', '95']","<S sid =""56"" ssid = ""20"">For the TM  this is: where cI(s  t) is the count in the IN phrase table of pair (s  t)  po(s|t) is its probability under the OUT TM  and cI(t) = &quot;s  cI(s'  t).</S><S sid =""96"" ssid = ""33"">We used it to score all phrase pairs in the OUT table  in order to provide a feature for the instance-weighting model.</S><S sid =""69"" ssid = ""6"">Intuitively  as suggested by the example in the introduction  this is the right granularity to capture domain effects.</S><S sid =""23"" ssid = ""20"">Our second contribution is to apply instance weighting at the level of phrase pairs.</S><S sid =""95"" ssid = ""32"">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models)  and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples  with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.</S>",['Implication_Citation']
16,D10-1044,D11-1033,0,2010,0,"The ranking of the sentences in a general-domain corpus according to in-domain perplexity has also been applied to machine translation by both Yasuda et al (2008), and Foster et al (2010)","The ranking of the sentences in a general-domain corpus according to in-domain perplexity has also been applied to machine translation by both Yasuda et al (2008), and Foster et al (2010)","['59', '152', '142', '113', '132']","<S sid =""59"" ssid = ""23"">To set β  we used the same criterion as for α  over a dev corpus: The MAP combination was used for TM probabilities only  in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney  1995).3 Motivated by information retrieval  a number of approaches choose “relevant” sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al.  2005; L¨u et al.  2007)  or individual target hypotheses (Zhao et al.  2004).</S><S sid =""152"" ssid = ""9"">We will also directly compare with a baseline similar to the Matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences.</S><S sid =""142"" ssid = ""11"">There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan  2007; Wu et al.  2005)  and on dynamically choosing a dev set (Xu et al.  2007).</S><S sid =""113"" ssid = ""17"">The corpus was wordaligned using both HMM and IBM2 models  and the phrase table was the union of phrases extracted from these separate alignments  with a length limit of 7.</S><S sid =""132"" ssid = ""1"">We have already mentioned the closely related work by Matsoukas et al (2009) on discriminative corpus weighting  and Jiang and Zhai (2007) on (nondiscriminative) instance weighting.</S>",['Method_Citation']
17,D10-1044,D11-1033,0,2010,0,"Foster et al (2010) do not mention what percentage of the corpus they select for their IR-baseline, but they concatenate the data to their in-domain corpus and re port a decrease in performance","Foster et al (2010) do not mention what percentage of the corpus they select for their IR-baseline, but they concatenate the data to their in-domain corpus and report a decrease in performance","['50', '43', '119', '41', '70']","<S sid =""50"" ssid = ""14"">Linear weights are difficult to incorporate into the standard MERT procedure because they are “hidden” within a top-level probability that represents the linear combination.1 Following previous work (Foster and Kuhn  2007)  we circumvent this problem by choosing weights to optimize corpus loglikelihood  which is roughly speaking the training criterion used by the LM and TM themselves.</S><S sid =""43"" ssid = ""7"">Its success depends on the two domains being relatively close  and on the OUT corpus not being so large as to overwhelm the contribution of IN.</S><S sid =""119"" ssid = ""23"">The 2nd block contains the IR system  which was tuned by selecting text in multiples of the size of the EMEA training corpus  according to dev set performance.</S><S sid =""41"" ssid = ""5"">We do not adapt the alignment procedure for generating the phrase table from which the TM distributions are derived.</S><S sid =""70"" ssid = ""7"">Second  rather than relying on a division of the corpus into manually-assigned portions  we use features intended to capture the usefulness of each phrase pair.</S>",['Aim_Citation']
18,D10-1044,D11-1033,0,2010,0,"Foster et al (2010) further perform this on extracted phrase pairs, not just sentences","Foster et al (2010) further perform this on extracted phrase pairs, not just sentences","['142', '65', '152', '82', '133']","<S sid =""142"" ssid = ""11"">There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan  2007; Wu et al.  2005)  and on dynamically choosing a dev set (Xu et al.  2007).</S><S sid =""65"" ssid = ""2"">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S><S sid =""152"" ssid = ""9"">We will also directly compare with a baseline similar to the Matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences.</S><S sid =""82"" ssid = ""19"">This is not unreasonable given the application to phrase pairs from OUT  but it suggests that an interesting alternative might be to use a plain log-linear weighting function exp(Ei Aifi(s  t))  with outputs in [0  oo].</S><S sid =""133"" ssid = ""2"">It is difficult to directly compare the Matsoukas et al results with ours  since our out-of-domain corpus is homogeneous; given heterogeneous training data  however  it would be trivial to include Matsoukas-style identity features in our instance-weighting model.</S>",['Aim_Citation']
19,D10-1044,P14-1012,0,"Foster et al, 2010",0,"To address the first shortcoming, we adapt and extend some simple but effective phrase features as the input features for new DNN feature learning, and these features have been shown significant improvement for SMT, such as, phrase pair similarity (Zhao et al, 2004), phrase frequency, phrase length (Hopkins and May, 2011), and phrase generative probability (Foster et al, 2010), which also show further improvement for new phrase feature learning in our experiments","To address the first shortcoming, we adapt and extend some simple but effective phrase features as the input features for new DNN feature learning, and these features have been shown significant improvement for SMT, such as, phrase pair similarity (Zhao et al, 2004), phrase frequency, phrase length (Hopkins and May, 2011), and phrase generative probability (Foster et al, 2010), which also show further improvement for new phrase feature learning in our experiments","['40', '95', '131', '113', '96']","<S sid =""40"" ssid = ""4"">We focus here instead on adapting the two most important features: the language model (LM)  which estimates the probability p(wIh) of a target word w following an ngram h; and the translation models (TM) p(slt) and p(t1s)  which give the probability of source phrase s translating to target phrase t  and vice versa.</S><S sid =""95"" ssid = ""32"">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models)  and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples  with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.</S><S sid =""131"" ssid = ""35"">The general-language features have a slight advantage over the similarity features  and both are better than the SVM feature.</S><S sid =""113"" ssid = ""17"">The corpus was wordaligned using both HMM and IBM2 models  and the phrase table was the union of phrases extracted from these separate alignments  with a length limit of 7.</S><S sid =""96"" ssid = ""33"">We used it to score all phrase pairs in the OUT table  in order to provide a feature for the instance-weighting model.</S>","['Implication_Citation', 'Aim_Citation']"
