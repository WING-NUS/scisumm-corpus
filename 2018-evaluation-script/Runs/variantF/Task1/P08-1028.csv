Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P08-1028,D08-1094,0,2008,0,"Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p+ a as a function f operating on four components: c= f (p, a, R, K) (3) R is the relation holding between p and a, and Kadditional knowledge","Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p+ a as a function f operating on four components: c= f (p, a, R, K) (3) R is the relation holding between p and a, and K additional knowledge","['64', '57', '58', '32', '65']","<S sid =""64"" ssid = ""12"">Now  if we assume that p lies in the same space as u and v  avoiding the issues of dimensionality associated with tensor products  and that f is a linear function  for simplicity  of the cartesian product of u and v  then we generate a class of additive models: where A and B are matrices which determine the contributions made by u and v to the product p. In contrast  if we assume that f is a linear function of the tensor product of u and v  then we obtain multiplicative models: where C is a tensor of rank 3  which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.</S><S sid =""57"" ssid = ""5"">Let p denote the composition of two vectors u and v  representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S><S sid =""58"" ssid = ""6"">We define a general class of models for this process of composition as: The expression above allows us to derive models for which p is constructed in a distinct space from u and v  as is the case for tensor products.</S><S sid =""32"" ssid = ""5"">Either there is a failure to distinguish between these two structures  because the network fails to keep track of the fact that John is subject in one and object in the other  or there is a failure to recognize that both structures involve the same participants  because John as a subject has a distinct representation from John as an object.</S><S sid =""65"" ssid = ""13"">So  if we assume that only the ith components of u and v contribute to the ith component of p  that these components are not dependent on i  and that the function is symmetric with regard to the interchange of u and v  we obtain a simpler instantiation of an additive model: Analogously  under the same assumptions  we obtain the following simpler multiplicative model: only the ith components of u and v contribute to the ith component of p. Another class of models can be derived by relaxing this constraint.</S>",['Method_Citation']
4,P08-1028,P14-1060,0,2008,0,"While works such asthe SDSM model suffer from the problem of sparsity in composing structures beyond bi grams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstetteand Sadrzadeh (2011) are restricted by significant model biases in representing semantic com position by generic algebraic operations","While works such as the SDSM model suffer from the problem of sparsity in composing structures beyond bigrams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstette and Sadrzadeh (2011) are restricted by significant model biases in representing semantic com position by generic algebraic operations","['19', '200', '58', '108', '56']","<S sid =""19"" ssid = ""15"">While vector addition has been effective in some applications such as essay grading (Landauer and Dumais  1997) and coherence assessment (Foltz et al.  1998)  there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al.  1991; West and Stanovich  1986) and modulate cognitive behavior in sentence priming (Till et al.  1988) and inference tasks (Heit and Rubinstein  1994).</S><S sid =""200"" ssid = ""12"">The applications of the framework discussed here are many and varied both for cognitive science and NLP.</S><S sid =""58"" ssid = ""6"">We define a general class of models for this process of composition as: The expression above allows us to derive models for which p is constructed in a distinct space from u and v  as is the case for tensor products.</S><S sid =""108"" ssid = ""21"">Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).</S><S sid =""56"" ssid = ""4"">Here  the space has only five dimensions  and the matrix cells denote the co-occurrence of the target words (horse and run) with the context words animal  stable  and so on.</S>",['Method_Citation']
6,P08-1028,P10-1097,0,2008,0,"Mitchell and Lapata (2008), henceforth M& amp; L, propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression","Mitchell and Lapata (2008), henceforth M& amp; L, propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression","['21', '1', '56', '146', '75']","<S sid =""21"" ssid = ""17"">Central in these models is the notion of compositionality — the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.</S><S sid =""1"" ssid = ""1"">This paper proposes a framework for representing the meaning of phrases and sentences in vector space.</S><S sid =""56"" ssid = ""4"">Here  the space has only five dimensions  and the matrix cells denote the co-occurrence of the target words (horse and run) with the context words animal  stable  and so on.</S><S sid =""146"" ssid = ""59"">These components were set to the ratio of the probability of the context word given the target word to the probability of the context word overall.</S><S sid =""75"" ssid = ""23"">An extreme form of this differential in the contribution of constituents is where one of the vectors  say u  contributes nothing at all to the combination: Admittedly the model in (8) is impoverished and rather simplistic  however it can serve as a simple baseline against which to compare more sophisticated models.</S>",['Method_Citation']
7,P08-1028,P10-1097,0,2008,0,"Interestingly, Mitchell and Lapata (2008) came to the same result in a different setting","Interestingly, Mitchell and Lapata (2008) came to the same result in a different setting","['32', '36', '110', '109', '58']","<S sid =""32"" ssid = ""5"">Either there is a failure to distinguish between these two structures  because the network fails to keep track of the fact that John is subject in one and object in the other  or there is a failure to recognize that both structures involve the same participants  because John as a subject has a distinct representation from John as an object.</S><S sid =""36"" ssid = ""9"">To overcome this problem  other techniques have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components.</S><S sid =""110"" ssid = ""23"">In the pretest  subjects saw a reference sentence containing a subject-verb tuple and its landmarks and were asked to choose which landmark was most similar to the reference or neither.</S><S sid =""109"" ssid = ""22"">These were further pretested to allow the selection of a subset of items showing clear variations in sense as we wanted to have a balanced set of similar and dissimilar sentences.</S><S sid =""58"" ssid = ""6"">We define a general class of models for this process of composition as: The expression above allows us to derive models for which p is constructed in a distinct space from u and v  as is the case for tensor products.</S>",['Implication_Citation']
8,P08-1028,D11-1094,0,2008,0,"And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to combine the constituent vectors","And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to combine the constituent vectors","['75', '48', '76', '53', '36']","<S sid =""75"" ssid = ""23"">An extreme form of this differential in the contribution of constituents is where one of the vectors  say u  contributes nothing at all to the combination: Admittedly the model in (8) is impoverished and rather simplistic  however it can serve as a simple baseline against which to compare more sophisticated models.</S><S sid =""48"" ssid = ""21"">The idea is to add not only the vectors representing the predicate and its argument but also the neighbors associated with both of them.</S><S sid =""76"" ssid = ""24"">The models considered so far assume that components do not ‘interfere’ with each other  i.e.  that It is also possible to re-introduce the dependence on K into the model of vector composition.</S><S sid =""53"" ssid = ""1"">We formulate semantic composition as a function of two vectors  u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word’s vector typically represents its co-occurrence with neighboring words.</S><S sid =""36"" ssid = ""9"">To overcome this problem  other techniques have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components.</S>",['Method_Citation']
9,P08-1028,W11-0131,0,2008,0,"Mitchell and Lapata (2008) provide a general framework for semantic vector composition: p= f (u, v, R, K) (1) 295 where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base, and p is a resulting composed vector (or tensor)","Mitchell and Lapata (2008) provide a general framework for semantic vector composition: p= f (u, v, R, K) (1) 295 where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base, and p is a resulting composed vector (or tensor)","['64', '58', '57', '35', '95']","<S sid =""64"" ssid = ""12"">Now  if we assume that p lies in the same space as u and v  avoiding the issues of dimensionality associated with tensor products  and that f is a linear function  for simplicity  of the cartesian product of u and v  then we generate a class of additive models: where A and B are matrices which determine the contributions made by u and v to the product p. In contrast  if we assume that f is a linear function of the tensor product of u and v  then we obtain multiplicative models: where C is a tensor of rank 3  which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.</S><S sid =""58"" ssid = ""6"">We define a general class of models for this process of composition as: The expression above allows us to derive models for which p is constructed in a distinct space from u and v  as is the case for tensor products.</S><S sid =""57"" ssid = ""5"">Let p denote the composition of two vectors u and v  representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S><S sid =""35"" ssid = ""8"">The tensor product u ® v is a matrix whose components are all the possible products uivj of the components of vectors u and v. A major difficulty with tensor products is their dimensionality which is higher than the dimensionality of the original vectors (precisely  the tensor product has dimensionality m x n).</S><S sid =""95"" ssid = ""8"">Focusing on a single compositional structure  namely intransitive verbs and their subjects  is a good point of departure for studying vector combination.</S>",['Method_Citation']
10,P08-1028,W11-0131,0,2008,0,"As Mitchell and Lapata (2008) did, let us temporarily suspend discussion on what semantics populate our vectors for now","As Mitchell and Lapata (2008) did, let us temporarily suspend discussion on what semantics populate our vectors for now","['75', '48', '200', '100', '64']","<S sid =""75"" ssid = ""23"">An extreme form of this differential in the contribution of constituents is where one of the vectors  say u  contributes nothing at all to the combination: Admittedly the model in (8) is impoverished and rather simplistic  however it can serve as a simple baseline against which to compare more sophisticated models.</S><S sid =""48"" ssid = ""21"">The idea is to add not only the vectors representing the predicate and its argument but also the neighbors associated with both of them.</S><S sid =""200"" ssid = ""12"">The applications of the framework discussed here are many and varied both for cognitive science and NLP.</S><S sid =""100"" ssid = ""13"">In the following we describe our data collection procedure and give details on how our composition models were constructed and evaluated.</S><S sid =""64"" ssid = ""12"">Now  if we assume that p lies in the same space as u and v  avoiding the issues of dimensionality associated with tensor products  and that f is a linear function  for simplicity  of the cartesian product of u and v  then we generate a class of additive models: where A and B are matrices which determine the contributions made by u and v to the product p. In contrast  if we assume that f is a linear function of the tensor product of u and v  then we obtain multiplicative models: where C is a tensor of rank 3  which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.</S>",['Method_Citation']
11,P08-1028,P13-2083,0,2008,0,"Mitchell and Lapata (2008) propose a framework to define the composition c= f (a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition","Mitchell and Lapata (2008) propose a framework to define the composition c= f (a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition","['58', '64', '57', '59', '32']","<S sid =""58"" ssid = ""6"">We define a general class of models for this process of composition as: The expression above allows us to derive models for which p is constructed in a distinct space from u and v  as is the case for tensor products.</S><S sid =""64"" ssid = ""12"">Now  if we assume that p lies in the same space as u and v  avoiding the issues of dimensionality associated with tensor products  and that f is a linear function  for simplicity  of the cartesian product of u and v  then we generate a class of additive models: where A and B are matrices which determine the contributions made by u and v to the product p. In contrast  if we assume that f is a linear function of the tensor product of u and v  then we obtain multiplicative models: where C is a tensor of rank 3  which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.</S><S sid =""57"" ssid = ""5"">Let p denote the composition of two vectors u and v  representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S><S sid =""59"" ssid = ""7"">It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence  via the argument R  on syntax.</S><S sid =""32"" ssid = ""5"">Either there is a failure to distinguish between these two structures  because the network fails to keep track of the fact that John is subject in one and object in the other  or there is a failure to recognize that both structures involve the same participants  because John as a subject has a distinct representation from John as an object.</S>",['Method_Citation']
12,P08-1028,P13-2083,0,"Mitchell and Lapata, 2008",0,"As our final set of baselines, we extend two simple techniques proposed by (Mitchell and Lapata, 2008) that use element-wise addition and multiplication operators to perform composition","As our final set of baselines, we extend two simple techniques proposed by (Mitchell and Lapata, 2008) that use element-wise addition and multiplication operators to perform composition","['51', '108', '75', '190', '64']","<S sid =""51"" ssid = ""24"">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).</S><S sid =""108"" ssid = ""21"">Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).</S><S sid =""75"" ssid = ""23"">An extreme form of this differential in the contribution of constituents is where one of the vectors  say u  contributes nothing at all to the combination: Admittedly the model in (8) is impoverished and rather simplistic  however it can serve as a simple baseline against which to compare more sophisticated models.</S><S sid =""190"" ssid = ""2"">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S><S sid =""64"" ssid = ""12"">Now  if we assume that p lies in the same space as u and v  avoiding the issues of dimensionality associated with tensor products  and that f is a linear function  for simplicity  of the cartesian product of u and v  then we generate a class of additive models: where A and B are matrices which determine the contributions made by u and v to the product p. In contrast  if we assume that f is a linear function of the tensor product of u and v  then we obtain multiplicative models: where C is a tensor of rank 3  which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.</S>",['Method_Citation']
13,P08-1028,P10-1021,0,2008,0,"Althoughthis model has been shown to successfully simulate single and multiple-word priming (McDonald and Brew 2004), it failed to predict processing costs in the Embra eye-tracking corpus (McDonald and Shillcock 2003) .In this work we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008)","Although this model has been shown to successfully simulate single and multiple-word priming (McDonald and Brew 2004), it failed to predict processing costs in the Embra eye-tracking corpus (McDonald and Shillcock 2003). In this work we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008)","['68', '75', '58', '36', '200']","<S sid =""68"" ssid = ""16"">Although the composition model in (5) is commonly used in the literature  from a linguistic perspective  the model in (6) is more appealing.</S><S sid =""75"" ssid = ""23"">An extreme form of this differential in the contribution of constituents is where one of the vectors  say u  contributes nothing at all to the combination: Admittedly the model in (8) is impoverished and rather simplistic  however it can serve as a simple baseline against which to compare more sophisticated models.</S><S sid =""58"" ssid = ""6"">We define a general class of models for this process of composition as: The expression above allows us to derive models for which p is constructed in a distinct space from u and v  as is the case for tensor products.</S><S sid =""36"" ssid = ""9"">To overcome this problem  other techniques have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components.</S><S sid =""200"" ssid = ""12"">The applications of the framework discussed here are many and varied both for cognitive science and NLP.</S>",['Implication_Citation']
14,P08-1028,P10-1021,0,2008,0,"Assuming that h is a linear function of the Cartesian product of u and v allows to specify additive models which are by far the most common method of vector combination in the literature: hi =ui+ vi (3) Alternatively, we can assume that h is a linear function of the tensor product of u and v, and thus derive models based on multiplication: hi =ui? vi (4) Mitchell and Lapata (2008) show that several additive and multiplicative models can be formulated under this framework, including the well known tensor products (Smolensky 1990) and circular convolution (Plate 1995)","Mitchell and Lapata (2008) show that several additive and multiplicative models can be formulated under this framework, including the well known tensor products (Smolensky 1990) and circular convolution (Plate 1995)","['64', '27', '200', '194', '56']","<S sid =""64"" ssid = ""12"">Now  if we assume that p lies in the same space as u and v  avoiding the issues of dimensionality associated with tensor products  and that f is a linear function  for simplicity  of the cartesian product of u and v  then we generate a class of additive models: where A and B are matrices which determine the contributions made by u and v to the product p. In contrast  if we assume that f is a linear function of the tensor product of u and v  then we obtain multiplicative models: where C is a tensor of rank 3  which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.</S><S sid =""27"" ssid = ""23"">Our results show that the multiplicative models are superior and correlate significantly with behavioral data.</S><S sid =""200"" ssid = ""12"">The applications of the framework discussed here are many and varied both for cognitive science and NLP.</S><S sid =""194"" ssid = ""6"">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid =""56"" ssid = ""4"">Here  the space has only five dimensions  and the matrix cells denote the co-occurrence of the target words (horse and run) with the context words animal  stable  and so on.</S>",['Method_Citation']
15,P08-1028,W11-0115,0,2008,0,"Mitchell and Lapata (2008) introduce a whole family of models of compositionality based on vector addition and point wise-multiplication (and a weighted combination of both), evaluated on a sentence similarity task inspired by Kintsch (2001)","Mitchell and Lapata (2008) introduce a whole family of models of compositionality based on vector addition and point wise-multiplication (and a weighted combination of both), evaluated on a sentence similarity task inspired by Kintsch (2001)","['47', '196', '144', '26', '138']","<S sid =""47"" ssid = ""20"">Kintsch (2001) proposes a variation on the vector addition theme in an attempt to model how the meaning of a predicate (e.g.  run) varies depending on the arguments it operates upon (e.g  the horse ran vs. the color ran).</S><S sid =""196"" ssid = ""8"">Further research is needed to gain a deeper understanding of vector composition  both in terms of modeling a wider range of structures (e.g.  adjectivenoun  noun-noun) and also in terms of exploring the space of models more fully.</S><S sid =""144"" ssid = ""57"">We obtained best results with a model using a context window of five words on either side of the target word  the cosine measure  and 2 000 vector components.</S><S sid =""26"" ssid = ""22"">Specifically  we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.</S><S sid =""138"" ssid = ""51"">Model Parameters Irrespectively of their form  all composition models discussed here are based on a semantic space for representing the meanings of individual words.</S>",['Implication_Citation']
16,P08-1028,W11-0115,0,2008,0,"The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression","The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression","['75', '65', '48', '82', '184']","<S sid =""75"" ssid = ""23"">An extreme form of this differential in the contribution of constituents is where one of the vectors  say u  contributes nothing at all to the combination: Admittedly the model in (8) is impoverished and rather simplistic  however it can serve as a simple baseline against which to compare more sophisticated models.</S><S sid =""65"" ssid = ""13"">So  if we assume that only the ith components of u and v contribute to the ith component of p  that these components are not dependent on i  and that the function is symmetric with regard to the interchange of u and v  we obtain a simpler instantiation of an additive model: Analogously  under the same assumptions  we obtain the following simpler multiplicative model: only the ith components of u and v contribute to the ith component of p. Another class of models can be derived by relaxing this constraint.</S><S sid =""48"" ssid = ""21"">The idea is to add not only the vectors representing the predicate and its argument but also the neighbors associated with both of them.</S><S sid =""82"" ssid = ""30"">In contrast to the simple additive model  this extended model is sensitive to syntactic structure  since n is chosen from among the neighbors of the predicate  distinguishing it from the argument.</S><S sid =""184"" ssid = ""18"">Given that the basis of Kintsch’s model is the summation of the verb  a neighbor close to the verb and the noun  it is not surprising that it produces results similar to a summation which weights the verb more heavily than the noun.</S>",['Method_Citation']
17,P08-1028,W11-0115,0,2008,0,"For example, Mitchell and Lapata (2008) use their models to approximate the human ratings in their sentence similarity dataset","For example, Mitchell and Lapata (2008) use their models to approximate the human ratings in their sentence similarity dataset","['162', '21', '26', '191', '154']","<S sid =""162"" ssid = ""75"">First  we used the models to estimate the cosine similarity between the reference sentence and its landmarks.</S><S sid =""21"" ssid = ""17"">Central in these models is the notion of compositionality — the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.</S><S sid =""26"" ssid = ""22"">Specifically  we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.</S><S sid =""191"" ssid = ""3"">Despite the popularity of additive models  our experimental results showed the superiority of models utilizing multiplicative combinations  at least for the sentence similarity task attempted here.</S><S sid =""154"" ssid = ""67"">For the best performing model the weight for the verb was 80% and for the noun 20%.</S>",['Method_Citation']
18,P08-1028,W11-1310,0,2008,0,We use other WSM settings following Mitchell and Lapata (2008),We use other WSM settings following Mitchell and Lapata (2008),"['56', '164', '116', '51', '200']","<S sid =""56"" ssid = ""4"">Here  the space has only five dimensions  and the matrix cells denote the co-occurrence of the target words (horse and run) with the context words animal  stable  and so on.</S><S sid =""164"" ssid = ""77"">A more scrupulous evaluation requires directly correlating all the individual participants’ similarity judgments with those of the models.6 We used Spearman’s p for our correlation analyses.</S><S sid =""116"" ssid = ""29"">We used Fisher’s exact test to determine which verbs and nouns showed the greatest variation in landmark preference and items with p-values greater than 0.001 were discarded.</S><S sid =""51"" ssid = ""24"">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).</S><S sid =""200"" ssid = ""12"">The applications of the framework discussed here are many and varied both for cognitive science and NLP.</S>",['Implication_Citation']
19,P08-1028,W11-1310,0,2008,0,Mitchell and Lapata (2008) observed that a simple multiplication function modelled compositionality better than addition,Mitchell and Lapata (2008) observed that a simple multiplication function modelled compositionality better than addition,"['190', '168', '51', '93', '104']","<S sid =""190"" ssid = ""2"">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S><S sid =""168"" ssid = ""2"">These included three additive models  i.e.  simple addition (equation (5)  Add)  weighted addition (equation (7)  WeightAdd)  and Kintsch’s (2001) model (equation (10)  Kintsch)  a multiplicative model (equation (6)  Multiply)  and also a model which combines multiplication with addition (equation (11)  Combined).</S><S sid =""51"" ssid = ""24"">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).</S><S sid =""93"" ssid = ""6"">The idea here is that an appropriate composition model when applied to horse and ran will yield a vector closer to the landmark gallop than dissolve.</S><S sid =""104"" ssid = ""17"">Verbs and nouns that were attested less than fifty times in the BNC were removed as they would result in unreliable vectors.</S>",['Method_Citation']
20,P08-1028,W11-1310,0,"Mitchell and Lapata, 2008",0,"We use the compositionality functions, simple addition and simple multiplication to build compositional vectors Vwr1+wr2 and Vwr1 ?wr2. These are as described in (Mitchell and Lapata, 2008)","We use the compositionality functions, simple addition and simple multiplication to build compositional vectors Vwr1+wr2 and Vwr1 ?wr2. These are as described in (Mitchell and Lapata, 2008)","['19', '58', '2', '116', '200']","<S sid =""19"" ssid = ""15"">While vector addition has been effective in some applications such as essay grading (Landauer and Dumais  1997) and coherence assessment (Foltz et al.  1998)  there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al.  1991; West and Stanovich  1986) and modulate cognitive behavior in sentence priming (Till et al.  1988) and inference tasks (Heit and Rubinstein  1994).</S><S sid =""58"" ssid = ""6"">We define a general class of models for this process of composition as: The expression above allows us to derive models for which p is constructed in a distinct space from u and v  as is the case for tensor products.</S><S sid =""2"" ssid = ""2"">Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions.</S><S sid =""116"" ssid = ""29"">We used Fisher’s exact test to determine which verbs and nouns showed the greatest variation in landmark preference and items with p-values greater than 0.001 were discarded.</S><S sid =""200"" ssid = ""12"">The applications of the framework discussed here are many and varied both for cognitive science and NLP.</S>",['Aim_Citation']
