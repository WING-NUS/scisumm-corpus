Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,W99-0613,N01-1023,0,"Collins and Singer, 1999",0,"Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named entity identification (Collins and Singer, 1999)","Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named entity identification (Collins and Singer, 1999)","['140', '203', '137', '123', '31']","<S sid =""140"" ssid = ""7"">AdaBoost was first introduced in (Freund and Schapire 97); (Schapire and Singer 98) gave a generalization of AdaBoost which we will use in this paper.</S><S sid =""203"" ssid = ""70"">Several extensions of AdaBoost for multiclass problems have been suggested (Freund and Schapire 97; Schapire and Singer 98).</S><S sid =""137"" ssid = ""4"">The new algorithm  which we call CoBoost  uses labeled and unlabeled data and builds two classifiers in parallel.</S><S sid =""123"" ssid = ""56"">Limitations of (Blum and Mitchell 98): While the assumptions of (Blum and Mitchell 98) are useful in developing both theoretical results and an intuition for the problem  the assumptions are quite limited.</S><S sid =""31"" ssid = ""25"">Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).</S>",['Method_Citation']
2,W99-0613,N01-1023,0,"Collins and Singer, 1999",0,"They also discuss an application of classifying web pages by using their method of mutually constrained models. (Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms toAdaBoost which force the classifiers to agree (called Co Boosting)","(Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms to AdaBoost which force the classifiers to agree (called Co Boosting)","['201', '182', '3', '247', '125']","<S sid =""201"" ssid = ""68"">This allow the learners to &quot;bootstrap&quot; each other by filling the labels of the instances on which the other side has abstained so far.</S><S sid =""182"" ssid = ""49"">Denote the unthresholded classifiers after t — 1 rounds by git—1 and assume that it is the turn for the first classifier to be updated while the second one is kept fixed.</S><S sid =""3"" ssid = ""3"">The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.</S><S sid =""247"" ssid = ""14"">N  portion of examples on which both classifiers give a label rather than abstaining)  and the proportion of these examples on which the two classifiers agree.</S><S sid =""125"" ssid = ""58"">It may be more realistic to replace the second criteria with a softer one  for example (Blum and Mitchell 98) suggest the alternative Alternatively  if Ii and 12 are probabilistic learners  it might make sense to encode the second constraint as one of minimizing some measure of the distance between the distributions given by the two learners.</S>",['Method_Citation']
3,W99-0613,W03-1509,0,Collins and Singer 1999,0,"Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick, et al 1999] and so on","Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick, et al 1999] and so on","['192', '57', '187', '220', '213']","<S sid =""192"" ssid = ""59"">On each step CoBoost searches for a feature and a weight so as to minimize either 40 or 40.</S><S sid =""57"" ssid = ""11"">From here on we will refer to the named-entity string itself as the spelling of the entity  and the contextual predicate as the context.</S><S sid =""187"" ssid = ""54"">Using the virtual distribution Di (i) and pseudo-labels&quot;y. „ values for Wo  W± and W_ can be calculated for each possible weak hypothesis (i.e.  for each feature x E Xi); the weak hypothesis with minimal value for Wo + 2/WW _ can be chosen as before; and the weight for this weak hypothesis at = ln ww+411:) can be calculated.</S><S sid =""220"" ssid = ""87"">(7)  such as the likelihood function used in maximum-entropy problems and other generalized additive models (Lafferty 99).</S><S sid =""213"" ssid = ""80"">Thus at each iteration the algorithm is forced to pick features for the location  person and organization in turn for the classifier being trained.</S>",['Method_Citation']
4,W99-0613,C02-1154,0,"Collins and Singer, 1999",0,"DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syn tactically analyzed corpus","DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syntactically analyzed corpus","['40', '254', '41', '14', '145']","<S sid =""40"" ssid = ""34"">(Berland and Charniak 99) describe a method for extracting parts of objects from wholes (e.g.  &quot;speedometer&quot; from &quot;car&quot;) from a large corpus using hand-crafted patterns.</S><S sid =""254"" ssid = ""5"">Future work should also extend the approach to build a complete named entity extractor - a method that pulls proper names from text and then classifies them.</S><S sid =""41"" ssid = ""35"">(Hearst 92) describes a method for extracting hyponyms from a corpus (pairs of words in &quot;isa&quot; relations).</S><S sid =""14"" ssid = ""8"">A contextual rule considers words surrounding the string in the sentence in which it appears (e.g.  a rule that any proper name modified by an appositive whose head is president is a person).</S><S sid =""145"" ssid = ""12"">AdaBoost is given access to a weak learning algorithm  which accepts as input the training examples  along with a distribution over the instances.</S>",['Implication_Citation']
5,W99-0613,C02-1154,0,"Collins and Singer, 1999",0,"(Collins and Singer, 1999) also makes use of competing categories (person, organization, and location), which cover 96% of all the instances itset out to classify","(Collins and Singer, 1999) also makes use of competing categories (person, organization, and location), which cover 96% of all the instances it set out to classify","['3', '247', '167', '108', '201']","<S sid =""3"" ssid = ""3"">The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.</S><S sid =""247"" ssid = ""14"">N  portion of examples on which both classifiers give a label rather than abstaining)  and the proportion of these examples on which the two classifiers agree.</S><S sid =""167"" ssid = ""34"">We make the assumption that for each example  both xi . and x2 2 alone are sufficient to determine the label yi.</S><S sid =""108"" ssid = ""41"">In the cotraining case  (Blum and Mitchell 98) argue that the task should be to induce functions Ii and f2 such that So Ii and 12 must (1) correctly classify the labeled examples  and (2) must agree with each other on the unlabeled examples.</S><S sid =""201"" ssid = ""68"">This allow the learners to &quot;bootstrap&quot; each other by filling the labels of the instances on which the other side has abstained so far.</S>",['Method_Citation']
6,W99-0613,W06-2204,0,"Collins and Singer, 1999",0,"In (Collins and Singer, 1999) Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification","In (Collins and Singer, 1999) Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification","['18', '108', '2', '250', '153']","<S sid =""18"" ssid = ""12"">But we will show that the use of unlabeled data can drastically reduce the need for supervision.</S><S sid =""108"" ssid = ""41"">In the cotraining case  (Blum and Mitchell 98) argue that the task should be to induce functions Ii and f2 such that So Ii and 12 must (1) correctly classify the labeled examples  and (2) must agree with each other on the unlabeled examples.</S><S sid =""2"" ssid = ""2"">A large number of rules is needed for coverage of the domain  suggesting that a fairly large number of labeled examples should be required to train a classi- However  we show that the use of data can reduce the requirements for supervision to just 7 simple &quot;seed&quot; rules.</S><S sid =""250"" ssid = ""1"">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S><S sid =""153"" ssid = ""20"">Schapire and Singer show that the training error is bounded above by Thus  in order to greedily minimize an upper bound on training error  on each iteration we should search for the weak hypothesis ht and the weight at that minimize Z.</S>",['Method_Citation']
8,W99-0613,W03-1022,0,"Collins and Singer, 1999",0,"Collins and Singer (1999) for example report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999)","Collins and Singer (1999) for example report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999)","['247', '3', '129', '201', '167']","<S sid =""247"" ssid = ""14"">N  portion of examples on which both classifiers give a label rather than abstaining)  and the proportion of these examples on which the two classifiers agree.</S><S sid =""3"" ssid = ""3"">The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.</S><S sid =""129"" ssid = ""62"">Inspection of the data shows that at n = 2500  the two classifiers both give labels on 44 281 (49.2%) of the unlabeled examples  and give the same label on 99.25% of these cases.</S><S sid =""201"" ssid = ""68"">This allow the learners to &quot;bootstrap&quot; each other by filling the labels of the instances on which the other side has abstained so far.</S><S sid =""167"" ssid = ""34"">We make the assumption that for each example  both xi . and x2 2 alone are sufficient to determine the label yi.</S>",['Method_Citation']
9,W99-0613,E09-1018,0,"Collinsand Singer, 1999",0,"While EM has worked quite well for a few tasks, notably ma chine translations (starting with the IBM models 1-5 (Brown et al, 1993), it has not had success inmost others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collinsand Singer, 1999) and context-free-grammar induction (numerous attempts, too many to mention)","While EM has worked quite well for a few tasks, notably machine translations (starting with the IBM models 1-5 (Brown et al, 1993), it has not had success in most others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collinsand Singer, 1999) and context-free-grammar induction (numerous attempts, too many to mention)","['232', '3', '29', '233', '125']","<S sid =""232"" ssid = ""11"">For example  the independence assumptions mean that the model fails to capture the dependence between specific and more general features (for example the fact that the feature full.-string=New_York is always seen with the features contains (New) and The baseline method tags all entities as the most frequent class type (organization). contains (York) and is never seen with a feature such as contains (Group) ).</S><S sid =""3"" ssid = ""3"">The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.</S><S sid =""29"" ssid = ""23"">Unfortunately  Yarowsky's method is not well understood from a theoretical viewpoint: we would like to formalize the notion of redundancy in unlabeled data  and set up the learning task as optimization of some appropriate objective function.</S><S sid =""233"" ssid = ""12"">Unfortunately  modifying the model to account for these kind of dependencies is not at all straightforward.</S><S sid =""125"" ssid = ""58"">It may be more realistic to replace the second criteria with a softer one  for example (Blum and Mitchell 98) suggest the alternative Alternatively  if Ii and 12 are probabilistic learners  it might make sense to encode the second constraint as one of minimizing some measure of the distance between the distributions given by the two learners.</S>",['Method_Citation']
11,W99-0613,W07-1712,0,"Collins and Singer, 1999",0,"In addition, we would also like to explore the semi-supervised techniques such as co-training and self-training (Collins and Singer, 1999)","In addition, we would also like to explore the semi-supervised techniques such as co-training and self-training (Collins and Singer, 1999)","['219', '29', '108', '79', '81']","<S sid =""219"" ssid = ""86"">Finally  we would like to note that it is possible to devise similar algorithms based with other objective functions than the one given in Equ.</S><S sid =""29"" ssid = ""23"">Unfortunately  Yarowsky's method is not well understood from a theoretical viewpoint: we would like to formalize the notion of redundancy in unlabeled data  and set up the learning task as optimization of some appropriate objective function.</S><S sid =""108"" ssid = ""41"">In the cotraining case  (Blum and Mitchell 98) argue that the task should be to induce functions Ii and f2 such that So Ii and 12 must (1) correctly classify the labeled examples  and (2) must agree with each other on the unlabeled examples.</S><S sid =""79"" ssid = ""12"">2 We now introduce a new algorithm for learning from unlabeled examples  which we will call DLCoTrain (DL stands for decision list  the term Cotrain is taken from (Blum and Mitchell 98)).</S><S sid =""81"" ssid = ""14"">It's not clear how to apply these methods in the unsupervised case  as they required cross-validation techniques: for this reason we use the simpler smoothing method shown here. input to the unsupervised algorithm is an initial  &quot;seed&quot; set of rules.</S>",['Method_Citation']
12,W99-0613,W09-2208,0,"Collins and Singer, 1999",0,"Collins et al (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky ?smethod (Yarowsky, 1995) and the framework suggested by (Blum and Mitchell, 1998)","Collins et al (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky's method (Yarowsky, 1995) and the framework suggested by (Blum and Mitchell, 1998)","['6', '31', '123', '91', '27']","<S sid =""6"" ssid = ""6"">The second algorithm extends ideas from boosting algorithms  designed for supervised learning tasks  to the framework suggested by (Blum and Mitchell 98).</S><S sid =""31"" ssid = ""25"">Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).</S><S sid =""123"" ssid = ""56"">Limitations of (Blum and Mitchell 98): While the assumptions of (Blum and Mitchell 98) are useful in developing both theoretical results and an intuition for the problem  the assumptions are quite limited.</S><S sid =""91"" ssid = ""24"">There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features  alternating between labeling and learning with the two types of features.</S><S sid =""27"" ssid = ""21"">The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).</S>",['Implication_Citation']
13,W99-0613,W06-2207,0,"Collins and Singer, 1999",0,"This approach was shown to perform well on real-world natural language problems (Collins and Singer, 1999)","This approach was shown to perform well on real-world natural language problems (Collins and Singer, 1999)","['43', '32', '223', '12', '3']","<S sid =""43"" ssid = ""37"">The approach builds from an initial seed set for a category  and is quite similar to the decision list approach described in (Yarowsky 95).</S><S sid =""32"" ssid = ""26"">The algorithm can be viewed as heuristically optimizing an objective function suggested by (Blum and Mitchell 98); empirically it is shown to be quite successful in optimizing this criterion.</S><S sid =""223"" ssid = ""2"">A generative model was applied (similar to naive Bayes) with the three labels as hidden vanables on unlabeled examples  and observed variables on (seed) labeled examples.</S><S sid =""12"" ssid = ""6"">The approach uses both spelling and contextual rules.</S><S sid =""3"" ssid = ""3"">The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.</S>",['Method_Citation']
15,W99-0613,W06-2207,0,"Collins and Singer, 1999",0,"(6) Similarly to (Collins and Singer, 1999) we used T= 0.95 for all experiments reported here","(6) Similarly to (Collins and Singer, 1999) we used T= 0.95 for all experiments reported here","['120', '85', '167', '140', '79']","<S sid =""120"" ssid = ""53"">(Blum and Mitchell 98) go on to give PAC results for learning in the cotraining case.</S><S sid =""85"" ssid = ""18"">(If fewer than n rules have Precision greater than pin  we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events  hence we do not use smoothing  allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm n was fixed at 0.95 in all experiments in this paper.</S><S sid =""167"" ssid = ""34"">We make the assumption that for each example  both xi . and x2 2 alone are sufficient to determine the label yi.</S><S sid =""140"" ssid = ""7"">AdaBoost was first introduced in (Freund and Schapire 97); (Schapire and Singer 98) gave a generalization of AdaBoost which we will use in this paper.</S><S sid =""79"" ssid = ""12"">2 We now introduce a new algorithm for learning from unlabeled examples  which we will call DLCoTrain (DL stands for decision list  the term Cotrain is taken from (Blum and Mitchell 98)).</S>",['Implication_Citation']
16,W99-0613,P12-1065,0,1999,0,We use Collins and Singer (1999) for our exact specification of Yarowsky.2 It uses DL rule scores ?fj?| ?fj |+ |? f |+ L (1) where  is a smoothing constant,We use Collins and Singer (1999) for our exact specification of Yarowsky,"['141', '1', '9', '123', '101']","<S sid =""141"" ssid = ""8"">For a description of the application of AdaBoost to various NLP problems see the paper by Abney  Schapire  and Singer in this volume.</S><S sid =""1"" ssid = ""1"">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid =""9"" ssid = ""3"">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid =""123"" ssid = ""56"">Limitations of (Blum and Mitchell 98): While the assumptions of (Blum and Mitchell 98) are useful in developing both theoretical results and an intuition for the problem  the assumptions are quite limited.</S><S sid =""101"" ssid = ""34"">An important reason for separating the two types of features is that this opens up the possibility of theoretical analysis of the use of unlabeled examples.</S>",['Method_Citation']
