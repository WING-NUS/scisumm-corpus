Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P05-1013,W05-1505,0,"Nivre and Nilsson, 2005",0,"Recent work by Nivre and Nilsson introduces a technique where the projectivization transformation is encoded in the non-terminals of constituents during parsing (Nivre and Nilsson, 2005)","Recent work by Nivre and Nilsson introduces a technique where the projectivization transformation is encoded in the non-terminals of constituents during parsing (Nivre and Nilsson, 2005)","['108', '107', '49', '55', '47']","<S sid =""108"" ssid = ""19"">However  while Dienes and Dubey recognize empty categories in a pre-processing step and only let the parser find their antecedents  we use the parser both to detect dislocated dependents and to predict either the type or the location of their syntactic head (or both) and use post-processing only to transform the graph in accordance with the parser’s analysis.</S><S sid =""107"" ssid = ""18"">Compared to related work on the recovery of long-distance dependencies in constituency-based parsing  our approach is similar to that of Dienes and Dubey (2003) in that the processing of non-local dependencies is partly integrated in the parsing process  via an extension of the set of syntactic categories  whereas most other approaches rely on postprocessing only.</S><S sid =""49"" ssid = ""20"">The baseline simply retains the original labels for all arcs  regardless of whether they have been lifted or not  and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme  called Head  we use a new label d↑h for each lifted arc  where d is the dependency relation between the syntactic head and the dependent in the non-projective representation  and h is the dependency relation that the syntactic head has to its own head in the underlying structure.</S><S sid =""55"" ssid = ""26"">As can be seen from the last column in Table 1  both Head and Head+Path may theoretically lead to a quadratic increase in the number of distinct arc labels (Head+Path being worse than Head only by a constant factor)  while the increase is only linear in the case of Path.</S><S sid =""47"" ssid = ""18"">In practice  we can therefore expect a trade-off such that increasing the amount of information encoded in arc labels will cause an increase in the accuracy of the inverse transformation but a decrease in the accuracy with which the parser can construct the labeled representations.</S>",['Method_Citation']
2,P05-1013,P08-1006,0,"Nivre and Nilsson, 2005",0,"1http: //sourceforge.net/projects/mstparser Figure 1: CoNLL-X dependency tree Figure 2: Penn Treebank-style phrase structure tree KSDEP Sagae and Tsujii (2007)? s dependencyparser,2 based on a probabilistic shift-reduce algorithm extended by the pseudo-projective parsing technique (Nivre and Nilsson, 2005)","Sagae and Tsujii (2007)'s dependency parser, based on a probabilistic shift-reduce algorithm extended by the pseudo-projective parsing technique (Nivre and Nilsson, 2005)","['109', '72', '108', '66', '113']","<S sid =""109"" ssid = ""1"">We have presented a new method for non-projective dependency parsing  based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S sid =""72"" ssid = ""11"">The prediction based on these features is a knearest neighbor classification  using the IB1 algorithm and k = 5  the modified value difference metric (MVDM) and class voting with inverse distance weighting  as implemented in the TiMBL software package (Daelemans et al.  2003).</S><S sid =""108"" ssid = ""19"">However  while Dienes and Dubey recognize empty categories in a pre-processing step and only let the parser find their antecedents  we use the parser both to detect dislocated dependents and to predict either the type or the location of their syntactic head (or both) and use post-processing only to transform the graph in accordance with the parser’s analysis.</S><S sid =""66"" ssid = ""5"">The choice between different actions is in general nondeterministic  and the parser relies on a memorybased classifier  trained on treebank data  to predict the next action based on features of the current parser configuration.</S><S sid =""113"" ssid = ""3"">Special thanks to Jan Hajiˇc and Matthias Trautner Kromann for assistance with the Czech and Danish data  respectively  and to Jan Hajiˇc  Tom´aˇs Holan  Dan Zeman and three anonymous reviewers for valuable comments on a preliminary version of the paper.</S>",['Method_Citation']
3,P05-1013,W10-1401,0,"Nivre and Nilsson, 2005",0,"Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that the pseudo-projective transformation of (Nivre and Nilsson, 2005) improves accuracy for dependency parsing of Basque","Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that the pseudo-projective transformation of (Nivre and Nilsson, 2005) improves accuracy for dependency parsing of Basque","['7', '107', '108', '49', '9']","<S sid =""7"" ssid = ""3"">From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S><S sid =""107"" ssid = ""18"">Compared to related work on the recovery of long-distance dependencies in constituency-based parsing  our approach is similar to that of Dienes and Dubey (2003) in that the processing of non-local dependencies is partly integrated in the parsing process  via an extension of the set of syntactic categories  whereas most other approaches rely on postprocessing only.</S><S sid =""108"" ssid = ""19"">However  while Dienes and Dubey recognize empty categories in a pre-processing step and only let the parser find their antecedents  we use the parser both to detect dislocated dependents and to predict either the type or the location of their syntactic head (or both) and use post-processing only to transform the graph in accordance with the parser’s analysis.</S><S sid =""49"" ssid = ""20"">The baseline simply retains the original labels for all arcs  regardless of whether they have been lifted or not  and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme  called Head  we use a new label d↑h for each lifted arc  where d is the dependency relation between the syntactic head and the dependent in the non-projective representation  and h is the dependency relation that the syntactic head has to its own head in the underlying structure.</S><S sid =""9"" ssid = ""5"">This is true of the widely used link grammar parser for English (Sleator and Temperley  1993)  which uses a dependency grammar of sorts  the probabilistic dependency parser of Eisner (1996)  and more recently proposed deterministic dependency parsers (Yamada and Matsumoto  2003; Nivre et al.  2004).</S>",['Method_Citation']
4,P05-1013,P12-3029,0,"Nivre and Nilsson, 2005",0,"For tree banks with non-projective trees weuse the pseudo-projective parsing technique to trans form the tree bank into projective structures (Nivre and Nilsson, 2005)","For tree banks with non-projective trees we use the pseudo-projective parsing technique to transform the tree bank into projective structures (Nivre and Nilsson, 2005)","['108', '61', '110', '90', '40']","<S sid =""108"" ssid = ""19"">However  while Dienes and Dubey recognize empty categories in a pre-processing step and only let the parser find their antecedents  we use the parser both to detect dislocated dependents and to predict either the type or the location of their syntactic head (or both) and use post-processing only to transform the graph in accordance with the parser’s analysis.</S><S sid =""61"" ssid = ""32"">Before we turn to the evaluation  however  we need to introduce the data-driven dependency parser used in the latter experiments.</S><S sid =""110"" ssid = ""2"">The main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy  especially with respect to the exact match criterion  leading to the best reported performance for robust non-projective parsing of Czech.</S><S sid =""90"" ssid = ""1"">The second experiment is limited to data from PDT.5 The training part of the treebank was projectivized under different encoding schemes and used to train memory-based dependency parsers  which were run on the test part of the treebank  consisting of 7 507 sentences and 125 713 tokens.6 The inverse transformation was applied to the output of the parsers and the result compared to the gold standard test set.</S><S sid =""40"" ssid = ""11"">Even this may be nondeterministic  in case the graph contains several non-projective arcs whose lifts interact  but we use the following algorithm to construct a minimal projective transformation D0 = (W  A0) of a (nonprojective) dependency graph D = (W  A): The function SMALLEST-NONP-ARC returns the non-projective arc with the shortest distance from head to dependent (breaking ties from left to right).</S>",['Implication_Citation']
5,P05-1013,W10-1403,0,"Nivre and Nilsson, 2005",0,"It uses graph transformation to handle non-projective trees (Nivre and Nilsson, 2005)","It uses graph transformation to handle non-projective trees (Nivre and Nilsson, 2005)","['2', '40', '108', '109', '60']","<S sid =""2"" ssid = ""2"">We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.</S><S sid =""40"" ssid = ""11"">Even this may be nondeterministic  in case the graph contains several non-projective arcs whose lifts interact  but we use the following algorithm to construct a minimal projective transformation D0 = (W  A0) of a (nonprojective) dependency graph D = (W  A): The function SMALLEST-NONP-ARC returns the non-projective arc with the shortest distance from head to dependent (breaking ties from left to right).</S><S sid =""108"" ssid = ""19"">However  while Dienes and Dubey recognize empty categories in a pre-processing step and only let the parser find their antecedents  we use the parser both to detect dislocated dependents and to predict either the type or the location of their syntactic head (or both) and use post-processing only to transform the graph in accordance with the parser’s analysis.</S><S sid =""109"" ssid = ""1"">We have presented a new method for non-projective dependency parsing  based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S sid =""60"" ssid = ""31"">In section 4 we evaluate these transformations with respect to projectivized dependency treebanks  and in section 5 they are applied to parser output.</S>",['Method_Citation']
6,P05-1013,D08-1008,0,"Nivre and Nilsson, 2005",0,"To simplify implementation, we instead opted for the pseudo-projective approach (Nivre and Nilsson,2005), in which non projective links are lifted upwards in the tree to achieve projectivity, and special trace labels are used to enable recovery of the non projective links at parse time","To simplify implementation, we instead opted for the pseudo-projective approach (Nivre and Nilsson, 2005), in which non projective links are lifted upwards in the tree to achieve projectivity, and special trace labels are used to enable recovery of the non projective links at parse time","['108', '49', '90', '88', '55']","<S sid =""108"" ssid = ""19"">However  while Dienes and Dubey recognize empty categories in a pre-processing step and only let the parser find their antecedents  we use the parser both to detect dislocated dependents and to predict either the type or the location of their syntactic head (or both) and use post-processing only to transform the graph in accordance with the parser’s analysis.</S><S sid =""49"" ssid = ""20"">The baseline simply retains the original labels for all arcs  regardless of whether they have been lifted or not  and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme  called Head  we use a new label d↑h for each lifted arc  where d is the dependency relation between the syntactic head and the dependent in the non-projective representation  and h is the dependency relation that the syntactic head has to its own head in the underlying structure.</S><S sid =""90"" ssid = ""1"">The second experiment is limited to data from PDT.5 The training part of the treebank was projectivized under different encoding schemes and used to train memory-based dependency parsers  which were run on the test part of the treebank  consisting of 7 507 sentences and 125 713 tokens.6 The inverse transformation was applied to the output of the parsers and the result compared to the gold standard test set.</S><S sid =""88"" ssid = ""15"">We also see that the increase in the size of the label sets for Head and Head+Path is far below the theoretical upper bounds given in Table 1.</S><S sid =""55"" ssid = ""26"">As can be seen from the last column in Table 1  both Head and Head+Path may theoretically lead to a quadratic increase in the number of distinct arc labels (Head+Path being worse than Head only by a constant factor)  while the increase is only linear in the case of Path.</S>",['Method_Citation']
7,P05-1013,D07-1013,0,2005,0,",wn in O (n) time, producing a projective dependency graph satisfying conditions 1? 4 in section 2.1, possibly after adding arcs (0, i ,lr) for every node i 6= 0 that is a root in the output graph (where lr is a special label for root modifiers) .Nivre and Nilsson (2005) showed how the restriction to projective dependency graphs could be lifted by using graph transformation techniques to pre process training data and post-process parser output, so-called pseudo-projective parsing. To learn transition scores, these systems use discriminative learning methods ,e.g., memory-based learning or support vector machines","Nivre and Nilsson (2005) showed how the restriction to projective dependency graphs could be lifted by using graph transformation techniques to preprocess training data and post-process parser output, so-called pseudo-projective parsing","['2', '20', '108', '63', '26']","<S sid =""2"" ssid = ""2"">We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.</S><S sid =""20"" ssid = ""16"">In this paper  we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S><S sid =""108"" ssid = ""19"">However  while Dienes and Dubey recognize empty categories in a pre-processing step and only let the parser find their antecedents  we use the parser both to detect dislocated dependents and to predict either the type or the location of their syntactic head (or both) and use post-processing only to transform the graph in accordance with the parser’s analysis.</S><S sid =""63"" ssid = ""2"">The parser builds dependency graphs by traversing the input from left to right  using a stack to store tokens that are not yet complete with respect to their dependents.</S><S sid =""26"" ssid = ""22"">In section 2 we introduce the graph transformation techniques used to projectivize and deprojectivize dependency graphs  and in section 3 we describe the data-driven dependency parser that is the core of our system.</S>",['Method_Citation']
8,P05-1013,D07-1119,0,2005,0,"For handling non-projective relations, Nivre and Nilsson (2005) suggested applying a preprocessing step to a dependency parser, which consists in lifting non-projective arcs to their head repeatedly, until the tree becomes pseudo-projective","For handling non-projective relations, Nivre and Nilsson (2005) suggested applying a preprocessing step to a dependency parser, which consists in lifting non-projective arcs to their head repeatedly, until the tree becomes pseudo-projective","['37', '40', '23', '108', '110']","<S sid =""37"" ssid = ""8"">Here we use a slightly different notion of lift  applying to individual arcs and moving their head upwards one step at a time: Intuitively  lifting an arc makes the word wk dependent on the head wi of its original head wj (which is unique in a well-formed dependency graph)  unless wj is a root in which case the operation is undefined (but then wj —* wk is necessarily projective if the dependency graph is well-formed).</S><S sid =""40"" ssid = ""11"">Even this may be nondeterministic  in case the graph contains several non-projective arcs whose lifts interact  but we use the following algorithm to construct a minimal projective transformation D0 = (W  A0) of a (nonprojective) dependency graph D = (W  A): The function SMALLEST-NONP-ARC returns the non-projective arc with the shortest distance from head to dependent (breaking ties from left to right).</S><S sid =""23"" ssid = ""19"">By applying an inverse transformation to the output of the parser  arcs with non-standard labels can be lowered to their proper place in the dependency graph  giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.</S><S sid =""108"" ssid = ""19"">However  while Dienes and Dubey recognize empty categories in a pre-processing step and only let the parser find their antecedents  we use the parser both to detect dislocated dependents and to predict either the type or the location of their syntactic head (or both) and use post-processing only to transform the graph in accordance with the parser’s analysis.</S><S sid =""110"" ssid = ""2"">The main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy  especially with respect to the exact match criterion  leading to the best reported performance for robust non-projective parsing of Czech.</S>",['Method_Citation']
9,P05-1013,N07-1050,0,"Nivre and Nilsson, 2005",0,"Whereas most of the early approaches were limited to strictly projective dependency structures, where the projection of a syntactic head must be continuous, attention has recently shifted to the analysis of non-projective structures, which are required for linguistically adequate representations, especially in languages with free or flexible word order. The most popular strategy for capturing non projective structures in data-driven dependency parsing is to apply some kind of post-processing to the output of a strictly projective dependency parser, as in pseudo-projective parsing (Nivre and Nilsson, 2005), corrective modeling (Hall and Nova? k, 2005), or approximate non-projective parsing (McDonald and Pereira, 2006)","The most popular strategy for capturing non projective structures in data-driven dependency parsing is to apply some kind of post-processing to the output of a strictly projective dependency parser, as in pseudo-projective parsing (Nivre and Nilsson, 2005), corrective modeling (Hall and Nova? k, 2005), or approximate non-projective parsing (McDonald and Pereira, 2006)","['108', '107', '109', '9', '110']","<S sid =""108"" ssid = ""19"">However  while Dienes and Dubey recognize empty categories in a pre-processing step and only let the parser find their antecedents  we use the parser both to detect dislocated dependents and to predict either the type or the location of their syntactic head (or both) and use post-processing only to transform the graph in accordance with the parser’s analysis.</S><S sid =""107"" ssid = ""18"">Compared to related work on the recovery of long-distance dependencies in constituency-based parsing  our approach is similar to that of Dienes and Dubey (2003) in that the processing of non-local dependencies is partly integrated in the parsing process  via an extension of the set of syntactic categories  whereas most other approaches rely on postprocessing only.</S><S sid =""109"" ssid = ""1"">We have presented a new method for non-projective dependency parsing  based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S sid =""9"" ssid = ""5"">This is true of the widely used link grammar parser for English (Sleator and Temperley  1993)  which uses a dependency grammar of sorts  the probabilistic dependency parser of Eisner (1996)  and more recently proposed deterministic dependency parsers (Yamada and Matsumoto  2003; Nivre et al.  2004).</S><S sid =""110"" ssid = ""2"">The main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy  especially with respect to the exact match criterion  leading to the best reported performance for robust non-projective parsing of Czech.</S>",['Method_Citation']
10,P05-1013,W09-1207,0,"Nivre and Nilsson, 2005",0,"troduced in (Nivre and Nilsson, 2005) to handle the non-projective languages including Czech, German and English","We adopt the pseudo-projective approach introduced in (Nivre and Nilsson, 2005) to handle the non-projective languages including Czech, German and English","['108', '113', '28', '90', '88']","<S sid =""108"" ssid = ""19"">However  while Dienes and Dubey recognize empty categories in a pre-processing step and only let the parser find their antecedents  we use the parser both to detect dislocated dependents and to predict either the type or the location of their syntactic head (or both) and use post-processing only to transform the graph in accordance with the parser’s analysis.</S><S sid =""113"" ssid = ""3"">Special thanks to Jan Hajiˇc and Matthias Trautner Kromann for assistance with the Czech and Danish data  respectively  and to Jan Hajiˇc  Tom´aˇs Holan  Dan Zeman and three anonymous reviewers for valuable comments on a preliminary version of the paper.</S><S sid =""28"" ssid = ""24"">First  in section 4  we evaluate the graph transformation techniques in themselves  with data from the Prague Dependency Treebank and the Danish Dependency Treebank.</S><S sid =""90"" ssid = ""1"">The second experiment is limited to data from PDT.5 The training part of the treebank was projectivized under different encoding schemes and used to train memory-based dependency parsers  which were run on the test part of the treebank  consisting of 7 507 sentences and 125 713 tokens.6 The inverse transformation was applied to the output of the parsers and the result compared to the gold standard test set.</S><S sid =""88"" ssid = ""15"">We also see that the increase in the size of the label sets for Head and Head+Path is far below the theoretical upper bounds given in Table 1.</S>",['Implication_Citation']
11,P05-1013,E09-1034,0,"Nivre and Nilsson, 2005",0,"non projective (Nivre and Nilsson, 2005), we char ac terise a sense in which the structures appearing in tree banks can be viewed as being only? slightly? ill-nested","However, just as it has been noted that most non-projective structures appearing in practice are only 'slightly' non projective (Nivre and Nilsson, 2005), we characterise a sense in which the structures appearing in tree banks can be viewed as being only 'slightly' ill-nested","['83', '55', '99', '94', '22']","<S sid =""83"" ssid = ""10"">It is worth noting that  although nonprojective constructions are less frequent in DDT than in PDT  they seem to be more deeply nested  since only about 80% can be projectivized with a single lift  while almost 95% of the non-projective arcs in PDT only require a single lift.</S><S sid =""55"" ssid = ""26"">As can be seen from the last column in Table 1  both Head and Head+Path may theoretically lead to a quadratic increase in the number of distinct arc labels (Head+Path being worse than Head only by a constant factor)  while the increase is only linear in the case of Path.</S><S sid =""99"" ssid = ""10"">This may seem surprising  given the experiments reported in section 4  but the explanation is probably that the non-projective dependencies that can be recovered at all are of the simple kind that only requires a single lift  where the encoding of path information is often redundant.</S><S sid =""94"" ssid = ""5"">The first thing to note is that projectivizing helps in itself  even if no encoding is used  as seen from the fact that the projective baseline outperforms the non-projective training condition by more than half a percentage point on attachment score  although the gain is much smaller with respect to exact match.</S><S sid =""22"" ssid = ""18"">When the parser is trained on the transformed data  it will ideally learn not only to construct projective dependency structures but also to assign arc labels that encode information about lifts.</S>",['Method_Citation']
12,P05-1013,W09-1218,0,"Nivre and Nilsson, 2005",0,"In order to avoid losing the benefits of higher-order parsing, we considered applying pseudo-projective transformation (Nivre and Nilsson, 2005)","In order to avoid losing the benefits of higher-order parsing, we considered applying pseudo-projective transformation (Nivre and Nilsson, 2005)","['26', '23', '108', '90', '47']","<S sid =""26"" ssid = ""22"">In section 2 we introduce the graph transformation techniques used to projectivize and deprojectivize dependency graphs  and in section 3 we describe the data-driven dependency parser that is the core of our system.</S><S sid =""23"" ssid = ""19"">By applying an inverse transformation to the output of the parser  arcs with non-standard labels can be lowered to their proper place in the dependency graph  giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.</S><S sid =""108"" ssid = ""19"">However  while Dienes and Dubey recognize empty categories in a pre-processing step and only let the parser find their antecedents  we use the parser both to detect dislocated dependents and to predict either the type or the location of their syntactic head (or both) and use post-processing only to transform the graph in accordance with the parser’s analysis.</S><S sid =""90"" ssid = ""1"">The second experiment is limited to data from PDT.5 The training part of the treebank was projectivized under different encoding schemes and used to train memory-based dependency parsers  which were run on the test part of the treebank  consisting of 7 507 sentences and 125 713 tokens.6 The inverse transformation was applied to the output of the parsers and the result compared to the gold standard test set.</S><S sid =""47"" ssid = ""18"">In practice  we can therefore expect a trade-off such that increasing the amount of information encoded in arc labels will cause an increase in the accuracy of the inverse transformation but a decrease in the accuracy with which the parser can construct the labeled representations.</S>",['Implication_Citation']
13,P05-1013,C08-1081,0,"Nivre and Nilsson, 2005",0,"Pseudo-projective parsing for recovering non projective structures (Nivre and Nilsson, 2005)","Pseudo-projective parsing for recovering non projective structures (Nivre and Nilsson, 2005)","['17', '113', '19', '109', '62']","<S sid =""17"" ssid = ""13"">There exist a few robust broad-coverage parsers that produce non-projective dependency structures  notably Tapanainen and J¨arvinen (1997) and Wang and Harper (2004) for English  Foth et al. (2004) for German  and Holan (2004) for Czech.</S><S sid =""113"" ssid = ""3"">Special thanks to Jan Hajiˇc and Matthias Trautner Kromann for assistance with the Czech and Danish data  respectively  and to Jan Hajiˇc  Tom´aˇs Holan  Dan Zeman and three anonymous reviewers for valuable comments on a preliminary version of the paper.</S><S sid =""19"" ssid = ""15"">Finally  since non-projective constructions often involve long-distance dependencies  the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson  2002; Dienes and Dubey  2003; Jijkoun and de Rijke  2004; Cahill et al.  2004; Levy and Manning  2004; Campbell  2004).</S><S sid =""109"" ssid = ""1"">We have presented a new method for non-projective dependency parsing  based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S sid =""62"" ssid = ""1"">In the experiments below  we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs 3 previously tested on Swedish (Nivre et al.  2004) and English (Nivre and Scholz  2004).</S>",['Method_Citation']
14,P05-1013,C08-1081,0,2005,0,"Although the parser only derives projective graphs, the fact that these graphs are labeled allows non-projective dependencies to be captured using the pseudo-projective approach of Nivre and Nilsson (2005) (section 3.4)","Although the parser only derives projective graphs, the fact that these graphs are labeled allows non-projective dependencies to be captured using the pseudo-projective approach of Nivre and Nilsson (2005) (section 3.4)","['99', '49', '103', '107', '92']","<S sid =""99"" ssid = ""10"">This may seem surprising  given the experiments reported in section 4  but the explanation is probably that the non-projective dependencies that can be recovered at all are of the simple kind that only requires a single lift  where the encoding of path information is often redundant.</S><S sid =""49"" ssid = ""20"">The baseline simply retains the original labels for all arcs  regardless of whether they have been lifted or not  and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme  called Head  we use a new label d↑h for each lifted arc  where d is the dependency relation between the syntactic head and the dependent in the non-projective representation  and h is the dependency relation that the syntactic head has to its own head in the underlying structure.</S><S sid =""103"" ssid = ""14"">On the other hand  given that all schemes have similar parsing accuracy overall  this means that the Path scheme is the least likely to introduce errors on projective arcs.</S><S sid =""107"" ssid = ""18"">Compared to related work on the recovery of long-distance dependencies in constituency-based parsing  our approach is similar to that of Dienes and Dubey (2003) in that the processing of non-local dependencies is partly integrated in the parsing process  via an extension of the set of syntactic categories  whereas most other approaches rely on postprocessing only.</S><S sid =""92"" ssid = ""3"">Evaluation metrics used are Attachment Score (AS)  i.e. the proportion of tokens that are attached to the correct head  and Exact Match (EM)  i.e. the proportion of sentences for which the dependency graph exactly matches the gold standard.</S>",['Method_Citation']
15,P05-1013,C08-1081,0,2005,0,Pseudo-projective parsing was proposed by Nivreand Nilsson (2005) as a way of dealing with non projective structures in a projective data-driven parser,Pseudo-projective parsing was proposed by Nivreand Nilsson (2005) as a way of dealing with non projective structures in a projective data-driven parser,"['109', '110', '55', '2', '23']","<S sid =""109"" ssid = ""1"">We have presented a new method for non-projective dependency parsing  based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S sid =""110"" ssid = ""2"">The main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy  especially with respect to the exact match criterion  leading to the best reported performance for robust non-projective parsing of Czech.</S><S sid =""55"" ssid = ""26"">As can be seen from the last column in Table 1  both Head and Head+Path may theoretically lead to a quadratic increase in the number of distinct arc labels (Head+Path being worse than Head only by a constant factor)  while the increase is only linear in the case of Path.</S><S sid =""2"" ssid = ""2"">We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.</S><S sid =""23"" ssid = ""19"">By applying an inverse transformation to the output of the parser  arcs with non-standard labels can be lowered to their proper place in the dependency graph  giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.</S>",['Implication_Citation']
16,P05-1013,C08-1081,0,2005,0,"Weprojectivize training data by a minimal transformation, lifting non-projective arcs one step at a time, and extending the arc label of lifted arcs using the encoding scheme called HEAD by Nivre and Nilsson (2005), which means that a lifted arc is assigned the label r? h, where r is the original label and h is the label of the original head in the non-projective dependency graph","We projectivize training data by a minimal transformation, lifting non-projective arcs one step at a time, and extending the arc label of lifted arcs using the encoding scheme called HEAD by Nivre and Nilsson (2005), which means that a lifted arc is assigned the label r? h, where r is the original label and h is the label of the original head in the non-projective dependency graph","['49', '51', '37', '54', '46']","<S sid =""49"" ssid = ""20"">The baseline simply retains the original labels for all arcs  regardless of whether they have been lifted or not  and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme  called Head  we use a new label d↑h for each lifted arc  where d is the dependency relation between the syntactic head and the dependent in the non-projective representation  and h is the dependency relation that the syntactic head has to its own head in the underlying structure.</S><S sid =""51"" ssid = ""22"">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p↓.</S><S sid =""37"" ssid = ""8"">Here we use a slightly different notion of lift  applying to individual arcs and moving their head upwards one step at a time: Intuitively  lifting an arc makes the word wk dependent on the head wi of its original head wj (which is unique in a well-formed dependency graph)  unless wj is a root in which case the operation is undefined (but then wj —* wk is necessarily projective if the dependency graph is well-formed).</S><S sid =""54"" ssid = ""25"">For Experiment 1 it is meaningless as a baseline  since it would result in 0% accuracy. mation on path labels but drop the information about the syntactic head of the lifted arc  using the label d↑ instead of d↑h (AuxP↑ instead of AuxP↑Sb).</S><S sid =""46"" ssid = ""17"">In principle  it would be possible to encode the exact position of the syntactic head in the label of the arc from the linear head  but this would give a potentially infinite set of arc labels and would make the training of the parser very hard.</S>",['Method_Citation']
17,P05-1013,D11-1006,0,"Nivre and Nilsson, 2005",0,"For tree banks with non-projective trees we use the pseudo-projective parsing technique to transform the tree bank into projective structures (Nivre and Nilsson, 2005)","For tree banks with non-projective trees we use the pseudo-projective parsing technique to transform the tree bank into projective structures (Nivre and Nilsson, 2005)","['108', '61', '110', '90', '40']","<S sid =""108"" ssid = ""19"">However  while Dienes and Dubey recognize empty categories in a pre-processing step and only let the parser find their antecedents  we use the parser both to detect dislocated dependents and to predict either the type or the location of their syntactic head (or both) and use post-processing only to transform the graph in accordance with the parser’s analysis.</S><S sid =""61"" ssid = ""32"">Before we turn to the evaluation  however  we need to introduce the data-driven dependency parser used in the latter experiments.</S><S sid =""110"" ssid = ""2"">The main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy  especially with respect to the exact match criterion  leading to the best reported performance for robust non-projective parsing of Czech.</S><S sid =""90"" ssid = ""1"">The second experiment is limited to data from PDT.5 The training part of the treebank was projectivized under different encoding schemes and used to train memory-based dependency parsers  which were run on the test part of the treebank  consisting of 7 507 sentences and 125 713 tokens.6 The inverse transformation was applied to the output of the parsers and the result compared to the gold standard test set.</S><S sid =""40"" ssid = ""11"">Even this may be nondeterministic  in case the graph contains several non-projective arcs whose lifts interact  but we use the following algorithm to construct a minimal projective transformation D0 = (W  A0) of a (nonprojective) dependency graph D = (W  A): The function SMALLEST-NONP-ARC returns the non-projective arc with the shortest distance from head to dependent (breaking ties from left to right).</S>",['Aim_Citation']
18,P05-1013,P11-2121,0,"Nivre and Nilsson, 2005",0,"Since the number of non-projective dependencies is much smaller than the number of projective dependencies (Nivre and Nilsson, 2005), it is not efficient to perform non-projective parsing for all cases","Since the number of non-projective dependencies is much smaller than the number of projective dependencies (Nivre and Nilsson, 2005), it is not efficient to perform non-projective parsing for all cases","['49', '107', '7', '14', '94']","<S sid =""49"" ssid = ""20"">The baseline simply retains the original labels for all arcs  regardless of whether they have been lifted or not  and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme  called Head  we use a new label d↑h for each lifted arc  where d is the dependency relation between the syntactic head and the dependent in the non-projective representation  and h is the dependency relation that the syntactic head has to its own head in the underlying structure.</S><S sid =""107"" ssid = ""18"">Compared to related work on the recovery of long-distance dependencies in constituency-based parsing  our approach is similar to that of Dienes and Dubey (2003) in that the processing of non-local dependencies is partly integrated in the parsing process  via an extension of the set of syntactic categories  whereas most other approaches rely on postprocessing only.</S><S sid =""7"" ssid = ""3"">From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S><S sid =""14"" ssid = ""10"">While the proportion of sentences containing non-projective dependencies is often 15–25%  the total proportion of non-projective arcs is normally only 1–2%.</S><S sid =""94"" ssid = ""5"">The first thing to note is that projectivizing helps in itself  even if no encoding is used  as seen from the fact that the projective baseline outperforms the non-projective training condition by more than half a percentage point on attachment score  although the gain is much smaller with respect to exact match.</S>",['Aim_Citation']
19,P05-1013,E06-1010,0,"Nivre and Nilsson, 2005",0,"Itshould be noted that the proportion of lost dependencies is about twice as high as the proportion of dependencies that are non-projective in them selves (Nivre and Nilsson, 2005)","It should be noted that the proportion of lost dependencies is about twice as high as the proportion of dependencies that are non-projective in themselves (Nivre and Nilsson, 2005)","['99', '14', '49', '92', '98']","<S sid =""99"" ssid = ""10"">This may seem surprising  given the experiments reported in section 4  but the explanation is probably that the non-projective dependencies that can be recovered at all are of the simple kind that only requires a single lift  where the encoding of path information is often redundant.</S><S sid =""14"" ssid = ""10"">While the proportion of sentences containing non-projective dependencies is often 15–25%  the total proportion of non-projective arcs is normally only 1–2%.</S><S sid =""49"" ssid = ""20"">The baseline simply retains the original labels for all arcs  regardless of whether they have been lifted or not  and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme  called Head  we use a new label d↑h for each lifted arc  where d is the dependency relation between the syntactic head and the dependent in the non-projective representation  and h is the dependency relation that the syntactic head has to its own head in the underlying structure.</S><S sid =""92"" ssid = ""3"">Evaluation metrics used are Attachment Score (AS)  i.e. the proportion of tokens that are attached to the correct head  and Exact Match (EM)  i.e. the proportion of sentences for which the dependency graph exactly matches the gold standard.</S><S sid =""98"" ssid = ""9"">By contrast  when we turn to a comparison of the three encoding schemes it is hard to find any significant differences  and the overall impression is that it makes little or no difference which encoding scheme is used  as long as there is some indication of which words are assigned their linear head instead of their syntactic head by the projective parser.</S>","['Implication_Citation', 'Aim_Citation']"
20,P05-1013,D07-1111,0,"Nivre and Nilsson, 2005",0,"The resulting algorithm is projective, and nonprojectivity is handled by pseudo-projective trans formations as described in (Nivre and Nilsson, 2005)","The resulting algorithm is projective, and nonprojectivity is handled by pseudo-projective transformations as described in (Nivre and Nilsson, 2005)","['78', '72', '98', '55', '89']","<S sid =""78"" ssid = ""5"">The entire treebank is used in the experiment  but only primary dependencies are considered.4 In all experiments  punctuation tokens are included in the data but omitted in evaluation scores.</S><S sid =""72"" ssid = ""11"">The prediction based on these features is a knearest neighbor classification  using the IB1 algorithm and k = 5  the modified value difference metric (MVDM) and class voting with inverse distance weighting  as implemented in the TiMBL software package (Daelemans et al.  2003).</S><S sid =""98"" ssid = ""9"">By contrast  when we turn to a comparison of the three encoding schemes it is hard to find any significant differences  and the overall impression is that it makes little or no difference which encoding scheme is used  as long as there is some indication of which words are assigned their linear head instead of their syntactic head by the projective parser.</S><S sid =""55"" ssid = ""26"">As can be seen from the last column in Table 1  both Head and Head+Path may theoretically lead to a quadratic increase in the number of distinct arc labels (Head+Path being worse than Head only by a constant factor)  while the increase is only linear in the case of Path.</S><S sid =""89"" ssid = ""16"">The increase is generally higher for PDT than for DDT  which indicates a greater diversity in non-projective constructions.</S>",['Results_Citation']
