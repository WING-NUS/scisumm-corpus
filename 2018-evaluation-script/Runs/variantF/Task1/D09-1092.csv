Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,D09-1092,P14-1004,0,"Mimno et al, 2009",0,"This configuration is similar to PolyLDA (Mimno et al, 2009) or LinkLDA (Yano et al, 2009), such that utterances from different parties are treated as different languages or blog-post and comments pairs","This configuration is similar to PolyLDA (Mimno et al, 2009) or LinkLDA (Yano et al, 2009), such that utterances from different parties are treated as different languages or blog-post and comments pairs","['36', '171', '107', '162', '196']","<S sid =""36"" ssid = ""2"">Each tuple is a set of documents that are loosely equivalent to each other  but written in different languages  e.g.  corresponding Wikipedia articles in French  English and German.</S><S sid =""171"" ssid = ""5"">This property is useful for building machine translation systems as well as for human readers who are either learning new languages or analyzing texts in languages they do not know.</S><S sid =""107"" ssid = ""56"">No paper is exactly comparable to any other paper  but they are all roughly topically similar.</S><S sid =""162"" ssid = ""111"">Restricting the query/target pairs to only those with query and target documents that are both longer than 50 words results in significant improvement and reduced variance: the average proportion of query documents for which the true translation is ranked highest goes from 53.9% to 72.7%.</S><S sid =""196"" ssid = ""5"">When applied to comparable document collections such as Wikipedia  PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.</S>",['Method_Citation']
2,D09-1092,P10-1044,0,"Mimno et al, 2009",0,"Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009)","Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009)","['131', '22', '128', '6', '114']","<S sid =""131"" ssid = ""80"">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid =""22"" ssid = ""18"">We use this corpus to explore the ability of the model both to infer similarities between vocabularies in different languages  and to detect differences in topic emphasis between languages.</S><S sid =""128"" ssid = ""77"">Although the PLTM is clearly not a substitute for a machine translation system—it has no way to represent syntax or even multi-word phrases—it is clear from the examples in figure 2 that the sets of high probability words in different languages for a given topic are likely to include translations.</S><S sid =""6"" ssid = ""2"">Topic models have been used for analyzing topic trends in research literature (Mann et al.  2006; Hall et al.  2008)  inferring captions for images (Blei and Jordan  2003)  social network analysis in email (McCallum et al.  2005)  and expanding queries with topically related words in information retrieval (Wei and Croft  2006).</S><S sid =""114"" ssid = ""63"">Ideally  the “glue” documents in g will be sufficient to align the topics across languages  and will cause comparable documents in S to have similar distributions over topics even though they are modeled independently.</S>",['Method_Citation']
3,D09-1092,P11-2084,0,"Mimno et al, 2009",0,"(Mimno et al, 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then add the Cartesian product of these sets for every topic to a set of candidate translations","(Mimno et al, 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then add the Cartesian product of these sets for every topic to a set of candidate translations","['138', '148', '128', '155', '51']","<S sid =""138"" ssid = ""87"">We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.</S><S sid =""148"" ssid = ""97"">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid =""128"" ssid = ""77"">Although the PLTM is clearly not a substitute for a machine translation system—it has no way to represent syntax or even multi-word phrases—it is clear from the examples in figure 2 that the sets of high probability words in different languages for a given topic are likely to include translations.</S><S sid =""155"" ssid = ""104"">Finally  for each pair of languages (“query” and “target”) we calculate the difference between the topic distribution for each held-out document in the query language and the topic distribution for each held-out document in the target language.</S><S sid =""51"" ssid = ""17"">Gibbs sampling involves sequentially resampling each zln from its conditional posterior: where z\l n is the current set of topic assignments for all other tokens in the tuple  while (Nt)\l n is the number of occurrences of topic t in the tuple  excluding zln  the variable being resampled.</S>",['Method_Citation']
4,D09-1092,E12-1014,0,"Mimno et al, 2009",0,"Our Wikipedia-based topic similarity feature, w (f, e), is similar in spirit to polylingualtopic models (Mimno et al 2009), but it is scalable to full bilingual lexicon induction","Our Wikipedia-based topic similarity feature, w (f, e), is similar in spirit to polylingual topic models (Mimno et al 2009), but it is scalable to full bilingual lexicon induction","['108', '105', '151', '193', '13']","<S sid =""108"" ssid = ""57"">If we wish to perform topic-based bibliometric analysis  it is vital to be able to track the same topics across all languages.</S><S sid =""105"" ssid = ""54"">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct  non-comparable documents in multiple languages.</S><S sid =""151"" ssid = ""100"">We are therefore interested in determining whether the information in the document-specific topic distributions is sufficient to identify semantically identical documents.</S><S sid =""193"" ssid = ""2"">We analyzed the characteristics of PLTM in comparison to monolingual LDA  and demonstrated that it is possible to discover aligned topics.</S><S sid =""13"" ssid = ""9"">To perform topic-based bibliometric analysis on these collections  it is necessary to have topic models that are aligned across languages.</S>",['Implication_Citation']
5,D09-1092,D11-1086,0,"Mimno et al, 2009",0,"of English document and the second half of its aligned foreign language document (Mimno et al,2009)","For Europarl data sets, we artificially make them comparable by considering the first half of English document and the second half of its aligned foreign language document (Mimno et al,2009)","['157', '155', '88', '148', '187']","<S sid =""157"" ssid = ""106"">For each document in the query language we rank all documents in the target language and record the rank of the actual translation.</S><S sid =""155"" ssid = ""104"">Finally  for each pair of languages (“query” and “target”) we calculate the difference between the topic distribution for each held-out document in the query language and the topic distribution for each held-out document in the target language.</S><S sid =""88"" ssid = ""37"">The higher the probability of the held-out document tuples  the better the generalization ability of the model.</S><S sid =""148"" ssid = ""97"">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid =""187"" ssid = ""21"">To demonstrate the wide variation in topics  we calculated the proportion of tokens in each language assigned to each topic.</S>",['Method_Citation']
6,D09-1092,N12-1007,0,"Mimno et al, 2009",0,"Since the PLTM is not a contribution of this paper, we refer the interested reader to (Mimno et al, 2009) for more details","Since the PLTM is not a contribution of this paper, we refer the interested reader to (Mimno et al, 2009) for more details","['128', '77', '138', '112', '118']","<S sid =""128"" ssid = ""77"">Although the PLTM is clearly not a substitute for a machine translation system—it has no way to represent syntax or even multi-word phrases—it is clear from the examples in figure 2 that the sets of high probability words in different languages for a given topic are likely to include translations.</S><S sid =""77"" ssid = ""26"">Maximum topic probability in document Although the posterior distribution over topics for each tuple is not concentrated on one topic  it is worth checking that this is not simply because the model is assigning a single topic to the 1We use the R density function. tokens in each of the languages.</S><S sid =""138"" ssid = ""87"">We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.</S><S sid =""112"" ssid = ""61"">To do this  we divide the corpus W into two sets of document tuples: a “glue” set G and a “separate” set S such that |G |/ |W |= p. In other words  the proportion of tuples in the corpus that are treated as “glue” (i.e.  placed in G) is p. For every tuple in S  we assign each document in that tuple to a new singledocument tuple.</S><S sid =""118"" ssid = ""67"">We calculate the Jensen-Shannon divergence between the topic distributions for each pair of individual documents in S that were originally part of the same tuple prior to separation.</S>",['Method_Citation']
7,D09-1092,N12-1007,0,2009,0,"Evaluation Corpus The automatic evaluation of cross-lingual co reference systems requires annotated 10Mimno et al (2009) showed that so long as the proportion of topically-aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly","Mimno et al (2009) showed that so long as the proportion of topically-aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly","['118', '77', '155', '154', '148']","<S sid =""118"" ssid = ""67"">We calculate the Jensen-Shannon divergence between the topic distributions for each pair of individual documents in S that were originally part of the same tuple prior to separation.</S><S sid =""77"" ssid = ""26"">Maximum topic probability in document Although the posterior distribution over topics for each tuple is not concentrated on one topic  it is worth checking that this is not simply because the model is assigning a single topic to the 1We use the R density function. tokens in each of the languages.</S><S sid =""155"" ssid = ""104"">Finally  for each pair of languages (“query” and “target”) we calculate the difference between the topic distribution for each held-out document in the query language and the topic distribution for each held-out document in the target language.</S><S sid =""154"" ssid = ""103"">We then use this model to infer topic distributions for each of the 11 documents in each of the held-out document tuples using a method similar to that used to calculate held-out probabilities (Wallach et al.  2009).</S><S sid =""148"" ssid = ""97"">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S>",['Method_Citation']
8,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"Similarly, Polylingual Topic Models (PLTM) (Mimno et al, 2009) generalized LDA to tuples of documents from multiple languages","Similarly, Polylingual Topic Models (PLTM) (Mimno et al, 2009) generalized LDA to tuples of documents from multiple languages","['105', '148', '22', '0', '154']","<S sid =""105"" ssid = ""54"">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct  non-comparable documents in multiple languages.</S><S sid =""148"" ssid = ""97"">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid =""22"" ssid = ""18"">We use this corpus to explore the ability of the model both to infer similarities between vocabularies in different languages  and to detect differences in topic emphasis between languages.</S><S sid =""0"" ssid = ""0"">Polylingual Topic Models</S><S sid =""154"" ssid = ""103"">We then use this model to infer topic distributions for each of the 11 documents in each of the held-out document tuples using a method similar to that used to calculate held-out probabilities (Wallach et al.  2009).</S>",['Method_Citation']
9,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"Our baseline joint PLSA model (JPLSA) is closely related to the poly-lingual LDA model of (Mimno et al, 2009)","Our baseline joint PLSA model (JPLSA) is closely related to the poly-lingual LDA model of (Mimno et al, 2009)","['129', '154', '32', '98', '77']","<S sid =""129"" ssid = ""78"">We therefore evaluate the ability of the PLTM to generate bilingual lexica  similar to other work in unsupervised translation modeling (Haghighi et al.  2008).</S><S sid =""154"" ssid = ""103"">We then use this model to infer topic distributions for each of the 11 documents in each of the held-out document tuples using a method similar to that used to calculate held-out probabilities (Wallach et al.  2009).</S><S sid =""32"" ssid = ""8"">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid =""98"" ssid = ""47"">As the number of topics is increased  the word counts per topic become very sparse in monolingual LDA models  proportional to the size of the vocabulary.</S><S sid =""77"" ssid = ""26"">Maximum topic probability in document Although the posterior distribution over topics for each tuple is not concentrated on one topic  it is worth checking that this is not simply because the model is assigning a single topic to the 1We use the R density function. tokens in each of the languages.</S>",['Method_Citation']
10,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"We describe the model for two languages, but it is straightforward to generalize to more than two languages, as in (Mimno et al, 2009)","We describe the model for two languages, but it is straightforward to generalize to more than two languages, as in (Mimno et al, 2009)","['162', '22', '28', '154', '193']","<S sid =""162"" ssid = ""111"">Restricting the query/target pairs to only those with query and target documents that are both longer than 50 words results in significant improvement and reduced variance: the average proportion of query documents for which the true translation is ranked highest goes from 53.9% to 72.7%.</S><S sid =""22"" ssid = ""18"">We use this corpus to explore the ability of the model both to infer similarities between vocabularies in different languages  and to detect differences in topic emphasis between languages.</S><S sid =""28"" ssid = ""4"">We take a simpler approach that is more suitable for topically similar document tuples (where documents are not direct translations of one another) in more than two languages.</S><S sid =""154"" ssid = ""103"">We then use this model to infer topic distributions for each of the 11 documents in each of the held-out document tuples using a method similar to that used to calculate held-out probabilities (Wallach et al.  2009).</S><S sid =""193"" ssid = ""2"">We analyzed the characteristics of PLTM in comparison to monolingual LDA  and demonstrated that it is possible to discover aligned topics.</S>",['Implication_Citation']
11,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"j=1 P (z2j|?) P (w 2 j| ?z2j) The difference between the JPLSA model and the poly-lingual topic model of (Mimno et al, 2009) is that we merge the vocabularies in the two languages and learn topic-specific word distributions over these merged vocabularies, instead of having pairs of topic-specific word distributions, one for each language, like in (Mimno et al, 2009)","The difference between the JPLSA model and the poly-lingual topic model of (Mimno et al, 2009) is that we merge the vocabularies in the two languages and learn topic-specific word distributions over these merged vocabularies, instead of having pairs of topic-specific word distributions, one for each language, like in (Mimno et al, 2009)","['155', '118', '157', '51', '138']","<S sid =""155"" ssid = ""104"">Finally  for each pair of languages (“query” and “target”) we calculate the difference between the topic distribution for each held-out document in the query language and the topic distribution for each held-out document in the target language.</S><S sid =""118"" ssid = ""67"">We calculate the Jensen-Shannon divergence between the topic distributions for each pair of individual documents in S that were originally part of the same tuple prior to separation.</S><S sid =""157"" ssid = ""106"">For each document in the query language we rank all documents in the target language and record the rank of the actual translation.</S><S sid =""51"" ssid = ""17"">Gibbs sampling involves sequentially resampling each zln from its conditional posterior: where z\l n is the current set of topic assignments for all other tokens in the tuple  while (Nt)\l n is the number of occurrences of topic t in the tuple  excluding zln  the variable being resampled.</S><S sid =""138"" ssid = ""87"">We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.</S>",['Method_Citation']
12,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"Another difference between our model and the poly-lingual LDA model of (Mimno et al, 2009) is that we use maximum aposteriori (MAP) instead of Bayesian inference","Another difference between our model and the poly-lingual LDA model of (Mimno et al, 2009) is that we use maximum aposteriori (MAP) instead of Bayesian inference","['138', '97', '154', '104', '98']","<S sid =""138"" ssid = ""87"">We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.</S><S sid =""97"" ssid = ""46"">Rather  these results are intended as a quantitative analysis of the difference between the two models.</S><S sid =""154"" ssid = ""103"">We then use this model to infer topic distributions for each of the 11 documents in each of the held-out document tuples using a method similar to that used to calculate held-out probabilities (Wallach et al.  2009).</S><S sid =""104"" ssid = ""53"">This result is important: informally  we have found that increasing the granularity of topics correlates strongly with user perceptions of the utility of a topic model.</S><S sid =""98"" ssid = ""47"">As the number of topics is increased  the word counts per topic become very sparse in monolingual LDA models  proportional to the size of the vocabulary.</S>",['Implication_Citation']
13,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"For computing distance we used the L1-norm of the difference, which worked a bit better than the Jensen Shannon divergence between the topic vectors used in (Mimno et al, 2009)","For computing distance we used the L1-norm of the difference, which worked a bit better than the Jensen Shannon divergence between the topic vectors used in (Mimno et al, 2009)","['118', '88', '155', '138', '51']","<S sid =""118"" ssid = ""67"">We calculate the Jensen-Shannon divergence between the topic distributions for each pair of individual documents in S that were originally part of the same tuple prior to separation.</S><S sid =""88"" ssid = ""37"">The higher the probability of the held-out document tuples  the better the generalization ability of the model.</S><S sid =""155"" ssid = ""104"">Finally  for each pair of languages (“query” and “target”) we calculate the difference between the topic distribution for each held-out document in the query language and the topic distribution for each held-out document in the target language.</S><S sid =""138"" ssid = ""87"">We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.</S><S sid =""51"" ssid = ""17"">Gibbs sampling involves sequentially resampling each zln from its conditional posterior: where z\l n is the current set of topic assignments for all other tokens in the tuple  while (Nt)\l n is the number of occurrences of topic t in the tuple  excluding zln  the variable being resampled.</S>",['Method_Citation']
15,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"In previously reported work, (Mimno et al, 2009) evaluate parallel document retrieval using PLTM on Europarl speeches in English and Spanish, using training and test sets of size similar to ours","In previously reported work, (Mimno et al, 2009) evaluate parallel document retrieval using PLTM on Europarl speeches in English and Spanish, using training and test sets of size similar to ours","['148', '131', '102', '99', '93']","<S sid =""148"" ssid = ""97"">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid =""131"" ssid = ""80"">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid =""102"" ssid = ""51"">In contrast  PLTM assigns a significant number of tokens to almost all 800 topics  in very similar proportions in both languages.</S><S sid =""99"" ssid = ""48"">Figure 5 shows the proportion of all tokens in English and Finnish assigned to each topic under LDA and PLTM with 800 topics.</S><S sid =""93"" ssid = ""42"">Table 2 shows the log probability of held-out data in nats per word for PLTM and LDA  both trained with 200 topics.</S>",['Method_Citation']
16,D09-1092,W12-3117,0,"Mimno et al, 2009",0,"We assume that a higher dimensionality can lead to a better repartitioning of the vocabulary over the topics. Multilingual LDA has been used before in natural language processing ,e.g .polylingual topic models (Mimno et al, 2009) or multilingual topic models for unaligned text (Boyd-Graber and Blei, 2009)","Multilingual LDA has been used before in natural language processing, e.g. polylingual topic models (Mimno et al, 2009) or multilingual topic models for unaligned text (Boyd-Graber and Blei, 2009)","['25', '1', '6', '24', '131']","<S sid =""25"" ssid = ""1"">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid =""1"" ssid = ""1"">Topic models are a useful tool for analyzing large text collections  but have previously been applied in only monolingual  or at most bilingual  contexts.</S><S sid =""6"" ssid = ""2"">Topic models have been used for analyzing topic trends in research literature (Mann et al.  2006; Hall et al.  2008)  inferring captions for images (Blei and Jordan  2003)  social network analysis in email (McCallum et al.  2005)  and expanding queries with topically related words in information retrieval (Wei and Croft  2006).</S><S sid =""24"" ssid = ""20"">By linking topics across languages  polylingual topic models can increase cross-cultural understanding by providing readers with the ability to characterize the contents of collections in unfamiliar languages and identify trends in topic prevalence.</S><S sid =""131"" ssid = ""80"">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S>",['Implication_Citation']
17,D09-1092,W11-2133,0,2009,0,"ji =wjk? M j i? m k=1w j k, (1) where M j is the topic distribution of document j and wk is the number of occurrences of phrase pair Xk in document j. Mimno et al (2009) extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipediaarticles)","Mimno et al (2009) extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipedia articles)","['154', '162', '155', '131', '148']","<S sid =""154"" ssid = ""103"">We then use this model to infer topic distributions for each of the 11 documents in each of the held-out document tuples using a method similar to that used to calculate held-out probabilities (Wallach et al.  2009).</S><S sid =""162"" ssid = ""111"">Restricting the query/target pairs to only those with query and target documents that are both longer than 50 words results in significant improvement and reduced variance: the average proportion of query documents for which the true translation is ranked highest goes from 53.9% to 72.7%.</S><S sid =""155"" ssid = ""104"">Finally  for each pair of languages (“query” and “target”) we calculate the difference between the topic distribution for each held-out document in the query language and the topic distribution for each held-out document in the target language.</S><S sid =""131"" ssid = ""80"">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid =""148"" ssid = ""97"">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S>",['Method_Citation']
18,D09-1092,W11-2133,0,2009,0,Tuple-specific topic distributions arelearned using LDA with distinct topic-word concentration parameters? l. Mimno et al (2009) show that PLTM sufficiently aligns topics in parallel corpora,Mimno et al (2009) show that PLTM sufficiently aligns topics in parallel corpora,"['126', '129', '193', '114', '102']","<S sid =""126"" ssid = ""75"">These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.</S><S sid =""129"" ssid = ""78"">We therefore evaluate the ability of the PLTM to generate bilingual lexica  similar to other work in unsupervised translation modeling (Haghighi et al.  2008).</S><S sid =""193"" ssid = ""2"">We analyzed the characteristics of PLTM in comparison to monolingual LDA  and demonstrated that it is possible to discover aligned topics.</S><S sid =""114"" ssid = ""63"">Ideally  the “glue” documents in g will be sufficient to align the topics across languages  and will cause comparable documents in S to have similar distributions over topics even though they are modeled independently.</S><S sid =""102"" ssid = ""51"">In contrast  PLTM assigns a significant number of tokens to almost all 800 topics  in very similar proportions in both languages.</S>",['Aim_Citation']
19,D09-1092,P14-2110,0,"Mimno et al, 2009",0,"A good candidate for multilingual topic analyses are polylin gual topic models (Mimno et al, 2009), which learn topics for multiple languages, creating tuples of language specific distributions over monolingual vocabularies for each topic","A good candidate for multilingual topic analyses are polylingual topic models (Mimno et al, 2009), which learn topics for multiple languages, creating tuples of language specific distributions over monolingual vocabularies for each topic","['78', '155', '24', '77', '83']","<S sid =""78"" ssid = ""27"">Although the model does not distinguish between topic assignment variables within a given document tuple (so it is technically incorrect to speak of different posterior distributions over topics for different documents in a given tuple)  we can nevertheless divide topic assignment variables between languages and use them to estimate a Dirichlet-multinomial posterior distribution for each language in each tuple.</S><S sid =""155"" ssid = ""104"">Finally  for each pair of languages (“query” and “target”) we calculate the difference between the topic distribution for each held-out document in the query language and the topic distribution for each held-out document in the target language.</S><S sid =""24"" ssid = ""20"">By linking topics across languages  polylingual topic models can increase cross-cultural understanding by providing readers with the ability to characterize the contents of collections in unfamiliar languages and identify trends in topic prevalence.</S><S sid =""77"" ssid = ""26"">Maximum topic probability in document Although the posterior distribution over topics for each tuple is not concentrated on one topic  it is worth checking that this is not simply because the model is assigning a single topic to the 1We use the R density function. tokens in each of the languages.</S><S sid =""83"" ssid = ""32"">The vast majority of divergences are relatively low (1.0 indicates no overlap in topics between languages in a given document tuple) indicating that  for each tuple  the model is not simply assigning all tokens in a particular language to a single topic.</S>",['Aim_Citation']
20,D09-1092,P14-2110,0,2009,0,"3csLDATo train a polylingual topic model on social media, we make two modifications to the model of Mimno et al (2009): add a token specific language variable, and a process for identifying aligned top ics. First ,polylingual topic models require parallel or comparable corpora in which each document has an assigned language","To train a polylingual topic model on social media, we make two modifications to the model of Mimno et al (2009): add a token specific language variable, and a process for identifying aligned top ics","['83', '148', '73', '154', '78']","<S sid =""83"" ssid = ""32"">The vast majority of divergences are relatively low (1.0 indicates no overlap in topics between languages in a given document tuple) indicating that  for each tuple  the model is not simply assigning all tokens in a particular language to a single topic.</S><S sid =""148"" ssid = ""97"">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid =""73"" ssid = ""22"">If the model assigns all tokens in a tuple to a single topic  the maximum posterior topic probability for that tuple will be near to 1.0.</S><S sid =""154"" ssid = ""103"">We then use this model to infer topic distributions for each of the 11 documents in each of the held-out document tuples using a method similar to that used to calculate held-out probabilities (Wallach et al.  2009).</S><S sid =""78"" ssid = ""27"">Although the model does not distinguish between topic assignment variables within a given document tuple (so it is technically incorrect to speak of different posterior distributions over topics for different documents in a given tuple)  we can nevertheless divide topic assignment variables between languages and use them to estimate a Dirichlet-multinomial posterior distribution for each language in each tuple.</S>","['Implication_Citation', 'Aim_Citation']"
