Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Reference Citation
1,W99-0623,A00-2005,0,1999,0,1 Introduct ion Henderson and Brill (1999) showed that independent human research efforts produce parsers that can be combined for an overall boost in accuracy,Henderson and Brill (1999) showed that independent human research efforts produce parsers that can be combined for an overall boost in accuracy,"['93', '135', '95', '85', '49']","<S sid =""93"" ssid = ""22"">Because we are working with only three parsers  the only situation in which context will help us is when it can indicate we should choose to believe a single parser that disagrees with the majority hypothesis instead of the majority hypothesis itself.</S><S sid =""135"" ssid = ""64"">The average individual parser accuracy was reduced by more than 5% when we added this new parser  but the precision of the constituent voting technique was the only result that decreased significantly.</S><S sid =""95"" ssid = ""24"">One side of the decision making process is when we choose to believe a constituent should be in the parse  even though only one parser suggests it.</S><S sid =""85"" ssid = ""14"">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S><S sid =""49"" ssid = ""35"">In general  the lemma of the previous section does not ensure that all the productions in the combined parse are found in the grammars of the member parsers.</S>",['Method_Citation']
2,W99-0623,A00-2005,0,1999,0,the collection of hypotheses ti =fi (Stest) using the unweighted constituent voting scheme of Henderson and Brill (1999),"Given a novel sentence Stest E Ctest, combine the collection of hypotheses ti = fi(Stest) using the unweighted constituent voting scheme of Henderson and Brill (1999)","['41', '105', '46', '24', '133']","<S sid =""41"" ssid = ""27"">IL+-1Proof: Assume a pair of crossing constituents appears in the output of the constituent voting technique using k parsers.</S><S sid =""105"" ssid = ""34"">Similarly Figures 1 and 2 show how the isolated constituent precision varies by sentence length and the size of the span of the hypothesized constituent.</S><S sid =""46"" ssid = ""32"">None of the parsers produce parses with crossing brackets  so none of them votes for both of the assumed constituents.</S><S sid =""24"" ssid = ""10"">We include a constituent in our hypothesized parse if it appears in the output of a majority of the parsers.</S><S sid =""133"" ssid = ""62"">The entries in this table can be compared with those of Table 3 to see how the performance of the combining techniques degrades in the presence of an inferior parser.</S>",['Method_Citation']
4,W99-0623,N10-1091,0,"Henderson and Brill, 1999",0,"5 (Henderson and Brill, 1999) used a similar framework in the context of constituent parsing and only three base parsers","(Henderson and Brill, 1999) used a similar framework in the context of constituent parsing and only three base parsers","['19', '120', '70', '131', '46']","<S sid =""19"" ssid = ""5"">The precision and recall measures (described in more detail in Section 3) used in evaluating Treebank parsing treat each constituent as a separate entity  a minimal unit of correctness.</S><S sid =""120"" ssid = ""49"">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser  and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S><S sid =""70"" ssid = ""56"">In this case we are interested in finding' the maximum probability parse  ri  and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers.</S><S sid =""131"" ssid = ""60"">It was then tested on section 22 of the Treebank in conjunction with the other parsers.</S><S sid =""46"" ssid = ""32"">None of the parsers produce parses with crossing brackets  so none of them votes for both of the assumed constituents.</S>",['Method_Citation']
5,W99-0623,W05-1518,0,"Henderson and Brill, 1999",0,"A successful application of voting and of a stacked classifier to constituent parsing followed in (Henderson and Brill, 1999)","A successful application of voting and of a stacked classifier to constituent parsing followed in (Henderson and Brill, 1999)","['19', '120', '24', '46', '41']","<S sid =""19"" ssid = ""5"">The precision and recall measures (described in more detail in Section 3) used in evaluating Treebank parsing treat each constituent as a separate entity  a minimal unit of correctness.</S><S sid =""120"" ssid = ""49"">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser  and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S><S sid =""24"" ssid = ""10"">We include a constituent in our hypothesized parse if it appears in the output of a majority of the parsers.</S><S sid =""46"" ssid = ""32"">None of the parsers produce parses with crossing brackets  so none of them votes for both of the assumed constituents.</S><S sid =""41"" ssid = ""27"">IL+-1Proof: Assume a pair of crossing constituents appears in the output of the constituent voting technique using k parsers.</S>",['Implication_Citation']
6,W99-0623,W05-1518,0,"Henderson and Brill, 1999",0,"This approach roughly corresponds to (Henderson and Brill, 1999)? s Na ?ve Bayes parse hybridization","This approach roughly corresponds to (Henderson and Brill, 1999)'s Naive Bayes parse hybridization","['27', '139', '84', '146', '52']","<S sid =""27"" ssid = ""13"">Another technique for parse hybridization is to use a naïve Bayes classifier to determine which constituents to include in the parse.</S><S sid =""139"" ssid = ""1"">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S><S sid =""84"" ssid = ""13"">The first shows how constituent features and context do not help in deciding which parser to trust.</S><S sid =""146"" ssid = ""1"">We would like to thank Eugene Charniak  Michael Collins  and Adwait Ratnaparkhi for enabling all of this research by providing us with their parsers and helpful comments.</S><S sid =""52"" ssid = ""38"">This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.</S>",['Method_Citation']
7,W99-0623,W05-1518,0,1999,0,Henderson and Brill (1999) also reported that context did not help them to outperform simple voting,Henderson and Brill (1999) also reported that context did not help them to outperform simple voting,"['84', '93', '39', '26', '48']","<S sid =""84"" ssid = ""13"">The first shows how constituent features and context do not help in deciding which parser to trust.</S><S sid =""93"" ssid = ""22"">Because we are working with only three parsers  the only situation in which context will help us is when it can indicate we should choose to believe a single parser that disagrees with the majority hypothesis instead of the majority hypothesis itself.</S><S sid =""39"" ssid = ""25"">There are simply not enough votes remaining to allow any of the crossing structures to enter the hypothesized constituent set.</S><S sid =""26"" ssid = ""12"">This technique has the advantage of requiring no training  but it has the disadvantage of treating all parsers equally even though they may have differing accuracies or may specialize in modeling different phenomena.</S><S sid =""48"" ssid = ""34"">• Similarly  when the naïve Bayes classifier is configured such that the constituents require estimated probabilities strictly larger than 0.5 to be accepted  there is not enough probability mass remaining on crossing brackets for them to be included in the hypothesis.</S>",['Method_Citation']
8,W99-0623,W05-1518,0,"Henderson and Brill, 1999",0,"(Henderson and Brill, 1999) improved their best parser? s F-measure of 89.7 to 91.3, using their na ?ve Bayes voting on the Penn TreeBank constituent structures (16% error reduction)","(Henderson and Brill, 1999) improved their best parser's F-measure of 89.7 to 91.3, using their naive Bayes voting on the Penn TreeBank constituent structures (16% error reduction)","['120', '46', '72', '131', '67']","<S sid =""120"" ssid = ""49"">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser  and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S><S sid =""46"" ssid = ""32"">None of the parsers produce parses with crossing brackets  so none of them votes for both of the assumed constituents.</S><S sid =""72"" ssid = ""1"">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank  leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S><S sid =""131"" ssid = ""60"">It was then tested on section 22 of the Treebank in conjunction with the other parsers.</S><S sid =""67"" ssid = ""53"">The set of candidate constituents comes from the union of all the constituents suggested by the member parsers.</S>",['Method_Citation']
10,W99-0623,P01-1005,0,"Henderson and Brill, 1999",0,"Voting has proven to be an effective technique for improving classifier accuracy for many applications, including part-of-speech tagging (van Halteren, et al 1998), parsing (Henderson and Brill, 1999), and word sense disambiguation (Pederson, 2000)","Voting has proven to be an effective technique for improving classifier accuracy for many applications, including part-of-speech tagging (van Halteren, et al 1998), parsing (Henderson and Brill, 1999), and word sense disambiguation (Pederson, 2000)","['9', '146', '27', '120', '12']","<S sid =""9"" ssid = ""5"">Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).</S><S sid =""146"" ssid = ""1"">We would like to thank Eugene Charniak  Michael Collins  and Adwait Ratnaparkhi for enabling all of this research by providing us with their parsers and helpful comments.</S><S sid =""27"" ssid = ""13"">Another technique for parse hybridization is to use a naïve Bayes classifier to determine which constituents to include in the parse.</S><S sid =""120"" ssid = ""49"">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser  and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S><S sid =""12"" ssid = ""8"">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S>",['Method_Citation']
11,W99-0623,D09-1161,0,1999,0,"Regarding the system combination study, Henderson and Brill (1999) propose two parser combination schemes, one that selects an entire tree from one of the parsers, and one that builds a new tree by selecting constituents suggested by the initial trees","Regarding the system combination study, Henderson and Brill (1999) propose two parser combination schemes, one that selects an entire tree from one of the parsers, and one that builds a new tree by selecting constituents suggested by the initial trees","['61', '49', '64', '67', '117']","<S sid =""61"" ssid = ""47"">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid =""49"" ssid = ""35"">In general  the lemma of the previous section does not ensure that all the productions in the combined parse are found in the grammars of the member parsers.</S><S sid =""64"" ssid = ""50"">Furthermore  we know one of the original parses will be the hypothesized parse  so the direct method of determining which one is best is to compute the probability of each of the candidate parses using the probabilistic model we developed in Section 2.1.</S><S sid =""67"" ssid = ""53"">The set of candidate constituents comes from the union of all the constituents suggested by the member parsers.</S><S sid =""117"" ssid = ""46"">Another way to interpret this is that less than 5% of the correct constituents are missing from the hypotheses generated by the union of the three parsers.</S>",['Method_Citation']
12,W99-0623,D09-1161,0,1999,0,"Henderson and Brill (1999) combine three parsers and obtained an F1 score of 90.6, which is better than the score of 88.6 obtained by the best individual parser as reported in their paper","Henderson and Brill (1999) combine three parsers and obtained an F1 score of 90.6, which is better than the score of 88.6 obtained by the best individual parser as reported in their paper","['120', '87', '72', '70', '46']","<S sid =""120"" ssid = ""49"">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser  and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S><S sid =""87"" ssid = ""16"">It is possible one could produce better models by introducing features describing constituents and their contexts because one parser could be much better than the majority of the others in particular situations.</S><S sid =""72"" ssid = ""1"">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank  leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S><S sid =""70"" ssid = ""56"">In this case we are interested in finding' the maximum probability parse  ri  and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers.</S><S sid =""46"" ssid = ""32"">None of the parsers produce parses with crossing brackets  so none of them votes for both of the assumed constituents.</S>",['Implication_Citation']
13,W99-0623,D09-1161,0,"Henderson and Brill, 1999",0,"Besides the two model scores, we also adopt constituent count as an additional feature in spired by (Henderson and Brill 1999) and (Sagae and Lavie 2006)","Besides the two model scores, we also adopt constituent count as an additional feature in spired by (Henderson and Brill 1999) and (Sagae and Lavie 2006)","['80', '146', '65', '120', '87']","<S sid =""80"" ssid = ""9"">For our experiments we also report the mean of precision and recall  which we denote by (P + R)I2 and F-measure.</S><S sid =""146"" ssid = ""1"">We would like to thank Eugene Charniak  Michael Collins  and Adwait Ratnaparkhi for enabling all of this research by providing us with their parsers and helpful comments.</S><S sid =""65"" ssid = ""51"">We model each parse as the decisions made to create it  and model those decisions as independent events.</S><S sid =""120"" ssid = ""49"">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser  and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S><S sid =""87"" ssid = ""16"">It is possible one could produce better models by introducing features describing constituents and their contexts because one parser could be much better than the majority of the others in particular situations.</S>",['Method_Citation']
14,W99-0623,N06-2033,0,"Henderson and Brill, 1999",0,"Henderson and Brill (1999) proposed two parser combination schemes, one that picks an entire tree from one of the parsers, and one that, like ours, builds a new tree from constituents from the initial trees","Henderson and Brill (1999) proposed two parser combination schemes, one that picks an entire tree from one of the parsers, and one that, like ours, builds a new tree from constituents from the initial trees","['61', '67', '64', '130', '49']","<S sid =""61"" ssid = ""47"">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid =""67"" ssid = ""53"">The set of candidate constituents comes from the union of all the constituents suggested by the member parsers.</S><S sid =""64"" ssid = ""50"">Furthermore  we know one of the original parses will be the hypothesized parse  so the direct method of determining which one is best is to compute the probability of each of the candidate parses using the probabilistic model we developed in Section 2.1.</S><S sid =""130"" ssid = ""59"">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S><S sid =""49"" ssid = ""35"">In general  the lemma of the previous section does not ensure that all the productions in the combined parse are found in the grammars of the member parsers.</S>",['Implication_Citation']
15,W99-0623,N09-2064,0,"Henderson and Brill, 1999",0,"(Henderson and Brill, 1999) perform parse selection by maximizing the expected precision of the selected parse with respect to the set of parses being combined","(Henderson and Brill, 1999) perform parse selection by maximizing the expected precision of the selected parse with respect to the set of parses being combined","['61', '70', '78', '46', '64']","<S sid =""61"" ssid = ""47"">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid =""70"" ssid = ""56"">In this case we are interested in finding' the maximum probability parse  ri  and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers.</S><S sid =""78"" ssid = ""7"">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid =""46"" ssid = ""32"">None of the parsers produce parses with crossing brackets  so none of them votes for both of the assumed constituents.</S><S sid =""64"" ssid = ""50"">Furthermore  we know one of the original parses will be the hypothesized parse  so the direct method of determining which one is best is to compute the probability of each of the candidate parses using the probabilistic model we developed in Section 2.1.</S>",['Method_Citation']
16,W99-0623,N09-2064,0,"Henderson and Brill, 1999",0,"(Henderson and Brill, 1999) and (Sagae and Lavie, 2006) propose methods for parse hybridization by recombining constituents","(Henderson and Brill, 1999) and (Sagae and Lavie, 2006) propose methods for parse hybridization by recombining constituents","['146', '120', '80', '73', '59']","<S sid =""146"" ssid = ""1"">We would like to thank Eugene Charniak  Michael Collins  and Adwait Ratnaparkhi for enabling all of this research by providing us with their parsers and helpful comments.</S><S sid =""120"" ssid = ""49"">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser  and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S><S sid =""80"" ssid = ""9"">For our experiments we also report the mean of precision and recall  which we denote by (P + R)I2 and F-measure.</S><S sid =""73"" ssid = ""2"">We used section 23 as the development set for our combining techniques  and section 22 only for final testing.</S><S sid =""59"" ssid = ""45"">Once again we present both a non-parametric and a parametric technique for this task.</S>",['Method_Citation']
17,W99-0623,N09-2064,0,"Henderson and Brill, 1999",0,"output (Figure 3) .Second, the parse selection method of (Henderson and Brill, 1999) selects the parse with maximum expected precision; here, we present an efficient, linear-time algorithm for selecting the parse with maximum expected f-score within the Mini mum Bayes Risk (MBR) framework. Third, we extend these parser combination methods from 1-best outputs to n-best outputs","Second, the parse selection method of (Henderson and Brill, 1999) selects the parse with maximum expected precision; here, we present an efficient, linear-time algorithm for selecting the parse with maximum expected f-score within the Mini mum Bayes Risk (MBR) framework","['64', '61', '70', '62', '46']","<S sid =""64"" ssid = ""50"">Furthermore  we know one of the original parses will be the hypothesized parse  so the direct method of determining which one is best is to compute the probability of each of the candidate parses using the probabilistic model we developed in Section 2.1.</S><S sid =""61"" ssid = ""47"">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid =""70"" ssid = ""56"">In this case we are interested in finding' the maximum probability parse  ri  and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers.</S><S sid =""62"" ssid = ""48"">This is the parse that is closest to the centroid of the observed parses under the similarity metric.</S><S sid =""46"" ssid = ""32"">None of the parsers produce parses with crossing brackets  so none of them votes for both of the assumed constituents.</S>",['Implication_Citation']
18,W99-0623,P09-1065,0,"Henderson and Brill, 1999",0,"System combination has benefited various NLP tasks in recent years, such as products-of-experts (e.g., (Smith and Eisner, 2005)) and ensemble based parsing (e.g., (Henderson and Brill, 1999))","System combination has benefited various NLP tasks in recent years, such as products-of-experts (e.g., (Smith and Eisner, 2005)) and ensemble based parsing (e.g., (Henderson and Brill, 1999))","['146', '59', '120', '26', '12']","<S sid =""146"" ssid = ""1"">We would like to thank Eugene Charniak  Michael Collins  and Adwait Ratnaparkhi for enabling all of this research by providing us with their parsers and helpful comments.</S><S sid =""59"" ssid = ""45"">Once again we present both a non-parametric and a parametric technique for this task.</S><S sid =""120"" ssid = ""49"">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser  and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S><S sid =""26"" ssid = ""12"">This technique has the advantage of requiring no training  but it has the disadvantage of treating all parsers equally even though they may have differing accuracies or may specialize in modeling different phenomena.</S><S sid =""12"" ssid = ""8"">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S>",['Method_Citation']
20,W99-0623,C10-1151,0,1999,0,Henderson and Brill (1999) performs parse selection by maximizing the expected precision of selected parse with respect to the set of parses to be combined,Henderson and Brill (1999) performs parse selection by maximizing the expected precision of selected parse with respect to the set of parses to be combined,"['78', '61', '70', '46', '35']","<S sid =""78"" ssid = ""7"">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid =""61"" ssid = ""47"">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid =""70"" ssid = ""56"">In this case we are interested in finding' the maximum probability parse  ri  and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers.</S><S sid =""46"" ssid = ""32"">None of the parsers produce parses with crossing brackets  so none of them votes for both of the assumed constituents.</S><S sid =""35"" ssid = ""21"">The hypothesized parse is then the set of constituents that are likely (P > 0.5) to be in the parse according to this model.</S>",['Aim_Citation']
