Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,J01-2004,W05-0104,0,2001,0,"Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark, see Roark (2001))","Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark, see Roark (2001))","['16', '102', '14', '369', '399']","<S sid =""16"" ssid = ""4"">While language models built around shallow local dependencies are still the standard in state-of-the-art speech recognition systems  there is reason to hope that better language models can and will be developed by computational linguists for this task.</S><S sid =""102"" ssid = ""6"">One approach to syntactic language modeling is to use this distribution directly as a language model.</S><S sid =""14"" ssid = ""2"">Perhaps one reason for this is that  until relatively recently  few methods have come out of the natural language processing community that were shown to improve upon the very simple language models still standardly in use in speech recognition systems.</S><S sid =""369"" ssid = ""125"">The log of the language model score is multiplied by the language model (LM) weight when summing the logs of the language and acoustic scores  as a way of increasing the relative contribution of the language model to the composite score.</S><S sid =""399"" ssid = ""12"">In addition  as mentioned above  we would like to further test our language model in speech recognition tasks  to see if the perplexity improvement that we have seen can lead to significant reductions in word error rate.</S>",['Method_Citation']
2,J01-2004,P08-1013,0,2001,0,"Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition. These models have in common that they explicitly or implicitly use a context-free grammar induced from a tree bank, with the exception of Chelba and Jelinek (2000)",Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition,"['413', '207', '16', '106', '322']","<S sid =""413"" ssid = ""3"">Thanks also to four anonymous reviewers for valuable and insightful comments  and to Ciprian Chelba  Sanjeev Khudanpur  and Frederick Jelinek for comments and suggestions.</S><S sid =""207"" ssid = ""111"">The model allows us to assign probabilities to derivations  which can be used by the parsing algorithm to decide heuristically which candidates are promising and should be expanded  and which are less promising and should be pruned.</S><S sid =""16"" ssid = ""4"">While language models built around shallow local dependencies are still the standard in state-of-the-art speech recognition systems  there is reason to hope that better language models can and will be developed by computational linguists for this task.</S><S sid =""106"" ssid = ""10"">String prefix probabilities can be straightforwardly used to compute conditional word probabilities by definition: Stolcke and Segal (1994) and Jurafsky et al. (1995) used these basic ideas to estimate bigram probabilities from hand-written PCFGs  which were then used in language models.</S><S sid =""322"" ssid = ""78"">Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.</S>",['Method_Citation']
4,J01-2004,P04-1015,0,"Roark, 2001a",0,"Theperceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn tree bank","The perceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn treebank","['248', '100', '369', '181', '367']","<S sid =""248"" ssid = ""4"">In principle  if two models are tested on the same test corpus  the model that assigns the lower perplexity to the test corpus is the model closest to the true distribution of the language  and thus better as a prior model for speech recognition.</S><S sid =""100"" ssid = ""4"">The approach that we will subsequently present uses the probabilistic grammar as its language model  but only includes probability mass from those parses that are found  that is  it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S><S sid =""369"" ssid = ""125"">The log of the language model score is multiplied by the language model (LM) weight when summing the logs of the language and acoustic scores  as a way of increasing the relative contribution of the language model to the composite score.</S><S sid =""181"" ssid = ""85"">If the left-hand side of the production is a POS  then the algorithm takes the right branch of the decision tree  and returns (at level 4) the POS of the closest c-commanding lexical head to A  which it finds by walking the parse tree; if the left-hand side of the rule is not a POS  then the algorithm returns (at level 4) the closest sibling to the left of the parent of constituent (A).</S><S sid =""367"" ssid = ""123"">Table 5 reports the word and sentence error rates for five different models: (i) the trigram model that comes with the lattices  trained on approximately 40M words  with a vocabulary of 20 000; (ii) the best-performing model from Chelba (2000)  which was interpolated with the lattice trigram at A -= 0.4; (iii) our parsing model  with the same training and vocabulary as the perplexity trials above; (iv) a trigram model with the same training and vocabulary as the parsing model; and (v) no language model at all.</S>",['Method_Citation']
5,J01-2004,P04-1015,0,"Roark, 2001a",0,"We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the per ceptron model gives performance competitive to that of the generative model on parsing the Penn tree bank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search","We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the perceptron model gives performance competitive to that of the generative model on parsing the Penn treebank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search","['248', '100', '165', '369', '181']","<S sid =""248"" ssid = ""4"">In principle  if two models are tested on the same test corpus  the model that assigns the lower perplexity to the test corpus is the model closest to the true distribution of the language  and thus better as a prior model for speech recognition.</S><S sid =""100"" ssid = ""4"">The approach that we will subsequently present uses the probabilistic grammar as its language model  but only includes probability mass from those parses that are found  that is  it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S><S sid =""165"" ssid = ""69"">We can then condition the probability of the production on the values that were returned by the set of functions.</S><S sid =""369"" ssid = ""125"">The log of the language model score is multiplied by the language model (LM) weight when summing the logs of the language and acoustic scores  as a way of increasing the relative contribution of the language model to the composite score.</S><S sid =""181"" ssid = ""85"">If the left-hand side of the production is a POS  then the algorithm takes the right branch of the decision tree  and returns (at level 4) the POS of the closest c-commanding lexical head to A  which it finds by walking the parse tree; if the left-hand side of the rule is not a POS  then the algorithm returns (at level 4) the closest sibling to the left of the parent of constituent (A).</S>",['Implication_Citation']
6,J01-2004,P04-1015,0,2001a,0,"One way around this problem is to adopt a two-pass approach, where GEN (x) is the top N analyses under some initial model, as in the re ranking approach of Collins (2000) .In the current paper we explore alternatives to rerank ing approaches, namely heuristic methods for finding the arg max, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999)","In the current paper we explore alternatives to reranking approaches, namely heuristic methods for finding the argmax, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999)","['141', '167', '364', '287', '147']","<S sid =""141"" ssid = ""45"">We will then present empirical results in two domains: one to compare with previous work in the parsing literature  and the other to compare with previous work using parsing for language modeling for speech recognition  in particular with the Chelba and Jelinek results mentioned above.</S><S sid =""167"" ssid = ""71"">In order to avoid any confusions in identifying the nonterminal label of a particular rule production in either its factored or rionfactored version  we introduce the function constituent (A) for every nonterminal in the factored grammar Gf  which is simply the label of the constituent whose factorization results in A.</S><S sid =""364"" ssid = ""120"">We follow Chelba (2000) in dealing with this problem: for parsing purposes  we use the Penn Treebank tokenization; for interpolation with the provided trigram model  and for evaluation  the lattice tokenization is used.</S><S sid =""287"" ssid = ""43"">The bulk of the improvement comes from simply conditioning on the labels of the parent and the closest sibling to the node being expanded.</S><S sid =""147"" ssid = ""51"">One way to estimate the probabilities of these rules is to annotate the heads onto the constituent labels in the training corpus and simply count the number of times particular productions occur (relative frequency estimation).</S>",['Method_Citation']
7,J01-2004,P04-1015,0,2001a,0,"approach The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights","The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights","['141', '345', '327', '287', '147']","<S sid =""141"" ssid = ""45"">We will then present empirical results in two domains: one to compare with previous work in the parsing literature  and the other to compare with previous work using parsing for language modeling for speech recognition  in particular with the Chelba and Jelinek results mentioned above.</S><S sid =""345"" ssid = ""101"">This is not surprising  since our conditioning information is in many ways orthogonal to that of the trigram  insofar as it includes the probability mass of the derivations; in contrast  their model in some instances is very close to the trigram  by conditioning on two words in the prefix string  which may happen to be the two adjacent words.</S><S sid =""327"" ssid = ""83"">Differences in performance will give an indication of the impact on parser performance of the different modifications to the corpora.</S><S sid =""287"" ssid = ""43"">The bulk of the improvement comes from simply conditioning on the labels of the parent and the closest sibling to the node being expanded.</S><S sid =""147"" ssid = ""51"">One way to estimate the probabilities of these rules is to annotate the heads onto the constituent labels in the training corpus and simply count the number of times particular productions occur (relative frequency estimation).</S>",['Method_Citation']
9,J01-2004,P04-1015,0,2001a,0,"Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word","Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word","['178', '145', '181', '126', '287']","<S sid =""178"" ssid = ""82"">If the left-hand side of the rule is a POS  and there is no sibling to the left of constituent (A) in the derivation  then the algorithm takes the right branch of the decision tree to decide what value to return; otherwise the left branch.</S><S sid =""145"" ssid = ""49"">Examples of this are bilexical grammars—such as Eisner and Satta (1999)  Charniak (1997)  Collins (1997)—where the lexical heads of each constituent are annotated on both the right- and left-hand sides of the context-free rules  under the constraint that every constituent inherits the lexical head from exactly one of its children  and the lexical head of a POS is its terminal item.</S><S sid =""181"" ssid = ""85"">If the left-hand side of the production is a POS  then the algorithm takes the right branch of the decision tree  and returns (at level 4) the POS of the closest c-commanding lexical head to A  which it finds by walking the parse tree; if the left-hand side of the rule is not a POS  then the algorithm returns (at level 4) the closest sibling to the left of the parent of constituent (A).</S><S sid =""126"" ssid = ""30"">When all of the parser operations have finished at a particular point in the string  the next word is predicted as follows: For each derivation in the beam  the headwords of the two topmost stack entries form a trigram with the conditioned word.</S><S sid =""287"" ssid = ""43"">The bulk of the improvement comes from simply conditioning on the labels of the parent and the closest sibling to the node being expanded.</S>",['Method_Citation']
10,J01-2004,P05-1022,0,"Roark, 2001",0,"A good example of this is the Roark parser (Roark, 2001) which works left-to right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning","A good example of this is the Roark parser (Roark, 2001) which works left-to-right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning","['110', '126', '181', '128', '157']","<S sid =""110"" ssid = ""14"">The parser performs two basic operations: (i) shifting  which involves pushing the POS label of the next word onto the stack and moving the pointer to the following word in the input string; and (ii) reducing  which takes the top k stack entries and replaces them with a single new entry  the nonterminal label of which is the left-hand side of a rule in the grammar that has the k top stack entry labels on the right-hand side.</S><S sid =""126"" ssid = ""30"">When all of the parser operations have finished at a particular point in the string  the next word is predicted as follows: For each derivation in the beam  the headwords of the two topmost stack entries form a trigram with the conditioned word.</S><S sid =""181"" ssid = ""85"">If the left-hand side of the production is a POS  then the algorithm takes the right branch of the decision tree  and returns (at level 4) the POS of the closest c-commanding lexical head to A  which it finds by walking the parse tree; if the left-hand side of the rule is not a POS  then the algorithm returns (at level 4) the closest sibling to the left of the parent of constituent (A).</S><S sid =""128"" ssid = ""32"">More precisely  for a beam of derivations D  where hod and hid are the lexical heads of the top two entries on the stack of d. Figure 3 gives a partial tree representation of a potential derivation state for the string &quot;the dog chased the cat with spots&quot;  at the point when the word &quot;with&quot; is to be predicted.</S><S sid =""157"" ssid = ""61"">In a top-down parser  each rule expansion is made for a particular candidate parse  which carries with it the entire rooted derivation to that point; in a sense  the left-hand side of the rule is annotated with the entire left context  and the rule probabilities can be conditioned on any aspect of this derivation.</S>",['Method_Citation']
11,J01-2004,P05-1022,0,"Roark, 2001",0,"At the end one has a beam-width? s number of best parses (Roark, 2001) .The Collins parser (Collins, 1997) does use dynamic programming in its search","At the end one has a beam-width's number of best parses (Roark, 2001)","['126', '229', '181', '251', '269']","<S sid =""126"" ssid = ""30"">When all of the parser operations have finished at a particular point in the string  the next word is predicted as follows: For each derivation in the beam  the headwords of the two topmost stack entries form a trigram with the conditioned word.</S><S sid =""229"" ssid = ""133"">The LAP is the probability of a particular terminal being the next left-corner of a particular analysis.</S><S sid =""181"" ssid = ""85"">If the left-hand side of the production is a POS  then the algorithm takes the right branch of the decision tree  and returns (at level 4) the POS of the closest c-commanding lexical head to A  which it finds by walking the parse tree; if the left-hand side of the rule is not a POS  then the algorithm returns (at level 4) the closest sibling to the left of the parent of constituent (A).</S><S sid =""251"" ssid = ""7"">Then  under certain assumptions  given a large enough sample  the sample mean of the negative log probability of a model will converge to its cross entropy with the true mode1.14 That is where w'ol is a string of the language L. In practice  one takes a large sample of the language  and calculates the negative log probability of the sample  normalized by its size.15 The lower the cross entropy (i.e.  the higher the probability the model assigns to the sample)  the better the model.</S><S sid =""269"" ssid = ""25"">In such a model  it is possible to commit to a set of partial analyses at a particular point that cannot be completed given the rest of the input string (i.e.  the parser can &quot;garden path&quot;).</S>",['Method_Citation']
12,J01-2004,P05-1022,0,"Roark, 2001",0,"To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses","To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses","['126', '291', '196', '116', '181']","<S sid =""126"" ssid = ""30"">When all of the parser operations have finished at a particular point in the string  the next word is predicted as follows: For each derivation in the beam  the headwords of the two topmost stack entries form a trigram with the conditioned word.</S><S sid =""291"" ssid = ""47"">Also  the parser returns a set of candidate parses  from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence)  we find an average labeled precision/recall of 94.1  for sentences of length < 100.</S><S sid =""196"" ssid = ""100"">In presenting the parsing results  we will systematically vary the amount of conditioning information  so as to get an idea of the behavior of the parser.</S><S sid =""116"" ssid = ""20"">The SLM is like a trigram  except that the conditioning words are taken from the tops of the stacks of candidate parses in the beam  rather than from the linear order of the string.</S><S sid =""181"" ssid = ""85"">If the left-hand side of the production is a POS  then the algorithm takes the right branch of the decision tree  and returns (at level 4) the POS of the closest c-commanding lexical head to A  which it finds by walking the parse tree; if the left-hand side of the rule is not a POS  then the algorithm returns (at level 4) the closest sibling to the left of the parent of constituent (A).</S>",['Implication_Citation']
13,J01-2004,P04-1006,0,"Roark, 2001",0,"We ran the first stage parser with 4-timesoverparsing for each string in 7The n? best lists were provided by Brian Roark (Roark, 2001) 8A local-tree is an explicit expansion of an edge and its children","The n-best lists were provided by Brian Roark (Roark, 2001)","['243', '402', '174', '244', '391']","<S sid =""243"" ssid = ""147"">The only c-productions are those introduced by left-factorization.</S><S sid =""402"" ssid = ""15"">In fact  left-corner parsing can be simulated by a top-down parser by transforming the grammar  as was done in Roark and Johnson (1999)  and so an approach very similar to the one outlined here could be used in that case.</S><S sid =""174"" ssid = ""78"">The functions that were used for the present study to condition the probability of the rule  A a  are presented in Figure 4  in a tree structure.</S><S sid =""244"" ssid = ""148"">Our factored grammar was produced by factoring the trees in the training corpus before grammar induction  which proceeded in the standard way  by counting rule frequencies.</S><S sid =""391"" ssid = ""4"">These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.</S>",['Method_Citation']
14,J01-2004,P05-1063,0,"Roark, 2001a",0,"Incremental top down and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models","Incremental top-down and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models","['2', '8', '402', '207', '401']","<S sid =""2"" ssid = ""2"">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid =""8"" ssid = ""2"">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid =""402"" ssid = ""15"">In fact  left-corner parsing can be simulated by a top-down parser by transforming the grammar  as was done in Roark and Johnson (1999)  and so an approach very similar to the one outlined here could be used in that case.</S><S sid =""207"" ssid = ""111"">The model allows us to assign probabilities to derivations  which can be used by the parsing algorithm to decide heuristically which candidates are promising and should be expanded  and which are less promising and should be pruned.</S><S sid =""401"" ssid = ""14"">Earley and left-corner parsers  as mentioned in the introduction  also have rooted derivations that can be used to calculated generative string prefix probabilities incrementally.</S>",['Implication_Citation']
15,J01-2004,W10-2009,0,"Roark, 2001",0,"Levy, on the other hand ,argued that studies of probabilistic parsing reveal that typically a small number of analyses are as signed the majority of probability mass (Roark, 2001)","Levy, on the other hand, argued that studies of probabilistic parsing reveal that typically a small number of analyses are assigned the majority of probability mass (Roark, 2001)","['100', '60', '144', '217', '352']","<S sid =""100"" ssid = ""4"">The approach that we will subsequently present uses the probabilistic grammar as its language model  but only includes probability mass from those parses that are found  that is  it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S><S sid =""60"" ssid = ""18"">The probability of a parse tree is the product of the probabilities of each rule in the tree.</S><S sid =""144"" ssid = ""48"">One way of thinking about conditioning the probabilities of productions on contextual information (e.g.  the label of the parent of a constituent or the lexical heads of constituents)  is as annotating the extra conditioning information onto the labels in the context-free rules.</S><S sid =""217"" ssid = ""121"">The probability PD is the product of the probabilities of all rules in the derivation D. F is the product of PD and a look-ahead probability  LAP(S w )  which is a measure of the likelihood of the stack S rewriting with w  at its left corner.</S><S sid =""352"" ssid = ""108"">One of the sentences was a failure  so that 12 of the word probabilities (all of the words after the point of the failure) were not estimated by our model.</S>",['Method_Citation']
17,J01-2004,D09-1034,0,2001,0,"For example, in Demberg and Keller (2008), trials were run deriving surprisal from the Roark (2001) parser under two different conditions: fully lex icalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags)","For example, in Demberg and Keller (2008), trials were run deriving surprisal from the Roark (2001) parser under two different conditions: fully lexicalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags)","['141', '402', '3', '9', '143']","<S sid =""141"" ssid = ""45"">We will then present empirical results in two domains: one to compare with previous work in the parsing literature  and the other to compare with previous work using parsing for language modeling for speech recognition  in particular with the Chelba and Jelinek results mentioned above.</S><S sid =""402"" ssid = ""15"">In fact  left-corner parsing can be simulated by a top-down parser by transforming the grammar  as was done in Roark and Johnson (1999)  and so an approach very similar to the one outlined here could be used in that case.</S><S sid =""3"" ssid = ""3"">A lexicalized probabilistic topdown parser is then presented  which performs very well  in terms of both the accuracy of returned parses and the efficiency with which they are found  relative to the best broad-coverage statistical parsers.</S><S sid =""9"" ssid = ""3"">A lexicalized probabilistic topdown parser is then presented  which performs very well  in terms of both the accuracy of returned parses and the efficiency with which they are found  relative to the best broad-coverage statistical parsers.</S><S sid =""143"" ssid = ""47"">It has been shown repeatedly—e.g.  Briscoe and Carroll (1993)  Charniak (1997)  Collins (1997)  Inui et al. (1997)  Johnson (1998)—that conditioning the probabilities of structures on the context within which they appear  for example on the lexical head of a constituent (Charniak 1997; Collins 1997)  on the label of its parent nonterminal (Johnson 1998)  or  ideally  on both and many other things besides  leads to a much better parsing model and results in higher parsing accuracies.</S>",['Method_Citation']
18,J01-2004,D09-1034,0,2001,0,"We modified the Roark (2001) parser to calculate the discussed measures 1, and the empirical results in? 4 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time","We modified the Roark (2001) parser to calculate the discussed measures, and the empirical results in ?4 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time","['141', '143', '3', '9', '21']","<S sid =""141"" ssid = ""45"">We will then present empirical results in two domains: one to compare with previous work in the parsing literature  and the other to compare with previous work using parsing for language modeling for speech recognition  in particular with the Chelba and Jelinek results mentioned above.</S><S sid =""143"" ssid = ""47"">It has been shown repeatedly—e.g.  Briscoe and Carroll (1993)  Charniak (1997)  Collins (1997)  Inui et al. (1997)  Johnson (1998)—that conditioning the probabilities of structures on the context within which they appear  for example on the lexical head of a constituent (Charniak 1997; Collins 1997)  on the label of its parent nonterminal (Johnson 1998)  or  ideally  on both and many other things besides  leads to a much better parsing model and results in higher parsing accuracies.</S><S sid =""3"" ssid = ""3"">A lexicalized probabilistic topdown parser is then presented  which performs very well  in terms of both the accuracy of returned parses and the efficiency with which they are found  relative to the best broad-coverage statistical parsers.</S><S sid =""9"" ssid = ""3"">A lexicalized probabilistic topdown parser is then presented  which performs very well  in terms of both the accuracy of returned parses and the efficiency with which they are found  relative to the best broad-coverage statistical parsers.</S><S sid =""21"" ssid = ""9"">First  the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string  which allows it to calculate a generative probability for each prefix string from the probabilistic grammar  and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>",['Implication_Citation']
19,J01-2004,D09-1034,0,2001,0,"In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here","In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here","['206', '196', '32', '84', '190']","<S sid =""206"" ssid = ""110"">This was an outline of the conditional probability model that we used for the PCFG.</S><S sid =""196"" ssid = ""100"">In presenting the parsing results  we will systematically vary the amount of conditioning information  so as to get an idea of the behavior of the parser.</S><S sid =""32"" ssid = ""20"">A second key feature of our approach is that top-down guidance improves the efficiency of the search as more and more conditioning events are extracted from the derivation for use in the probabilistic model.</S><S sid =""84"" ssid = ""42"">We will use this notion in our conditional probability model  and it is also useful for understanding some of the previous work in this area.</S><S sid =""190"" ssid = ""94"">In such a case  the head of the last child within the constituent is used as a proxy for the constituent head.</S>",['Method_Citation']
20,J01-2004,D09-1034,0,2001,0,"At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is re quired to calculate such measures. Let H (D) be the entropy over a set of derivations D, calculated as follows: H (D)=? X D? D? (D) P D?? D? (D?) log? (D) P D?? D? (D?) (10) If the set of derivations D= D (G, W [1, i]) is a set of partial derivations for string W [1, i], then H (D) is a measure of uncertainty over the partial derivations ,i.e., the uncertainty regarding the correct analysis of what has already been processed","At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is required to calculate such measures","['310', '126', '345', '147', '349']","<S sid =""310"" ssid = ""66"">This is a subset of the possible leftmost partial derivations with respect to the prefix string W. Since RV is produced by expanding only analyses on priority queue H;'  the set of complete trees consistent with the partial derivations on priority queue Ht is a subset of the set of complete trees consistent with the partial derivations on priority queue HT''  that is  the total probability mass represented by the priority queues is monotonically decreasing.</S><S sid =""126"" ssid = ""30"">When all of the parser operations have finished at a particular point in the string  the next word is predicted as follows: For each derivation in the beam  the headwords of the two topmost stack entries form a trigram with the conditioned word.</S><S sid =""345"" ssid = ""101"">This is not surprising  since our conditioning information is in many ways orthogonal to that of the trigram  insofar as it includes the probability mass of the derivations; in contrast  their model in some instances is very close to the trigram  by conditioning on two words in the prefix string  which may happen to be the two adjacent words.</S><S sid =""147"" ssid = ""51"">One way to estimate the probabilities of these rules is to annotate the heads onto the constituent labels in the training corpus and simply count the number of times particular productions occur (relative frequency estimation).</S><S sid =""349"" ssid = ""105"">One way to test this is the following: at each point in the sentence  calculate the conditional probability of each word in the vocabulary given the previous words  and sum them.'</S>",['Aim_Citation']
