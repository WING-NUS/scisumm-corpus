 The set of n-grams appearing in a model is sparse and we want to efficiently find their associated probabilities and backoff penalties 1 and b is the backoff penalty The code is opensource has minimal dependencies and offers both C++ and Java interfaces for integration In addition to the optimizations specific to each datastructure described in Section 2 we implement several general optimizations for language modeling Our code is thread-safe and integrated into the Moses cdec and Joshua translation systems The cost of storing these averages in bits is Because there are comparatively few unigrams we elected to store them byte-aligned and unquantized making every query faster 2008) is an open-source toolkit for building and querying language models The problem is to store these two values for a large and sparse set of n-grams in a way that makes queries efficient We have described two data structures for language modeling that achieve substantial reductions in time and memory cost Another option is the closedsource data structures from Sheffield (Guthrie and Hepple 2010) The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM We offer a state function s(wn1) = wn� where substring wn� is guaranteed to extend (to the right) in the same way that wn1 does for purposes of language modeling Minimal perfect hashing is used to find the index at which a quantized probability and possibly backoff are stored Backoff-smoothed models estimate this probability based on the observed entry with longest matching history wnf returning where the probability p(wn|wn−1 f ) and backoff penalties b(wn−1 i ) are given by an already-estimated model