 We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results As shown in Figure 1 the character-based perceptron is used as the inside-layer linear model and sends its output to the outside-layer However can the perceptron incorporate all the knowledge used in the outside-layer linear model?In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result a 4-gram POS language model functioning as the product of statetransition probabilities in HMM and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence According to Ng and Low (2004) the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM and usually behaves the best in the two tasks When we derive a candidate result from a word-POS pair p and a candidate q at prior position of p we calculate the scores of the word LM the POS LM the labelling probability and the generating probability Algorithm 2 Decoding algorithm We noticed that the templates related to word unigrams and bigrams bring to the feature space an enlargement much rapider than the character-base ones not to mention the higher-order grams such as trigrams or 4-grams