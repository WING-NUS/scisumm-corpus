Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P08-1102,C08-1049,0,2008,0,"Following Jiang et al (2008), we describe segmentation and Joint S& amp; T as below: For a given Chinese sentence appearing as a character sequence: C 1: n= C 1 C 2.","Following Jiang et al (2008), we describe segmentation and Joint S& amp; T as below: For a given Chinese sentence appearing as a character sequence: C 1: n= C 1 C 2.","'9','37','32','22'","<S sid=""9"" ssid=""5"">To segment and tag a character sequence, there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&amp;T).</S><S sid=""37"" ssid=""9"">C represents a Chinese character while the subscript of C indicates its position in the sentence relative to the current character (it has the subscript 0).</S><S sid=""32"" ssid=""4"">We trained a character-based perceptron for Chinese Joint S&amp;T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&amp;T.</S><S sid=""22"" ssid=""18"">2 Segmentation and POS Tagging Given a Chinese character sequence: while the segmentation and POS tagging result can be depicted as: Here, Ci (i = L.n) denotes Chinese character, ti (i = L.m) denotes POS tag, and Cl:r (l &lt; r) denotes character sequence ranges from Cl to Cr.</S>",
2,P08-1102,C08-1049,0,2008,0,"As described in Ng and Low (2004 )andJiang et al (2008), we use s indicating a single character word, while b, m and e indicating the be gin, middle and end of a word respectively","As described in Ng and Low (2004) and Jiang et al (2008), we use s indicating a single character word, while b, m and e indicating the be gin, middle and end of a word respectively","'25','37','35','87'","<S sid=""25"" ssid=""21"">According to Ng and Low (2004), the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S><S sid=""37"" ssid=""9"">C represents a Chinese character while the subscript of C indicates its position in the sentence relative to the current character (it has the subscript 0).</S><S sid=""35"" ssid=""7"">To compare with others conveniently, we excluded the ones forbidden by the close test regulation of SIGHAN, for example, Pu(C0), indicating whether character C0 is a punctuation.</S><S sid=""87"" ssid=""12"">Line 6 enumerates all POS&#8217;s for the word w spanning length l and ending at position i.</S>",
3,P08-1102,C08-1049,0,2008,0,plates called lexical-target in the column below areintroduced by Jiang et al (2008),plates called lexical-target in the column below are introduced by Jiang et al (2008),"'39','42','6','43'","<S sid=""39"" ssid=""11"">We called them non-lexical-target because predications derived from them can predicate without considering the current character C0.</S><S sid=""42"" ssid=""14"">As predications generated from such templates depend on the current character, we name these templates lexical-target.</S><S sid=""6"" ssid=""2"">Several models were introduced for these problems, for example, the Hidden Markov Model (HMM) (Rabiner, 1989), Maximum Entropy Model (ME) (Ratnaparkhi and Adwait, 1996), and Conditional Random Fields (CRFs) (Lafferty et al., 2001).</S><S sid=""43"" ssid=""15"">Note that the templates of Ng and Low (2004) have already contained some lexical-target ones.</S>",
4,P08-1102,P12-1110,0,2008,0,"For CTB-5, we refer to the split by Duan et al (2007) as CTB-5d, and to the split by Jiang et al (2008) as CTB-5j","For CTB-5, we refer to the split by Duan et al (2007) as CTB-5d, and to the split by Jiang et al (2008) as CTB-5j","'140','110','118','101'","<S sid=""140"" ssid=""3"">2006AA010108 (W. J., Q. L., and Y. L.), and by NSF ITR EIA-0205456 (L. H.).</S><S sid=""110"" ssid=""21"">Similar trend appeared in experiments of Ng and Low (2004), where they conducted experiments on CTB 3.0 and achieved Fmeasure 0.919 on Joint S&amp;T, a ratio of 96% to the F-measure 0.952 on segmentation.</S><S sid=""118"" ssid=""29"">We find that the cascaded model achieves a F-measure increment of about 0.5 points on segmentation and about 0.9 points on Joint S&amp;T, over the perceptron-only model POS+.</S><S sid=""101"" ssid=""12"">On the three corpora, it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S>",
5,P08-1102,D12-1126,0,2008,0,Jiang et al (2008) proposes a cascaded linear model for joint Chinese word segmentation and POS tagging,Jiang et al (2008) proposes a cascaded linear model for joint Chinese word segmentation and POS tagging,"'0','1','130','9'","<S sid=""0"">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S sid=""1"" ssid=""1"">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S sid=""130"" ssid=""1"">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S><S sid=""9"" ssid=""5"">To segment and tag a character sequence, there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&amp;T).</S>",
6,P08-1102,C10-1135,0,2008,0,"We use the feature templates the same as Jiang et al, (2008) to extract features form E model","We use the feature templates the same as Jiang et al, (2008) to extract features form E model","'36','34','33','46'","<S sid=""36"" ssid=""8"">All feature templates and their instances are shown in Table 1.</S><S sid=""34"" ssid=""6"">The feature templates we adopted are selected from those of Ng and Low (2004).</S><S sid=""33"" ssid=""5"">In following subsections, we describe the feature templates and the perceptron training algorithm.</S><S sid=""46"" ssid=""18"">Following Collins, we use a function GEN(x) generating all candidate results of an input x , a representation 4) mapping each training example (x, y) &#8712; X &#215; Y to a feature vector 4)(x, y) &#8712; Rd, and a parameter vector &#945;&#65533; &#8712; Rd corresponding to the feature vector. d means the dimension of the vector space, it equals to the amount of features in the model.</S>",
8,P08-1102,P12-1025,0,"Jiangetal., 2008a",0,"approach, where basic processing units are characters which compose words (Jiangetal., 2008a)","basic processing units are characters which compose words (Jiangetal., 2008a)","'5','86','12','48'","<S sid=""5"" ssid=""1"">Word segmentation and part-of-speech (POS) tagging are important tasks in computer processing of Chinese and other Asian languages.</S><S sid=""86"" ssid=""11"">Line 4 scans words of all possible lengths l (l = 1.. min(i, K), where i points to the current considering character).</S><S sid=""12"" ssid=""8"">Besides the usual character-based features, additional features dependent on POS&#8217;s or words can also be employed to improve the performance.</S><S sid=""48"" ssid=""20"">We used the algorithm depicted in Algorithm 1 to tune the parameter vector a.</S>",
9,P08-1102,C10-2096,0,2008b,0,"The solid lines show the 1-best result, which is wrong. Jiang et al (2008b) stress the problems in re ranking phase","The solid lines show the 1-best result, which is wrong. Jiang et al (2008b) stress the problems in re ranking phase","'85','88','100','25'","<S sid=""85"" ssid=""10"">Lines 3 &#8212; 11 generate a N-best list for each character position i.</S><S sid=""88"" ssid=""13"">Line 8 considers each candidate result in N-best list at prior position of the current word.</S><S sid=""100"" ssid=""11"">Test results listed in Table 2 shows that this model obtains higher accuracy than the best of SIGHAN Bakeoff 2 in three corpora (AS, CityU and MSR).</S><S sid=""25"" ssid=""21"">According to Ng and Low (2004), the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S>",
10,P08-1102,C10-2096,0,"Jiang et al, 2008a",0,"6.1.1 Baseline Forest-based System We first segment the Chinese sentences into the1-best segmentations using a state-of-the-art system (Jiang et al, 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results","We first segment the Chinese sentences into the 1-best segmentations using a state-of-the-art system (Jiang et al, 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results","'0','5','4','22'","<S sid=""0"">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S sid=""5"" ssid=""1"">Word segmentation and part-of-speech (POS) tagging are important tasks in computer processing of Chinese and other Asian languages.</S><S sid=""4"" ssid=""4"">On the Penn Chinese Treebank 5.0, we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.</S><S sid=""22"" ssid=""18"">2 Segmentation and POS Tagging Given a Chinese character sequence: while the segmentation and POS tagging result can be depicted as: Here, Ci (i = L.n) denotes Chinese character, ti (i = L.m) denotes POS tag, and Cl:r (l &lt; r) denotes character sequence ranges from Cl to Cr.</S>",
11,P08-1102,C10-2096,0,"Jiang et al, 2008a",0,"6.1.2 Lattice-forest SystemWe first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al, 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm","We first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al, 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm","'81','23','58','73'","<S sid=""81"" ssid=""6"">When we derive a candidate result from a word-POS pair p and a candidate q at prior position of p, we calculate the scores of the word LM, the POS LM, the labelling probability and the generating probability, Algorithm 2 Decoding algorithm. as well as the score of the perceptron model.</S><S sid=""23"" ssid=""19"">We can see that segmentation and POS tagging task is to divide a character sequence into several subsequences and label each of them a POS tag.</S><S sid=""58"" ssid=""9"">Instead of incorporating all features into the perceptron directly, we first trained the perceptron using character-based features, and several other sub-models using additional ones such as word or POS n-grams, then trained the outside-layer linear model using the outputs of these sub-models, including the perceptron.</S><S sid=""73"" ssid=""24"">For instance, if the word w appears N times in training corpus and is labelled as POS t for n times, the probability Pr(t|w) can be estimated by the formula below: The probability Pr(w|t) could be estimated through the same approach.</S>",
12,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"However, when we repeat the work of (Jiang et al, 2008), which reports to achieve the state-of-art performance in the data-sets that we adopt, it has been found that some features (e.g., C0) are unnoticeably trained several times in their model (which are implicitly generated from different feature templates used in the paper)","However, when we repeat the work of (Jiang et al, 2008), which reports to achieve the state-of-art performance in the data-sets that we adopt, it has been found that some features (e.g., C0) are unnoticeably trained several times in their model (which are implicitly generated from different feature templates used in the paper)","'34','8','116','58'","<S sid=""34"" ssid=""6"">The feature templates we adopted are selected from those of Ng and Low (2004).</S><S sid=""8"" ssid=""4"">Another widely used discriminative method is the perceptron algorithm (Collins, 2002), which achieves comparable performance to CRFs with much faster training, so we base this work on the perceptron.</S><S sid=""116"" ssid=""27"">In order to inspect how much improvement each feature brings into the cascaded model, every time we removed a feature while retaining others, then retrained the model and tested its performance on the test set.</S><S sid=""58"" ssid=""9"">Instead of incorporating all features into the perceptron directly, we first trained the perceptron using character-based features, and several other sub-models using additional ones such as word or POS n-grams, then trained the outside-layer linear model using the outputs of these sub-models, including the perceptron.</S>",
13,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"Unicode/CP936 1.1M/55K 104K/13K 0.035 Table 3: Corpus statistics for the second SIGHAN Bakeoff appears twice, which is generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al, 2008), with n=0, generates [C0C0])","Table 3: Corpus statistics for the second SIGHAN Bakeoff appears twice, which is generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al, 2008), with n=0, generates [C0C0])","'42','91','35','41'","<S sid=""42"" ssid=""14"">As predications generated from such templates depend on the current character, we name these templates lexical-target.</S><S sid=""91"" ssid=""2"">The first was conducted to test the performance of the perceptron on segmentation on the corpus from SIGHAN Bakeoff 2, including the Academia Sinica Corpus (AS), the Hong Kong City University Corpus (CityU), the Peking University Corpus (PKU) and the Microsoft Research Corpus (MSR).</S><S sid=""35"" ssid=""7"">To compare with others conveniently, we excluded the ones forbidden by the close test regulation of SIGHAN, for example, Pu(C0), indicating whether character C0 is a punctuation.</S><S sid=""41"" ssid=""13"">We add a field C0 to each template in the upper column, so that it can carry out predication according to not only the context but also the current character itself.</S>",
14,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"As all the features adopted in (Jiang et al, 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be? n?, at least in principle","As all the features adopted in (Jiang et al, 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be n, at least in principle","'34','36','16','50'","<S sid=""34"" ssid=""6"">The feature templates we adopted are selected from those of Ng and Low (2004).</S><S sid=""36"" ssid=""8"">All feature templates and their instances are shown in Table 1.</S><S sid=""16"" ssid=""12"">Shown in Figure 1, the cascaded model has a two-layer architecture, with a characterbased perceptron as the core combined with other real-valued features such as language models.</S><S sid=""50"" ssid=""1"">In theory, any useful knowledge can be incorporated into the perceptron directly, besides the characterbased features already adopted.</S>",
15,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"Inspired by (Jiang et al, 2008), we set the real d Although Table 5 has shown that the proposed all the value of C0 to be 2.0, the value of C-1C0anC0C1 to be 3.0, and the values of all other features to be 1.0 for the character-based discriminative-plus model","Inspired by (Jiang et al, 2008), we set the real d Although Table 5 has shown that the proposed all the value of C0 to be 2.0, the value of C-1C0anC0C1 to be 3.0, and the values of all other features to be 1.0 for the character-based discriminative-plus model","'36','37','35','39'","<S sid=""36"" ssid=""8"">All feature templates and their instances are shown in Table 1.</S><S sid=""37"" ssid=""9"">C represents a Chinese character while the subscript of C indicates its position in the sentence relative to the current character (it has the subscript 0).</S><S sid=""35"" ssid=""7"">To compare with others conveniently, we excluded the ones forbidden by the close test regulation of SIGHAN, for example, Pu(C0), indicating whether character C0 is a punctuation.</S><S sid=""39"" ssid=""11"">We called them non-lexical-target because predications derived from them can predicate without considering the current character C0.</S>",
17,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"Last, (Jiang et al, 2008) 5 adds repeated features implicitly based on (Ng and Low, 2004)","Last, (Jiang et al, 2008) 5 adds repeated features implicitly based on (Ng and Low, 2004)","'12','53','82','54'","<S sid=""12"" ssid=""8"">Besides the usual character-based features, additional features dependent on POS&#8217;s or words can also be employed to improve the performance.</S><S sid=""53"" ssid=""4"">Figure 2 shows the growing tendency of feature space with the introduction of these features as well as the character-based ones.</S><S sid=""82"" ssid=""7"">In addition, we add the score of the word count penalty as another feature to alleviate the tendency of LMs to favor shorter candidates.</S><S sid=""54"" ssid=""5"">We noticed that the templates related to word unigrams and bigrams bring to the feature space an enlargement much rapider than the character-base ones, not to mention the higher-order grams such as trigrams or 4-grams.</S>",
20,P08-1102,D12-1046,0,Jiang et al2008a,0,"Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a) ,perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b)","Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a), perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b)","'0','31','58','113'","<S sid=""0"">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S sid=""31"" ssid=""3"">The perceptron has been used in many NLP tasks, such as POS tagging (Collins, 2002), Chinese word segmentation (Ng and Low, 2004; Zhang and Clark, 2007) and so on.</S><S sid=""58"" ssid=""9"">Instead of incorporating all features into the perceptron directly, we first trained the perceptron using character-based features, and several other sub-models using additional ones such as word or POS n-grams, then trained the outside-layer linear model using the outputs of these sub-models, including the perceptron.</S><S sid=""113"" ssid=""24"">Besides this perceptron, other sub-models are trained and used as additional features of the outside-layer linear model.</S>",
