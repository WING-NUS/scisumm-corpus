 We use this corpus to explore the ability of the model both to infer similarities between vocabularies in different languages and to detect differences in topic emphasis between languages An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct non-comparable documents in multiple languages We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages) In order to simulate this scenario we create a set of variations of the EuroParl corpus by treating some documents as if they have no parallel/comparable texts â€“ i To evaluate this scenario we train PLTM on a set of document tuples from EuroParl infer topic distributions for a set of held-out documents and then measure our ability to align documents in one language with their translations in another language Although direct translations in multiple languages are relatively rare (in contrast with comparable documents) we use direct translations to explore the characteristics of the model Meanwhile massive collections of interlinked documents in dozens of languages such as Wikipedia are now widely available calling for tools that can characterize content in many languages We analyzed the characteristics of PLTM in comparison to monolingual LDA and demonstrated that it is possible to discover aligned topics The second corpus Wikipedia articles in twelve languages contains sets of documents that are not translations of one another but are very likely to be about similar concepts