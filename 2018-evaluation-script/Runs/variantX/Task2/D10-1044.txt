This is a straightforward technique that is arguably better suited to the adaptation task than the standard method of treating representative IN sentences as queries  then pooling the match results.This extends previous work on discriminative weighting by using a finer granularity  focusing on the properties of instances rather than corpus components  and using a simpler training procedure.It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.The second setting uses the news-related subcorpora for the NIST09 MT Chinese to English evaluation8 as IN  and the remaining NIST parallel Chinese/English corpora (UN  Hong Kong Laws  and Hong Kong Hansard) as OUT.(Thus the domain of the dev and test corpora matches IN.)We extend the Matsoukas et al approach in several ways.However  for multinomial models like our LMs and TMs  there is a one to one correspondence between instances and features  eg the correspondence between a phrase pair (s  t) and its conditional multinomial probability p(s1t).In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.It is difficult to directly compare the Matsoukas et al results with ours  since our out-of-domain corpus is homogeneous; given heterogeneous training data  however  it would be trivial to include Matsoukas-style identity features in our instance-weighting model.The dev and test sets were randomly chosen from the EMEA corpus.There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan  2007; Wu et al.  2005)  and on dynamically choosing a dev set (Xu et al.  2007).)  which precludes a single universal approach to adaptation.The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.We have not yet tried this.This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates  and po(s|t) is an instance-weighted model derived from the OUT corpus.It is also worth pointing out a connection with Daum´e’s (2007) work that splits each feature into domain-specific and general copies.Log-linear combination (loglin) improves on this in all cases  and also beats the pure IN system.Even when there is training data available in the domain of interest  there is often additional data from other domains that could in principle be used to improve performance.We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.This is a standard adaptation problem for SMT.There is a fairly large body of work on SMT adaptation.The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models)  and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples  with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.Somewhat surprisingly  there do not appear to be large systematic differences between linear and MAP combinations.Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.In future work we plan to try this approach with more competitive SMT systems  and to extend instance weighting to other standard SMT components such as the LM  lexical phrase weights  and lexicalized distortion.Section 5 covers relevant previous work on SMT adaptation  and section 6 concludes.The idea of distinguishing between general and domain-specific examples is due to Daum´e and Marcu (2006)  who used a maximum-entropy model with latent variables to capture the degree of specificity.This combination generalizes (2) and (3): we use either at = a to obtain a fixed-weight linear combination  or at = cI(t)/(cI(t) + 0) to obtain a MAP combination.Linear weights are difficult to incorporate into the standard MERT procedure because they are “hidden” within a top-level probability that represents the linear combination.1 Following previous work (Foster and Kuhn  2007)  we circumvent this problem by choosing weights to optimize corpus loglikelihood  which is roughly speaking the training criterion used by the LM and TM themselves.We have not explored this strategy.For the LM  adaptive weights are set as follows: where α is a weight vector containing an element αi for each domain (just IN and OUT in our case)  pi are the corresponding domain-specific models  and ˜p(w  h) is an empirical distribution from a targetlanguage training corpus—we used the IN dev set for this.The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.We used it to score all phrase pairs in the OUT table  in order to provide a feature for the instance-weighting model.Clearly  retaining the original frequencies is important for good performance  and globally smoothing the final weighted frequencies is crucial.The corpora for both settings are summarized in table 1.We used a standard one-pass phrase-based system (Koehn et al.  2003)  with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count.Its success depends on the two domains being relatively close  and on the OUT corpus not being so large as to overwhelm the contribution of IN.We do not adapt the alignment procedure for generating the phrase table from which the TM distributions are derived.The matching sentence pairs are then added to the IN corpus  and the system is re-trained.Discriminative Instance Weighting for Domain Adaptation in Statistical Machine TranslationWe introduce several new ideas.Finally  we incorporate the instance-weighting model into a general linear combination  and learn weights and mixing parameters simultaneously. where cλ(s  t) is a modified count for pair (s  t) in OUT  u(s|t) is a prior distribution  and y is a prior weight.Finally  we note that Jiang’s instance-weighting framework is broader than we have presented above  encompassing among other possibilities the use of unlabelled IN data  which is applicable to SMT settings where source-only IN corpora are available.The features are weighted within a logistic model to give an overall weight that is applied to the phrase pair’s frequency prior to making MAP-smoothed relative-frequency estimates (different weights are learned for each conditioning direction).First  we aim to explicitly characterize examples from OUT as belonging to general language or not.We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.Previous approaches have tried to find examples that are similar to the target domain.We used 22 features for the logistic weighting model  divided into two groups: one intended to reflect the degree to which a phrase pair belongs to general language  and one intended to capture similarity to the IN domain.Je voudrais pr´eciser  a` l’adresse du commissaire Liikanen  qu’il n’est pas ais´e de recourir aux tribunaux nationaux.— I would also like to point out to commissioner Liikanen that it is not easy to take a matter to a national court.The general-language features have a slight advantage over the similarity features  and both are better than the SVM feature.Apart from MERT difficulties  a conceptual problem with log-linear combination is that it multiplies feature probabilities  essentially forcing different features to agree on high-scoring candidates.It is difficult when IN and OUT are dissimilar  as they are in the cases we study.6One of our experimental settings lacks document boundaries  and we used this approximation in both settings for consistency.