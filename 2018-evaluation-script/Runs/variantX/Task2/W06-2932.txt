N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.With the availability of resources such as the Penn WSJ Treebank  much of the focus in the parsing community had been on producing syntactic representations based on phrase-structure.However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'ˇcuk  1988).Including rich morphology features naturally helped with highly inflected languages  in particular Spanish  Arabic  Turkish  Slovene and to a lesser extent Dutch and Portuguese.The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.This system is primarily based on the parsing models described by McDonald and Pereira (2006).Even with this improvement  the labeling of verb dependents remains the highest source of error.In fact  for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).For example  in the test sentence Lo que decia Mae West de si misma podriamos decirlo tambi´en los hombres:...  decia’s head is given as decirlo  although the main verbs of relative clauses are normally dependent on what the relative modifies  in this case the article Lo.Consider a proposed dependency of a dependent xj on the head xi  each with morphological features Mj and Mi respectively.Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.Dependency graphs also encode much of the deep syntactic information needed for further processing.We evaluate this parser on a diverse set of 13 languages using data provided by the CoNLL-X shared-task organizers (Buchholz et al.  2006; Hajiˇc et al.  2004; Simov et al.  2005; Simov and Osenova  2003; Chen et al.  2003; B¨ohmov´a et al.  2003; Kromann  2003; van der Beek et al.  2002; Brants et al.  2002; Kawata and Bartels  2000; Afonso et al.  2002; Dˇzeroski et al.  2006; Civit Torruella and MartiAntonin  2002; Nilsson et al.  2005; Oflazer et al.  2003; Atalay et al.  2003).However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.These features play the obvious role of explicitly modeling consistencies and commonalities between a head and its dependents in terms of attributes like gender  case  or number.Derived morphological features improved accuracy in all these languages by 1-3% absolute.The current system simply includes all morphological bi-gram features.Furthermore  these results show that a twostage system can achieve a relatively high performance.It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.For example if we train on 200  400  800 and the full training set  labeled accuracies are 54%  60%  62% and 67%.The fact that Arabic has only 1500 training instances might also be problematic.These results report the average labeled and unlabeled precision for the 10 languages with the smallest training sets.We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216–220  New York City  June 2006. c�2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.These weaknesses are not surprising  since these decisions encode the more global aspects of sentence structure: arrangement of clauses and adverbial dependents in multi-clause sentences  and prepositional phrase attachment.To model this we treat the labeling of the edges (i  j1)  ...   (i  jM) as a sequence labeling problem  We use a first-order Markov factorization of the score s(l(i jm)  l(i jm�1)  i  y  x) in which each factor is the score of labeling the adjacent edges (i  jm) and (i  jm−1) in the tree y.Note that many of these features are beyond the scope of the edge based factorizations of the unlabeled parser.In doing this preliminary analysis  we noticed some inconsistencies in the reference dependency structures.We need to look more carefully at verb features that may be useful here  in particular features that distinguish finite and non-finite forms.However  if we only allow projective parses  do not use morphological features and label edges with a simple atomic classifier  the overall drop in performance becomes significant (row 5 versus row 1).A quick look at unlabeled attachment accuracies indicate that errors in Arabic parsing are the most common across all languages: prepositions (62%)  conjunctions (69%) and to a lesser extent verbs (73%).Multilingual Dependency Analysis with a Two-Stage Discriminative ParserIs this the left/rightmost dependent for the head?Nivre (2005) gives an introduction to dependency representations of sentences and recent developments in dependency parsing strategies.The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).