In our trials  we used the unigram  with a very small mixing coefficient: following words since the denominator is zero.Note that the model perplexity and parser accuracy are quite similarly affected  but that the interpolated perplexity remained far below the trigram baseline  even with extremely narrow beams.Since each word is (almost certainly  because of our pruning strategy) losing some probability mass  the probability model is not &quot;proper &quot;â€”the sum of the probabilities over the vocabulary is less than one.However  when we interpolate with the trigram  we see that the additional improvement is greater than the one they experienced.Perhaps some compromise between the fully connected structures and extreme underspecification will yield an efficiency improvement.Future work will include more substantial word recognition experiments.There are a couple of things to notice from these results.These perplexity improvements are particularly promising  because the parser is providing information that is  in some sense  orthogonal to the information provided by a trigram model  as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.The fact that the efficiency was improved more than the accuracy in this case (as was also seen in Figure 5)  seems to indicate that this additional information is causing the distribution to become more peaked  so that fewer analyses are making it into the beam.One way to test this is the following: at each point in the sentence  calculate the conditional probability of each word in the vocabulary given the previous words  and sum them.'Table 4 compares the perplexity of our model with Chelba and Jelinek (1998a  1998b) on the same training and testing corpora.We obtained the training and testing corpora from them (which we will denote C&J corpus)  and also created intermediate corpora  upon which only the first two modifications were carried out (which we will denote no punct).Our parsing model's perplexity improves upon their first result fairly substantially  but is only slightly better than their second result.'The small size of our training data  as well as the fact that we are rescoring n-best lists  rather than working directly on lattices  makes comparison with the other models not particularly informative.By definition  a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1).Let Ht be the priority queue H  before any processing has begun with word w  in the look-ahead.From this set of measures  we will also include the crossing bracket scores: average crossing brackets (CB)  percentage of sentences with no crossing brackets (0 CB)  and the percentage of sentences with two crossing brackets or fewer (< 2 CB).This is an incremental parser with a pruning strategy and no backtracking.In order to get a sense of whether these perplexity reduction results can translate to improvement in a speech recognition task  we performed a very small preliminary experiment on n-best lists.The trigram model was also trained on Sections 00-20 of the C&J corpus.Interestingly  at the word where the failure occurred  the sum of the probabilities was 0.9301.One complicating issue has to do with the tokenization in the Penn Treebank versus that in the HUB1 lattices.The hope  however  is that the improved parsing model provided by our conditional probability model will cause the distribution over structures to be more peaked  thus enabling us to capture more of the total probability mass  and making this a fairly snug upper bound on the perplexity.Their modifications included: (i) removing orthographic cues to structure (e.g.  punctuation); (ii) replacing all numbers with the single token N; and (iii) closing the vocabulary at 10 000  replacing all other words with the UNK token.These results are particularly remarkable  given that we did not build our model as a language model per se  but rather as a parsing model.The point of this small experiment was to see if our parsing model could provide useful information even in the case that recognition errors occur  as opposed to the (generally) fully grammatical strings upon which the perplexity results were obtained.Interestingly  conditioning all POS expansions on two c-commanding heads made no difference in accuracy compared to conditioning only leftmost POS expansions on a single c-commanding head; but it did improve the efficiency.We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice  along with the acoustic and trigram scores.'The base beam factor that we have used to this point is 10'  which is quite wide.Unlike the Roark and Johnson parser  however  our coverage did not substantially drop as the amount of conditioning information increased  and in some cases  coverage improved slightly.Recall is the number of common constituents in GOLD and TEST divided by the number of constituents in GOLD.Our observed times look polynomial  which is to be expected given our pruning strategy: the denser the competitors within a narrow probability range of the best analysis  the more time will be spent working on these competitors; and the farther along in the sentence  the more chance for ambiguities that can lead to such a situation.For example  in Figure 1(a)  there is a VP that spans the words &quot;chased the ball&quot;.What is perhaps surprising is that the difference is not greater.In such a case  the parser fails to return a complete parse.By including these nodes (which are in the original annotation of the Penn Treebank)  we may be able to bring certain long-distance dependencies into a local focus.Also  the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram  and lead to better off-line language models than those that we have presented here.In fact  left-corner parsing can be simulated by a top-down parser by transforming the grammar  as was done in Roark and Johnson (1999)  and so an approach very similar to the one outlined here could be used in that case.For our model and the Treebank trigram model  the LM weight that resulted in the lowest error rates is given.With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.We built an interpolated trigram model to serve as a baseline (as they did)  and also interpolated our model's perplexity with the trigram  using the same mixing coefficient as they did in their trials (taking 36 percent of the estimate from the trigram).'A constituent for evaluation purposes consists of a label (e.g.  NP) and a span (beginning and ending word positions).