This information is readily available in TRIE where adjacent records with equal pointers indicate no further extension of context is possible.The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.Chris Dyer integrated the code into cdec.Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.Alon Lavie advised on this work.These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.Juri Ganitkevitch answered questions about Joshua.For speed  we plan to implement the direct-mapped cache from BerkeleyLM.Nicola Bertoldi and Marcello Federico assisted with IRSTLM.Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.Generalizing state minimization  the model could also provide explicit bounds on probability for both backward and forward extension.For example  syntactic decoders (Koehn et al.  2007; Dyer et al.  2010; Li et al.  2009) perform dynamic programming parametrized by both backward- and forward-looking state.We elected run Moses single-threaded to minimize the impact of RandLMâ€™s cache on memory use.For even larger models  storing counts (Talbot and Osborne  2007; Pauls and Klein  2011; Guthrie and Hepple  2010) is a possibility.While we have minimized forward-looking state in Section 4.1  machine translation systems could also benefit by minimizing backward-looking state.The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.Much could be done to further reduce memory consumption.The code is opensource  has minimal dependencies  and offers both C++ and Java interfaces for integration.0750271 and by the DARPA GALE program.If they knew that the first four words in a hypergraph node would never extend to the left and form a 5-gram  then three or even fewer words could be kept in the backward state.We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.Quantization can be improved by jointly encoding probability and backoff.However  the point of RandLM is to scale to even larger data  compensating for this loss in quality.