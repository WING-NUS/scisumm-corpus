A Hebrew surface token may have several readings  each of which corresponding to a sequence of segments and their corresponding PoS tags.Each connected path (l1 ... lk) E L corresponds to one morphological segmentation possibility of W. The Parser Given a sequence of input tokens W = w1 ... wn and a morphological analyzer  we look for the most probable parse tree π s.t.On the surface  our model may seem as a special case of Cohen and Smith in which α = 0.Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.Removing the leaves from the resulting tree yields a parse for L under G  with the desired probabilities.In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.The current work treats both segmental and super-segmental phenomena  yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg  2008).To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).Table 2 compares the performance of our system on the setup of Cohen and Smith (2007) to the best results reported by them for the same tasks.The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.To facilitate the comparison of our results to those reported by (Cohen and Smith  2007) we use their data set in which 177 empty and “malformed”7 were removed.Tsarfaty (2006) was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5000 sentences treebank.Furthermore  the combination of pruning and vertical markovization of the grammar outperforms the Oracle results reported by Cohen and Smith.Thus our proposed model is a proper model assigning probability mass to all (7r  L) pairs  where 7r is a parse tree and L is the one and only lattice that a sequence of characters (and spaces) W over our alpha-beth gives rise to.Tsarfaty and Sima’an (2007) have reported state-of-the-art results on Hebrew unlexicalized parsing (74.41%) albeit assuming oracle morphological segmentation.The accuracy results for segmentation  tagging and parsing using our different models and our standard data split are summarized in Table 1.This is by now a fairly standard representation for multiple morphological segmentation of Hebrew utterances (Adler  2001; Bar-Haim et al.  2005; Smith et al.  2005; Cohen and Smith  2007; Adler  2007).However  there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context.Morphological disambiguators that consider a token in context (an utterance) and propose the most likely morphological analysis of an utterance (including segmentation) were presented by Bar-Haim et al. (2005)  Adler and Elhadad (2006)  Shacham and Wintner (2007)  and achieved good results (the best segmentation result so far is around 98%).SEGTok(noH) is the segmentation accuracy ignoring mistakes involving the implicit definite article h.11 To evaluate our performance on the tagging task we report CPOS and FPOS corresponding to coarse- and fine-grained PoS tagging results (F1) measure.We report the F1 value of both measures.Hence  we take the probability of the event fmnh analyzed as REL VB to be This means that we generate f and mnh independently depending on their corresponding PoS tags  and the context (as well as the syntactic relation between the two) is modeled via the derivation resulting in a sequence REL VB spanning the form fmnh. based on linear context.M(wi) = Li).Furthermore  the systematic way in which particles are prefixed to one another and onto an open-class category gives rise to a distinct sort of morphological ambiguity: space-delimited tokens may be ambiguous between several different segmentation possibilities.The relativizer f(“that”) for example  may attach to an arbitrarily long relative clause that goes beyond token boundaries.We use double-circles to indicate the space-delimited token boundaries.The form mnh itself can be read as at least three different verbs (“counted”  “appointed”  “was appointed”)  a noun (“a portion”)  and a possessed noun (“her kind”).Secondly  some segments in a proposed segment sequence may in fact be seen lexical events  i.e.  for some p tag Prf(p —* (s  p)) > 0  while other segments have never been observed as a lexical event before.Evaluating parsing results in our joint framework  as argued by Tsarfaty (2006)  is not trivial under the joint disambiguation task  as the hypothesized yield need not coincide with the correct one.In addition  as the CRF and PCFG look at similar sorts of information from within two inherently different models  they are far from independent and optimizing their product is meaningless.The possible analyses of a surface token pose constraints on the analyses of specific segments.Such tag sequences are often treated as “complex tags” (e.g.Such analyzers propose multiple segmentation possibilities and their corresponding analyses for a token in isolation but have no means to determine the most likely ones.We further thank Khalil Simaan (ILLCUvA) for his careful advise concerning the formal details of the proposal.The Grammar Our parser looks for the most likely tree spanning a single path through the lattice of which the yield is a sequence of lexemes.In the current work morphological analyses and lexical probabilities are derived from a small Treebank  which is by no means the best way to go.From now on all lattice arcs are tagged segments and the assignment of probability P(p —* (s  p)) to lattice arcs proceeds as usual.4 A rather pathological case is when our lexical heuristics prune away all segmentation possibilities and we remain with an empty lattice.The input for the segmentation task is however highly ambiguous for Semitic languages  and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al.  2007; Adler and Elhadad  2006).For all grammars  we use fine-grained PoS tags indicating various morphological features annotated therein.Acknowledgments We thank Meni Adler and Michael Elhadad (BGU) for helpful comments and discussion.We use a patched version of BitPar allowing for direct input of probabilities instead of counts.Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth  ‘tokens’) that constitute the unanalyzed surface forms (utterances).Furthermore  some of the arcs represent lexemes not present in the input tokens (e.g. h/DT  fl/POS)  however these are parts of valid analyses of the token (cf. super-segmental morphology section 2).Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.Many morphological decisions are based on long distance dependencies  and when the global syntactic evidence disagrees with evidence based on local linear context  the two models compete with one another  despite the fact that the PCFG takes also local context into account.A compatible view is presented by Charniak et al. (1996) who consider the kind of probabilities a generative parser should get from a PoS tagger  and concludes that these should be P(w|t) “and nothing fancier”.3 In our setting  therefore  the Lattice is not used to induce a probability distribution on a linear context  but rather  it is used as a common-denominator of state-indexation of all segmentations possibilities of a surface form.In addition we report for each model its performance on goldsegmented input (GS) to indicate the upper bound 11Overt definiteness errors may be seen as a wrong feature rather than as wrong constituent and it is by now an accepted standard to report accuracy with and without such errors. for the grammars’ performance on the parsing task.Segmental morphology Hebrew consists of seven particles m(“from”) f(“when”/“who”/“that”) h(“the”) w(“and”) k(“like”) l(“to”) and b(“in”). which may never appear in isolation and must always attach as prefixes to the following open-class category item we refer to as stem.When a comparison against previous results requires additional pre-processing  we state it explicitly to allow for the reader to replicate the reported results.The addition of vertical markovization enables non-pruned models to outperform all previously reported re12Cohen and Smith (2007) make use of a parameter (α) which is tuned separately for each of the tasks.