But consider Figure 4: (a) is headed by borders  but states needs to be extracted; in (b)  the quantifier no is syntactically dominated by the head verb borders but needs to take wider scope.Such a z defines a constraint satisfaction problem (CSP) with nodes as variables.Then higher up in the tree  we invoke it with an execute relation Xi to create the desired semantic scope.2 This mark-execute construct acts non-locally  so to maintain compositionality  we must augment the denotation d = JzKw to include any information about the marked nodes in z that can be accessed by an execute relation later on.The logical forms in this framework are trees  which is desirable for two reasons: (i) they parallel syntactic dependency trees  which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.In DCS  we start with lexical triggers  which are 6 Conclusion more basic than CCG lexical entries.Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.Let z be a DCS tree.Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.GEO has 48 non-value predicates and JOBS has 26.Extraction allows us to return the set of consistent values of a marked non-root node.Formally: Definition 1 (DCS trees) Let Z be the set of DCS trees  where each z ∈ Z consists of (i) a predicate for each child i  the ji-th component of v must equal the j'i-th component of some t in the child’s denotation (t ∈ JciKw).It is impossible to represent the semantics of this phrase with just a CSP  so we introduce a new aggregate relation  notated E. Consider a tree hE:ci  whose root is connected to a child c via E. If the denotation of c is a set of values s  the parent’s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.However  training on just these examples is enough to improve the parameters  and this 29% increases to 66% and then to 95% over the next few iterations.There are three cases: Extraction (d.ri = E) In the basic version  the denotation of a tree was always the set of consistent values of the root node.Our features as soft preferences.The filtering function F rules out improperly-typed trees such as hcity; 00:hstateii.For JOBS  if we use the standard Jobs database  close to half the y’s are empty  which makes it uninteresting.We now turn to the task of mapping natural language For the example in Figure 4(b)  the de- utterances to DCS trees.As a running example  consider x = city that is in California and z = hcity; 11:hloc; 21:hCAiii  where city triggers city and California triggers CA.Our first question is: given notation of the DCS tree before execution is an utterance x  what trees z ∈ Z are permissible?Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular.We want to induce latent logical forms z (and parameters 0) given only question-answer pairs (x  y)  which is much cheaper to obtain than (x  z) pairs.In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.Let P be a set of predicates (e.g.  state  count ∈ P)  which are just symbols.Join The join of two denotations d and d' with respect to components j and j' (∗ means all components) is formed by concatenating all arrays a of d with all compatible arrays a' of d'  where compatibility means a1j = a'1j0.Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms  we hope to use our without becoming mired in the semantic formalism. techniques in developing even more accurate and Quantifiers and superlatives significantly compli- broader-coverage language understanding systems. cate scoping in lambda calculus  and often type rais- Acknowledgments We thank Luke Zettlemoyer ing needs to be employed.To California cities)  and it also allows us to underspecify L. In particular  our L will not include verbs or prepositions; rather  we rely on the predicates corresponding to those words to be triggered by traces.Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.For each data predicate p (e.g.  language)  we add each possible tuple (e.g.  (job37  Java)) to w(p) independently with probability 0.8.However  in order to learn  we need to sum over {z ∈ ZL(x) : JzKw = y}  and unfortunately  the additional constraint JzKw = y does not factorize.We represent logical forms z as labeled trees  induced automatically from (x  y) pairs.We also define an augmented lexicon L+ which includes a prototype word x for each predicate appearing in (iii) above (e.g.  (large  size))  which cancels the predicates triggered by x’s POS tag.On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.This option is not available to us since we do not have annotated logical forms  so we must instead rely on lexical triggers to define the search space.However  we still model the logical form (now as a latent variable) to capture the complexities of language.Intuitions How is our system learning?Our learning algorithm alternates between (i) using the current parameters θ to generate the K-best set ˜ZL θ(x) for each training example x  and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.Table 2 shows that our system using lexical triggers L (henceforth  DCS) outperforms SEMRESP (78.9% over 73.2%).Feedback from the context; for example  the lexical entry for borders world has been used to guide both syntactic parsing is S\NP/NP : Ay.Ax.border(x  y)  which means (Schuler  2003) and semantic parsing (Popescu et borders looks right for the first argument and left al.  2003; Clarke et al.  2010).Formally  extraction simply moves the i-th column to the front: Xi(d) = d[i  −(i  ø)]{α1 = ø}.Formally  let ˜O(θ  θ') be the objective function O(θ) with ZL(x) ˜ZL θI(x).The tree structure still enables us to compute denotations efficiently based on (1) and (2).The full definition of join is as follows: Aggregate The aggregate operation takes a denotation and forms a set out of the tuples in the first column for each setting of the rest of the columns: Now we turn to the mark (M) and execute (Xi) operations  which handles the divergence between syntactic and semantic scope.The restriction to present (for example  [in  loc] has high weight). trees is similar to economical DRT (Bos  2009).For example  consider the phrase number of major cities  and suppose that number corresponds to the count predicate.During development  we further held out a random 30% of the training sets for validation.Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.