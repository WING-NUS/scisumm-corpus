Our work is closest to that of Yarowsky and Ngai (2001)  but differs in two important ways.However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.The availability of these resources guided our selection of foreign languages.We use a squared loss to penalize neighboring vertices that have different label distributions: kqi − qjk2 = Ey(qi(y) − qj(y))2  and additionally regularize the label distributions towards the uniform distribution U over all possible labels Y.Finally  note that while most feature concepts are lexicalized  others  such as the suffix concept  are not.The following three sections elaborate these different stages is more detail.Fortunately  performance was stable across various values  and we were able to use the same hyperparameters for all languages.As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.So far the graph has been completely unlabeled.They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value τ: We describe how we choose τ in §6.4.In general  the neighborhoods can be more diverse and we allow a soft label distribution over the vertices.In other words  the set of hidden states F was chosen to be the fine set of treebank tags.For monolingual treebank data we relied on the CoNLL-X and CoNLL-2007 shared tasks on dependency parsing (Buchholz and Marsi  2006; Nivre et al.  2007).Examining the word fidanzato for the “No LP” and “With LP” models is particularly instructive.7).Overall  it gives improvements ranging from 1.1% for German to 14.7% for Italian  for an average improvement of 8.3% over the unsupervised feature-HMM model.The function A : F —* C maps from the language specific fine-grained tagset F to the coarser universal tagset C and is described in detail in §6.2: Note that when tx(y) = 1 the feature value is 0 and has no effect on the model  while its value is −oc when tx(y) = 0 and constrains the HMM’s state space.This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.In the label propagation stage  we propagate the automatic English tags to the aligned Italian trigram types  followed by further propagation solely among the Italian vertices. the Italian vertices are connected to an automatically labeled English vertex.However  we needed to restrict ourselves to these languages in order to be able to evaluate the performance of our approach.Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.For each language  we took the same number of sentences from the bitext as there are in its treebank  and trained a supervised feature-HMM.Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.By aggregating the POS labels of the English tokens to types  we can generate label distributions for the English vertices.Note that because we extracted only high-confidence alignments  many foreign vertices will not be connected to any English vertices.The focus of this work is on building POS taggers for foreign languages  assuming that we have an English POS tagger and some parallel text between the two languages.For all languages  the vocabulary sizes increase by several thousand words.As expected  the vanilla HMM trained with EM performs the worst.Furthermore  we do not connect the English vertices to each other  but only to foreign language vertices.4 The graph vertices are extracted from the different sides of a parallel corpus (De  Df) and an additional unlabeled monolingual foreign corpus Ff  which will be used later for training.Label propagation is used to propagate these tags inwards and results in tag distributions for the middle word of each Italian trigram.Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.Across eight European languages  our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline  and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.Given this similarity function  we define a nearest neighbor graph  where the edge weight for the n most similar vertices is set to the value of the similarity function and to 0 for all other vertices.We were intentionally lenient with our baselines: bilingual information by projecting POS tags directly across alignments in the parallel data.Because all English vertices are going to be labeled  we do not need to disambiguate them by embedding them in trigrams.Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections  and bridge the gap between purely supervised and unsupervised POS tagging models.Label propagation can then be used to transfer the labels to the peripheral foreign vertices (i.e. the ones adjacent to the English vertices) first  and then among all of the foreign vertices (§4).For a sentence x and a state sequence z  a first order Markov model defines a distribution: (9) where Val(X) corresponds to the entire vocabulary.When extracting the vector t  used to compute the constraint feature from the graph  we tried three threshold values for r (see Eq.We would also like to thank Amarnag Subramanya for helping us with the implementation of label propagation and Shankar Kumar for access to the parallel data.Based on these high-confidence alignments we can extract tuples of the form [u H v]  where u is a foreign trigram type  whose middle word aligns to an English word type v. Our bilingual similarity function then sets the edge weights in proportion to these tuple counts.Because we don’t have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.In the first stage  we run a single step of label propagation  which transfers the label distributions from the English vertices to the connected foreign language vertices (say  Vf�) at the periphery of the graph.The POS distributions over the foreign trigram types are used as features to learn a better unsupervised POS tagger (§5).All features were conjoined with the state z.