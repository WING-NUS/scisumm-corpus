Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P08-1102,C08-1049,0,2008,0,"Following Jiang et al (2008), we describe segmentation and Joint S& amp; T as below: For a given Chinese sentence appearing as a character sequence: C 1: n= C 1 C 2.","Following Jiang et al (2008), we describe segmentation and Joint S& amp; T as below: For a given Chinese sentence appearing as a character sequence: C 1: n= C 1 C 2.","['120', '58', '136', '31', '134']","<S sid =""120"" ssid = ""31"">Without the perceptron  the cascaded model (if we can still call it “cascaded”) performs poorly on both segmentation and Joint S&T.</S><S sid =""58"" ssid = ""9"">Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.</S><S sid =""136"" ssid = ""7"">How can we utilize these knowledge sources effectively?</S><S sid =""31"" ssid = ""3"">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S><S sid =""134"" ssid = ""5"">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S>",['Implication_Citation']
2,P08-1102,C08-1049,0,2008,0,"As described in Ng and Low (2004 )andJiang et al (2008), we use s indicating a single character word, while b, m and e indicating the be gin, middle and end of a word respectively","As described in Ng and Low (2004) and Jiang et al (2008), we use s indicating a single character word, while b, m and e indicating the be gin, middle and end of a word respectively","['68', '117', '93', '18', '136']","<S sid =""68"" ssid = ""19"">It is an important measure of fluency of the translation in SMT.</S><S sid =""117"" ssid = ""28"">Table 4 shows experiments results.</S><S sid =""93"" ssid = ""4"">In all experiments  we use the averaged parameters for the perceptrons  and F-measure as the accuracy measure.</S><S sid =""18"" ssid = ""14"">In this architecture  knowledge sources that are intractable to incorporate into the perceptron  can be easily incorporated into the outside linear model.</S><S sid =""136"" ssid = ""7"">How can we utilize these knowledge sources effectively?</S>",['Implication_Citation']
3,P08-1102,C08-1049,0,2008,0,plates called lexical-target in the column below areintroduced by Jiang et al (2008),plates called lexical-target in the column below are introduced by Jiang et al (2008),"['45', '78', '79', '7', '46']","<S sid =""45"" ssid = ""17"">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S><S sid =""78"" ssid = ""3"">Given a Chinese character sequence C1:n  the decoding procedure can proceed in a left-right fashion with a dynamic programming approach.</S><S sid =""79"" ssid = ""4"">By maintaining a stack of size N at each position i of the sequence  we can preserve the top N best candidate labelled results of subsequence C1:i during decoding.</S><S sid =""7"" ssid = ""3"">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S><S sid =""46"" ssid = ""18"">Following Collins  we use a function GEN(x) generating all candidate results of an input x   a representation 4) mapping each training example (x  y) ∈ X × Y to a feature vector 4)(x  y) ∈ Rd  and a parameter vector α� ∈ Rd corresponding to the feature vector. d means the dimension of the vector space  it equals to the amount of features in the model.</S>",['Results_Citation']
4,P08-1102,P12-1110,0,2008,0,"For CTB-5, we refer to the split by Duan et al (2007) as CTB-5d, and to the split by Jiang et al (2008) as CTB-5j","For CTB-5, we refer to the split by Duan et al (2007) as CTB-5d, and to the split by Jiang et al (2008) as CTB-5j","['133', '65', '101', '95', '58']","<S sid =""133"" ssid = ""4"">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S><S sid =""65"" ssid = ""16"">As shown in Figure 1  the character-based perceptron is used as the inside-layer linear model and sends its output to the outside-layer.</S><S sid =""101"" ssid = ""12"">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S><S sid =""95"" ssid = ""6"">For convenience of comparing with others  we focus only on the close test  which means that any extra resource is forbidden except the designated training corpus.</S><S sid =""58"" ssid = ""9"">Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.</S>",['Method_Citation']
5,P08-1102,D12-1126,0,2008,0,Jiang et al (2008) proposes a cascaded linear model for joint Chinese word segmentation and POS tagging,Jiang et al (2008) proposes a cascaded linear model for joint Chinese word segmentation and POS tagging,"['101', '47', '95', '61', '136']","<S sid =""101"" ssid = ""12"">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S><S sid =""47"" ssid = ""19"">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid =""95"" ssid = ""6"">For convenience of comparing with others  we focus only on the close test  which means that any extra resource is forbidden except the designated training corpus.</S><S sid =""61"" ssid = ""12"">In this layer  each knowledge source is treated as a feature with a corresponding weight denoting its relative importance.</S><S sid =""136"" ssid = ""7"">How can we utilize these knowledge sources effectively?</S>",['Method_Citation']
6,P08-1102,C10-1135,0,2008,0,"We use the feature templates the same as Jiang et al, (2008) to extract features form E model","We use the feature templates the same as Jiang et al, (2008) to extract features form E model","['68', '49', '71', '92', '82']","<S sid =""68"" ssid = ""19"">It is an important measure of fluency of the translation in SMT.</S><S sid =""49"" ssid = ""21"">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S><S sid =""71"" ssid = ""22"">Using W = w1:m to denote the word sequence  T = t1:m to denote the corresponding POS sequence  P (T |W) to denote the probability that W is labelled as T  and P(W|T) to denote the probability that T generates W  we can define the cooccurrence model as follows: λwt and λtw denote the corresponding weights of the two components.</S><S sid =""92"" ssid = ""3"">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.</S><S sid =""82"" ssid = ""7"">In addition  we add the score of the word count penalty as another feature to alleviate the tendency of LMs to favor shorter candidates.</S>",['Method_Citation']
8,P08-1102,P12-1025,0,"Jiangetal., 2008a",0,"approach, where basic processing units are characters which compose words (Jiangetal., 2008a)","basic processing units are characters which compose words (Jiangetal., 2008a)","['38', '58', '40', '43', '25']","<S sid =""38"" ssid = ""10"">Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target.</S><S sid =""58"" ssid = ""9"">Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.</S><S sid =""40"" ssid = ""12"">Templates in the column below are expanded from the upper ones.</S><S sid =""43"" ssid = ""15"">Note that the templates of Ng and Low (2004) have already contained some lexical-target ones.</S><S sid =""25"" ssid = ""21"">According to Ng and Low (2004)  the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S>",['Method_Citation']
9,P08-1102,C10-2096,0,2008b,0,"The solid lines show the 1-best result, which is wrong. Jiang et al (2008b) stress the problems in re ranking phase","The solid lines show the 1-best result, which is wrong. Jiang et al (2008b) stress the problems in re ranking phase","['93', '57', '136', '40', '17']","<S sid =""93"" ssid = ""4"">In all experiments  we use the averaged parameters for the perceptrons  and F-measure as the accuracy measure.</S><S sid =""57"" ssid = ""8"">It has a two-layer architecture  with a perceptron as the core and another linear model as the outside-layer.</S><S sid =""136"" ssid = ""7"">How can we utilize these knowledge sources effectively?</S><S sid =""40"" ssid = ""12"">Templates in the column below are expanded from the upper ones.</S><S sid =""17"" ssid = ""13"">We will describe it in detail in Section 4.</S>",['Method_Citation']
10,P08-1102,C10-2096,0,"Jiang et al, 2008a",0,"6.1.1 Baseline Forest-based System We first segment the Chinese sentences into the1-best segmentations using a state-of-the-art system (Jiang et al, 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results","We first segment the Chinese sentences into the 1-best segmentations using a state-of-the-art system (Jiang et al, 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results","['101', '68', '33', '134', '96']","<S sid =""101"" ssid = ""12"">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S><S sid =""68"" ssid = ""19"">It is an important measure of fluency of the translation in SMT.</S><S sid =""33"" ssid = ""5"">In following subsections  we describe the feature templates and the perceptron training algorithm.</S><S sid =""134"" ssid = ""5"">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S><S sid =""96"" ssid = ""7"">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus  we randomly chosen 2  000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84  294 sentences)  then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S>",['Method_Citation']
11,P08-1102,C10-2096,0,"Jiang et al, 2008a",0,"6.1.2 Lattice-forest SystemWe first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al, 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm","We first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al, 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm","['68', '89', '9', '79', '104']","<S sid =""68"" ssid = ""19"">It is an important measure of fluency of the translation in SMT.</S><S sid =""89"" ssid = ""14"">Function D derives the candidate result from the word-POS pair p and the candidate q at prior position of p.</S><S sid =""9"" ssid = ""5"">To segment and tag a character sequence  there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T).</S><S sid =""79"" ssid = ""4"">By maintaining a stack of size N at each position i of the sequence  we can preserve the top N best candidate labelled results of subsequence C1:i during decoding.</S><S sid =""104"" ssid = ""15"">According to the usual practice in syntactic analysis  we choose chapters 1 − 260 (18074 sentences) as training set  chapter 271 − 300 (348 sentences) as test set and chapter 301 − 325 (350 sentences) as development set.</S>",['Implication_Citation']
12,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"However, when we repeat the work of (Jiang et al, 2008), which reports to achieve the state-of-art performance in the data-sets that we adopt, it has been found that some features (e.g., C0) are unnoticeably trained several times in their model (which are implicitly generated from different feature templates used in the paper)","However, when we repeat the work of (Jiang et al, 2008), which reports to achieve the state-of-art performance in the data-sets that we adopt, it has been found that some features (e.g., C0) are unnoticeably trained several times in their model (which are implicitly generated from different feature templates used in the paper)","['125', '40', '94', '2', '58']","<S sid =""125"" ssid = ""36"">However unlike the three features  the word LM brings very tiny improvement.</S><S sid =""40"" ssid = ""12"">Templates in the column below are expanded from the upper ones.</S><S sid =""94"" ssid = ""5"">With precision P and recall R  the balance F-measure is defined as: F = 2PR/(P + R).</S><S sid =""2"" ssid = ""2"">With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S><S sid =""58"" ssid = ""9"">Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.</S>",['Method_Citation']
13,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"Unicode/CP936 1.1M/55K 104K/13K 0.035 Table 3: Corpus statistics for the second SIGHAN Bakeoff appears twice, which is generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al, 2008), with n=0, generates [C0C0])","Table 3: Corpus statistics for the second SIGHAN Bakeoff appears twice, which is generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al, 2008), with n=0, generates [C0C0])","['136', '56', '61', '93', '112']","<S sid =""136"" ssid = ""7"">How can we utilize these knowledge sources effectively?</S><S sid =""56"" ssid = ""7"">To alleviate the drawbacks  we propose a cascaded linear model.</S><S sid =""61"" ssid = ""12"">In this layer  each knowledge source is treated as a feature with a corresponding weight denoting its relative importance.</S><S sid =""93"" ssid = ""4"">In all experiments  we use the averaged parameters for the perceptrons  and F-measure as the accuracy measure.</S><S sid =""112"" ssid = ""23"">Here the core perceptron was just the POS+ model in experiments above.</S>",['Method_Citation']
14,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"As all the features adopted in (Jiang et al, 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be? n?, at least in principle","As all the features adopted in (Jiang et al, 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be n, at least in principle","['99', '120', '121', '42', '117']","<S sid =""99"" ssid = ""10"">Then we trained LEX on each of the four corpora for 7 iterations.</S><S sid =""120"" ssid = ""31"">Without the perceptron  the cascaded model (if we can still call it “cascaded”) performs poorly on both segmentation and Joint S&T.</S><S sid =""121"" ssid = ""32"">Among other features  the 4-gram POS LM plays the most important role  removing this feature causes F-measure decrement of 0.33 points on segmentation and 0.71 points on Joint S&T.</S><S sid =""42"" ssid = ""14"">As predications generated from such templates depend on the current character  we name these templates lexical-target.</S><S sid =""117"" ssid = ""28"">Table 4 shows experiments results.</S>",['Method_Citation']
15,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"Inspired by (Jiang et al, 2008), we set the real d Although Table 5 has shown that the proposed all the value of C0 to be 2.0, the value of C-1C0anC0C1 to be 3.0, and the values of all other features to be 1.0 for the character-based discriminative-plus model","Inspired by (Jiang et al, 2008), we set the real d Although Table 5 has shown that the proposed all the value of C0 to be 2.0, the value of C-1C0anC0C1 to be 3.0, and the values of all other features to be 1.0 for the character-based discriminative-plus model","['68', '134', '64', '53', '7']","<S sid =""68"" ssid = ""19"">It is an important measure of fluency of the translation in SMT.</S><S sid =""134"" ssid = ""5"">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S><S sid =""64"" ssid = ""15"">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result  a 4-gram POS language model functioning as the product of statetransition probabilities in HMM  and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S><S sid =""53"" ssid = ""4"">Figure 2 shows the growing tendency of feature space with the introduction of these features as well as the character-based ones.</S><S sid =""7"" ssid = ""3"">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S>",['Results_Citation']
17,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"Last, (Jiang et al, 2008) 5 adds repeated features implicitly based on (Ng and Low, 2004)","Last, (Jiang et al, 2008) 5 adds repeated features implicitly based on (Ng and Low, 2004)","['3', '136', '84', '18', '92']","<S sid =""3"" ssid = ""3"">Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.</S><S sid =""136"" ssid = ""7"">How can we utilize these knowledge sources effectively?</S><S sid =""84"" ssid = ""9"">Algorithm 2 shows the decoding algorithm.</S><S sid =""18"" ssid = ""14"">In this architecture  knowledge sources that are intractable to incorporate into the perceptron  can be easily incorporated into the outside linear model.</S><S sid =""92"" ssid = ""3"">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.</S>",['Implication_Citation']
20,P08-1102,D12-1046,0,Jiang et al2008a,0,"Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a) ,perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b)","Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a), perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b)","['51', '13', '7', '49', '36']","<S sid =""51"" ssid = ""2"">Additional features most widely used are related to word or POS ngrams.</S><S sid =""13"" ssid = ""9"">However  as such features are generated dynamically during the decoding procedure  two limitation arise: on the one hand  the amount of parameters increases rapidly  which is apt to overfit on training corpus; on the other hand  exact inference by dynamic programming is intractable because the current predication relies on the results of prior predications.</S><S sid =""7"" ssid = ""3"">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S><S sid =""49"" ssid = ""21"">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S><S sid =""36"" ssid = ""8"">All feature templates and their instances are shown in Table 1.</S>",['Method_Citation']
