Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P11-1061,P11-1144,0,2011,0,Subramanya et al? s model was extended by Das and Petrov (2011) to induce part-of-speech dictionaries for unsupervised learning of taggers,Subramanya et al's model was extended by Das and Petrov (2011) to induce part-of-speech dictionaries for unsupervised learning of taggers,"['14', '7', '56', '135', '103']","<S sid =""14"" ssid = ""10"">Our work is closest to that of Yarowsky and Ngai (2001)  but differs in two important ways.</S><S sid =""7"" ssid = ""3"">However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.</S><S sid =""56"" ssid = ""22"">To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.</S><S sid =""135"" ssid = ""35"">For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.</S><S sid =""103"" ssid = ""3"">The availability of these resources guided our selection of foreign languages.</S>",['Implication_Citation']
3,P11-1061,P14-1126,0,2011,0,"Fortunately, some recently proposed POS taggers, such as the POStagger of Das and Petrov (2011), rely only on labeled training data for English and the same kind of parallel text in our approach","Fortunately, some recently proposed POS taggers, such as the POS tagger of Das and Petrov (2011), rely only on labeled training data for English and the same kind of parallel text in our approach","['75', '53', '34', '129', '26']","<S sid =""75"" ssid = ""6"">We use a squared loss to penalize neighboring vertices that have different label distributions: kqi − qjk2 = Ey(qi(y) − qj(y))2  and additionally regularize the label distributions towards the uniform distribution U over all possible labels Y.</S><S sid =""53"" ssid = ""19"">Finally  note that while most feature concepts are lexicalized  others  such as the suffix concept  are not.</S><S sid =""34"" ssid = ""11"">The following three sections elaborate these different stages is more detail.</S><S sid =""129"" ssid = ""29"">Fortunately  performance was stable across various values  and we were able to use the same hyperparameters for all languages.</S><S sid =""26"" ssid = ""3"">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S>",['Implication_Citation']
4,P11-1061,N12-1086,0,"Das and Petrov, 2011",0,"Applications have ranged from domain adaptation of part-of-speech (POS) taggers (Subramanya et al, 2010), unsupervised learning ofPOS taggers by using bilingual graph-based projections (Das and Petrov, 2011), and shallow semantic parsing for unknown predicates (Das and Smith,2011)","Applications have ranged from domain adaptation of part-of-speech (POS) taggers (Subramanya et al, 2010), unsupervised learning of POS taggers by using bilingual graph-based projections (Das and Petrov, 2011), and shallow semantic parsing for unknown predicates (Das and Smith,2011)","['1', '107', '59', '56', '39']","<S sid =""1"" ssid = ""1"">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid =""107"" ssid = ""7"">Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.</S><S sid =""59"" ssid = ""25"">So far the graph has been completely unlabeled.</S><S sid =""56"" ssid = ""22"">To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.</S><S sid =""39"" ssid = ""5"">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S>",['Results_Citation']
5,P11-1061,N12-1086,0,2011,0,"Following Das and Petrov (2011) and Subramanya et al (2010), a similarity score between two trigram types was computed by measuring the cosine similarity between their empirical sentential context statistics","Following Das and Petrov (2011) and Subramanya et al (2010), a similarity score between two trigram types was computed by measuring the cosine similarity between their empirical sentential context statistics","['83', '66', '107', '117', '135']","<S sid =""83"" ssid = ""14"">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value τ: We describe how we choose τ in §6.4.</S><S sid =""66"" ssid = ""32"">In general  the neighborhoods can be more diverse and we allow a soft label distribution over the vertices.</S><S sid =""107"" ssid = ""7"">Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.</S><S sid =""117"" ssid = ""17"">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid =""135"" ssid = ""35"">For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.</S>",['Method_Citation']
6,P11-1061,N12-1086,0,"Das and Petrov, 2011",0,"Sparsity is desirable in settings where labeled development data for tuning thresholds that select the most probable labels for a given type is unavailable (e.g., Das and Petrov, 2011)","Sparsity is desirable in settings where labeled development data for tuning thresholds that select the most probable labels for a given type is unavailable (e.g., Das and Petrov, 2011)","['104', '1', '155', '39', '98']","<S sid =""104"" ssid = ""4"">For monolingual treebank data we relied on the CoNLL-X and CoNLL-2007 shared tasks on dependency parsing (Buchholz and Marsi  2006; Nivre et al.  2007).</S><S sid =""1"" ssid = ""1"">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid =""155"" ssid = ""18"">Examining the word fidanzato for the “No LP” and “With LP” models is particularly instructive.</S><S sid =""39"" ssid = ""5"">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid =""98"" ssid = ""29"">7).</S>",['Method_Citation']
7,P11-1061,N12-1052,0,2011,0,"Specifically, by replacing fine-grained language specific part-of-speech tags with universal part-of-speech tags, generated with the method described by Das and Petrov (2011), a universal parser is achieved that can be applied to any language for which universal part-of-speech tags are available. Below, we extend this approach to universal parsing by adding cross-lingual word cluster features","Specifically, by replacing fine-grained language specific part-of-speech tags with universal part-of-speech tags, generated with the method described by Das and Petrov (2011), a universal parser is achieved that can be applied to any language for which universal part-of-speech tags are available. Below, we extend this approach to universal parsing by adding cross-lingual word cluster features","['143', '96', '97', '68', '26']","<S sid =""143"" ssid = ""6"">Overall  it gives improvements ranging from 1.1% for German to 14.7% for Italian  for an average improvement of 8.3% over the unsupervised feature-HMM model.</S><S sid =""96"" ssid = ""27"">The function A : F —* C maps from the language specific fine-grained tagset F to the coarser universal tagset C and is described in detail in §6.2: Note that when tx(y) = 1 the feature value is 0 and has no effect on the model  while its value is −oc when tx(y) = 0 and constrains the HMM’s state space.</S><S sid =""97"" ssid = ""28"">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid =""68"" ssid = ""34"">In the label propagation stage  we propagate the automatic English tags to the aligned Italian trigram types  followed by further propagation solely among the Italian vertices. the Italian vertices are connected to an automatically labeled English vertex.</S><S sid =""26"" ssid = ""3"">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S>",['Method_Citation']
8,P11-1061,N12-1052,0,2011,0,"We study the impact of using cross-lingual cluster features by comparing the strong delexicalized baseline model of McDonald et al (2011), which only has features derived from universal part-of-speech tags, projected from English with the method of Das and Petrov (2011), to the same model when adding features derived from cross-lingual clusters","We study the impact of using cross-lingual cluster features by comparing the strong delexicalized baseline model of McDonald et al (2011), which only has features derived from universal part-of-speech tags, projected from English with the method of Das and Petrov (2011), to the same model when adding features derived from cross-lingual clusters","['7', '26', '129', '108', '37']","<S sid =""7"" ssid = ""3"">However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.</S><S sid =""26"" ssid = ""3"">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid =""129"" ssid = ""29"">Fortunately  performance was stable across various values  and we were able to use the same hyperparameters for all languages.</S><S sid =""108"" ssid = ""8"">However  we needed to restrict ourselves to these languages in order to be able to evaluate the performance of our approach.</S><S sid =""37"" ssid = ""3"">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S>",['Method_Citation']
9,P11-1061,N12-1090,0,"Das and Petrov, (2011)",0,"MT-based projection has been applied to various NLP tasks, such as part of-speech tagging (e.g., Das and Petrov (2011)), mention detection (e.g., Zitouni and Florian (2008)), and sentiment analysis (e.g., Mihalcea et al (2007)) .There have been two initial attempts to apply projection to create co reference-annotated data for aresource-poor language, both of which involve projecting hand-annotated co reference data from English to Romanian via a parallel corpus","MT-based projection has been applied to various NLP tasks, such as part of-speech tagging (e.g., Das and Petrov (2011)), mention detection (e.g., Zitouni and Florian (2008)), and sentiment analysis (e.g., Mihalcea et al (2007))","['39', '110', '122', '19', '31']","<S sid =""39"" ssid = ""5"">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid =""110"" ssid = ""10"">We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.</S><S sid =""122"" ssid = ""22"">For each language  we took the same number of sentences from the bitext as there are in its treebank  and trained a supervised feature-HMM.</S><S sid =""19"" ssid = ""15"">Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S><S sid =""31"" ssid = ""8"">By aggregating the POS labels of the English tokens to types  we can generate label distributions for the English vertices.</S>",['Method_Citation']
10,P11-1061,W11-2205,0,2011,0,"For example, the multilingual PoS induction approach of Das and Petrov (2011) assumes no supervision for the language whose PoS tags are being 35 induced, but it assumes access to a labeled dataset of a different language. We begin by surveying recent work on unsupervised PoS tagging, focusing on the issue of evaluation (Section 2)","For example, the multilingual PoS induction approach of Das and Petrov (2011) assumes no supervision for the language whose PoS tags are being 35 induced, but it assumes access to a labeled dataset of a different language. We begin by surveying recent work on unsupervised PoS tagging, focusing on the issue of evaluation (Section 2)","['73', '24', '150', '39', '139']","<S sid =""73"" ssid = ""4"">Note that because we extracted only high-confidence alignments  many foreign vertices will not be connected to any English vertices.</S><S sid =""24"" ssid = ""1"">The focus of this work is on building POS taggers for foreign languages  assuming that we have an English POS tagger and some parallel text between the two languages.</S><S sid =""150"" ssid = ""13"">For all languages  the vocabulary sizes increase by several thousand words.</S><S sid =""39"" ssid = ""5"">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid =""139"" ssid = ""2"">As expected  the vanilla HMM trained with EM performs the worst.</S>",['Method_Citation']
11,P11-1061,P13-1155,0,"Das and Petrov, 2011",0,"(Das and Petrov, 2011) used graph-based label propagation for cross-lingual knowledge transfers to induce POS tags between two languages","(Das and Petrov, 2011) used graph-based label propagation for cross-lingual knowledge transfers to induce POS tags between two languages","['107', '45', '69', '135', '110']","<S sid =""107"" ssid = ""7"">Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.</S><S sid =""45"" ssid = ""11"">Furthermore  we do not connect the English vertices to each other  but only to foreign language vertices.4 The graph vertices are extracted from the different sides of a parallel corpus (De  Df) and an additional unlabeled monolingual foreign corpus Ff  which will be used later for training.</S><S sid =""69"" ssid = ""35"">Label propagation is used to propagate these tags inwards and results in tag distributions for the middle word of each Italian trigram.</S><S sid =""135"" ssid = ""35"">For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.</S><S sid =""110"" ssid = ""10"">We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.</S>",['Implication_Citation']
12,P11-1061,D12-1127,0,2011,0,Recent work by Das and Petrov (2011 )buildsa dictionary for a particular language by transfer ring annotated data from a resource-rich language through the use of word alignments in parallel text,Recent work by Das and Petrov (2011 ) builds a dictionary for a particular language by transferring annotated data from a resource-rich language through the use of word alignments in parallel text,"['37', '1', '83', '25', '4']","<S sid =""37"" ssid = ""3"">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S><S sid =""1"" ssid = ""1"">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.</S><S sid =""83"" ssid = ""14"">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value τ: We describe how we choose τ in §6.4.</S><S sid =""25"" ssid = ""2"">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S sid =""4"" ssid = ""4"">Across eight European languages  our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline  and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.</S>",['Method_Citation']
13,P11-1061,D12-1127,0,"Das and Petrov, 2011",0,"Theseapproaches build a dictionary by transferring labeled data from a resource rich language (English) to a re source poor language (Das and Petrov, 2011)","These approaches build a dictionary by transferring labeled data from a resource rich language (English) to a resource poor language (Das and Petrov, 2011)","['54', '120', '117', '44', '26']","<S sid =""54"" ssid = ""20"">Given this similarity function  we define a nearest neighbor graph  where the edge weight for the n most similar vertices is set to the value of the similarity function and to 0 for all other vertices.</S><S sid =""120"" ssid = ""20"">We were intentionally lenient with our baselines: bilingual information by projecting POS tags directly across alignments in the parallel data.</S><S sid =""117"" ssid = ""17"">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid =""44"" ssid = ""10"">Because all English vertices are going to be labeled  we do not need to disambiguate them by embedding them in trigrams.</S><S sid =""26"" ssid = ""3"">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S>",['Method_Citation']
14,P11-1061,P12-3012,0,"Das and Petrov, 2011",0,"In recent years research in Natural Language Processing (NLP) has been steadily moving towards multilingual processing: the availability of ever growing amounts of text in different languages ,infact, has been a major driving force behind research on multilingual approaches, from morphosyntactic (Das and Petrov, 2011) and syntactico semantic (Peirsman and Pado?, 2010) phenomena to high-end tasks like textual entailment (Mehdad et al., 2011) and sentiment analysis (Lu et al, 2011)","In recent years research in Natural Language Processing (NLP) has been steadily moving towards multilingual processing: the availability of ever growing amounts of text in different languages, in fact, has been a major driving force behind research on multilingual approaches, from morphosyntactic (Das and Petrov, 2011) and syntactico semantic (Peirsman and Pado?, 2010) phenomena to high-end tasks like textual entailment (Mehdad et al., 2011) and sentiment analysis (Lu et al, 2011)","['39', '117', '47', '161', '7']","<S sid =""39"" ssid = ""5"">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid =""117"" ssid = ""17"">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid =""47"" ssid = ""13"">Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).</S><S sid =""161"" ssid = ""4"">Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections  and bridge the gap between purely supervised and unsupervised POS tagging models.</S><S sid =""7"" ssid = ""3"">However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.</S>",['Method_Citation']
15,P11-1061,D11-1006,0,2011,0,"Furthermore, we evaluate with both gold-standard part-of-speech tags, as well as predicted part-of speech tags from the projected part-of-speech tagger of Das and Petrov (2011) .2 This tagger relies only onlabeled training data for English, and achieves accuracies around 85% on the languages that we con sider","Furthermore, we evaluate with both gold-standard part-of-speech tags, as well as predicted part-of speech tags from the projected part-of-speech tagger of Das and Petrov (2011). This tagger relies only on labeled training data for English, and achieves accuracies around 85% on the languages that we consider","['37', '117', '32', '86', '132']","<S sid =""37"" ssid = ""3"">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S><S sid =""117"" ssid = ""17"">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid =""32"" ssid = ""9"">Label propagation can then be used to transfer the labels to the peripheral foreign vertices (i.e. the ones adjacent to the English vertices) first  and then among all of the foreign vertices (§4).</S><S sid =""86"" ssid = ""17"">For a sentence x and a state sequence z  a first order Markov model defines a distribution: (9) where Val(X) corresponds to the entire vocabulary.</S><S sid =""132"" ssid = ""32"">When extracting the vector t  used to compute the constraint feature from the graph  we tried three threshold values for r (see Eq.</S>",['Results_Citation']
16,P11-1061,D11-1006,0,2011,0,"In the first, we assumed that the test set for each target language had gold part-of-speech tags, and in the second we used predicted part-of-speech tags from the projection tagger of Das and Petrov (2011), which also uses English as the source language","In the first, we assumed that the test set for each target language had gold part-of-speech tags, and in the second we used predicted part-of-speech tags from the projection tagger of Das and Petrov (2011), which also uses English as the source language","['163', '110', '139', '58', '83']","<S sid =""163"" ssid = ""2"">We would also like to thank Amarnag Subramanya for helping us with the implementation of label propagation and Shankar Kumar for access to the parallel data.</S><S sid =""110"" ssid = ""10"">We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.</S><S sid =""139"" ssid = ""2"">As expected  the vanilla HMM trained with EM performs the worst.</S><S sid =""58"" ssid = ""24"">Based on these high-confidence alignments we can extract tuples of the form [u H v]  where u is a foreign trigram type  whose middle word aligns to an English word type v. Our bilingual similarity function then sets the edge weights in proportion to these tuple counts.</S><S sid =""83"" ssid = ""14"">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value τ: We describe how we choose τ in §6.4.</S>",['Implication_Citation']
17,P11-1061,P13-2112,0,2011,0,"This parallel data can be exploited to bridge languages, and in particular, transfer information from a highly-resourced language to a lesser-resourced language, to build unsupervised POS taggers. In this paper, we propose an unsupervised approach to POS tagging in a similar vein to the work of Das and Petrov (2011)","This parallel data can be exploited to bridge languages, and in particular, transfer information from a highly-resourced language to a lesser-resourced language, to build unsupervised POS taggers. In this paper, we propose an unsupervised approach to POS tagging in a similar vein to the work of Das and Petrov (2011)","['37', '117', '134', '110', '72']","<S sid =""37"" ssid = ""3"">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S><S sid =""117"" ssid = ""17"">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid =""134"" ssid = ""34"">Because we don’t have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.</S><S sid =""110"" ssid = ""10"">We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.</S><S sid =""72"" ssid = ""3"">In the first stage  we run a single step of label propagation  which transfers the label distributions from the English vertices to the connected foreign language vertices (say  Vf�) at the periphery of the graph.</S>",['Method_Citation']
18,P11-1061,P13-2112,0,2011,0,Das and Petrov (2011) achieved the current state-of-the-art for unsupervised tagging by exploiting high confidence alignments to copy tags from the source language to the target language,Das and Petrov (2011) achieved the current state-of-the-art for unsupervised tagging by exploiting high confidence alignments to copy tags from the source language to the target language,"['54', '39', '26', '104', '135']","<S sid =""54"" ssid = ""20"">Given this similarity function  we define a nearest neighbor graph  where the edge weight for the n most similar vertices is set to the value of the similarity function and to 0 for all other vertices.</S><S sid =""39"" ssid = ""5"">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid =""26"" ssid = ""3"">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid =""104"" ssid = ""4"">For monolingual treebank data we relied on the CoNLL-X and CoNLL-2007 shared tasks on dependency parsing (Buchholz and Marsi  2006; Nivre et al.  2007).</S><S sid =""135"" ssid = ""35"">For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.</S>",['Results_Citation']
19,P11-1061,P13-2112,0,"Das and Petrov, 2011",0,"We have proposed a method for unsupervised POStagging that performs on par with the current state of-the-art (Das and Petrov, 2011), but is subs tan tially less-sophisticated (specifically not requiring convex optimization or a feature-based HMM)","We have proposed a method for unsupervised POS tagging that performs on par with the current state of-the-art (Das and Petrov, 2011), but is substantially less-sophisticated (specifically not requiring convex optimization or a feature-based HMM)","['33', '117', '4', '90', '96']","<S sid =""33"" ssid = ""10"">The POS distributions over the foreign trigram types are used as features to learn a better unsupervised POS tagger (§5).</S><S sid =""117"" ssid = ""17"">In other words  the set of hidden states F was chosen to be the fine set of treebank tags.</S><S sid =""4"" ssid = ""4"">Across eight European languages  our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline  and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.</S><S sid =""90"" ssid = ""21"">All features were conjoined with the state z.</S><S sid =""96"" ssid = ""27"">The function A : F —* C maps from the language specific fine-grained tagset F to the coarser universal tagset C and is described in detail in §6.2: Note that when tx(y) = 1 the feature value is 0 and has no effect on the model  while its value is −oc when tx(y) = 0 and constrains the HMM’s state space.</S>",['Aim_Citation']
