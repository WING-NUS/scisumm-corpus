Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,W99-0613,N01-1023,0,"Collins and Singer, 1999",0,"Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named entity identification (Collins and Singer, 1999)","Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named entity identification (Collins and Singer, 1999)","['256', '198', '178', '125', '41']","<S sid =""256"" ssid = ""7"">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid =""198"" ssid = ""65"">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid =""178"" ssid = ""45"">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid =""125"" ssid = ""58"">It may be more realistic to replace the second criteria with a softer one  for example (Blum and Mitchell 98) suggest the alternative Alternatively  if Ii and 12 are probabilistic learners  it might make sense to encode the second constraint as one of minimizing some measure of the distance between the distributions given by the two learners.</S><S sid =""41"" ssid = ""35"">(Hearst 92) describes a method for extracting hyponyms from a corpus (pairs of words in &quot;isa&quot; relations).</S>",['Implication_Citation']
2,W99-0613,N01-1023,0,"Collins and Singer, 1999",0,"They also discuss an application of classifying web pages by using their method of mutually constrained models. (Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms toAdaBoost which force the classifiers to agree (called Co Boosting)","(Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms to AdaBoost which force the classifiers to agree (called Co Boosting)","['256', '178', '97', '70', '198']","<S sid =""256"" ssid = ""7"">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid =""178"" ssid = ""45"">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid =""97"" ssid = ""30"">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid =""70"" ssid = ""3"">.</S><S sid =""198"" ssid = ""65"">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S>",['Implication_Citation']
3,W99-0613,W03-1509,0,Collins and Singer 1999,0,"Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick, et al 1999] and so on","Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick, et al 1999] and so on","['256', '128', '198', '178', '70']","<S sid =""256"" ssid = ""7"">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid =""128"" ssid = ""61"">At each iteration the algorithm increases the number of rules  while maintaining a high level of agreement between the spelling and contextual decision lists.</S><S sid =""198"" ssid = ""65"">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid =""178"" ssid = ""45"">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid =""70"" ssid = ""3"">.</S>",['Results_Citation']
4,W99-0613,C02-1154,0,"Collins and Singer, 1999",0,"DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syn tactically analyzed corpus","DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syntactically analyzed corpus","['256', '97', '70', '198', '178']","<S sid =""256"" ssid = ""7"">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid =""97"" ssid = ""30"">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid =""70"" ssid = ""3"">.</S><S sid =""198"" ssid = ""65"">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid =""178"" ssid = ""45"">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S>",['Method_Citation']
5,W99-0613,C02-1154,0,"Collins and Singer, 1999",0,"(Collins and Singer, 1999) also makes use of competing categories (person, organization, and location), which cover 96% of all the instances itset out to classify","(Collins and Singer, 1999) also makes use of competing categories (person, organization, and location), which cover 96% of all the instances it set out to classify","['256', '178', '198', '70', '220']","<S sid =""256"" ssid = ""7"">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid =""178"" ssid = ""45"">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid =""198"" ssid = ""65"">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid =""70"" ssid = ""3"">.</S><S sid =""220"" ssid = ""87"">(7)  such as the likelihood function used in maximum-entropy problems and other generalized additive models (Lafferty 99).</S>",['Method_Citation']
6,W99-0613,W06-2204,0,"Collins and Singer, 1999",0,"In (Collins and Singer, 1999) Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification","In (Collins and Singer, 1999) Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification","['256', '198', '178', '70', '236']","<S sid =""256"" ssid = ""7"">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid =""198"" ssid = ""65"">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid =""178"" ssid = ""45"">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid =""70"" ssid = ""3"">.</S><S sid =""236"" ssid = ""3"">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S>",['Method_Citation']
8,W99-0613,W03-1022,0,"Collins and Singer, 1999",0,"Collins and Singer (1999) for example report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999)","Collins and Singer (1999) for example report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999)","['256', '198', '70', '128', '97']","<S sid =""256"" ssid = ""7"">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid =""198"" ssid = ""65"">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid =""70"" ssid = ""3"">.</S><S sid =""128"" ssid = ""61"">At each iteration the algorithm increases the number of rules  while maintaining a high level of agreement between the spelling and contextual decision lists.</S><S sid =""97"" ssid = ""30"">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S>",['Method_Citation']
9,W99-0613,E09-1018,0,"Collinsand Singer, 1999",0,"While EM has worked quite well for a few tasks, notably ma chine translations (starting with the IBM models 1-5 (Brown et al, 1993), it has not had success inmost others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collinsand Singer, 1999) and context-free-grammar induction (numerous attempts, too many to mention)","While EM has worked quite well for a few tasks, notably machine translations (starting with the IBM models 1-5 (Brown et al, 1993), it has not had success in most others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collinsand Singer, 1999) and context-free-grammar induction (numerous attempts, too many to mention)","['256', '178', '70', '220', '198']","<S sid =""256"" ssid = ""7"">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid =""178"" ssid = ""45"">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid =""70"" ssid = ""3"">.</S><S sid =""220"" ssid = ""87"">(7)  such as the likelihood function used in maximum-entropy problems and other generalized additive models (Lafferty 99).</S><S sid =""198"" ssid = ""65"">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S>",['Method_Citation']
11,W99-0613,W07-1712,0,"Collins and Singer, 1999",0,"In addition, we would also like to explore the semi-supervised techniques such as co-training and self-training (Collins and Singer, 1999)","In addition, we would also like to explore the semi-supervised techniques such as co-training and self-training (Collins and Singer, 1999)","['256', '198', '236', '41', '97']","<S sid =""256"" ssid = ""7"">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid =""198"" ssid = ""65"">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid =""236"" ssid = ""3"">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S><S sid =""41"" ssid = ""35"">(Hearst 92) describes a method for extracting hyponyms from a corpus (pairs of words in &quot;isa&quot; relations).</S><S sid =""97"" ssid = ""30"">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S>",['Method_Citation']
12,W99-0613,W09-2208,0,"Collins and Singer, 1999",0,"Collins et al (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky ?smethod (Yarowsky, 1995) and the framework suggested by (Blum and Mitchell, 1998)","Collins et al (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky's method (Yarowsky, 1995) and the framework suggested by (Blum and Mitchell, 1998)","['256', '178', '97', '236', '70']","<S sid =""256"" ssid = ""7"">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid =""178"" ssid = ""45"">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid =""97"" ssid = ""30"">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid =""236"" ssid = ""3"">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S><S sid =""70"" ssid = ""3"">.</S>",['Implication_Citation']
13,W99-0613,W06-2207,0,"Collins and Singer, 1999",0,"This approach was shown to perform well on real-world natural language problems (Collins and Singer, 1999)","This approach was shown to perform well on real-world natural language problems (Collins and Singer, 1999)","['256', '178', '198', '125', '41']","<S sid =""256"" ssid = ""7"">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid =""178"" ssid = ""45"">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid =""198"" ssid = ""65"">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid =""125"" ssid = ""58"">It may be more realistic to replace the second criteria with a softer one  for example (Blum and Mitchell 98) suggest the alternative Alternatively  if Ii and 12 are probabilistic learners  it might make sense to encode the second constraint as one of minimizing some measure of the distance between the distributions given by the two learners.</S><S sid =""41"" ssid = ""35"">(Hearst 92) describes a method for extracting hyponyms from a corpus (pairs of words in &quot;isa&quot; relations).</S>",['Method_Citation']
15,W99-0613,W06-2207,0,"Collins and Singer, 1999",0,"(6) Similarly to (Collins and Singer, 1999) we used T= 0.95 for all experiments reported here","(6) Similarly to (Collins and Singer, 1999) we used T= 0.95 for all experiments reported here","['256', '97', '178', '198', '236']","<S sid =""256"" ssid = ""7"">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid =""97"" ssid = ""30"">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid =""178"" ssid = ""45"">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid =""198"" ssid = ""65"">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid =""236"" ssid = ""3"">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S>",['Method_Citation']
16,W99-0613,P12-1065,0,1999,0,We use Collins and Singer (1999) for our exact specification of Yarowsky.2 It uses DL rule scores ?fj?| ?fj |+ |? f |+ L (1) where  is a smoothing constant,We use Collins and Singer (1999) for our exact specification of Yarowsky,"['256', '70', '198', '178', '236']","<S sid =""256"" ssid = ""7"">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S><S sid =""70"" ssid = ""3"">.</S><S sid =""198"" ssid = ""65"">In fact  during the first rounds many of the predictions of Th.  g2 are zero.</S><S sid =""178"" ssid = ""45"">Then  it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S><S sid =""236"" ssid = ""3"">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S>",['Method_Citation']
