Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
2,A00-2018,N10-1002,0,"Charniak, 2000",0,"As a benchmark VPC extraction system, we use the Charniak parser (Charniak, 2000)","As a benchmark VPC extraction system, we use the Charniak parser (Charniak, 2000)","['114', '159', '2', '165', '163']","<S sid =""114"" ssid = ""5"">That parser  as stated in Figure 1  achieves an average precision/recall of 87.5.</S><S sid =""159"" ssid = ""50"">Something very much like this is done in [15].</S><S sid =""2"" ssid = ""2"">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid =""165"" ssid = ""56"">Note that we also tried including this information using a standard deleted-interpolation model.</S><S sid =""163"" ssid = ""54"">Next we add the less obvious conditioning events noted in our previous discussion of the final model — grandparent label lg and left sibling label /b.</S>",['Implication_Citation']
3,A00-2018,W11-0610,0,"Charniak, 2000",0,"Each of these scores can be calculated from a provided syntactic parse tree, and to generate these we made use of the Charniakparser (Charniak, 2000), also trained on the Switch board tree bank","Each of these scores can be calculated from a provided syntactic parse tree, and to generate these we made use of the Charniak parser (Charniak, 2000), also trained on the Switch board tree bank","['114', '3', '99', '129', '92']","<S sid =""114"" ssid = ""5"">That parser  as stated in Figure 1  achieves an average precision/recall of 87.5.</S><S sid =""3"" ssid = ""3"">The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.</S><S sid =""99"" ssid = ""10"">Also  the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events.</S><S sid =""129"" ssid = ""20"">It makes no use of special maximum-entropyinspired features (though their presence made it much easier to perform these experiments)  it does not guess the pre-terminal before guessing the lexical head  and it uses a tree-bank grammar rather than a Markov grammar.</S><S sid =""92"" ssid = ""3"">For runs with the generative model based upon Markov grammar statistics  the first pass uses the same statistics  but conditioned only on standard PCFG information.</S>",['Implication_Citation']
4,A00-2018,W06-3119,0,"Charniak, 2000",0,"We then use Charniak? s parser (Charniak, 2000) to generate the most likely parse tree for each English target sentence in the training corpus","We then use Charniak's parser (Charniak, 2000) to generate the most likely parse tree for each English target sentence in the training corpus","['132', '69', '184', '157', '85']","<S sid =""132"" ssid = ""23"">Between the Old model and the Best model  Figure 2 gives precision/recall measurements for several different versions of our parser.</S><S sid =""69"" ssid = ""38"">For example  one can well imagine that one might want to condition on the parent's lexical head without conditioning on the left sibling  or the grandparent label.</S><S sid =""184"" ssid = ""11"">The first is the slight  but important  improvement achieved by using this model over conventional deleted interpolation  as indicated in Figure 2.</S><S sid =""157"" ssid = ""48"">Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.</S><S sid =""85"" ssid = ""54"">As partition-function calculation is typically the major on-line computational problem for maximum-entropy models  this simplifies the model significantly.</S>",['Results_Citation']
5,A00-2018,N03-2024,0,"Charniak, 2000",0,"We were interested in the occurrence of features such as type and number of premodifiers, presence and type of post modifiers, and form of name reference for people. We constructed a large, automatically annotated corpus by merging the output of Charniak? s statistical parser (Charniak, 2000) with that of the IBM named entity recognition system Nominator (Wacholder et al,1997)","We were interested in the occurrence of features such as type and number of premodifiers, presence and type of post modifiers, and form of name reference for people. We constructed a large, automatically annotated corpus by merging the output of Charniak's statistical parser (Charniak, 2000) with that of the IBM named entity recognition system Nominator (Wacholder et al,1997)","['137', '51', '2', '58', '150']","<S sid =""137"" ssid = ""28"">However  Collins in [10] does not stress the decision to guess the head's pre-terminal first  and it might be lost on the casual reader.</S><S sid =""51"" ssid = ""20"">Second  and this is a point we have not yet mentioned  the features used in these models need have no particular independence of one another.</S><S sid =""2"" ssid = ""2"">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid =""58"" ssid = ""27"">Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.</S><S sid =""150"" ssid = ""41"">The second modification is the explicit marking of noun and verb-phrase coordination.</S>",['Method_Citation']
6,A00-2018,N06-1039,0,"Charniak, 2000",0,"After getting a set of basic clusters, we pass them to an existing statistical parser (Charniak, 2000) and rule-based tree normalizer to obtain a GLARFstructure for each sentence in every article","After getting a set of basic clusters, we pass them to an existing statistical parser (Charniak, 2000) and rule-based tree normalizer to obtain a GLARF structure for each sentence in every article","['49', '4', '13', '2', '165']","<S sid =""49"" ssid = ""18"">First  as already implicit in our discussion  factoring the probability computation into a sequence of values corresponding to various &quot;features&quot; suggests that the probability model should be easily changeable — just change the set of features used.</S><S sid =""4"" ssid = ""4"">We also present some partial results showing the effects of different conditioning information  including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.</S><S sid =""13"" ssid = ""2"">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question.</S><S sid =""2"" ssid = ""2"">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid =""165"" ssid = ""56"">Note that we also tried including this information using a standard deleted-interpolation model.</S>",['Method_Citation']
7,A00-2018,C04-1180,0,2000,0,"The levels of accuracy and robustness recently achieved by statistical parsers (e.g. Collins (1999), Charniak (2000)) have led to their use in a number of NLP applications, such as question-answering (Pasca and Harabagiu, 2001), machine translation (Charniak et al, 2003), sentence simplification (Carroll et al, 1999), and a linguist? s search engine (Resnik and Elkiss, 2003)","The levels of accuracy and robustness recently achieved by statistical parsers (e.g. Collins (1999), Charniak (2000)) have led to their use in a number of NLP applications, such as question-answering (Pasca and Harabagiu, 2001), machine translation (Charniak et al, 2003), sentence simplification (Carroll et al, 1999), and a linguist? s search engine (Resnik and Elkiss, 2003)","['34', '11', '96', '99', '19']","<S sid =""34"" ssid = ""3"">Also  remember that H is a pla ceholder for any other information beyond the constituent c that may be useful in assigning c a probability.</S><S sid =""11"" ssid = ""7"">What fundamentally distinguishes probabilistic generative parsers is how they compute p(r)  and it is to that topic we turn next.</S><S sid =""96"" ssid = ""7"">As noted above  the probability model uses five smoothed probability distributions  one each for Li  M Ri t  and h. The equation for the (unsmoothed) conditional probability distribution for t is given in Equation 7.</S><S sid =""99"" ssid = ""10"">Also  the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events.</S><S sid =""19"" ssid = ""8"">The method we use follows that of [10].</S>",['Method_Citation']
8,A00-2018,W05-0638,0,"Charniak, 2000",0,"In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000)","In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000)","['3', '185', '80', '54', '144']","<S sid =""3"" ssid = ""3"">The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.</S><S sid =""185"" ssid = ""12"">We expect that as we experiment with other  more semantic conditioning information  the importance of this aspect of the model will increase.</S><S sid =""80"" ssid = ""49"">(Our experience is that rather than requiring 50 or so iterations  three suffice.)</S><S sid =""54"" ssid = ""23"">The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).</S><S sid =""144"" ssid = ""35"">This quantity is a relatively intuitive one (as  for example  it is the quantity used in a PCFG to relate words to their pre-terminals) and it seems particularly good to condition upon here since we use it  in effect  as the unsmoothed probability upon which all smoothing of p(h) is based.</S>",['Method_Citation']
9,A00-2018,P05-1065,0,"Charniak, 2000",0,"We also use a standard statistical parser (Charniak, 2000) to provide syntactic analysis","We also use a standard statistical parser (Charniak, 2000) to provide syntactic analysis","['47', '2', '41', '85', '22']","<S sid =""47"" ssid = ""16"">The intuitive idea is that each factor gi is larger than one if the feature in question makes the probability more likely  one if the feature has no effect  and smaller than one if it makes the probability less likely.</S><S sid =""2"" ssid = ""2"">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid =""41"" ssid = ""10"">For example  in computing the probability of the head's pre-terminal t we might want a feature schema f (t  1) that returns 1 if the observed pre-terminal of c = t and the label of c = 1  and zero otherwise.</S><S sid =""85"" ssid = ""54"">As partition-function calculation is typically the major on-line computational problem for maximum-entropy models  this simplifies the model significantly.</S><S sid =""22"" ssid = ""11"">For us the non-terminal symbols are those of the tree-bank  augmented by the symbols aux and auxg  which have been assigned deterministically to certain auxiliary verbs such as &quot;have&quot; or &quot;having&quot;.</S>",['Method_Citation']
10,A00-2018,P05-1065,0,"Charniak, 2000",0,"For each article, we calculated the per cent age of a) all word instances (tokens) and b) all unique words (types) not on these lists, resulting in three token OOV rate features and three type OOV rate features per article. The parse features are generated using the Charniak parser (Charniak, 2000) trained on the standard Wall Street Journal Treebank corpus","For each article, we calculated the percentage of a) all word instances (tokens) and b) all unique words (types) not on these lists, resulting in three token OOV rate features and three type OOV rate features per article. The parse features are generated using the Charniak parser (Charniak, 2000) trained on the standard Wall Street Journal Treebank corpus","['157', '55', '66', '4', '121']","<S sid =""157"" ssid = ""48"">Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.</S><S sid =""55"" ssid = ""24"">This method is known as &quot;deleted interpolation&quot; smoothing.</S><S sid =""66"" ssid = ""35"">In many cases this is clearly warranted.</S><S sid =""4"" ssid = ""4"">We also present some partial results showing the effects of different conditioning information  including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.</S><S sid =""121"" ssid = ""12"">Without these enhancements Char97 performs at the 86.6% level for sentences of length < 40.</S>",['Method_Citation']
11,A00-2018,P04-1040,0,2000,0,"The evaluation of the transformed output of the parsers of Charniak (2000) and Collins (1999) gives 90 %unlabelled and 84 %labelled accuracy with respect to dependencies, when measured against a dependency corpus derived from the Penn Treebank.The paper is organized as follows","The evaluation of the transformed output of the parsers of Charniak (2000) and Collins (1999) gives 90 % unlabelled and 84 % labelled accuracy with respect to dependencies, when measured against a dependency corpus derived from the Penn Treebank. The paper is organized as follows","['2', '157', '0', '37', '21']","<S sid =""2"" ssid = ""2"">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid =""157"" ssid = ""48"">Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.</S><S sid =""0"" ssid = ""0"">A Maximum-Entropy-Inspired Parser *</S><S sid =""37"" ssid = ""6"">Rather  we concentrate on the aspects of these models that most directly influenced the model presented here.</S><S sid =""21"" ssid = ""10"">(We assume that all terminal symbols are generated by rules of the form &quot;preterm word&quot; and we treat these as a special case.)</S>",['Implication_Citation']
12,A00-2018,P04-1040,0,2000,0,"Blaheta and Charniak (2000) presented the first method for assigning Pennfunctional tags to constituents identified by a parser. Pattern-matching approaches were used in (John son, 2002) and (Jijkoun, 2003) to recover non-local dependencies in phrase trees","Blaheta and Charniak (2000) presented the first method for assigning Penn functional tags to constituents identified by a parser. Pattern-matching approaches were used in (Johnson, 2002) and (Jijkoun, 2003) to recover non-local dependencies in phrase trees","['48', '21', '54', '177', '159']","<S sid =""48"" ssid = ""17"">Maximum-entropy models have two benefits for a parser builder.</S><S sid =""21"" ssid = ""10"">(We assume that all terminal symbols are generated by rules of the form &quot;preterm word&quot; and we treat these as a special case.)</S><S sid =""54"" ssid = ""23"">The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).</S><S sid =""177"" ssid = ""4"">The results reported here disprove this conjecture.</S><S sid =""159"" ssid = ""50"">Something very much like this is done in [15].</S>",['Method_Citation']
13,A00-2018,P04-1040,0,2000,0,"As an alternative to hard coded heuristics, Blaheta and Charniak (2000) proposed to recover the Penn functional tags automatically","As an alternative to hard coded heuristics, Blaheta and Charniak (2000) proposed to recover the Penn functional tags automatically","['35', '121', '69', '138', '114']","<S sid =""35"" ssid = ""4"">In the past few years the maximum entropy  or log-linear  approach has recommended itself to probabilistic model builders for its flexibility and its novel approach to smoothing [1 17].</S><S sid =""121"" ssid = ""12"">Without these enhancements Char97 performs at the 86.6% level for sentences of length < 40.</S><S sid =""69"" ssid = ""38"">For example  one can well imagine that one might want to condition on the parent's lexical head without conditioning on the left sibling  or the grandparent label.</S><S sid =""138"" ssid = ""29"">Indeed  it was lost on the present author until he went back after the fact and found it there.</S><S sid =""114"" ssid = ""5"">That parser  as stated in Figure 1  achieves an average precision/recall of 87.5.</S>",['Method_Citation']
17,A00-2018,N06-1022,0,2000,0,"The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents","The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents","['169', '125', '134', '99', '155']","<S sid =""169"" ssid = ""60"">Up to this point all the models considered in this section are tree-bank grammar models.</S><S sid =""125"" ssid = ""16"">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S><S sid =""134"" ssid = ""25"">As already noted  Char97 first guesses the lexical head of a constituent and then  given the head  guesses the PCFG rule used to expand the constituent in question.</S><S sid =""99"" ssid = ""10"">Also  the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events.</S><S sid =""155"" ssid = ""46"">For example  in the Penn Treebank a vp with both main and auxiliary verbs has the structure shown in Figure 3.</S>",['Method_Citation']
18,A00-2018,N06-1022,0,2000,0,"Most recently, McDonald et al (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000))","Most recently, McDonald et al (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000))","['7', '34', '165', '69', '0']","<S sid =""7"" ssid = ""3"">Following [5 10]  our parser is based upon a probabilistic generative model.</S><S sid =""34"" ssid = ""3"">Also  remember that H is a pla ceholder for any other information beyond the constituent c that may be useful in assigning c a probability.</S><S sid =""165"" ssid = ""56"">Note that we also tried including this information using a standard deleted-interpolation model.</S><S sid =""69"" ssid = ""38"">For example  one can well imagine that one might want to condition on the parent's lexical head without conditioning on the left sibling  or the grandparent label.</S><S sid =""0"" ssid = ""0"">A Maximum-Entropy-Inspired Parser *</S>",['Results_Citation']
19,A00-2018,H05-1035,0,"Charniak, 2000",0,"The feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (Charniak, 2000), trees that will be improved by more accurate PP-attachment decisions","The feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (Charniak, 2000), trees that will be improved by more accurate PP-attachment decisions","['41', '37', '169', '7', '125']","<S sid =""41"" ssid = ""10"">For example  in computing the probability of the head's pre-terminal t we might want a feature schema f (t  1) that returns 1 if the observed pre-terminal of c = t and the label of c = 1  and zero otherwise.</S><S sid =""37"" ssid = ""6"">Rather  we concentrate on the aspects of these models that most directly influenced the model presented here.</S><S sid =""169"" ssid = ""60"">Up to this point all the models considered in this section are tree-bank grammar models.</S><S sid =""7"" ssid = ""3"">Following [5 10]  our parser is based upon a probabilistic generative model.</S><S sid =""125"" ssid = ""16"">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S>",['Implication_Citation']
20,A00-2018,P04-1042,0,2000,0,"Note that the dependency figures of Dienes lag behind even the parsed results for Johnson? s model; this may well be due to the fact that Dienes built his model as an extension of Collins (1999), which lags behind Charniak (2000) by about 1.3-1.5% .Manual investigation of errors on English gold standard data revealed two major issues that suggest further potential for improvement in performance without further increase in algorithmic complexity or training set size","Note that the dependency figures of Dienes lag behind even the parsed results for Johnson's model; this may well be due to the fact that Dienes built his model as an extension of Collins (1999), which lags behind Charniak (2000) by about 1.3-1.5%. Manual investigation of errors on English gold standard data revealed two major issues that suggest further potential for improvement in performance without further increase in algorithmic complexity or training set size","['88', '124', '54', '21', '63']","<S sid =""88"" ssid = ""57"">While we could have smoothed in the same fashion  we choose instead to use standard deleted interpolation.</S><S sid =""124"" ssid = ""15"">We note here that this corpus is somewhat more difficult than the &quot;official&quot; test corpus.</S><S sid =""54"" ssid = ""23"">The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).</S><S sid =""21"" ssid = ""10"">(We assume that all terminal symbols are generated by rules of the form &quot;preterm word&quot; and we treat these as a special case.)</S><S sid =""63"" ssid = ""32"">As we discuss in more detail in Section 5  several different features in the context surrounding c are useful to include in H: the label  head pre-terminal and head of the parent of c (denoted as lp  tp  hp)  the label of c's left sibling (lb for &quot;before&quot;)  and the label of the grandparent of c (la).</S>",['Method_Citation']
