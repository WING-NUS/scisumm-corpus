Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,D09-1092,P14-1004,0,"Mimno et al, 2009",0,"This configuration is similar to PolyLDA (Mimno et al, 2009) or LinkLDA (Yano et al, 2009), such that utterances from different parties are treated as different languages or blog-post and comments pairs","This configuration is similar to PolyLDA (Mimno et al, 2009) or LinkLDA (Yano et al, 2009), such that utterances from different parties are treated as different languages or blog-post and comments pairs","['147', '18', '176', '145', '155']","<S sid =""147"" ssid = ""96"">These aligned document pairs could then be fed into standard machine translation systems as training data.</S><S sid =""18"" ssid = ""14"">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid =""176"" ssid = ""10"">We dropped all articles in non-English languages that did not link to an English article.</S><S sid =""145"" ssid = ""94"">For T = 800  the top English and Spanish words in 448 topics were exact translations of one another.</S><S sid =""155"" ssid = ""104"">Finally  for each pair of languages (“query” and “target”) we calculate the difference between the topic distribution for each held-out document in the query language and the topic distribution for each held-out document in the target language.</S>",['Implication_Citation']
2,D09-1092,P10-1044,0,"Mimno et al, 2009",0,"Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009)","Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009)","['63', '156', '32', '23', '64']","<S sid =""63"" ssid = ""12"">Figure 2 shows the most probable words in all languages for four example topics  from PLTM with 400 topics.</S><S sid =""156"" ssid = ""105"">We use both Jensen-Shannon divergence and cosine distance.</S><S sid =""32"" ssid = ""8"">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid =""23"" ssid = ""19"">The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.</S><S sid =""64"" ssid = ""13"">The first topic contains words relating to the European Central Bank.</S>",['Implication_Citation']
3,D09-1092,P11-2084,0,"Mimno et al, 2009",0,"(Mimno et al, 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then add the Cartesian product of these sets for every topic to a set of candidate translations","(Mimno et al, 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then add the Cartesian product of these sets for every topic to a set of candidate translations","['38', '30', '182', '190', '104']","<S sid =""38"" ssid = ""4"">This is unlike LDA  in which each document is assumed to have its own document-specific distribution over topics.</S><S sid =""30"" ssid = ""6"">However  they evaluate their model on only two languages (English and Chinese)  and do not use the model to detect differences between languages.</S><S sid =""182"" ssid = ""16"">As with EuroParl  we can calculate the JensenShannon divergence between pairs of documents within a comparable document tuple.</S><S sid =""190"" ssid = ""24"">Meanwhile  interest in actors and actresses (center) is consistent across all languages.</S><S sid =""104"" ssid = ""53"">This result is important: informally  we have found that increasing the granularity of topics correlates strongly with user perceptions of the utility of a topic model.</S>",['Results_Citation']
4,D09-1092,E12-1014,0,"Mimno et al, 2009",0,"Our Wikipedia-based topic similarity feature, w (f, e), is similar in spirit to polylingualtopic models (Mimno et al 2009), but it is scalable to full bilingual lexicon induction","Our Wikipedia-based topic similarity feature, w (f, e), is similar in spirit to polylingual topic models (Mimno et al 2009), but it is scalable to full bilingual lexicon induction","['164', '160', '69', '104', '43']","<S sid =""164"" ssid = ""113"">Results vary by language.</S><S sid =""160"" ssid = ""109"">It is important to note that the length of documents matters.</S><S sid =""69"" ssid = ""18"">English and the Romance languages use only singular and plural versions of “objective.” The other Germanic languages include compound words  while Greek and Finnish are dominated by inflected variants of the same lexical item.</S><S sid =""104"" ssid = ""53"">This result is important: informally  we have found that increasing the granularity of topics correlates strongly with user perceptions of the utility of a topic model.</S><S sid =""43"" ssid = ""9"">These tasks can either be accomplished by averaging over samples of Φ1  .</S>",['Method_Citation']
5,D09-1092,D11-1086,0,"Mimno et al, 2009",0,"of English document and the second half of its aligned foreign language document (Mimno et al,2009)","For Europarl data sets, we artificially make them comparable by considering the first half of English document and the second half of its aligned foreign language document (Mimno et al,2009)","['32', '23', '38', '172', '63']","<S sid =""32"" ssid = ""8"">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid =""23"" ssid = ""19"">The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.</S><S sid =""38"" ssid = ""4"">This is unlike LDA  in which each document is assumed to have its own document-specific distribution over topics.</S><S sid =""172"" ssid = ""6"">Second  because comparable texts may not use exactly the same topics  it becomes crucially important to be able to characterize differences in topic prevalence at the document level (do different languages have different perspectives on the same article?) and at the language-wide level (which topics do particular languages focus on?).</S><S sid =""63"" ssid = ""12"">Figure 2 shows the most probable words in all languages for four example topics  from PLTM with 400 topics.</S>",['Method_Citation']
6,D09-1092,N12-1007,0,"Mimno et al, 2009",0,"Since the PLTM is not a contribution of this paper, we refer the interested reader to (Mimno et al, 2009) for more details","Since the PLTM is not a contribution of this paper, we refer the interested reader to (Mimno et al, 2009) for more details","['135', '69', '175', '32', '176']","<S sid =""135"" ssid = ""84"">We do not consider multi-word terms.</S><S sid =""69"" ssid = ""18"">English and the Romance languages use only singular and plural versions of “objective.” The other Germanic languages include compound words  while Greek and Finnish are dominated by inflected variants of the same lexical item.</S><S sid =""175"" ssid = ""9"">We preprocessed the data by removing tables  references  images and info-boxes.</S><S sid =""32"" ssid = ""8"">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid =""176"" ssid = ""10"">We dropped all articles in non-English languages that did not link to an English article.</S>",['Method_Citation']
7,D09-1092,N12-1007,0,2009,0,"Evaluation Corpus The automatic evaluation of cross-lingual co reference systems requires annotated 10Mimno et al (2009) showed that so long as the proportion of topically-aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly","Mimno et al (2009) showed that so long as the proportion of topically-aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly","['147', '97', '40', '21', '75']","<S sid =""147"" ssid = ""96"">These aligned document pairs could then be fed into standard machine translation systems as training data.</S><S sid =""97"" ssid = ""46"">Rather  these results are intended as a quantitative analysis of the difference between the two models.</S><S sid =""40"" ssid = ""6"">Anew document tuple w = (w1  ...   wL) is generated by first drawing a tuple-specific topic distribution from an asymmetric Dirichlet prior with concentration parameter α and base measure m: Then  for each language l  a latent topic assignment is drawn for each token in that language: Finally  the observed tokens are themselves drawn using the language-specific topic parameters: wl ∼ P(wl  |zl Φl) = 11n φlwl |zl .</S><S sid =""21"" ssid = ""17"">The second corpus  Wikipedia articles in twelve languages  contains sets of documents that are not translations of one another  but are very likely to be about similar concepts.</S><S sid =""75"" ssid = ""24"">We compute histograms of these maximum topic probabilities for T ∈ {50 100  200  400  800}.</S>",['Method_Citation']
8,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"Similarly, Polylingual Topic Models (PLTM) (Mimno et al, 2009) generalized LDA to tuples of documents from multiple languages","Similarly, Polylingual Topic Models (PLTM) (Mimno et al, 2009) generalized LDA to tuples of documents from multiple languages","['32', '184', '87', '145', '178']","<S sid =""32"" ssid = ""8"">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid =""184"" ssid = ""18"">Interestingly  we find that almost all languages in our corpus  including several pairs that have historically been in conflict  show average JS divergences of between approximately 0.08 and 0.12 for T = 400  consistent with our findings for EuroParl translations.</S><S sid =""87"" ssid = ""36"">The probability of previously unseen held-out document tuples given these estimates can then be computed.</S><S sid =""145"" ssid = ""94"">For T = 800  the top English and Spanish words in 448 topics were exact translations of one another.</S><S sid =""178"" ssid = ""12"">For efficiency  we truncated each article to the nearest word after 1000 characters and dropped the 50 most common word types in each language.</S>",['Method_Citation']
9,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"Our baseline joint PLSA model (JPLSA) is closely related to the poly-lingual LDA model of (Mimno et al, 2009)","Our baseline joint PLSA model (JPLSA) is closely related to the poly-lingual LDA model of (Mimno et al, 2009)","['30', '48', '124', '1', '38']","<S sid =""30"" ssid = ""6"">However  they evaluate their model on only two languages (English and Chinese)  and do not use the model to detect differences between languages.</S><S sid =""48"" ssid = ""14"">.</S><S sid =""124"" ssid = ""73"">At p = 0.01 (1% “glue” documents)  German and French both include words relating to Russia  while the English and Italian word distributions appear locally consistent but unrelated to Russia.</S><S sid =""1"" ssid = ""1"">Topic models are a useful tool for analyzing large text collections  but have previously been applied in only monolingual  or at most bilingual  contexts.</S><S sid =""38"" ssid = ""4"">This is unlike LDA  in which each document is assumed to have its own document-specific distribution over topics.</S>",['Method_Citation']
10,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"We describe the model for two languages, but it is straightforward to generalize to more than two languages, as in (Mimno et al, 2009)","We describe the model for two languages, but it is straightforward to generalize to more than two languages, as in (Mimno et al, 2009)","['69', '23', '9', '40', '186']","<S sid =""69"" ssid = ""18"">English and the Romance languages use only singular and plural versions of “objective.” The other Germanic languages include compound words  while Greek and Finnish are dominated by inflected variants of the same lexical item.</S><S sid =""23"" ssid = ""19"">The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.</S><S sid =""9"" ssid = ""5"">In this paper  we present the polylingual topic model (PLTM).</S><S sid =""40"" ssid = ""6"">Anew document tuple w = (w1  ...   wL) is generated by first drawing a tuple-specific topic distribution from an asymmetric Dirichlet prior with concentration parameter α and base measure m: Then  for each language l  a latent topic assignment is drawn for each token in that language: Finally  the observed tokens are themselves drawn using the language-specific topic parameters: wl ∼ P(wl  |zl Φl) = 11n φlwl |zl .</S><S sid =""186"" ssid = ""20"">Although we find that if Wikipedia contains an article on a particular subject in some language  the article will tend to be topically similar to the articles about that subject in other languages  we also find that across the whole collection different languages emphasize topics to different extents.</S>",['Implication_Citation']
11,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"j=1 P (z2j|?) P (w 2 j| ?z2j) The difference between the JPLSA model and the poly-lingual topic model of (Mimno et al, 2009) is that we merge the vocabularies in the two languages and learn topic-specific word distributions over these merged vocabularies, instead of having pairs of topic-specific word distributions, one for each language, like in (Mimno et al, 2009)","The difference between the JPLSA model and the poly-lingual topic model of (Mimno et al, 2009) is that we merge the vocabularies in the two languages and learn topic-specific word distributions over these merged vocabularies, instead of having pairs of topic-specific word distributions, one for each language, like in (Mimno et al, 2009)","['46', '30', '32', '72', '28']","<S sid =""46"" ssid = ""12"">  ΦL and αm from P(Φ1  ...   ΦL  αm  |W'  β) or by evaluating a point estimate.</S><S sid =""30"" ssid = ""6"">However  they evaluate their model on only two languages (English and Chinese)  and do not use the model to detect differences between languages.</S><S sid =""32"" ssid = ""8"">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid =""72"" ssid = ""21"">To answer this question  we compute the posterior probability of each topic in each tuple under the trained model.</S><S sid =""28"" ssid = ""4"">We take a simpler approach that is more suitable for topically similar document tuples (where documents are not direct translations of one another) in more than two languages.</S>",['Method_Citation']
12,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"Another difference between our model and the poly-lingual LDA model of (Mimno et al, 2009) is that we use maximum aposteriori (MAP) instead of Bayesian inference","Another difference between our model and the poly-lingual LDA model of (Mimno et al, 2009) is that we use maximum aposteriori (MAP) instead of Bayesian inference","['30', '131', '45', '59', '67']","<S sid =""30"" ssid = ""6"">However  they evaluate their model on only two languages (English and Chinese)  and do not use the model to detect differences between languages.</S><S sid =""131"" ssid = ""80"">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid =""45"" ssid = ""11"">.</S><S sid =""59"" ssid = ""8"">The remaining collection consists of over 121 million words.</S><S sid =""67"" ssid = ""16"">(Interestingly  all languages except Greek and Finnish use closely related words for “youth” or “young” in a separate topic.)</S>",['Method_Citation']
13,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"For computing distance we used the L1-norm of the difference, which worked a bit better than the Jensen Shannon divergence between the topic vectors used in (Mimno et al, 2009)","For computing distance we used the L1-norm of the difference, which worked a bit better than the Jensen Shannon divergence between the topic vectors used in (Mimno et al, 2009)","['155', '29', '19', '173', '103']","<S sid =""155"" ssid = ""104"">Finally  for each pair of languages (“query” and “target”) we calculate the difference between the topic distribution for each held-out document in the query language and the topic distribution for each held-out document in the target language.</S><S sid =""29"" ssid = ""5"">A recent extended abstract  developed concurrently by Ni et al. (Ni et al.  2009)  discusses a multilingual topic model similar to the one presented here.</S><S sid =""19"" ssid = ""15"">We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S><S sid =""173"" ssid = ""7"">We downloaded XML copies of all Wikipedia articles in twelve different languages: Welsh  German  Greek  English  Farsi  Finnish  French  Hebrew  Italian  Polish  Russian and Turkish.</S><S sid =""103"" ssid = ""52"">PLTM topics therefore have a higher granularity – i.e.  they are more specific.</S>",['Method_Citation']
15,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"In previously reported work, (Mimno et al, 2009) evaluate parallel document retrieval using PLTM on Europarl speeches in English and Spanish, using training and test sets of size similar to ours","In previously reported work, (Mimno et al, 2009) evaluate parallel document retrieval using PLTM on Europarl speeches in English and Spanish, using training and test sets of size similar to ours","['135', '131', '147', '1', '89']","<S sid =""135"" ssid = ""84"">We do not consider multi-word terms.</S><S sid =""131"" ssid = ""80"">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid =""147"" ssid = ""96"">These aligned document pairs could then be fed into standard machine translation systems as training data.</S><S sid =""1"" ssid = ""1"">Topic models are a useful tool for analyzing large text collections  but have previously been applied in only monolingual  or at most bilingual  contexts.</S><S sid =""89"" ssid = ""38"">Analytically calculating the probability of a set of held-out document tuples given Φ1  ...   ΦL and αm is intractable  due to the summation over an exponential number of topic assignments for these held-out documents.</S>",['Results_Citation']
16,D09-1092,W12-3117,0,"Mimno et al, 2009",0,"We assume that a higher dimensionality can lead to a better repartitioning of the vocabulary over the topics. Multilingual LDA has been used before in natural language processing ,e.g .polylingual topic models (Mimno et al, 2009) or multilingual topic models for unaligned text (Boyd-Graber and Blei, 2009)","Multilingual LDA has been used before in natural language processing, e.g. polylingual topic models (Mimno et al, 2009) or multilingual topic models for unaligned text (Boyd-Graber and Blei, 2009)","['43', '136', '156', '45', '23']","<S sid =""43"" ssid = ""9"">These tasks can either be accomplished by averaging over samples of Φ1  .</S><S sid =""136"" ssid = ""85"">We expect that simple analysis of topic assignments for sequential words would yield such collocations  but we leave this for future work.</S><S sid =""156"" ssid = ""105"">We use both Jensen-Shannon divergence and cosine distance.</S><S sid =""45"" ssid = ""11"">.</S><S sid =""23"" ssid = ""19"">The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.</S>",['Implication_Citation']
17,D09-1092,W11-2133,0,2009,0,"ji =wjk? M j i? m k=1w j k, (1) where M j is the topic distribution of document j and wk is the number of occurrences of phrase pair Xk in document j. Mimno et al (2009) extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipediaarticles)","Mimno et al (2009) extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipedia articles)","['131', '147', '84', '63', '175']","<S sid =""131"" ssid = ""80"">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid =""147"" ssid = ""96"">These aligned document pairs could then be fed into standard machine translation systems as training data.</S><S sid =""84"" ssid = ""33"">As the number of topics increases  greater variability in topic distributions causes divergence to increase.</S><S sid =""63"" ssid = ""12"">Figure 2 shows the most probable words in all languages for four example topics  from PLTM with 400 topics.</S><S sid =""175"" ssid = ""9"">We preprocessed the data by removing tables  references  images and info-boxes.</S>",['Method_Citation']
18,D09-1092,W11-2133,0,2009,0,Tuple-specific topic distributions arelearned using LDA with distinct topic-word concentration parameters? l. Mimno et al (2009) show that PLTM sufficiently aligns topics in parallel corpora,Mimno et al (2009) show that PLTM sufficiently aligns topics in parallel corpora,"['43', '32', '175', '53', '193']","<S sid =""43"" ssid = ""9"">These tasks can either be accomplished by averaging over samples of Φ1  .</S><S sid =""32"" ssid = ""8"">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid =""175"" ssid = ""9"">We preprocessed the data by removing tables  references  images and info-boxes.</S><S sid =""53"" ssid = ""2"">In this case  we can be confident that the topic distribution is genuinely shared across all languages.</S><S sid =""193"" ssid = ""2"">We analyzed the characteristics of PLTM in comparison to monolingual LDA  and demonstrated that it is possible to discover aligned topics.</S>",['Results_Citation']
19,D09-1092,P14-2110,0,"Mimno et al, 2009",0,"A good candidate for multilingual topic analyses are polylin gual topic models (Mimno et al, 2009), which learn topics for multiple languages, creating tuples of language specific distributions over monolingual vocabularies for each topic","A good candidate for multilingual topic analyses are polylingual topic models (Mimno et al, 2009), which learn topics for multiple languages, creating tuples of language specific distributions over monolingual vocabularies for each topic","['97', '52', '30', '135', '103']","<S sid =""97"" ssid = ""46"">Rather  these results are intended as a quantitative analysis of the difference between the two models.</S><S sid =""52"" ssid = ""1"">Our first set of experiments focuses on document tuples that are known to consist of direct translations.</S><S sid =""30"" ssid = ""6"">However  they evaluate their model on only two languages (English and Chinese)  and do not use the model to detect differences between languages.</S><S sid =""135"" ssid = ""84"">We do not consider multi-word terms.</S><S sid =""103"" ssid = ""52"">PLTM topics therefore have a higher granularity – i.e.  they are more specific.</S>",['Aim_Citation']
20,D09-1092,P14-2110,0,2009,0,"3csLDATo train a polylingual topic model on social media, we make two modifications to the model of Mimno et al (2009): add a token specific language variable, and a process for identifying aligned top ics. First ,polylingual topic models require parallel or comparable corpora in which each document has an assigned language","To train a polylingual topic model on social media, we make two modifications to the model of Mimno et al (2009): add a token specific language variable, and a process for identifying aligned top ics","['14', '59', '190', '1', '31']","<S sid =""14"" ssid = ""10"">Such analysis could be significant in tracking international research trends  where language barriers slow the transfer of ideas.</S><S sid =""59"" ssid = ""8"">The remaining collection consists of over 121 million words.</S><S sid =""190"" ssid = ""24"">Meanwhile  interest in actors and actresses (center) is consistent across all languages.</S><S sid =""1"" ssid = ""1"">Topic models are a useful tool for analyzing large text collections  but have previously been applied in only monolingual  or at most bilingual  contexts.</S><S sid =""31"" ssid = ""7"">They also provide little analysis of the differences between polylingual and single-language topic models.</S>",['Method_Citation']
