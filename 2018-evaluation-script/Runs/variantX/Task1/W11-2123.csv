Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,W11-2123,W11-2138,0,"Heafield, 2011",0,"We used common tools for phrase-based translation? Moses (Koehn et al, 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modelling and GIZA++ (Och and Ney, 2000) for word alignments","We used common tools for phrase-based translation Moses (Koehn et al, 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modelling and GIZA++ (Och and Ney, 2000) for word alignments","['270', '276', '265', '286', '284']","<S sid =""270"" ssid = ""12"">This information is readily available in TRIE where adjacent records with equal pointers indicate no further extension of context is possible.</S><S sid =""276"" ssid = ""3"">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S><S sid =""265"" ssid = ""7"">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid =""286"" ssid = ""7"">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid =""284"" ssid = ""5"">Chris Dyer integrated the code into cdec.</S>",['Implication_Citation']
2,W11-2123,P14-2022,0,"Heafield, 2011",0,"The language model was com piled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run","The language model was compiled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run","['262', '280', '265', '277', '284']","<S sid =""262"" ssid = ""4"">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid =""280"" ssid = ""1"">Alon Lavie advised on this work.</S><S sid =""265"" ssid = ""7"">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid =""277"" ssid = ""4"">These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.</S><S sid =""284"" ssid = ""5"">Chris Dyer integrated the code into cdec.</S>",['Implication_Citation']
3,W11-2123,W12-3145,0,"Heafield, 2011",0,"Thus given afragment tf consisting of a sequence of target to kens, we compute LM scores for (i) < s& gt ;tf, (ii )tf and (iii )tf < /s& gt; and use the best score (only) for pruning.2 While this increases the number ofLM queries, we exploit the language model state in formation in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states","Thus given a fragment tf consisting of a sequence of target tokens, we compute LM scores for (i) < s& gt ;tf, (ii )tf and (iii )tf < /s& gt; and use the best score (only) for pruning. While this increases the number of LM queries, we exploit the language model state in formation in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states","['265', '284', '278', '285', '276']","<S sid =""265"" ssid = ""7"">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid =""284"" ssid = ""5"">Chris Dyer integrated the code into cdec.</S><S sid =""278"" ssid = ""5"">We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.</S><S sid =""285"" ssid = ""6"">Juri Ganitkevitch answered questions about Joshua.</S><S sid =""276"" ssid = ""3"">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S>",['Results_Citation']
4,W11-2123,W12-3131,0,"Heafield, 2011",0,"Our translation system uses cdec (Dyer et al,2010), an implementation of the hierarchical phrase based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference","Our translation system uses cdec (Dyer et al,2010), an implementation of the hierarchical phrase based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference","['260', '262', '280', '283', '266']","<S sid =""260"" ssid = ""2"">For speed  we plan to implement the direct-mapped cache from BerkeleyLM.</S><S sid =""262"" ssid = ""4"">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid =""280"" ssid = ""1"">Alon Lavie advised on this work.</S><S sid =""283"" ssid = ""4"">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S><S sid =""266"" ssid = ""8"">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S>",['Method_Citation']
5,W11-2123,W12-3154,0,"Heafield, 2011",0,"The three data sets in use in this paper are summarised in Table 1.The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al, 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime","The three data sets in use in this paper are summarised in Table 1.The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al, 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime","['272', '265', '283', '286', '268']","<S sid =""272"" ssid = ""14"">Generalizing state minimization  the model could also provide explicit bounds on probability for both backward and forward extension.</S><S sid =""265"" ssid = ""7"">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid =""283"" ssid = ""4"">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S><S sid =""286"" ssid = ""7"">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid =""268"" ssid = ""10"">For example  syntactic decoders (Koehn et al.  2007; Dyer et al.  2010; Li et al.  2009) perform dynamic programming parametrized by both backward- and forward-looking state.</S>",['Method_Citation']
6,W11-2123,P12-2058,0,"Heafield, 2011",0,"The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011)","The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011)","['256', '284', '265', '264', '267']","<S sid =""256"" ssid = ""75"">We elected run Moses single-threaded to minimize the impact of RandLM’s cache on memory use.</S><S sid =""284"" ssid = ""5"">Chris Dyer integrated the code into cdec.</S><S sid =""265"" ssid = ""7"">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid =""264"" ssid = ""6"">For even larger models  storing counts (Talbot and Osborne  2007; Pauls and Klein  2011; Guthrie and Hepple  2010) is a possibility.</S><S sid =""267"" ssid = ""9"">While we have minimized forward-looking state in Section 4.1  machine translation systems could also benefit by minimizing backward-looking state.</S>",['Method_Citation']
7,W11-2123,W11-2139,0,2011,0,Inference was carried out using the language modeling library described by Heafield (2011),Inference was carried out using the language modeling library described by Heafield (2011),"['283', '284', '280', '265', '275']","<S sid =""283"" ssid = ""4"">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S><S sid =""284"" ssid = ""5"">Chris Dyer integrated the code into cdec.</S><S sid =""280"" ssid = ""1"">Alon Lavie advised on this work.</S><S sid =""265"" ssid = ""7"">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid =""275"" ssid = ""2"">The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.</S>",['Method_Citation']
8,W11-2123,P13-2003,0,"Heafield, 2011",0,"We used the MADA ATB segmentation for Arabic (Roth et al, 2008) and true casing for English, phrases of maximal length 7, KneserNey smoothing, and lexicalized reordering (Koehn et al, 2005), and a 5-gram language model, trained on GigaWordv.5 using KenLM (Heafield, 2011)","We used the MADA ATB segmentation for Arabic (Roth et al, 2008) and true casing for English, phrases of maximal length 7, KneserNey smoothing, and lexicalized reordering (Koehn et al, 2005), and a 5-gram language model, trained on GigaWordv.5 using KenLM (Heafield, 2011)","['261', '277', '286', '284', '283']","<S sid =""261"" ssid = ""3"">Much could be done to further reduce memory consumption.</S><S sid =""277"" ssid = ""4"">These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.</S><S sid =""286"" ssid = ""7"">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid =""284"" ssid = ""5"">Chris Dyer integrated the code into cdec.</S><S sid =""283"" ssid = ""4"">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S>",['Method_Citation']
9,W11-2123,W12-3134,0,2011,0,The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language mod els by Heafield (2011) and Pauls and Klein (2011)? both language model implementations are now integrated with Joshua,The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language models by Heafield (2011) and Pauls and Klein (2011)? both language model implementations are now integrated with Joshua,"['286', '280', '267', '279', '283']","<S sid =""286"" ssid = ""7"">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid =""280"" ssid = ""1"">Alon Lavie advised on this work.</S><S sid =""267"" ssid = ""9"">While we have minimized forward-looking state in Section 4.1  machine translation systems could also benefit by minimizing backward-looking state.</S><S sid =""279"" ssid = ""6"">The code is opensource  has minimal dependencies  and offers both C++ and Java interfaces for integration.</S><S sid =""283"" ssid = ""4"">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S>",['Method_Citation']
10,W11-2123,W12-3134,0,2011,0,Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets,Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets,"['256', '287', '266', '262', '265']","<S sid =""256"" ssid = ""75"">We elected run Moses single-threaded to minimize the impact of RandLM’s cache on memory use.</S><S sid =""287"" ssid = ""8"">0750271 and by the DARPA GALE program.</S><S sid =""266"" ssid = ""8"">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S><S sid =""262"" ssid = ""4"">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid =""265"" ssid = ""7"">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S>",['Implication_Citation']
11,W11-2123,W12-3134,0,2011,0,"With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult to compile SRILM toolkit (Stolcke, 2002)","With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult to compile SRILM toolkit (Stolcke, 2002)","['262', '278', '277', '265', '264']","<S sid =""262"" ssid = ""4"">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid =""278"" ssid = ""5"">We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.</S><S sid =""277"" ssid = ""4"">These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.</S><S sid =""265"" ssid = ""7"">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid =""264"" ssid = ""6"">For even larger models  storing counts (Talbot and Osborne  2007; Pauls and Klein  2011; Guthrie and Hepple  2010) is a possibility.</S>",['Method_Citation']
12,W11-2123,W12-3160,0,"Heafield, 2011",0,"This was used to create a KenLM (Heafield, 2011)","This was used to create a KenLM (Heafield, 2011)","['261', '265', '266', '267', '269']","<S sid =""261"" ssid = ""3"">Much could be done to further reduce memory consumption.</S><S sid =""265"" ssid = ""7"">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid =""266"" ssid = ""8"">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S><S sid =""267"" ssid = ""9"">While we have minimized forward-looking state in Section 4.1  machine translation systems could also benefit by minimizing backward-looking state.</S><S sid =""269"" ssid = ""11"">If they knew that the first four words in a hypergraph node would never extend to the left and form a 5-gram  then three or even fewer words could be kept in the backward state.</S>",['Method_Citation']
13,W11-2123,W12-3706,0,"Heafield, 2011",0,"In the Opinum system we query the M p, M n mod els with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable fora web application","In the Opinum system we query the M p, M n models with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable for a web application","['287', '266', '286', '262', '274']","<S sid =""287"" ssid = ""8"">0750271 and by the DARPA GALE program.</S><S sid =""266"" ssid = ""8"">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S><S sid =""286"" ssid = ""7"">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid =""262"" ssid = ""4"">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid =""274"" ssid = ""1"">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S>",['Method_Citation']
14,W11-2123,W11-2147,0,"Heafield, 2011",0,"Our base line is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al, 2007) for translation model training and decoding, GIZA++ (Ochand Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights","Our base line is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al, 2007) for translation model training and decoding, GIZA++ (Ochand Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights","['286', '256', '265', '284', '276']","<S sid =""286"" ssid = ""7"">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid =""256"" ssid = ""75"">We elected run Moses single-threaded to minimize the impact of RandLM’s cache on memory use.</S><S sid =""265"" ssid = ""7"">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid =""284"" ssid = ""5"">Chris Dyer integrated the code into cdec.</S><S sid =""276"" ssid = ""3"">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S>",['Results_Citation']
15,W11-2123,E12-1083,0,"Heafield, 2011",0,"For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011)","For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011)","['278', '262', '265', '275', '270']","<S sid =""278"" ssid = ""5"">We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.</S><S sid =""262"" ssid = ""4"">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid =""265"" ssid = ""7"">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid =""275"" ssid = ""2"">The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.</S><S sid =""270"" ssid = ""12"">This information is readily available in TRIE where adjacent records with equal pointers indicate no further extension of context is possible.</S>",['Implication_Citation']
16,W11-2123,P12-1002,0,"Heafield, 2011",0,"Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Sima ?an,2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well.3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using kenlm (Heafield, 2011)","Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Simaan,2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well.3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using kenlm (Heafield, 2011)","['280', '262', '265', '279', '256']","<S sid =""280"" ssid = ""1"">Alon Lavie advised on this work.</S><S sid =""262"" ssid = ""4"">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid =""265"" ssid = ""7"">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid =""279"" ssid = ""6"">The code is opensource  has minimal dependencies  and offers both C++ and Java interfaces for integration.</S><S sid =""256"" ssid = ""75"">We elected run Moses single-threaded to minimize the impact of RandLM’s cache on memory use.</S>",['Method_Citation']
17,W11-2123,D12-1108,0,"Heafield, 2011",0,"n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3","n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3","['279', '256', '262', '265', '276']","<S sid =""279"" ssid = ""6"">The code is opensource  has minimal dependencies  and offers both C++ and Java interfaces for integration.</S><S sid =""256"" ssid = ""75"">We elected run Moses single-threaded to minimize the impact of RandLM’s cache on memory use.</S><S sid =""262"" ssid = ""4"">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid =""265"" ssid = ""7"">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid =""276"" ssid = ""3"">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S>",['Results_Citation']
18,W11-2123,P12-2006,0,"Heafield, 2011",0,"Research efforts to increase search efficiency for phrase-based MT (Koehn et al, 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al, 2006) to additional early pruning techniques (Delaney et al, 2006), (Moore and Quirk, 2007) and more efficient language model (LM) querying (Heafield, 2011)","Research efforts to increase search efficiency for phrase-based MT (Koehn et al, 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al, 2006) to additional early pruning techniques (Delaney et al, 2006), (Moore and Quirk, 2007) and more efficient language model (LM) querying (Heafield, 2011)","['256', '277', '286', '287', '264']","<S sid =""256"" ssid = ""75"">We elected run Moses single-threaded to minimize the impact of RandLM’s cache on memory use.</S><S sid =""277"" ssid = ""4"">These performance gains transfer to improved system runtime performance; though we focused on Moses  our code is the best lossless option with cdec and Joshua.</S><S sid =""286"" ssid = ""7"">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.</S><S sid =""287"" ssid = ""8"">0750271 and by the DARPA GALE program.</S><S sid =""264"" ssid = ""6"">For even larger models  storing counts (Talbot and Osborne  2007; Pauls and Klein  2011; Guthrie and Hepple  2010) is a possibility.</S>",['Aim_Citation']
19,W11-2123,P13-2073,0,"Heafield, 2011",0,"For English language modeling, we use English Giga word Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011)","For English language modeling, we use English Giga word Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011)","['263', '265', '287', '283', '258']","<S sid =""263"" ssid = ""5"">Quantization can be improved by jointly encoding probability and backoff.</S><S sid =""265"" ssid = ""7"">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid =""287"" ssid = ""8"">0750271 and by the DARPA GALE program.</S><S sid =""283"" ssid = ""4"">Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S><S sid =""258"" ssid = ""77"">However  the point of RandLM is to scale to even larger data  compensating for this loss in quality.</S>",['Method_Citation']
20,W11-2123,P13-1109,0,"Heafield, 2011",0,"For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with KneserNey smoothing","For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with KneserNey smoothing","['265', '280', '256', '278', '267']","<S sid =""265"" ssid = ""7"">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S><S sid =""280"" ssid = ""1"">Alon Lavie advised on this work.</S><S sid =""256"" ssid = ""75"">We elected run Moses single-threaded to minimize the impact of RandLM’s cache on memory use.</S><S sid =""278"" ssid = ""5"">We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.</S><S sid =""267"" ssid = ""9"">While we have minimized forward-looking state in Section 4.1  machine translation systems could also benefit by minimizing backward-looking state.</S>",['Aim_Citation']
