Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P08-1028,D08-1094,0,2008,0,"Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p+ a as a function f operating on four components: c= f (p, a, R, K) (3) R is the relation holding between p and a, and Kadditional knowledge","Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p+ a as a function f operating on four components: c= f (p, a, R, K) (3) R is the relation holding between p and a, and K additional knowledge","['200', '136', '164', '135', '149']","<S sid =""200"" ssid = ""12"">The applications of the framework discussed here are many and varied both for cognitive science and NLP.</S><S sid =""136"" ssid = ""49"">We believe that this level of agreement is satisfactory given that naive subjects are asked to provide judgments on fine-grained semantic distinctions (see Table 1).</S><S sid =""164"" ssid = ""77"">A more scrupulous evaluation requires directly correlating all the individual participants’ similarity judgments with those of the models.6 We used Spearman’s p for our correlation analyses.</S><S sid =""135"" ssid = ""48"">The average inter-subject agreement5 was ρ = 0.40.</S><S sid =""149"" ssid = ""62"">Our composition models have no additional parameters beyond the semantic space just described  with three exceptions.</S>",['Implication_Citation']
4,P08-1028,P14-1060,0,2008,0,"While works such asthe SDSM model suffer from the problem of sparsity in composing structures beyond bi grams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstetteand Sadrzadeh (2011) are restricted by significant model biases in representing semantic com position by generic algebraic operations","While works such as the SDSM model suffer from the problem of sparsity in composing structures beyond bigrams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstette and Sadrzadeh (2011) are restricted by significant model biases in representing semantic com position by generic algebraic operations","['107', '63', '42', '45', '192']","<S sid =""107"" ssid = ""20"">Landmarks were taken from WordNet (Fellbaum  1998).</S><S sid =""63"" ssid = ""11"">This reduces the class of models to: However  this still leaves the particular form of the function f unspecified.</S><S sid =""42"" ssid = ""15"">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S><S sid =""45"" ssid = ""18"">Vector addition does not increase the dimensionality of the resulting vector.</S><S sid =""192"" ssid = ""4"">We conjecture that the additive models are not sensitive to the fine-grained meaning distinctions involved in our materials.</S>",['Implication_Citation']
6,P08-1028,P10-1097,0,2008,0,"Mitchell and Lapata (2008), henceforth M& amp; L, propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression","Mitchell and Lapata (2008), henceforth M& amp; L, propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression","['30', '78', '164', '171', '176']","<S sid =""30"" ssid = ""3"">For the hierarchical structure of natural language this binding problem becomes particularly acute.</S><S sid =""78"" ssid = ""26"">These vectors are not arbitrary and ideally they must exhibit some relation to the words of the construction under consideration.</S><S sid =""164"" ssid = ""77"">A more scrupulous evaluation requires directly correlating all the individual participants’ similarity judgments with those of the models.6 We used Spearman’s p for our correlation analyses.</S><S sid =""171"" ssid = ""5"">For comparison  we also show the human ratings for these items (UpperBound).</S><S sid =""176"" ssid = ""10"">The multiplicative and combined models yield means closer to the human ratings.</S>",['Results_Citation']
7,P08-1028,P10-1097,0,2008,0,"Interestingly, Mitchell and Lapata (2008) came to the same result in a different setting","Interestingly, Mitchell and Lapata (2008) came to the same result in a different setting","['42', '103', '85', '101', '63']","<S sid =""42"" ssid = ""15"">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S><S sid =""103"" ssid = ""16"">All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).</S><S sid =""85"" ssid = ""33"">One potential drawback of multiplicative models is the effect of components with value zero.</S><S sid =""101"" ssid = ""14"">Materials and Design Our materials consisted of sentences with an an intransitive verb and its subject.</S><S sid =""63"" ssid = ""11"">This reduces the class of models to: However  this still leaves the particular form of the function f unspecified.</S>",['Method_Citation']
8,P08-1028,D11-1094,0,2008,0,"And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to combine the constituent vectors","And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to combine the constituent vectors","['97', '126', '192', '120', '165']","<S sid =""97"" ssid = ""10"">Moreover by using a minimal structure we factor out inessential degrees of freedom and are able to assess the merits of different models on an equal footing.</S><S sid =""126"" ssid = ""39"">49 unpaid volunteers completed the experiment  all native speakers of English.</S><S sid =""192"" ssid = ""4"">We conjecture that the additive models are not sensitive to the fine-grained meaning distinctions involved in our materials.</S><S sid =""120"" ssid = ""33"">Examples of our items are given in Table 1.</S><S sid =""165"" ssid = ""78"">Again  better models should correlate better with the experimental data.</S>",['Method_Citation']
9,P08-1028,W11-0131,0,2008,0,"Mitchell and Lapata (2008) provide a general framework for semantic vector composition: p= f (u, v, R, K) (1) 295 where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base, and p is a resulting composed vector (or tensor)","Mitchell and Lapata (2008) provide a general framework for semantic vector composition: p= f (u, v, R, K) (1) 295 where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base, and p is a resulting composed vector (or tensor)","['42', '121', '45', '116', '68']","<S sid =""42"" ssid = ""15"">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S><S sid =""121"" ssid = ""34"">Here  burn is a high similarity landmark (High) for the reference The fire glowed  whereas beam is a low similarity landmark (Low).</S><S sid =""45"" ssid = ""18"">Vector addition does not increase the dimensionality of the resulting vector.</S><S sid =""116"" ssid = ""29"">We used Fisher’s exact test to determine which verbs and nouns showed the greatest variation in landmark preference and items with p-values greater than 0.001 were discarded.</S><S sid =""68"" ssid = ""16"">Although the composition model in (5) is commonly used in the literature  from a linguistic perspective  the model in (6) is more appealing.</S>",['Method_Citation']
10,P08-1028,W11-0131,0,2008,0,"As Mitchell and Lapata (2008) did, let us temporarily suspend discussion on what semantics populate our vectors for now","As Mitchell and Lapata (2008) did, let us temporarily suspend discussion on what semantics populate our vectors for now","['100', '123', '112', '25', '63']","<S sid =""100"" ssid = ""13"">In the following we describe our data collection procedure and give details on how our composition models were constructed and evaluated.</S><S sid =""123"" ssid = ""36"">Sentence pairs were presented serially in random order.</S><S sid =""112"" ssid = ""25"">The stimuli were administered to four separate groups; each group saw one set of 100 sentences.</S><S sid =""25"" ssid = ""21"">We present a general framework for vector-based composition which allows us to consider different classes of models.</S><S sid =""63"" ssid = ""11"">This reduces the class of models to: However  this still leaves the particular form of the function f unspecified.</S>",['Method_Citation']
11,P08-1028,P13-2083,0,2008,0,"Mitchell and Lapata (2008) propose a framework to define the composition c= f (a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition","Mitchell and Lapata (2008) propose a framework to define the composition c= f (a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition","['4', '85', '145', '121', '103']","<S sid =""4"" ssid = ""4"">Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.</S><S sid =""85"" ssid = ""33"">One potential drawback of multiplicative models is the effect of components with value zero.</S><S sid =""145"" ssid = ""58"">The latter were the most common context words (excluding a stop list of function words).</S><S sid =""121"" ssid = ""34"">Here  burn is a high similarity landmark (High) for the reference The fire glowed  whereas beam is a low similarity landmark (Low).</S><S sid =""103"" ssid = ""16"">All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).</S>",['Method_Citation']
12,P08-1028,P13-2083,0,"Mitchell and Lapata, 2008",0,"As our final set of baselines, we extend two simple techniques proposed by (Mitchell and Lapata, 2008) that use element-wise addition and multiplication operators to perform composition","As our final set of baselines, we extend two simple techniques proposed by (Mitchell and Lapata, 2008) that use element-wise addition and multiplication operators to perform composition","['25', '45', '95', '150', '42']","<S sid =""25"" ssid = ""21"">We present a general framework for vector-based composition which allows us to consider different classes of models.</S><S sid =""45"" ssid = ""18"">Vector addition does not increase the dimensionality of the resulting vector.</S><S sid =""95"" ssid = ""8"">Focusing on a single compositional structure  namely intransitive verbs and their subjects  is a good point of departure for studying vector combination.</S><S sid =""150"" ssid = ""63"">First  the additive model in (7) weighs differentially the contribution of the two constituents.</S><S sid =""42"" ssid = ""15"">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S>",['Method_Citation']
13,P08-1028,P10-1021,0,2008,0,"Althoughthis model has been shown to successfully simulate single and multiple-word priming (McDonald and Brew 2004), it failed to predict processing costs in the Embra eye-tracking corpus (McDonald and Shillcock 2003) .In this work we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008)","Although this model has been shown to successfully simulate single and multiple-word priming (McDonald and Brew 2004), it failed to predict processing costs in the Embra eye-tracking corpus (McDonald and Shillcock 2003). In this work we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008)","['136', '132', '158', '200', '17']","<S sid =""136"" ssid = ""49"">We believe that this level of agreement is satisfactory given that naive subjects are asked to provide judgments on fine-grained semantic distinctions (see Table 1).</S><S sid =""132"" ssid = ""45"">We also measured how well humans agree in their ratings.</S><S sid =""158"" ssid = ""71"">The m neighbors most similar to the predicate  and the k of m neighbors closest to its argument.</S><S sid =""200"" ssid = ""12"">The applications of the framework discussed here are many and varied both for cognitive science and NLP.</S><S sid =""17"" ssid = ""13"">It was not the sales manager who hit the bottle that day  but the office worker with the serious drinking problem. b.</S>",['Implication_Citation']
14,P08-1028,P10-1021,0,2008,0,"Assuming that h is a linear function of the Cartesian product of u and v allows to specify additive models which are by far the most common method of vector combination in the literature: hi =ui+ vi (3) Alternatively, we can assume that h is a linear function of the tensor product of u and v, and thus derive models based on multiplication: hi =ui? vi (4) Mitchell and Lapata (2008) show that several additive and multiplicative models can be formulated under this framework, including the well known tensor products (Smolensky 1990) and circular convolution (Plate 1995)","Mitchell and Lapata (2008) show that several additive and multiplicative models can be formulated under this framework, including the well known tensor products (Smolensky 1990) and circular convolution (Plate 1995)","['95', '150', '190', '46', '29']","<S sid =""95"" ssid = ""8"">Focusing on a single compositional structure  namely intransitive verbs and their subjects  is a good point of departure for studying vector combination.</S><S sid =""150"" ssid = ""63"">First  the additive model in (7) weighs differentially the contribution of the two constituents.</S><S sid =""190"" ssid = ""2"">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S><S sid =""46"" ssid = ""19"">However  since it is order independent  it cannot capture meaning differences that are modulated by differences in syntactic structure.</S><S sid =""29"" ssid = ""2"">While neural networks can readily represent single distinct objects  in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.</S>",['Method_Citation']
15,P08-1028,W11-0115,0,2008,0,"Mitchell and Lapata (2008) introduce a whole family of models of compositionality based on vector addition and point wise-multiplication (and a weighted combination of both), evaluated on a sentence similarity task inspired by Kintsch (2001)","Mitchell and Lapata (2008) introduce a whole family of models of compositionality based on vector addition and point wise-multiplication (and a weighted combination of both), evaluated on a sentence similarity task inspired by Kintsch (2001)","['34', '103', '116', '150', '156']","<S sid =""34"" ssid = ""7"">Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.</S><S sid =""103"" ssid = ""16"">All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).</S><S sid =""116"" ssid = ""29"">We used Fisher’s exact test to determine which verbs and nouns showed the greatest variation in landmark preference and items with p-values greater than 0.001 were discarded.</S><S sid =""150"" ssid = ""63"">First  the additive model in (7) weighs differentially the contribution of the two constituents.</S><S sid =""156"" ssid = ""69"">This yielded a weighted sum consisting of 95% verb  0% noun and 5% of their multiplicative combination.</S>",['Method_Citation']
16,P08-1028,W11-0115,0,2008,0,"The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression","The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression","['176', '42', '157', '88', '38']","<S sid =""176"" ssid = ""10"">The multiplicative and combined models yield means closer to the human ratings.</S><S sid =""42"" ssid = ""15"">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S><S sid =""157"" ssid = ""70"">Finally  Kintsch’s (2001) additive model has two extra parameters.</S><S sid =""88"" ssid = ""1"">We evaluated the models presented in Section 3 on a sentence similarity task initially proposed by Kintsch (2001).</S><S sid =""38"" ssid = ""11"">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S>",['Method_Citation']
17,P08-1028,W11-0115,0,2008,0,"For example, Mitchell and Lapata (2008) use their models to approximate the human ratings in their sentence similarity dataset","For example, Mitchell and Lapata (2008) use their models to approximate the human ratings in their sentence similarity dataset","['63', '116', '150', '59', '114']","<S sid =""63"" ssid = ""11"">This reduces the class of models to: However  this still leaves the particular form of the function f unspecified.</S><S sid =""116"" ssid = ""29"">We used Fisher’s exact test to determine which verbs and nouns showed the greatest variation in landmark preference and items with p-values greater than 0.001 were discarded.</S><S sid =""150"" ssid = ""63"">First  the additive model in (7) weighs differentially the contribution of the two constituents.</S><S sid =""59"" ssid = ""7"">It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence  via the argument R  on syntax.</S><S sid =""114"" ssid = ""27"">For each reference verb  the subjects’ responses were entered into a contingency table  whose rows corresponded to nouns and columns to each possible answer (i.e.  one of the two landmarks).</S>",['Results_Citation']
18,P08-1028,W11-1310,0,2008,0,We use other WSM settings following Mitchell and Lapata (2008),We use other WSM settings following Mitchell and Lapata (2008),"['42', '121', '116', '64', '103']","<S sid =""42"" ssid = ""15"">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S><S sid =""121"" ssid = ""34"">Here  burn is a high similarity landmark (High) for the reference The fire glowed  whereas beam is a low similarity landmark (Low).</S><S sid =""116"" ssid = ""29"">We used Fisher’s exact test to determine which verbs and nouns showed the greatest variation in landmark preference and items with p-values greater than 0.001 were discarded.</S><S sid =""64"" ssid = ""12"">Now  if we assume that p lies in the same space as u and v  avoiding the issues of dimensionality associated with tensor products  and that f is a linear function  for simplicity  of the cartesian product of u and v  then we generate a class of additive models: where A and B are matrices which determine the contributions made by u and v to the product p. In contrast  if we assume that f is a linear function of the tensor product of u and v  then we obtain multiplicative models: where C is a tensor of rank 3  which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.</S><S sid =""103"" ssid = ""16"">All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).</S>",['Implication_Citation']
19,P08-1028,W11-1310,0,2008,0,Mitchell and Lapata (2008) observed that a simple multiplication function modelled compositionality better than addition,Mitchell and Lapata (2008) observed that a simple multiplication function modelled compositionality better than addition,"['194', '77', '70', '59', '85']","<S sid =""194"" ssid = ""6"">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid =""77"" ssid = ""25"">For additive models  a natural way to achieve this is to include further vectors into the summation.</S><S sid =""70"" ssid = ""18"">Instead  it could be argued that the contribution of the ith component of u should be scaled according to its relevance to v  and vice versa.</S><S sid =""59"" ssid = ""7"">It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence  via the argument R  on syntax.</S><S sid =""85"" ssid = ""33"">One potential drawback of multiplicative models is the effect of components with value zero.</S>",['Method_Citation']
20,P08-1028,W11-1310,0,"Mitchell and Lapata, 2008",0,"We use the compositionality functions, simple addition and simple multiplication to build compositional vectors Vwr1+wr2 and Vwr1 ?wr2. These are as described in (Mitchell and Lapata, 2008)","We use the compositionality functions, simple addition and simple multiplication to build compositional vectors Vwr1+wr2 and Vwr1 ?wr2. These are as described in (Mitchell and Lapata, 2008)","['121', '103', '130', '82', '73']","<S sid =""121"" ssid = ""34"">Here  burn is a high similarity landmark (High) for the reference The fire glowed  whereas beam is a low similarity landmark (Low).</S><S sid =""103"" ssid = ""16"">All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll  2002) version of the British National Corpus (BNC).</S><S sid =""130"" ssid = ""43"">As we can see sentences with high similarity landmarks are perceived as more similar to the reference sentence.</S><S sid =""82"" ssid = ""30"">In contrast to the simple additive model  this extended model is sensitive to syntactic structure  since n is chosen from among the neighbors of the predicate  distinguishing it from the argument.</S><S sid =""73"" ssid = ""21"">Relaxing the assumption of symmetry in the case of the simple additive model produces a model which weighs the contribution of the two components differently: This allows additive models to become more syntax aware  since semantically important constituents can participate more actively in the composition.</S>",['Results_Citation']
