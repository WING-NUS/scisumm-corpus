Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,J01-2004,W05-0104,0,2001,0,"Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark, see Roark (2001))","Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark, see Roark (2001))","['224', '322', '358', '168', '356']","<S sid =""224"" ssid = ""128"">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid =""322"" ssid = ""78"">Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.</S><S sid =""358"" ssid = ""114"">We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice  along with the acoustic and trigram scores.'</S><S sid =""168"" ssid = ""72"">For example  in Figure 2  constituent (NP-DT-NN) is simply NP.</S><S sid =""356"" ssid = ""112"">The DARPA '93 HUB1 test setup consists of 213 utterances read from the Wall Street Journal  a total of 3 446 words.</S>",['Results_Citation']
2,J01-2004,P08-1013,0,2001,0,"Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition. These models have in common that they explicitly or implicitly use a context-free grammar induced from a tree bank, with the exception of Chelba and Jelinek (2000)",Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition,"['17', '102', '340', '113', '12']","<S sid =""17"" ssid = ""5"">This paper will examine language modeling for speech recognition from a natural language processing point of view.</S><S sid =""102"" ssid = ""6"">One approach to syntactic language modeling is to use this distribution directly as a language model.</S><S sid =""340"" ssid = ""96"">The trigram model was also trained on Sections 00-20 of the C&J corpus.</S><S sid =""113"" ssid = ""17"">In empirical trials  Goddeau used the top two stack entries to condition the word probability.</S><S sid =""12"" ssid = ""6"">A small recognition experiment also demonstrates the utility of the model.</S>",['Method_Citation']
4,J01-2004,P04-1015,0,"Roark, 2001a",0,"Theperceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn tree bank","The perceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn treebank","['134', '95', '302', '224', '104']","<S sid =""134"" ssid = ""38"">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid =""95"" ssid = ""53"">For an interpolated (n + 1)-gram: Here P is the empirically observed relative frequency  and An is a function from Vn to [0  1].</S><S sid =""302"" ssid = ""58"">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid =""224"" ssid = ""128"">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid =""104"" ssid = ""8"">The algorithms both utilize a left-corner matrix  which can be calculated in closed form through matrix inversion.</S>",['Method_Citation']
5,J01-2004,P04-1015,0,"Roark, 2001a",0,"We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the per ceptron model gives performance competitive to that of the generative model on parsing the Penn tree bank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search","We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the perceptron model gives performance competitive to that of the generative model on parsing the Penn treebank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search","['134', '167', '112', '85', '390']","<S sid =""134"" ssid = ""38"">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid =""167"" ssid = ""71"">In order to avoid any confusions in identifying the nonterminal label of a particular rule production in either its factored or rionfactored version  we introduce the function constituent (A) for every nonterminal in the factored grammar Gf  which is simply the label of the constituent whose factorization results in A.</S><S sid =""112"" ssid = ""16"">Goddeau (1992) used a robust deterministic shift-reduce parser to condition word probabilities by extracting a specified number of stack entries from the top of the current state  and conditioning on those entries in a way similar to an n-gram.</S><S sid =""85"" ssid = ""43"">The simple definition of c-command that we will be using in this paper is the following: a node A c-commands a node B if and only if (i) A does not dominate B; and (ii) the lowest branching node (i.e.  non-unary node) that dominates A also dominates B.'</S><S sid =""390"" ssid = ""3"">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S>",['Method_Citation']
6,J01-2004,P04-1015,0,2001a,0,"One way around this problem is to adopt a two-pass approach, where GEN (x) is the top N analyses under some initial model, as in the re ranking approach of Collins (2000) .In the current paper we explore alternatives to rerank ing approaches, namely heuristic methods for finding the arg max, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999)","In the current paper we explore alternatives to reranking approaches, namely heuristic methods for finding the argmax, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999)","['321', '95', '199', '5', '11']","<S sid =""321"" ssid = ""77"">In our trials  we used the unigram  with a very small mixing coefficient: following words since the denominator is zero.</S><S sid =""95"" ssid = ""53"">For an interpolated (n + 1)-gram: Here P is the empirically observed relative frequency  and An is a function from Vn to [0  1].</S><S sid =""199"" ssid = ""103"">Table 1 gives a breakdown of the different levels of conditioning information used in the empirical trials  with a mnemonic label that will be used when presenting results.</S><S sid =""5"" ssid = ""5"">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S><S sid =""11"" ssid = ""5"">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S>",['Method_Citation']
7,J01-2004,P04-1015,0,2001a,0,"approach The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights","The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights","['134', '87', '302', '104', '85']","<S sid =""134"" ssid = ""38"">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid =""87"" ssid = ""45"">Notice that the subject NP c-commands the object NP  but not vice versa  since the lowest branching node that dominates the object NP is the VP  which does not dominate the subject NP.</S><S sid =""302"" ssid = ""58"">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid =""104"" ssid = ""8"">The algorithms both utilize a left-corner matrix  which can be calculated in closed form through matrix inversion.</S><S sid =""85"" ssid = ""43"">The simple definition of c-command that we will be using in this paper is the following: a node A c-commands a node B if and only if (i) A does not dominate B; and (ii) the lowest branching node (i.e.  non-unary node) that dominates A also dominates B.'</S>",['Method_Citation']
9,J01-2004,P04-1015,0,2001a,0,"Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word","Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word","['224', '134', '52', '322', '306']","<S sid =""224"" ssid = ""128"">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid =""134"" ssid = ""38"">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid =""52"" ssid = ""10"">Consider  for example  the parse tree shown in (a) in Figure 1: the start symbol is St  which expands into an S. The S node expands into an NP followed by a VP.</S><S sid =""322"" ssid = ""78"">Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.</S><S sid =""306"" ssid = ""62"">Recall the discussion of the grammar models above  and our definition of the set of partial derivations Dw  with respect to a prefix string wil) (see Equations 2 and 7).</S>",['Method_Citation']
10,J01-2004,P05-1022,0,"Roark, 2001",0,"A good example of this is the Roark parser (Roark, 2001) which works left-to right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning","A good example of this is the Roark parser (Roark, 2001) which works left-to-right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning","['112', '129', '302', '87', '109']","<S sid =""112"" ssid = ""16"">Goddeau (1992) used a robust deterministic shift-reduce parser to condition word probabilities by extracting a specified number of stack entries from the top of the current state  and conditioning on those entries in a way similar to an n-gram.</S><S sid =""129"" ssid = ""33"">The shift-reduce parser will have  perhaps  built the structure shown  and the stack state will have an NP entry with the head &quot;cat&quot; at the top of the stack  and a VBD entry with the head &quot;chased&quot; second on the stack.</S><S sid =""302"" ssid = ""58"">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid =""87"" ssid = ""45"">Notice that the subject NP c-commands the object NP  but not vice versa  since the lowest branching node that dominates the object NP is the VP  which does not dominate the subject NP.</S><S sid =""109"" ssid = ""13"">A shift-reduce parser operates from left to right using a stack and a pointer to the next word in the input string.9 Each stack entry consists minimally of a nonterminal label.</S>",['Method_Citation']
11,J01-2004,P05-1022,0,"Roark, 2001",0,"At the end one has a beam-width? s number of best parses (Roark, 2001) .The Collins parser (Collins, 1997) does use dynamic programming in its search","At the end one has a beam-width's number of best parses (Roark, 2001)","['358', '306', '224', '213', '322']","<S sid =""358"" ssid = ""114"">We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice  along with the acoustic and trigram scores.'</S><S sid =""306"" ssid = ""62"">Recall the discussion of the grammar models above  and our definition of the set of partial derivations Dw  with respect to a prefix string wil) (see Equations 2 and 7).</S><S sid =""224"" ssid = ""128"">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid =""213"" ssid = ""117"">The parser takes an input string 4  a PCFG G  and a priority queue of candidate analyses.</S><S sid =""322"" ssid = ""78"">Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.</S>",['Method_Citation']
12,J01-2004,P05-1022,0,"Roark, 2001",0,"To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses","To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses","['302', '59', '134', '52', '104']","<S sid =""302"" ssid = ""58"">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid =""59"" ssid = ""17"">A PCFG is a CFG with a probability assigned to each rule; specifically  each righthand side has a probability given the left-hand side of the rule.</S><S sid =""134"" ssid = ""38"">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid =""52"" ssid = ""10"">Consider  for example  the parse tree shown in (a) in Figure 1: the start symbol is St  which expands into an S. The S node expands into an NP followed by a VP.</S><S sid =""104"" ssid = ""8"">The algorithms both utilize a left-corner matrix  which can be calculated in closed form through matrix inversion.</S>",['Method_Citation']
13,J01-2004,P04-1006,0,"Roark, 2001",0,"We ran the first stage parser with 4-timesoverparsing for each string in 7The n? best lists were provided by Brian Roark (Roark, 2001) 8A local-tree is an explicit expansion of an edge and its children","The n-best lists were provided by Brian Roark (Roark, 2001)","['210', '142', '323', '0', '370']","<S sid =""210"" ssid = ""114"">It uses a PCFG with a conditional probability model of the sort defined in the previous section.</S><S sid =""142"" ssid = ""46"">A simple PCFG conditions rule probabilities on the left-hand side of the rule.</S><S sid =""323"" ssid = ""79"">They trained and tested the SLM on a modified  more &quot;speech-like&quot; version of the Penn Treebank.</S><S sid =""0"" ssid = ""0"">Probabilistic Top-Down Parsing and Language Modeling</S><S sid =""370"" ssid = ""126"">We followed Chelba (2000) in using an LM weight of 16 for the lattice trigram.</S>",['Method_Citation']
14,J01-2004,P05-1063,0,"Roark, 2001a",0,"Incremental top down and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models","Incremental top-down and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models","['134', '224', '104', '154', '0']","<S sid =""134"" ssid = ""38"">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid =""224"" ssid = ""128"">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid =""104"" ssid = ""8"">The algorithms both utilize a left-corner matrix  which can be calculated in closed form through matrix inversion.</S><S sid =""154"" ssid = ""58"">We use the relative frequency estimator  and smooth our production probabilities by interpolating the relative frequency estimates with those obtained by &quot;annotating&quot; less contextual information.</S><S sid =""0"" ssid = ""0"">Probabilistic Top-Down Parsing and Language Modeling</S>",['Method_Citation']
15,J01-2004,W10-2009,0,"Roark, 2001",0,"Levy, on the other hand ,argued that studies of probabilistic parsing reveal that typically a small number of analyses are as signed the majority of probability mass (Roark, 2001)","Levy, on the other hand, argued that studies of probabilistic parsing reveal that typically a small number of analyses are assigned the majority of probability mass (Roark, 2001)","['302', '95', '124', '147', '168']","<S sid =""302"" ssid = ""58"">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid =""95"" ssid = ""53"">For an interpolated (n + 1)-gram: Here P is the empirically observed relative frequency  and An is a function from Vn to [0  1].</S><S sid =""124"" ssid = ""28"">Each distinct derivation path within the beam has a probability and a stack state associated with it.</S><S sid =""147"" ssid = ""51"">One way to estimate the probabilities of these rules is to annotate the heads onto the constituent labels in the training corpus and simply count the number of times particular productions occur (relative frequency estimation).</S><S sid =""168"" ssid = ""72"">For example  in Figure 2  constituent (NP-DT-NN) is simply NP.</S>",['Results_Citation']
17,J01-2004,D09-1034,0,2001,0,"For example, in Demberg and Keller (2008), trials were run deriving surprisal from the Roark (2001) parser under two different conditions: fully lex icalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags)","For example, in Demberg and Keller (2008), trials were run deriving surprisal from the Roark (2001) parser under two different conditions: fully lexicalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags)","['358', '306', '322', '224', '134']","<S sid =""358"" ssid = ""114"">We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice  along with the acoustic and trigram scores.'</S><S sid =""306"" ssid = ""62"">Recall the discussion of the grammar models above  and our definition of the set of partial derivations Dw  with respect to a prefix string wil) (see Equations 2 and 7).</S><S sid =""322"" ssid = ""78"">Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.</S><S sid =""224"" ssid = ""128"">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid =""134"" ssid = ""38"">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S>",['Method_Citation']
18,J01-2004,D09-1034,0,2001,0,"We modified the Roark (2001) parser to calculate the discussed measures 1, and the empirical results in? 4 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time","We modified the Roark (2001) parser to calculate the discussed measures, and the empirical results in ?4 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time","['289', '25', '390', '141', '22']","<S sid =""289"" ssid = ""45"">These results  achieved using very straightforward conditioning events and considering only the left context  are within one to four points of the best published Observed running time on Section 23 of the Penn Treebank  with the full conditional probability model and beam of 10-11  using one 300 MHz UltraSPARC processor and 256MB of RAM of a Sun Enterprise 450. accuracies cited above.'</S><S sid =""25"" ssid = ""13"">A parser that is not left to right  but which has rooted derivations  e.g.  a headfirst parser  will be able to calculate generative joint probabilities for entire strings; however  it will not be able to calculate probabilities for each word conditioned on previously generated words  unless each derivation generates the words in the string in exactly the same order.</S><S sid =""390"" ssid = ""3"">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S><S sid =""141"" ssid = ""45"">We will then present empirical results in two domains: one to compare with previous work in the parsing literature  and the other to compare with previous work using parsing for language modeling for speech recognition  in particular with the Chelba and Jelinek results mentioned above.</S><S sid =""22"" ssid = ""10"">A left-toright parser whose derivations are not rooted  i.e.  with derivations that can consist of disconnected tree fragments  such as an LR or shift-reduce parser  cannot incrementally calculate the probability of each prefix string being generated by the probabilistic grammar  because their derivations include probability mass from unrooted structures.</S>",['Method_Citation']
19,J01-2004,D09-1034,0,2001,0,"In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here","In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here","['224', '306', '358', '134', '322']","<S sid =""224"" ssid = ""128"">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid =""306"" ssid = ""62"">Recall the discussion of the grammar models above  and our definition of the set of partial derivations Dw  with respect to a prefix string wil) (see Equations 2 and 7).</S><S sid =""358"" ssid = ""114"">We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice  along with the acoustic and trigram scores.'</S><S sid =""134"" ssid = ""38"">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid =""322"" ssid = ""78"">Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.</S>",['Method_Citation']
20,J01-2004,D09-1034,0,2001,0,"At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is re quired to calculate such measures. Let H (D) be the entropy over a set of derivations D, calculated as follows: H (D)=? X D? D? (D) P D?? D? (D?) log? (D) P D?? D? (D?) (10) If the set of derivations D= D (G, W [1, i]) is a set of partial derivations for string W [1, i], then H (D) is a measure of uncertainty over the partial derivations ,i.e., the uncertainty regarding the correct analysis of what has already been processed","At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is required to calculate such measures","['147', '302', '278', '141', '390']","<S sid =""147"" ssid = ""51"">One way to estimate the probabilities of these rules is to annotate the heads onto the constituent labels in the training corpus and simply count the number of times particular productions occur (relative frequency estimation).</S><S sid =""302"" ssid = ""58"">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid =""278"" ssid = ""34"">Section 22 (41 817 words  1 700 sentences) served as the development corpus  on which the parser was tested until stable versions were ready to run on the test data  to avoid developing the parser to fit the specific test data.</S><S sid =""141"" ssid = ""45"">We will then present empirical results in two domains: one to compare with previous work in the parsing literature  and the other to compare with previous work using parsing for language modeling for speech recognition  in particular with the Chelba and Jelinek results mentioned above.</S><S sid =""390"" ssid = ""3"">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S>",['Method_Citation']
