Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,W11-2123,W11-2138,0,"Heafield, 2011",0,"We used common tools for phrase-based translation? Moses (Koehn et al, 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modelling and GIZA++ (Och and Ney, 2000) for word alignments","We used common tools for phrase-based translation Moses (Koehn et al, 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modelling and GIZA++ (Och and Ney, 2000) for word alignments","['0', '183', '69', '27', '231']","<S sid =""0"" ssid = ""0"">KenLM: Faster and Smaller Language Model Queries</S><S sid =""183"" ssid = ""2"">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid =""69"" ssid = ""47"">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid =""27"" ssid = ""5"">Hash tables are a common sparse mapping technique used by SRILM’s default and BerkeleyLM’s hashed variant.</S><S sid =""231"" ssid = ""50"">The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI’s speed reported in Guthrie and Hepple (2010) under different conditions.</S>",['Results_Citation']
2,W11-2123,P14-2022,0,"Heafield, 2011",0,"The language model was com piled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run","The language model was compiled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run","['0', '183', '69', '149', '193']","<S sid =""0"" ssid = ""0"">KenLM: Faster and Smaller Language Model Queries</S><S sid =""183"" ssid = ""2"">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid =""69"" ssid = ""47"">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid =""149"" ssid = ""21"">RandLM and SRILM also remove context that will not extend  but SRILM performs a second lookup in its trie whereas our approach has minimal additional cost.</S><S sid =""193"" ssid = ""12"">It also uses less memory  with 8 bytes of overhead per entry (we store 16-byte entries with m = 1.5); linked list implementations hash set and unordered require at least 8 bytes per entry for pointers.</S>",['Method_Citation']
3,W11-2123,W12-3145,0,"Heafield, 2011",0,"Thus given afragment tf consisting of a sequence of target to kens, we compute LM scores for (i) < s& gt ;tf, (ii )tf and (iii )tf < /s& gt; and use the best score (only) for pruning.2 While this increases the number ofLM queries, we exploit the language model state in formation in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states","Thus given a fragment tf consisting of a sequence of target tokens, we compute LM scores for (i) < s& gt ;tf, (ii )tf and (iii )tf < /s& gt; and use the best score (only) for pruning. While this increases the number of LM queries, we exploit the language model state in formation in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states","['62', '142', '3', '216', '50']","<S sid =""62"" ssid = ""40"">If the key distribution’s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |− 1] to estimate pivots; this optimization alone led to a 24% speed improvement.</S><S sid =""142"" ssid = ""14"">Language models that contain wi must also contain prefixes wi for 1 G i G k. Therefore  when the model is queried for p(wnjwn−1 1 ) but the longest matching suffix is wnf   it may return state s(wn1) = wnf since no longer context will be found.</S><S sid =""3"" ssid = ""3"">Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S sid =""216"" ssid = ""35"">Though we are not able to calculate their memory usage on our model  results reported in their paper suggest lower memory consumption than TRIE on large-scale models  at the expense of CPU time.</S><S sid =""50"" ssid = ""28"">Given counts cn1 where e.g. c1 is the vocabulary size  total memory consumption  in bits  is Our PROBING data structure places all n-grams of the same order into a single giant hash table.</S>",['Method_Citation']
4,W11-2123,W12-3131,0,"Heafield, 2011",0,"Our translation system uses cdec (Dyer et al,2010), an implementation of the hierarchical phrase based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference","Our translation system uses cdec (Dyer et al,2010), an implementation of the hierarchical phrase based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference","['0', '137', '183', '157', '27']","<S sid =""0"" ssid = ""0"">KenLM: Faster and Smaller Language Model Queries</S><S sid =""137"" ssid = ""9"">The state function is integrated into the query process so that  in lieu of the query p(wnjwn−1 1 )  the application issues query p(wnjs(wn−1 1 )) which also returns s(wn1 ).</S><S sid =""183"" ssid = ""2"">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid =""157"" ssid = ""29"">Each visited entry wni stores backoff b(wni ).</S><S sid =""27"" ssid = ""5"">Hash tables are a common sparse mapping technique used by SRILM’s default and BerkeleyLM’s hashed variant.</S>",['Method_Citation']
5,W11-2123,W12-3154,0,"Heafield, 2011",0,"The three data sets in use in this paper are summarised in Table 1.The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al, 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime","The three data sets in use in this paper are summarised in Table 1.The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al, 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime","['183', '3', '233', '193', '69']","<S sid =""183"" ssid = ""2"">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid =""3"" ssid = ""3"">Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S sid =""233"" ssid = ""52"">The authors provided us with a ratio between TPT and SRI under different conditions. aLossy compression with the same weights. bLossy compression with retuned weights. ditions make the value appropriate for estimating repeated run times  such as in parameter tuning.</S><S sid =""193"" ssid = ""12"">It also uses less memory  with 8 bytes of overhead per entry (we store 16-byte entries with m = 1.5); linked list implementations hash set and unordered require at least 8 bytes per entry for pointers.</S><S sid =""69"" ssid = ""47"">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S>",['Method_Citation']
6,W11-2123,P12-2058,0,"Heafield, 2011",0,"The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011)","The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011)","['183', '0', '38', '88', '71']","<S sid =""183"" ssid = ""2"">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid =""0"" ssid = ""0"">KenLM: Faster and Smaller Language Model Queries</S><S sid =""38"" ssid = ""16"">The ratio of buckets to entries is controlled by space multiplier m > 1.</S><S sid =""88"" ssid = ""66"">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S><S sid =""71"" ssid = ""49"">Nodes in the trie are based on arrays sorted by vocabulary identifier.</S>",['Method_Citation']
7,W11-2123,W11-2139,0,2011,0,Inference was carried out using the language modeling library described by Heafield (2011),Inference was carried out using the language modeling library described by Heafield (2011),"['69', '0', '14', '71', '88']","<S sid =""69"" ssid = ""47"">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid =""0"" ssid = ""0"">KenLM: Faster and Smaller Language Model Queries</S><S sid =""14"" ssid = ""9"">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S><S sid =""71"" ssid = ""49"">Nodes in the trie are based on arrays sorted by vocabulary identifier.</S><S sid =""88"" ssid = ""66"">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S>",['Method_Citation']
8,W11-2123,P13-2003,0,"Heafield, 2011",0,"We used the MADA ATB segmentation for Arabic (Roth et al, 2008) and true casing for English, phrases of maximal length 7, KneserNey smoothing, and lexicalized reordering (Koehn et al, 2005), and a 5-gram language model, trained on GigaWordv.5 using KenLM (Heafield, 2011)","We used the MADA ATB segmentation for Arabic (Roth et al, 2008) and true casing for English, phrases of maximal length 7, KneserNey smoothing, and lexicalized reordering (Koehn et al, 2005), and a 5-gram language model, trained on GigaWordv.5 using KenLM (Heafield, 2011)","['124', '69', '227', '21', '13']","<S sid =""124"" ssid = ""28"">Lossy compressed models RandLM (Talbot and Osborne  2007) and Sheffield (Guthrie and Hepple  2010) offer better memory consumption at the expense of CPU and accuracy.</S><S sid =""69"" ssid = ""47"">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid =""227"" ssid = ""46"">The first value reports use immediately after loading while the second reports the increase during scoring. dBerkeleyLM is written in Java which requires memory be specified in advance.</S><S sid =""21"" ssid = ""16"">Performance improvements transfer to the Moses (Koehn et al.  2007)  cdec (Dyer et al.  2010)  and Joshua (Li et al.  2009) translation systems where our code has been integrated.</S><S sid =""13"" ssid = ""8"">IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.</S>",['Method_Citation']
9,W11-2123,W12-3134,0,2011,0,The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language mod els by Heafield (2011) and Pauls and Klein (2011)? both language model implementations are now integrated with Joshua,The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language models by Heafield (2011) and Pauls and Klein (2011)? both language model implementations are now integrated with Joshua,"['183', '0', '137', '116', '231']","<S sid =""183"" ssid = ""2"">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid =""0"" ssid = ""0"">KenLM: Faster and Smaller Language Model Queries</S><S sid =""137"" ssid = ""9"">The state function is integrated into the query process so that  in lieu of the query p(wnjwn−1 1 )  the application issues query p(wnjs(wn−1 1 )) which also returns s(wn1 ).</S><S sid =""116"" ssid = ""20"">Both implementations employ a state object  opaque to the application  that carries information from one query to the next; we discuss both further in Section 4.2.</S><S sid =""231"" ssid = ""50"">The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI’s speed reported in Guthrie and Hepple (2010) under different conditions.</S>",['Method_Citation']
10,W11-2123,W12-3134,0,2011,0,Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets,Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets,"['88', '183', '69', '118', '14']","<S sid =""88"" ssid = ""66"">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S><S sid =""183"" ssid = ""2"">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid =""69"" ssid = ""47"">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid =""118"" ssid = ""22"">The hash variant is a reverse trie with hash tables  a more memory-efficient version of SRILM’s default.</S><S sid =""14"" ssid = ""9"">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S>",['Method_Citation']
11,W11-2123,W12-3134,0,2011,0,"With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult to compile SRILM toolkit (Stolcke, 2002)","With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult to compile SRILM toolkit (Stolcke, 2002)","['69', '231', '183', '227', '124']","<S sid =""69"" ssid = ""47"">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid =""231"" ssid = ""50"">The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI’s speed reported in Guthrie and Hepple (2010) under different conditions.</S><S sid =""183"" ssid = ""2"">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid =""227"" ssid = ""46"">The first value reports use immediately after loading while the second reports the increase during scoring. dBerkeleyLM is written in Java which requires memory be specified in advance.</S><S sid =""124"" ssid = ""28"">Lossy compressed models RandLM (Talbot and Osborne  2007) and Sheffield (Guthrie and Hepple  2010) offer better memory consumption at the expense of CPU and accuracy.</S>",['Method_Citation']
12,W11-2123,W12-3160,0,"Heafield, 2011",0,"This was used to create a KenLM (Heafield, 2011)","This was used to create a KenLM (Heafield, 2011)","['157', '0', '53', '27', '118']","<S sid =""157"" ssid = ""29"">Each visited entry wni stores backoff b(wni ).</S><S sid =""0"" ssid = ""0"">KenLM: Faster and Smaller Language Model Queries</S><S sid =""53"" ssid = ""31"">Sorted arrays store key-value pairs in an array sorted by key  incurring no space overhead.</S><S sid =""27"" ssid = ""5"">Hash tables are a common sparse mapping technique used by SRILM’s default and BerkeleyLM’s hashed variant.</S><S sid =""118"" ssid = ""22"">The hash variant is a reverse trie with hash tables  a more memory-efficient version of SRILM’s default.</S>",['Method_Citation']
13,W11-2123,W12-3706,0,"Heafield, 2011",0,"In the Opinum system we query the M p, M n mod els with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable fora web application","In the Opinum system we query the M p, M n models with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable for a web application","['183', '195', '116', '137', '62']","<S sid =""183"" ssid = ""2"">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid =""195"" ssid = ""14"">Interpolation search has a more expensive pivot but performs less pivoting and reads  so it is slow on small data and faster on large data.</S><S sid =""116"" ssid = ""20"">Both implementations employ a state object  opaque to the application  that carries information from one query to the next; we discuss both further in Section 4.2.</S><S sid =""137"" ssid = ""9"">The state function is integrated into the query process so that  in lieu of the query p(wnjwn−1 1 )  the application issues query p(wnjs(wn−1 1 )) which also returns s(wn1 ).</S><S sid =""62"" ssid = ""40"">If the key distribution’s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |− 1] to estimate pivots; this optimization alone led to a 24% speed improvement.</S>",['Results_Citation']
14,W11-2123,W11-2147,0,"Heafield, 2011",0,"Our base line is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al, 2007) for translation model training and decoding, GIZA++ (Ochand Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights","Our base line is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al, 2007) for translation model training and decoding, GIZA++ (Ochand Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights","['183', '69', '231', '233', '193']","<S sid =""183"" ssid = ""2"">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid =""69"" ssid = ""47"">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid =""231"" ssid = ""50"">The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI’s speed reported in Guthrie and Hepple (2010) under different conditions.</S><S sid =""233"" ssid = ""52"">The authors provided us with a ratio between TPT and SRI under different conditions. aLossy compression with the same weights. bLossy compression with retuned weights. ditions make the value appropriate for estimating repeated run times  such as in parameter tuning.</S><S sid =""193"" ssid = ""12"">It also uses less memory  with 8 bytes of overhead per entry (we store 16-byte entries with m = 1.5); linked list implementations hash set and unordered require at least 8 bytes per entry for pointers.</S>",['Method_Citation']
15,W11-2123,E12-1083,0,"Heafield, 2011",0,"For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011)","For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011)","['0', '69', '88', '183', '201']","<S sid =""0"" ssid = ""0"">KenLM: Faster and Smaller Language Model Queries</S><S sid =""69"" ssid = ""47"">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid =""88"" ssid = ""66"">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S><S sid =""183"" ssid = ""2"">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid =""201"" ssid = ""20"">Unlike Germann et al. (2009)  we chose a model size so that all benchmarks fit comfortably in main memory.</S>",['Method_Citation']
16,W11-2123,P12-1002,0,"Heafield, 2011",0,"Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Sima ?an,2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well.3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using kenlm (Heafield, 2011)","Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Simaan,2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well.3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using kenlm (Heafield, 2011)","['3', '62', '102', '179', '142']","<S sid =""3"" ssid = ""3"">Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S sid =""62"" ssid = ""40"">If the key distribution’s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |− 1] to estimate pivots; this optimization alone led to a 24% speed improvement.</S><S sid =""102"" ssid = ""6"">The PROBING model was designed to improve upon SRILM by using linear probing hash tables (though not arranged in a trie)  allocating memory all at once (eliminating the need for full pointers)  and being easy to compile.</S><S sid =""179"" ssid = ""51"">We do not experiment with models larger than physical memory in this paper because TPT is unreleased  factors such as disk speed are hard to replicate  and in such situations we recommend switching to a more compact representation  such as RandLM.</S><S sid =""142"" ssid = ""14"">Language models that contain wi must also contain prefixes wi for 1 G i G k. Therefore  when the model is queried for p(wnjwn−1 1 ) but the longest matching suffix is wnf   it may return state s(wn1) = wnf since no longer context will be found.</S>",['Method_Citation']
17,W11-2123,D12-1108,0,"Heafield, 2011",0,"n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3","n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3","['69', '2', '129', '71', '201']","<S sid =""69"" ssid = ""47"">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid =""2"" ssid = ""2"">The structure uses linear probing hash tables and is designed for speed.</S><S sid =""129"" ssid = ""1"">In addition to the optimizations specific to each datastructure described in Section 2  we implement several general optimizations for language modeling.</S><S sid =""71"" ssid = ""49"">Nodes in the trie are based on arrays sorted by vocabulary identifier.</S><S sid =""201"" ssid = ""20"">Unlike Germann et al. (2009)  we chose a model size so that all benchmarks fit comfortably in main memory.</S>",['Method_Citation']
18,W11-2123,P12-2006,0,"Heafield, 2011",0,"Research efforts to increase search efficiency for phrase-based MT (Koehn et al, 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al, 2006) to additional early pruning techniques (Delaney et al, 2006), (Moore and Quirk, 2007) and more efficient language model (LM) querying (Heafield, 2011)","Research efforts to increase search efficiency for phrase-based MT (Koehn et al, 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al, 2006) to additional early pruning techniques (Delaney et al, 2006), (Moore and Quirk, 2007) and more efficient language model (LM) querying (Heafield, 2011)","['199', '205', '182', '67', '45']","<S sid =""199"" ssid = ""18"">For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid =""205"" ssid = ""24"">We evaluate the time and memory consumption of each data structure by computing perplexity on 4 billion tokens from the English Gigaword corpus (Parker et al.  2009).</S><S sid =""182"" ssid = ""1"">This section measures performance on shared tasks in order of increasing complexity: sparse lookups  evaluating perplexity of a large file  and translation with Moses.</S><S sid =""67"" ssid = ""45"">While sorted arrays could be used to implement the same data structure as PROBING  effectively making m = 1  we abandoned this implementation because it is slower and larger than a trie implementation.</S><S sid =""45"" ssid = ""23"">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S>",['Method_Citation']
19,W11-2123,P13-2073,0,"Heafield, 2011",0,"For English language modeling, we use English Giga word Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011)","For English language modeling, we use English Giga word Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011)","['0', '183', '69', '149', '201']","<S sid =""0"" ssid = ""0"">KenLM: Faster and Smaller Language Model Queries</S><S sid =""183"" ssid = ""2"">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid =""69"" ssid = ""47"">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid =""149"" ssid = ""21"">RandLM and SRILM also remove context that will not extend  but SRILM performs a second lookup in its trie whereas our approach has minimal additional cost.</S><S sid =""201"" ssid = ""20"">Unlike Germann et al. (2009)  we chose a model size so that all benchmarks fit comfortably in main memory.</S>","['Aim_Citation', 'Results_Citation']"
20,W11-2123,P13-1109,0,"Heafield, 2011",0,"For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with KneserNey smoothing","For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with KneserNey smoothing","['183', '0', '69', '88', '118']","<S sid =""183"" ssid = ""2"">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid =""0"" ssid = ""0"">KenLM: Faster and Smaller Language Model Queries</S><S sid =""69"" ssid = ""47"">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid =""88"" ssid = ""66"">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S><S sid =""118"" ssid = ""22"">The hash variant is a reverse trie with hash tables  a more memory-efficient version of SRILM’s default.</S>",['Method_Citation']
