Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P11-1061,P11-1144,0,2011,0,Subramanya et al? s model was extended by Das and Petrov (2011) to induce part-of-speech dictionaries for unsupervised learning of taggers,Subramanya et al's model was extended by Das and Petrov (2011) to induce part-of-speech dictionaries for unsupervised learning of taggers,"['19', '62', '68', '42', '160']","<S sid =""19"" ssid = ""15"">Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S><S sid =""62"" ssid = ""28"">Note that since all English vertices were extracted from the parallel text  we will have an initial label distribution for all vertices in Ve.</S><S sid =""68"" ssid = ""34"">In the label propagation stage  we propagate the automatic English tags to the aligned Italian trigram types  followed by further propagation solely among the Italian vertices. the Italian vertices are connected to an automatically labeled English vertex.</S><S sid =""42"" ssid = ""8"">The foreign language vertices (denoted by Vf) correspond to foreign trigram types  exactly as in Subramanya et al. (2010).</S><S sid =""160"" ssid = ""3"">Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.</S>",['Results_Citation']
3,P11-1061,P14-1126,0,2011,0,"Fortunately, some recently proposed POS taggers, such as the POStagger of Das and Petrov (2011), rely only on labeled training data for English and the same kind of parallel text in our approach","Fortunately, some recently proposed POS taggers, such as the POS tagger of Das and Petrov (2011), rely only on labeled training data for English and the same kind of parallel text in our approach","['26', '113', '30', '102', '25']","<S sid =""26"" ssid = ""3"">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid =""113"" ssid = ""13"">For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S><S sid =""30"" ssid = ""7"">To initialize the graph we tag the English side of the parallel text using a supervised model.</S><S sid =""102"" ssid = ""2"">We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.</S><S sid =""25"" ssid = ""2"">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>",['Method_Citation']
4,P11-1061,N12-1086,0,"Das and Petrov, 2011",0,"Applications have ranged from domain adaptation of part-of-speech (POS) taggers (Subramanya et al, 2010), unsupervised learning ofPOS taggers by using bilingual graph-based projections (Das and Petrov, 2011), and shallow semantic parsing for unknown predicates (Das and Smith,2011)","Applications have ranged from domain adaptation of part-of-speech (POS) taggers (Subramanya et al, 2010), unsupervised learning of POS taggers by using bilingual graph-based projections (Das and Petrov, 2011), and shallow semantic parsing for unknown predicates (Das and Smith,2011)","['16', '113', '25', '3', '19']","<S sid =""16"" ssid = ""12"">To this end  we construct a bilingual graph over word types to establish a connection between the two languages (§3)  and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S sid =""113"" ssid = ""13"">For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S><S sid =""25"" ssid = ""2"">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S sid =""3"" ssid = ""3"">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).</S><S sid =""19"" ssid = ""15"">Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S>",['Method_Citation']
5,P11-1061,N12-1086,0,2011,0,"Following Das and Petrov (2011) and Subramanya et al (2010), a similarity score between two trigram types was computed by measuring the cosine similarity between their empirical sentential context statistics","Following Das and Petrov (2011) and Subramanya et al (2010), a similarity score between two trigram types was computed by measuring the cosine similarity between their empirical sentential context statistics","['134', '119', '54', '82', '28']","<S sid =""134"" ssid = ""34"">Because we don’t have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.</S><S sid =""119"" ssid = ""19"">To provide a thorough analysis  we evaluated three baselines and two oracles in addition to two variants of our graph-based approach.</S><S sid =""54"" ssid = ""20"">Given this similarity function  we define a nearest neighbor graph  where the edge weight for the n most similar vertices is set to the value of the similarity function and to 0 for all other vertices.</S><S sid =""82"" ssid = ""13"">We formulate the update as follows: where ∀ui ∈ Vf \ Vfl  γi(y) and κi are defined as: We ran this procedure for 10 iterations.</S><S sid =""28"" ssid = ""5"">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S>",['Method_Citation']
6,P11-1061,N12-1086,0,"Das and Petrov, 2011",0,"Sparsity is desirable in settings where labeled development data for tuning thresholds that select the most probable labels for a given type is unavailable (e.g., Das and Petrov, 2011)","Sparsity is desirable in settings where labeled development data for tuning thresholds that select the most probable labels for a given type is unavailable (e.g., Das and Petrov, 2011)","['71', '61', '3', '86', '124']","<S sid =""71"" ssid = ""2"">We use label propagation in two stages to generate soft labels on all the vertices in the graph.</S><S sid =""61"" ssid = ""27"">These tag distributions are used to initialize the label distributions over the English vertices in the graph.</S><S sid =""3"" ssid = ""3"">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).</S><S sid =""86"" ssid = ""17"">For a sentence x and a state sequence z  a first order Markov model defines a distribution: (9) where Val(X) corresponds to the entire vocabulary.</S><S sid =""124"" ssid = ""24"">We tried two versions of our graph-based approach: feature after the first stage of label propagation (Eq.</S>",['Method_Citation']
7,P11-1061,N12-1052,0,2011,0,"Specifically, by replacing fine-grained language specific part-of-speech tags with universal part-of-speech tags, generated with the method described by Das and Petrov (2011), a universal parser is achieved that can be applied to any language for which universal part-of-speech tags are available. Below, we extend this approach to universal parsing by adding cross-lingual word cluster features","Specifically, by replacing fine-grained language specific part-of-speech tags with universal part-of-speech tags, generated with the method described by Das and Petrov (2011), a universal parser is achieved that can be applied to any language for which universal part-of-speech tags are available. Below, we extend this approach to universal parsing by adding cross-lingual word cluster features","['89', '26', '57', '97', '28']","<S sid =""89"" ssid = ""20"">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S><S sid =""26"" ssid = ""3"">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid =""57"" ssid = ""23"">Since our graph is built from a parallel corpus  we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g.  when the trigram type and the feature instantiation don’t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall  and we therefore extract only intersected high-confidence (> 0.9) alignments De�f.</S><S sid =""97"" ssid = ""28"">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid =""28"" ssid = ""5"">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S>",['Method_Citation']
8,P11-1061,N12-1052,0,2011,0,"We study the impact of using cross-lingual cluster features by comparing the strong delexicalized baseline model of McDonald et al (2011), which only has features derived from universal part-of-speech tags, projected from English with the method of Das and Petrov (2011), to the same model when adding features derived from cross-lingual clusters","We study the impact of using cross-lingual cluster features by comparing the strong delexicalized baseline model of McDonald et al (2011), which only has features derived from universal part-of-speech tags, projected from English with the method of Das and Petrov (2011), to the same model when adding features derived from cross-lingual clusters","['93', '26', '28', '97', '89']","<S sid =""93"" ssid = ""24"">For English POS tagging  BergKirkpatrick et al. (2010) found that this direct gradient method performed better (>7% absolute accuracy) than using a feature-enhanced modification of the Expectation-Maximization (EM) algorithm (Dempster et al.  1977).8 Moreover  this route of optimization outperformed a vanilla HMM trained with EM by 12%.</S><S sid =""26"" ssid = ""3"">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid =""28"" ssid = ""5"">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S><S sid =""97"" ssid = ""28"">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid =""89"" ssid = ""20"">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S>",['Method_Citation']
9,P11-1061,N12-1090,0,"Das and Petrov, (2011)",0,"MT-based projection has been applied to various NLP tasks, such as part of-speech tagging (e.g., Das and Petrov (2011)), mention detection (e.g., Zitouni and Florian (2008)), and sentiment analysis (e.g., Mihalcea et al (2007)) .There have been two initial attempts to apply projection to create co reference-annotated data for aresource-poor language, both of which involve projecting hand-annotated co reference data from English to Romanian via a parallel corpus","MT-based projection has been applied to various NLP tasks, such as part of-speech tagging (e.g., Das and Petrov (2011)), mention detection (e.g., Zitouni and Florian (2008)), and sentiment analysis (e.g., Mihalcea et al (2007))","['92', '102', '19', '49', '142']","<S sid =""92"" ssid = ""23"">To optimize this function  we used L-BFGS  a quasi-Newton method (Liu and Nocedal  1989).</S><S sid =""102"" ssid = ""2"">We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.</S><S sid =""19"" ssid = ""15"">Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S><S sid =""49"" ssid = ""15"">We define a symmetric similarity function K(uZ7 uj) over two foreign language vertices uZ7 uj E Vf based on the co-occurrence statistics of the nine feature concepts given in Table 1.</S><S sid =""142"" ssid = ""5"">The “No LP” model does not outperform direct projection for German and Greek  but performs better for six out of eight languages.</S>",['Method_Citation']
10,P11-1061,W11-2205,0,2011,0,"For example, the multilingual PoS induction approach of Das and Petrov (2011) assumes no supervision for the language whose PoS tags are being 35 induced, but it assumes access to a labeled dataset of a different language. We begin by surveying recent work on unsupervised PoS tagging, focusing on the issue of evaluation (Section 2)","For example, the multilingual PoS induction approach of Das and Petrov (2011) assumes no supervision for the language whose PoS tags are being 35 induced, but it assumes access to a labeled dataset of a different language. We begin by surveying recent work on unsupervised PoS tagging, focusing on the issue of evaluation (Section 2)","['26', '159', '97', '89', '28']","<S sid =""26"" ssid = ""3"">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid =""159"" ssid = ""2"">Because we are interested in applying our techniques to languages for which no labeled resources are available  we paid particular attention to minimize the number of free parameters and used the same hyperparameters for all language pairs.</S><S sid =""97"" ssid = ""28"">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid =""89"" ssid = ""20"">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S><S sid =""28"" ssid = ""5"">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S>",['Method_Citation']
11,P11-1061,P13-1155,0,"Das and Petrov, 2011",0,"(Das and Petrov, 2011) used graph-based label propagation for cross-lingual knowledge transfers to induce POS tags between two languages","(Das and Petrov, 2011) used graph-based label propagation for cross-lingual knowledge transfers to induce POS tags between two languages","['134', '97', '102', '28', '83']","<S sid =""134"" ssid = ""34"">Because we don’t have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.</S><S sid =""97"" ssid = ""28"">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid =""102"" ssid = ""2"">We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.</S><S sid =""28"" ssid = ""5"">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S><S sid =""83"" ssid = ""14"">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value τ: We describe how we choose τ in §6.4.</S>",['Method_Citation']
12,P11-1061,D12-1127,0,2011,0,Recent work by Das and Petrov (2011 )buildsa dictionary for a particular language by transfer ring annotated data from a resource-rich language through the use of word alignments in parallel text,Recent work by Das and Petrov (2011 ) builds a dictionary for a particular language by transferring annotated data from a resource-rich language through the use of word alignments in parallel text,"['26', '97', '28', '102', '72']","<S sid =""26"" ssid = ""3"">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid =""97"" ssid = ""28"">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid =""28"" ssid = ""5"">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S><S sid =""102"" ssid = ""2"">We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.</S><S sid =""72"" ssid = ""3"">In the first stage  we run a single step of label propagation  which transfers the label distributions from the English vertices to the connected foreign language vertices (say  Vf�) at the periphery of the graph.</S>",['Method_Citation']
13,P11-1061,D12-1127,0,"Das and Petrov, 2011",0,"Theseapproaches build a dictionary by transferring labeled data from a resource rich language (English) to a re source poor language (Das and Petrov, 2011)","These approaches build a dictionary by transferring labeled data from a resource rich language (English) to a resource poor language (Das and Petrov, 2011)","['95', '15', '25', '41', '113']","<S sid =""95"" ssid = ""26"">This feature ft incorporates information from the smoothed graph and prunes hidden states that are inconsistent with the thresholded vector tx.</S><S sid =""15"" ssid = ""11"">First  we use a novel graph-based framework for projecting syntactic information across language boundaries.</S><S sid =""25"" ssid = ""2"">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S sid =""41"" ssid = ""7"">Because the information flow in our graph is asymmetric (from English to the foreign language)  we use different types of vertices for each language.</S><S sid =""113"" ssid = ""13"">For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S>",['Method_Citation']
14,P11-1061,P12-3012,0,"Das and Petrov, 2011",0,"In recent years research in Natural Language Processing (NLP) has been steadily moving towards multilingual processing: the availability of ever growing amounts of text in different languages ,infact, has been a major driving force behind research on multilingual approaches, from morphosyntactic (Das and Petrov, 2011) and syntactico semantic (Peirsman and Pado?, 2010) phenomena to high-end tasks like textual entailment (Mehdad et al., 2011) and sentiment analysis (Lu et al, 2011)","In recent years research in Natural Language Processing (NLP) has been steadily moving towards multilingual processing: the availability of ever growing amounts of text in different languages, in fact, has been a major driving force behind research on multilingual approaches, from morphosyntactic (Das and Petrov, 2011) and syntactico semantic (Peirsman and Pado?, 2010) phenomena to high-end tasks like textual entailment (Mehdad et al., 2011) and sentiment analysis (Lu et al, 2011)","['21', '16', '10', '29', '36']","<S sid =""21"" ssid = ""17"">These universal POS categories not only facilitate the transfer of POS information from one language to another  but also relieve us from using controversial evaluation metrics 2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.</S><S sid =""16"" ssid = ""12"">To this end  we construct a bilingual graph over word types to establish a connection between the two languages (§3)  and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S sid =""10"" ssid = ""6"">To bridge this gap  we consider a practically motivated scenario  in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest  but that we have access to parallel data with a resource-rich language.</S><S sid =""29"" ssid = ""6"">To establish a soft correspondence between the two languages  we use a second similarity function  which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data  our goal is to project syntactic information from the English side to the foreign side.</S><S sid =""36"" ssid = ""2"">Graph construction for structured prediction problems such as POS tagging is non-trivial: on the one hand  using individual words as the vertices throws away the context necessary for disambiguation; on the other hand  it is unclear how to define (sequence) similarity if the vertices correspond to entire sentences.</S>",['Results_Citation']
15,P11-1061,D11-1006,0,2011,0,"Furthermore, we evaluate with both gold-standard part-of-speech tags, as well as predicted part-of speech tags from the projected part-of-speech tagger of Das and Petrov (2011) .2 This tagger relies only onlabeled training data for English, and achieves accuracies around 85% on the languages that we con sider","Furthermore, we evaluate with both gold-standard part-of-speech tags, as well as predicted part-of speech tags from the projected part-of-speech tagger of Das and Petrov (2011). This tagger relies only on labeled training data for English, and achieves accuracies around 85% on the languages that we consider","['26', '54', '89', '35', '159']","<S sid =""26"" ssid = ""3"">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid =""54"" ssid = ""20"">Given this similarity function  we define a nearest neighbor graph  where the edge weight for the n most similar vertices is set to the value of the similarity function and to 0 for all other vertices.</S><S sid =""89"" ssid = ""20"">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S><S sid =""35"" ssid = ""1"">In graph-based learning approaches one constructs a graph whose vertices are labeled and unlabeled examples  and whose weighted edges encode the degree to which the examples they link have the same label (Zhu et al.  2003).</S><S sid =""159"" ssid = ""2"">Because we are interested in applying our techniques to languages for which no labeled resources are available  we paid particular attention to minimize the number of free parameters and used the same hyperparameters for all language pairs.</S>",['Method_Citation']
16,P11-1061,D11-1006,0,2011,0,"In the first, we assumed that the test set for each target language had gold part-of-speech tags, and in the second we used predicted part-of-speech tags from the projection tagger of Das and Petrov (2011), which also uses English as the source language","In the first, we assumed that the test set for each target language had gold part-of-speech tags, and in the second we used predicted part-of-speech tags from the projection tagger of Das and Petrov (2011), which also uses English as the source language","['26', '28', '134', '97', '89']","<S sid =""26"" ssid = ""3"">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid =""28"" ssid = ""5"">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S><S sid =""134"" ssid = ""34"">Because we don’t have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.</S><S sid =""97"" ssid = ""28"">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid =""89"" ssid = ""20"">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S>",['Method_Citation']
17,P11-1061,P13-2112,0,2011,0,"This parallel data can be exploited to bridge languages, and in particular, transfer information from a highly-resourced language to a lesser-resourced language, to build unsupervised POS taggers. In this paper, we propose an unsupervised approach to POS tagging in a similar vein to the work of Das and Petrov (2011)","This parallel data can be exploited to bridge languages, and in particular, transfer information from a highly-resourced language to a lesser-resourced language, to build unsupervised POS taggers. In this paper, we propose an unsupervised approach to POS tagging in a similar vein to the work of Das and Petrov (2011)","['16', '21', '160', '113', '29']","<S sid =""16"" ssid = ""12"">To this end  we construct a bilingual graph over word types to establish a connection between the two languages (§3)  and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S sid =""21"" ssid = ""17"">These universal POS categories not only facilitate the transfer of POS information from one language to another  but also relieve us from using controversial evaluation metrics 2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.</S><S sid =""160"" ssid = ""3"">Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.</S><S sid =""113"" ssid = ""13"">For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S><S sid =""29"" ssid = ""6"">To establish a soft correspondence between the two languages  we use a second similarity function  which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data  our goal is to project syntactic information from the English side to the foreign side.</S>",['Method_Citation']
18,P11-1061,P13-2112,0,2011,0,Das and Petrov (2011) achieved the current state-of-the-art for unsupervised tagging by exploiting high confidence alignments to copy tags from the source language to the target language,Das and Petrov (2011) achieved the current state-of-the-art for unsupervised tagging by exploiting high confidence alignments to copy tags from the source language to the target language,"['134', '28', '97', '26', '89']","<S sid =""134"" ssid = ""34"">Because we don’t have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.</S><S sid =""28"" ssid = ""5"">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S><S sid =""97"" ssid = ""28"">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid =""26"" ssid = ""3"">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid =""89"" ssid = ""20"">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S>",['Method_Citation']
19,P11-1061,P13-2112,0,"Das and Petrov, 2011",0,"We have proposed a method for unsupervised POStagging that performs on par with the current state of-the-art (Das and Petrov, 2011), but is subs tan tially less-sophisticated (specifically not requiring convex optimization or a feature-based HMM)","We have proposed a method for unsupervised POS tagging that performs on par with the current state of-the-art (Das and Petrov, 2011), but is substantially less-sophisticated (specifically not requiring convex optimization or a feature-based HMM)","['97', '102', '49', '136', '83']","<S sid =""97"" ssid = ""28"">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid =""102"" ssid = ""2"">We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.</S><S sid =""49"" ssid = ""15"">We define a symmetric similarity function K(uZ7 uj) over two foreign language vertices uZ7 uj E Vf based on the co-occurrence statistics of the nine feature concepts given in Table 1.</S><S sid =""136"" ssid = ""36"">For graph propagation  the hyperparameter v was set to 2 x 10−6 and was not tuned.</S><S sid =""83"" ssid = ""14"">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value τ: We describe how we choose τ in §6.4.</S>",['Method_Citation']
