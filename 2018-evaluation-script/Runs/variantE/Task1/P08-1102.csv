Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P08-1102,C08-1049,0,2008,0,"Following Jiang et al (2008), we describe segmentation and Joint S& amp; T as below: For a given Chinese sentence appearing as a character sequence: C 1: n= C 1 C 2.","Following Jiang et al (2008), we describe segmentation and Joint S& amp; T as below: For a given Chinese sentence appearing as a character sequence: C 1: n= C 1 C 2.","['87', '22', '16', '94', '104']","<S sid =""87"" ssid = ""12"">Line 6 enumerates all POS’s for the word w spanning length l and ending at position i.</S><S sid =""22"" ssid = ""18"">2 Segmentation and POS Tagging Given a Chinese character sequence: while the segmentation and POS tagging result can be depicted as: Here  Ci (i = L.n) denotes Chinese character  ti (i = L.m) denotes POS tag  and Cl:r (l < r) denotes character sequence ranges from Cl to Cr.</S><S sid =""16"" ssid = ""12"">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S><S sid =""94"" ssid = ""5"">With precision P and recall R  the balance F-measure is defined as: F = 2PR/(P + R).</S><S sid =""104"" ssid = ""15"">According to the usual practice in syntactic analysis  we choose chapters 1 − 260 (18074 sentences) as training set  chapter 271 − 300 (348 sentences) as test set and chapter 301 − 325 (350 sentences) as development set.</S>",['Results_Citation']
2,P08-1102,C08-1049,0,2008,0,"As described in Ng and Low (2004 )andJiang et al (2008), we use s indicating a single character word, while b, m and e indicating the be gin, middle and end of a word respectively","As described in Ng and Low (2004) and Jiang et al (2008), we use s indicating a single character word, while b, m and e indicating the be gin, middle and end of a word respectively","['69', '47', '48', '32', '133']","<S sid =""69"" ssid = ""20"">Formally  an n-gram word LM approximates the probability of a word sequence W = w1:m with the following product: Notice that a bi-gram POS LM functions as the product of transition probabilities in HMM.</S><S sid =""47"" ssid = ""19"">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid =""48"" ssid = ""20"">We used the algorithm depicted in Algorithm 1 to tune the parameter vector a.</S><S sid =""32"" ssid = ""4"">We trained a character-based perceptron for Chinese Joint S&T  and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.</S><S sid =""133"" ssid = ""4"">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S>",['Method_Citation']
3,P08-1102,C08-1049,0,2008,0,plates called lexical-target in the column below areintroduced by Jiang et al (2008),plates called lexical-target in the column below are introduced by Jiang et al (2008),"['68', '99', '30', '101', '122']","<S sid =""68"" ssid = ""19"">It is an important measure of fluency of the translation in SMT.</S><S sid =""99"" ssid = ""10"">Then we trained LEX on each of the four corpora for 7 iterations.</S><S sid =""30"" ssid = ""2"">It has comparable performance to CRFs  while with much faster training.</S><S sid =""101"" ssid = ""12"">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S><S sid =""122"" ssid = ""33"">Another important feature is the labelling model.</S>",['Method_Citation']
4,P08-1102,P12-1110,0,2008,0,"For CTB-5, we refer to the split by Duan et al (2007) as CTB-5d, and to the split by Jiang et al (2008) as CTB-5j","For CTB-5, we refer to the split by Duan et al (2007) as CTB-5d, and to the split by Jiang et al (2008) as CTB-5j","['101', '87', '31', '94', '30']","<S sid =""101"" ssid = ""12"">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S><S sid =""87"" ssid = ""12"">Line 6 enumerates all POS’s for the word w spanning length l and ending at position i.</S><S sid =""31"" ssid = ""3"">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S><S sid =""94"" ssid = ""5"">With precision P and recall R  the balance F-measure is defined as: F = 2PR/(P + R).</S><S sid =""30"" ssid = ""2"">It has comparable performance to CRFs  while with much faster training.</S>",['Method_Citation']
5,P08-1102,D12-1126,0,2008,0,Jiang et al (2008) proposes a cascaded linear model for joint Chinese word segmentation and POS tagging,Jiang et al (2008) proposes a cascaded linear model for joint Chinese word segmentation and POS tagging,"['47', '0', '87', '69', '48']","<S sid =""47"" ssid = ""19"">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid =""0"" ssid = ""0"">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S sid =""87"" ssid = ""12"">Line 6 enumerates all POS’s for the word w spanning length l and ending at position i.</S><S sid =""69"" ssid = ""20"">Formally  an n-gram word LM approximates the probability of a word sequence W = w1:m with the following product: Notice that a bi-gram POS LM functions as the product of transition probabilities in HMM.</S><S sid =""48"" ssid = ""20"">We used the algorithm depicted in Algorithm 1 to tune the parameter vector a.</S>",['Method_Citation']
6,P08-1102,C10-1135,0,2008,0,"We use the feature templates the same as Jiang et al, (2008) to extract features form E model","We use the feature templates the same as Jiang et al, (2008) to extract features form E model","['47', '48', '49', '133', '32']","<S sid =""47"" ssid = ""19"">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid =""48"" ssid = ""20"">We used the algorithm depicted in Algorithm 1 to tune the parameter vector a.</S><S sid =""49"" ssid = ""21"">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S><S sid =""133"" ssid = ""4"">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S><S sid =""32"" ssid = ""4"">We trained a character-based perceptron for Chinese Joint S&T  and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.</S>",['Method_Citation']
8,P08-1102,P12-1025,0,"Jiangetal., 2008a",0,"approach, where basic processing units are characters which compose words (Jiangetal., 2008a)","basic processing units are characters which compose words (Jiangetal., 2008a)","['16', '68', '94', '87', '7']","<S sid =""16"" ssid = ""12"">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S><S sid =""68"" ssid = ""19"">It is an important measure of fluency of the translation in SMT.</S><S sid =""94"" ssid = ""5"">With precision P and recall R  the balance F-measure is defined as: F = 2PR/(P + R).</S><S sid =""87"" ssid = ""12"">Line 6 enumerates all POS’s for the word w spanning length l and ending at position i.</S><S sid =""7"" ssid = ""3"">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S>",['Method_Citation']
9,P08-1102,C10-2096,0,2008b,0,"The solid lines show the 1-best result, which is wrong. Jiang et al (2008b) stress the problems in re ranking phase","The solid lines show the 1-best result, which is wrong. Jiang et al (2008b) stress the problems in re ranking phase","['108', '138', '61', '90', '107']","<S sid =""108"" ssid = ""19"">We find that Joint S&T can also improve the segmentation accuracy.</S><S sid =""138"" ssid = ""1"">This work was done while L. H. was visiting CAS/ICT.</S><S sid =""61"" ssid = ""12"">In this layer  each knowledge source is treated as a feature with a corresponding weight denoting its relative importance.</S><S sid =""90"" ssid = ""1"">We reported results from two set of experiments.</S><S sid =""107"" ssid = ""18"">The evaluation results are shown in Table 3.</S>",['Method_Citation']
10,P08-1102,C10-2096,0,"Jiang et al, 2008a",0,"6.1.1 Baseline Forest-based System We first segment the Chinese sentences into the1-best segmentations using a state-of-the-art system (Jiang et al, 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results","We first segment the Chinese sentences into the 1-best segmentations using a state-of-the-art system (Jiang et al, 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results","['48', '133', '32', '69', '49']","<S sid =""48"" ssid = ""20"">We used the algorithm depicted in Algorithm 1 to tune the parameter vector a.</S><S sid =""133"" ssid = ""4"">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S><S sid =""32"" ssid = ""4"">We trained a character-based perceptron for Chinese Joint S&T  and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.</S><S sid =""69"" ssid = ""20"">Formally  an n-gram word LM approximates the probability of a word sequence W = w1:m with the following product: Notice that a bi-gram POS LM functions as the product of transition probabilities in HMM.</S><S sid =""49"" ssid = ""21"">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S>",['Method_Citation']
11,P08-1102,C10-2096,0,"Jiang et al, 2008a",0,"6.1.2 Lattice-forest SystemWe first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al, 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm","We first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al, 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm","['47', '7', '49', '69', '16']","<S sid =""47"" ssid = ""19"">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid =""7"" ssid = ""3"">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S><S sid =""49"" ssid = ""21"">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S><S sid =""69"" ssid = ""20"">Formally  an n-gram word LM approximates the probability of a word sequence W = w1:m with the following product: Notice that a bi-gram POS LM functions as the product of transition probabilities in HMM.</S><S sid =""16"" ssid = ""12"">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S>",['Method_Citation']
12,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"However, when we repeat the work of (Jiang et al, 2008), which reports to achieve the state-of-art performance in the data-sets that we adopt, it has been found that some features (e.g., C0) are unnoticeably trained several times in their model (which are implicitly generated from different feature templates used in the paper)","However, when we repeat the work of (Jiang et al, 2008), which reports to achieve the state-of-art performance in the data-sets that we adopt, it has been found that some features (e.g., C0) are unnoticeably trained several times in their model (which are implicitly generated from different feature templates used in the paper)","['2', '54', '64', '58', '135']","<S sid =""2"" ssid = ""2"">With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S><S sid =""54"" ssid = ""5"">We noticed that the templates related to word unigrams and bigrams bring to the feature space an enlargement much rapider than the character-base ones  not to mention the higher-order grams such as trigrams or 4-grams.</S><S sid =""64"" ssid = ""15"">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result  a 4-gram POS language model functioning as the product of statetransition probabilities in HMM  and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S><S sid =""58"" ssid = ""9"">Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.</S><S sid =""135"" ssid = ""6"">In addition  all knowledge sources we used in the core perceptron and the outside-layer linear model come from the training corpus  whereas many open knowledge sources (lexicon etc.) can be used to improve performance (Ng and Low  2004).</S>",['Method_Citation']
13,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"Unicode/CP936 1.1M/55K 104K/13K 0.035 Table 3: Corpus statistics for the second SIGHAN Bakeoff appears twice, which is generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al, 2008), with n=0, generates [C0C0])","Table 3: Corpus statistics for the second SIGHAN Bakeoff appears twice, which is generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al, 2008), with n=0, generates [C0C0])","['100', '47', '134', '32', '121']","<S sid =""100"" ssid = ""11"">Test results listed in Table 2 shows that this model obtains higher accuracy than the best of SIGHAN Bakeoff 2 in three corpora (AS  CityU and MSR).</S><S sid =""47"" ssid = ""19"">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid =""134"" ssid = ""5"">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S><S sid =""32"" ssid = ""4"">We trained a character-based perceptron for Chinese Joint S&T  and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.</S><S sid =""121"" ssid = ""32"">Among other features  the 4-gram POS LM plays the most important role  removing this feature causes F-measure decrement of 0.33 points on segmentation and 0.71 points on Joint S&T.</S>",['Method_Citation']
14,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"As all the features adopted in (Jiang et al, 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be? n?, at least in principle","As all the features adopted in (Jiang et al, 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be n, at least in principle","['133', '52', '2', '131', '135']","<S sid =""133"" ssid = ""4"">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S><S sid =""52"" ssid = ""3"">However  such features are generated dynamically during the decoding procedure so that the feature space enlarges much more rapidly.</S><S sid =""2"" ssid = ""2"">With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S><S sid =""131"" ssid = ""2"">Under this model  many knowledge sources that may be intractable to be incorporated into the perceptron directly  can be utilized effectively in the outside-layer linear model.</S><S sid =""135"" ssid = ""6"">In addition  all knowledge sources we used in the core perceptron and the outside-layer linear model come from the training corpus  whereas many open knowledge sources (lexicon etc.) can be used to improve performance (Ng and Low  2004).</S>",['Results_Citation']
15,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"Inspired by (Jiang et al, 2008), we set the real d Although Table 5 has shown that the proposed all the value of C0 to be 2.0, the value of C-1C0anC0C1 to be 3.0, and the values of all other features to be 1.0 for the character-based discriminative-plus model","Inspired by (Jiang et al, 2008), we set the real d Although Table 5 has shown that the proposed all the value of C0 to be 2.0, the value of C-1C0anC0C1 to be 3.0, and the values of all other features to be 1.0 for the character-based discriminative-plus model","['79', '73', '98', '37', '70']","<S sid =""79"" ssid = ""4"">By maintaining a stack of size N at each position i of the sequence  we can preserve the top N best candidate labelled results of subsequence C1:i during decoding.</S><S sid =""73"" ssid = ""24"">For instance  if the word w appears N times in training corpus and is labelled as POS t for n times  the probability Pr(t|w) can be estimated by the formula below: The probability Pr(w|t) could be estimated through the same approach.</S><S sid =""98"" ssid = ""9"">We found that LEX outperforms NON-LEX with a margin of about 0.002 at each iteration  and its learning curve reaches a tableland at iteration 7.</S><S sid =""37"" ssid = ""9"">C represents a Chinese character while the subscript of C indicates its position in the sentence relative to the current character (it has the subscript 0).</S><S sid =""70"" ssid = ""21"">Given a training corpus with POS tags  we can train a word-POS co-occurrence model to approximate the probability that the word sequence of the labelled result co-exists with its corresponding POS sequence.</S>",['Method_Citation']
17,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"Last, (Jiang et al, 2008) 5 adds repeated features implicitly based on (Ng and Low, 2004)","Last, (Jiang et al, 2008) 5 adds repeated features implicitly based on (Ng and Low, 2004)","['16', '87', '103', '68', '101']","<S sid =""16"" ssid = ""12"">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S><S sid =""87"" ssid = ""12"">Line 6 enumerates all POS’s for the word w spanning length l and ending at position i.</S><S sid =""103"" ssid = ""14"">We turned to experiments on CTB 5.0 to test the performance of the cascaded model.</S><S sid =""68"" ssid = ""19"">It is an important measure of fluency of the translation in SMT.</S><S sid =""101"" ssid = ""12"">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S>",['Method_Citation']
20,P08-1102,D12-1046,0,Jiang et al2008a,0,"Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a) ,perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b)","Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a), perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b)","['31', '22', '91', '45', '16']","<S sid =""31"" ssid = ""3"">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S><S sid =""22"" ssid = ""18"">2 Segmentation and POS Tagging Given a Chinese character sequence: while the segmentation and POS tagging result can be depicted as: Here  Ci (i = L.n) denotes Chinese character  ti (i = L.m) denotes POS tag  and Cl:r (l < r) denotes character sequence ranges from Cl to Cr.</S><S sid =""91"" ssid = ""2"">The first was conducted to test the performance of the perceptron on segmentation on the corpus from SIGHAN Bakeoff 2  including the Academia Sinica Corpus (AS)  the Hong Kong City University Corpus (CityU)  the Peking University Corpus (PKU) and the Microsoft Research Corpus (MSR).</S><S sid =""45"" ssid = ""17"">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S><S sid =""16"" ssid = ""12"">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S>",['Method_Citation']
