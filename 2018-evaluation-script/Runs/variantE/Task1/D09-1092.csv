Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,D09-1092,P14-1004,0,"Mimno et al, 2009",0,"This configuration is similar to PolyLDA (Mimno et al, 2009) or LinkLDA (Yano et al, 2009), such that utterances from different parties are treated as different languages or blog-post and comments pairs","This configuration is similar to PolyLDA (Mimno et al, 2009) or LinkLDA (Yano et al, 2009), such that utterances from different parties are treated as different languages or blog-post and comments pairs","['131', '4', '134', '54', '35']","<S sid =""131"" ssid = ""80"">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid =""4"" ssid = ""4"">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid =""134"" ssid = ""83"">Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.</S><S sid =""54"" ssid = ""3"">Although direct translations in multiple languages are relatively rare (in contrast with comparable documents)  we use direct translations to explore the characteristics of the model.</S><S sid =""35"" ssid = ""1"">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S>",['Results_Citation']
2,D09-1092,P10-1044,0,"Mimno et al, 2009",0,"Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009)","Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009)","['148', '146', '4', '131', '6']","<S sid =""148"" ssid = ""97"">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid =""146"" ssid = ""95"">In addition to enhancing lexicons by aligning topic-specific vocabulary  PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.</S><S sid =""4"" ssid = ""4"">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid =""131"" ssid = ""80"">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid =""6"" ssid = ""2"">Topic models have been used for analyzing topic trends in research literature (Mann et al.  2006; Hall et al.  2008)  inferring captions for images (Blei and Jordan  2003)  social network analysis in email (McCallum et al.  2005)  and expanding queries with topically related words in information retrieval (Wei and Croft  2006).</S>",['Method_Citation']
3,D09-1092,P11-2084,0,"Mimno et al, 2009",0,"(Mimno et al, 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then add the Cartesian product of these sets for every topic to a set of candidate translations","(Mimno et al, 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then add the Cartesian product of these sets for every topic to a set of candidate translations","['146', '131', '148', '25', '69']","<S sid =""146"" ssid = ""95"">In addition to enhancing lexicons by aligning topic-specific vocabulary  PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.</S><S sid =""131"" ssid = ""80"">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid =""148"" ssid = ""97"">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid =""25"" ssid = ""1"">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid =""69"" ssid = ""18"">English and the Romance languages use only singular and plural versions of “objective.” The other Germanic languages include compound words  while Greek and Finnish are dominated by inflected variants of the same lexical item.</S>",['Method_Citation']
4,D09-1092,E12-1014,0,"Mimno et al, 2009",0,"Our Wikipedia-based topic similarity feature, w (f, e), is similar in spirit to polylingualtopic models (Mimno et al 2009), but it is scalable to full bilingual lexicon induction","Our Wikipedia-based topic similarity feature, w (f, e), is similar in spirit to polylingual topic models (Mimno et al 2009), but it is scalable to full bilingual lexicon induction","['25', '146', '195', '4', '19']","<S sid =""25"" ssid = ""1"">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid =""146"" ssid = ""95"">In addition to enhancing lexicons by aligning topic-specific vocabulary  PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.</S><S sid =""195"" ssid = ""4"">Additionally  PLTM can support the creation of bilingual lexica for low resource language pairs  providing candidate translations for more computationally intense alignment processes without the sentence-aligned translations typically used in such tasks.</S><S sid =""4"" ssid = ""4"">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid =""19"" ssid = ""15"">We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S>",['Method_Citation']
5,D09-1092,D11-1086,0,"Mimno et al, 2009",0,"of English document and the second half of its aligned foreign language document (Mimno et al,2009)","For Europarl data sets, we artificially make them comparable by considering the first half of English document and the second half of its aligned foreign language document (Mimno et al,2009)","['131', '47', '148', '86', '35']","<S sid =""131"" ssid = ""80"">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid =""47"" ssid = ""13"">We take the latter approach  and use the MAP estimate for αm and the predictive distributions over words for Φ1  .</S><S sid =""148"" ssid = ""97"">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid =""86"" ssid = ""35"">Given a set of training document tuples  PLTM can be used to obtain posterior estimates of Φ'  ...   ΦL and αm.</S><S sid =""35"" ssid = ""1"">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S>",['Method_Citation']
6,D09-1092,N12-1007,0,"Mimno et al, 2009",0,"Since the PLTM is not a contribution of this paper, we refer the interested reader to (Mimno et al, 2009) for more details","Since the PLTM is not a contribution of this paper, we refer the interested reader to (Mimno et al, 2009) for more details","['161', '59', '182', '37', '91']","<S sid =""161"" ssid = ""110"">As noted before  many of the documents in the EuroParl collection consist of short  formulaic sentences.</S><S sid =""59"" ssid = ""8"">The remaining collection consists of over 121 million words.</S><S sid =""182"" ssid = ""16"">As with EuroParl  we can calculate the JensenShannon divergence between pairs of documents within a comparable document tuple.</S><S sid =""37"" ssid = ""3"">PLTM assumes that the documents in a tuple share the same tuple-specific distribution over topics.</S><S sid =""91"" ssid = ""40"">We use the “left-to-right” method of (Wallach et al.  2009).</S>",['Method_Citation']
7,D09-1092,N12-1007,0,2009,0,"Evaluation Corpus The automatic evaluation of cross-lingual co reference systems requires annotated 10Mimno et al (2009) showed that so long as the proportion of topically-aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly","Mimno et al (2009) showed that so long as the proportion of topically-aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly","['158', '187', '95', '72', '154']","<S sid =""158"" ssid = ""107"">Results averaged over all query/target language pairs are shown in figure 7 for Jensen-Shannon divergence.</S><S sid =""187"" ssid = ""21"">To demonstrate the wide variation in topics  we calculated the proportion of tokens in each language assigned to each topic.</S><S sid =""95"" ssid = ""44"">Additionally  the predictive ability of PLTM is consistently slightly worse than that of (monolingual) LDA.</S><S sid =""72"" ssid = ""21"">To answer this question  we compute the posterior probability of each topic in each tuple under the trained model.</S><S sid =""154"" ssid = ""103"">We then use this model to infer topic distributions for each of the 11 documents in each of the held-out document tuples using a method similar to that used to calculate held-out probabilities (Wallach et al.  2009).</S>",['Method_Citation']
8,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"Similarly, Polylingual Topic Models (PLTM) (Mimno et al, 2009) generalized LDA to tuples of documents from multiple languages","Similarly, Polylingual Topic Models (PLTM) (Mimno et al, 2009) generalized LDA to tuples of documents from multiple languages","['134', '131', '35', '54', '91']","<S sid =""134"" ssid = ""83"">Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.</S><S sid =""131"" ssid = ""80"">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid =""35"" ssid = ""1"">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S><S sid =""54"" ssid = ""3"">Although direct translations in multiple languages are relatively rare (in contrast with comparable documents)  we use direct translations to explore the characteristics of the model.</S><S sid =""91"" ssid = ""40"">We use the “left-to-right” method of (Wallach et al.  2009).</S>",['Method_Citation']
9,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"Our baseline joint PLSA model (JPLSA) is closely related to the poly-lingual LDA model of (Mimno et al, 2009)","Our baseline joint PLSA model (JPLSA) is closely related to the poly-lingual LDA model of (Mimno et al, 2009)","['35', '86', '131', '91', '46']","<S sid =""35"" ssid = ""1"">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S><S sid =""86"" ssid = ""35"">Given a set of training document tuples  PLTM can be used to obtain posterior estimates of Φ'  ...   ΦL and αm.</S><S sid =""131"" ssid = ""80"">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid =""91"" ssid = ""40"">We use the “left-to-right” method of (Wallach et al.  2009).</S><S sid =""46"" ssid = ""12"">  ΦL and αm from P(Φ1  ...   ΦL  αm  |W'  β) or by evaluating a point estimate.</S>",['Method_Citation']
10,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"We describe the model for two languages, but it is straightforward to generalize to more than two languages, as in (Mimno et al, 2009)","We describe the model for two languages, but it is straightforward to generalize to more than two languages, as in (Mimno et al, 2009)","['67', '54', '38', '36', '65']","<S sid =""67"" ssid = ""16"">(Interestingly  all languages except Greek and Finnish use closely related words for “youth” or “young” in a separate topic.)</S><S sid =""54"" ssid = ""3"">Although direct translations in multiple languages are relatively rare (in contrast with comparable documents)  we use direct translations to explore the characteristics of the model.</S><S sid =""38"" ssid = ""4"">This is unlike LDA  in which each document is assumed to have its own document-specific distribution over topics.</S><S sid =""36"" ssid = ""2"">Each tuple is a set of documents that are loosely equivalent to each other  but written in different languages  e.g.  corresponding Wikipedia articles in French  English and German.</S><S sid =""65"" ssid = ""14"">This topic provides an illustration of the variation in technical terminology captured by PLTM  including the wide array of acronyms used by different languages.</S>",['Method_Citation']
11,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"j=1 P (z2j|?) P (w 2 j| ?z2j) The difference between the JPLSA model and the poly-lingual topic model of (Mimno et al, 2009) is that we merge the vocabularies in the two languages and learn topic-specific word distributions over these merged vocabularies, instead of having pairs of topic-specific word distributions, one for each language, like in (Mimno et al, 2009)","The difference between the JPLSA model and the poly-lingual topic model of (Mimno et al, 2009) is that we merge the vocabularies in the two languages and learn topic-specific word distributions over these merged vocabularies, instead of having pairs of topic-specific word distributions, one for each language, like in (Mimno et al, 2009)","['148', '143', '39', '78', '40']","<S sid =""148"" ssid = ""97"">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid =""143"" ssid = ""92"">We also do not count morphological variants: the model finds EN “rules” and DE “vorschriften ” but the lexicon contains only “rule” and “vorschrift.” Results remain strong as we increase K. With K = 3  T = 800  1349 of the 7200 candidate pairs for Spanish appeared in the lexicon. topic in different languages translations of each other?</S><S sid =""39"" ssid = ""5"">Additionally  PLTM assumes that each “topic” consists of a set of discrete distributions over words—one for each language l = 1  ...   L. In other words  rather than using a single set of topics Φ = {φ1  ...   φT}  as in LDA  there are L sets of language-specific topics  Φ1  ...   ΦL  each of which is drawn from a language-specific symmetric Dirichlet with concentration parameter βl.</S><S sid =""78"" ssid = ""27"">Although the model does not distinguish between topic assignment variables within a given document tuple (so it is technically incorrect to speak of different posterior distributions over topics for different documents in a given tuple)  we can nevertheless divide topic assignment variables between languages and use them to estimate a Dirichlet-multinomial posterior distribution for each language in each tuple.</S><S sid =""40"" ssid = ""6"">Anew document tuple w = (w1  ...   wL) is generated by first drawing a tuple-specific topic distribution from an asymmetric Dirichlet prior with concentration parameter α and base measure m: Then  for each language l  a latent topic assignment is drawn for each token in that language: Finally  the observed tokens are themselves drawn using the language-specific topic parameters: wl ∼ P(wl  |zl Φl) = 11n φlwl |zl .</S>",['Method_Citation']
12,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"Another difference between our model and the poly-lingual LDA model of (Mimno et al, 2009) is that we use maximum aposteriori (MAP) instead of Bayesian inference","Another difference between our model and the poly-lingual LDA model of (Mimno et al, 2009) is that we use maximum aposteriori (MAP) instead of Bayesian inference","['52', '68', '193', '25', '129']","<S sid =""52"" ssid = ""1"">Our first set of experiments focuses on document tuples that are known to consist of direct translations.</S><S sid =""68"" ssid = ""17"">The third topic demonstrates differences in inflectional variation.</S><S sid =""193"" ssid = ""2"">We analyzed the characteristics of PLTM in comparison to monolingual LDA  and demonstrated that it is possible to discover aligned topics.</S><S sid =""25"" ssid = ""1"">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid =""129"" ssid = ""78"">We therefore evaluate the ability of the PLTM to generate bilingual lexica  similar to other work in unsupervised translation modeling (Haghighi et al.  2008).</S>",['Method_Citation']
13,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"For computing distance we used the L1-norm of the difference, which worked a bit better than the Jensen Shannon divergence between the topic vectors used in (Mimno et al, 2009)","For computing distance we used the L1-norm of the difference, which worked a bit better than the Jensen Shannon divergence between the topic vectors used in (Mimno et al, 2009)","['147', '43', '52', '25', '92']","<S sid =""147"" ssid = ""96"">These aligned document pairs could then be fed into standard machine translation systems as training data.</S><S sid =""43"" ssid = ""9"">These tasks can either be accomplished by averaging over samples of Φ1  .</S><S sid =""52"" ssid = ""1"">Our first set of experiments focuses on document tuples that are known to consist of direct translations.</S><S sid =""25"" ssid = ""1"">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid =""92"" ssid = ""41"">We perform five estimation runs for each document and then calculate standard errors using a bootstrap method.</S>",['Results_Citation']
15,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"In previously reported work, (Mimno et al, 2009) evaluate parallel document retrieval using PLTM on Europarl speeches in English and Spanish, using training and test sets of size similar to ours","In previously reported work, (Mimno et al, 2009) evaluate parallel document retrieval using PLTM on Europarl speeches in English and Spanish, using training and test sets of size similar to ours","['25', '43', '148', '47', '131']","<S sid =""25"" ssid = ""1"">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid =""43"" ssid = ""9"">These tasks can either be accomplished by averaging over samples of Φ1  .</S><S sid =""148"" ssid = ""97"">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid =""47"" ssid = ""13"">We take the latter approach  and use the MAP estimate for αm and the predictive distributions over words for Φ1  .</S><S sid =""131"" ssid = ""80"">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S>",['Method_Citation']
16,D09-1092,W12-3117,0,"Mimno et al, 2009",0,"We assume that a higher dimensionality can lead to a better repartitioning of the vocabulary over the topics. Multilingual LDA has been used before in natural language processing ,e.g .polylingual topic models (Mimno et al, 2009) or multilingual topic models for unaligned text (Boyd-Graber and Blei, 2009)","Multilingual LDA has been used before in natural language processing, e.g. polylingual topic models (Mimno et al, 2009) or multilingual topic models for unaligned text (Boyd-Graber and Blei, 2009)","['4', '35', '134', '131', '55']","<S sid =""4"" ssid = ""4"">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid =""35"" ssid = ""1"">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S><S sid =""134"" ssid = ""83"">Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.</S><S sid =""131"" ssid = ""80"">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid =""55"" ssid = ""4"">The EuroParl corpus consists of parallel texts in eleven western European languages: Danish  German  Greek  English  Spanish  Finnish  French  Italian  Dutch  Portuguese and Swedish.</S>",['Method_Citation']
17,D09-1092,W11-2133,0,2009,0,"ji =wjk? M j i? m k=1w j k, (1) where M j is the topic distribution of document j and wk is the number of occurrences of phrase pair Xk in document j. Mimno et al (2009) extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipediaarticles)","Mimno et al (2009) extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipedia articles)","['131', '134', '4', '35', '25']","<S sid =""131"" ssid = ""80"">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid =""134"" ssid = ""83"">Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.</S><S sid =""4"" ssid = ""4"">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid =""35"" ssid = ""1"">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S><S sid =""25"" ssid = ""1"">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S>",['Method_Citation']
18,D09-1092,W11-2133,0,2009,0,Tuple-specific topic distributions arelearned using LDA with distinct topic-word concentration parameters? l. Mimno et al (2009) show that PLTM sufficiently aligns topics in parallel corpora,Mimno et al (2009) show that PLTM sufficiently aligns topics in parallel corpora,"['59', '161', '134', '91', '164']","<S sid =""59"" ssid = ""8"">The remaining collection consists of over 121 million words.</S><S sid =""161"" ssid = ""110"">As noted before  many of the documents in the EuroParl collection consist of short  formulaic sentences.</S><S sid =""134"" ssid = ""83"">Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.</S><S sid =""91"" ssid = ""40"">We use the “left-to-right” method of (Wallach et al.  2009).</S><S sid =""164"" ssid = ""113"">Results vary by language.</S>",['Method_Citation']
19,D09-1092,P14-2110,0,"Mimno et al, 2009",0,"A good candidate for multilingual topic analyses are polylin gual topic models (Mimno et al, 2009), which learn topics for multiple languages, creating tuples of language specific distributions over monolingual vocabularies for each topic","A good candidate for multilingual topic analyses are polylingual topic models (Mimno et al, 2009), which learn topics for multiple languages, creating tuples of language specific distributions over monolingual vocabularies for each topic","['25', '146', '195', '24', '83']","<S sid =""25"" ssid = ""1"">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid =""146"" ssid = ""95"">In addition to enhancing lexicons by aligning topic-specific vocabulary  PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.</S><S sid =""195"" ssid = ""4"">Additionally  PLTM can support the creation of bilingual lexica for low resource language pairs  providing candidate translations for more computationally intense alignment processes without the sentence-aligned translations typically used in such tasks.</S><S sid =""24"" ssid = ""20"">By linking topics across languages  polylingual topic models can increase cross-cultural understanding by providing readers with the ability to characterize the contents of collections in unfamiliar languages and identify trends in topic prevalence.</S><S sid =""83"" ssid = ""32"">The vast majority of divergences are relatively low (1.0 indicates no overlap in topics between languages in a given document tuple) indicating that  for each tuple  the model is not simply assigning all tokens in a particular language to a single topic.</S>",['Method_Citation']
20,D09-1092,P14-2110,0,2009,0,"3csLDATo train a polylingual topic model on social media, we make two modifications to the model of Mimno et al (2009): add a token specific language variable, and a process for identifying aligned top ics. First ,polylingual topic models require parallel or comparable corpora in which each document has an assigned language","To train a polylingual topic model on social media, we make two modifications to the model of Mimno et al (2009): add a token specific language variable, and a process for identifying aligned top ics","['43', '47', '25', '120', '148']","<S sid =""43"" ssid = ""9"">These tasks can either be accomplished by averaging over samples of Φ1  .</S><S sid =""47"" ssid = ""13"">We take the latter approach  and use the MAP estimate for αm and the predictive distributions over words for Φ1  .</S><S sid =""25"" ssid = ""1"">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid =""120"" ssid = ""69"">From the results in figure 4  we know that leaving all document tuples intact should result in a mean JS divergence of less than 0.1.</S><S sid =""148"" ssid = ""97"">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S>","['Aim_Citation', 'Results_Citation']"
