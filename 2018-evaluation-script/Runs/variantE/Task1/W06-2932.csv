Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
2,W06-2932,W06-2920,nan,"McDonald et al, 2006",0,"Introduce through post-processing ,e.g. through reattachment rules (Bick, 2006) or if the change increases overall parse tree probability (McDonald et al, 2006)","Introduce through post-processing ,e.g. through reattachment rules (Bick, 2006) or if the change increases overall parse tree probability (McDonald et al, 2006)","['3', '4', '108', '10', '106']","<S sid =""3"" ssid = ""3"">The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.</S><S sid =""4"" ssid = ""4"">We report results on the CoNLL-X shared task (Buchholz et al.  2006) data sets and present an error analysis.</S><S sid =""108"" ssid = ""5"">Second  we plan on integrating any available morphological features in a more principled manner.</S><S sid =""10"" ssid = ""6"">Dependency graphs also encode much of the deep syntactic information needed for further processing.</S><S sid =""106"" ssid = ""3"">First  we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.</S>",['Results_Citation']
3,W06-2932,W06-2920,0,2006,0,Table 5 shows the official results for submitted parser outputs.31 The two participant groups with the highest total score are McDonald et al (2006) and Nivre et al (2006),Table 5 shows the official results for submitted parser outputs. The two participant groups with the highest total score are McDonald et al (2006) and Nivre et al (2006),"['101', '17', '16', '2', '20']","<S sid =""101"" ssid = ""23"">For example if we train on 200  400  800 and the full training set  labeled accuracies are 54%  60%  62% and 67%.</S><S sid =""17"" ssid = ""13"">Each edge can be assigned a label l(ij) from a finite set L of predefined labels.</S><S sid =""16"" ssid = ""12"">A dependency graph is represented by a set of ordered pairs (i  j) E y in which xj is a dependent and xi is the corresponding head.</S><S sid =""2"" ssid = ""2"">The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.</S><S sid =""20"" ssid = ""2"">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S>",['Method_Citation']
4,W06-2932,W06-2920,0,2006,0,"Even though McDonald et al (2006) and Nivre et al (2006) obtained very similar overall scores, a more detailed look at their performance shows clear differences","Even though McDonald et al (2006) and Nivre et al (2006) obtained very similar overall scores, a more detailed look at their performance shows clear differences","['61', '66', '88', '83', '50']","<S sid =""61"" ssid = ""9"">In fact  for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).</S><S sid =""66"" ssid = ""4"">These results report the average labeled and unlabeled precision for the 10 languages with the smallest training sets.</S><S sid =""88"" ssid = ""10"">Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.</S><S sid =""83"" ssid = ""5"">In a preliminary test of this hypothesis  we looked at all of the sentences from a development set in which a main verb is incorrectly attached.</S><S sid =""50"" ssid = ""19"">Various conjunctions of these were included based on performance on held-out data.</S>",['Method_Citation']
5,W06-2932,W08-1007,0,2006,0,"The high est score on parsing German in the CoNLL-X shared task was obtained by the system of McDonald et al (2006) with a LAS of 87.34 based on the TIGER tree bank, but we want to stress that these results are not comparable due to different data sets (anda different policy regarding the inclusion of punctuation) .The constituency versions were evaluated according to the labeled recall (LR), labeled precision (LP) and labeled F-score (LF)","The highest score on parsing German in the CoNLL-X shared task was obtained by the system of McDonald et al (2006) with a LAS of 87.34 based on the TIGER tree bank, but we want to stress that these results are not comparable due to different data sets (and a different policy regarding the inclusion of punctuation). The constituency versions were evaluated according to the labeled recall (LR), labeled precision (LP) and labeled F-score (LF)","['76', '41', '96', '57', '64']","<S sid =""76"" ssid = ""14"">For instance  sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.</S><S sid =""41"" ssid = ""10"">To model this we treat the labeling of the edges (i  j1)  ...   (i  jM) as a sequence labeling problem  We use a first-order Markov factorization of the score s(l(i jm)  l(i jm�1)  i  y  x) in which each factor is the score of labeling the adjacent edges (i  jm) and (i  jm−1) in the tree y.</S><S sid =""96"" ssid = ""18"">Each stage by itself is relatively accurate (unlabeled accuracy is 79% and labeling accuracy3 is also 79%)  but since there is very little overlap in the kinds of errors each makes  overall labeled accuracy drops to 67%.</S><S sid =""57"" ssid = ""5"">Performance is measured through unlabeled accuracy  which is the percentage of words that modify the correct head in the dependency graph  and labeled accuracy  which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph.</S><S sid =""64"" ssid = ""2"">N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.</S>",['Method_Citation']
6,W06-2932,W09-1210,0,2006,nan,McDonald et al (2006) use an additional algorithm,McDonald et al (2006) use an additional algorithm,"['19', '47', '32', '36', '17']","<S sid =""19"" ssid = ""1"">The first stage of our system creates an unlabeled parse y for an input sentence x.</S><S sid =""47"" ssid = ""16"">We used the following: dependent have identical values?</S><S sid =""32"" ssid = ""1"">The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).</S><S sid =""36"" ssid = ""5"">However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S><S sid =""17"" ssid = ""13"">Each edge can be assigned a label l(ij) from a finite set L of predefined labels.</S>",['Method_Citation']
7,W06-2932,W12-3407,0,"McDonald et al, 2006",0,"Regarding the data-driven parsers, we have made use of MaltParser (Nivre et al, 2007b) and MST Parser (McDonald et al, 2006), two state of the art dependency parsers representing two dominant approaches in data-driven dependency parsing, and that have been successfully applied to typologically different languages and tree banks (McDonald and Nivre, 2007)","Regarding the data-driven parsers, we have made use of MaltParser (Nivre et al, 2007b) and MST Parser (McDonald et al, 2006), two state of the art dependency parsers representing two dominant approaches in data-driven dependency parsing, and that have been successfully applied to typologically different languages and tree banks (McDonald and Nivre, 2007)","['54', '11', '104', '21', '63']","<S sid =""54"" ssid = ""2"">Based on performance from a held-out section of the training data  we used non-projective parsing algorithms for Czech  Danish  Dutch  German  Japanese  Portuguese and Slovene  and projective parsing algorithms for Arabic  Bulgarian  Chinese  Spanish  Swedish and Turkish.</S><S sid =""11"" ssid = ""7"">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid =""104"" ssid = ""1"">We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al.  2005b; McDonald and Pereira  2006) generalizes well to languages other than English.</S><S sid =""21"" ssid = ""3"">That work extends the maximum spanning tree dependency parsing framework (McDonald et al.  2005a; McDonald et al.  2005b) to incorporate features over multiple edges in the dependency graph.</S><S sid =""63"" ssid = ""1"">Our system has several components  including the ability to produce non-projective edges  sequential Japanese  Portuguese  Slovene  Spanish  Swedish and Turkish.</S>",['Method_Citation']
8,W06-2932,I08-1012,0,"McDonald et al, 2006",0,"In fact, our approach can also be applied to other parsers, such as (Yamada and Matsumoto, 2003)? s parser, (McDonald et al., 2006)? s parser, and so on","In fact, our approach can also be applied to other parsers, such as (Yamada and Matsumoto, 2003)'s parser, (McDonald et al., 2006)'s parser, and so on","['1', '58', '0', '63', '108']","<S sid =""1"" ssid = ""1"">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S><S sid =""58"" ssid = ""6"">These results show that the discriminative spanning tree parsing framework (McDonald et al.  2005b; McDonald and Pereira  2006) is easily adapted across all these languages.</S><S sid =""0"" ssid = ""0"">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S sid =""63"" ssid = ""1"">Our system has several components  including the ability to produce non-projective edges  sequential Japanese  Portuguese  Slovene  Spanish  Swedish and Turkish.</S><S sid =""108"" ssid = ""5"">Second  we plan on integrating any available morphological features in a more principled manner.</S>",['Method_Citation']
11,W06-2932,N07-1050,0,"McDonald et al, 2006",0,"We have shown that, for languages with a7McDonald et al (2006) use post-processing for non projective dependencies and for labeling",McDonald et al (2006) use post-processing for non-projective dependencies and for labeling,"['0', '20', '17', '53', '1']","<S sid =""0"" ssid = ""0"">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S sid =""20"" ssid = ""2"">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid =""17"" ssid = ""13"">Each edge can be assigned a label l(ij) from a finite set L of predefined labels.</S><S sid =""53"" ssid = ""1"">We trained models for all 13 languages provided by the CoNLL organizers (Buchholz et al.  2006).</S><S sid =""1"" ssid = ""1"">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S>",['Method_Citation']
12,W06-2932,D07-1122,0,"McDonald et al, 2006",0,"As described in (McDonald et al, 2006), we treat the labeling of dependencies as a sequence labeling problem","As described in (McDonald et al, 2006), we treat the labeling of dependencies as a sequence labeling problem","['47', '2', '19', '20', '32']","<S sid =""47"" ssid = ""16"">We used the following: dependent have identical values?</S><S sid =""2"" ssid = ""2"">The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.</S><S sid =""19"" ssid = ""1"">The first stage of our system creates an unlabeled parse y for an input sentence x.</S><S sid =""20"" ssid = ""2"">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid =""32"" ssid = ""1"">The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).</S>",['Method_Citation']
14,W06-2932,D07-1015,0,2006,0,5It should be noted that McDonald et al (2006) use a richer feature set that is incomparable to our features,It should be noted that McDonald et al (2006) use a richer feature set that is incomparable to our features,"['81', '23', '86', '43', '36']","<S sid =""81"" ssid = ""3"">Other high-frequency word classes with relatively low attachment accuracy are prepositions (80%)  adverbs (82%) and subordinating conjunctions (80%)  for a total of another 23% of the test corpus.</S><S sid =""23"" ssid = ""5"">That system uses MIRA  an online large-margin learning algorithm  to compute model parameters.</S><S sid =""86"" ssid = ""8"">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S><S sid =""43"" ssid = ""12"">For score functions  we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation  we can find the highest scoring label sequence with Viterbi’s algorithm.</S><S sid =""36"" ssid = ""5"">However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S>",['Method_Citation']
18,W06-2932,D10-1004,0,2006,0,"Entries marked with? are the highest reported in the literature, to the best of our knowledge, beating (sometimes slightly) McDonald et al (2006), Martins et al (2008), Martins et al (2009), and, in the case of English Proj., also the third-order parser of Koo and Collins (2010), which achieves 93.04% on that dataset (their experiments in Czech are not comparable, since the datasets are different)","Entries marked with are the highest reported in the literature, to the best of our knowledge, beating (sometimes slightly) McDonald et al (2006), Martins et al (2008), Martins et al (2009), and, in the case of English Proj., also the third-order parser of Koo and Collins (2010), which achieves 93.04% on that dataset (their experiments in Czech are not comparable, since the datasets are different)","['18', '41', '76', '64', '79']","<S sid =""18"" ssid = ""14"">We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216–220  New York City  June 2006. c�2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.</S><S sid =""41"" ssid = ""10"">To model this we treat the labeling of the edges (i  j1)  ...   (i  jM) as a sequence labeling problem  We use a first-order Markov factorization of the score s(l(i jm)  l(i jm�1)  i  y  x) in which each factor is the score of labeling the adjacent edges (i  jm) and (i  jm−1) in the tree y.</S><S sid =""76"" ssid = ""14"">For instance  sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.</S><S sid =""64"" ssid = ""2"">N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.</S><S sid =""79"" ssid = ""1"">Although overall unlabeled accuracy is 86%  most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs  75% for the verb ser  and 65% for coordinating conjunctions.</S>",['Method_Citation']
19,W06-2932,P08-1108,0,"McDonald et al, 2006",0,"The specific graph-based model studied in this work is that presented by McDonald et al (2006), which factors scores over pairs of arcs (instead of just single arcs) and uses near exhaustive search for unlabeled parsing coupled with a separate classifier to label each arc. We call this system MSTParser, or simply MST for short, which is also the name of the freely available implementation.2 2.3 Transition-Based Models","The specific graph-based model studied in this work is that presented by McDonald et al (2006), which factors scores over pairs of arcs (instead of just single arcs) and uses near exhaustive search for unlabeled parsing coupled with a separate classifier to label each arc. We call this system MSTParser, or simply MST for short, which is also the name of the freely available implementation.","['18', '3', '96', '11', '95']","<S sid =""18"" ssid = ""14"">We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216–220  New York City  June 2006. c�2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.</S><S sid =""3"" ssid = ""3"">The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.</S><S sid =""96"" ssid = ""18"">Each stage by itself is relatively accurate (unlabeled accuracy is 79% and labeling accuracy3 is also 79%)  but since there is very little overlap in the kinds of errors each makes  overall labeled accuracy drops to 67%.</S><S sid =""11"" ssid = ""7"">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid =""95"" ssid = ""17"">Note the difference in error between the unlabeled parser and the edge labeler: the former makes mistakes on edges into prepositions  conjunctions and verbs  and the latter makes mistakes on edges into nouns (subject/objects).</S>",['Method_Citation']
20,W06-2932,P08-1108,0,"McDonald et al, 2006",0,"More precisely, dependency arcs (or pairs of arcs) are first represented by a high dimensional feature vector f (i, j, l)? Rk, where f is typically a bi nary feature vector over properties of the arc as well as the surrounding input (McDonald et al, 2005a; McDonald et al, 2006)","More precisely, dependency arcs (or pairs of arcs) are first represented by a high dimensional feature vector f (i, j, l) Rk, where f is typically a binary feature vector over properties of the arc as well as the surrounding input (McDonald et al, 2005a; McDonald et al, 2006)","['32', '11', '41', '36', '16']","<S sid =""32"" ssid = ""1"">The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).</S><S sid =""11"" ssid = ""7"">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid =""41"" ssid = ""10"">To model this we treat the labeling of the edges (i  j1)  ...   (i  jM) as a sequence labeling problem  We use a first-order Markov factorization of the score s(l(i jm)  l(i jm�1)  i  y  x) in which each factor is the score of labeling the adjacent edges (i  jm) and (i  jm−1) in the tree y.</S><S sid =""36"" ssid = ""5"">However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S><S sid =""16"" ssid = ""12"">A dependency graph is represented by a set of ordered pairs (i  j) E y in which xj is a dependent and xi is the corresponding head.</S>",['Results_Citation']
