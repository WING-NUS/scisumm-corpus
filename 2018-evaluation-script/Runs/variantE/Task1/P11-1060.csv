Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P11-1060,D11-1039,0,2011,0,"Clarkeet al (2010) and Liang et al (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al (2011) presents work on unsupervised learning","Clarke et al (2010) and Liang et al (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al (2011) presents work on unsupervised learning","['26', '158', '21', '23', '132']","<S sid =""26"" ssid = ""2"">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid =""158"" ssid = ""43"">5 Discussion Piantadosi et al. (2008) induces first-order formuA major focus of this work is on our semantic rep- lae using CCG in a small domain assuming observed resentation  DCS  which offers a new perspective lexical semantics.</S><S sid =""21"" ssid = ""17"">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid =""23"" ssid = ""19"">We trained our model using an EM-like algorithm (Section 3) on two benchmarks  GEO and JOBS (Section 4).</S><S sid =""132"" ssid = ""17"">Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.</S>",['Results_Citation']
2,P11-1060,P13-1092,0,2011,0,"In particular, Clarke et al (2010) and Liang et al (2011) proposed methods to learn from question answer pairs alone, which represents a significant advance","In particular, Clarke et al (2010) and Liang et al (2011) proposed methods to learn from question answer pairs alone, which represents a significant advance","['39', '56', '4', '136', '83']","<S sid =""39"" ssid = ""15"">Such a z defines a constraint satisfaction problem (CSP) with nodes as variables.</S><S sid =""56"" ssid = ""32"">The basic version of DCS described thus far handles a core subset of language.</S><S sid =""4"" ssid = ""4"">On two stansemantic parsing benchmarks our system obtains the highest published accuracies  despite requiring no annotated logical forms.</S><S sid =""136"" ssid = ""21"">In fact  DCS performs comparably to even the version of SEMRESP trained using logical forms.</S><S sid =""83"" ssid = ""59"">It suffices to define Xi(d) for a single column i.</S>",['Method_Citation']
3,P11-1060,P13-1092,0,2011,0,"To handle syntax-semantics mismatch, GUSP introduces a novel dependency-based meaning representation 1Clarke et al (2010) and Liang et al (2011) used the annotated logical forms to compute answers for their experiments","To handle syntax-semantics mismatch, GUSP introduces a novel dependency-based meaning representation. Clarke et al (2010) and Liang et al (2011) used the annotated logical forms to compute answers for their experiments","['26', '100', '115', '88', '148']","<S sid =""26"" ssid = ""2"">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid =""100"" ssid = ""76"">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid =""115"" ssid = ""91"">After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy pθ(T)(y  |x  z ∈ ˜ZL θ(T)).</S><S sid =""88"" ssid = ""64"">Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.</S><S sid =""148"" ssid = ""33"">This bootstrapping behavior occurs naturally: The “easy” examples are processed first  where easy is defined by the ability of the current model to generate the correct answer using any tree. with scope variation.</S>",['Method_Citation']
4,P11-1060,P13-1092,0,2011,0,"More recently, Liang et al (2011 )proposedDCS for dependency-based compositional semantics, which represents a semantic parse as a tree with nodes representing database elements and operations, and edges representing relational joins","More recently, Liang et al (2011) proposed DCS for dependency-based compositional semantics, which represents a semantic parse as a tree with nodes representing database elements and operations, and edges representing relational joins","['100', '21', '138', '26', '53']","<S sid =""100"" ssid = ""76"">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid =""21"" ssid = ""17"">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid =""138"" ssid = ""23"">Next  we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).</S><S sid =""26"" ssid = ""2"">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid =""53"" ssid = ""29"">Having instantiated s as a value  everything above this node is an ordinary CSP: s constrains the count node  which in turns constrains the root node to |s|.</S>",['Method_Citation']
5,P11-1060,P13-1092,0,"Liang et al, 2011",0,"GUSP represents meaning by a semantic tree, which is similar to DCS (Liang et al, 2011)","GUSP represents meaning by a semantic tree, which is similar to DCS (Liang et al, 2011)","['18', '94', '134', '8', '58']","<S sid =""18"" ssid = ""14"">CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).</S><S sid =""94"" ssid = ""70"">We now turn to the task of mapping natural language For the example in Figure 4(b)  the de- utterances to DCS trees.</S><S sid =""134"" ssid = ""19"">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid =""8"" ssid = ""4"">On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.</S><S sid =""58"" ssid = ""34"">We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.</S>",['Method_Citation']
6,P11-1060,W12-2802,0,2011,0,"Matuszek et al [2010], Liang et al [2011] and Chen and Mooney [2011] describe models that learn compositional semantics, but word meanings are symbolic structures rather than patterns of features in the external world","Matuszek et al [2010], Liang et al [2011] and Chen and Mooney [2011] describe models that learn compositional semantics, but word meanings are symbolic structures rather than patterns of features in the external world","['26', '21', '100', '115', '42']","<S sid =""26"" ssid = ""2"">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid =""21"" ssid = ""17"">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid =""100"" ssid = ""76"">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid =""115"" ssid = ""91"">After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy pθ(T)(y  |x  z ∈ ˜ZL θ(T)).</S><S sid =""42"" ssid = ""18"">The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).</S>",['Method_Citation']
7,P11-1060,P13-2009,0,"Liang et al, 2011",0,"It is well-studied in NLP, and a wide variety of methods have been proposed to tackle it ,e.g. rule-based (Popescu et al, 2003), super vised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al, 2011)","It is well-studied in NLP, and a wide variety of methods have been proposed to tackle it ,e.g. rule-based (Popescu et al, 2003), super vised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al, 2011)","['87', '113', '20', '154', '74']","<S sid =""87"" ssid = ""63"">For example  in Figure 4(a)  before execution  the denotation of the DCS tree is hh{[(CA  OR)  (OR)] ... }; ø; (E  Qhstatei�w  ø)ii; after applying X1  we have hh{[(OR)]  ... }; øii.</S><S sid =""113"" ssid = ""89"">Formally  let ˜O(θ  θ') be the objective function O(θ) with ZL(x) ˜ZL θI(x).</S><S sid =""20"" ssid = ""16"">At the same time  representations such as FunQL (Kate et al.  2005)  which was used in Clarke et al. (2010)  are simpler but lack the full expressive power of lambda calculus.</S><S sid =""154"" ssid = ""39"">The restriction to present (for example  [in  loc] has high weight). trees is similar to economical DRT (Bos  2009).</S><S sid =""74"" ssid = ""50"">Extending this notation to denotations  let (hA; αii[i] = hh{ai : a ∈ A}; αiii.</S>",['Method_Citation']
8,P11-1060,D12-1069,0,Liangetal2011,0,"One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Liangetal2011) or even a binary correct/incorrect signal (Clarke et al2010)","One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Lianget al 2011) or even a binary correct/incorrect signal (Clarke et al2010)","['117', '1', '15', '88', '62']","<S sid =""117"" ssid = ""2"">In each dataset  each sentence x is annotated with a Prolog logical form  which we use only to evaluate and get an answer y.</S><S sid =""1"" ssid = ""1"">Compositional question answering begins by mapping questions to logical forms  but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.</S><S sid =""15"" ssid = ""11"">Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.</S><S sid =""88"" ssid = ""64"">Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.</S><S sid =""62"" ssid = ""38"">Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes)  as well as information stored about each marked node.</S>",['Method_Citation']
9,P11-1060,N12-1049,0,2011,0,"For example, Liang et al (2011) constructs a latent parse similar in structure to a dependency grammar, but representing a logical form","For example, Liang et al (2011) constructs a latent parse similar in structure to a dependency grammar, but representing a logical form","['83', '169', '8', '101', '134']","<S sid =""83"" ssid = ""59"">It suffices to define Xi(d) for a single column i.</S><S sid =""169"" ssid = ""54"">The combination rules are encoded in the tems  despite using no annotated logical forms.</S><S sid =""8"" ssid = ""4"">On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.</S><S sid =""101"" ssid = ""77"">Formally  pθ(z  |x) ∝ eφ(x z)Tθ  where θ and φ(x  z) are parameter and feature vectors  respectively.</S><S sid =""134"" ssid = ""19"">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S>",['Method_Citation']
10,P11-1060,P12-1045,0,2011,0,Clarke et al (2010) and Liang et al (2011) trained systems on question and answer pairs by automatically finding semantic interpretations of the questions that would generate the correct answers,Clarke et al (2010) and Liang et al (2011) trained systems on question and answer pairs by automatically finding semantic interpretations of the questions that would generate the correct answers,"['1', '99', '132', '129', '26']","<S sid =""1"" ssid = ""1"">Compositional question answering begins by mapping questions to logical forms  but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.</S><S sid =""99"" ssid = ""75"">To further reduce the search space  F imposes a few additional constraints  e.g.  limiting the number of marked nodes to 2 and only allowing trace predicates between arity 1 predicates.</S><S sid =""132"" ssid = ""17"">Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.</S><S sid =""129"" ssid = ""14"">We also define an augmented lexicon L+ which includes a prototype word x for each predicate appearing in (iii) above (e.g.  (large  size))  which cancels the predicates triggered by x’s POS tag.</S><S sid =""26"" ssid = ""2"">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S>",['Method_Citation']
11,P11-1060,P14-1008,0,"Liang et al,2011",0,"Dependency-based Compositional Semantics (DCS) provides an intuitive way to model semantics of questions, by using simple dependency-like trees (Liang et al, 2011)","Dependency-based Compositional Semantics (DCS) provides an intuitive way to model semantics of questions, by using simple dependency-like trees (Liang et al, 2011)","['21', '54', '42', '166', '132']","<S sid =""21"" ssid = ""17"">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid =""54"" ssid = ""30"">A DCS tree that contains only join and aggregate relations can be viewed as a collection of treestructured CSPs connected via aggregate relations.</S><S sid =""42"" ssid = ""18"">The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).</S><S sid =""166"" ssid = ""51"">Our employed (Zettlemoyer and Collins  2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language—think grounded al.  2010). compositional semantics.</S><S sid =""132"" ssid = ""17"">Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.</S>",['Method_Citation']
12,P11-1060,P14-1008,0,"Liang et al, 2011",0,"DCS trees has been proposed to represent natural language semantics with a structure similar to dependency trees (Liang et al, 2011) (Figure 1)","DCS trees has been proposed to represent natural language semantics with a structure similar to dependency trees (Liang et al, 2011) (Figure 1)","['59', '54', '154', '45', '43']","<S sid =""59"" ssid = ""35"">The key idea that allows us to give semanticallyscoped denotations to syntactically-scoped trees is as follows: We mark a node low in the tree with a mark relation (one of E  Q  or C).</S><S sid =""54"" ssid = ""30"">A DCS tree that contains only join and aggregate relations can be viewed as a collection of treestructured CSPs connected via aggregate relations.</S><S sid =""154"" ssid = ""39"">The restriction to present (for example  [in  loc] has high weight). trees is similar to economical DRT (Bos  2009).</S><S sid =""45"" ssid = ""21"">The logical forms in DCS are called DCS trees  where nodes are labeled with predicates  and edges are labeled with relations.</S><S sid =""43"" ssid = ""19"">Computation We can compute the denotation JzKw of a DCS tree z by exploiting dynamic programming on trees (Dechter  2003).</S>",['Method_Citation']
13,P11-1060,P14-1008,0,"Liang et al, 2011",0,"are explained in? 2.5. 5http: //nlp.stanford.edu/software/corenlp.shtml 6 In (Liang et al, 2011) DCS trees are learned from QApairs and database entries","In (Liang et al, 2011) DCS trees are learned from QA pairs and database entries","['54', '42', '21', '74', '107']","<S sid =""54"" ssid = ""30"">A DCS tree that contains only join and aggregate relations can be viewed as a collection of treestructured CSPs connected via aggregate relations.</S><S sid =""42"" ssid = ""18"">The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).</S><S sid =""21"" ssid = ""17"">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid =""74"" ssid = ""50"">Extending this notation to denotations  let (hA; αii[i] = hh{ai : a ∈ A}; αiii.</S><S sid =""107"" ssid = ""83"">Our model is arc-factored  so we can sum over all DCS trees in ZL(x) using dynamic programming.</S>",['Results_Citation']
14,P11-1060,P14-1008,0,"Liang et al, 2011",0,"as in the sentence? Tropi cal storm Debby is blamed for death?, which is a tropical storm, is Debby, etc. Technically, each germ in a DCS tree indicates a variable when the DCS tree is translated to a FOL formula, and the abstract denotation of the germ corresponds to the set of consistent values (Liang et al, 2011) of that variable","Technically, each germ in a DCS tree indicates a variable when the DCS tree is translated to a FOL formula, and the abstract denotation of the germ corresponds to the set of consistent values (Liang et al, 2011) of that variable","['59', '148', '47', '103', '62']","<S sid =""59"" ssid = ""35"">The key idea that allows us to give semanticallyscoped denotations to syntactically-scoped trees is as follows: We mark a node low in the tree with a mark relation (one of E  Q  or C).</S><S sid =""148"" ssid = ""33"">This bootstrapping behavior occurs naturally: The “easy” examples are processed first  where easy is defined by the ability of the current model to generate the correct answer using any tree. with scope variation.</S><S sid =""47"" ssid = ""23"">This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure  which will facilitate parsing.</S><S sid =""103"" ssid = ""79"">To define the features  we technically need to augment each tree z ∈ ZL(x) with alignment information—namely  for each predicate in z  the span in x (if any) that triggered it.</S><S sid =""62"" ssid = ""38"">Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes)  as well as information stored about each marked node.</S>",['Method_Citation']
15,P11-1060,D11-1140,0,2011,0,Clarke et al (2010) and Liang et al (2011) replace semantic annotations in the training set with target answers which are more easily available,Clarke et al (2010) and Liang et al (2011) replace semantic annotations in the training set with target answers which are more easily available,"['26', '23', '86', '115', '100']","<S sid =""26"" ssid = ""2"">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid =""23"" ssid = ""19"">We trained our model using an EM-like algorithm (Section 3) on two benchmarks  GEO and JOBS (Section 4).</S><S sid =""86"" ssid = ""62"">Formally  extraction simply moves the i-th column to the front: Xi(d) = d[i  −(i  ø)]{α1 = ø}.</S><S sid =""115"" ssid = ""91"">After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy pθ(T)(y  |x  z ∈ ˜ZL θ(T)).</S><S sid =""100"" ssid = ""76"">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S>",['Method_Citation']
16,P11-1060,D11-1140,0,"Liang et al, 2011",0,"and Collins, 2005, 2007),? -WASP (Wong and Mooney, 2007), UBL (Kwiatkowski et al, 2010) systems and DCS (Liang et al, 2011)","WASP (Wong and Mooney, 2007), UBL (Kwiatkowski et al, 2010) systems and DCS (Liang et al, 2011)","['113', '78', '74', '154', '111']","<S sid =""113"" ssid = ""89"">Formally  let ˜O(θ  θ') be the objective function O(θ) with ZL(x) ˜ZL θI(x).</S><S sid =""78"" ssid = ""54"">The stores are also concatenated (α + α').</S><S sid =""74"" ssid = ""50"">Extending this notation to denotations  let (hA; αii[i] = hh{ai : a ∈ A}; αiii.</S><S sid =""154"" ssid = ""39"">The restriction to present (for example  [in  loc] has high weight). trees is similar to economical DRT (Bos  2009).</S><S sid =""111"" ssid = ""87"">Let ˜ZL θ(x) be this approximation of ZL(x).</S>",['Method_Citation']
17,P11-1060,P13-1007,0,2011,0,"In general, every plural NPpotentially introduces an implicit universal, ranging 1For example, Liang et al (2011) in their state-of-the-art statistical semantic parser within the domain of natural language queries to databases, explicitly devise quantifier scoping in the semantic model","For example, Liang et al (2011) in their state-of-the-art statistical semantic parser within the domain of natural language queries to databases, explicitly devise quantifier scoping in the semantic model","['26', '100', '63', '115', '86']","<S sid =""26"" ssid = ""2"">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid =""100"" ssid = ""76"">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid =""63"" ssid = ""39"">Think of d as consisting of n columns  one for each active node according to a pre-order traversal of z.</S><S sid =""115"" ssid = ""91"">After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy pθ(T)(y  |x  z ∈ ˜ZL θ(T)).</S><S sid =""86"" ssid = ""62"">Formally  extraction simply moves the i-th column to the front: Xi(d) = d[i  −(i  ø)]{α1 = ø}.</S>",['Method_Citation']
18,P11-1060,D11-1022,0,2011,0,"DD-ADMM may be useful in other frameworks involving logical constraints, such as the models for compositional semantics presented by Liang et al (2011)","DD-ADMM may be useful in other frameworks involving logical constraints, such as the models for compositional semantics presented by Liang et al (2011)","['100', '21', '26', '115', '42']","<S sid =""100"" ssid = ""76"">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid =""21"" ssid = ""17"">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid =""26"" ssid = ""2"">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid =""115"" ssid = ""91"">After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy pθ(T)(y  |x  z ∈ ˜ZL θ(T)).</S><S sid =""42"" ssid = ""18"">The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).</S>",['Method_Citation']
19,P11-1060,P12-1051,0,2011,0,"In fact, for any CFG G, it 1See Liang et al (2011) for work in representing lambda calculus expressions with trees","In fact, for any CFG G, it 1See Liang et al (2011) for work in representing lambda calculus expressions with trees","['151', '56', '16', '118', '33']","<S sid =""151"" ssid = ""36"">Inspecting the final parameters calculus formulae.</S><S sid =""56"" ssid = ""32"">The basic version of DCS described thus far handles a core subset of language.</S><S sid =""16"" ssid = ""12"">Which one should we use?</S><S sid =""118"" ssid = ""3"">This evaluation is done with respect to a world w. Recall that a world w maps each predicate p ∈ P to a set of tuples w(p).</S><S sid =""33"" ssid = ""9"">As another example  w(average) = {(S  ¯x) : We write a DCS tree z as hp; r1 : c1; ... ; rm : cmi.</S>","['Aim_Citation', 'Results_Citation']"
