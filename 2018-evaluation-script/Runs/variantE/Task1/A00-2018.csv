Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
2,A00-2018,N10-1002,0,"Charniak, 2000",0,"As a benchmark VPC extraction system, we use the Charniak parser (Charniak, 2000)","As a benchmark VPC extraction system, we use the Charniak parser (Charniak, 2000)","['30', '23', '62', '33', '27']","<S sid =""30"" ssid = ""19"">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid =""23"" ssid = ""12"">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S><S sid =""62"" ssid = ""31"">In Equation 1 we wrote this as p(t I 1  H).</S><S sid =""33"" ssid = ""2"">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S><S sid =""27"" ssid = ""16"">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S>",['Results_Citation']
3,A00-2018,W11-0610,0,"Charniak, 2000",0,"Each of these scores can be calculated from a provided syntactic parse tree, and to generate these we made use of the Charniakparser (Charniak, 2000), also trained on the Switch board tree bank","Each of these scores can be calculated from a provided syntactic parse tree, and to generate these we made use of the Charniak parser (Charniak, 2000), also trained on the Switch board tree bank","['30', '27', '62', '33', '23']","<S sid =""30"" ssid = ""19"">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid =""27"" ssid = ""16"">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid =""62"" ssid = ""31"">In Equation 1 we wrote this as p(t I 1  H).</S><S sid =""33"" ssid = ""2"">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S><S sid =""23"" ssid = ""12"">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S>",['Method_Citation']
4,A00-2018,W06-3119,0,"Charniak, 2000",0,"We then use Charniak? s parser (Charniak, 2000) to generate the most likely parse tree for each English target sentence in the training corpus","We then use Charniak's parser (Charniak, 2000) to generate the most likely parse tree for each English target sentence in the training corpus","['30', '62', '33', '89', '23']","<S sid =""30"" ssid = ""19"">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid =""62"" ssid = ""31"">In Equation 1 we wrote this as p(t I 1  H).</S><S sid =""33"" ssid = ""2"">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S><S sid =""89"" ssid = ""58"">(Actually  we use a minor variant described in [4].)</S><S sid =""23"" ssid = ""12"">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S>",['Method_Citation']
5,A00-2018,N03-2024,0,"Charniak, 2000",0,"We were interested in the occurrence of features such as type and number of premodifiers, presence and type of post modifiers, and form of name reference for people. We constructed a large, automatically annotated corpus by merging the output of Charniak? s statistical parser (Charniak, 2000) with that of the IBM named entity recognition system Nominator (Wacholder et al,1997)","We were interested in the occurrence of features such as type and number of premodifiers, presence and type of post modifiers, and form of name reference for people. We constructed a large, automatically annotated corpus by merging the output of Charniak's statistical parser (Charniak, 2000) with that of the IBM named entity recognition system Nominator (Wacholder et al,1997)","['27', '20', '174', '29', '91']","<S sid =""27"" ssid = ""16"">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid =""20"" ssid = ""9"">In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar  and a right-hand side that is a sequence of one or more such symbols.</S><S sid =""174"" ssid = ""1"">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S sid =""29"" ssid = ""18"">So  for example  in a second-order Markov PCFG  L2 would be conditioned on L1 and M. In our complete model  of course  the probability of each label in the expansions is also conditioned on other material as specified in Equation 1  e.g.  p(e t  h  H).</S><S sid =""91"" ssid = ""2"">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>",['Method_Citation']
6,A00-2018,N06-1039,0,"Charniak, 2000",0,"After getting a set of basic clusters, we pass them to an existing statistical parser (Charniak, 2000) and rule-based tree normalizer to obtain a GLARFstructure for each sentence in every article","After getting a set of basic clusters, we pass them to an existing statistical parser (Charniak, 2000) and rule-based tree normalizer to obtain a GLARF structure for each sentence in every article","['30', '23', '62', '27', '33']","<S sid =""30"" ssid = ""19"">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid =""23"" ssid = ""12"">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S><S sid =""62"" ssid = ""31"">In Equation 1 we wrote this as p(t I 1  H).</S><S sid =""27"" ssid = ""16"">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid =""33"" ssid = ""2"">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S>",['Method_Citation']
7,A00-2018,C04-1180,0,2000,0,"The levels of accuracy and robustness recently achieved by statistical parsers (e.g. Collins (1999), Charniak (2000)) have led to their use in a number of NLP applications, such as question-answering (Pasca and Harabagiu, 2001), machine translation (Charniak et al, 2003), sentence simplification (Carroll et al, 1999), and a linguist? s search engine (Resnik and Elkiss, 2003)","The levels of accuracy and robustness recently achieved by statistical parsers (e.g. Collins (1999), Charniak (2000)) have led to their use in a number of NLP applications, such as question-answering (Pasca and Harabagiu, 2001), machine translation (Charniak et al, 2003), sentence simplification (Carroll et al, 1999), and a linguist? s search engine (Resnik and Elkiss, 2003)","['101', '1', '5', '27', '63']","<S sid =""101"" ssid = ""12"">In keeping with the standard methodology [5  9 10 15 17]  we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training  section 23 for testing  and section 24 for development (debugging and tuning).</S><S sid =""1"" ssid = ""1"">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid =""5"" ssid = ""1"">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S><S sid =""27"" ssid = ""16"">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid =""63"" ssid = ""32"">As we discuss in more detail in Section 5  several different features in the context surrounding c are useful to include in H: the label  head pre-terminal and head of the parent of c (denoted as lp  tp  hp)  the label of c's left sibling (lb for &quot;before&quot;)  and the label of the grandparent of c (la).</S>",['Method_Citation']
8,A00-2018,W05-0638,0,"Charniak, 2000",0,"In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000)","In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000)","['27', '30', '62', '89', '23']","<S sid =""27"" ssid = ""16"">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid =""30"" ssid = ""19"">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid =""62"" ssid = ""31"">In Equation 1 we wrote this as p(t I 1  H).</S><S sid =""89"" ssid = ""58"">(Actually  we use a minor variant described in [4].)</S><S sid =""23"" ssid = ""12"">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S>",['Method_Citation']
9,A00-2018,P05-1065,0,"Charniak, 2000",0,"We also use a standard statistical parser (Charniak, 2000) to provide syntactic analysis","We also use a standard statistical parser (Charniak, 2000) to provide syntactic analysis","['30', '62', '23', '89', '33']","<S sid =""30"" ssid = ""19"">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid =""62"" ssid = ""31"">In Equation 1 we wrote this as p(t I 1  H).</S><S sid =""23"" ssid = ""12"">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S><S sid =""89"" ssid = ""58"">(Actually  we use a minor variant described in [4].)</S><S sid =""33"" ssid = ""2"">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S>",['Method_Citation']
10,A00-2018,P05-1065,0,"Charniak, 2000",0,"For each article, we calculated the per cent age of a) all word instances (tokens) and b) all unique words (types) not on these lists, resulting in three token OOV rate features and three type OOV rate features per article. The parse features are generated using the Charniak parser (Charniak, 2000) trained on the standard Wall Street Journal Treebank corpus","For each article, we calculated the percentage of a) all word instances (tokens) and b) all unique words (types) not on these lists, resulting in three token OOV rate features and three type OOV rate features per article. The parse features are generated using the Charniak parser (Charniak, 2000) trained on the standard Wall Street Journal Treebank corpus","['38', '91', '174', '20', '26']","<S sid =""38"" ssid = ""7"">To compute a probability in a log-linear model one first defines a set of &quot;features&quot;  functions from the space of configurations over which one is trying to compute probabilities to integers that denote the number of times some pattern occurs in the input.</S><S sid =""91"" ssid = ""2"">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S><S sid =""174"" ssid = ""1"">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S sid =""20"" ssid = ""9"">In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar  and a right-hand side that is a sequence of one or more such symbols.</S><S sid =""26"" ssid = ""15"">Thus an expansion e(c) looks like: The expansion is generated by guessing first M  then in order L1 through L „.+1 (= A)  and similarly for RI through In a pure Markov PCFG we are given the left-hand side label 1 and then probabilistically generate the right-hand side conditioning on no information other than 1 and (possibly) previously generated pieces of the right-hand side itself.</S>",['Method_Citation']
11,A00-2018,P04-1040,0,2000,0,"The evaluation of the transformed output of the parsers of Charniak (2000) and Collins (1999) gives 90 %unlabelled and 84 %labelled accuracy with respect to dependencies, when measured against a dependency corpus derived from the Penn Treebank.The paper is organized as follows","The evaluation of the transformed output of the parsers of Charniak (2000) and Collins (1999) gives 90 % unlabelled and 84 % labelled accuracy with respect to dependencies, when measured against a dependency corpus derived from the Penn Treebank. The paper is organized as follows","['29', '27', '174', '91', '33']","<S sid =""29"" ssid = ""18"">So  for example  in a second-order Markov PCFG  L2 would be conditioned on L1 and M. In our complete model  of course  the probability of each label in the expansions is also conditioned on other material as specified in Equation 1  e.g.  p(e t  h  H).</S><S sid =""27"" ssid = ""16"">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid =""174"" ssid = ""1"">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S sid =""91"" ssid = ""2"">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S><S sid =""33"" ssid = ""2"">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S>",['Method_Citation']
12,A00-2018,P04-1040,0,2000,0,"Blaheta and Charniak (2000) presented the first method for assigning Pennfunctional tags to constituents identified by a parser. Pattern-matching approaches were used in (John son, 2002) and (Jijkoun, 2003) to recover non-local dependencies in phrase trees","Blaheta and Charniak (2000) presented the first method for assigning Penn functional tags to constituents identified by a parser. Pattern-matching approaches were used in (Johnson, 2002) and (Jijkoun, 2003) to recover non-local dependencies in phrase trees","['62', '27', '89', '101', '30']","<S sid =""62"" ssid = ""31"">In Equation 1 we wrote this as p(t I 1  H).</S><S sid =""27"" ssid = ""16"">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid =""89"" ssid = ""58"">(Actually  we use a minor variant described in [4].)</S><S sid =""101"" ssid = ""12"">In keeping with the standard methodology [5  9 10 15 17]  we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training  section 23 for testing  and section 24 for development (debugging and tuning).</S><S sid =""30"" ssid = ""19"">Thus we would use p(L2 I L1  M  1  t  h  H).</S>",['Method_Citation']
13,A00-2018,P04-1040,0,2000,0,"As an alternative to hard coded heuristics, Blaheta and Charniak (2000) proposed to recover the Penn functional tags automatically","As an alternative to hard coded heuristics, Blaheta and Charniak (2000) proposed to recover the Penn functional tags automatically","['23', '115', '141', '62', '184']","<S sid =""23"" ssid = ""12"">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S><S sid =""115"" ssid = ""6"">As noted in [5]  that system is based upon a &quot;tree-bank grammar&quot; - a grammar read directly off the training corpus.</S><S sid =""141"" ssid = ""32"">(For example  part-ofspeech tagging using the most probable preterminal for each word is 90% accurate [8].)</S><S sid =""62"" ssid = ""31"">In Equation 1 we wrote this as p(t I 1  H).</S><S sid =""184"" ssid = ""11"">The first is the slight  but important  improvement achieved by using this model over conventional deleted interpolation  as indicated in Figure 2.</S>",['Method_Citation']
17,A00-2018,N06-1022,0,2000,0,"The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents","The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents","['20', '76', '27', '110', '101']","<S sid =""20"" ssid = ""9"">In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar  and a right-hand side that is a sequence of one or more such symbols.</S><S sid =""76"" ssid = ""45"">This requires finding the appropriate Ais for Equation 3  which is accomplished using an algorithm such as iterative scaling [II] in which values for the Ai are initially &quot;guessed&quot; and then modified until they converge on stable values.</S><S sid =""27"" ssid = ""16"">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid =""110"" ssid = ""1"">In the previous sections we have concentrated on the relation of the parser to a maximumentropy approach  the aspect of the parser that is most novel.</S><S sid =""101"" ssid = ""12"">In keeping with the standard methodology [5  9 10 15 17]  we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training  section 23 for testing  and section 24 for development (debugging and tuning).</S>",['Results_Citation']
18,A00-2018,N06-1022,0,2000,0,"Most recently, McDonald et al (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000))","Most recently, McDonald et al (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000))","['74', '126', '125', '174', '27']","<S sid =""74"" ssid = ""43"">Suppose we were  in fact  going to compute a true maximum entropy model based upon the features used in Equation 7  Ii (t 1)  f2(t 1 1p)  f3(t 1 lp) .</S><S sid =""126"" ssid = ""17"">This is indicated in Figure 2  where the model labeled &quot;Best&quot; has precision of 89.8% and recall of 89.6% for an average of 89.7%  0.4% lower than the results on the official test corpus.</S><S sid =""125"" ssid = ""16"">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S><S sid =""174"" ssid = ""1"">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S sid =""27"" ssid = ""16"">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S>",['Method_Citation']
19,A00-2018,H05-1035,0,"Charniak, 2000",0,"The feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (Charniak, 2000), trees that will be improved by more accurate PP-attachment decisions","The feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (Charniak, 2000), trees that will be improved by more accurate PP-attachment decisions","['30', '89', '23', '42', '62']","<S sid =""30"" ssid = ""19"">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid =""89"" ssid = ""58"">(Actually  we use a minor variant described in [4].)</S><S sid =""23"" ssid = ""12"">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S><S sid =""42"" ssid = ""11"">This feature is obviously composed of two sub-features  one recognizing t  the other 1.</S><S sid =""62"" ssid = ""31"">In Equation 1 we wrote this as p(t I 1  H).</S>",['Method_Citation']
20,A00-2018,P04-1042,0,2000,0,"Note that the dependency figures of Dienes lag behind even the parsed results for Johnson? s model; this may well be due to the fact that Dienes built his model as an extension of Collins (1999), which lags behind Charniak (2000) by about 1.3-1.5% .Manual investigation of errors on English gold standard data revealed two major issues that suggest further potential for improvement in performance without further increase in algorithmic complexity or training set size","Note that the dependency figures of Dienes lag behind even the parsed results for Johnson's model; this may well be due to the fact that Dienes built his model as an extension of Collins (1999), which lags behind Charniak (2000) by about 1.3-1.5%. Manual investigation of errors on English gold standard data revealed two major issues that suggest further potential for improvement in performance without further increase in algorithmic complexity or training set size","['176', '58', '180', '44', '13']","<S sid =""176"" ssid = ""3"">That the previous three best parsers on this test [5 9 17] all perform within a percentage point of each other  despite quite different basic mechanisms  led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training  and to conjecture that perhaps we were at it.</S><S sid =""58"" ssid = ""27"">Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.</S><S sid =""180"" ssid = ""7"">From our perspective  perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head.</S><S sid =""44"" ssid = ""13"">Now consider computing a conditional probability p(a H) with a set of features h that connect a to the history H. In a log-linear model the probability function takes the following form: Here the Ai are weights between negative and positive infinity that indicate the relative importance of a feature: the more relevant the feature to the value of the probability  the higher the absolute value of the associated A.</S><S sid =""13"" ssid = ""2"">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question.</S>",['Method_Citation']
