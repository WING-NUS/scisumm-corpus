Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P08-1028,D08-1094,0,2008,0,"Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p+ a as a function f operating on four components: c= f (p, a, R, K) (3) R is the relation holding between p and a, and Kadditional knowledge","Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p+ a as a function f operating on four components: c= f (p, a, R, K) (3) R is the relation holding between p and a, and K additional knowledge","['194', '140', '51', '40', '155']","<S sid =""194"" ssid = ""6"">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid =""140"" ssid = ""53"">Following previous work (Bullinaria and Levy  2007)  we optimized its parameters on a word-based semantic similarity task.</S><S sid =""51"" ssid = ""24"">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).</S><S sid =""40"" ssid = ""13"">Noisy versions of the original vectors can be recovered by means of circular correlation which is the approximate inverse of circular convolution.</S><S sid =""155"" ssid = ""68"">Secondly  we optimized the weightings in the combined model (11) with a similar grid search over its three parameters.</S>",['Results_Citation']
4,P08-1028,P14-1060,0,2008,0,"While works such asthe SDSM model suffer from the problem of sparsity in composing structures beyond bi grams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstetteand Sadrzadeh (2011) are restricted by significant model biases in representing semantic com position by generic algebraic operations","While works such as the SDSM model suffer from the problem of sparsity in composing structures beyond bigrams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstette and Sadrzadeh (2011) are restricted by significant model biases in representing semantic com position by generic algebraic operations","['194', '58', '59', '10', '51']","<S sid =""194"" ssid = ""6"">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid =""58"" ssid = ""6"">We define a general class of models for this process of composition as: The expression above allows us to derive models for which p is constructed in a distinct space from u and v  as is the case for tensor products.</S><S sid =""59"" ssid = ""7"">It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence  via the argument R  on syntax.</S><S sid =""10"" ssid = ""6"">Moreover  the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald  2000) and word association norms (Denhire and Lemaire  2004).</S><S sid =""51"" ssid = ""24"">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).</S>",['Method_Citation']
6,P08-1028,P10-1097,0,2008,0,"Mitchell and Lapata (2008), henceforth M& amp; L, propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression","Mitchell and Lapata (2008), henceforth M& amp; L, propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression","['108', '194', '38', '141', '36']","<S sid =""108"" ssid = ""21"">Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).</S><S sid =""194"" ssid = ""6"">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid =""38"" ssid = ""11"">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S><S sid =""141"" ssid = ""54"">The task involves examining the degree of linear relationship between the human judgments for two individual words and vector-based similarity values.</S><S sid =""36"" ssid = ""9"">To overcome this problem  other techniques have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components.</S>",['Method_Citation']
7,P08-1028,P10-1097,0,2008,0,"Interestingly, Mitchell and Lapata (2008) came to the same result in a different setting","Interestingly, Mitchell and Lapata (2008) came to the same result in a different setting","['160', '156', '34', '190', '117']","<S sid =""160"" ssid = ""73"">Specifically  m was set to 20 and m to 1.</S><S sid =""156"" ssid = ""69"">This yielded a weighted sum consisting of 95% verb  0% noun and 5% of their multiplicative combination.</S><S sid =""34"" ssid = ""7"">Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.</S><S sid =""190"" ssid = ""2"">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S><S sid =""117"" ssid = ""30"">This yielded a reduced set of experimental items (120 in total) consisting of 15 reference verbs  each with 4 nouns  and 2 landmarks.</S>",['Method_Citation']
8,P08-1028,D11-1094,0,2008,0,"And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to combine the constituent vectors","And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to combine the constituent vectors","['0', '36', '51', '142', '194']","<S sid =""0"" ssid = ""0"">Vector-based Models of Semantic Composition</S><S sid =""36"" ssid = ""9"">To overcome this problem  other techniques have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components.</S><S sid =""51"" ssid = ""24"">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).</S><S sid =""142"" ssid = ""55"">We experimented with a variety of dimensions (ranging from 50 to 500 000)  vector component definitions (e.g.  pointwise mutual information or log likelihood ratio) and similarity measures (e.g.  cosine or confusion probability).</S><S sid =""194"" ssid = ""6"">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S>",['Method_Citation']
9,P08-1028,W11-0131,0,2008,0,"Mitchell and Lapata (2008) provide a general framework for semantic vector composition: p= f (u, v, R, K) (1) 295 where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base, and p is a resulting composed vector (or tensor)","Mitchell and Lapata (2008) provide a general framework for semantic vector composition: p= f (u, v, R, K) (1) 295 where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base, and p is a resulting composed vector (or tensor)","['194', '59', '51', '58', '201']","<S sid =""194"" ssid = ""6"">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid =""59"" ssid = ""7"">It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence  via the argument R  on syntax.</S><S sid =""51"" ssid = ""24"">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).</S><S sid =""58"" ssid = ""6"">We define a general class of models for this process of composition as: The expression above allows us to derive models for which p is constructed in a distinct space from u and v  as is the case for tensor products.</S><S sid =""201"" ssid = ""13"">We intend to assess the potential of our composition models on context sensitive semantic priming (Till et al.  1988) and inductive inference (Heit and Rubinstein  1994).</S>",['Method_Citation']
10,P08-1028,W11-0131,0,2008,0,"As Mitchell and Lapata (2008) did, let us temporarily suspend discussion on what semantics populate our vectors for now","As Mitchell and Lapata (2008) did, let us temporarily suspend discussion on what semantics populate our vectors for now","['30', '96', '1', '24', '44']","<S sid =""30"" ssid = ""3"">For the hierarchical structure of natural language this binding problem becomes particularly acute.</S><S sid =""96"" ssid = ""9"">Any adequate model of composition must be able to represent argument-verb meaning.</S><S sid =""1"" ssid = ""1"">This paper proposes a framework for representing the meaning of phrases and sentences in vector space.</S><S sid =""24"" ssid = ""20"">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S><S sid =""44"" ssid = ""17"">For example  assuming that individual words are represented by vectors  we can compute the meaning of a sentence by taking their mean (Foltz et al.  1998; Landauer and Dumais  1997).</S>",['Method_Citation']
11,P08-1028,P13-2083,0,2008,0,"Mitchell and Lapata (2008) propose a framework to define the composition c= f (a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition","Mitchell and Lapata (2008) propose a framework to define the composition c= f (a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition","['194', '3', '101', '37', '141']","<S sid =""194"" ssid = ""6"">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid =""3"" ssid = ""3"">Under this framework  we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.</S><S sid =""101"" ssid = ""14"">Materials and Design Our materials consisted of sentences with an an intransitive verb and its subject.</S><S sid =""37"" ssid = ""10"">Holographic reduced representations (Plate  1991) are one implementation of this idea where the tensor product is projected back onto the space of its components.</S><S sid =""141"" ssid = ""54"">The task involves examining the degree of linear relationship between the human judgments for two individual words and vector-based similarity values.</S>",['Method_Citation']
12,P08-1028,P13-2083,0,"Mitchell and Lapata, 2008",0,"As our final set of baselines, we extend two simple techniques proposed by (Mitchell and Lapata, 2008) that use element-wise addition and multiplication operators to perform composition","As our final set of baselines, we extend two simple techniques proposed by (Mitchell and Lapata, 2008) that use element-wise addition and multiplication operators to perform composition","['39', '140', '144', '194', '37']","<S sid =""39"" ssid = ""12"">The compression is achieved by summing along the transdiagonal elements of the tensor product.</S><S sid =""140"" ssid = ""53"">Following previous work (Bullinaria and Levy  2007)  we optimized its parameters on a word-based semantic similarity task.</S><S sid =""144"" ssid = ""57"">We obtained best results with a model using a context window of five words on either side of the target word  the cosine measure  and 2 000 vector components.</S><S sid =""194"" ssid = ""6"">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid =""37"" ssid = ""10"">Holographic reduced representations (Plate  1991) are one implementation of this idea where the tensor product is projected back onto the space of its components.</S>",['Method_Citation']
13,P08-1028,P10-1021,0,2008,0,"Althoughthis model has been shown to successfully simulate single and multiple-word priming (McDonald and Brew 2004), it failed to predict processing costs in the Embra eye-tracking corpus (McDonald and Shillcock 2003) .In this work we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008)","Although this model has been shown to successfully simulate single and multiple-word priming (McDonald and Brew 2004), it failed to predict processing costs in the Embra eye-tracking corpus (McDonald and Shillcock 2003). In this work we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008)","['108', '194', '168', '35', '144']","<S sid =""108"" ssid = ""21"">Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).</S><S sid =""194"" ssid = ""6"">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid =""168"" ssid = ""2"">These included three additive models  i.e.  simple addition (equation (5)  Add)  weighted addition (equation (7)  WeightAdd)  and Kintsch’s (2001) model (equation (10)  Kintsch)  a multiplicative model (equation (6)  Multiply)  and also a model which combines multiplication with addition (equation (11)  Combined).</S><S sid =""35"" ssid = ""8"">The tensor product u ® v is a matrix whose components are all the possible products uivj of the components of vectors u and v. A major difficulty with tensor products is their dimensionality which is higher than the dimensionality of the original vectors (precisely  the tensor product has dimensionality m x n).</S><S sid =""144"" ssid = ""57"">We obtained best results with a model using a context window of five words on either side of the target word  the cosine measure  and 2 000 vector components.</S>",['Method_Citation']
14,P08-1028,P10-1021,0,2008,0,"Assuming that h is a linear function of the Cartesian product of u and v allows to specify additive models which are by far the most common method of vector combination in the literature: hi =ui+ vi (3) Alternatively, we can assume that h is a linear function of the tensor product of u and v, and thus derive models based on multiplication: hi =ui? vi (4) Mitchell and Lapata (2008) show that several additive and multiplicative models can be formulated under this framework, including the well known tensor products (Smolensky 1990) and circular convolution (Plate 1995)","Mitchell and Lapata (2008) show that several additive and multiplicative models can be formulated under this framework, including the well known tensor products (Smolensky 1990) and circular convolution (Plate 1995)","['39', '117', '35', '194', '108']","<S sid =""39"" ssid = ""12"">The compression is achieved by summing along the transdiagonal elements of the tensor product.</S><S sid =""117"" ssid = ""30"">This yielded a reduced set of experimental items (120 in total) consisting of 15 reference verbs  each with 4 nouns  and 2 landmarks.</S><S sid =""35"" ssid = ""8"">The tensor product u ® v is a matrix whose components are all the possible products uivj of the components of vectors u and v. A major difficulty with tensor products is their dimensionality which is higher than the dimensionality of the original vectors (precisely  the tensor product has dimensionality m x n).</S><S sid =""194"" ssid = ""6"">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid =""108"" ssid = ""21"">Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).</S>",['Method_Citation']
15,P08-1028,W11-0115,0,2008,0,"Mitchell and Lapata (2008) introduce a whole family of models of compositionality based on vector addition and point wise-multiplication (and a weighted combination of both), evaluated on a sentence similarity task inspired by Kintsch (2001)","Mitchell and Lapata (2008) introduce a whole family of models of compositionality based on vector addition and point wise-multiplication (and a weighted combination of both), evaluated on a sentence similarity task inspired by Kintsch (2001)","['108', '101', '3', '153', '194']","<S sid =""108"" ssid = ""21"">Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).</S><S sid =""101"" ssid = ""14"">Materials and Design Our materials consisted of sentences with an an intransitive verb and its subject.</S><S sid =""3"" ssid = ""3"">Under this framework  we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.</S><S sid =""153"" ssid = ""66"">Specifically  we considered eleven models  varying in their weightings  in steps of 10%  from 100% noun through 50% of both verb and noun to 100% verb.</S><S sid =""194"" ssid = ""6"">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S>",['Method_Citation']
16,P08-1028,W11-0115,0,2008,0,"The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression","The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression","['148', '164', '177', '4', '127']","<S sid =""148"" ssid = ""61"">In addition  Bullinaria and Levy (2007) found that these parameters perform well on a number of other tasks such as the synonymy task from the Test ofEnglish as a Foreign Language (TOEFL).</S><S sid =""164"" ssid = ""77"">A more scrupulous evaluation requires directly correlating all the individual participants’ similarity judgments with those of the models.6 We used Spearman’s p for our correlation analyses.</S><S sid =""177"" ssid = ""11"">The difference between High and Low similarity values estimated by these models are statistically significant (p < 0.01 using the Wilcoxon rank sum test).</S><S sid =""4"" ssid = ""4"">Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.</S><S sid =""127"" ssid = ""40"">Analysis of Similarity Ratings The reliability of the collected judgments is important for our evaluation experiments; we therefore performed several tests to validate the quality of the ratings.</S>",['Results_Citation']
17,P08-1028,W11-0115,0,2008,0,"For example, Mitchell and Lapata (2008) use their models to approximate the human ratings in their sentence similarity dataset","For example, Mitchell and Lapata (2008) use their models to approximate the human ratings in their sentence similarity dataset","['115', '131', '147', '173', '164']","<S sid =""115"" ssid = ""28"">Each cell recorded the number of times our subjects selected the landmark as compatible with the noun or not.</S><S sid =""131"" ssid = ""44"">A Wilcoxon rank sum test confirmed that the difference is statistically significant (p < 0.01).</S><S sid =""147"" ssid = ""60"">This configuration gave high correlations with the WordSim353 similarity judgments using the cosine measure.</S><S sid =""173"" ssid = ""7"">Model similarities have been estimated using cosine which ranges from 0 to 1  whereas our subjects rated the sentences on a scale from 1 to 7.</S><S sid =""164"" ssid = ""77"">A more scrupulous evaluation requires directly correlating all the individual participants’ similarity judgments with those of the models.6 We used Spearman’s p for our correlation analyses.</S>",['Method_Citation']
18,P08-1028,W11-1310,0,2008,0,We use other WSM settings following Mitchell and Lapata (2008),We use other WSM settings following Mitchell and Lapata (2008),"['140', '3', '101', '2', '139']","<S sid =""140"" ssid = ""53"">Following previous work (Bullinaria and Levy  2007)  we optimized its parameters on a word-based semantic similarity task.</S><S sid =""3"" ssid = ""3"">Under this framework  we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.</S><S sid =""101"" ssid = ""14"">Materials and Design Our materials consisted of sentences with an an intransitive verb and its subject.</S><S sid =""2"" ssid = ""2"">Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions.</S><S sid =""139"" ssid = ""52"">The semantic space we used in our experiments was built on a lemmatised version of the BNC.</S>",['Method_Citation']
19,P08-1028,W11-1310,0,2008,0,Mitchell and Lapata (2008) observed that a simple multiplication function modelled compositionality better than addition,Mitchell and Lapata (2008) observed that a simple multiplication function modelled compositionality better than addition,"['115', '147', '39', '173', '164']","<S sid =""115"" ssid = ""28"">Each cell recorded the number of times our subjects selected the landmark as compatible with the noun or not.</S><S sid =""147"" ssid = ""60"">This configuration gave high correlations with the WordSim353 similarity judgments using the cosine measure.</S><S sid =""39"" ssid = ""12"">The compression is achieved by summing along the transdiagonal elements of the tensor product.</S><S sid =""173"" ssid = ""7"">Model similarities have been estimated using cosine which ranges from 0 to 1  whereas our subjects rated the sentences on a scale from 1 to 7.</S><S sid =""164"" ssid = ""77"">A more scrupulous evaluation requires directly correlating all the individual participants’ similarity judgments with those of the models.6 We used Spearman’s p for our correlation analyses.</S>",['Method_Citation']
20,P08-1028,W11-1310,0,"Mitchell and Lapata, 2008",0,"We use the compositionality functions, simple addition and simple multiplication to build compositional vectors Vwr1+wr2 and Vwr1 ?wr2. These are as described in (Mitchell and Lapata, 2008)","We use the compositionality functions, simple addition and simple multiplication to build compositional vectors Vwr1+wr2 and Vwr1 ?wr2. These are as described in (Mitchell and Lapata, 2008)","['38', '153', '194', '183', '40']","<S sid =""38"" ssid = ""11"">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S><S sid =""153"" ssid = ""66"">Specifically  we considered eleven models  varying in their weightings  in steps of 10%  from 100% noun through 50% of both verb and noun to 100% verb.</S><S sid =""194"" ssid = ""6"">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid =""183"" ssid = ""17"">The weighted additive model (p = 0.09) is not significantly different from the baseline either or Kintsch (2001) (p = 0.09).</S><S sid =""40"" ssid = ""13"">Noisy versions of the original vectors can be recovered by means of circular correlation which is the approximate inverse of circular convolution.</S>",['Method_Citation']
