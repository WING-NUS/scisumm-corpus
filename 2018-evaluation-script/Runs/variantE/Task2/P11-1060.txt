We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.5 Discussion Piantadosi et al. (2008) induces first-order formuA major focus of this work is on our semantic rep- lae using CCG in a small domain assuming observed resentation  DCS  which offers a new perspective lexical semantics.The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).We trained our model using an EM-like algorithm (Section 3) on two benchmarks  GEO and JOBS (Section 4).Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.Such a z defines a constraint satisfaction problem (CSP) with nodes as variables.The basic version of DCS described thus far handles a core subset of language.On two stansemantic parsing benchmarks our system obtains the highest published accuracies  despite requiring no annotated logical forms.In fact  DCS performs comparably to even the version of SEMRESP trained using logical forms.It suffices to define Xi(d) for a single column i.Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy pθ(T)(y  |x  z ∈ ˜ZL θ(T)).Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.This bootstrapping behavior occurs naturally: The “easy” examples are processed first  where easy is defined by the ability of the current model to generate the correct answer using any tree. with scope variation.Next  we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).Having instantiated s as a value  everything above this node is an ordinary CSP: s constrains the count node  which in turns constrains the root node to |s|.CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).We now turn to the task of mapping natural language For the example in Figure 4(b)  the de- utterances to DCS trees.In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).For example  in Figure 4(a)  before execution  the denotation of the DCS tree is hh{[(CA  OR)  (OR)] ... }; ø; (E  Qhstatei�w  ø)ii; after applying X1  we have hh{[(OR)]  ... }; øii.Formally  let ˜O(θ  θ') be the objective function O(θ) with ZL(x) ˜ZL θI(x).At the same time  representations such as FunQL (Kate et al.  2005)  which was used in Clarke et al. (2010)  are simpler but lack the full expressive power of lambda calculus.The restriction to present (for example  [in  loc] has high weight). trees is similar to economical DRT (Bos  2009).Extending this notation to denotations  let (hA; αii[i] = hh{ai : a ∈ A}; αiii.In each dataset  each sentence x is annotated with a Prolog logical form  which we use only to evaluate and get an answer y.Compositional question answering begins by mapping questions to logical forms  but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes)  as well as information stored about each marked node.The combination rules are encoded in the tems  despite using no annotated logical forms.Formally  pθ(z  |x) ∝ eφ(x z)Tθ  where θ and φ(x  z) are parameter and feature vectors  respectively.To further reduce the search space  F imposes a few additional constraints  e.g.  limiting the number of marked nodes to 2 and only allowing trace predicates between arity 1 predicates.We also define an augmented lexicon L+ which includes a prototype word x for each predicate appearing in (iii) above (e.g.  (large  size))  which cancels the predicates triggered by x’s POS tag.A DCS tree that contains only join and aggregate relations can be viewed as a collection of treestructured CSPs connected via aggregate relations.Our employed (Zettlemoyer and Collins  2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language—think grounded al.  2010). compositional semantics.The key idea that allows us to give semanticallyscoped denotations to syntactically-scoped trees is as follows: We mark a node low in the tree with a mark relation (one of E  Q  or C).The logical forms in DCS are called DCS trees  where nodes are labeled with predicates  and edges are labeled with relations.Computation We can compute the denotation JzKw of a DCS tree z by exploiting dynamic programming on trees (Dechter  2003).Our model is arc-factored  so we can sum over all DCS trees in ZL(x) using dynamic programming.This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure  which will facilitate parsing.To define the features  we technically need to augment each tree z ∈ ZL(x) with alignment information—namely  for each predicate in z  the span in x (if any) that triggered it.Formally  extraction simply moves the i-th column to the front: Xi(d) = d[i  −(i  ø)]{α1 = ø}.The stores are also concatenated (α + α').Let ˜ZL θ(x) be this approximation of ZL(x).Think of d as consisting of n columns  one for each active node according to a pre-order traversal of z.Inspecting the final parameters calculus formulae.Which one should we use?This evaluation is done with respect to a world w. Recall that a world w maps each predicate p ∈ P to a set of tuples w(p).As another example  w(average) = {(S  ¯x) : We write a DCS tree z as hp; r1 : c1; ... ; rm : cmi.