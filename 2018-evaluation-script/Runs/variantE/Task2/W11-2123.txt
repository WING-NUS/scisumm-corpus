KenLM: Faster and Smaller Language Model QueriesOur test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.Hash tables are a common sparse mapping technique used by SRILM’s default and BerkeleyLM’s hashed variant.The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI’s speed reported in Guthrie and Hepple (2010) under different conditions.RandLM and SRILM also remove context that will not extend  but SRILM performs a second lookup in its trie whereas our approach has minimal additional cost.It also uses less memory  with 8 bytes of overhead per entry (we store 16-byte entries with m = 1.5); linked list implementations hash set and unordered require at least 8 bytes per entry for pointers.If the key distribution’s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |− 1] to estimate pivots; this optimization alone led to a 24% speed improvement.Language models that contain wi must also contain prefixes wi for 1 G i G k. Therefore  when the model is queried for p(wnjwn−1 1 ) but the longest matching suffix is wnf   it may return state s(wn1) = wnf since no longer context will be found.Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.Though we are not able to calculate their memory usage on our model  results reported in their paper suggest lower memory consumption than TRIE on large-scale models  at the expense of CPU time.Given counts cn1 where e.g. c1 is the vocabulary size  total memory consumption  in bits  is Our PROBING data structure places all n-grams of the same order into a single giant hash table.The state function is integrated into the query process so that  in lieu of the query p(wnjwn−1 1 )  the application issues query p(wnjs(wn−1 1 )) which also returns s(wn1 ).Each visited entry wni stores backoff b(wni ).The authors provided us with a ratio between TPT and SRI under different conditions. aLossy compression with the same weights. bLossy compression with retuned weights. ditions make the value appropriate for estimating repeated run times  such as in parameter tuning.The ratio of buckets to entries is controlled by space multiplier m > 1.By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.Nodes in the trie are based on arrays sorted by vocabulary identifier.MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.Lossy compressed models RandLM (Talbot and Osborne  2007) and Sheffield (Guthrie and Hepple  2010) offer better memory consumption at the expense of CPU and accuracy.The first value reports use immediately after loading while the second reports the increase during scoring. dBerkeleyLM is written in Java which requires memory be specified in advance.Performance improvements transfer to the Moses (Koehn et al.  2007)  cdec (Dyer et al.  2010)  and Joshua (Li et al.  2009) translation systems where our code has been integrated.IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.Both implementations employ a state object  opaque to the application  that carries information from one query to the next; we discuss both further in Section 4.2.The hash variant is a reverse trie with hash tables  a more memory-efficient version of SRILM’s default.Sorted arrays store key-value pairs in an array sorted by key  incurring no space overhead.Interpolation search has a more expensive pivot but performs less pivoting and reads  so it is slow on small data and faster on large data.Unlike Germann et al. (2009)  we chose a model size so that all benchmarks fit comfortably in main memory.The PROBING model was designed to improve upon SRILM by using linear probing hash tables (though not arranged in a trie)  allocating memory all at once (eliminating the need for full pointers)  and being easy to compile.We do not experiment with models larger than physical memory in this paper because TPT is unreleased  factors such as disk speed are hard to replicate  and in such situations we recommend switching to a more compact representation  such as RandLM.The structure uses linear probing hash tables and is designed for speed.In addition to the optimizations specific to each datastructure described in Section 2  we implement several general optimizations for language modeling.For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.We evaluate the time and memory consumption of each data structure by computing perplexity on 4 billion tokens from the English Gigaword corpus (Parker et al.  2009).This section measures performance on shared tasks in order of increasing complexity: sparse lookups  evaluating perplexity of a large file  and translation with Moses.While sorted arrays could be used to implement the same data structure as PROBING  effectively making m = 1  we abandoned this implementation because it is slower and larger than a trie implementation.The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.