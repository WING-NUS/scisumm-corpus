Line 6 enumerates all POS’s for the word w spanning length l and ending at position i.2 Segmentation and POS Tagging Given a Chinese character sequence: while the segmentation and POS tagging result can be depicted as: Here  Ci (i = L.n) denotes Chinese character  ti (i = L.m) denotes POS tag  and Cl:r (l < r) denotes character sequence ranges from Cl to Cr.Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.With precision P and recall R  the balance F-measure is defined as: F = 2PR/(P + R).According to the usual practice in syntactic analysis  we choose chapters 1 − 260 (18074 sentences) as training set  chapter 271 − 300 (348 sentences) as test set and chapter 301 − 325 (350 sentences) as development set.Formally  an n-gram word LM approximates the probability of a word sequence W = w1:m with the following product: Notice that a bi-gram POS LM functions as the product of transition probabilities in HMM.For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.We used the algorithm depicted in Algorithm 1 to tune the parameter vector a.We trained a character-based perceptron for Chinese Joint S&T  and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?It is an important measure of fluency of the translation in SMT.Then we trained LEX on each of the four corpora for 7 iterations.It has comparable performance to CRFs  while with much faster training.On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).Another important feature is the labelling model.The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech TaggingTo alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.We find that Joint S&T can also improve the segmentation accuracy.This work was done while L. H. was visiting CAS/ICT.In this layer  each knowledge source is treated as a feature with a corresponding weight denoting its relative importance.We reported results from two set of experiments.The evaluation results are shown in Table 3.With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.We noticed that the templates related to word unigrams and bigrams bring to the feature space an enlargement much rapider than the character-base ones  not to mention the higher-order grams such as trigrams or 4-grams.In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result  a 4-gram POS language model functioning as the product of statetransition probabilities in HMM  and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.In addition  all knowledge sources we used in the core perceptron and the outside-layer linear model come from the training corpus  whereas many open knowledge sources (lexicon etc.) can be used to improve performance (Ng and Low  2004).Test results listed in Table 2 shows that this model obtains higher accuracy than the best of SIGHAN Bakeoff 2 in three corpora (AS  CityU and MSR).If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?Among other features  the 4-gram POS LM plays the most important role  removing this feature causes F-measure decrement of 0.33 points on segmentation and 0.71 points on Joint S&T.However  such features are generated dynamically during the decoding procedure so that the feature space enlarges much more rapidly.Under this model  many knowledge sources that may be intractable to be incorporated into the perceptron directly  can be utilized effectively in the outside-layer linear model.By maintaining a stack of size N at each position i of the sequence  we can preserve the top N best candidate labelled results of subsequence C1:i during decoding.For instance  if the word w appears N times in training corpus and is labelled as POS t for n times  the probability Pr(t|w) can be estimated by the formula below: The probability Pr(w|t) could be estimated through the same approach.We found that LEX outperforms NON-LEX with a margin of about 0.002 at each iteration  and its learning curve reaches a tableland at iteration 7.C represents a Chinese character while the subscript of C indicates its position in the sentence relative to the current character (it has the subscript 0).Given a training corpus with POS tags  we can train a word-POS co-occurrence model to approximate the probability that the word sequence of the labelled result co-exists with its corresponding POS sequence.We turned to experiments on CTB 5.0 to test the performance of the cascaded model.The first was conducted to test the performance of the perceptron on segmentation on the corpus from SIGHAN Bakeoff 2  including the Academia Sinica Corpus (AS)  the Hong Kong City University Corpus (CityU)  the Peking University Corpus (PKU) and the Microsoft Research Corpus (MSR).We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.