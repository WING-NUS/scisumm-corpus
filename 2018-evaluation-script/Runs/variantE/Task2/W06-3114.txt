Again  we can compute average scores for all systems for the different language pairs (Figure 6).The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.We collected around 300–400 judgements per judgement type (adequacy or fluency)  per system  per language pair.The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).Manual and Automatic Evaluation of Machine Translation between European Languages(b) does the translation have the same meaning  including connotations?The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).Pairwise comparison: We can use the same method to assess the statistical significance of one system outperforming another.The out-of-domain test set differs from the Europarl data in various ways.Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.All systems (except for Systran  which was not tuned to Europarl) did considerably worse on outof-domain training data.For the automatic evaluation  we used BLEU  since it is the most established metric in the field.The confidence intervals are computed by bootstrap resampling for BLEU  and by standard significance testing for the manual scores  as described earlier in the paper.The scores and confidence intervals are detailed first in the Figures 7–10 in table form (including ranks)  and then in graphical form in Figures 11–16.Training and testing is based on the Europarl corpus.We computed BLEU scores for each submission with a single reference translation.For instance  for out-ofdomain English-French  Systran has the best BLEU and manual scores.We received submissions from 14 groups from 11 institutions  as listed in Figure 2.We asked participants to each judge 200–300 sentences in terms of fluency and adequacy  the most commonly used manual evaluation metrics.For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.English was again paired with German  French  and Spanish.For some language pairs (such as GermanEnglish) system performance is more divergent than for others (such as English-French)  at least as measured by BLEU.