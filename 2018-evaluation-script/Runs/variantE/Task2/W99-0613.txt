The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).Unsupervised Models for Named Entity Classification CollinsAlternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.(Blum and Mitchell 98) describe learning in the following situation: X = X1 X X2 where X1 and X2 correspond to two different &quot;views&quot; of an example.The NP is a complement to a preposition  which is the head of a PP.We again assume a training set of n examples {x1 . xri} where the first m examples have labels {y1 ... yin}  and the last (n — m) examples are unlabeled.Several extensions of AdaBoost for multiclass problems have been suggested (Freund and Schapire 97; Schapire and Singer 98).AdaBoost was first introduced in (Freund and Schapire 97); (Schapire and Singer 98) gave a generalization of AdaBoost which we will use in this paper.2 We now introduce a new algorithm for learning from unlabeled examples  which we will call DLCoTrain (DL stands for decision list  the term Cotrain is taken from (Blum and Mitchell 98)).Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.Pseudo-labels are formed by taking seed labels on the labeled examples  and the output of the fixed classifier on the unlabeled examples.With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.The first m pairs have labels yi  whereas for i = m + 1    n the pairs are unlabeled.For each label (Per s on  organization and Location)  take the n contextual rules with the highest value of Count' (x) whose unsmoothed3 strength is above some threshold pmin.Thus at each iteration the method induces at most n x k rules  where k is the number of possible labels (k = 3 in the experiments in this paper). step 3.In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.The task is to learn a function from an input string (proper name) to its type  which we will assume to be one of the categories Person  Organization  or Location.The CoBoost algorithm just described is for the case where there are two labels: for the named entity task there are three labels  and in general it will be useful to generalize the CoBoost algorithm to the multiclass case.In particular  it may not be possible to learn functions fi (x f2(x2 t) for i = m + 1...n: either because there is some noise in the data  or because it is just not realistic to expect to learn perfect classifiers given the features used for representation.They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page  and other pages pointing to the page).It is a sequence of proper nouns within an NP; its last word Cooper is the head of the NP; and the NP has an appositive modifier (a vice president at S.&P.) whose head is a singular noun (president).The unlabeled data gives many such &quot;hints&quot; that two features should predict the same label  and these hints turn out to be surprisingly useful when building a classifier.Zt can be written as follows Following the derivation of Schapire and Singer  providing that W+ > W_  Equ.The approach uses both spelling and contextual rules.Pseudo-code describing the generalized boosting algorithm of Schapire and Singer is given in Figure 1.In this case nonalpha is the string formed by removing all upper/lower case letters from the spelling (e.g.  for Thomas E. Petry nonalpha= .971 746 sentences of New York Times text were parsed using the parser of (Collins 96).1 Word sequences that met the following criteria were then extracted as named entity examples: whose head is a singular noun (tagged NN).Of these cases  38 were temporal expressions (either a day of the week or month of the year).To measure the contribution of each modification  a third  intermediate algorithm  Yarowsky-cautious was also tested.The test accuracy more or less asymptotes.Note that in our formalism a weakhypothesis can abstain.