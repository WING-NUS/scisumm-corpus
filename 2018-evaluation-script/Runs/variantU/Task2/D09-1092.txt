A recent extended abstract  developed concurrently by Ni et al. (Ni et al.  2009)  discusses a multilingual topic model similar to the one presented here.We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).We use the “left-to-right” method of (Wallach et al.  2009).Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.English and the Romance languages use only singular and plural versions of “objective.” The other Germanic languages include compound words  while Greek and Finnish are dominated by inflected variants of the same lexical item.We therefore evaluate the ability of the PLTM to generate bilingual lexica  similar to other work in unsupervised translation modeling (Haghighi et al.  2008).Tam  Lane and Schultz (Tam et al.  2007) also show improvements in machine translation using bilingual topic models.Topic models have been used for analyzing topic trends in research literature (Mann et al.  2006; Hall et al.  2008)  inferring captions for images (Blei and Jordan  2003)  social network analysis in email (McCallum et al.  2005)  and expanding queries with topically related words in information retrieval (Wei and Croft  2006).Although the PLTM is clearly not a substitute for a machine translation system—it has no way to represent syntax or even multi-word phrases—it is clear from the examples in figure 2 that the sets of high probability words in different languages for a given topic are likely to include translations.For every topic t we select a small number K of the most probable words in English (e) and in each “translation” language (E): Wte and Wtt  respectively.We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.The second corpus  Wikipedia articles in twelve languages  contains sets of documents that are not translations of one another  but are very likely to be about similar concepts.We use this corpus to explore the ability of the model both to infer similarities between vocabularies in different languages  and to detect differences in topic emphasis between languages.We argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages.To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.We also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in non-comparable corpora.For each document in the query language we rank all documents in the target language and record the rank of the actual translation.For each tuple we can then calculate the JensenShannon divergence (the average of the KL divergences between each distribution and a mean distribution) between these distributions.Restricting the query/target pairs to only those with query and target documents that are both longer than 50 words results in significant improvement and reduced variance: the average proportion of query documents for which the true translation is ranked highest goes from 53.9% to 72.7%.From the results in figure 4  we know that leaving all document tuples intact should result in a mean JS divergence of less than 0.1.We calculate the Jensen-Shannon divergence between the topic distributions for each pair of individual documents in S that were originally part of the same tuple prior to separation.As the number of topics increases  greater variability in topic distributions causes divergence to increase.An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct  non-comparable documents in multiple languages.The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.We then use this model to infer topic distributions for each of the 11 documents in each of the held-out document tuples using a method similar to that used to calculate held-out probabilities (Wallach et al.  2009).Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).We take a simpler approach that is more suitable for topically similar document tuples (where documents are not direct translations of one another) in more than two languages.However  they evaluate their model on only two languages (English and Chinese)  and do not use the model to detect differences between languages.We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.Finally  for each pair of languages (“query” and “target”) we calculate the difference between the topic distribution for each held-out document in the query language and the topic distribution for each held-out document in the target language.We take the latter approach  and use the MAP estimate for αm and the predictive distributions over words for Φ1  .We use both Jensen-Shannon divergence and cosine distance.As with EuroParl  we can calculate the JensenShannon divergence between pairs of documents within a comparable document tuple.Given a corpus of training and test document tuples—W and W'  respectively—two possible inference tasks of interest are: computing the probability of the test tuples given the training tuples and inferring latent topic assignments for test documents.Topic models are a useful tool for analyzing large text collections  but have previously been applied in only monolingual  or at most bilingual  contexts.First  we explore whether comparable document tuples support the alignment of fine-grained topics  as demonstrated earlier using parallel documents.When applied to comparable document collections such as Wikipedia  PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.By linking topics across languages  polylingual topic models can increase cross-cultural understanding by providing readers with the ability to characterize the contents of collections in unfamiliar languages and identify trends in topic prevalence.We introduce a polylingual topic model that discovers topics aligned across multiple languages.We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.