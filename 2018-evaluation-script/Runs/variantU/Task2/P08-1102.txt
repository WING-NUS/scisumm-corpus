2 Segmentation and POS Tagging Given a Chinese character sequence: while the segmentation and POS tagging result can be depicted as: Here  Ci (i = L.n) denotes Chinese character  ti (i = L.m) denotes POS tag  and Cl:r (l < r) denotes character sequence ranges from Cl to Cr.According to Ng and Low (2004)  the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.On the Penn Chinese Treebank 5.0  we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.To segment and tag a character sequence  there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T).C represents a Chinese character while the subscript of C indicates its position in the sentence relative to the current character (it has the subscript 0).The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target.Compared to performing segmentation and POS tagging one at a time  Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging (Ng and Low  2004).Since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a boundary tag (Xue and Shen  2003)  Joint S&T can be conducted in a labelling fashion by expanding boundary tags to include POS information (Ng and Low  2004).Templates in the column below are expanded from the upper ones.Besides this perceptron  other sub-models are trained and used as additional features of the outside-layer linear model.On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).The feature templates we adopted are selected from those of Ng and Low (2004).We can see that segmentation and POS tagging task is to divide a character sequence into several subsequences and label each of them a POS tag.We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech TaggingIn following subsections  we describe the feature templates and the perceptron training algorithm.We also find that the perceptron model functions as the kernel of the outside-layer linear model.We find that the cascaded model achieves a F-measure increment of about 0.5 points on segmentation and about 0.9 points on Joint S&T  over the perceptron-only model POS+.In the rest of the paper  we will call them labelling model and generating model respectively.Word segmentation and part-of-speech (POS) tagging are important tasks in computer processing of Chinese and other Asian languages.Line 4 scans words of all possible lengths l (l = 1.. min(i  K)  where i points to the current considering character).Lines 3 â€” 11 generate a N-best list for each character position i.Given a Chinese character sequence C1:n  the decoding procedure can proceed in a left-right fashion with a dynamic programming approach.The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.Experimental results show that  it achieves obvious improvement over the perceptron-only model  about from 0.973 to 0.978 on segmentation  and from 0.925 to 0.934 on Joint S&T  with error reductions of 18.5% and 12% respectively.Figure 3 shows their learning curves depicting the F-measure on the development set after 1 to 10 training iterations.In order to perform POS tagging at the same time  we expand boundary tags to include POS information by attaching a POS to the tail of a boundary tag as a postfix following Ng and Low (2004).Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.As all the sub-models  including the perceptron  are regarded as separate features of the outside-layer linear model  we can train them respectively with special algorithms.If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus  we randomly chosen 2  000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84  294 sentences)  then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.Several models were introduced for these problems  for example  the Hidden Markov Model (HMM) (Rabiner  1989)  Maximum Entropy Model (ME) (Ratnaparkhi and Adwait  1996)  and Conditional Random Fields (CRFs) (Lafferty et al.  2001).Test results listed in Table 2 shows that this model obtains higher accuracy than the best of SIGHAN Bakeoff 2 in three corpora (AS  CityU and MSR).A subsequence of boundary-POS labelling result indicates a word with POS t only if the boundary tag sequence composed of its boundary part conforms to s or bm*e style  and all POS tags in its POS part equal to t. For example  a tag sequence b NN m NN e NN represents a threecharacter word with POS tag NN.In theory  any useful knowledge can be incorporated into the perceptron directly  besides the characterbased features already adopted.As a result  many theoretically useful features such as higherorder word or POS n-grams are difficult to be incorporated in the model efficiently.CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.In order to inspect how much improvement each feature brings into the cascaded model  every time we removed a feature while retaining others  then retrained the model and tested its performance on the test set.Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.It has a two-layer architecture  with a perceptron as the core and another linear model as the outside-layer.Note that the templates of Ng and Low (2004) have already contained some lexical-target ones.Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.