We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).To make the projection practical  we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).Unfortunately  the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al.  2010)  making its practical usability questionable at best.We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.We were intentionally lenient with our baselines: bilingual information by projecting POS tags directly across alignments in the parallel data.The focus of this work is on building POS taggers for foreign languages  assuming that we have an English POS tagger and some parallel text between the two languages.They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed)  making it applicable to a wide array of resource-poor languages.(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.This is similar to stacking the different feature instantiations into long (sparse) vectors and computing the cosine similarity between them.The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).We use two different similarity functions to define the edge weights among the foreign vertices and between vertices from different languages.Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).Furthermore we expect the label distributions on the foreign to be fairly noisy  because the graph constraints have not been taken into account yet.Because many foreign word types are not aligned to an English word (see Table 3)  and we do not run label propagation on the foreign side  we expect the projected information to have less coverage.Because there might be some controversy about the exact definitions of such universals  this set of coarse-grained POS categories is defined operationally  by collapsing language (or treebank) specific distinctions to a set of categories that exists across all languages.This stage of label propagation results in a tag distribution ri over labels y  which encodes the proportion of times the middle word of ui E Vf aligns to English words vy tagged with label y: The second stage consists of running traditional label propagation to propagate labels from these peripheral vertices Vf� to all foreign language vertices in the graph  optimizing the following objective: 5 POS Induction After running label propagation (LP)  we compute tag probabilities for foreign word types x by marginalizing the POS tag distributions of foreign trigrams ui = x− x x+ over the left and right context words: where the qi (i = 1  ...   |Vf|) are the label distributions over the foreign language vertices and µ and ν are hyperparameters that we discuss in §6.4.Therefore  the number of fine tags varied across languages for our experiments; however  one could as well have fixed the set of HMM states to be a constant across languages  and created one mapping to the universal POS tagset.Since our graph is built from a parallel corpus  we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g.  when the trigram type and the feature instantiation don’t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall  and we therefore extract only intersected high-confidence (> 0.9) alignments De�f.In our experiments  we used the same set of features as BergKirkpatrick et al. (2010): an indicator feature based In a traditional Markov model  the emission distribution PΘ(Xi = xi  |Zi = zi) is a set of multinomials.For monolingual treebank data we relied on the CoNLL-X and CoNLL-2007 shared tasks on dependency parsing (Buchholz and Marsi  2006; Nivre et al.  2007).Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.Supervised part-of-speech (POS) taggers  for example  approach the level of inter-annotator agreement (Shen et al.  2007  97.3% accuracy for English).More recently  Subramanya et al. (2010) defined a graph over the cliques in an underlying structured prediction model.Given the bilingual graph described in the previous section  we can use label propagation to project the English POS labels to the foreign language.These universal POS categories not only facilitate the transfer of POS information from one language to another  but also relieve us from using controversial evaluation metrics 2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.First  we use a novel graph-based framework for projecting syntactic information across language boundaries.To bridge this gap  we consider a practically motivated scenario  in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest  but that we have access to parallel data with a resource-rich language.Taking the intersection of languages in these resources  and selecting languages with large amounts of parallel data  yields the following set of eight Indo-European languages: Danish  Dutch  German  Greek  Italian  Portuguese  Spanish and Swedish.This scenario is applicable to a large set of languages and has been considered by a number of authors in the past (Alshawi et al.  2000; Xi and Hwa  2005; Ganchev et al.  2009).To initialize the graph for label propagation we use a supervised English tagger to label the English side of the bitext.7 We then simply count the individual labels of the English tokens and normalize the counts to produce tag distributions over English word types.The number of latent HMM states for each language in our experiments was set to the number of fine tags in the language’s treebank.This vector tx is constructed for every word in the foreign vocabulary and will be used to provide features for the unsupervised foreign language POS tagger.For English POS tagging  BergKirkpatrick et al. (2010) found that this direct gradient method performed better (>7% absolute accuracy) than using a feature-enhanced modification of the Expectation-Maximization (EM) algorithm (Dempster et al.  1977).8 Moreover  this route of optimization outperformed a vanilla HMM trained with EM by 12%.Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections  and bridge the gap between purely supervised and unsupervised POS tagging models.Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%)  and considerably bridges the gap to fully supervised POS tagging performance (96.6%).