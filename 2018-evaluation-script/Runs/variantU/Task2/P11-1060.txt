Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.Feedback from the context; for example  the lexical entry for borders world has been used to guide both syntactic parsing is S\NP/NP : Ay.Ax.border(x  y)  which means (Schuler  2003) and semantic parsing (Popescu et borders looks right for the first argument and left al.  2003; Clarke et al.  2010).As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.Eisenciations due to data sparsity  and having an insuffi- stein et al. (2009) induces conjunctive formulae and ciently large K. uses them as features in another learning problem.At the same time  representations such as FunQL (Kate et al.  2005)  which was used in Clarke et al. (2010)  are simpler but lack the full expressive power of lambda calculus.To contrast  consider et al. (2010)  which we discussed earlier.Trace inspired by Discourse Representation Theory (DRT) predicates can be inserted anywhere  but the fea- (Kamp and Reyle  1993; Kamp et al.  2005)  where tures favor some insertions depending on the words variables are discourse referents.The logical forms in DCS are called DCS trees  where nodes are labeled with predicates  and edges are labeled with relations.Aggregate relation DCS trees that only use join relations can represent arbitrarily complex compositional structures  but they cannot capture higherorder phenomena in language.This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure  which will facilitate parsing.We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS)  which captures the core idea of using trees to represent formal semantics.Although a DCS tree is a logical form  note that it looks like a syntactic dependency tree with predicates in place of words.It is impossible to represent the semantics of this phrase with just a CSP  so we introduce a new aggregate relation  notated E. Consider a tree hE:ci  whose root is connected to a child c via E. If the denotation of c is a set of values s  the parent’s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).5 Discussion Piantadosi et al. (2008) induces first-order formuA major focus of this work is on our semantic rep- lae using CCG in a small domain assuming observed resentation  DCS  which offers a new perspective lexical semantics.Our employed (Zettlemoyer and Collins  2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language—think grounded al.  2010). compositional semantics.Supervised semantic parsers (Zelle and Mooney  1996; Tang and Mooney  2001; Ge and Mooney  2005; Zettlemoyer and Collins  2005; Kate and Mooney  2007; Zettlemoyer and Collins  2007; Wong and Mooney  2007; Kwiatkowski et al.  2010) rely on manual annotation of logical forms  which is expensive.In each dataset  each sentence x is annotated with a Prolog logical form  which we use only to evaluate and get an answer y.This option is not available to us since we do not have annotated logical forms  so we must instead rely on lexical triggers to define the search space.The denotation of a DCS tree can now be defined recursively: The base case is defined in (3): if z is a single node with predicate p  then the denotation of z has one column with the tuples w(p) and an empty store.Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.The logical forms in this framework are trees  which is desirable for two reasons: (i) they parallel syntactic dependency trees  which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.It is this transparency between syntax and semantics provided by DCS which leads to a simple and streamlined compositional semantics suitable for program induction.We now turn to the task of mapping natural language For the example in Figure 4(b)  the de- utterances to DCS trees.Our model is arc-factored  so we can sum over all DCS trees in ZL(x) using dynamic programming.Let z be a DCS tree.Our first question is: given notation of the DCS tree before execution is an utterance x  what trees z ∈ Z are permissible?A DCS tree that contains only join and aggregate relations can be viewed as a collection of treestructured CSPs connected via aggregate relations.Formally: Definition 1 (DCS trees) Let Z be the set of DCS trees  where each z ∈ Z consists of (i) a predicate for each child i  the ji-th component of v must equal the j'i-th component of some t in the child’s denotation (t ∈ JciKw).In some sense  this is the technical core of DCS.We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.Think of DCS as a higher-level Our system learns lexical associations between programming language tailored to natural language  words and predicates.On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.SEMRESP requires a lexicon of 1.42 words per non-value predicate  WordNet features  and syntactic parse trees; DCS requires only words for the domain-independent predicates (overall  around 0.5 words per non-value predicate)  POS tags  and very simple indicator features.Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms  we hope to use our without becoming mired in the semantic formalism. techniques in developing even more accurate and Quantifiers and superlatives significantly compli- broader-coverage language understanding systems. cate scoping in lambda calculus  and often type rais- Acknowledgments We thank Luke Zettlemoyer ing needs to be employed.A world w is mapping from each predicate p ∈ P to a set of tuples; for example  w(state) = {(CA)  (OR) ... }.There has been a fair amount of past work on no predicates)  confusion of Washington state with this topic: Liang et al. (2010) induces combinatory Washington D.C.  learning the wrong lexical asso- logic programs in a non-linguistic setting.Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.