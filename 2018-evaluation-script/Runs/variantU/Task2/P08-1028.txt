Let p denote the composition of two vectors u and v  representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.Central in these models is the notion of compositionality — the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.So  if we assume that only the ith components of u and v contribute to the ith component of p  that these components are not dependent on i  and that the function is symmetric with regard to the interchange of u and v  we obtain a simpler instantiation of an additive model: Analogously  under the same assumptions  we obtain the following simpler multiplicative model: only the ith components of u and v contribute to the ith component of p. Another class of models can be derived by relaxing this constraint.Now  if we assume that p lies in the same space as u and v  avoiding the issues of dimensionality associated with tensor products  and that f is a linear function  for simplicity  of the cartesian product of u and v  then we generate a class of additive models: where A and B are matrices which determine the contributions made by u and v to the product p. In contrast  if we assume that f is a linear function of the tensor product of u and v  then we obtain multiplicative models: where C is a tensor of rank 3  which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.We define a general class of models for this process of composition as: The expression above allows us to derive models for which p is constructed in a distinct space from u and v  as is the case for tensor products.While vector addition has been effective in some applications such as essay grading (Landauer and Dumais  1997) and coherence assessment (Foltz et al.  1998)  there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al.  1991; West and Stanovich  1986) and modulate cognitive behavior in sentence priming (Till et al.  1988) and inference tasks (Heit and Rubinstein  1994).In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess  1996; Landauer and Dumais  1997) and text comprehension (Landauer and Dumais  1997; Foltz et al.  1998).Vector-based models of word meaning (Lund and Burgess  1996; Landauer and Dumais  1997) have become increasingly popular in natural language processing (NLP) and cognitive science.In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.This paper proposes a framework for representing the meaning of phrases and sentences in vector space.We formulate semantic composition as a function of two vectors  u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word’s vector typically represents its co-occurrence with neighboring words.Vector averaging is unfortunately insensitive to word order  and more generally syntactic structure  giving the same representation to any constructions that happen to share the same vocabulary.Vector addition is by far the most common method for representing the meaning of linguistic sequences.He argues that the subjects of ran in The color ran and The horse ran select different senses of ran.As we can see sentences with high similarity landmarks are perceived as more similar to the reference sentence.This is illustrated in the example below taken from Landauer et al. (1997).Following previous work (Bullinaria and Levy  2007)  we optimized its parameters on a word-based semantic similarity task.Kintsch (2001) considers only the m most similar neighbors to the predicate  from which he subsequently selects k  those most similar to its argument.We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence  via the argument R  on syntax.The tensor product u ® v is a matrix whose components are all the possible products uivj of the components of vectors u and v. A major difficulty with tensor products is their dimensionality which is higher than the dimensionality of the original vectors (precisely  the tensor product has dimensionality m x n).The resulting vector is sparser but expresses more succinctly the meaning of the predicate-argument structure  and thus allows semantic similarity to be modelled more accurately.The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.The semantic space we used in our experiments was built on a lemmatised version of the BNC.Verbs and nouns that were attested less than fifty times in the BNC were removed as they would result in unreliable vectors.Kintsch (2001) proposes a variation on the vector addition theme in an attempt to model how the meaning of a predicate (e.g.  run) varies depending on the arguments it operates upon (e.g  the horse ran vs. the color ran).To overcome this problem  other techniques have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components.Although we have presented multiplicative and additive models separately  there is nothing inherent in our formulation that disallows their combination.We observe a similar pattern for the non compositional baseline model  the weighted additive model and Kintsch (2001).We intend to assess the potential of our composition models on context sensitive semantic priming (Till et al.  1988) and inductive inference (Heit and Rubinstein  1994).Specifically  we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.First  we used the models to estimate the cosine similarity between the reference sentence and its landmarks.In order to establish an independent measure of sentence similarity  we assembled a set of experimental materials and elicited similarity ratings from human subjects.The lowest correlation (p = 0.04) is observed for the simple additive model which is not significantly different from the non-compositional baseline model.An extreme form of this differential in the contribution of constituents is where one of the vectors  say u  contributes nothing at all to the combination: Admittedly the model in (8) is impoverished and rather simplistic  however it can serve as a simple baseline against which to compare more sophisticated models.In contrast to the simple additive model  this extended model is sensitive to syntactic structure  since n is chosen from among the neighbors of the predicate  distinguishing it from the argument.Relaxing the assumption of symmetry in the case of the simple additive model produces a model which weighs the contribution of the two components differently: This allows additive models to become more syntax aware  since semantically important constituents can participate more actively in the composition.The difference between High and Low similarity values estimated by these models are statistically significant (p < 0.01 using the Wilcoxon rank sum test).We expect better models to yield a pattern of similarity scores like those observed in the human ratings (see Figure 2).First  we examined whether participants gave high ratings to high similarity sentence pairs and low ratings to low similarity ones.As can be seen  all models are significantly correlated with the human ratings.In addition  Bullinaria and Levy (2007) found that these parameters perform well on a number of other tasks such as the synonymy task from the Test ofEnglish as a Foreign Language (TOEFL).In our experiments we selected parameters that Kintsch reports as optimal.Finally  Kintsch’s (2001) additive model has two extra parameters.These included three additive models  i.e.  simple addition (equation (5)  Add)  weighted addition (equation (7)  WeightAdd)  and Kintsch’s (2001) model (equation (10)  Kintsch)  a multiplicative model (equation (6)  Multiply)  and also a model which combines multiplication with addition (equation (11)  Combined).Also note that in contrast to the combined model  the multiplicative model does not have any free parameters and hence does not require optimization for this particular task.Unfortunately  Kintsch (2001) demonstrates how his own composition algorithm works intuitively on a few hand selected examples but does not provide a comprehensive test set.Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions.