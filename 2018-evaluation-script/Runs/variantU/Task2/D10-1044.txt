There is a fairly large body of work on SMT adaptation.There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan  2007; Wu et al.  2005)  and on dynamically choosing a dev set (Xu et al.  2007).This is a standard adaptation problem for SMT.Moving beyond directly related work  major themes in SMT adaptation include the IR (Hildebrand et al.  2005; L¨u et al.  2007; Zhao et al.  2004) and mixture (Finch and Sumita  2008; Foster and Kuhn  2007; Koehn and Schroeder  2007; L¨u et al.  2007) approaches for LMs and TMs described above  as well as methods for exploiting monolingual in-domain text  typically by translating it automatically and then performing self training (Bertoldi and Federico  2009; Ueffing et al.  2007; Schwenk and Senellart  2009).Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.We have already mentioned the closely related work by Matsoukas et al (2009) on discriminative corpus weighting  and Jiang and Zhai (2007) on (nondiscriminative) instance weighting.Phrase-level granularity distinguishes our work from previous work by Matsoukas et al (2009)  who weight sentences according to sub-corpus and genre membership.This has led previous workers to adopt ad hoc linear weighting schemes (Finch and Sumita  2008; Foster and Kuhn  2007; L¨u et al.  2007).To set β  we used the same criterion as for α  over a dev corpus: The MAP combination was used for TM probabilities only  in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney  1995).3 Motivated by information retrieval  a number of approaches choose “relevant” sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al.  2005; L¨u et al.  2007)  or individual target hypotheses (Zhao et al.  2004).This has the potential drawback of increasing the number of features  which can make MERT less stable (Foster and Kuhn  2009).For comparison to information-retrieval inspired baselines  eg (L¨u et al.  2007)  we select sentences from OUT using language model perplexities from IN.To approximate these baselines  we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.Linear weights are difficult to incorporate into the standard MERT procedure because they are “hidden” within a top-level probability that represents the linear combination.1 Following previous work (Foster and Kuhn  2007)  we circumvent this problem by choosing weights to optimize corpus loglikelihood  which is roughly speaking the training criterion used by the LM and TM themselves.We incorporate instance weighting into a mixture-model framework  and find that it yields consistent improvements over a wide range of baselines.Finally  we incorporate the instance-weighting model into a general linear combination  and learn weights and mixing parameters simultaneously. where cλ(s  t) is a modified count for pair (s  t) in OUT  u(s|t) is a prior distribution  and y is a prior weight.We model po(s|t) using a MAP criterion over weighted phrase-pair counts: and from the similarity to (5)  assuming y = 0  we see that wλ(s  t) can be interpreted as approximating pf(s  t)/po(s  t).We will also directly compare with a baseline similar to the Matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences.In both cases  the instanceweighting approach improved over a wide range of baselines  giving gains of over 2 BLEU points over the best non-adapted baseline  and gains of between 0.6 and 1.8 over an equivalent mixture model (with an identical training procedure but without instance weighting).We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.An easy way to achieve this is to put the domain-specific LMs and TMs into the top-level log-linear model and learn optimal weights with MERT (Och  2003).This suggests a direct parallel to (1): where ˜p(s  t) is a joint empirical distribution extracted from the IN dev set using the standard procedure.2 An alternative form of linear combination is a maximum a posteriori (MAP) combination (Bacchiani et al.  2004).We focus here instead on adapting the two most important features: the language model (LM)  which estimates the probability p(wIh) of a target word w following an ngram h; and the translation models (TM) p(slt) and p(t1s)  which give the probability of source phrase s translating to target phrase t  and vice versa.For developers of Statistical Machine Translation (SMT) systems  an additional complication is the heterogeneous nature of SMT components (word-alignment model  language model  translation model  etc.A similar maximumlikelihood approach was used by Foster and Kuhn (2007)  but for language models only.6One of our experimental settings lacks document boundaries  and we used this approximation in both settings for consistency.This is less effective in our setting  where IN and OUT are disparate.We extend the Matsoukas et al approach in several ways.However  it is robust  efficient  and easy to implement.4 To perform the maximization in (7)  we used the popular L-BFGS algorithm (Liu and Nocedal  1989)  which requires gradient information.