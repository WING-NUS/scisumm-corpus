The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser  and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.Three state-of-the-art statistical parsers are combined to produce more accurate parses  as well as new bounds on achievable Treebank parsing accuracy.We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.The average individual parser accuracy was reduced by more than 5% when we added this new parser  but the precision of the constituent voting technique was the only result that decreased significantly.Finally we show the combining techniques degrade very little when a poor parser is added to the set.The development set contained 44088 constituents in 2416 sentences and the test set contained 30691 constituents in 1699 sentences.The substructures that are unanimously hypothesized by the parsers should be preserved after combination  and the combination technique should not foolishly create substructures for which there is no supporting evidence.We used these three parsers to explore parser combination techniques.In the interest of testing the robustness of these combining techniques  we added a fourth  simple nonlexicalized PCFG parser.Parser 3  the most accurate parser  was chosen 71% of the time  and Parser 1  the least accurate parser was chosen 16% of the time.We have presented two general approaches to studying parser combination: parser switching and parse hybridization.The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).The combining algorithm is presented with the candidate parses and asked to choose which one is best.We call this approach parse hybridization.Another technique for parse hybridization is to use a naïve Bayes classifier to determine which constituents to include in the parse.For this reason  naïve Bayes classifiers are well-matched to this problem.We call this approach parser switching.Through parser combination we have reduced the precision error rate by 30% and the recall error rate by 6% compared to the best previously published result.The first shows how constituent features and context do not help in deciding which parser to trust.We call this technique constituent voting.The resulting parsers surpass the best previously published performance results for the Penn Treebank.The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank  leaving only sections 22 and 23 completely untouched during the development of any of the parsers.Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).We have developed a general approach for combining parsers when preserving the entire structure of a parse tree is important.We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes  a completely flat structure.These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).In Table 1 we see with very few exceptions that the isolated constituent precision is less than 0.5 when we use the constituent label as a feature.We used section 23 as the development set for our combining techniques  and section 22 only for final testing.In this case we are interested in finding' the maximum probability parse  ri  and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers.The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.The standard measures for evaluating Penn Treebank parsing performance are precision and recall of the predicted constituents.Furthermore  we know one of the original parses will be the hypothesized parse  so the direct method of determining which one is best is to compute the probability of each of the candidate parses using the probabilistic model we developed in Section 2.1.