In this paper  we report adapting a lexicalized  probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.In our statistical model  trees are generated according to a process similar to that described in (Collins 1996  1997).Our annotation staff found syntactic analysis particularly complex and slow going.For the following example  the template relation in Figure 2 was to be generated: &quot;Donald M. Goldstein  a historian at the University of Pittsburgh who helped write...&quot;In this paper we report adapting a lexic al ized  probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.Almost all approaches to information extraction — even at the sentence level — are based on the divide-and-conquer strategy of reducing a complex problem to a set of simpler ones.The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.Given multiple constituents that cover identical spans in the chart  only those constituents with probabilities within a While our focus throughout the project was on TE and TR  we became curious about how well the model did at part-of-speech tagging  syntactic parsing  and at name finding.We have demonstrated  at least for one problem  that a lexicalized  probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.We were already using a generative statistical model for part-of-speech tagging (Weischedel et al. 1993)  and more recently  had begun using a generative statistical model for name finding (Bikel et al.Figure 4 shows an example of the semantic annotation  which was the only type of manual annotation we performed.We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.Our integrated model represents syntax and semantics jointly using augmented parse trees.Instead  our parsing algorithm  trained on the UPenn TREEBANK  was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.In these trees  the standard TREEBANK structures are augmented to convey semantic information  that is  entities and relations.If the single generalized model could then be extended to semantic analysis  all necessary sentence level processing would be contained in that model.For this reason  we focused on designing an integrated model in which tagging  namefinding  parsing  and semantic interpretation decisions all have the opportunity to mutually influence each other.The semantics — that is  the entities and relations — can then be directly extracted from these sentential trees.In this section  we describe the algorithm that was used to automatically produce augmented trees  starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97)  and evaluated name finding accuracy on the MUC7 named entity test.Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.Several technical challenges confronted us and were solved: TREEBANK on Wall Street Journal adequately train the algorithm for New York Times newswire  which includes dozens of newspapers?Whenever a relation involves an entity that is not a direct descendant of that relation in the parse tree  semantic pointer labels are attached to all of the intermediate nodes.Currently  the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing  replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.To train our integrated model  we required a large corpus of augmented parse trees.For the following example  the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.Since 1995  a few statistical parsing algorithms (Magerman  1995; Collins  1996 and 1997; Charniak  1997; Rathnaparki  1997) demonstrated a breakthrough in parsing accuracy  as measured against the University of Pennsylvania TREEBANK as a gold standard.A Novel Use of Statistical Parsing to Extract Information from Text