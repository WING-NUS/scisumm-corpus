As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.We created a parser based upon the maximumentropy-inspired model of the last section  smoothed using standard deleted interpolation.Also  the earlier parser uses two techniques not employed in the current parser.We take as our starting point the parser labled Char97 in Figure 1 [5]  as that is the program from which our current parser derives.Following [5 10]  our parser is based upon a probabilistic generative model.We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.The results for the new parser as well as for the previous top-three individual parsers on this corpus are given in Figure 1.Indeed  we initiated this line of work in an attempt to create a parser that would be flexible enough to allow modifications for parsing down to more semantic levels of detail.Second  Char97 uses unsupervised learning in that the original system was run on about thirty million words of unparsed text  the output was taken as &quot;correct&quot;  and statistics were collected on the resulting parses.First  it uses a clustering scheme on words to give the system a &quot;soft&quot; clustering of heads and sub-heads.This is indicated in Figure 2  where the model labeled &quot;Best&quot; has precision of 89.8% and recall of 89.6% for an average of 89.7%  0.4% lower than the results on the official test corpus.For runs with the generative model based upon Markov grammar statistics  the first pass uses the same statistics  but conditioned only on standard PCFG information.We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.In particular  we measure labeled precision (LP) and recall (LR)  average number of crossbrackets per sentence (CB)  percentage of sentences with zero cross brackets (OCB)  and percentage of sentences with < 2 cross brackets (2CB).Between the Old model and the Best model  Figure 2 gives precision/recall measurements for several different versions of our parser.Ultimately it is this flexibility that let us try the various conditioning events  to move on to a Markov grammar approach  and to try several Markov grammars of different orders  without significant programming.A Maximum-Entropy-Inspired Parser *In the previous sections we have concentrated on the relation of the parser to a maximumentropy approach  the aspect of the parser that is most novel.This parser achieves an average precision/recall of 86.2%.This is as opposed to the &quot;Markovgrammar&quot; approach used in the current parser.As is typical  all of the standard measures tell pretty much the same story  with the new parser outperforming the other three parsers.That is  for all sentences s and all parses 7r  the parser assigns a probability p(s   7r) = p(r)  the equality holding when we restrict consideration to 7r whose yield * This research was supported in part by NSF grant LIS SBR 9720368.In keeping with the standard methodology [5  9 10 15 17]  we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training  section 23 for testing  and section 24 for development (debugging and tuning).The results of [13] achieved by combining the aforementioned three-best parsers also suggest that the limit on tree-bank trained parsers is much higher than previously thought.L and R are conditioned on three previous labels so we are using a third-order Markov grammar.As one can see in Figure 2  a firstorder Markov grammar (with all the aforementioned improvements) performs slightly worse than the equivalent tree-bank-grammar parser.It makes no use of special maximum-entropyinspired features (though their presence made it much easier to perform these experiments)  it does not guess the pre-terminal before guessing the lexical head  and it uses a tree-bank grammar rather than a Markov grammar.Indeed  it may be that adding this new parser to the mix may yield still higher results.Maximum-entropy models have two benefits for a parser builder.In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.That the previous three best parsers on this test [5 9 17] all perform within a percentage point of each other  despite quite different basic mechanisms  led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training  and to conjecture that perhaps we were at it.From our perspective  perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head.We also present some partial results showing the effects of different conditioning information  including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.Looking in particular at the precision and recall figures  the new parser's give us a 13% error reduction over the best of the previous work  Co1199 [9].