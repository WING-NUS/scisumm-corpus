This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al.  2005b; McDonald and Pereira  2006) generalizes well to languages other than English.That work extends the maximum spanning tree dependency parsing framework (McDonald et al.  2005a; McDonald et al.  2005b) to incorporate features over multiple edges in the dependency graph.These results show that the discriminative spanning tree parsing framework (McDonald et al.  2005b; McDonald and Pereira  2006) is easily adapted across all these languages.For instance  the system of McDonald et al. (2005a) incorporates features over the part of speech of words occurring between and around a possible head-dependent relation.We report results on the CoNLL-X shared task (Buchholz et al.  2006) data sets and present an error analysis.In fact  for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).For instance  sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.Performance is measured through unlabeled accuracy  which is the percentage of words that modify the correct head in the dependency graph  and labeled accuracy  which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph.These results report the average labeled and unlabeled precision for the 10 languages with the smallest training sets.An exact projective and an approximate non-projective parsing algorithm are presented  since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.Furthermore  it made the system homogeneous in terms of learning algorithms since that is what is used to train our unlabeled parser (McDonald and Pereira  2006).The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.First  we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.Multilingual Dependency Analysis with a Two-Stage Discriminative ParserIt is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.For instance  if we consider a head xi with dependents xj1  ...   xjM  it is often the case that many of these dependencies will have correlated labels.We evaluate this parser on a diverse set of 13 languages using data provided by the CoNLL-X shared-task organizers (Buchholz et al.  2006; Hajiˇc et al.  2004; Simov et al.  2005; Simov and Osenova  2003; Chen et al.  2003; B¨ohmov´a et al.  2003; Kromann  2003; van der Beek et al.  2002; Brants et al.  2002; Kawata and Bartels  2000; Afonso et al.  2002; Dˇzeroski et al.  2006; Civit Torruella and MartiAntonin  2002; Nilsson et al.  2005; Oflazer et al.  2003; Atalay et al.  2003).We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216–220  New York City  June 2006. c�2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.For score functions  we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation  we can find the highest scoring label sequence with Viterbi’s algorithm.The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.However  if we only allow projective parses  do not use morphological features and label edges with a simple atomic classifier  the overall drop in performance becomes significant (row 5 versus row 1).