In the experiments below  we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs 3 previously tested on Swedish (Nivre et al.  2004) and English (Nivre and Scholz  2004).Finally  since non-projective constructions often involve long-distance dependencies  the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson  2002; Dienes and Dubey  2003; Jijkoun and de Rijke  2004; Cahill et al.  2004; Levy and Manning  2004; Campbell  2004).We have presented a new method for non-projective dependency parsing  based on a combination of data-driven projective dependency parsing and graph transformation techniques.There exist a few robust broad-coverage parsers that produce non-projective dependency structures  notably Tapanainen and J¨arvinen (1997) and Wang and Harper (2004) for English  Foth et al. (2004) for German  and Holan (2004) for Czech.It is also true of the adaptation of the Collins parser for Czech (Collins et al.  1999) and the finite-state dependency parser for Turkish by Oflazer (2003).In this paper  we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.This is true of the widely used link grammar parser for English (Sleator and Temperley  1993)  which uses a dependency grammar of sorts  the probabilistic dependency parser of Eisner (1996)  and more recently proposed deterministic dependency parsers (Yamada and Matsumoto  2003; Nivre et al.  2004).However  while Dienes and Dubey recognize empty categories in a pre-processing step and only let the parser find their antecedents  we use the parser both to detect dislocated dependents and to predict either the type or the location of their syntactic head (or both) and use post-processing only to transform the graph in accordance with the parser’s analysis.We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.Thus  most broad-coverage parsers based on dependency grammar have been restricted to projective structures.Instead  we want to apply an inverse transformation to recover the underlying (nonprojective) dependency graph.By applying an inverse transformation to the output of the parser  arcs with non-standard labels can be lowered to their proper place in the dependency graph  giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.When the parser is trained on the transformed data  it will ideally learn not only to construct projective dependency structures but also to assign arc labels that encode information about lifts.Projectivizing a dependency graph by lifting nonprojective arcs is a nondeterministic operation in the general case.Compared to related work on the recovery of long-distance dependencies in constituency-based parsing  our approach is similar to that of Dienes and Dubey (2003) in that the processing of non-local dependencies is partly integrated in the parsing process  via an extension of the set of syntactic categories  whereas most other approaches rely on postprocessing only.Special thanks to Jan Hajiˇc and Matthias Trautner Kromann for assistance with the Czech and Danish data  respectively  and to Jan Hajiˇc  Tom´aˇs Holan  Dan Zeman and three anonymous reviewers for valuable comments on a preliminary version of the paper.This leads to the best reported performance for robust non-projective parsing of Czech.The fact that projective dependency parsers can never exactly reproduce the analyses found in non-projective treebanks is often neglected because of the relative scarcity of problematic constructions.However  this argument is only plausible if the formal framework allows non-projective dependency structures  i.e. structures where a head and its dependents may correspond to a discontinuous constituent.Still  from a theoretical point of view  projective parsing of non-projective structures has the drawback that it rules out perfect accuracy even as an asymptotic goal.It is worth noting that  although nonprojective constructions are less frequent in DDT than in PDT  they seem to be more deeply nested  since only about 80% can be projectivized with a single lift  while almost 95% of the non-projective arcs in PDT only require a single lift.In practice  we can therefore expect a trade-off such that increasing the amount of information encoded in arc labels will cause an increase in the accuracy of the inverse transformation but a decrease in the accuracy with which the parser can construct the labeled representations.Unlike Kahane et al. (1998)  we do not regard a projectivized representation as the final target of the parsing process.The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.We assume that the goal in dependency parsing is to construct a labeled dependency graph of the kind depicted in Figure 1.In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p↓.The baseline simply retains the original labels for all arcs  regardless of whether they have been lifted or not  and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme  called Head  we use a new label d↑h for each lifted arc  where d is the dependency relation between the syntactic head and the dependent in the non-projective representation  and h is the dependency relation that the syntactic head has to its own head in the underlying structure.In principle  it would be possible to encode the exact position of the syntactic head in the label of the arc from the linear head  but this would give a potentially infinite set of arc labels and would make the training of the parser very hard.For each token  three types of features may be taken into account: the word form; the part-of-speech assigned by an automatic tagger; and labels on previously assigned dependency arcs involving the token – the arc from its head and the arcs to its leftmost and rightmost dependent  respectively.First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.For robustness reasons  the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token  dependency type features are limited to tokens on the stack.Even this may be nondeterministic  in case the graph contains several non-projective arcs whose lifts interact  but we use the following algorithm to construct a minimal projective transformation D0 = (W  A0) of a (nonprojective) dependency graph D = (W  A): The function SMALLEST-NONP-ARC returns the non-projective arc with the shortest distance from head to dependent (breaking ties from left to right).While the proportion of sentences containing non-projective dependencies is often 15–25%  the total proportion of non-projective arcs is normally only 1–2%.The first thing to note is that projectivizing helps in itself  even if no encoding is used  as seen from the fact that the projective baseline outperforms the non-projective training condition by more than half a percentage point on attachment score  although the gain is much smaller with respect to exact match.Evaluation metrics used are Attachment Score (AS)  i.e. the proportion of tokens that are attached to the correct head  and Exact Match (EM)  i.e. the proportion of sentences for which the dependency graph exactly matches the gold standard.The prediction based on these features is a knearest neighbor classification  using the IB1 algorithm and k = 5  the modified value difference metric (MVDM) and class voting with inverse distance weighting  as implemented in the TiMBL software package (Daelemans et al.  2003).Memory-based classifiers for the experiments were created using TiMBL (Daelemans et al.  2003).