2 We now introduce a new algorithm for learning from unlabeled examples  which we will call DLCoTrain (DL stands for decision list  the term Cotrain is taken from (Blum and Mitchell 98)).AdaBoost was first introduced in (Freund and Schapire 97); (Schapire and Singer 98) gave a generalization of AdaBoost which we will use in this paper.The first method uses a similar algorithm to that of (Yarowsky 95)  with modifications motivated by (Blum and Mitchell 98).The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).The two new terms force the two classifiers to agree  as much as possible  on the unlabeled examples.The second algorithm extends ideas from boosting algorithms  designed for supervised learning tasks  to the framework suggested by (Blum and Mitchell 98).We then discuss how we adapt and generalize a boosting algorithm  AdaBoost  to the problem of named entity classification.(Blum and Mitchell 98) offer a promising formulation of redundancy  also prove some results about how the use of unlabeled examples can help classification  and suggest an objective function when training with unlabeled examples.Recent results (e.g.  (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).Limitations of (Blum and Mitchell 98): While the assumptions of (Blum and Mitchell 98) are useful in developing both theoretical results and an intuition for the problem  the assumptions are quite limited.971 746 sentences of New York Times text were parsed using the parser of (Collins 96).1 Word sequences that met the following criteria were then extracted as named entity examples: whose head is a singular noun (tagged NN).Unsupervised Models for Named Entity Classification CollinsIt is a sequence of proper nouns within an NP; its last word Cooper is the head of the NP; and the NP has an appositive modifier (a vice president at S.&P.) whose head is a singular noun (president).In the appositive case  the contextual predictor was the head of the modifying appositive (president in the Maury Cooper example); in the second case  the contextual predictor was the preposition together with the noun it modifies (plant_in in the Georgia example).The numbers falling into the location  person  organization categories were 186  289 and 402 respectively.We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.(Riloff and Shepherd 97) describe a bootstrapping approach for acquiring nouns in particular categories (such as &quot;vehicle&quot; or &quot;weapon&quot; categories).For example  a good classifier would identify Mrs. Frank as a person  Steptoe & Johnson as a company  and Honduras as a location.In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.The likelihood of the observed data under the model is where P(yi  xi) is defined as in (9).Supervised methods have been applied quite successfully to the full MUC named-entity task (Bikel et al. 97).Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision  in the form of labeled training examples.A generative model was applied (similar to naive Bayes) with the three labels as hidden vanables on unlabeled examples  and observed variables on (seed) labeled examples.In addition to a heuristic based on decision list learning  we also presented a boosting-like framework that builds on ideas from (Blum and Mitchell 98).Our derivation is slightly different from the one presented in (Schapire and Singer 98) as we restrict at to be positive.(Blum and Mitchell 98) give an example that illustrates just how powerful the second constraint can be.We are currently exploring other methods that employ similar ideas and their formal properties.It may be more realistic to replace the second criteria with a softer one  for example (Blum and Mitchell 98) suggest the alternative Alternatively  if Ii and 12 are probabilistic learners  it might make sense to encode the second constraint as one of minimizing some measure of the distance between the distributions given by the two learners.Several extensions of AdaBoost for multiclass problems have been suggested (Freund and Schapire 97; Schapire and Singer 98).For a description of the application of AdaBoost to various NLP problems see the paper by Abney  Schapire  and Singer in this volume.In our implementation  we make perhaps the simplest choice of weak hypothesis.