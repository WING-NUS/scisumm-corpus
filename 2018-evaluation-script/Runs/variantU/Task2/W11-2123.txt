Performance improvements transfer to the Moses (Koehn et al.  2007)  cdec (Dyer et al.  2010)  and Joshua (Li et al.  2009) translation systems where our code has been integrated.IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.Lossy compressed models RandLM (Talbot and Osborne  2007) and Sheffield (Guthrie and Hepple  2010) offer better memory consumption at the expense of CPU and accuracy.We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.The binary language model from Section 5.2 and text phrase table were forced into disk cache before each run.The PROBING model was designed to improve upon SRILM by using linear probing hash tables (though not arranged in a trie)  allocating memory all at once (eliminating the need for full pointers)  and being easy to compile.In all of our experiments  the binary file (whether mapped or  in the case of most other packages  interpreted) is loaded into the disk cache in advance so that lazy mapping will never fault to disk.This is especially effective at reducing load time  since raw bytes are read directly to memory—or  as happens with repeatedly used models  are already in the disk cache.As noted in Section 4.4  disk cache state is controlled by reading the entire binary file before each test begins.Dynamic programming efficiently scores many hypotheses by exploiting the fact that an N-gram language model conditions on at most N − 1 preceding words.We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.Language models that contain wi must also contain prefixes wi for 1 G i G k. Therefore  when the model is queried for p(wnjwn−1 1 ) but the longest matching suffix is wnf   it may return state s(wn1) = wnf since no longer context will be found.Given counts cn1 where e.g. c1 is the vocabulary size  total memory consumption  in bits  is Our PROBING data structure places all n-grams of the same order into a single giant hash table.If the key distribution’s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |− 1] to estimate pivots; this optimization alone led to a 24% speed improvement.All language model queries issued by machine translation decoders follow a left-to-right pattern  starting with either the begin of sentence token or null context for mid-sentence fragments.This paper presents methods to query N-gram language models  minimizing time and space costs.We run the baseline Moses system for the French-English track of the 2011 Workshop on Machine Translation 9 translating the 3003-sentence test set.Time for Moses itself to load  including loading the language model and phrase table  is included.Using cn to denote the number of n-grams  total memory consumption of TRIE  in bits  is plus quantization tables  if used.Given n-gram counts {cn}Nn=1  we use Flog2 c1] bits per vocabulary identifier and Flog2 cn] per index into the table of ngrams.Sparse lookup is a key subproblem of language model queries.Language models are widely applied in natural language processing  and applications such as machine translation make very frequent queries.KenLM: Faster and Smaller Language Model QueriesThe trie data structure is commonly used for language modeling.Overall  language modeling significantly impacts decoder performance.BerkeleyLM revision 152 (Pauls and Klein  2011) implements tries based on hash tables and sorted arrays in Java with lossy quantization.For example  syntactic decoders (Koehn et al.  2007; Dyer et al.  2010; Li et al.  2009) perform dynamic programming parametrized by both backward- and forward-looking state.IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.We use two common techniques  hash tables and sorted arrays  describing each before the model that uses the technique.For even larger models  storing counts (Talbot and Osborne  2007; Pauls and Klein  2011; Guthrie and Hepple  2010) is a possibility.Later  BerkeleyLM (Pauls and Klein  2011) described ideas similar to ours.This differs from other implementations (Stolcke  2002; Pauls and Klein  2011) that use hash tables as nodes in a trie  as explained in the next section.To quantize  we use the binning method (Federico and Bertoldi  2006) that sorts values  divides into equally sized bins  and averages within each bin.For 2 < n < N  we use a hash table mapping from the n-gram to the probability and backoff3.We reduce this to O(log log |A|) time by evenly distributing keys over their range then using interpolation search4 (Perl et al.  1978).Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke  2002) is a popular toolkit based on tries used in several decoders.This technique was introduced by Clarkson and Rosenfeld (1997) and is also implemented by IRSTLM and BerkeleyLM’s compressed option.Our code has been publicly available and intergrated into Moses since October 2010.Sheffield Guthrie and Hepple (2010) explore several randomized compression techniques  but did not release code.We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.A direct-mapped cache makes BerkeleyLM faster on repeated queries  but their fastest (scrolling) cached version is still slower than uncached PROBING  even on cache-friendly queries.To optimize left-to-right queries  we extend state to store backoff information: where m is the minimal context from Section 4.1 and b is the backoff penalty.Most similar is scrolling queries  wherein left-to-right queries that add one word at a time are optimized.MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.For even larger models  we recommend RandLM; the memory consumption of the cache is not expected to grow with model size  and it has been reported to scale well.Many packages perform language model queries.RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).In a model we built with default settings  1.2% of n + 1-grams were missing their ngram suffix.Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.We offer a state function s(wn1) = wn� where substring wn� is guaranteed to extend (to the right) in the same way that wn1 does for purposes of language modeling.