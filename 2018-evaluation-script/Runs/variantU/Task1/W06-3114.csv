Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,W06-3114,W06-3120,0,"Koehn and Monz, 2006",0,"The official results were slightly better because a lowercase evaluation was used, see (Koehn and Monz, 2006)","The official results were slightly better because a lowercase evaluation was used, see (Koehn and Monz, 2006)","['112', '154', '175', '57', '108']","<S sid =""112"" ssid = ""5"">The normalization on a per-judge basis gave very similar ranking  only slightly less consistent with the ranking from the pairwise comparisons.</S><S sid =""154"" ssid = ""47"">While we used the standard metrics of the community  the we way presented translations and prompted for assessment differed from other evaluation campaigns.</S><S sid =""175"" ssid = ""6"">Replacing this with an ranked evaluation seems to be more suitable.</S><S sid =""57"" ssid = ""23"">We are therefore applying a different method  which has been used at the 2005 DARPA/NIST evaluation.</S><S sid =""108"" ssid = ""1"">The results of the manual and automatic evaluation of the participating system translations is detailed in the figures at the end of this paper.</S>",['Results_Citation']
2,W06-3114,D07-1092,0,"Koehn and Monz, 2006",0,"We are further focusing on the shared task of the workshop on Statistical Machine Translation, which took place last year (Koehn and Monz, 2006) and consisted in translating Spanish, German, and French texts from and to English","We are further focusing on the shared task of the workshop on Statistical Machine Translation, which took place last year (Koehn and Monz, 2006) and consisted in translating Spanish, German, and French texts from and to English","['170', '6', '5', '14', '0']","<S sid =""170"" ssid = ""1"">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid =""6"" ssid = ""4"">English was again paired with German  French  and Spanish.</S><S sid =""5"" ssid = ""3"">• We evaluated translation from English  in addition to into English.</S><S sid =""14"" ssid = ""7"">There is twice as much language modelling data  since training data for the machine translation system is filtered against sentences of length larger than 40 words.</S><S sid =""0"" ssid = ""0"">Manual and Automatic Evaluation of Machine Translation between European Languages</S>",['Method_Citation']
3,W06-3114,C08-1074,0,"Koehn and Monz, 2006",0,"For our training and test data we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference","For our training and test data we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference","['47', '126', '16', '18', '14']","<S sid =""47"" ssid = ""13"">Because of this  we retokenized and lowercased submitted output with our own tokenizer  which was also used to prepare the training and test data.</S><S sid =""126"" ssid = ""19"">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid =""16"" ssid = ""9"">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid =""18"" ssid = ""11"">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S><S sid =""14"" ssid = ""7"">There is twice as much language modelling data  since training data for the machine translation system is filtered against sentences of length larger than 40 words.</S>",['Method_Citation']
4,W06-3114,W07-0718,0,"Koehn and Monz, 2006",0,"The results of last year? s workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006)","The results of last year's workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006)","['38', '90', '62', '140', '171']","<S sid =""38"" ssid = ""4"">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S><S sid =""90"" ssid = ""6"">Another way to view the judgements is that they are less quality judgements of machine translation systems per se  but rankings of machine translation systems.</S><S sid =""62"" ssid = ""1"">While automatic measures are an invaluable tool for the day-to-day development of machine translation systems  they are only a imperfect substitute for human assessment of translation quality  or as the acronym BLEU puts it  a bilingual evaluation understudy.</S><S sid =""140"" ssid = ""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S><S sid =""171"" ssid = ""2"">While many systems had similar performance  the results offer interesting insights  especially about the relative performance of statistical and rule-based systems.</S>",['Method_Citation']
5,W06-3114,P07-1083,0,"Koehn and Monz, 2006",0,"For the bi text-based annotation, we use publicly available word alignments from the Europarl corpus, automatically generated by GIZA++ for FrenchEnglish (Fr), Spanish-English (Es) and GermanEnglish (De) (Koehn and Monz, 2006)","For the bi text-based annotation, we use publicly available word alignments from the Europarl corpus, automatically generated by GIZA++ for FrenchEnglish (Fr), Spanish-English (Es) and German-English (De) (Koehn and Monz, 2006)","['165', '155', '126', '19', '46']","<S sid =""165"" ssid = ""58"">However  ince we extracted the test corpus automatically from web sources  the reference translation was not always accurate — due to sentence alignment errors  or because translators did not adhere to a strict sentence-by-sentence translation (say  using pronouns when referring to entities mentioned in the previous sentence).</S><S sid =""155"" ssid = ""48"">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S><S sid =""126"" ssid = ""19"">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid =""19"" ssid = ""12"">We aligned the texts at a sentence level across all four languages  resulting in 1064 sentence per language.</S><S sid =""46"" ssid = ""12"">By taking the ratio of matching n-grams to the total number of n-grams in the system output  we obtain the precision pn for each n-gram order n. These values for n-gram precision are combined into a BLEU score: The formula for the BLEU metric also includes a brevity penalty for too short output  which is based on the total number of words in the system output c and in the reference r. BLEU is sensitive to tokenization.</S>",['Method_Citation']
6,W06-3114,W07-0738,0,2006,0,"Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MTquality indicator","Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MTquality indicator","['140', '38', '39', '56', '130']","<S sid =""140"" ssid = ""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S><S sid =""38"" ssid = ""4"">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S><S sid =""39"" ssid = ""5"">However  a recent study (Callison-Burch et al.  2006)  pointed out that this correlation may not always be strong.</S><S sid =""56"" ssid = ""22"">The bootstrap method has been critized by Riezler and Maxwell (2005) and Collins et al. (2005)  as being too optimistic in deciding for statistical significant difference between systems.</S><S sid =""130"" ssid = ""23"">This is demonstrated by average scores over all systems  in terms of BLEU  fluency and adequacy  as displayed in Figure 5.</S>",['Method_Citation']
7,W06-3114,W07-0738,0,2006,0,"For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001)","For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001)","['140', '56', '172', '38', '39']","<S sid =""140"" ssid = ""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S><S sid =""56"" ssid = ""22"">The bootstrap method has been critized by Riezler and Maxwell (2005) and Collins et al. (2005)  as being too optimistic in deciding for statistical significant difference between systems.</S><S sid =""172"" ssid = ""3"">Due to many similarly performing systems  we are not able to draw strong conclusions on the question of correlation of manual and automatic evaluation metrics.</S><S sid =""38"" ssid = ""4"">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S><S sid =""39"" ssid = ""5"">However  a recent study (Callison-Burch et al.  2006)  pointed out that this correlation may not always be strong.</S>",['Method_Citation']
8,W06-3114,W07-0738,0,2006,0,Wepresenta comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006) (see Section 3),We present a comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006) (see Section 3),"['154', '63', '39', '171', '172']","<S sid =""154"" ssid = ""47"">While we used the standard metrics of the community  the we way presented translations and prompted for assessment differed from other evaluation campaigns.</S><S sid =""63"" ssid = ""2"">Many human evaluation metrics have been proposed.</S><S sid =""39"" ssid = ""5"">However  a recent study (Callison-Burch et al.  2006)  pointed out that this correlation may not always be strong.</S><S sid =""171"" ssid = ""2"">While many systems had similar performance  the results offer interesting insights  especially about the relative performance of statistical and rule-based systems.</S><S sid =""172"" ssid = ""3"">Due to many similarly performing systems  we are not able to draw strong conclusions on the question of correlation of manual and automatic evaluation metrics.</S>",['Method_Citation']
9,W06-3114,W07-0738,0,2006,0,Weanalyze some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006),We analyze some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006),"['140', '39', '63', '83', '56']","<S sid =""140"" ssid = ""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S><S sid =""39"" ssid = ""5"">However  a recent study (Callison-Burch et al.  2006)  pointed out that this correlation may not always be strong.</S><S sid =""63"" ssid = ""2"">Many human evaluation metrics have been proposed.</S><S sid =""83"" ssid = ""22"">The number of judgements is additionally fragmented by our breakup of sentences into in-domain and out-of-domain.</S><S sid =""56"" ssid = ""22"">The bootstrap method has been critized by Riezler and Maxwell (2005) and Collins et al. (2005)  as being too optimistic in deciding for statistical significant difference between systems.</S>",['Method_Citation']
10,W06-3114,D07-1030,0,"Koehn and Monz, 2006",0,"We use the same method described in (Koehn and Monz, 2006) to perform the significance test","We use the same method described in (Koehn and Monz, 2006) to perform the significance test","['57', '47', '49', '140', '107']","<S sid =""57"" ssid = ""23"">We are therefore applying a different method  which has been used at the 2005 DARPA/NIST evaluation.</S><S sid =""47"" ssid = ""13"">Because of this  we retokenized and lowercased submitted output with our own tokenizer  which was also used to prepare the training and test data.</S><S sid =""49"" ssid = ""15"">Hence  we use the bootstrap resampling method described by Koehn (2004).</S><S sid =""140"" ssid = ""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S><S sid =""107"" ssid = ""23"">Still  for about good number of sentences  we do have this direct comparison  which allows us to apply the sign test  as described in Section 2.2.</S>",['Method_Citation']
11,W06-3114,D07-1030,0,"Koehn and Monz, 2016",0,"We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006)","We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006)","['123', '42', '118', '68', '106']","<S sid =""123"" ssid = ""16"">For the manual scoring  we can distinguish only half of the systems  both in terms of fluency and adequacy.</S><S sid =""42"" ssid = ""8"">It was our hope that this competition  which included the manual and automatic evaluation of statistical systems and one rulebased commercial system  will give further insight into the relation between automatic and manual evaluation.</S><S sid =""118"" ssid = ""11"">At first glance  we quickly recognize that many systems are scored very similar  both in terms of manual judgement and BLEU.</S><S sid =""68"" ssid = ""7"">We asked participants to each judge 200–300 sentences in terms of fluency and adequacy  the most commonly used manual evaluation metrics.</S><S sid =""106"" ssid = ""22"">Automatic scores are computed on a larger tested than manual scores (3064 sentences vs. 300–400 sentences). collected manual judgements  we do not necessarily have the same sentence judged for both systems (judges evaluate 5 systems out of the 8–10 participating systems).</S>",['Method_Citation']
12,W06-3114,W08-0406,0,"Koehn and Monz, 2017",0,"The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002)","The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002)","['38', '14', '62', '108', '19']","<S sid =""38"" ssid = ""4"">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S><S sid =""14"" ssid = ""7"">There is twice as much language modelling data  since training data for the machine translation system is filtered against sentences of length larger than 40 words.</S><S sid =""62"" ssid = ""1"">While automatic measures are an invaluable tool for the day-to-day development of machine translation systems  they are only a imperfect substitute for human assessment of translation quality  or as the acronym BLEU puts it  a bilingual evaluation understudy.</S><S sid =""108"" ssid = ""1"">The results of the manual and automatic evaluation of the participating system translations is detailed in the figures at the end of this paper.</S><S sid =""19"" ssid = ""12"">We aligned the texts at a sentence level across all four languages  resulting in 1064 sentence per language.</S>",['Method_Citation']
13,W06-3114,W11-1002,0,2006,0,"Callison-Burch et al (2006 )andKoehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality","Callison-Burch et al (2006) and Koehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality","['140', '39', '38', '139', '62']","<S sid =""140"" ssid = ""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S><S sid =""39"" ssid = ""5"">However  a recent study (Callison-Burch et al.  2006)  pointed out that this correlation may not always be strong.</S><S sid =""38"" ssid = ""4"">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S><S sid =""139"" ssid = ""32"">Given the closeness of most systems and the wide over-lapping confidence intervals it is hard to make strong statements about the correlation between human judgements and automatic scoring methods such as BLEU.</S><S sid =""62"" ssid = ""1"">While automatic measures are an invaluable tool for the day-to-day development of machine translation systems  they are only a imperfect substitute for human assessment of translation quality  or as the acronym BLEU puts it  a bilingual evaluation understudy.</S>",['Results_Citation']
14,W06-3114,D07-1091,0,"Koehn and Monz, 2006",0,"The English? German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006)","The English German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006)","['126', '140', '38', '9', '47']","<S sid =""126"" ssid = ""19"">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid =""140"" ssid = ""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S><S sid =""38"" ssid = ""4"">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S><S sid =""9"" ssid = ""2"">Training and testing is based on the Europarl corpus.</S><S sid =""47"" ssid = ""13"">Because of this  we retokenized and lowercased submitted output with our own tokenizer  which was also used to prepare the training and test data.</S>",['Method_Citation']
15,W06-3114,D07-1091,0,"Koehn and Monz, 2006",0,"We report results on the development test set, which is also the out-of-domain test set of the WMT06 workshop shared task (Koehn and Monz, 2006)","We report results on the development test set, which is also the out-of-domain test set of the WMT06 workshop shared task (Koehn and Monz, 2006)","['47', '18', '13', '69', '58']","<S sid =""47"" ssid = ""13"">Because of this  we retokenized and lowercased submitted output with our own tokenizer  which was also used to prepare the training and test data.</S><S sid =""18"" ssid = ""11"">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S><S sid =""13"" ssid = ""6"">We are currently working on a complete open source implementation of a training and decoding system  which should become available over the summer. pus  from which also the in-domain test set is taken.</S><S sid =""69"" ssid = ""8"">We settled on contrastive evaluations of 5 system outputs for a single test sentence.</S><S sid =""58"" ssid = ""24"">We divide up each test set into blocks of 20 sentences (100 blocks for the in-domain test set  53 blocks for the out-of-domain test set)  check for each block  if one system has a higher BLEU score than the other  and then use the sign test.</S>",['Method_Citation']
16,W06-3114,P07-1108,0,"Koehn and Monz, 2006",0,"A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Ma chine Translation (Koehn and Monz, 2006)","A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Machine Translation (Koehn and Monz, 2006)","['170', '62', '38', '90', '64']","<S sid =""170"" ssid = ""1"">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid =""62"" ssid = ""1"">While automatic measures are an invaluable tool for the day-to-day development of machine translation systems  they are only a imperfect substitute for human assessment of translation quality  or as the acronym BLEU puts it  a bilingual evaluation understudy.</S><S sid =""38"" ssid = ""4"">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S><S sid =""90"" ssid = ""6"">Another way to view the judgements is that they are less quality judgements of machine translation systems per se  but rankings of machine translation systems.</S><S sid =""64"" ssid = ""3"">Also  the argument has been made that machine translation performance should be evaluated via task-based evaluation metrics  i.e. how much it assists performing a useful task  such as supporting human translators or aiding the analysis of texts.</S>",['Method_Citation']
18,W06-3114,E12-3010,0,"Koehn and Monz, 2006",0,"For the same reason, human evaluation metrics based on adequacy and fluency were not suitable either (Koehn and Monz, 2006)","For the same reason, human evaluation metrics based on adequacy and fluency were not suitable either (Koehn and Monz, 2006)","['172', '154', '166', '68', '63']","<S sid =""172"" ssid = ""3"">Due to many similarly performing systems  we are not able to draw strong conclusions on the question of correlation of manual and automatic evaluation metrics.</S><S sid =""154"" ssid = ""47"">While we used the standard metrics of the community  the we way presented translations and prompted for assessment differed from other evaluation campaigns.</S><S sid =""166"" ssid = ""59"">Lack of correct reference translations was pointed out as a short-coming of our evaluation.</S><S sid =""68"" ssid = ""7"">We asked participants to each judge 200–300 sentences in terms of fluency and adequacy  the most commonly used manual evaluation metrics.</S><S sid =""63"" ssid = ""2"">Many human evaluation metrics have been proposed.</S>",['Aim_Citation']
19,W06-3114,W09-0402,0,"Koehn and Monz, 2006",0,"The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008)","The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008)","['170', '108', '138', '38', '45']","<S sid =""170"" ssid = ""1"">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid =""108"" ssid = ""1"">The results of the manual and automatic evaluation of the participating system translations is detailed in the figures at the end of this paper.</S><S sid =""138"" ssid = ""31"">Hence  the different averages of manual scores for the different language pairs reflect the behaviour of the judges  not the quality of the systems on different language pairs.</S><S sid =""38"" ssid = ""4"">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S><S sid =""45"" ssid = ""11"">For each sentence  we counted how many n-grams in the system output also occurred in the reference translation.</S>",['Results_Citation']
