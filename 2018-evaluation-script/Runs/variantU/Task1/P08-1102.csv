Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P08-1102,C08-1049,0,2008,0,"Following Jiang et al (2008), we describe segmentation and Joint S& amp; T as below: For a given Chinese sentence appearing as a character sequence: C 1: n= C 1 C 2.","Following Jiang et al (2008), we describe segmentation and Joint S& amp; T as below: For a given Chinese sentence appearing as a character sequence: C 1: n= C 1 C 2.","['22', '25', '4', '9', '37']","<S sid =""22"" ssid = ""18"">2 Segmentation and POS Tagging Given a Chinese character sequence: while the segmentation and POS tagging result can be depicted as: Here  Ci (i = L.n) denotes Chinese character  ti (i = L.m) denotes POS tag  and Cl:r (l < r) denotes character sequence ranges from Cl to Cr.</S><S sid =""25"" ssid = ""21"">According to Ng and Low (2004)  the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S><S sid =""4"" ssid = ""4"">On the Penn Chinese Treebank 5.0  we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.</S><S sid =""9"" ssid = ""5"">To segment and tag a character sequence  there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T).</S><S sid =""37"" ssid = ""9"">C represents a Chinese character while the subscript of C indicates its position in the sentence relative to the current character (it has the subscript 0).</S>",['Results_Citation']
2,P08-1102,C08-1049,0,2008,0,"As described in Ng and Low (2004 )andJiang et al (2008), we use s indicating a single character word, while b, m and e indicating the be gin, middle and end of a word respectively","As described in Ng and Low (2004) and Jiang et al (2008), we use s indicating a single character word, while b, m and e indicating the be gin, middle and end of a word respectively","['25', '31', '38', '11', '10']","<S sid =""25"" ssid = ""21"">According to Ng and Low (2004)  the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S><S sid =""31"" ssid = ""3"">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S><S sid =""38"" ssid = ""10"">Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target.</S><S sid =""11"" ssid = ""7"">Compared to performing segmentation and POS tagging one at a time  Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging (Ng and Low  2004).</S><S sid =""10"" ssid = ""6"">Since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a boundary tag (Xue and Shen  2003)  Joint S&T can be conducted in a labelling fashion by expanding boundary tags to include POS information (Ng and Low  2004).</S>",['Method_Citation']
3,P08-1102,C08-1049,0,2008,0,plates called lexical-target in the column below areintroduced by Jiang et al (2008),plates called lexical-target in the column below are introduced by Jiang et al (2008),"['38', '40', '113', '101', '34']","<S sid =""38"" ssid = ""10"">Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target.</S><S sid =""40"" ssid = ""12"">Templates in the column below are expanded from the upper ones.</S><S sid =""113"" ssid = ""24"">Besides this perceptron  other sub-models are trained and used as additional features of the outside-layer linear model.</S><S sid =""101"" ssid = ""12"">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S><S sid =""34"" ssid = ""6"">The feature templates we adopted are selected from those of Ng and Low (2004).</S>",['Method_Citation']
4,P08-1102,P12-1110,0,2008,0,"For CTB-5, we refer to the split by Duan et al (2007) as CTB-5d, and to the split by Jiang et al (2008) as CTB-5j","For CTB-5, we refer to the split by Duan et al (2007) as CTB-5d, and to the split by Jiang et al (2008) as CTB-5j","['4', '9', '25', '23', '1']","<S sid =""4"" ssid = ""4"">On the Penn Chinese Treebank 5.0  we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.</S><S sid =""9"" ssid = ""5"">To segment and tag a character sequence  there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T).</S><S sid =""25"" ssid = ""21"">According to Ng and Low (2004)  the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S><S sid =""23"" ssid = ""19"">We can see that segmentation and POS tagging task is to divide a character sequence into several subsequences and label each of them a POS tag.</S><S sid =""1"" ssid = ""1"">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S>",['Method_Citation']
5,P08-1102,D12-1126,0,2008,0,Jiang et al (2008) proposes a cascaded linear model for joint Chinese word segmentation and POS tagging,Jiang et al (2008) proposes a cascaded linear model for joint Chinese word segmentation and POS tagging,"['0', '1', '31', '4', '9']","<S sid =""0"" ssid = ""0"">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S sid =""1"" ssid = ""1"">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S sid =""31"" ssid = ""3"">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S><S sid =""4"" ssid = ""4"">On the Penn Chinese Treebank 5.0  we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.</S><S sid =""9"" ssid = ""5"">To segment and tag a character sequence  there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T).</S>",['Method_Citation']
6,P08-1102,C10-1135,0,2008,0,"We use the feature templates the same as Jiang et al, (2008) to extract features form E model","We use the feature templates the same as Jiang et al, (2008) to extract features form E model","['33', '34', '119', '118', '75']","<S sid =""33"" ssid = ""5"">In following subsections  we describe the feature templates and the perceptron training algorithm.</S><S sid =""34"" ssid = ""6"">The feature templates we adopted are selected from those of Ng and Low (2004).</S><S sid =""119"" ssid = ""30"">We also find that the perceptron model functions as the kernel of the outside-layer linear model.</S><S sid =""118"" ssid = ""29"">We find that the cascaded model achieves a F-measure increment of about 0.5 points on segmentation and about 0.9 points on Joint S&T  over the perceptron-only model POS+.</S><S sid =""75"" ssid = ""26"">In the rest of the paper  we will call them labelling model and generating model respectively.</S>",['Method_Citation']
8,P08-1102,P12-1025,0,"Jiangetal., 2008a",0,"approach, where basic processing units are characters which compose words (Jiangetal., 2008a)","basic processing units are characters which compose words (Jiangetal., 2008a)","['5', '86', '85', '33', '78']","<S sid =""5"" ssid = ""1"">Word segmentation and part-of-speech (POS) tagging are important tasks in computer processing of Chinese and other Asian languages.</S><S sid =""86"" ssid = ""11"">Line 4 scans words of all possible lengths l (l = 1.. min(i  K)  where i points to the current considering character).</S><S sid =""85"" ssid = ""10"">Lines 3 â€” 11 generate a N-best list for each character position i.</S><S sid =""33"" ssid = ""5"">In following subsections  we describe the feature templates and the perceptron training algorithm.</S><S sid =""78"" ssid = ""3"">Given a Chinese character sequence C1:n  the decoding procedure can proceed in a left-right fashion with a dynamic programming approach.</S>",['Method_Citation']
9,P08-1102,C10-2096,0,2008b,0,"The solid lines show the 1-best result, which is wrong. Jiang et al (2008b) stress the problems in re ranking phase","The solid lines show the 1-best result, which is wrong. Jiang et al (2008b) stress the problems in re ranking phase","['4', '92', '129', '11', '97']","<S sid =""4"" ssid = ""4"">On the Penn Chinese Treebank 5.0  we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.</S><S sid =""92"" ssid = ""3"">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.</S><S sid =""129"" ssid = ""40"">Experimental results show that  it achieves obvious improvement over the perceptron-only model  about from 0.973 to 0.978 on segmentation  and from 0.925 to 0.934 on Joint S&T  with error reductions of 18.5% and 12% respectively.</S><S sid =""11"" ssid = ""7"">Compared to performing segmentation and POS tagging one at a time  Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging (Ng and Low  2004).</S><S sid =""97"" ssid = ""8"">Figure 3 shows their learning curves depicting the F-measure on the development set after 1 to 10 training iterations.</S>",['Method_Citation']
10,P08-1102,C10-2096,0,"Jiang et al, 2008a",0,"6.1.1 Baseline Forest-based System We first segment the Chinese sentences into the1-best segmentations using a state-of-the-art system (Jiang et al, 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results","We first segment the Chinese sentences into the 1-best segmentations using a state-of-the-art system (Jiang et al, 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results","['26', '10', '4', '1', '25']","<S sid =""26"" ssid = ""22"">In order to perform POS tagging at the same time  we expand boundary tags to include POS information by attaching a POS to the tail of a boundary tag as a postfix following Ng and Low (2004).</S><S sid =""10"" ssid = ""6"">Since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a boundary tag (Xue and Shen  2003)  Joint S&T can be conducted in a labelling fashion by expanding boundary tags to include POS information (Ng and Low  2004).</S><S sid =""4"" ssid = ""4"">On the Penn Chinese Treebank 5.0  we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.</S><S sid =""1"" ssid = ""1"">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S sid =""25"" ssid = ""21"">According to Ng and Low (2004)  the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S>",['Method_Citation']
11,P08-1102,C10-2096,0,"Jiang et al, 2008a",0,"6.1.2 Lattice-forest SystemWe first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al, 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm","We first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al, 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm","['23', '4', '25', '26', '1']","<S sid =""23"" ssid = ""19"">We can see that segmentation and POS tagging task is to divide a character sequence into several subsequences and label each of them a POS tag.</S><S sid =""4"" ssid = ""4"">On the Penn Chinese Treebank 5.0  we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.</S><S sid =""25"" ssid = ""21"">According to Ng and Low (2004)  the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S><S sid =""26"" ssid = ""22"">In order to perform POS tagging at the same time  we expand boundary tags to include POS information by attaching a POS to the tail of a boundary tag as a postfix following Ng and Low (2004).</S><S sid =""1"" ssid = ""1"">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S>",['Method_Citation']
12,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"However, when we repeat the work of (Jiang et al, 2008), which reports to achieve the state-of-art performance in the data-sets that we adopt, it has been found that some features (e.g., C0) are unnoticeably trained several times in their model (which are implicitly generated from different feature templates used in the paper)","However, when we repeat the work of (Jiang et al, 2008), which reports to achieve the state-of-art performance in the data-sets that we adopt, it has been found that some features (e.g., C0) are unnoticeably trained several times in their model (which are implicitly generated from different feature templates used in the paper)","['58', '63', '134', '96', '2']","<S sid =""58"" ssid = ""9"">Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.</S><S sid =""63"" ssid = ""14"">As all the sub-models  including the perceptron  are regarded as separate features of the outside-layer linear model  we can train them respectively with special algorithms.</S><S sid =""134"" ssid = ""5"">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S><S sid =""96"" ssid = ""7"">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus  we randomly chosen 2  000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84  294 sentences)  then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S><S sid =""2"" ssid = ""2"">With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S>",['Method_Citation']
13,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"Unicode/CP936 1.1M/55K 104K/13K 0.035 Table 3: Corpus statistics for the second SIGHAN Bakeoff appears twice, which is generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al, 2008), with n=0, generates [C0C0])","Table 3: Corpus statistics for the second SIGHAN Bakeoff appears twice, which is generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al, 2008), with n=0, generates [C0C0])","['118', '6', '129', '58', '100']","<S sid =""118"" ssid = ""29"">We find that the cascaded model achieves a F-measure increment of about 0.5 points on segmentation and about 0.9 points on Joint S&T  over the perceptron-only model POS+.</S><S sid =""6"" ssid = ""2"">Several models were introduced for these problems  for example  the Hidden Markov Model (HMM) (Rabiner  1989)  Maximum Entropy Model (ME) (Ratnaparkhi and Adwait  1996)  and Conditional Random Fields (CRFs) (Lafferty et al.  2001).</S><S sid =""129"" ssid = ""40"">Experimental results show that  it achieves obvious improvement over the perceptron-only model  about from 0.973 to 0.978 on segmentation  and from 0.925 to 0.934 on Joint S&T  with error reductions of 18.5% and 12% respectively.</S><S sid =""58"" ssid = ""9"">Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.</S><S sid =""100"" ssid = ""11"">Test results listed in Table 2 shows that this model obtains higher accuracy than the best of SIGHAN Bakeoff 2 in three corpora (AS  CityU and MSR).</S>",['Method_Citation']
14,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"As all the features adopted in (Jiang et al, 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be? n?, at least in principle","As all the features adopted in (Jiang et al, 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be n, at least in principle","['28', '50', '63', '14', '7']","<S sid =""28"" ssid = ""24"">A subsequence of boundary-POS labelling result indicates a word with POS t only if the boundary tag sequence composed of its boundary part conforms to s or bm*e style  and all POS tags in its POS part equal to t. For example  a tag sequence b NN m NN e NN represents a threecharacter word with POS tag NN.</S><S sid =""50"" ssid = ""1"">In theory  any useful knowledge can be incorporated into the perceptron directly  besides the characterbased features already adopted.</S><S sid =""63"" ssid = ""14"">As all the sub-models  including the perceptron  are regarded as separate features of the outside-layer linear model  we can train them respectively with special algorithms.</S><S sid =""14"" ssid = ""10"">As a result  many theoretically useful features such as higherorder word or POS n-grams are difficult to be incorporated in the model efficiently.</S><S sid =""7"" ssid = ""3"">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S>",['Results_Citation']
15,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"Inspired by (Jiang et al, 2008), we set the real d Although Table 5 has shown that the proposed all the value of C0 to be 2.0, the value of C-1C0anC0C1 to be 3.0, and the values of all other features to be 1.0 for the character-based discriminative-plus model","Inspired by (Jiang et al, 2008), we set the real d Although Table 5 has shown that the proposed all the value of C0 to be 2.0, the value of C-1C0anC0C1 to be 3.0, and the values of all other features to be 1.0 for the character-based discriminative-plus model","['118', '116', '96', '16', '57']","<S sid =""118"" ssid = ""29"">We find that the cascaded model achieves a F-measure increment of about 0.5 points on segmentation and about 0.9 points on Joint S&T  over the perceptron-only model POS+.</S><S sid =""116"" ssid = ""27"">In order to inspect how much improvement each feature brings into the cascaded model  every time we removed a feature while retaining others  then retrained the model and tested its performance on the test set.</S><S sid =""96"" ssid = ""7"">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus  we randomly chosen 2  000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84  294 sentences)  then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S><S sid =""16"" ssid = ""12"">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S><S sid =""57"" ssid = ""8"">It has a two-layer architecture  with a perceptron as the core and another linear model as the outside-layer.</S>",['Method_Citation']
17,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"Last, (Jiang et al, 2008) 5 adds repeated features implicitly based on (Ng and Low, 2004)","Last, (Jiang et al, 2008) 5 adds repeated features implicitly based on (Ng and Low, 2004)","['34', '38', '43', '31', '113']","<S sid =""34"" ssid = ""6"">The feature templates we adopted are selected from those of Ng and Low (2004).</S><S sid =""38"" ssid = ""10"">Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target.</S><S sid =""43"" ssid = ""15"">Note that the templates of Ng and Low (2004) have already contained some lexical-target ones.</S><S sid =""31"" ssid = ""3"">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S><S sid =""113"" ssid = ""24"">Besides this perceptron  other sub-models are trained and used as additional features of the outside-layer linear model.</S>",['Method_Citation']
20,P08-1102,D12-1046,0,Jiang et al2008a,0,"Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a) ,perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b)","Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a), perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b)","['31', '0', '3', '1', '10']","<S sid =""31"" ssid = ""3"">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S><S sid =""0"" ssid = ""0"">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S sid =""3"" ssid = ""3"">Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.</S><S sid =""1"" ssid = ""1"">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S sid =""10"" ssid = ""6"">Since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a boundary tag (Xue and Shen  2003)  Joint S&T can be conducted in a labelling fashion by expanding boundary tags to include POS information (Ng and Low  2004).</S>",['Method_Citation']
