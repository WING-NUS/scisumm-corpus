Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,W99-0613,N01-1023,0,"Collins and Singer, 1999",0,"Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named entity identification (Collins and Singer, 1999)","Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named entity identification (Collins and Singer, 1999)","['79', '140', '5', '27', '33']","<S sid =""79"" ssid = ""12"">2 We now introduce a new algorithm for learning from unlabeled examples  which we will call DLCoTrain (DL stands for decision list  the term Cotrain is taken from (Blum and Mitchell 98)).</S><S sid =""140"" ssid = ""7"">AdaBoost was first introduced in (Freund and Schapire 97); (Schapire and Singer 98) gave a generalization of AdaBoost which we will use in this paper.</S><S sid =""5"" ssid = ""5"">The first method uses a similar algorithm to that of (Yarowsky 95)  with modifications motivated by (Blum and Mitchell 98).</S><S sid =""27"" ssid = ""21"">The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).</S><S sid =""33"" ssid = ""27"">The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).</S>",['Results_Citation']
2,W99-0613,N01-1023,0,"Collins and Singer, 1999",0,"They also discuss an application of classifying web pages by using their method of mutually constrained models. (Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms toAdaBoost which force the classifiers to agree (called Co Boosting)","(Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms to AdaBoost which force the classifiers to agree (called Co Boosting)","['33', '174', '6', '136', '30']","<S sid =""33"" ssid = ""27"">The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).</S><S sid =""174"" ssid = ""41"">The two new terms force the two classifiers to agree  as much as possible  on the unlabeled examples.</S><S sid =""6"" ssid = ""6"">The second algorithm extends ideas from boosting algorithms  designed for supervised learning tasks  to the framework suggested by (Blum and Mitchell 98).</S><S sid =""136"" ssid = ""3"">We then discuss how we adapt and generalize a boosting algorithm  AdaBoost  to the problem of named entity classification.</S><S sid =""30"" ssid = ""24"">(Blum and Mitchell 98) offer a promising formulation of redundancy  also prove some results about how the use of unlabeled examples can help classification  and suggest an objective function when training with unlabeled examples.</S>",['Method_Citation']
3,W99-0613,W03-1509,0,Collins and Singer 1999,0,"Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick, et al 1999] and so on","Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick, et al 1999] and so on","['8', '31', '6', '123', '5']","<S sid =""8"" ssid = ""2"">Recent results (e.g.  (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.</S><S sid =""31"" ssid = ""25"">Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).</S><S sid =""6"" ssid = ""6"">The second algorithm extends ideas from boosting algorithms  designed for supervised learning tasks  to the framework suggested by (Blum and Mitchell 98).</S><S sid =""123"" ssid = ""56"">Limitations of (Blum and Mitchell 98): While the assumptions of (Blum and Mitchell 98) are useful in developing both theoretical results and an intuition for the problem  the assumptions are quite limited.</S><S sid =""5"" ssid = ""5"">The first method uses a similar algorithm to that of (Yarowsky 95)  with modifications motivated by (Blum and Mitchell 98).</S>",['Method_Citation']
4,W99-0613,C02-1154,0,"Collins and Singer, 1999",0,"DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syn tactically analyzed corpus","DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syntactically analyzed corpus","['47', '0', '50', '140', '56']","<S sid =""47"" ssid = ""1"">971 746 sentences of New York Times text were parsed using the parser of (Collins 96).1 Word sequences that met the following criteria were then extracted as named entity examples: whose head is a singular noun (tagged NN).</S><S sid =""0"" ssid = ""0"">Unsupervised Models for Named Entity Classification Collins</S><S sid =""50"" ssid = ""4"">It is a sequence of proper nouns within an NP; its last word Cooper is the head of the NP; and the NP has an appositive modifier (a vice president at S.&P.) whose head is a singular noun (president).</S><S sid =""140"" ssid = ""7"">AdaBoost was first introduced in (Freund and Schapire 97); (Schapire and Singer 98) gave a generalization of AdaBoost which we will use in this paper.</S><S sid =""56"" ssid = ""10"">In the appositive case  the contextual predictor was the head of the modifying appositive (president in the Maury Cooper example); in the second case  the contextual predictor was the preposition together with the noun it modifies (plant_in in the Georgia example).</S>",['Method_Citation']
5,W99-0613,C02-1154,0,"Collins and Singer, 1999",0,"(Collins and Singer, 1999) also makes use of competing categories (person, organization, and location), which cover 96% of all the instances itset out to classify","(Collins and Singer, 1999) also makes use of competing categories (person, organization, and location), which cover 96% of all the instances it set out to classify","['237', '236', '140', '42', '0']","<S sid =""237"" ssid = ""4"">The numbers falling into the location  person  organization categories were 186  289 and 402 respectively.</S><S sid =""236"" ssid = ""3"">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S><S sid =""140"" ssid = ""7"">AdaBoost was first introduced in (Freund and Schapire 97); (Schapire and Singer 98) gave a generalization of AdaBoost which we will use in this paper.</S><S sid =""42"" ssid = ""36"">(Riloff and Shepherd 97) describe a bootstrapping approach for acquiring nouns in particular categories (such as &quot;vehicle&quot; or &quot;weapon&quot; categories).</S><S sid =""0"" ssid = ""0"">Unsupervised Models for Named Entity Classification Collins</S>",['Method_Citation']
6,W99-0613,W06-2204,0,"Collins and Singer, 1999",0,"In (Collins and Singer, 1999) Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification","In (Collins and Singer, 1999) Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification","['0', '11', '136', '204', '8']","<S sid =""0"" ssid = ""0"">Unsupervised Models for Named Entity Classification Collins</S><S sid =""11"" ssid = ""5"">For example  a good classifier would identify Mrs. Frank as a person  Steptoe & Johnson as a company  and Honduras as a location.</S><S sid =""136"" ssid = ""3"">We then discuss how we adapt and generalize a boosting algorithm  AdaBoost  to the problem of named entity classification.</S><S sid =""204"" ssid = ""71"">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid =""8"" ssid = ""2"">Recent results (e.g.  (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.</S>",['Method_Citation']
8,W99-0613,W03-1022,0,"Collins and Singer, 1999",0,"Collins and Singer (1999) for example report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999)","Collins and Singer (1999) for example report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999)","['0', '237', '236', '140', '227']","<S sid =""0"" ssid = ""0"">Unsupervised Models for Named Entity Classification Collins</S><S sid =""237"" ssid = ""4"">The numbers falling into the location  person  organization categories were 186  289 and 402 respectively.</S><S sid =""236"" ssid = ""3"">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S><S sid =""140"" ssid = ""7"">AdaBoost was first introduced in (Freund and Schapire 97); (Schapire and Singer 98) gave a generalization of AdaBoost which we will use in this paper.</S><S sid =""227"" ssid = ""6"">The likelihood of the observed data under the model is where P(yi  xi) is defined as in (9).</S>",['Method_Citation']
9,W99-0613,E09-1018,0,"Collinsand Singer, 1999",0,"While EM has worked quite well for a few tasks, notably ma chine translations (starting with the IBM models 1-5 (Brown et al, 1993), it has not had success inmost others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collinsand Singer, 1999) and context-free-grammar induction (numerous attempts, too many to mention)","While EM has worked quite well for a few tasks, notably machine translations (starting with the IBM models 1-5 (Brown et al, 1993), it has not had success in most others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collinsand Singer, 1999) and context-free-grammar induction (numerous attempts, too many to mention)","['16', '30', '8', '7', '223']","<S sid =""16"" ssid = ""10"">Supervised methods have been applied quite successfully to the full MUC named-entity task (Bikel et al. 97).</S><S sid =""30"" ssid = ""24"">(Blum and Mitchell 98) offer a promising formulation of redundancy  also prove some results about how the use of unlabeled examples can help classification  and suggest an objective function when training with unlabeled examples.</S><S sid =""8"" ssid = ""2"">Recent results (e.g.  (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.</S><S sid =""7"" ssid = ""1"">Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision  in the form of labeled training examples.</S><S sid =""223"" ssid = ""2"">A generative model was applied (similar to naive Bayes) with the three labels as hidden vanables on unlabeled examples  and observed variables on (seed) labeled examples.</S>",['Method_Citation']
11,W99-0613,W07-1712,0,"Collins and Singer, 1999",0,"In addition, we would also like to explore the semi-supervised techniques such as co-training and self-training (Collins and Singer, 1999)","In addition, we would also like to explore the semi-supervised techniques such as co-training and self-training (Collins and Singer, 1999)","['251', '156', '110', '253', '125']","<S sid =""251"" ssid = ""2"">In addition to a heuristic based on decision list learning  we also presented a boosting-like framework that builds on ideas from (Blum and Mitchell 98).</S><S sid =""156"" ssid = ""23"">Our derivation is slightly different from the one presented in (Schapire and Singer 98) as we restrict at to be positive.</S><S sid =""110"" ssid = ""43"">(Blum and Mitchell 98) give an example that illustrates just how powerful the second constraint can be.</S><S sid =""253"" ssid = ""4"">We are currently exploring other methods that employ similar ideas and their formal properties.</S><S sid =""125"" ssid = ""58"">It may be more realistic to replace the second criteria with a softer one  for example (Blum and Mitchell 98) suggest the alternative Alternatively  if Ii and 12 are probabilistic learners  it might make sense to encode the second constraint as one of minimizing some measure of the distance between the distributions given by the two learners.</S>",['Method_Citation']
12,W99-0613,W09-2208,0,"Collins and Singer, 1999",0,"Collins et al (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky ?smethod (Yarowsky, 1995) and the framework suggested by (Blum and Mitchell, 1998)","Collins et al (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky's method (Yarowsky, 1995) and the framework suggested by (Blum and Mitchell, 1998)","['31', '5', '33', '6', '123']","<S sid =""31"" ssid = ""25"">Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).</S><S sid =""5"" ssid = ""5"">The first method uses a similar algorithm to that of (Yarowsky 95)  with modifications motivated by (Blum and Mitchell 98).</S><S sid =""33"" ssid = ""27"">The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).</S><S sid =""6"" ssid = ""6"">The second algorithm extends ideas from boosting algorithms  designed for supervised learning tasks  to the framework suggested by (Blum and Mitchell 98).</S><S sid =""123"" ssid = ""56"">Limitations of (Blum and Mitchell 98): While the assumptions of (Blum and Mitchell 98) are useful in developing both theoretical results and an intuition for the problem  the assumptions are quite limited.</S>",['Method_Citation']
13,W99-0613,W06-2207,0,"Collins and Singer, 1999",0,"This approach was shown to perform well on real-world natural language problems (Collins and Singer, 1999)","This approach was shown to perform well on real-world natural language problems (Collins and Singer, 1999)","['204', '140', '203', '123', '141']","<S sid =""204"" ssid = ""71"">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid =""140"" ssid = ""7"">AdaBoost was first introduced in (Freund and Schapire 97); (Schapire and Singer 98) gave a generalization of AdaBoost which we will use in this paper.</S><S sid =""203"" ssid = ""70"">Several extensions of AdaBoost for multiclass problems have been suggested (Freund and Schapire 97; Schapire and Singer 98).</S><S sid =""123"" ssid = ""56"">Limitations of (Blum and Mitchell 98): While the assumptions of (Blum and Mitchell 98) are useful in developing both theoretical results and an intuition for the problem  the assumptions are quite limited.</S><S sid =""141"" ssid = ""8"">For a description of the application of AdaBoost to various NLP problems see the paper by Abney  Schapire  and Singer in this volume.</S>",['Method_Citation']
15,W99-0613,W06-2207,0,"Collins and Singer, 1999",0,"(6) Similarly to (Collins and Singer, 1999) we used T= 0.95 for all experiments reported here","(6) Similarly to (Collins and Singer, 1999) we used T= 0.95 for all experiments reported here","['140', '204', '156', '5', '203']","<S sid =""140"" ssid = ""7"">AdaBoost was first introduced in (Freund and Schapire 97); (Schapire and Singer 98) gave a generalization of AdaBoost which we will use in this paper.</S><S sid =""204"" ssid = ""71"">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid =""156"" ssid = ""23"">Our derivation is slightly different from the one presented in (Schapire and Singer 98) as we restrict at to be positive.</S><S sid =""5"" ssid = ""5"">The first method uses a similar algorithm to that of (Yarowsky 95)  with modifications motivated by (Blum and Mitchell 98).</S><S sid =""203"" ssid = ""70"">Several extensions of AdaBoost for multiclass problems have been suggested (Freund and Schapire 97; Schapire and Singer 98).</S>",['Method_Citation']
16,W99-0613,P12-1065,0,1999,0,We use Collins and Singer (1999) for our exact specification of Yarowsky.2 It uses DL rule scores ?fj?| ?fj |+ |? f |+ L (1) where  is a smoothing constant,We use Collins and Singer (1999) for our exact specification of Yarowsky,"['156', '140', '204', '31', '154']","<S sid =""156"" ssid = ""23"">Our derivation is slightly different from the one presented in (Schapire and Singer 98) as we restrict at to be positive.</S><S sid =""140"" ssid = ""7"">AdaBoost was first introduced in (Freund and Schapire 97); (Schapire and Singer 98) gave a generalization of AdaBoost which we will use in this paper.</S><S sid =""204"" ssid = ""71"">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid =""31"" ssid = ""25"">Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).</S><S sid =""154"" ssid = ""21"">In our implementation  we make perhaps the simplest choice of weak hypothesis.</S>",['Results_Citation']
