Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,J01-2004,W05-0104,0,2001,0,"Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark, see Roark (2001))","Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark, see Roark (2001))","['40', '15', '17', '132', '88']","<S sid =""40"" ssid = ""28"">The following section will provide some background in probabilistic context-free grammars and language modeling for speech recognition.</S><S sid =""15"" ssid = ""3"">In the past few years  however  some improvements have been made over these language models through the use of statistical methods of natural language processing  and the development of innovative  linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.</S><S sid =""17"" ssid = ""5"">This paper will examine language modeling for speech recognition from a natural language processing point of view.</S><S sid =""132"" ssid = ""36"">The rest of this paper will present our parsing model  its application to language modeling for speech recognition  and empirical results.</S><S sid =""88"" ssid = ""46"">This section will briefly introduce language modeling for statistical speech recognition.'</S>",['Results_Citation']
2,J01-2004,P08-1013,0,2001,0,"Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition. These models have in common that they explicitly or implicitly use a context-free grammar induced from a tree bank, with the exception of Chelba and Jelinek (2000)",Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition,"['40', '15', '132', '17', '247']","<S sid =""40"" ssid = ""28"">The following section will provide some background in probabilistic context-free grammars and language modeling for speech recognition.</S><S sid =""15"" ssid = ""3"">In the past few years  however  some improvements have been made over these language models through the use of statistical methods of natural language processing  and the development of innovative  linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.</S><S sid =""132"" ssid = ""36"">The rest of this paper will present our parsing model  its application to language modeling for speech recognition  and empirical results.</S><S sid =""17"" ssid = ""5"">This paper will examine language modeling for speech recognition from a natural language processing point of view.</S><S sid =""247"" ssid = ""3"">Perplexity is a standard measure within the speech recognition community for comparing language models.</S>",['Method_Citation']
4,J01-2004,P04-1015,0,"Roark, 2001a",0,"Theperceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn tree bank","The perceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn treebank","['10', '4', '364', '19', '11']","<S sid =""10"" ssid = ""4"">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid =""4"" ssid = ""4"">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid =""364"" ssid = ""120"">We follow Chelba (2000) in dealing with this problem: for parsing purposes  we use the Penn Treebank tokenization; for interpolation with the provided trigram model  and for evaluation  the lattice tokenization is used.</S><S sid =""19"" ssid = ""7"">A new language model  based on probabilistic top-down parsing  will be outlined and compared with the previous literature  and extensive empirical results will be presented which demonstrate its utility.</S><S sid =""11"" ssid = ""5"">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S>",['Method_Citation']
5,J01-2004,P04-1015,0,"Roark, 2001a",0,"We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the per ceptron model gives performance competitive to that of the generative model on parsing the Penn tree bank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search","We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the perceptron model gives performance competitive to that of the generative model on parsing the Penn treebank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search","['5', '11', '93', '364', '19']","<S sid =""5"" ssid = ""5"">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S><S sid =""11"" ssid = ""5"">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S><S sid =""93"" ssid = ""51"">The standard language model used in many speech recognition systems is the trigram model  i.e.  a Markov model of order 2  which can be characterized by the following equation: To smooth the trigram models that are used in this paper  we interpolate the probability estimates of higher-order Markov models with lower-order Markov models (Jelinek and Mercer 1980).</S><S sid =""364"" ssid = ""120"">We follow Chelba (2000) in dealing with this problem: for parsing purposes  we use the Penn Treebank tokenization; for interpolation with the provided trigram model  and for evaluation  the lattice tokenization is used.</S><S sid =""19"" ssid = ""7"">A new language model  based on probabilistic top-down parsing  will be outlined and compared with the previous literature  and extensive empirical results will be presented which demonstrate its utility.</S>",['Method_Citation']
6,J01-2004,P04-1015,0,2001a,0,"One way around this problem is to adopt a two-pass approach, where GEN (x) is the top N analyses under some initial model, as in the re ranking approach of Collins (2000) .In the current paper we explore alternatives to rerank ing approaches, namely heuristic methods for finding the arg max, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999)","In the current paper we explore alternatives to reranking approaches, namely heuristic methods for finding the argmax, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999)","['31', '402', '400', '134', '32']","<S sid =""31"" ssid = ""19"">Thus  our top-down parser allows for the incremental calculation of generative conditional word probabilities  a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S><S sid =""402"" ssid = ""15"">In fact  left-corner parsing can be simulated by a top-down parser by transforming the grammar  as was done in Roark and Johnson (1999)  and so an approach very similar to the one outlined here could be used in that case.</S><S sid =""400"" ssid = ""13"">Other parsing approaches might also be used in the way that we have used a topdown parser.</S><S sid =""134"" ssid = ""38"">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid =""32"" ssid = ""20"">A second key feature of our approach is that top-down guidance improves the efficiency of the search as more and more conditioning events are extracted from the derivation for use in the probabilistic model.</S>",['Method_Citation']
7,J01-2004,P04-1015,0,2001a,0,"approach The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights","The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights","['209', '402', '400', '268', '292']","<S sid =""209"" ssid = ""113"">This parser is essentially a stochastic version of the top-down parser described in Aho  Sethi  and Ullman (1986).</S><S sid =""402"" ssid = ""15"">In fact  left-corner parsing can be simulated by a top-down parser by transforming the grammar  as was done in Roark and Johnson (1999)  and so an approach very similar to the one outlined here could be used in that case.</S><S sid =""400"" ssid = ""13"">Other parsing approaches might also be used in the way that we have used a topdown parser.</S><S sid =""268"" ssid = ""24"">This is an incremental parser with a pruning strategy and no backtracking.</S><S sid =""292"" ssid = ""48"">The parser  thus  could be used as a front end to some other model  with the hopes of selecting a more accurate parse from among the final candidates.</S>",['Method_Citation']
9,J01-2004,P04-1015,0,2001a,0,"Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word","Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word","['24', '112', '43', '231', '401']","<S sid =""24"" ssid = ""12"">These parsers can calculate word probabilities based upon the parser state—as in Chelba and Jelinek (1998a)—but such a distribution is not generative from the probabilistic grammar.</S><S sid =""112"" ssid = ""16"">Goddeau (1992) used a robust deterministic shift-reduce parser to condition word probabilities by extracting a specified number of stack entries from the top of the current state  and conditioning on those entries in a way similar to an n-gram.</S><S sid =""43"" ssid = ""1"">This section will introduce probabilistic (or stochastic) context-free grammars (PCFGs)  as well as such notions as complete and partial parse trees  which will be important in defining our language model later in the paper.'</S><S sid =""231"" ssid = ""135"">Since we do not know the POS for the word  we must sum the LAP for all POS For a PCFG G  a stack S = Ao An$ (which we will write AN and a look-ahead terminal item wi  we define the look-ahead probability as follows: We recursively estimate this with two empirically observed conditional probabilities for every nonterminal A : 13(A  w a) and P(A  c).</S><S sid =""401"" ssid = ""14"">Earley and left-corner parsers  as mentioned in the introduction  also have rooted derivations that can be used to calculated generative string prefix probabilities incrementally.</S>",['Method_Citation']
10,J01-2004,P05-1022,0,"Roark, 2001",0,"A good example of this is the Roark parser (Roark, 2001) which works left-to right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning","A good example of this is the Roark parser (Roark, 2001) which works left-to-right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning","['402', '207', '126', '112', '31']","<S sid =""402"" ssid = ""15"">In fact  left-corner parsing can be simulated by a top-down parser by transforming the grammar  as was done in Roark and Johnson (1999)  and so an approach very similar to the one outlined here could be used in that case.</S><S sid =""207"" ssid = ""111"">The model allows us to assign probabilities to derivations  which can be used by the parsing algorithm to decide heuristically which candidates are promising and should be expanded  and which are less promising and should be pruned.</S><S sid =""126"" ssid = ""30"">When all of the parser operations have finished at a particular point in the string  the next word is predicted as follows: For each derivation in the beam  the headwords of the two topmost stack entries form a trigram with the conditioned word.</S><S sid =""112"" ssid = ""16"">Goddeau (1992) used a robust deterministic shift-reduce parser to condition word probabilities by extracting a specified number of stack entries from the top of the current state  and conditioning on those entries in a way similar to an n-gram.</S><S sid =""31"" ssid = ""19"">Thus  our top-down parser allows for the incremental calculation of generative conditional word probabilities  a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>",['Method_Citation']
11,J01-2004,P05-1022,0,"Roark, 2001",0,"At the end one has a beam-width? s number of best parses (Roark, 2001) .The Collins parser (Collins, 1997) does use dynamic programming in its search","At the end one has a beam-width's number of best parses (Roark, 2001)","['291', '227', '292', '282', '361']","<S sid =""291"" ssid = ""47"">Also  the parser returns a set of candidate parses  from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence)  we find an average labeled precision/recall of 94.1  for sentences of length < 100.</S><S sid =""227"" ssid = ""131"">The parse on H i+i with the highest probability is returned for evaluation.</S><S sid =""292"" ssid = ""48"">The parser  thus  could be used as a front end to some other model  with the hopes of selecting a more accurate parse from among the final candidates.</S><S sid =""282"" ssid = ""38"">Like the nonlexicalized parser in Roark and Johnson (1999)  we found that the search efficiency  in terms of number of rule expansions considered or number of analyses advanced  also improved as we increased the amount of conditioning.</S><S sid =""361"" ssid = ""117"">One complicating issue has to do with the tokenization in the Penn Treebank versus that in the HUB1 lattices.</S>",['Method_Citation']
12,J01-2004,P05-1022,0,"Roark, 2001",0,"To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses","To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses","['402', '31', '77', '291', '292']","<S sid =""402"" ssid = ""15"">In fact  left-corner parsing can be simulated by a top-down parser by transforming the grammar  as was done in Roark and Johnson (1999)  and so an approach very similar to the one outlined here could be used in that case.</S><S sid =""31"" ssid = ""19"">Thus  our top-down parser allows for the incremental calculation of generative conditional word probabilities  a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S><S sid =""77"" ssid = ""35"">See that paper for more discussion of the benefits of Two parse trees: (a) a complete left-factored parse tree with epsilon productions and an explicit stop symbol; and (b) a partial left-factored parse tree. factorization for top-down and left-corner parsing.</S><S sid =""291"" ssid = ""47"">Also  the parser returns a set of candidate parses  from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence)  we find an average labeled precision/recall of 94.1  for sentences of length < 100.</S><S sid =""292"" ssid = ""48"">The parser  thus  could be used as a front end to some other model  with the hopes of selecting a more accurate parse from among the final candidates.</S>",['Method_Citation']
13,J01-2004,P04-1006,0,"Roark, 2001",0,"We ran the first stage parser with 4-timesoverparsing for each string in 7The n? best lists were provided by Brian Roark (Roark, 2001) 8A local-tree is an explicit expansion of an edge and its children","The n-best lists were provided by Brian Roark (Roark, 2001)","['243', '76', '115', '340', '134']","<S sid =""243"" ssid = ""147"">The only c-productions are those introduced by left-factorization.</S><S sid =""76"" ssid = ""34"">The leftfactorization transform that we use is identical to what is called right binarization in Roark and Johnson (1999).</S><S sid =""115"" ssid = ""19"">The structured language model (SLM) used in Chelba and Jelinek (1998a  1998b  1999)  Jelinek and Chelba (1999)  and Chelba (2000) is similar to that of Goddeau  except that (i) their shift-reduce parser follows a nondeterministic beam search  and (ii) each stack entry contains  in addition to the nonterminal node label  the headword of the constituent.</S><S sid =""340"" ssid = ""96"">The trigram model was also trained on Sections 00-20 of the C&J corpus.</S><S sid =""134"" ssid = ""38"">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S>",['Method_Citation']
14,J01-2004,P05-1063,0,"Roark, 2001a",0,"Incremental top down and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models","Incremental top-down and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models","['2', '8', '108', '0', '19']","<S sid =""2"" ssid = ""2"">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid =""8"" ssid = ""2"">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid =""108"" ssid = ""12"">Another approach that uses syntactic structure for language modeling has been to use a shift-reduce parser to &quot;surface&quot; c-commanding phrasal headwords or part-of-speech (POS) tags from arbitrarily far back in the prefix string  for use in a trigram-like model.</S><S sid =""0"" ssid = ""0"">Probabilistic Top-Down Parsing and Language Modeling</S><S sid =""19"" ssid = ""7"">A new language model  based on probabilistic top-down parsing  will be outlined and compared with the previous literature  and extensive empirical results will be presented which demonstrate its utility.</S>",['Method_Citation']
15,J01-2004,W10-2009,0,"Roark, 2001",0,"Levy, on the other hand ,argued that studies of probabilistic parsing reveal that typically a small number of analyses are as signed the majority of probability mass (Roark, 2001)","Levy, on the other hand, argued that studies of probabilistic parsing reveal that typically a small number of analyses are assigned the majority of probability mass (Roark, 2001)","['394', '315', '238', '345', '232']","<S sid =""394"" ssid = ""7"">For example  the probability of a preposition is presumably more dependent on a c-commanding head than the probability of a determiner is.</S><S sid =""315"" ssid = ""71"">Since each word is (almost certainly  because of our pruning strategy) losing some probability mass  the probability model is not &quot;proper &quot;—the sum of the probabilities over the vocabulary is less than one.</S><S sid =""238"" ssid = ""142"">Thus  if 100 analyses have already been pushed onto then a candidate analysis must have a probability above 10-5/3 to avoid being pruned.</S><S sid =""345"" ssid = ""101"">This is not surprising  since our conditioning information is in many ways orthogonal to that of the trigram  insofar as it includes the probability mass of the derivations; in contrast  their model in some instances is very close to the trigram  by conditioning on two words in the prefix string  which may happen to be the two adjacent words.</S><S sid =""232"" ssid = ""136"">The same empirical probability  P(A  Xoe)  is collected for every preterminal X as well.</S>",['Results_Citation']
17,J01-2004,D09-1034,0,2001,0,"For example, in Demberg and Keller (2008), trials were run deriving surprisal from the Roark (2001) parser under two different conditions: fully lex icalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags)","For example, in Demberg and Keller (2008), trials were run deriving surprisal from the Roark (2001) parser under two different conditions: fully lexicalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags)","['260', '77', '31', '402', '209']","<S sid =""260"" ssid = ""16"">We will call the manual parse GOLD and the parse that the parser returns TEST.</S><S sid =""77"" ssid = ""35"">See that paper for more discussion of the benefits of Two parse trees: (a) a complete left-factored parse tree with epsilon productions and an explicit stop symbol; and (b) a partial left-factored parse tree. factorization for top-down and left-corner parsing.</S><S sid =""31"" ssid = ""19"">Thus  our top-down parser allows for the incremental calculation of generative conditional word probabilities  a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S><S sid =""402"" ssid = ""15"">In fact  left-corner parsing can be simulated by a top-down parser by transforming the grammar  as was done in Roark and Johnson (1999)  and so an approach very similar to the one outlined here could be used in that case.</S><S sid =""209"" ssid = ""113"">This parser is essentially a stochastic version of the top-down parser described in Aho  Sethi  and Ullman (1986).</S>",['Method_Citation']
18,J01-2004,D09-1034,0,2001,0,"We modified the Roark (2001) parser to calculate the discussed measures 1, and the empirical results in? 4 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time","We modified the Roark (2001) parser to calculate the discussed measures, and the empirical results in ?4 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time","['31', '24', '9', '3', '100']","<S sid =""31"" ssid = ""19"">Thus  our top-down parser allows for the incremental calculation of generative conditional word probabilities  a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S><S sid =""24"" ssid = ""12"">These parsers can calculate word probabilities based upon the parser state—as in Chelba and Jelinek (1998a)—but such a distribution is not generative from the probabilistic grammar.</S><S sid =""9"" ssid = ""3"">A lexicalized probabilistic topdown parser is then presented  which performs very well  in terms of both the accuracy of returned parses and the efficiency with which they are found  relative to the best broad-coverage statistical parsers.</S><S sid =""3"" ssid = ""3"">A lexicalized probabilistic topdown parser is then presented  which performs very well  in terms of both the accuracy of returned parses and the efficiency with which they are found  relative to the best broad-coverage statistical parsers.</S><S sid =""100"" ssid = ""4"">The approach that we will subsequently present uses the probabilistic grammar as its language model  but only includes probability mass from those parses that are found  that is  it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S>",['Method_Citation']
19,J01-2004,D09-1034,0,2001,0,"In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here","In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here","['364', '402', '137', '322', '209']","<S sid =""364"" ssid = ""120"">We follow Chelba (2000) in dealing with this problem: for parsing purposes  we use the Penn Treebank tokenization; for interpolation with the provided trigram model  and for evaluation  the lattice tokenization is used.</S><S sid =""402"" ssid = ""15"">In fact  left-corner parsing can be simulated by a top-down parser by transforming the grammar  as was done in Roark and Johnson (1999)  and so an approach very similar to the one outlined here could be used in that case.</S><S sid =""137"" ssid = ""41"">Here we will present a parser that uses simple search heuristics of this sort without DP.</S><S sid =""322"" ssid = ""78"">Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.</S><S sid =""209"" ssid = ""113"">This parser is essentially a stochastic version of the top-down parser described in Aho  Sethi  and Ullman (1986).</S>",['Method_Citation']
20,J01-2004,D09-1034,0,2001,0,"At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is re quired to calculate such measures. Let H (D) be the entropy over a set of derivations D, calculated as follows: H (D)=? X D? D? (D) P D?? D? (D?) log? (D) P D?? D? (D?) (10) If the set of derivations D= D (G, W [1, i]) is a set of partial derivations for string W [1, i], then H (D) is a measure of uncertainty over the partial derivations ,i.e., the uncertainty regarding the correct analysis of what has already been processed","At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is required to calculate such measures","['126', '269', '25', '303', '24']","<S sid =""126"" ssid = ""30"">When all of the parser operations have finished at a particular point in the string  the next word is predicted as follows: For each derivation in the beam  the headwords of the two topmost stack entries form a trigram with the conditioned word.</S><S sid =""269"" ssid = ""25"">In such a model  it is possible to commit to a set of partial analyses at a particular point that cannot be completed given the rest of the input string (i.e.  the parser can &quot;garden path&quot;).</S><S sid =""25"" ssid = ""13"">A parser that is not left to right  but which has rooted derivations  e.g.  a headfirst parser  will be able to calculate generative joint probabilities for entire strings; however  it will not be able to calculate probabilities for each word conditioned on previously generated words  unless each derivation generates the words in the string in exactly the same order.</S><S sid =""303"" ssid = ""59"">Since this is not an exhaustive search  the parses that are returned will be a subset of the total set of trees that would be used in the exact PCFG estimate; hence the estimate thus arrived at will be bounded above by the probability that would be generated from an exhaustive search.</S><S sid =""24"" ssid = ""12"">These parsers can calculate word probabilities based upon the parser state—as in Chelba and Jelinek (1998a)—but such a distribution is not generative from the probabilistic grammar.</S>",['Aim_Citation']
