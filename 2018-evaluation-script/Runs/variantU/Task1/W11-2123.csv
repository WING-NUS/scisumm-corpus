Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,W11-2123,W11-2138,0,"Heafield, 2011",0,"We used common tools for phrase-based translation? Moses (Koehn et al, 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modelling and GIZA++ (Och and Ney, 2000) for word alignments","We used common tools for phrase-based translation Moses (Koehn et al, 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modelling and GIZA++ (Och and Ney, 2000) for word alignments","['21', '103', '199', '124', '274']","<S sid =""21"" ssid = ""16"">Performance improvements transfer to the Moses (Koehn et al.  2007)  cdec (Dyer et al.  2010)  and Joshua (Li et al.  2009) translation systems where our code has been integrated.</S><S sid =""103"" ssid = ""7"">IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.</S><S sid =""199"" ssid = ""18"">For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid =""124"" ssid = ""28"">Lossy compressed models RandLM (Talbot and Osborne  2007) and Sheffield (Guthrie and Hepple  2010) offer better memory consumption at the expense of CPU and accuracy.</S><S sid =""274"" ssid = ""1"">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S>",['Results_Citation']
2,W11-2123,P14-2022,0,"Heafield, 2011",0,"The language model was com piled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run","The language model was compiled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run","['223', '102', '180', '172', '203']","<S sid =""223"" ssid = ""42"">The binary language model from Section 5.2 and text phrase table were forced into disk cache before each run.</S><S sid =""102"" ssid = ""6"">The PROBING model was designed to improve upon SRILM by using linear probing hash tables (though not arranged in a trie)  allocating memory all at once (eliminating the need for full pointers)  and being easy to compile.</S><S sid =""180"" ssid = ""52"">In all of our experiments  the binary file (whether mapped or  in the case of most other packages  interpreted) is loaded into the disk cache in advance so that lazy mapping will never fault to disk.</S><S sid =""172"" ssid = ""44"">This is especially effective at reducing load time  since raw bytes are read directly to memory—or  as happens with repeatedly used models  are already in the disk cache.</S><S sid =""203"" ssid = ""22"">As noted in Section 4.4  disk cache state is controlled by reading the entire binary file before each test begins.</S>",['Method_Citation']
3,W11-2123,W12-3145,0,"Heafield, 2011",0,"Thus given afragment tf consisting of a sequence of target to kens, we compute LM scores for (i) < s& gt ;tf, (ii )tf and (iii )tf < /s& gt; and use the best score (only) for pruning.2 While this increases the number ofLM queries, we exploit the language model state in formation in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states","Thus given a fragment tf consisting of a sequence of target tokens, we compute LM scores for (i) < s& gt ;tf, (ii )tf and (iii )tf < /s& gt; and use the best score (only) for pruning. While this increases the number of LM queries, we exploit the language model state in formation in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states","['131', '278', '142', '50', '62']","<S sid =""131"" ssid = ""3"">Dynamic programming efficiently scores many hypotheses by exploiting the fact that an N-gram language model conditions on at most N − 1 preceding words.</S><S sid =""278"" ssid = ""5"">We attain these results using several optimizations: hashing  custom lookup tables  bit-level packing  and state for left-to-right query patterns.</S><S sid =""142"" ssid = ""14"">Language models that contain wi must also contain prefixes wi for 1 G i G k. Therefore  when the model is queried for p(wnjwn−1 1 ) but the longest matching suffix is wnf   it may return state s(wn1) = wnf since no longer context will be found.</S><S sid =""50"" ssid = ""28"">Given counts cn1 where e.g. c1 is the vocabulary size  total memory consumption  in bits  is Our PROBING data structure places all n-grams of the same order into a single giant hash table.</S><S sid =""62"" ssid = ""40"">If the key distribution’s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |− 1] to estimate pivots; this optimization alone led to a 24% speed improvement.</S>",['Method_Citation']
4,W11-2123,W12-3131,0,"Heafield, 2011",0,"Our translation system uses cdec (Dyer et al,2010), an implementation of the hierarchical phrase based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference","Our translation system uses cdec (Dyer et al,2010), an implementation of the hierarchical phrase based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference","['199', '152', '7', '103', '218']","<S sid =""199"" ssid = ""18"">For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid =""152"" ssid = ""24"">All language model queries issued by machine translation decoders follow a left-to-right pattern  starting with either the begin of sentence token or null context for mid-sentence fragments.</S><S sid =""7"" ssid = ""2"">This paper presents methods to query N-gram language models  minimizing time and space costs.</S><S sid =""103"" ssid = ""7"">IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.</S><S sid =""218"" ssid = ""37"">We run the baseline Moses system for the French-English track of the 2011 Workshop on Machine Translation 9 translating the 3003-sentence test set.</S>",['Method_Citation']
5,W11-2123,W12-3154,0,"Heafield, 2011",0,"The three data sets in use in this paper are summarised in Table 1.The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al, 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime","The three data sets in use in this paper are summarised in Table 1.The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al, 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime","['244', '199', '95', '82', '152']","<S sid =""244"" ssid = ""63"">Time for Moses itself to load  including loading the language model and phrase table  is included.</S><S sid =""199"" ssid = ""18"">For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid =""95"" ssid = ""73"">Using cn to denote the number of n-grams  total memory consumption of TRIE  in bits  is plus quantization tables  if used.</S><S sid =""82"" ssid = ""60"">Given n-gram counts {cn}Nn=1  we use Flog2 c1] bits per vocabulary identifier and Flog2 cn] per index into the table of ngrams.</S><S sid =""152"" ssid = ""24"">All language model queries issued by machine translation decoders follow a left-to-right pattern  starting with either the begin of sentence token or null context for mid-sentence fragments.</S>",['Method_Citation']
6,W11-2123,P12-2058,0,"Heafield, 2011",0,"The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011)","The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011)","['184', '103', '6', '244', '0']","<S sid =""184"" ssid = ""3"">Sparse lookup is a key subproblem of language model queries.</S><S sid =""103"" ssid = ""7"">IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.</S><S sid =""6"" ssid = ""1"">Language models are widely applied in natural language processing  and applications such as machine translation make very frequent queries.</S><S sid =""244"" ssid = ""63"">Time for Moses itself to load  including loading the language model and phrase table  is included.</S><S sid =""0"" ssid = ""0"">KenLM: Faster and Smaller Language Model Queries</S>",['Method_Citation']
7,W11-2123,W11-2139,0,2011,0,Inference was carried out using the language modeling library described by Heafield (2011),Inference was carried out using the language modeling library described by Heafield (2011),"['103', '68', '131', '239', '16']","<S sid =""103"" ssid = ""7"">IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.</S><S sid =""68"" ssid = ""46"">The trie data structure is commonly used for language modeling.</S><S sid =""131"" ssid = ""3"">Dynamic programming efficiently scores many hypotheses by exploiting the fact that an N-gram language model conditions on at most N − 1 preceding words.</S><S sid =""239"" ssid = ""58"">Overall  language modeling significantly impacts decoder performance.</S><S sid =""16"" ssid = ""11"">BerkeleyLM revision 152 (Pauls and Klein  2011) implements tries based on hash tables and sorted arrays in Java with lossy quantization.</S>",['Method_Citation']
8,W11-2123,P13-2003,0,"Heafield, 2011",0,"We used the MADA ATB segmentation for Arabic (Roth et al, 2008) and true casing for English, phrases of maximal length 7, KneserNey smoothing, and lexicalized reordering (Koehn et al, 2005), and a 5-gram language model, trained on GigaWordv.5 using KenLM (Heafield, 2011)","We used the MADA ATB segmentation for Arabic (Roth et al, 2008) and true casing for English, phrases of maximal length 7, KneserNey smoothing, and lexicalized reordering (Koehn et al, 2005), and a 5-gram language model, trained on GigaWordv.5 using KenLM (Heafield, 2011)","['268', '21', '199', '13', '103']","<S sid =""268"" ssid = ""10"">For example  syntactic decoders (Koehn et al.  2007; Dyer et al.  2010; Li et al.  2009) perform dynamic programming parametrized by both backward- and forward-looking state.</S><S sid =""21"" ssid = ""16"">Performance improvements transfer to the Moses (Koehn et al.  2007)  cdec (Dyer et al.  2010)  and Joshua (Li et al.  2009) translation systems where our code has been integrated.</S><S sid =""199"" ssid = ""18"">For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid =""13"" ssid = ""8"">IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.</S><S sid =""103"" ssid = ""7"">IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.</S>",['Method_Citation']
9,W11-2123,W12-3134,0,2011,0,The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language mod els by Heafield (2011) and Pauls and Klein (2011)? both language model implementations are now integrated with Joshua,The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language models by Heafield (2011) and Pauls and Klein (2011)? both language model implementations are now integrated with Joshua,"['26', '264', '16', '114', '51']","<S sid =""26"" ssid = ""4"">We use two common techniques  hash tables and sorted arrays  describing each before the model that uses the technique.</S><S sid =""264"" ssid = ""6"">For even larger models  storing counts (Talbot and Osborne  2007; Pauls and Klein  2011; Guthrie and Hepple  2010) is a possibility.</S><S sid =""16"" ssid = ""11"">BerkeleyLM revision 152 (Pauls and Klein  2011) implements tries based on hash tables and sorted arrays in Java with lossy quantization.</S><S sid =""114"" ssid = ""18"">Later  BerkeleyLM (Pauls and Klein  2011) described ideas similar to ours.</S><S sid =""51"" ssid = ""29"">This differs from other implementations (Stolcke  2002; Pauls and Klein  2011) that use hash tables as nodes in a trie  as explained in the next section.</S>",['Method_Citation']
10,W11-2123,W12-3134,0,2011,0,Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets,Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets,"['92', '47', '56', '16', '95']","<S sid =""92"" ssid = ""70"">To quantize  we use the binning method (Federico and Bertoldi  2006) that sorts values  divides into equally sized bins  and averages within each bin.</S><S sid =""47"" ssid = ""25"">For 2 < n < N  we use a hash table mapping from the n-gram to the probability and backoff3.</S><S sid =""56"" ssid = ""34"">We reduce this to O(log log |A|) time by evenly distributing keys over their range then using interpolation search4 (Perl et al.  1978).</S><S sid =""16"" ssid = ""11"">BerkeleyLM revision 152 (Pauls and Klein  2011) implements tries based on hash tables and sorted arrays in Java with lossy quantization.</S><S sid =""95"" ssid = ""73"">Using cn to denote the number of n-grams  total memory consumption of TRIE  in bits  is plus quantization tables  if used.</S>",['Method_Citation']
11,W11-2123,W12-3134,0,2011,0,"With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult to compile SRILM toolkit (Stolcke, 2002)","With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult to compile SRILM toolkit (Stolcke, 2002)","['51', '16', '103', '114', '12']","<S sid =""51"" ssid = ""29"">This differs from other implementations (Stolcke  2002; Pauls and Klein  2011) that use hash tables as nodes in a trie  as explained in the next section.</S><S sid =""16"" ssid = ""11"">BerkeleyLM revision 152 (Pauls and Klein  2011) implements tries based on hash tables and sorted arrays in Java with lossy quantization.</S><S sid =""103"" ssid = ""7"">IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.</S><S sid =""114"" ssid = ""18"">Later  BerkeleyLM (Pauls and Klein  2011) described ideas similar to ours.</S><S sid =""12"" ssid = ""7"">Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke  2002) is a popular toolkit based on tries used in several decoders.</S>",['Method_Citation']
12,W11-2123,W12-3160,0,"Heafield, 2011",0,"This was used to create a KenLM (Heafield, 2011)","This was used to create a KenLM (Heafield, 2011)","['114', '76', '113', '17', '264']","<S sid =""114"" ssid = ""18"">Later  BerkeleyLM (Pauls and Klein  2011) described ideas similar to ours.</S><S sid =""76"" ssid = ""54"">This technique was introduced by Clarkson and Rosenfeld (1997) and is also implemented by IRSTLM and BerkeleyLM’s compressed option.</S><S sid =""113"" ssid = ""17"">Our code has been publicly available and intergrated into Moses since October 2010.</S><S sid =""17"" ssid = ""12"">Sheffield Guthrie and Hepple (2010) explore several randomized compression techniques  but did not release code.</S><S sid =""264"" ssid = ""6"">For even larger models  storing counts (Talbot and Osborne  2007; Pauls and Klein  2011; Guthrie and Hepple  2010) is a possibility.</S>",['Method_Citation']
13,W11-2123,W12-3706,0,"Heafield, 2011",0,"In the Opinum system we query the M p, M n mod els with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable fora web application","In the Opinum system we query the M p, M n models with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable for a web application","['1', '0', '121', '154', '115']","<S sid =""1"" ssid = ""1"">We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.</S><S sid =""0"" ssid = ""0"">KenLM: Faster and Smaller Language Model Queries</S><S sid =""121"" ssid = ""25"">A direct-mapped cache makes BerkeleyLM faster on repeated queries  but their fastest (scrolling) cached version is still slower than uncached PROBING  even on cache-friendly queries.</S><S sid =""154"" ssid = ""26"">To optimize left-to-right queries  we extend state to store backoff information: where m is the minimal context from Section 4.1 and b is the backoff penalty.</S><S sid =""115"" ssid = ""19"">Most similar is scrolling queries  wherein left-to-right queries that add one word at a time are optimized.</S>",['Results_Citation']
14,W11-2123,W11-2147,0,"Heafield, 2011",0,"Our base line is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al, 2007) for translation model training and decoding, GIZA++ (Ochand Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights","Our base line is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al, 2007) for translation model training and decoding, GIZA++ (Ochand Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights","['218', '103', '199', '6', '274']","<S sid =""218"" ssid = ""37"">We run the baseline Moses system for the French-English track of the 2011 Workshop on Machine Translation 9 translating the 3003-sentence test set.</S><S sid =""103"" ssid = ""7"">IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.</S><S sid =""199"" ssid = ""18"">For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid =""6"" ssid = ""1"">Language models are widely applied in natural language processing  and applications such as machine translation make very frequent queries.</S><S sid =""274"" ssid = ""1"">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S>",['Method_Citation']
15,W11-2123,E12-1083,0,"Heafield, 2011",0,"For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011)","For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011)","['103', '0', '274', '14', '214']","<S sid =""103"" ssid = ""7"">IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.</S><S sid =""0"" ssid = ""0"">KenLM: Faster and Smaller Language Model Queries</S><S sid =""274"" ssid = ""1"">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S><S sid =""14"" ssid = ""9"">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S><S sid =""214"" ssid = ""33"">For even larger models  we recommend RandLM; the memory consumption of the cache is not expected to grow with model size  and it has been reported to scale well.</S>",['Method_Citation']
16,W11-2123,P12-1002,0,"Heafield, 2011",0,"Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Sima ?an,2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well.3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using kenlm (Heafield, 2011)","Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Simaan,2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well.3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using kenlm (Heafield, 2011)","['82', '152', '50', '51', '199']","<S sid =""82"" ssid = ""60"">Given n-gram counts {cn}Nn=1  we use Flog2 c1] bits per vocabulary identifier and Flog2 cn] per index into the table of ngrams.</S><S sid =""152"" ssid = ""24"">All language model queries issued by machine translation decoders follow a left-to-right pattern  starting with either the begin of sentence token or null context for mid-sentence fragments.</S><S sid =""50"" ssid = ""28"">Given counts cn1 where e.g. c1 is the vocabulary size  total memory consumption  in bits  is Our PROBING data structure places all n-grams of the same order into a single giant hash table.</S><S sid =""51"" ssid = ""29"">This differs from other implementations (Stolcke  2002; Pauls and Klein  2011) that use hash tables as nodes in a trie  as explained in the next section.</S><S sid =""199"" ssid = ""18"">For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>",['Method_Citation']
17,W11-2123,D12-1108,0,"Heafield, 2011",0,"n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3","n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3","['11', '103', '184', '131', '68']","<S sid =""11"" ssid = ""6"">Many packages perform language model queries.</S><S sid =""103"" ssid = ""7"">IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.</S><S sid =""184"" ssid = ""3"">Sparse lookup is a key subproblem of language model queries.</S><S sid =""131"" ssid = ""3"">Dynamic programming efficiently scores many hypotheses by exploiting the fact that an N-gram language model conditions on at most N − 1 preceding words.</S><S sid =""68"" ssid = ""46"">The trie data structure is commonly used for language modeling.</S>",['Aim_Citation']
18,W11-2123,P12-2006,0,"Heafield, 2011",0,"Research efforts to increase search efficiency for phrase-based MT (Koehn et al, 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al, 2006) to additional early pruning techniques (Delaney et al, 2006), (Moore and Quirk, 2007) and more efficient language model (LM) querying (Heafield, 2011)","Research efforts to increase search efficiency for phrase-based MT (Koehn et al, 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al, 2006) to additional early pruning techniques (Delaney et al, 2006), (Moore and Quirk, 2007) and more efficient language model (LM) querying (Heafield, 2011)","['268', '21', '15', '124', '265']","<S sid =""268"" ssid = ""10"">For example  syntactic decoders (Koehn et al.  2007; Dyer et al.  2010; Li et al.  2009) perform dynamic programming parametrized by both backward- and forward-looking state.</S><S sid =""21"" ssid = ""16"">Performance improvements transfer to the Moses (Koehn et al.  2007)  cdec (Dyer et al.  2010)  and Joshua (Li et al.  2009) translation systems where our code has been integrated.</S><S sid =""15"" ssid = ""10"">RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.</S><S sid =""124"" ssid = ""28"">Lossy compressed models RandLM (Talbot and Osborne  2007) and Sheffield (Guthrie and Hepple  2010) offer better memory consumption at the expense of CPU and accuracy.</S><S sid =""265"" ssid = ""7"">Beyond optimizing the memory size of TRIE  there are alternative data structures such as those in Guthrie and Hepple (2010).</S>",['Results_Citation']
19,W11-2123,P13-2073,0,"Heafield, 2011",0,"For English language modeling, we use English Giga word Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011)","For English language modeling, we use English Giga word Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011)","['199', '274', '68', '218', '84']","<S sid =""199"" ssid = ""18"">For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid =""274"" ssid = ""1"">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S><S sid =""68"" ssid = ""46"">The trie data structure is commonly used for language modeling.</S><S sid =""218"" ssid = ""37"">We run the baseline Moses system for the French-English track of the 2011 Workshop on Machine Translation 9 translating the 3003-sentence test set.</S><S sid =""84"" ssid = ""62"">In a model we built with default settings  1.2% of n + 1-grams were missing their ngram suffix.</S>",['Aim_Citation']
20,W11-2123,P13-1109,0,"Heafield, 2011",0,"For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with KneserNey smoothing","For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with KneserNey smoothing","['199', '152', '52', '274', '136']","<S sid =""199"" ssid = ""18"">For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid =""152"" ssid = ""24"">All language model queries issued by machine translation decoders follow a left-to-right pattern  starting with either the begin of sentence token or null context for mid-sentence fragments.</S><S sid =""52"" ssid = ""30"">Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.</S><S sid =""274"" ssid = ""1"">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S><S sid =""136"" ssid = ""8"">We offer a state function s(wn1) = wn� where substring wn� is guaranteed to extend (to the right) in the same way that wn1 does for purposes of language modeling.</S>","['Aim_Citation', 'Results_Citation']"
