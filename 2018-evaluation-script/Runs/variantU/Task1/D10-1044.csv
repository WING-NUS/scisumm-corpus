Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,D10-1044,P11-2074,0,"Foster et al, 2010",0,"Another popular task in SMT is domain adaptation (Foster et al, 2010)","Another popular task in SMT is domain adaptation (Foster et al, 2010)","['14', '142', '10', '141', '143']","<S sid =""14"" ssid = ""11"">There is a fairly large body of work on SMT adaptation.</S><S sid =""142"" ssid = ""11"">There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan  2007; Wu et al.  2005)  and on dynamically choosing a dev set (Xu et al.  2007).</S><S sid =""10"" ssid = ""7"">This is a standard adaptation problem for SMT.</S><S sid =""141"" ssid = ""10"">Moving beyond directly related work  major themes in SMT adaptation include the IR (Hildebrand et al.  2005; L¨u et al.  2007; Zhao et al.  2004) and mixture (Finch and Sumita  2008; Foster and Kuhn  2007; Koehn and Schroeder  2007; L¨u et al.  2007) approaches for LMs and TMs described above  as well as methods for exploiting monolingual in-domain text  typically by translating it automatically and then performing self training (Bertoldi and Federico  2009; Ueffing et al.  2007; Schwenk and Senellart  2009).</S><S sid =""143"" ssid = ""12"">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S>",['Results_Citation']
2,D10-1044,P12-1048,0,"Foster et al, 2010",0,"In addition, discriminative weighting methods were proposed to assign appropriate weights to the sentences from training corpus (Matsoukas et al, 2009) or the phrase pairs of phrase table (Foster et al, 2010)","In addition, discriminative weighting methods were proposed to assign appropriate weights to the sentences from training corpus (Matsoukas et al, 2009) or the phrase pairs of phrase table (Foster et al, 2010)","['65', '132', '26', '53', '59']","<S sid =""65"" ssid = ""2"">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S><S sid =""132"" ssid = ""1"">We have already mentioned the closely related work by Matsoukas et al (2009) on discriminative corpus weighting  and Jiang and Zhai (2007) on (nondiscriminative) instance weighting.</S><S sid =""26"" ssid = ""23"">Phrase-level granularity distinguishes our work from previous work by Matsoukas et al (2009)  who weight sentences according to sub-corpus and genre membership.</S><S sid =""53"" ssid = ""17"">This has led previous workers to adopt ad hoc linear weighting schemes (Finch and Sumita  2008; Foster and Kuhn  2007; L¨u et al.  2007).</S><S sid =""59"" ssid = ""23"">To set β  we used the same criterion as for α  over a dev corpus: The MAP combination was used for TM probabilities only  in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney  1995).3 Motivated by information retrieval  a number of approaches choose “relevant” sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al.  2005; L¨u et al.  2007)  or individual target hypotheses (Zhao et al.  2004).</S>",['Method_Citation']
3,D10-1044,D12-1129,0,"Foster et al., 2010",0,"Domain knowledge also has the potential to improve open-text applications such as summarization (Ceylan et al 2010) and machine translation (Foster et al., 2010) .Research in Word Sense Disambiguation (Navigli, 2009, WSD), the task aimed at the automatic labeling of text with word senses, has been oriented towards domain text understanding for several years now","Domain knowledge also has the potential to improve open-text applications such as summarization (Ceylan et al 2010) and machine translation (Foster et al., 2010)","['141', '53', '46', '26', '143']","<S sid =""141"" ssid = ""10"">Moving beyond directly related work  major themes in SMT adaptation include the IR (Hildebrand et al.  2005; L¨u et al.  2007; Zhao et al.  2004) and mixture (Finch and Sumita  2008; Foster and Kuhn  2007; Koehn and Schroeder  2007; L¨u et al.  2007) approaches for LMs and TMs described above  as well as methods for exploiting monolingual in-domain text  typically by translating it automatically and then performing self training (Bertoldi and Federico  2009; Ueffing et al.  2007; Schwenk and Senellart  2009).</S><S sid =""53"" ssid = ""17"">This has led previous workers to adopt ad hoc linear weighting schemes (Finch and Sumita  2008; Foster and Kuhn  2007; L¨u et al.  2007).</S><S sid =""46"" ssid = ""10"">This has the potential drawback of increasing the number of features  which can make MERT less stable (Foster and Kuhn  2009).</S><S sid =""26"" ssid = ""23"">Phrase-level granularity distinguishes our work from previous work by Matsoukas et al (2009)  who weight sentences according to sub-corpus and genre membership.</S><S sid =""143"" ssid = ""12"">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S>",['Method_Citation']
4,D10-1044,P14-2093,0,2010,0,"Yasuda et al (2008) and Foster et al (2010) ranked the sentence pairs in the general-domain corpus according to the perplexity scores of sentences, which are computed with respect to in-domain language models","Yasuda et al (2008) and Foster et al (2010) ranked the sentence pairs in the general-domain corpus according to the perplexity scores of sentences, which are computed with respect to in-domain language models","['65', '59', '31', '62', '141']","<S sid =""65"" ssid = ""2"">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S><S sid =""59"" ssid = ""23"">To set β  we used the same criterion as for α  over a dev corpus: The MAP combination was used for TM probabilities only  in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney  1995).3 Motivated by information retrieval  a number of approaches choose “relevant” sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al.  2005; L¨u et al.  2007)  or individual target hypotheses (Zhao et al.  2004).</S><S sid =""31"" ssid = ""28"">For comparison to information-retrieval inspired baselines  eg (L¨u et al.  2007)  we select sentences from OUT using language model perplexities from IN.</S><S sid =""62"" ssid = ""26"">To approximate these baselines  we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S><S sid =""141"" ssid = ""10"">Moving beyond directly related work  major themes in SMT adaptation include the IR (Hildebrand et al.  2005; L¨u et al.  2007; Zhao et al.  2004) and mixture (Finch and Sumita  2008; Foster and Kuhn  2007; Koehn and Schroeder  2007; L¨u et al.  2007) approaches for LMs and TMs described above  as well as methods for exploiting monolingual in-domain text  typically by translating it automatically and then performing self training (Bertoldi and Federico  2009; Ueffing et al.  2007; Schwenk and Senellart  2009).</S>",['Method_Citation']
5,D10-1044,E12-1055,0,2010,0,"However, such confounding factors do not affect the optimization algorithm, which works with a fixed set of phrase pairs, and merely varies? .Our main technical contributions are as fol lows: Additionally to perplexity optimization for linear interpolation, which was first applied by Foster et al (2010), we propose perplexity optimization for weighted counts (equation 3), and a modified implementation of linear interpolation. Also, we independently perform perplexity minimization for all four features of the standard SMTtranslation model: the phrase translation probabilities p (t|s) and p (s|t), and the lexical weights lex (t|s) and lex (s|t)","Our main technical contributions are as follows: Additionally to perplexity optimization for linear interpolation, which was first applied by Foster et al (2010), we propose perplexity optimization for weighted counts (equation 3), and a modified implementation of linear interpolation","['50', '3', '59', '71', '80']","<S sid =""50"" ssid = ""14"">Linear weights are difficult to incorporate into the standard MERT procedure because they are “hidden” within a top-level probability that represents the linear combination.1 Following previous work (Foster and Kuhn  2007)  we circumvent this problem by choosing weights to optimize corpus loglikelihood  which is roughly speaking the training criterion used by the LM and TM themselves.</S><S sid =""3"" ssid = ""3"">We incorporate instance weighting into a mixture-model framework  and find that it yields consistent improvements over a wide range of baselines.</S><S sid =""59"" ssid = ""23"">To set β  we used the same criterion as for α  over a dev corpus: The MAP combination was used for TM probabilities only  in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney  1995).3 Motivated by information retrieval  a number of approaches choose “relevant” sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al.  2005; L¨u et al.  2007)  or individual target hypotheses (Zhao et al.  2004).</S><S sid =""71"" ssid = ""8"">Finally  we incorporate the instance-weighting model into a general linear combination  and learn weights and mixing parameters simultaneously. where cλ(s  t) is a modified count for pair (s  t) in OUT  u(s|t) is a prior distribution  and y is a prior weight.</S><S sid =""80"" ssid = ""17"">We model po(s|t) using a MAP criterion over weighted phrase-pair counts: and from the similarity to (5)  assuming y = 0  we see that wλ(s  t) can be interpreted as approximating pf(s  t)/po(s  t).</S>",['Method_Citation']
6,D10-1044,E12-1055,0,2010,0,"Matsoukas et al (2009) propose an approach where each sentence is weighted according to a classifier, and Foster et al (2010) ex tend this approach by weighting individual phrase pairs","Matsoukas et al (2009) propose an approach where each sentence is weighted according to a classifier, and Foster et al (2010) extend this approach by weighting individual phrase pairs","['65', '152', '59', '132', '26']","<S sid =""65"" ssid = ""2"">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S><S sid =""152"" ssid = ""9"">We will also directly compare with a baseline similar to the Matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences.</S><S sid =""59"" ssid = ""23"">To set β  we used the same criterion as for α  over a dev corpus: The MAP combination was used for TM probabilities only  in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney  1995).3 Motivated by information retrieval  a number of approaches choose “relevant” sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al.  2005; L¨u et al.  2007)  or individual target hypotheses (Zhao et al.  2004).</S><S sid =""132"" ssid = ""1"">We have already mentioned the closely related work by Matsoukas et al (2009) on discriminative corpus weighting  and Jiang and Zhai (2007) on (nondiscriminative) instance weighting.</S><S sid =""26"" ssid = ""23"">Phrase-level granularity distinguishes our work from previous work by Matsoukas et al (2009)  who weight sentences according to sub-corpus and genre membership.</S>",['Method_Citation']
7,D10-1044,E12-1055,0,2010,0,"These more fine-grained methods need not be seen as alternatives to coarse-grained ones. Foster et al (2010) combine the two, applying linear interpolation to combine the instance 542 weighted out-of-domain model with an in-domain model","Foster et al (2010) combine the two, applying linear interpolation to combine the instance weighted out-of-domain model with an in-domain model","['71', '150', '28', '45', '55']","<S sid =""71"" ssid = ""8"">Finally  we incorporate the instance-weighting model into a general linear combination  and learn weights and mixing parameters simultaneously. where cλ(s  t) is a modified count for pair (s  t) in OUT  u(s|t) is a prior distribution  and y is a prior weight.</S><S sid =""150"" ssid = ""7"">In both cases  the instanceweighting approach improved over a wide range of baselines  giving gains of over 2 BLEU points over the best non-adapted baseline  and gains of between 0.6 and 1.8 over an equivalent mixture model (with an identical training procedure but without instance weighting).</S><S sid =""28"" ssid = ""25"">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S><S sid =""45"" ssid = ""9"">An easy way to achieve this is to put the domain-specific LMs and TMs into the top-level log-linear model and learn optimal weights with MERT (Och  2003).</S><S sid =""55"" ssid = ""19"">This suggests a direct parallel to (1): where ˜p(s  t) is a joint empirical distribution extracted from the IN dev set using the standard procedure.2 An alternative form of linear combination is a maximum a posteriori (MAP) combination (Bacchiani et al.  2004).</S>",['Method_Citation']
8,D10-1044,E12-1055,0,Foster et al 2010,0,Note that both data sets have a relatively high ratio of in-domain to out-of-domain parallel training data (1:20 for DE? EN and 1:5 for HT? EN) Previous research has been performed with ratios of 1:100 (Foster et al 2010) or 1:400 (Axelrod et al 2011),Note that both data sets have a relatively high ratio of in-domain to out-of-domain parallel training data (1:20 for DE? EN and 1:5 for HT? EN); Previous research has been performed with ratios of 1:100 (Foster et al 2010) or 1:400 (Axelrod et al 2011),"['59', '53', '55', '132', '150']","<S sid =""59"" ssid = ""23"">To set β  we used the same criterion as for α  over a dev corpus: The MAP combination was used for TM probabilities only  in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney  1995).3 Motivated by information retrieval  a number of approaches choose “relevant” sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al.  2005; L¨u et al.  2007)  or individual target hypotheses (Zhao et al.  2004).</S><S sid =""53"" ssid = ""17"">This has led previous workers to adopt ad hoc linear weighting schemes (Finch and Sumita  2008; Foster and Kuhn  2007; L¨u et al.  2007).</S><S sid =""55"" ssid = ""19"">This suggests a direct parallel to (1): where ˜p(s  t) is a joint empirical distribution extracted from the IN dev set using the standard procedure.2 An alternative form of linear combination is a maximum a posteriori (MAP) combination (Bacchiani et al.  2004).</S><S sid =""132"" ssid = ""1"">We have already mentioned the closely related work by Matsoukas et al (2009) on discriminative corpus weighting  and Jiang and Zhai (2007) on (nondiscriminative) instance weighting.</S><S sid =""150"" ssid = ""7"">In both cases  the instanceweighting approach improved over a wide range of baselines  giving gains of over 2 BLEU points over the best non-adapted baseline  and gains of between 0.6 and 1.8 over an equivalent mixture model (with an identical training procedure but without instance weighting).</S>",['Method_Citation']
9,D10-1044,E12-1055,0,Foster et al 2010,0,"We expand on work by (Foster et al 2010) in establishing translation model perplexity minimization as a robust baseline for a weighted combination of translationmodels.15 We demonstrate perplexity optimization for weighted counts, which are a natural extension of unadapted MLE training, but are of little prominence in domain adaptation research",We expand on work by (Foster et al 2010) in establishing translation model perplexity minimization as a robust baseline for a weighted combination of translation models,"['31', '50', '40', '3', '7']","<S sid =""31"" ssid = ""28"">For comparison to information-retrieval inspired baselines  eg (L¨u et al.  2007)  we select sentences from OUT using language model perplexities from IN.</S><S sid =""50"" ssid = ""14"">Linear weights are difficult to incorporate into the standard MERT procedure because they are “hidden” within a top-level probability that represents the linear combination.1 Following previous work (Foster and Kuhn  2007)  we circumvent this problem by choosing weights to optimize corpus loglikelihood  which is roughly speaking the training criterion used by the LM and TM themselves.</S><S sid =""40"" ssid = ""4"">We focus here instead on adapting the two most important features: the language model (LM)  which estimates the probability p(wIh) of a target word w following an ngram h; and the translation models (TM) p(slt) and p(t1s)  which give the probability of source phrase s translating to target phrase t  and vice versa.</S><S sid =""3"" ssid = ""3"">We incorporate instance weighting into a mixture-model framework  and find that it yields consistent improvements over a wide range of baselines.</S><S sid =""7"" ssid = ""4"">For developers of Statistical Machine Translation (SMT) systems  an additional complication is the heterogeneous nature of SMT components (word-alignment model  language model  translation model  etc.</S>",['Method_Citation']
10,D10-1044,P12-1099,0,"Foster et al, 2010",0,"In addition to the basic approach of concatenation of in-domain and out-of-domain data, we also trained a log-linear mixture model (Foster and Kuhn, 2007) 940 as well as the linear mixture model of (Foster et al, 2010) for conditional phrase-pair probabilities over IN and OUT","In addition to the basic approach of concatenation of in-domain and out-of-domain data, we also trained a log-linear mixture model (Foster and Kuhn, 2007) as well as the linear mixture model of (Foster et al, 2010) for conditional phrase-pair probabilities over IN and OUT","['28', '53', '150', '50', '30']","<S sid =""28"" ssid = ""25"">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S><S sid =""53"" ssid = ""17"">This has led previous workers to adopt ad hoc linear weighting schemes (Finch and Sumita  2008; Foster and Kuhn  2007; L¨u et al.  2007).</S><S sid =""150"" ssid = ""7"">In both cases  the instanceweighting approach improved over a wide range of baselines  giving gains of over 2 BLEU points over the best non-adapted baseline  and gains of between 0.6 and 1.8 over an equivalent mixture model (with an identical training procedure but without instance weighting).</S><S sid =""50"" ssid = ""14"">Linear weights are difficult to incorporate into the standard MERT procedure because they are “hidden” within a top-level probability that represents the linear combination.1 Following previous work (Foster and Kuhn  2007)  we circumvent this problem by choosing weights to optimize corpus loglikelihood  which is roughly speaking the training criterion used by the LM and TM themselves.</S><S sid =""30"" ssid = ""27"">A similar maximumlikelihood approach was used by Foster and Kuhn (2007)  but for language models only.</S>",['Method_Citation']
11,D10-1044,P12-1099,0,2010,0,m ?mpm (e? |f?) Our technique for setting? m is similar to that outlined in Foster et al (2010),Our technique for setting ? m is similar to that outlined in Foster et al (2010),"['92', '30', '53', '18', '67']","<S sid =""92"" ssid = ""29"">6One of our experimental settings lacks document boundaries  and we used this approximation in both settings for consistency.</S><S sid =""30"" ssid = ""27"">A similar maximumlikelihood approach was used by Foster and Kuhn (2007)  but for language models only.</S><S sid =""53"" ssid = ""17"">This has led previous workers to adopt ad hoc linear weighting schemes (Finch and Sumita  2008; Foster and Kuhn  2007; L¨u et al.  2007).</S><S sid =""18"" ssid = ""15"">This is less effective in our setting  where IN and OUT are disparate.</S><S sid =""67"" ssid = ""4"">We extend the Matsoukas et al approach in several ways.</S>",['Method_Citation']
12,D10-1044,P12-1099,0,"Foster et al., 2010",0,"m ?mpm (e? |f?) For efficiency and stability, we use the EMalgorithm to find??, rather than L-BFGS as in (Foster et al., 2010)","For efficiency and stability, we use the EM algorithm to find ?, rather than L-BFGS as in (Foster et al., 2010)","['59', '53', '75', '152', '3']","<S sid =""59"" ssid = ""23"">To set β  we used the same criterion as for α  over a dev corpus: The MAP combination was used for TM probabilities only  in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney  1995).3 Motivated by information retrieval  a number of approaches choose “relevant” sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al.  2005; L¨u et al.  2007)  or individual target hypotheses (Zhao et al.  2004).</S><S sid =""53"" ssid = ""17"">This has led previous workers to adopt ad hoc linear weighting schemes (Finch and Sumita  2008; Foster and Kuhn  2007; L¨u et al.  2007).</S><S sid =""75"" ssid = ""12"">However  it is robust  efficient  and easy to implement.4 To perform the maximization in (7)  we used the popular L-BFGS algorithm (Liu and Nocedal  1989)  which requires gradient information.</S><S sid =""152"" ssid = ""9"">We will also directly compare with a baseline similar to the Matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences.</S><S sid =""3"" ssid = ""3"">We incorporate instance weighting into a mixture-model framework  and find that it yields consistent improvements over a wide range of baselines.</S>",['Method_Citation']
13,D10-1044,P12-1099,0,2010,0,"Foster et al (2010), however, uses a different approach to select related sentences from OUT","Foster et al (2010), however, uses a different approach to select related sentences from OUT","['67', '59', '53', '30', '31']","<S sid =""67"" ssid = ""4"">We extend the Matsoukas et al approach in several ways.</S><S sid =""59"" ssid = ""23"">To set β  we used the same criterion as for α  over a dev corpus: The MAP combination was used for TM probabilities only  in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney  1995).3 Motivated by information retrieval  a number of approaches choose “relevant” sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al.  2005; L¨u et al.  2007)  or individual target hypotheses (Zhao et al.  2004).</S><S sid =""53"" ssid = ""17"">This has led previous workers to adopt ad hoc linear weighting schemes (Finch and Sumita  2008; Foster and Kuhn  2007; L¨u et al.  2007).</S><S sid =""30"" ssid = ""27"">A similar maximumlikelihood approach was used by Foster and Kuhn (2007)  but for language models only.</S><S sid =""31"" ssid = ""28"">For comparison to information-retrieval inspired baselines  eg (L¨u et al.  2007)  we select sentences from OUT using language model perplexities from IN.</S>",['Results_Citation']
14,D10-1044,P12-1099,0,2010,0,Foster et al (2010) propose asimilar method for machine translation that uses features to capture degrees of generality,Foster et al (2010) propose a similar method for machine translation that uses features to capture degrees of generality,"['30', '141', '53', '65', '20']","<S sid =""30"" ssid = ""27"">A similar maximumlikelihood approach was used by Foster and Kuhn (2007)  but for language models only.</S><S sid =""141"" ssid = ""10"">Moving beyond directly related work  major themes in SMT adaptation include the IR (Hildebrand et al.  2005; L¨u et al.  2007; Zhao et al.  2004) and mixture (Finch and Sumita  2008; Foster and Kuhn  2007; Koehn and Schroeder  2007; L¨u et al.  2007) approaches for LMs and TMs described above  as well as methods for exploiting monolingual in-domain text  typically by translating it automatically and then performing self training (Bertoldi and Federico  2009; Ueffing et al.  2007; Schwenk and Senellart  2009).</S><S sid =""53"" ssid = ""17"">This has led previous workers to adopt ad hoc linear weighting schemes (Finch and Sumita  2008; Foster and Kuhn  2007; L¨u et al.  2007).</S><S sid =""65"" ssid = ""2"">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S><S sid =""20"" ssid = ""17"">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S>",['Method_Citation']
15,D10-1044,P13-1126,0,"Foster et al, 2010",0,"As in (Foster et al, 2010), this approach works at the level of phrase pairs","As in (Foster et al, 2010), this approach works at the level of phrase pairs","['53', '65', '23', '59', '144']","<S sid =""53"" ssid = ""17"">This has led previous workers to adopt ad hoc linear weighting schemes (Finch and Sumita  2008; Foster and Kuhn  2007; L¨u et al.  2007).</S><S sid =""65"" ssid = ""2"">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S><S sid =""23"" ssid = ""20"">Our second contribution is to apply instance weighting at the level of phrase pairs.</S><S sid =""59"" ssid = ""23"">To set β  we used the same criterion as for α  over a dev corpus: The MAP combination was used for TM probabilities only  in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney  1995).3 Motivated by information retrieval  a number of approaches choose “relevant” sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al.  2005; L¨u et al.  2007)  or individual target hypotheses (Zhao et al.  2004).</S><S sid =""144"" ssid = ""1"">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>",['Method_Citation']
16,D10-1044,D11-1033,0,2010,0,"The ranking of the sentences in a general-domain corpus according to in-domain perplexity has also been applied to machine translation by both Yasuda et al (2008), and Foster et al (2010)","The ranking of the sentences in a general-domain corpus according to in-domain perplexity has also been applied to machine translation by both Yasuda et al (2008), and Foster et al (2010)","['132', '141', '65', '26', '59']","<S sid =""132"" ssid = ""1"">We have already mentioned the closely related work by Matsoukas et al (2009) on discriminative corpus weighting  and Jiang and Zhai (2007) on (nondiscriminative) instance weighting.</S><S sid =""141"" ssid = ""10"">Moving beyond directly related work  major themes in SMT adaptation include the IR (Hildebrand et al.  2005; L¨u et al.  2007; Zhao et al.  2004) and mixture (Finch and Sumita  2008; Foster and Kuhn  2007; Koehn and Schroeder  2007; L¨u et al.  2007) approaches for LMs and TMs described above  as well as methods for exploiting monolingual in-domain text  typically by translating it automatically and then performing self training (Bertoldi and Federico  2009; Ueffing et al.  2007; Schwenk and Senellart  2009).</S><S sid =""65"" ssid = ""2"">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S><S sid =""26"" ssid = ""23"">Phrase-level granularity distinguishes our work from previous work by Matsoukas et al (2009)  who weight sentences according to sub-corpus and genre membership.</S><S sid =""59"" ssid = ""23"">To set β  we used the same criterion as for α  over a dev corpus: The MAP combination was used for TM probabilities only  in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney  1995).3 Motivated by information retrieval  a number of approaches choose “relevant” sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al.  2005; L¨u et al.  2007)  or individual target hypotheses (Zhao et al.  2004).</S>",['Method_Citation']
17,D10-1044,D11-1033,0,2010,0,"Foster et al (2010) do not mention what percentage of the corpus they select for their IR-baseline, but they concatenate the data to their in-domain corpus and re port a decrease in performance","Foster et al (2010) do not mention what percentage of the corpus they select for their IR-baseline, but they concatenate the data to their in-domain corpus and report a decrease in performance","['78', '59', '43', '57', '46']","<S sid =""78"" ssid = ""15"">This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates  and po(s|t) is an instance-weighted model derived from the OUT corpus.</S><S sid =""59"" ssid = ""23"">To set β  we used the same criterion as for α  over a dev corpus: The MAP combination was used for TM probabilities only  in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney  1995).3 Motivated by information retrieval  a number of approaches choose “relevant” sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al.  2005; L¨u et al.  2007)  or individual target hypotheses (Zhao et al.  2004).</S><S sid =""43"" ssid = ""7"">Its success depends on the two domains being relatively close  and on the OUT corpus not being so large as to overwhelm the contribution of IN.</S><S sid =""57"" ssid = ""21"">This is motivated by taking β po(s|t) to be the parameters of a Dirichlet prior on phrase probabilities  then maximizing posterior estimates p(s|t) given the IN corpus.</S><S sid =""46"" ssid = ""10"">This has the potential drawback of increasing the number of features  which can make MERT less stable (Foster and Kuhn  2009).</S>",['Aim_Citation']
18,D10-1044,D11-1033,0,2010,0,"Foster et al (2010) further perform this on extracted phrase pairs, not just sentences","Foster et al (2010) further perform this on extracted phrase pairs, not just sentences","['59', '60', '65', '68', '62']","<S sid =""59"" ssid = ""23"">To set β  we used the same criterion as for α  over a dev corpus: The MAP combination was used for TM probabilities only  in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney  1995).3 Motivated by information retrieval  a number of approaches choose “relevant” sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al.  2005; L¨u et al.  2007)  or individual target hypotheses (Zhao et al.  2004).</S><S sid =""60"" ssid = ""24"">The matching sentence pairs are then added to the IN corpus  and the system is re-trained.</S><S sid =""65"" ssid = ""2"">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S><S sid =""68"" ssid = ""5"">First  we learn weights on individual phrase pairs rather than sentences.</S><S sid =""62"" ssid = ""26"">To approximate these baselines  we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>",['Results_Citation']
19,D10-1044,P14-1012,0,"Foster et al, 2010",0,"To address the first shortcoming, we adapt and extend some simple but effective phrase features as the input features for new DNN feature learning, and these features have been shown significant improvement for SMT, such as, phrase pair similarity (Zhao et al, 2004), phrase frequency, phrase length (Hopkins and May, 2011), and phrase generative probability (Foster et al, 2010), which also show further improvement for new phrase feature learning in our experiments","To address the first shortcoming, we adapt and extend some simple but effective phrase features as the input features for new DNN feature learning, and these features have been shown significant improvement for SMT, such as, phrase pair similarity (Zhao et al, 2004), phrase frequency, phrase length (Hopkins and May, 2011), and phrase generative probability (Foster et al, 2010), which also show further improvement for new phrase feature learning in our experiments","['59', '143', '152', '141', '40']","<S sid =""59"" ssid = ""23"">To set β  we used the same criterion as for α  over a dev corpus: The MAP combination was used for TM probabilities only  in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney  1995).3 Motivated by information retrieval  a number of approaches choose “relevant” sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al.  2005; L¨u et al.  2007)  or individual target hypotheses (Zhao et al.  2004).</S><S sid =""143"" ssid = ""12"">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S><S sid =""152"" ssid = ""9"">We will also directly compare with a baseline similar to the Matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences.</S><S sid =""141"" ssid = ""10"">Moving beyond directly related work  major themes in SMT adaptation include the IR (Hildebrand et al.  2005; L¨u et al.  2007; Zhao et al.  2004) and mixture (Finch and Sumita  2008; Foster and Kuhn  2007; Koehn and Schroeder  2007; L¨u et al.  2007) approaches for LMs and TMs described above  as well as methods for exploiting monolingual in-domain text  typically by translating it automatically and then performing self training (Bertoldi and Federico  2009; Ueffing et al.  2007; Schwenk and Senellart  2009).</S><S sid =""40"" ssid = ""4"">We focus here instead on adapting the two most important features: the language model (LM)  which estimates the probability p(wIh) of a target word w following an ngram h; and the translation models (TM) p(slt) and p(t1s)  which give the probability of source phrase s translating to target phrase t  and vice versa.</S>",['Aim_Citation']
