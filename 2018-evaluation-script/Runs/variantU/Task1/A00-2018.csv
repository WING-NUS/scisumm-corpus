Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
2,A00-2018,N10-1002,0,"Charniak, 2000",0,"As a benchmark VPC extraction system, we use the Charniak parser (Charniak, 2000)","As a benchmark VPC extraction system, we use the Charniak parser (Charniak, 2000)","['91', '90', '117', '113', '7']","<S sid =""91"" ssid = ""2"">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S><S sid =""90"" ssid = ""1"">We created a parser based upon the maximumentropy-inspired model of the last section  smoothed using standard deleted interpolation.</S><S sid =""117"" ssid = ""8"">Also  the earlier parser uses two techniques not employed in the current parser.</S><S sid =""113"" ssid = ""4"">We take as our starting point the parser labled Char97 in Figure 1 [5]  as that is the program from which our current parser derives.</S><S sid =""7"" ssid = ""3"">Following [5 10]  our parser is based upon a probabilistic generative model.</S>",['Results_Citation']
3,A00-2018,W11-0610,0,"Charniak, 2000",0,"Each of these scores can be calculated from a provided syntactic parse tree, and to generate these we made use of the Charniakparser (Charniak, 2000), also trained on the Switch board tree bank","Each of these scores can be calculated from a provided syntactic parse tree, and to generate these we made use of the Charniak parser (Charniak, 2000), also trained on the Switch board tree bank","['91', '1', '5', '117', '113']","<S sid =""91"" ssid = ""2"">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S><S sid =""1"" ssid = ""1"">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid =""5"" ssid = ""1"">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S><S sid =""117"" ssid = ""8"">Also  the earlier parser uses two techniques not employed in the current parser.</S><S sid =""113"" ssid = ""4"">We take as our starting point the parser labled Char97 in Figure 1 [5]  as that is the program from which our current parser derives.</S>",['Method_Citation']
4,A00-2018,W06-3119,0,"Charniak, 2000",0,"We then use Charniak? s parser (Charniak, 2000) to generate the most likely parse tree for each English target sentence in the training corpus","We then use Charniak's parser (Charniak, 2000) to generate the most likely parse tree for each English target sentence in the training corpus","['107', '91', '1', '5', '189']","<S sid =""107"" ssid = ""18"">The results for the new parser as well as for the previous top-three individual parsers on this corpus are given in Figure 1.</S><S sid =""91"" ssid = ""2"">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S><S sid =""1"" ssid = ""1"">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid =""5"" ssid = ""1"">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S><S sid =""189"" ssid = ""16"">Indeed  we initiated this line of work in an attempt to create a parser that would be flexible enough to allow modifications for parsing down to more semantic levels of detail.</S>",['Method_Citation']
5,A00-2018,N03-2024,0,"Charniak, 2000",0,"We were interested in the occurrence of features such as type and number of premodifiers, presence and type of post modifiers, and form of name reference for people. We constructed a large, automatically annotated corpus by merging the output of Charniak? s statistical parser (Charniak, 2000) with that of the IBM named entity recognition system Nominator (Wacholder et al,1997)","We were interested in the occurrence of features such as type and number of premodifiers, presence and type of post modifiers, and form of name reference for people. We constructed a large, automatically annotated corpus by merging the output of Charniak's statistical parser (Charniak, 2000) with that of the IBM named entity recognition system Nominator (Wacholder et al,1997)","['91', '120', '107', '118', '126']","<S sid =""91"" ssid = ""2"">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S><S sid =""120"" ssid = ""11"">Second  Char97 uses unsupervised learning in that the original system was run on about thirty million words of unparsed text  the output was taken as &quot;correct&quot;  and statistics were collected on the resulting parses.</S><S sid =""107"" ssid = ""18"">The results for the new parser as well as for the previous top-three individual parsers on this corpus are given in Figure 1.</S><S sid =""118"" ssid = ""9"">First  it uses a clustering scheme on words to give the system a &quot;soft&quot; clustering of heads and sub-heads.</S><S sid =""126"" ssid = ""17"">This is indicated in Figure 2  where the model labeled &quot;Best&quot; has precision of 89.8% and recall of 89.6% for an average of 89.7%  0.4% lower than the results on the official test corpus.</S>",['Method_Citation']
6,A00-2018,N06-1039,0,"Charniak, 2000",0,"After getting a set of basic clusters, we pass them to an existing statistical parser (Charniak, 2000) and rule-based tree normalizer to obtain a GLARFstructure for each sentence in every article","After getting a set of basic clusters, we pass them to an existing statistical parser (Charniak, 2000) and rule-based tree normalizer to obtain a GLARF structure for each sentence in every article","['91', '1', '92', '5', '90']","<S sid =""91"" ssid = ""2"">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S><S sid =""1"" ssid = ""1"">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid =""92"" ssid = ""3"">For runs with the generative model based upon Markov grammar statistics  the first pass uses the same statistics  but conditioned only on standard PCFG information.</S><S sid =""5"" ssid = ""1"">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S><S sid =""90"" ssid = ""1"">We created a parser based upon the maximumentropy-inspired model of the last section  smoothed using standard deleted interpolation.</S>",['Method_Citation']
7,A00-2018,C04-1180,0,2000,0,"The levels of accuracy and robustness recently achieved by statistical parsers (e.g. Collins (1999), Charniak (2000)) have led to their use in a number of NLP applications, such as question-answering (Pasca and Harabagiu, 2001), machine translation (Charniak et al, 2003), sentence simplification (Carroll et al, 1999), and a linguist? s search engine (Resnik and Elkiss, 2003)","The levels of accuracy and robustness recently achieved by statistical parsers (e.g. Collins (1999), Charniak (2000)) have led to their use in a number of NLP applications, such as question-answering (Pasca and Harabagiu, 2001), machine translation (Charniak et al, 2003), sentence simplification (Carroll et al, 1999), and a linguist? s search engine (Resnik and Elkiss, 2003)","['174', '120', '103', '132', '188']","<S sid =""174"" ssid = ""1"">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S sid =""120"" ssid = ""11"">Second  Char97 uses unsupervised learning in that the original system was run on about thirty million words of unparsed text  the output was taken as &quot;correct&quot;  and statistics were collected on the resulting parses.</S><S sid =""103"" ssid = ""14"">In particular  we measure labeled precision (LP) and recall (LR)  average number of crossbrackets per sentence (CB)  percentage of sentences with zero cross brackets (OCB)  and percentage of sentences with < 2 cross brackets (2CB).</S><S sid =""132"" ssid = ""23"">Between the Old model and the Best model  Figure 2 gives precision/recall measurements for several different versions of our parser.</S><S sid =""188"" ssid = ""15"">Ultimately it is this flexibility that let us try the various conditioning events  to move on to a Markov grammar approach  and to try several Markov grammars of different orders  without significant programming.</S>",['Method_Citation']
8,A00-2018,W05-0638,0,"Charniak, 2000",0,"In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000)","In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000)","['117', '1', '0', '110', '130']","<S sid =""117"" ssid = ""8"">Also  the earlier parser uses two techniques not employed in the current parser.</S><S sid =""1"" ssid = ""1"">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid =""0"" ssid = ""0"">A Maximum-Entropy-Inspired Parser *</S><S sid =""110"" ssid = ""1"">In the previous sections we have concentrated on the relation of the parser to a maximumentropy approach  the aspect of the parser that is most novel.</S><S sid =""130"" ssid = ""21"">This parser achieves an average precision/recall of 86.2%.</S>",['Method_Citation']
9,A00-2018,P05-1065,0,"Charniak, 2000",0,"We also use a standard statistical parser (Charniak, 2000) to provide syntactic analysis","We also use a standard statistical parser (Charniak, 2000) to provide syntactic analysis","['117', '91', '116', '7', '130']","<S sid =""117"" ssid = ""8"">Also  the earlier parser uses two techniques not employed in the current parser.</S><S sid =""91"" ssid = ""2"">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S><S sid =""116"" ssid = ""7"">This is as opposed to the &quot;Markovgrammar&quot; approach used in the current parser.</S><S sid =""7"" ssid = ""3"">Following [5 10]  our parser is based upon a probabilistic generative model.</S><S sid =""130"" ssid = ""21"">This parser achieves an average precision/recall of 86.2%.</S>",['Method_Citation']
10,A00-2018,P05-1065,0,"Charniak, 2000",0,"For each article, we calculated the per cent age of a) all word instances (tokens) and b) all unique words (types) not on these lists, resulting in three token OOV rate features and three type OOV rate features per article. The parse features are generated using the Charniak parser (Charniak, 2000) trained on the standard Wall Street Journal Treebank corpus","For each article, we calculated the percentage of a) all word instances (tokens) and b) all unique words (types) not on these lists, resulting in three token OOV rate features and three type OOV rate features per article. The parse features are generated using the Charniak parser (Charniak, 2000) trained on the standard Wall Street Journal Treebank corpus","['5', '1', '108', '91', '8']","<S sid =""5"" ssid = ""1"">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S><S sid =""1"" ssid = ""1"">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid =""108"" ssid = ""19"">As is typical  all of the standard measures tell pretty much the same story  with the new parser outperforming the other three parsers.</S><S sid =""91"" ssid = ""2"">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S><S sid =""8"" ssid = ""4"">That is  for all sentences s and all parses 7r  the parser assigns a probability p(s   7r) = p(r)  the equality holding when we restrict consideration to 7r whose yield * This research was supported in part by NSF grant LIS SBR 9720368.</S>",['Method_Citation']
11,A00-2018,P04-1040,0,2000,0,"The evaluation of the transformed output of the parsers of Charniak (2000) and Collins (1999) gives 90 %unlabelled and 84 %labelled accuracy with respect to dependencies, when measured against a dependency corpus derived from the Penn Treebank.The paper is organized as follows","The evaluation of the transformed output of the parsers of Charniak (2000) and Collins (1999) gives 90 % unlabelled and 84 % labelled accuracy with respect to dependencies, when measured against a dependency corpus derived from the Penn Treebank. The paper is organized as follows","['91', '126', '120', '1', '113']","<S sid =""91"" ssid = ""2"">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S><S sid =""126"" ssid = ""17"">This is indicated in Figure 2  where the model labeled &quot;Best&quot; has precision of 89.8% and recall of 89.6% for an average of 89.7%  0.4% lower than the results on the official test corpus.</S><S sid =""120"" ssid = ""11"">Second  Char97 uses unsupervised learning in that the original system was run on about thirty million words of unparsed text  the output was taken as &quot;correct&quot;  and statistics were collected on the resulting parses.</S><S sid =""1"" ssid = ""1"">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid =""113"" ssid = ""4"">We take as our starting point the parser labled Char97 in Figure 1 [5]  as that is the program from which our current parser derives.</S>",['Method_Citation']
12,A00-2018,P04-1040,0,2000,0,"Blaheta and Charniak (2000) presented the first method for assigning Pennfunctional tags to constituents identified by a parser. Pattern-matching approaches were used in (John son, 2002) and (Jijkoun, 2003) to recover non-local dependencies in phrase trees","Blaheta and Charniak (2000) presented the first method for assigning Penn functional tags to constituents identified by a parser. Pattern-matching approaches were used in (Johnson, 2002) and (Jijkoun, 2003) to recover non-local dependencies in phrase trees","['1', '5', '91', '117', '174']","<S sid =""1"" ssid = ""1"">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid =""5"" ssid = ""1"">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S><S sid =""91"" ssid = ""2"">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S><S sid =""117"" ssid = ""8"">Also  the earlier parser uses two techniques not employed in the current parser.</S><S sid =""174"" ssid = ""1"">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S>",['Method_Citation']
13,A00-2018,P04-1040,0,2000,0,"As an alternative to hard coded heuristics, Blaheta and Charniak (2000) proposed to recover the Penn functional tags automatically","As an alternative to hard coded heuristics, Blaheta and Charniak (2000) proposed to recover the Penn functional tags automatically","['101', '91', '117', '1', '178']","<S sid =""101"" ssid = ""12"">In keeping with the standard methodology [5  9 10 15 17]  we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training  section 23 for testing  and section 24 for development (debugging and tuning).</S><S sid =""91"" ssid = ""2"">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S><S sid =""117"" ssid = ""8"">Also  the earlier parser uses two techniques not employed in the current parser.</S><S sid =""1"" ssid = ""1"">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid =""178"" ssid = ""5"">The results of [13] achieved by combining the aforementioned three-best parsers also suggest that the limit on tree-bank trained parsers is much higher than previously thought.</S>",['Method_Citation']
17,A00-2018,N06-1022,0,2000,0,"The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents","The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents","['174', '98', '91', '172', '129']","<S sid =""174"" ssid = ""1"">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S sid =""98"" ssid = ""9"">L and R are conditioned on three previous labels so we are using a third-order Markov grammar.</S><S sid =""91"" ssid = ""2"">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S><S sid =""172"" ssid = ""63"">As one can see in Figure 2  a firstorder Markov grammar (with all the aforementioned improvements) performs slightly worse than the equivalent tree-bank-grammar parser.</S><S sid =""129"" ssid = ""20"">It makes no use of special maximum-entropyinspired features (though their presence made it much easier to perform these experiments)  it does not guess the pre-terminal before guessing the lexical head  and it uses a tree-bank grammar rather than a Markov grammar.</S>",['Results_Citation']
18,A00-2018,N06-1022,0,2000,0,"Most recently, McDonald et al (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000))","Most recently, McDonald et al (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000))","['117', '179', '1', '172', '178']","<S sid =""117"" ssid = ""8"">Also  the earlier parser uses two techniques not employed in the current parser.</S><S sid =""179"" ssid = ""6"">Indeed  it may be that adding this new parser to the mix may yield still higher results.</S><S sid =""1"" ssid = ""1"">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid =""172"" ssid = ""63"">As one can see in Figure 2  a firstorder Markov grammar (with all the aforementioned improvements) performs slightly worse than the equivalent tree-bank-grammar parser.</S><S sid =""178"" ssid = ""5"">The results of [13] achieved by combining the aforementioned three-best parsers also suggest that the limit on tree-bank trained parsers is much higher than previously thought.</S>",['Method_Citation']
19,A00-2018,H05-1035,0,"Charniak, 2000",0,"The feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (Charniak, 2000), trees that will be improved by more accurate PP-attachment decisions","The feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (Charniak, 2000), trees that will be improved by more accurate PP-attachment decisions","['90', '48', '40', '1', '91']","<S sid =""90"" ssid = ""1"">We created a parser based upon the maximumentropy-inspired model of the last section  smoothed using standard deleted interpolation.</S><S sid =""48"" ssid = ""17"">Maximum-entropy models have two benefits for a parser builder.</S><S sid =""40"" ssid = ""9"">In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.</S><S sid =""1"" ssid = ""1"">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid =""91"" ssid = ""2"">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>",['Method_Citation']
20,A00-2018,P04-1042,0,2000,0,"Note that the dependency figures of Dienes lag behind even the parsed results for Johnson? s model; this may well be due to the fact that Dienes built his model as an extension of Collins (1999), which lags behind Charniak (2000) by about 1.3-1.5% .Manual investigation of errors on English gold standard data revealed two major issues that suggest further potential for improvement in performance without further increase in algorithmic complexity or training set size","Note that the dependency figures of Dienes lag behind even the parsed results for Johnson's model; this may well be due to the fact that Dienes built his model as an extension of Collins (1999), which lags behind Charniak (2000) by about 1.3-1.5%. Manual investigation of errors on English gold standard data revealed two major issues that suggest further potential for improvement in performance without further increase in algorithmic complexity or training set size","['132', '176', '180', '4', '109']","<S sid =""132"" ssid = ""23"">Between the Old model and the Best model  Figure 2 gives precision/recall measurements for several different versions of our parser.</S><S sid =""176"" ssid = ""3"">That the previous three best parsers on this test [5 9 17] all perform within a percentage point of each other  despite quite different basic mechanisms  led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training  and to conjecture that perhaps we were at it.</S><S sid =""180"" ssid = ""7"">From our perspective  perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head.</S><S sid =""4"" ssid = ""4"">We also present some partial results showing the effects of different conditioning information  including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.</S><S sid =""109"" ssid = ""20"">Looking in particular at the precision and recall figures  the new parser's give us a 13% error reduction over the best of the previous work  Co1199 [9].</S>",['Method_Citation']
