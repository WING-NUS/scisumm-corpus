 The idea of distinguishing between general and domain-specific examples is due to Daum´e and Marcu (2006) who used a maximum-entropy model with latent variables to capture the degree of specificity The natural baseline approach is to concatenate data from IN and OUT This highly effective approach is not directly applicable to the multinomial models used for core SMT components which have no natural method for combining split features so we rely on an instance-weighting approach (Jiang and Zhai 2007) to downweight domain-specific examples in OUT To approximate these baselines we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model Jiang and Zhai (2007) suggest the following derivation making use of the true OUT distribution po(s t): where each fi(s t) is a feature intended to charac- !0ˆ = argmax pf(s t) log pθ(s|t) (8) terize the usefulness of (s t) weighted by Ai In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance This is not unreasonable given the application to phrase pairs from OUT but it suggests that an interesting alternative might be to use a plain log-linear weighting function exp(Ei Aifi(s t)) with outputs in [0 oo] It is difficult when IN and OUT are dissimilar as they are in the cases we study We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain determined by both how similar to it they appear to be and whether they belong to general language or not