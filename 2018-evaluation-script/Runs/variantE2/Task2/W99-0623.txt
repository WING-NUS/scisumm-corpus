 In general the lemma of the previous section does not ensure that all the productions in the combined parse are found in the grammars of the member parsers Another way to interpret this is that less than 5% of the correct constituents are missing from the hypotheses generated by the union of the three parsers The constituent voting and na√Øve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank leaving only sections 22 and 23 completely untouched during the development of any of the parsers The maximum precision row is the upper bound on accuracy if we could pick exactly the correct constituents from among the constituents suggested by the three parsers The estimation of the probabilities in the model is carried out as shown in Equation 4 The PCFG was trained from the same sections of the Penn Treebank as the other three parsers In the interest of testing the robustness of these combining techniques we added a fourth simple nonlexicalized PCFG parser IL+-1Proof: Assume a pair of crossing constituents appears in the output of the constituent voting technique using k parsers Table 4 shows how much the Bayes switching technique uses each of the parsers on the test set We are interested in combining the substructures of the input parses to produce a better parse The natural language processing community is in the strong position of having many available approaches to solving some of its most fundamental problems