 Central in these models is the notion of compositionality â€” the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them Here the space has only five dimensions and the matrix cells denote the co-occurrence of the target words (horse and run) with the context words animal stable and so on Despite the popularity of additive models our experimental results showed the superiority of models utilizing multiplicative combinations at least for the sentence similarity task attempted here Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions The task involves examining the degree of linear relationship between the human judgments for two individual words and vector-based similarity values The compression is achieved by summing along the transdiagonal elements of the tensor product The applications of the framework discussed here are many and varied both for cognitive science and NLP Importantly additive models capture composition by considering all vector components representing the meaning of the verb and its subject whereas multiplicative models consider a subset namely non-zero components First the additive model in (7) weighs differentially the contribution of the two constituents Vector-based models of word meaning (Lund and Burgess 1996; Landauer and Dumais 1997) have become increasingly popular in natural language processing (NLP) and cognitive science In fact the commonest method for combining the vectors is to average them This paper proposes a framework for representing the meaning of phrases and sentences in vector space