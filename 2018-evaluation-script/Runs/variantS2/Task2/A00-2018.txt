 For example in computing the probability of the head's pre-terminal t we might want a feature schema f (t 1) that returns 1 if the observed pre-terminal of c = t and the label of c = 1 and zero otherwise The author would like to thank Mark Johnson and all the rest of the Brown Laboratory for Linguistic Information Processing That is suppose we want to compute a conditional probability p(a b c) but we are not sure that we have enough examples of the conditioning event b c in the training corpus to ensure that the empirically obtained probability P (a I b c) is accurate Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c t(c) (t for &quot;tag&quot;) then the lexical head of c h(c) and then the expansion of c into further constituents e(c) In this notation the above equation takes the following form: Next we describe how we assign a probability to the expansion e of a constituent