 This paper describes the functioning of a broad-coverage probabilistic top-down parser and its application to the problem of language modeling for speech recognition The hope however is that the improved parsing model provided by our conditional probability model will cause the distribution over structures to be more peaked thus enabling us to capture more of the total probability mass and making this a fairly snug upper bound on the perplexity The approach that we will subsequently present uses the probabilistic grammar as its language model but only includes probability mass from those parses that are found that is it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar While language models built around shallow local dependencies are still the standard in state-of-the-art speech recognition systems there is reason to hope that better language models can and will be developed by computational linguists for this task In the past few years however some improvements have been made over these language models through the use of statistical methods of natural language processing and the development of innovative linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists While the improvements over the trigram model in these trials are modest they do indicate that our model is robust enough to provide good information even in the face of noisy input