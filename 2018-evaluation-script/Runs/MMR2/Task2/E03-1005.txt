The probability of a parse tree is computed from the occurrencefrequencies of the subtrees in the treebank
That is the probability of a subtree t is taken as the number of occurrences of t in the training set I t I divided by the total number of occurrences of all subtrees t' with the same root label as t
In Bod (2000b) an alternative notion for the best parse tree was proposed based on a simplicity criterion: instead of producing the most probable tree this model produced the tree generated by the shortest derivation with the fewest training subtrees
Next the rank of each (shortest) derivation is computed as the sum of the ranks of the subtrees involved
In this paper we will estimate the most probable parse by computing the 10 000 most probable derivations by means of Viterbi n-best from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse
It easy to see that this is equivalent to reducing the probability of a tree by a factor of four for each pair of nonterminals it contains resulting in the PCFG reduction in figure 4
While the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head-words later models showed the importance of including context from higher nodes in the tree (Charniak 1997; Johnson 1998a)
The probability of a parse tree T is the sum of the probabilities of its distinct derivations
Moreover Goodman's PCFG reduction may also be used to estimate the most probable parse by Viterbi n-best search which computes the n most likely derivations and then sums up the probabilities of the derivations producing the same tree
