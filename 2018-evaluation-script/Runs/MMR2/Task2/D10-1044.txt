For the TM this is: where cI(s t) is the count in the IN phrase table of pair (s t) po(s|t) is its probability under the OUT TM and cI(t) = &quot;s cI(s' t)
Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models) and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set
For the LM adaptive weights are set as follows: where α is a weight vector containing an element αi for each domain (just IN and OUT in our case) pi are the corresponding domain-specific models and ˜p(w h) is an empirical distribution from a targetlanguage training corpus—we used the IN dev set for this
The second setting uses the news-related subcorpora for the NIST09 MT Chinese to English evaluation8 as IN and the remaining NIST parallel Chinese/English corpora (UN Hong Kong Laws and Hong Kong Hansard) as OUT
With the additional assumption that (s t) can be restricted to the support of co(s t) this is equivalent to a “flat” alternative to (6) in which each non-zero co(s t) is set to one
We used 22 features for the logistic weighting model divided into two groups: one intended to reflect the degree to which a phrase pair belongs to general language and one intended to capture similarity to the IN domain
