By taking the ratio of matching n-grams to the total number of n-grams in the system output we obtain the precision pn for each n-gram order n
The results of the manual and automatic evaluation of the participating system translations is detailed in the figures at the end of this paper
For more on the participating systems please refer to the respective system description in the proceedings of the workshop
We dropped however one of the languages Finnish partly to keep the number of tracks manageable partly because we assumed that it would be hard to find enough Finnish speakers for the manual evaluation
These values for n-gram precision are combined into a BLEU score: The formula for the BLEU metric also includes a brevity penalty for too short output which is based on the total number of words in the system output c and in the reference r
96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric we want to be able to rank different systems against each other for which we need assessments of statistical significance on the differences between a pair of systems
Hence the different averages of manual scores for the different language pairs reflect the behaviour of the judges not the quality of the systems on different language pairs
In addition to the Europarl test set we also collected 29 editorials from the Project Syndicate website2 which are published in all the four languages of the shared task
Presenting the output of several system allows the human judge to make more informed judgements contrasting the quality of the different systems
