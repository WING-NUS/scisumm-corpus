For each document in the query language we rank all documents in the target language and record the rank of the actual translation
Gibbs sampling involves sequentially resampling each zln from its conditional posterior: where z\l n is the current set of topic assignments for all other tokens in the tuple while (Nt)\l n is the number of occurrences of topic t in the tuple excluding zln the variable being resampled
We calculate the Jensen-Shannon divergence between the topic distributions for each pair of individual documents in S that were originally part of the same tuple prior to separation
We use this corpus to explore the ability of the model both to infer similarities between vocabularies in different languages and to detect differences in topic emphasis between languages
We then use this model to infer topic distributions for each of the 11 documents in each of the held-out document tuples using a method similar to that used to calculate held-out probabilities (Wallach et al
Competitive cross-country skiing (left) accounts for a significant proportion of the text in Finnish but barely exists in Welsh and the languages in the Southeastern region
Finally for each pair of languages (“query” and “target”) we calculate the difference between the topic distribution for each held-out document in the query language and the topic distribution for each held-out document in the target language
We report the number of elements of C that appear in the reference lexica
Although the PLTM is clearly not a substitute for a machine translation system—it has no way to represent syntax or even multi-word phrases—it is clear from the examples in figure 2 that the sets of high probability words in different languages for a given topic are likely to include translations
