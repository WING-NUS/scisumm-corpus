The 1-bit sign is almost always negative and the 8-bit exponent is not fully used on the range of values so in practice this corresponds to quantization ranging from 17 to 20 total bits
The TRIE model continues to use the least memory of ing (-P) with MAP POPULATE the default
Along with IRSTLM and TPT our binary format is memory mapped meaning the file and in-memory representation are the same
The fraction of buckets that are empty is m−1 m so average lookup time is O( m 1) and crucially constant in the number of entries
For 2 < n < N we use a hash table mapping from the n-gram to the probability and backoff3
As noted in Section 1 our code finds the longest matching entry wnf for query p(wn|s(wn−1 f ) The probability p(wn|wn−1 f ) is stored with wnf and the backoffs are immediately accessible in the provided state s(wn−1 When our code walks the data structure to find wnf it visits wnn wnn−1
For even larger models we recommend RandLM; the memory consumption of the cache is not expected to grow with model size and it has been reported to scale well
As the name implies space is O(m) and linear in the number of entries
For the PROBING implementation hash table sizes are in the millions so the most relevant values are on the right size of the graph where linear probing wins
The set of n-grams appearing in a model is sparse and we want to efficiently find their associated probabilities and backoff penalties
