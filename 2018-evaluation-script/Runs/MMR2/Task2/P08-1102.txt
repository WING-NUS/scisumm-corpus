Besides the output of the perceptron the outside-layer also receive the outputs of the word LM the POS LM the co-occurrence model and a word count penalty which is similar to the translation length penalty in SMT
In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus we randomly chosen 2 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84 294 sentences) then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features
We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results
0) to test the performance of the cascaded model on segmentation and Joint S&T
At the first step we conducted a group of contrasting experiments on the core perceptron the first concentrated on the segmentation regardless of the POS information and reported the F-measure on segmentation only while the second performed Joint S&T using POS information and reported the F-measure both on segmentation and on Joint S&T
d means the dimension of the vector space it equals to the amount of features in the model
The first was conducted to test the performance of the perceptron on segmentation on the corpus from SIGHAN Bakeoff 2 including the Academia Sinica Corpus (AS) the Hong Kong City University Corpus (CityU) the Peking University Corpus (PKU) and the Microsoft Research Corpus (MSR)
