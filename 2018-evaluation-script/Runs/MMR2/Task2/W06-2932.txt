Performance is measured through unlabeled accuracy which is the percentage of words that modify the correct head in the dependency graph and labeled accuracy which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph
(i jM) as a sequence labeling problem We use a first-order Markov factorization of the score s(l(i jm) l(i jm�1) i y x) in which each factor is the score of labeling the adjacent edges (i jm) and (i jm−1) in the tree y
The results are promising and show the language independence of our system under the assumption of a labeled dependency corpus in the target language
The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages
We then add to the representation of the edge: Mi as head features Mj as dependent features and also each conjunction of a feature from both sets
To model this we treat the labeling of the edges (i j1)
To test this further we added features to count the number of commas and conjunctions between a dependent verb and its candidate head
Is this the first dependent to the left/right of the head?
Note that many of these features are beyond the scope of the edge based factorizations of the unlabeled parser
Note the difference in error between the unlabeled parser and the edge labeler: the former makes mistakes on edges into prepositions conjunctions and verbs and the latter makes mistakes on edges into nouns (subject/objects)
