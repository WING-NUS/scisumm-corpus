In the previous sections we have concentrated on the relation of the parser to a maximumentropy approach the aspect of the parser that is most novel
In a log-linear model the probability function takes the following form: Here the Ai are weights between negative and positive infinity that indicate the relative importance of a feature: the more relevant the feature to the value of the probability the higher the absolute value of the associated A
The major problem confronting the author of a generative parser is what information to use to condition the probabilities required in the model and how to smooth the empirically obtained probabilities to take the sting out of the sparse data problems that are inevitable with even the most modest conditioning
As we discuss in more detail in Section 5 several different features in the context surrounding c are useful to include in H: the label head pre-terminal and head of the parent of c (denoted as lp tp hp) the label of c's left sibling (lb for &quot;before&quot;) and the label of the grandparent of c (la)
From our perspective perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head
That is suppose we want to compute a conditional probability p(a b c) but we are not sure that we have enough examples of the conditioning event b c in the training corpus to ensure that the empirically obtained probability P (a I b c) is accurate
