 The weak learner for two-class problems computes a weak hypothesis h from the input space into the reals (h : 2x -4 R) where the sign4 of h(x) is interpreted as the predicted label and the magnitude I h(x)I is the confidence in the prediction: large numbers for I h(x)I indicate high confidence in the prediction and numbers close to zero indicate low confidence So the success of the algorithm may well be due to its success in maximizing the number of unlabeled examples on which the two decision lists agree For example the independence assumptions mean that the model fails to capture the dependence between specific and more general features (for example the fact that the feature full They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page and other pages pointing to the page) The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type Output of the learning algorithm: a function h:Xxy [0 1] where h(x y) is an estimate of the conditional probability p(y1x) of seeing label y given that feature x is present Thus an explicit assumption about the redundancy of the features — that either the spelling or context alone should be sufficient to build a classifier — has been built into the algorithm It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations