 The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages The results are promising and show the language independence of our system under the assumption of a labeled dependency corpus in the target language Note the difference in error between the unlabeled parser and the edge labeler: the former makes mistakes on edges into prepositions conjunctions and verbs and the latter makes mistakes on edges into nouns (subject/objects) These features play the obvious role of explicitly modeling consistencies and commonalities between a head and its dependents in terms of attributes like gender case or number For instance the system of McDonald et al We use the MIRA online learner to set the weights (Crammer and Singer 2003; McDonald et al With the availability of resources such as the Penn WSJ Treebank much of the focus in the parsing community had been on producing syntactic representations based on phrase-structure Note that many of these features are beyond the scope of the edge based factorizations of the unlabeled parser 2006; Civit Torruella and MartiAntonin 2002; Nilsson et al The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph However in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages