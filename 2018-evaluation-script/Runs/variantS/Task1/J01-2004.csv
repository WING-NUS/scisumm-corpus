Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,J01-2004,W05-0104,0,2001,0,"Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark, see Roark (2001))","Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark, see Roark (2001))","['1', '7', '209', '132', '277']","<S sid =""1"" ssid = ""1"">This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.</S><S sid =""7"" ssid = ""1"">This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.</S><S sid =""209"" ssid = ""113"">This parser is essentially a stochastic version of the top-down parser described in Aho  Sethi  and Ullman (1986).</S><S sid =""132"" ssid = ""36"">The rest of this paper will present our parsing model  its application to language modeling for speech recognition  and empirical results.</S><S sid =""277"" ssid = ""33"">The first set of results looks at the performance of the parser on the standard corpora for statistical parsing trials: Sections 2-21 (989 860 words  39 832 sentences) of the Penn Treebank (Marcus  Santorini  and Marcinkiewicz 1993) served as the training data  Section 24 (34 199 words  1 346 sentences) as the held-out data for parameter estimation  and Section 23 (59 100 words  2 416 sentences) as the test data.</S>",['Aim_Citation']
2,J01-2004,P08-1013,0,2001,0,"Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition. These models have in common that they explicitly or implicitly use a context-free grammar induced from a tree bank, with the exception of Chelba and Jelinek (2000)",Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition,"['8', '2', '41', '18', '4']","<S sid =""8"" ssid = ""2"">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid =""2"" ssid = ""2"">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid =""41"" ssid = ""29"">There will also be a brief review of previous work using syntactic information for language modeling  before we introduce our model in Section 4.</S><S sid =""18"" ssid = ""6"">Some of the recent literature investigating approaches that use syntactic structure in an attempt to capture long-distance dependencies for language modeling will be reviewed.</S><S sid =""4"" ssid = ""4"">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S>",['Method_Citation']
4,J01-2004,P04-1015,0,"Roark, 2001a",0,"Theperceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn tree bank","The perceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn treebank","['3', '9', '390', '10', '4']","<S sid =""3"" ssid = ""3"">A lexicalized probabilistic topdown parser is then presented  which performs very well  in terms of both the accuracy of returned parses and the efficiency with which they are found  relative to the best broad-coverage statistical parsers.</S><S sid =""9"" ssid = ""3"">A lexicalized probabilistic topdown parser is then presented  which performs very well  in terms of both the accuracy of returned parses and the efficiency with which they are found  relative to the best broad-coverage statistical parsers.</S><S sid =""390"" ssid = ""3"">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S><S sid =""10"" ssid = ""4"">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid =""4"" ssid = ""4"">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S>",['Method_Citation']
5,J01-2004,P04-1015,0,"Roark, 2001a",0,"We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the per ceptron model gives performance competitive to that of the generative model on parsing the Penn tree bank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search","We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the perceptron model gives performance competitive to that of the generative model on parsing the Penn treebank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search","['10', '4', '100', '2', '8']","<S sid =""10"" ssid = ""4"">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid =""4"" ssid = ""4"">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid =""100"" ssid = ""4"">The approach that we will subsequently present uses the probabilistic grammar as its language model  but only includes probability mass from those parses that are found  that is  it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S><S sid =""2"" ssid = ""2"">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid =""8"" ssid = ""2"">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S>",['Aim_Citation']
6,J01-2004,P04-1015,0,2001a,0,"One way around this problem is to adopt a two-pass approach, where GEN (x) is the top N analyses under some initial model, as in the re ranking approach of Collins (2000) .In the current paper we explore alternatives to rerank ing approaches, namely heuristic methods for finding the arg max, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999)","In the current paper we explore alternatives to reranking approaches, namely heuristic methods for finding the argmax, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999)","['5', '11', '379', '300', '404']","<S sid =""5"" ssid = ""5"">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S><S sid =""11"" ssid = ""5"">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S><S sid =""379"" ssid = ""135"">While the improvements over the trigram model in these trials are modest  they do indicate that our model is robust enough to provide good information even in the face of noisy input.</S><S sid =""300"" ssid = ""56"">The next set of results will highlight what recommends this approach most: the ease with which one can estimate string probabilities in a single pass from left to right across the string.</S><S sid =""404"" ssid = ""17"">Also  the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram  and lead to better off-line language models than those that we have presented here.</S>",['Method_Citation']
7,J01-2004,P04-1015,0,2001a,0,"approach The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights","The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights","['12', '6', '39', '388', '114']","<S sid =""12"" ssid = ""6"">A small recognition experiment also demonstrates the utility of the model.</S><S sid =""6"" ssid = ""6"">A small recognition experiment also demonstrates the utility of the model.</S><S sid =""39"" ssid = ""27"">The top-down guidance that is provided makes this approach quite efficient in practice.</S><S sid =""388"" ssid = ""1"">The empirical results presented above are quite encouraging  and the potential of this kind of approach both for parsing and language modeling seems very promising.</S><S sid =""114"" ssid = ""18"">He was able to reduce both sentence and word error rates on the ATIS corpus using this method.</S>",['Method_Citation']
9,J01-2004,P04-1015,0,2001a,0,"Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word","Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word","['1', '7', '209', '132', '277']","<S sid =""1"" ssid = ""1"">This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.</S><S sid =""7"" ssid = ""1"">This paper describes the functioning of a broad-coverage probabilistic top-down parser  and its application to the problem of language modeling for speech recognition.</S><S sid =""209"" ssid = ""113"">This parser is essentially a stochastic version of the top-down parser described in Aho  Sethi  and Ullman (1986).</S><S sid =""132"" ssid = ""36"">The rest of this paper will present our parsing model  its application to language modeling for speech recognition  and empirical results.</S><S sid =""277"" ssid = ""33"">The first set of results looks at the performance of the parser on the standard corpora for statistical parsing trials: Sections 2-21 (989 860 words  39 832 sentences) of the Penn Treebank (Marcus  Santorini  and Marcinkiewicz 1993) served as the training data  Section 24 (34 199 words  1 346 sentences) as the held-out data for parameter estimation  and Section 23 (59 100 words  2 416 sentences) as the test data.</S>",['Method_Citation']
10,J01-2004,P05-1022,0,"Roark, 2001",0,"A good example of this is the Roark parser (Roark, 2001) which works left-to right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning","A good example of this is the Roark parser (Roark, 2001) which works left-to-right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning","['8', '2', '41', '18', '4']","<S sid =""8"" ssid = ""2"">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid =""2"" ssid = ""2"">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid =""41"" ssid = ""29"">There will also be a brief review of previous work using syntactic information for language modeling  before we introduce our model in Section 4.</S><S sid =""18"" ssid = ""6"">Some of the recent literature investigating approaches that use syntactic structure in an attempt to capture long-distance dependencies for language modeling will be reviewed.</S><S sid =""4"" ssid = ""4"">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S>",['Results_Citation']
11,J01-2004,P05-1022,0,"Roark, 2001",0,"At the end one has a beam-width? s number of best parses (Roark, 2001) .The Collins parser (Collins, 1997) does use dynamic programming in its search","At the end one has a beam-width's number of best parses (Roark, 2001)","['3', '9', '390', '10', '4']","<S sid =""3"" ssid = ""3"">A lexicalized probabilistic topdown parser is then presented  which performs very well  in terms of both the accuracy of returned parses and the efficiency with which they are found  relative to the best broad-coverage statistical parsers.</S><S sid =""9"" ssid = ""3"">A lexicalized probabilistic topdown parser is then presented  which performs very well  in terms of both the accuracy of returned parses and the efficiency with which they are found  relative to the best broad-coverage statistical parsers.</S><S sid =""390"" ssid = ""3"">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S><S sid =""10"" ssid = ""4"">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid =""4"" ssid = ""4"">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S>",['Method_Citation']
12,J01-2004,P05-1022,0,"Roark, 2001",0,"To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses","To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses","['10', '4', '100', '2', '8']","<S sid =""10"" ssid = ""4"">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid =""4"" ssid = ""4"">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid =""100"" ssid = ""4"">The approach that we will subsequently present uses the probabilistic grammar as its language model  but only includes probability mass from those parses that are found  that is  it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S><S sid =""2"" ssid = ""2"">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S><S sid =""8"" ssid = ""2"">The paper first introduces key notions in language modeling and probabilistic parsing  and briefly reviews some previous approaches to using syntactic structure for language modeling.</S>",['Method_Citation']
13,J01-2004,P04-1006,0,"Roark, 2001",0,"We ran the first stage parser with 4-timesoverparsing for each string in 7The n? best lists were provided by Brian Roark (Roark, 2001) 8A local-tree is an explicit expansion of an edge and its children","The n-best lists were provided by Brian Roark (Roark, 2001)","['5', '11', '379', '300', '404']","<S sid =""5"" ssid = ""5"">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S><S sid =""11"" ssid = ""5"">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S><S sid =""379"" ssid = ""135"">While the improvements over the trigram model in these trials are modest  they do indicate that our model is robust enough to provide good information even in the face of noisy input.</S><S sid =""300"" ssid = ""56"">The next set of results will highlight what recommends this approach most: the ease with which one can estimate string probabilities in a single pass from left to right across the string.</S><S sid =""404"" ssid = ""17"">Also  the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram  and lead to better off-line language models than those that we have presented here.</S>",['Method_Citation']
14,J01-2004,P05-1063,0,"Roark, 2001a",0,"Incremental top down and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models","Incremental top-down and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models","['12', '6', '39', '388', '114']","<S sid =""12"" ssid = ""6"">A small recognition experiment also demonstrates the utility of the model.</S><S sid =""6"" ssid = ""6"">A small recognition experiment also demonstrates the utility of the model.</S><S sid =""39"" ssid = ""27"">The top-down guidance that is provided makes this approach quite efficient in practice.</S><S sid =""388"" ssid = ""1"">The empirical results presented above are quite encouraging  and the potential of this kind of approach both for parsing and language modeling seems very promising.</S><S sid =""114"" ssid = ""18"">He was able to reduce both sentence and word error rates on the ATIS corpus using this method.</S>",['Aim_Citation']
15,J01-2004,W10-2009,0,"Roark, 2001",0,"Levy, on the other hand ,argued that studies of probabilistic parsing reveal that typically a small number of analyses are as signed the majority of probability mass (Roark, 2001)","Levy, on the other hand, argued that studies of probabilistic parsing reveal that typically a small number of analyses are assigned the majority of probability mass (Roark, 2001)","['13', '14', '15', '404', '383']","<S sid =""13"" ssid = ""1"">With certain exceptions  computational linguists have in the past generally formed a separate research community from speech recognition researchers  despite some obvious overlap of interest.</S><S sid =""14"" ssid = ""2"">Perhaps one reason for this is that  until relatively recently  few methods have come out of the natural language processing community that were shown to improve upon the very simple language models still standardly in use in speech recognition systems.</S><S sid =""15"" ssid = ""3"">In the past few years  however  some improvements have been made over these language models through the use of statistical methods of natural language processing  and the development of innovative  linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.</S><S sid =""404"" ssid = ""17"">Also  the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram  and lead to better off-line language models than those that we have presented here.</S><S sid =""383"" ssid = ""139"">It was selected with the goal of high parser accuracy; but in this new domain  parser accuracy is a secondary measure of performance.</S>",['Method_Citation']
17,J01-2004,D09-1034,0,2001,0,"For example, in Demberg and Keller (2008), trials were run deriving surprisal from the Roark (2001) parser under two different conditions: fully lex icalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags)","For example, in Demberg and Keller (2008), trials were run deriving surprisal from the Roark (2001) parser under two different conditions: fully lexicalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags)","['14', '13', '300', '15', '319']","<S sid =""14"" ssid = ""2"">Perhaps one reason for this is that  until relatively recently  few methods have come out of the natural language processing community that were shown to improve upon the very simple language models still standardly in use in speech recognition systems.</S><S sid =""13"" ssid = ""1"">With certain exceptions  computational linguists have in the past generally formed a separate research community from speech recognition researchers  despite some obvious overlap of interest.</S><S sid =""300"" ssid = ""56"">The next set of results will highlight what recommends this approach most: the ease with which one can estimate string probabilities in a single pass from left to right across the string.</S><S sid =""15"" ssid = ""3"">In the past few years  however  some improvements have been made over these language models through the use of statistical methods of natural language processing  and the development of innovative  linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.</S><S sid =""319"" ssid = ""75"">The hope  however  is that the improved parsing model provided by our conditional probability model will cause the distribution over structures to be more peaked  thus enabling us to capture more of the total probability mass  and making this a fairly snug upper bound on the perplexity.</S>",['Method_Citation']
18,J01-2004,D09-1034,0,2001,0,"We modified the Roark (2001) parser to calculate the discussed measures 1, and the empirical results in? 4 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time","We modified the Roark (2001) parser to calculate the discussed measures, and the empirical results in ?4 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time","['15', '97', '13', '38', '5']","<S sid =""15"" ssid = ""3"">In the past few years  however  some improvements have been made over these language models through the use of statistical methods of natural language processing  and the development of innovative  linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.</S><S sid =""97"" ssid = ""1"">There have been attempts to jump over adjacent words to words farther back in the left context  without the use of dependency links or syntactic structure  for example Saul and Pereira (1997) and Rosenfeld (1996  1997).</S><S sid =""13"" ssid = ""1"">With certain exceptions  computational linguists have in the past generally formed a separate research community from speech recognition researchers  despite some obvious overlap of interest.</S><S sid =""38"" ssid = ""26"">This has lead us to a formulation of the conditional probability model in terms of values returned from tree-walking functions that themselves are contextually sensitive.</S><S sid =""5"" ssid = ""5"">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S>",['Method_Citation']
19,J01-2004,D09-1034,0,2001,0,"In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here","In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here","['16', '397', '139', '4', '10']","<S sid =""16"" ssid = ""4"">While language models built around shallow local dependencies are still the standard in state-of-the-art speech recognition systems  there is reason to hope that better language models can and will be developed by computational linguists for this task.</S><S sid =""397"" ssid = ""10"">A top-down parser  in contrast to a standard bottom-up chart parser  has enough information to predict empty categories only where they are likely to occur.</S><S sid =""139"" ssid = ""43"">This on-line characteristic allows our language model to be interpolated on a word-by-word basis with other models  such as the trigram  yielding further improvements.</S><S sid =""4"" ssid = ""4"">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid =""10"" ssid = ""4"">A new language model that utilizes probabilistic top-down parsing is then outlined  and empirical results show that it improves upon previous work in test corpus perplexity.</S>",['Method_Citation']
20,J01-2004,D09-1034,0,2001,0,"At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is re quired to calculate such measures. Let H (D) be the entropy over a set of derivations D, calculated as follows: H (D)=? X D? D? (D) P D?? D? (D?) log? (D) P D?? D? (D?) (10) If the set of derivations D= D (G, W [1, i]) is a set of partial derivations for string W [1, i], then H (D) is a measure of uncertainty over the partial derivations ,i.e., the uncertainty regarding the correct analysis of what has already been processed","At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is required to calculate such measures","['17', '132', '88', '40', '247']","<S sid =""17"" ssid = ""5"">This paper will examine language modeling for speech recognition from a natural language processing point of view.</S><S sid =""132"" ssid = ""36"">The rest of this paper will present our parsing model  its application to language modeling for speech recognition  and empirical results.</S><S sid =""88"" ssid = ""46"">This section will briefly introduce language modeling for statistical speech recognition.'</S><S sid =""40"" ssid = ""28"">The following section will provide some background in probabilistic context-free grammars and language modeling for speech recognition.</S><S sid =""247"" ssid = ""3"">Perplexity is a standard measure within the speech recognition community for comparing language models.</S>",['Method_Citation']
