Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,D10-1044,P11-2074,0,"Foster et al, 2010",0,"Another popular task in SMT is domain adaptation (Foster et al, 2010)","Another popular task in SMT is domain adaptation (Foster et al, 2010)","['1', '65', '145', '64', '62']","<S sid =""1"" ssid = ""1"">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.</S><S sid =""65"" ssid = ""2"">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S><S sid =""145"" ssid = ""2"">Each out-of-domain phrase pair is characterized by a set of simple features intended to reflect how useful it will be.</S><S sid =""64"" ssid = ""1"">The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.</S><S sid =""62"" ssid = ""26"">To approximate these baselines  we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>",['Aim_Citation']
2,D10-1044,P12-1048,0,"Foster et al, 2010",0,"In addition, discriminative weighting methods were proposed to assign appropriate weights to the sentences from training corpus (Matsoukas et al, 2009) or the phrase pairs of phrase table (Foster et al, 2010)","In addition, discriminative weighting methods were proposed to assign appropriate weights to the sentences from training corpus (Matsoukas et al, 2009) or the phrase pairs of phrase table (Foster et al, 2010)","['2', '23', '20', '153', '68']","<S sid =""2"" ssid = ""2"">This extends previous work on discriminative weighting by using a finer granularity  focusing on the properties of instances rather than corpus components  and using a simpler training procedure.</S><S sid =""23"" ssid = ""20"">Our second contribution is to apply instance weighting at the level of phrase pairs.</S><S sid =""20"" ssid = ""17"">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S><S sid =""153"" ssid = ""10"">Finally  we intend to explore more sophisticated instanceweighting features for capturing the degree of generality of phrase pairs.</S><S sid =""68"" ssid = ""5"">First  we learn weights on individual phrase pairs rather than sentences.</S>",['Method_Citation']
3,D10-1044,D12-1129,0,"Foster et al., 2010",0,"Domain knowledge also has the potential to improve open-text applications such as summarization (Ceylan et al 2010) and machine translation (Foster et al., 2010) .Research in Word Sense Disambiguation (Navigli, 2009, WSD), the task aimed at the automatic labeling of text with word senses, has been oriented towards domain text understanding for several years now","Domain knowledge also has the potential to improve open-text applications such as summarization (Ceylan et al 2010) and machine translation (Foster et al., 2010)","['3', '150', '65', '38', '1']","<S sid =""3"" ssid = ""3"">We incorporate instance weighting into a mixture-model framework  and find that it yields consistent improvements over a wide range of baselines.</S><S sid =""150"" ssid = ""7"">In both cases  the instanceweighting approach improved over a wide range of baselines  giving gains of over 2 BLEU points over the best non-adapted baseline  and gains of between 0.6 and 1.8 over an equivalent mixture model (with an identical training procedure but without instance weighting).</S><S sid =""65"" ssid = ""2"">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S><S sid =""38"" ssid = ""2"">The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.</S><S sid =""1"" ssid = ""1"">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.</S>",['Method_Citation']
4,D10-1044,P14-2093,0,2010,0,"Yasuda et al (2008) and Foster et al (2010) ranked the sentence pairs in the general-domain corpus according to the perplexity scores of sentences, which are computed with respect to in-domain language models","Yasuda et al (2008) and Foster et al (2010) ranked the sentence pairs in the general-domain corpus according to the perplexity scores of sentences, which are computed with respect to in-domain language models","['4', '10', '6', '24', '17']","<S sid =""4"" ssid = ""1"">Domain adaptation is a common concern when optimizing empirical NLP applications.</S><S sid =""10"" ssid = ""7"">This is a standard adaptation problem for SMT.</S><S sid =""6"" ssid = ""3"">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid =""24"" ssid = ""21"">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid =""17"" ssid = ""14"">Previous approaches have tried to find examples that are similar to the target domain.</S>",['Aim_Citation']
5,D10-1044,E12-1055,0,2010,0,"However, such confounding factors do not affect the optimization algorithm, which works with a fixed set of phrase pairs, and merely varies? .Our main technical contributions are as fol lows: Additionally to perplexity optimization for linear interpolation, which was first applied by Foster et al (2010), we propose perplexity optimization for weighted counts (equation 3), and a modified implementation of linear interpolation. Also, we independently perform perplexity minimization for all four features of the standard SMTtranslation model: the phrase translation probabilities p (t|s) and p (s|t), and the lexical weights lex (t|s) and lex (s|t)","Our main technical contributions are as follows: Additionally to perplexity optimization for linear interpolation, which was first applied by Foster et al (2010), we propose perplexity optimization for weighted counts (equation 3), and a modified implementation of linear interpolation","['5', '138', '9', '6', '39']","<S sid =""5"" ssid = ""2"">Even when there is training data available in the domain of interest  there is often additional data from other domains that could in principle be used to improve performance.</S><S sid =""138"" ssid = ""7"">However  for multinomial models like our LMs and TMs  there is a one to one correspondence between instances and features  eg the correspondence between a phrase pair (s  t) and its conditional multinomial probability p(s1t).</S><S sid =""9"" ssid = ""6"">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material—though adequate for reasonable performance—is also available.</S><S sid =""6"" ssid = ""3"">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid =""39"" ssid = ""3"">Thus  provided at least this amount of IN data is available—as it is in our setting—adapting these weights is straightforward.</S>",['Method_Citation']
6,D10-1044,E12-1055,0,2010,0,"Matsoukas et al (2009) propose an approach where each sentence is weighted according to a classifier, and Foster et al (2010) ex tend this approach by weighting individual phrase pairs","Matsoukas et al (2009) propose an approach where each sentence is weighted according to a classifier, and Foster et al (2010) extend this approach by weighting individual phrase pairs","['6', '17', '24', '18', '4']","<S sid =""6"" ssid = ""3"">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid =""17"" ssid = ""14"">Previous approaches have tried to find examples that are similar to the target domain.</S><S sid =""24"" ssid = ""21"">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid =""18"" ssid = ""15"">This is less effective in our setting  where IN and OUT are disparate.</S><S sid =""4"" ssid = ""1"">Domain adaptation is a common concern when optimizing empirical NLP applications.</S>",['Method_Citation']
7,D10-1044,E12-1055,0,2010,0,"These more fine-grained methods need not be seen as alternatives to coarse-grained ones. Foster et al (2010) combine the two, applying linear interpolation to combine the instance 542 weighted out-of-domain model with an in-domain model","Foster et al (2010) combine the two, applying linear interpolation to combine the instance weighted out-of-domain model with an in-domain model","['7', '24', '21', '4', '9']","<S sid =""7"" ssid = ""4"">For developers of Statistical Machine Translation (SMT) systems  an additional complication is the heterogeneous nature of SMT components (word-alignment model  language model  translation model  etc.</S><S sid =""24"" ssid = ""21"">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid =""21"" ssid = ""18"">This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.</S><S sid =""4"" ssid = ""1"">Domain adaptation is a common concern when optimizing empirical NLP applications.</S><S sid =""9"" ssid = ""6"">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material—though adequate for reasonable performance—is also available.</S>",['Method_Citation']
8,D10-1044,E12-1055,0,Foster et al 2010,0,Note that both data sets have a relatively high ratio of in-domain to out-of-domain parallel training data (1:20 for DE? EN and 1:5 for HT? EN) Previous research has been performed with ratios of 1:100 (Foster et al 2010) or 1:400 (Axelrod et al 2011),Note that both data sets have a relatively high ratio of in-domain to out-of-domain parallel training data (1:20 for DE? EN and 1:5 for HT? EN); Previous research has been performed with ratios of 1:100 (Foster et al 2010) or 1:400 (Axelrod et al 2011),"['8', '10', '18', '6', '20']","<S sid =""8"" ssid = ""5"">)  which precludes a single universal approach to adaptation.</S><S sid =""10"" ssid = ""7"">This is a standard adaptation problem for SMT.</S><S sid =""18"" ssid = ""15"">This is less effective in our setting  where IN and OUT are disparate.</S><S sid =""6"" ssid = ""3"">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid =""20"" ssid = ""17"">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S>",['Results_Citation']
9,D10-1044,E12-1055,0,Foster et al 2010,0,"We expand on work by (Foster et al 2010) in establishing translation model perplexity minimization as a robust baseline for a weighted combination of translationmodels.15 We demonstrate perplexity optimization for weighted counts, which are a natural extension of unadapted MLE training, but are of little prominence in domain adaptation research",We expand on work by (Foster et al 2010) in establishing translation model perplexity minimization as a robust baseline for a weighted combination of translation models,"['9', '144', '1', '42', '2']","<S sid =""9"" ssid = ""6"">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material—though adequate for reasonable performance—is also available.</S><S sid =""144"" ssid = ""1"">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S><S sid =""1"" ssid = ""1"">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain  determined by both how similar to it they appear to be  and whether they belong to general language or not.</S><S sid =""42"" ssid = ""6"">The natural baseline approach is to concatenate data from IN and OUT.</S><S sid =""2"" ssid = ""2"">This extends previous work on discriminative weighting by using a finer granularity  focusing on the properties of instances rather than corpus components  and using a simpler training procedure.</S>",['Method_Citation']
10,D10-1044,P12-1099,0,"Foster et al, 2010",0,"In addition to the basic approach of concatenation of in-domain and out-of-domain data, we also trained a log-linear mixture model (Foster and Kuhn, 2007) 940 as well as the linear mixture model of (Foster et al, 2010) for conditional phrase-pair probabilities over IN and OUT","In addition to the basic approach of concatenation of in-domain and out-of-domain data, we also trained a log-linear mixture model (Foster and Kuhn, 2007) as well as the linear mixture model of (Foster et al, 2010) for conditional phrase-pair probabilities over IN and OUT","['10', '4', '18', '6', '29']","<S sid =""10"" ssid = ""7"">This is a standard adaptation problem for SMT.</S><S sid =""4"" ssid = ""1"">Domain adaptation is a common concern when optimizing empirical NLP applications.</S><S sid =""18"" ssid = ""15"">This is less effective in our setting  where IN and OUT are disparate.</S><S sid =""6"" ssid = ""3"">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid =""29"" ssid = ""26"">This is a simple and effective alternative to setting weights discriminatively to maximize a metric such as BLEU.</S>",['Method_Citation']
11,D10-1044,P12-1099,0,2010,0,m ?mpm (e? |f?) Our technique for setting? m is similar to that outlined in Foster et al (2010),Our technique for setting ? m is similar to that outlined in Foster et al (2010),"['11', '18', '6', '44', '92']","<S sid =""11"" ssid = ""8"">It is difficult when IN and OUT are dissimilar  as they are in the cases we study.</S><S sid =""18"" ssid = ""15"">This is less effective in our setting  where IN and OUT are disparate.</S><S sid =""6"" ssid = ""3"">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid =""44"" ssid = ""8"">When OUT is large and distinct  its contribution can be controlled by training separate IN and OUT models  and weighting their combination.</S><S sid =""92"" ssid = ""29"">6One of our experimental settings lacks document boundaries  and we used this approximation in both settings for consistency.</S>",['Method_Citation']
12,D10-1044,P12-1099,0,"Foster et al., 2010",0,"m ?mpm (e? |f?) For efficiency and stability, we use the EMalgorithm to find??, rather than L-BFGS as in (Foster et al., 2010)","For efficiency and stability, we use the EM algorithm to find ?, rather than L-BFGS as in (Foster et al., 2010)","['12', '18', '11', '92', '16']","<S sid =""12"" ssid = ""9"">For simplicity  we assume that OUT is homogeneous.</S><S sid =""18"" ssid = ""15"">This is less effective in our setting  where IN and OUT are disparate.</S><S sid =""11"" ssid = ""8"">It is difficult when IN and OUT are dissimilar  as they are in the cases we study.</S><S sid =""92"" ssid = ""29"">6One of our experimental settings lacks document boundaries  and we used this approximation in both settings for consistency.</S><S sid =""16"" ssid = ""13"">First  we aim to explicitly characterize examples from OUT as belonging to general language or not.</S>",['Aim_Citation']
13,D10-1044,P12-1099,0,2010,0,"Foster et al (2010), however, uses a different approach to select related sentences from OUT","Foster et al (2010), however, uses a different approach to select related sentences from OUT","['13', '139', '21', '82', '32']","<S sid =""13"" ssid = ""10"">The techniques we develop can be extended in a relatively straightforward manner to the more general case when OUT consists of multiple sub-domains.</S><S sid =""139"" ssid = ""8"">As mentioned above  it is not obvious how to apply Daum´e’s approach to multinomials  which do not have a mechanism for combining split features.</S><S sid =""21"" ssid = ""18"">This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.</S><S sid =""82"" ssid = ""19"">This is not unreasonable given the application to phrase pairs from OUT  but it suggests that an interesting alternative might be to use a plain log-linear weighting function exp(Ei Aifi(s  t))  with outputs in [0  oo].</S><S sid =""32"" ssid = ""29"">This is a straightforward technique that is arguably better suited to the adaptation task than the standard method of treating representative IN sentences as queries  then pooling the match results.</S>",['Method_Citation']
14,D10-1044,P12-1099,0,2010,0,Foster et al (2010) propose asimilar method for machine translation that uses features to capture degrees of generality,Foster et al (2010) propose a similar method for machine translation that uses features to capture degrees of generality,"['14', '141', '4', '10', '34']","<S sid =""14"" ssid = ""11"">There is a fairly large body of work on SMT adaptation.</S><S sid =""141"" ssid = ""10"">Moving beyond directly related work  major themes in SMT adaptation include the IR (Hildebrand et al.  2005; L¨u et al.  2007; Zhao et al.  2004) and mixture (Finch and Sumita  2008; Foster and Kuhn  2007; Koehn and Schroeder  2007; L¨u et al.  2007) approaches for LMs and TMs described above  as well as methods for exploiting monolingual in-domain text  typically by translating it automatically and then performing self training (Bertoldi and Federico  2009; Ueffing et al.  2007; Schwenk and Senellart  2009).</S><S sid =""4"" ssid = ""1"">Domain adaptation is a common concern when optimizing empirical NLP applications.</S><S sid =""10"" ssid = ""7"">This is a standard adaptation problem for SMT.</S><S sid =""34"" ssid = ""31"">Section 2 describes our baseline techniques for SMT adaptation  and section 3 describes the instance-weighting approach.</S>",['Method_Citation']
15,D10-1044,P13-1126,0,"Foster et al, 2010",0,"As in (Foster et al, 2010), this approach works at the level of phrase pairs","As in (Foster et al, 2010), this approach works at the level of phrase pairs","['15', '67', '27', '20', '88']","<S sid =""15"" ssid = ""12"">We introduce several new ideas.</S><S sid =""67"" ssid = ""4"">We extend the Matsoukas et al approach in several ways.</S><S sid =""27"" ssid = ""24"">Finally  we make some improvements to baseline approaches.</S><S sid =""20"" ssid = ""17"">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S><S sid =""88"" ssid = ""25"">We have not explored this strategy.</S>",['Method_Citation']
16,D10-1044,D11-1033,0,2010,0,"The ranking of the sentences in a general-domain corpus according to in-domain perplexity has also been applied to machine translation by both Yasuda et al (2008), and Foster et al (2010)","The ranking of the sentences in a general-domain corpus according to in-domain perplexity has also been applied to machine translation by both Yasuda et al (2008), and Foster et al (2010)","['16', '17', '24', '42', '68']","<S sid =""16"" ssid = ""13"">First  we aim to explicitly characterize examples from OUT as belonging to general language or not.</S><S sid =""17"" ssid = ""14"">Previous approaches have tried to find examples that are similar to the target domain.</S><S sid =""24"" ssid = ""21"">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid =""42"" ssid = ""6"">The natural baseline approach is to concatenate data from IN and OUT.</S><S sid =""68"" ssid = ""5"">First  we learn weights on individual phrase pairs rather than sentences.</S>",['Method_Citation']
17,D10-1044,D11-1033,0,2010,0,"Foster et al (2010) do not mention what percentage of the corpus they select for their IR-baseline, but they concatenate the data to their in-domain corpus and re port a decrease in performance","Foster et al (2010) do not mention what percentage of the corpus they select for their IR-baseline, but they concatenate the data to their in-domain corpus and report a decrease in performance","['17', '6', '24', '16', '100']","<S sid =""17"" ssid = ""14"">Previous approaches have tried to find examples that are similar to the target domain.</S><S sid =""6"" ssid = ""3"">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid =""24"" ssid = ""21"">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid =""16"" ssid = ""13"">First  we aim to explicitly characterize examples from OUT as belonging to general language or not.</S><S sid =""100"" ssid = ""4"">Figure 1 shows sample sentences from these domains  which are widely divergent.</S>",['Method_Citation']
18,D10-1044,D11-1033,0,2010,0,"Foster et al (2010) further perform this on extracted phrase pairs, not just sentences","Foster et al (2010) further perform this on extracted phrase pairs, not just sentences","['18', '11', '44', '6', '10']","<S sid =""18"" ssid = ""15"">This is less effective in our setting  where IN and OUT are disparate.</S><S sid =""11"" ssid = ""8"">It is difficult when IN and OUT are dissimilar  as they are in the cases we study.</S><S sid =""44"" ssid = ""8"">When OUT is large and distinct  its contribution can be controlled by training separate IN and OUT models  and weighting their combination.</S><S sid =""6"" ssid = ""3"">Realizing gains in practice can be challenging  however  particularly when the target domain is distant from the background data.</S><S sid =""10"" ssid = ""7"">This is a standard adaptation problem for SMT.</S>",['Method_Citation']
19,D10-1044,P14-1012,0,"Foster et al, 2010",0,"To address the first shortcoming, we adapt and extend some simple but effective phrase features as the input features for new DNN feature learning, and these features have been shown significant improvement for SMT, such as, phrase pair similarity (Zhao et al, 2004), phrase frequency, phrase length (Hopkins and May, 2011), and phrase generative probability (Foster et al, 2010), which also show further improvement for new phrase feature learning in our experiments","To address the first shortcoming, we adapt and extend some simple but effective phrase features as the input features for new DNN feature learning, and these features have been shown significant improvement for SMT, such as, phrase pair similarity (Zhao et al, 2004), phrase frequency, phrase length (Hopkins and May, 2011), and phrase generative probability (Foster et al, 2010), which also show further improvement for new phrase feature learning in our experiments","['19', '136', '73', '140', '64']","<S sid =""19"" ssid = ""16"">The idea of distinguishing between general and domain-specific examples is due to Daum´e and Marcu (2006)  who used a maximum-entropy model with latent variables to capture the degree of specificity.</S><S sid =""136"" ssid = ""5"">It is also worth pointing out a connection with Daum´e’s (2007) work that splits each feature into domain-specific and general copies.</S><S sid =""73"" ssid = ""10"">Jiang and Zhai (2007) suggest the following derivation  making use of the true OUT distribution po(s  t): where each fi(s  t) is a feature intended to charac- !0ˆ = argmax pf(s  t) log pθ(s|t) (8) terize the usefulness of (s  t)  weighted by Ai. θ s t pf(s  t)po(s  t) log pθ(s|t) The mixing parameters and feature weights (col- != argmax po (s  t) lectively 0) are optimized simultaneously using dev- θ s t pf(s  t)co(s  t) log pθ(s|t)  set maximum likelihood as before: !�argmax po (s  t) ! θ s t �ˆ = argmax ˜p(s  t) log p(s|t; 0).</S><S sid =""140"" ssid = ""9"">Recent work by Finkel and Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem.</S><S sid =""64"" ssid = ""1"">The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.</S>",['Method_Citation']
