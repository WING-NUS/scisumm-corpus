Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
2,A00-2018,N10-1002,0,"Charniak, 2000",0,"As a benchmark VPC extraction system, we use the Charniak parser (Charniak, 2000)","As a benchmark VPC extraction system, we use the Charniak parser (Charniak, 2000)","['1', '5', '2', '6', '125']","<S sid =""1"" ssid = ""1"">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid =""5"" ssid = ""1"">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S><S sid =""2"" ssid = ""2"">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid =""6"" ssid = ""2"">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid =""125"" ssid = ""16"">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S>",['Aim_Citation']
3,A00-2018,W11-0610,0,"Charniak, 2000",0,"Each of these scores can be calculated from a provided syntactic parse tree, and to generate these we made use of the Charniakparser (Charniak, 2000), also trained on the Switch board tree bank","Each of these scores can be calculated from a provided syntactic parse tree, and to generate these we made use of the Charniak parser (Charniak, 2000), also trained on the Switch board tree bank","['2', '6', '1', '125', '175']","<S sid =""2"" ssid = ""2"">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid =""6"" ssid = ""2"">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid =""1"" ssid = ""1"">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid =""125"" ssid = ""16"">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S><S sid =""175"" ssid = ""2"">This corresponds to an error reduction of 13% over the best previously published single parser results on this test set  those of Collins [9].</S>",['Method_Citation']
4,A00-2018,W06-3119,0,"Charniak, 2000",0,"We then use Charniak? s parser (Charniak, 2000) to generate the most likely parse tree for each English target sentence in the training corpus","We then use Charniak's parser (Charniak, 2000) to generate the most likely parse tree for each English target sentence in the training corpus","['3', '182', '128', '118', '116']","<S sid =""3"" ssid = ""3"">The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.</S><S sid =""182"" ssid = ""9"">As noted above  the main methodological innovation presented here is our &quot;maximumentropy-inspired&quot; model for conditioning and smoothing.</S><S sid =""128"" ssid = ""19"">The model labeled &quot;Old&quot; attempts to recreate the Char97 system using the current program.</S><S sid =""118"" ssid = ""9"">First  it uses a clustering scheme on words to give the system a &quot;soft&quot; clustering of heads and sub-heads.</S><S sid =""116"" ssid = ""7"">This is as opposed to the &quot;Markovgrammar&quot; approach used in the current parser.</S>",['Method_Citation']
5,A00-2018,N03-2024,0,"Charniak, 2000",0,"We were interested in the occurrence of features such as type and number of premodifiers, presence and type of post modifiers, and form of name reference for people. We constructed a large, automatically annotated corpus by merging the output of Charniak? s statistical parser (Charniak, 2000) with that of the IBM named entity recognition system Nominator (Wacholder et al,1997)","We were interested in the occurrence of features such as type and number of premodifiers, presence and type of post modifiers, and form of name reference for people. We constructed a large, automatically annotated corpus by merging the output of Charniak's statistical parser (Charniak, 2000) with that of the IBM named entity recognition system Nominator (Wacholder et al,1997)","['4', '139', '164', '17', '94']","<S sid =""4"" ssid = ""4"">We also present some partial results showing the effects of different conditioning information  including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.</S><S sid =""139"" ssid = ""30"">In Figure 2 we show that this one factor improves performance by nearly 2%.</S><S sid =""164"" ssid = ""55"">When we do so using our maximum-entropy-inspired conditioning  we get another 0.45% improvement in average precision/recall  as indicated in Figure 2 on the line labeled &quot;MaxEnt-Inspired'.</S><S sid =""17"" ssid = ""6"">In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.</S><S sid =""94"" ssid = ""5"">We use the gathered statistics for all observed words  even those with very low counts  though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.</S>",['Aim_Citation']
6,A00-2018,N06-1039,0,"Charniak, 2000",0,"After getting a set of basic clusters, we pass them to an existing statistical parser (Charniak, 2000) and rule-based tree normalizer to obtain a GLARFstructure for each sentence in every article","After getting a set of basic clusters, we pass them to an existing statistical parser (Charniak, 2000) and rule-based tree normalizer to obtain a GLARF structure for each sentence in every article","['5', '1', '2', '6', '7']","<S sid =""5"" ssid = ""1"">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S><S sid =""1"" ssid = ""1"">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid =""2"" ssid = ""2"">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid =""6"" ssid = ""2"">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid =""7"" ssid = ""3"">Following [5 10]  our parser is based upon a probabilistic generative model.</S>",['Method_Citation']
7,A00-2018,C04-1180,0,2000,0,"The levels of accuracy and robustness recently achieved by statistical parsers (e.g. Collins (1999), Charniak (2000)) have led to their use in a number of NLP applications, such as question-answering (Pasca and Harabagiu, 2001), machine translation (Charniak et al, 2003), sentence simplification (Carroll et al, 1999), and a linguist? s search engine (Resnik and Elkiss, 2003)","The levels of accuracy and robustness recently achieved by statistical parsers (e.g. Collins (1999), Charniak (2000)) have led to their use in a number of NLP applications, such as question-answering (Pasca and Harabagiu, 2001), machine translation (Charniak et al, 2003), sentence simplification (Carroll et al, 1999), and a linguist? s search engine (Resnik and Elkiss, 2003)","['2', '6', '1', '125', '175']","<S sid =""2"" ssid = ""2"">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid =""6"" ssid = ""2"">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid =""1"" ssid = ""1"">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid =""125"" ssid = ""16"">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S><S sid =""175"" ssid = ""2"">This corresponds to an error reduction of 13% over the best previously published single parser results on this test set  those of Collins [9].</S>",['Method_Citation']
8,A00-2018,W05-0638,0,"Charniak, 2000",0,"In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000)","In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000)","['7', '171', '90', '1', '5']","<S sid =""7"" ssid = ""3"">Following [5 10]  our parser is based upon a probabilistic generative model.</S><S sid =""171"" ssid = ""62"">As already noted  our best model uses a Markov-grammar approach.</S><S sid =""90"" ssid = ""1"">We created a parser based upon the maximumentropy-inspired model of the last section  smoothed using standard deleted interpolation.</S><S sid =""1"" ssid = ""1"">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid =""5"" ssid = ""1"">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S>",['Method_Citation']
9,A00-2018,P05-1065,0,"Charniak, 2000",0,"We also use a standard statistical parser (Charniak, 2000) to provide syntactic analysis","We also use a standard statistical parser (Charniak, 2000) to provide syntactic analysis","['8', '12', '10', '41', '147']","<S sid =""8"" ssid = ""4"">That is  for all sentences s and all parses 7r  the parser assigns a probability p(s   7r) = p(r)  the equality holding when we restrict consideration to 7r whose yield * This research was supported in part by NSF grant LIS SBR 9720368.</S><S sid =""12"" ssid = ""1"">The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c  t(c) (t for &quot;tag&quot;)  then the lexical head of c  h(c)  and then the expansion of c into further constituents e(c).</S><S sid =""10"" ssid = ""6"">That is  the parser implements the function arg maxrp(7r s) = arg maxirp(7r  s) = arg maxrp(w).</S><S sid =""41"" ssid = ""10"">For example  in computing the probability of the head's pre-terminal t we might want a feature schema f (t  1) that returns 1 if the observed pre-terminal of c = t and the label of c = 1  and zero otherwise.</S><S sid =""147"" ssid = ""38"">For example  when we first guess the lexical head we can move from computing p(r I 1 1p  h) to p(r I /  t  /p  h).</S>",['Results_Citation']
10,A00-2018,P05-1065,0,"Charniak, 2000",0,"For each article, we calculated the per cent age of a) all word instances (tokens) and b) all unique words (types) not on these lists, resulting in three token OOV rate features and three type OOV rate features per article. The parse features are generated using the Charniak parser (Charniak, 2000) trained on the standard Wall Street Journal Treebank corpus","For each article, we calculated the percentage of a) all word instances (tokens) and b) all unique words (types) not on these lists, resulting in three token OOV rate features and three type OOV rate features per article. The parse features are generated using the Charniak parser (Charniak, 2000) trained on the standard Wall Street Journal Treebank corpus","['9', '185', '5', '1', '171']","<S sid =""9"" ssid = ""5"">The author would like to thank Mark Johnson and all the rest of the Brown Laboratory for Linguistic Information Processing. is s. Then for any s the parser returns the parse ir that maximizes this probability.</S><S sid =""185"" ssid = ""12"">We expect that as we experiment with other  more semantic conditioning information  the importance of this aspect of the model will increase.</S><S sid =""5"" ssid = ""1"">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S><S sid =""1"" ssid = ""1"">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid =""171"" ssid = ""62"">As already noted  our best model uses a Markov-grammar approach.</S>",['Method_Citation']
11,A00-2018,P04-1040,0,2000,0,"The evaluation of the transformed output of the parsers of Charniak (2000) and Collins (1999) gives 90 %unlabelled and 84 %labelled accuracy with respect to dependencies, when measured against a dependency corpus derived from the Penn Treebank.The paper is organized as follows","The evaluation of the transformed output of the parsers of Charniak (2000) and Collins (1999) gives 90 % unlabelled and 84 % labelled accuracy with respect to dependencies, when measured against a dependency corpus derived from the Penn Treebank. The paper is organized as follows","['10', '13', '83', '54', '56']","<S sid =""10"" ssid = ""6"">That is  the parser implements the function arg maxrp(7r s) = arg maxirp(7r  s) = arg maxrp(w).</S><S sid =""13"" ssid = ""2"">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question.</S><S sid =""83"" ssid = ""52"">In the simple (content-free) form (Equation 6)  it is clear that Z(H) = 1.</S><S sid =""54"" ssid = ""23"">The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).</S><S sid =""56"" ssid = ""25"">In max-entropy models one can simply include features for all three events f1 (a  b  c)  f2 (a  b)  and f3(a  c) and combine them in the model according to Equation 3  or equivalently  Equation 4.</S>",['Method_Citation']
12,A00-2018,P04-1040,0,2000,0,"Blaheta and Charniak (2000) presented the first method for assigning Pennfunctional tags to constituents identified by a parser. Pattern-matching approaches were used in (John son, 2002) and (Jijkoun, 2003) to recover non-local dependencies in phrase trees","Blaheta and Charniak (2000) presented the first method for assigning Penn functional tags to constituents identified by a parser. Pattern-matching approaches were used in (Johnson, 2002) and (Jijkoun, 2003) to recover non-local dependencies in phrase trees","['11', '53', '82', '54', '188']","<S sid =""11"" ssid = ""7"">What fundamentally distinguishes probabilistic generative parsers is how they compute p(r)  and it is to that topic we turn next.</S><S sid =""53"" ssid = ""22"">That is  suppose we want to compute a conditional probability p(a b  c)  but we are not sure that we have enough examples of the conditioning event b  c in the training corpus to ensure that the empirically obtained probability P (a I b  c) is accurate.</S><S sid =""82"" ssid = ""51"">The major advantage of using Equation 7 is that one can generally get away without computing the partition function Z(H).</S><S sid =""54"" ssid = ""23"">The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).</S><S sid =""188"" ssid = ""15"">Ultimately it is this flexibility that let us try the various conditioning events  to move on to a Markov grammar approach  and to try several Markov grammars of different orders  without significant programming.</S>",['Method_Citation']
13,A00-2018,P04-1040,0,2000,0,"As an alternative to hard coded heuristics, Blaheta and Charniak (2000) proposed to recover the Penn functional tags automatically","As an alternative to hard coded heuristics, Blaheta and Charniak (2000) proposed to recover the Penn functional tags automatically","['12', '18', '134', '147', '33']","<S sid =""12"" ssid = ""1"">The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c  t(c) (t for &quot;tag&quot;)  then the lexical head of c  h(c)  and then the expansion of c into further constituents e(c).</S><S sid =""18"" ssid = ""7"">The method that gives the best results  however  uses a Markov grammar — a method for assigning probabilities to any possible expansion using statistics gathered from the training corpus [6 10 15].</S><S sid =""134"" ssid = ""25"">As already noted  Char97 first guesses the lexical head of a constituent and then  given the head  guesses the PCFG rule used to expand the constituent in question.</S><S sid =""147"" ssid = ""38"">For example  when we first guess the lexical head we can move from computing p(r I 1 1p  h) to p(r I /  t  /p  h).</S><S sid =""33"" ssid = ""2"">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S>",['Aim_Citation']
17,A00-2018,N06-1022,0,2000,0,"The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents","The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents","['13', '16', '45', '83', '41']","<S sid =""13"" ssid = ""2"">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question.</S><S sid =""16"" ssid = ""5"">In this notation the above equation takes the following form: Next we describe how we assign a probability to the expansion e of a constituent.</S><S sid =""45"" ssid = ""14"">The function Z(H)  called the partition function  is a normalizing constant (for fixed H)  so the probabilities over all a sum to one.</S><S sid =""83"" ssid = ""52"">In the simple (content-free) form (Equation 6)  it is clear that Z(H) = 1.</S><S sid =""41"" ssid = ""10"">For example  in computing the probability of the head's pre-terminal t we might want a feature schema f (t  1) that returns 1 if the observed pre-terminal of c = t and the label of c = 1  and zero otherwise.</S>",['Method_Citation']
18,A00-2018,N06-1022,0,2000,0,"Most recently, McDonald et al (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000))","Most recently, McDonald et al (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000))","['14', '73', '60', '159', '151']","<S sid =""14"" ssid = ""3"">Much of the interesting work is determining what goes into H (c).</S><S sid =""73"" ssid = ""42"">We make one more point on the connection of Equation 7 to a maximum entropy formulation.</S><S sid =""60"" ssid = ""29"">But let us look at how it works for a particular case in our parsing scheme.</S><S sid =""159"" ssid = ""50"">Something very much like this is done in [15].</S><S sid =""151"" ssid = ""42"">We have already noted the importance of conditioning on the parent label /p.</S>",['Method_Citation']
19,A00-2018,H05-1035,0,"Charniak, 2000",0,"The feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (Charniak, 2000), trees that will be improved by more accurate PP-attachment decisions","The feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (Charniak, 2000), trees that will be improved by more accurate PP-attachment decisions","['15', '67', '151', '157', '140']","<S sid =""15"" ssid = ""4"">Whenever it is clear to which constituent we are referring we omit the (c) in  e.g.  h(c).</S><S sid =""67"" ssid = ""36"">For example  it does not seem to make much sense to condition on  say  hp without first conditioning on ti .</S><S sid =""151"" ssid = ""42"">We have already noted the importance of conditioning on the parent label /p.</S><S sid =""157"" ssid = ""48"">Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.</S><S sid =""140"" ssid = ""31"">It may not be obvious why this should make so great a difference  since most words are effectively unambiguous.</S>",['Method_Citation']
20,A00-2018,P04-1042,0,2000,0,"Note that the dependency figures of Dienes lag behind even the parsed results for Johnson? s model; this may well be due to the fact that Dienes built his model as an extension of Collins (1999), which lags behind Charniak (2000) by about 1.3-1.5% .Manual investigation of errors on English gold standard data revealed two major issues that suggest further potential for improvement in performance without further increase in algorithmic complexity or training set size","Note that the dependency figures of Dienes lag behind even the parsed results for Johnson's model; this may well be due to the fact that Dienes built his model as an extension of Collins (1999), which lags behind Charniak (2000) by about 1.3-1.5%. Manual investigation of errors on English gold standard data revealed two major issues that suggest further potential for improvement in performance without further increase in algorithmic complexity or training set size","['16', '65', '13', '58', '46']","<S sid =""16"" ssid = ""5"">In this notation the above equation takes the following form: Next we describe how we assign a probability to the expansion e of a constituent.</S><S sid =""65"" ssid = ""34"">We can now rewrite this in the form of Equation 5 as follows: Here we have sequentially conditioned on steadily increasing portions of c's history.</S><S sid =""13"" ssid = ""2"">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question.</S><S sid =""58"" ssid = ""27"">Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.</S><S sid =""46"" ssid = ""15"">Now for our purposes it is useful to rewrite this as a sequence of multiplicative functions gi(a  H) for 0 < i < j: Here go(a  H) = 11Z (H) and gi(a  H) = eAi(a H) fi(° 11).</S>",['Method_Citation']
