Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,W99-0613,N01-1023,0,"Collins and Singer, 1999",0,"Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named entity identification (Collins and Singer, 1999)","Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named entity identification (Collins and Singer, 1999)","['1', '9', '250', '136', '163']","<S sid =""1"" ssid = ""1"">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid =""9"" ssid = ""3"">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid =""250"" ssid = ""1"">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S><S sid =""136"" ssid = ""3"">We then discuss how we adapt and generalize a boosting algorithm  AdaBoost  to the problem of named entity classification.</S><S sid =""163"" ssid = ""30"">We now describe the CoBoost algorithm for the named entity problem.</S>",['Aim_Citation']
2,W99-0613,N01-1023,0,"Collins and Singer, 1999",0,"They also discuss an application of classifying web pages by using their method of mutually constrained models. (Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms toAdaBoost which force the classifiers to agree (called Co Boosting)","(Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms to AdaBoost which force the classifiers to agree (called Co Boosting)","['2', '17', '250', '105', '248']","<S sid =""2"" ssid = ""2"">A large number of rules is needed for coverage of the domain  suggesting that a fairly large number of labeled examples should be required to train a classi- However  we show that the use of data can reduce the requirements for supervision to just 7 simple &quot;seed&quot; rules.</S><S sid =""17"" ssid = ""11"">At first glance  the problem seems quite complex: a large number of rules is needed to cover the domain  suggesting that a large number of labeled examples is required to train an accurate classifier.</S><S sid =""250"" ssid = ""1"">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S><S sid =""105"" ssid = ""38"">Thus the method makes the fairly strong assumption that the features can be partitioned into two types such that each type alone is sufficient for classification.</S><S sid =""248"" ssid = ""15"">With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.</S>",['Method_Citation']
3,W99-0613,W03-1509,0,Collins and Singer 1999,0,"Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick, et al 1999] and so on","Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick, et al 1999] and so on","['3', '92', '121', '58', '232']","<S sid =""3"" ssid = ""3"">The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.</S><S sid =""92"" ssid = ""25"">Thus an explicit assumption about the redundancy of the features â€” that either the spelling or context alone should be sufficient to build a classifier â€” has been built into the algorithm.</S><S sid =""121"" ssid = ""54"">They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page  and other pages pointing to the page).</S><S sid =""58"" ssid = ""12"">Having found (spelling  context) pairs in the parsed data  a number of features are extracted.</S><S sid =""232"" ssid = ""11"">For example  the independence assumptions mean that the model fails to capture the dependence between specific and more general features (for example the fact that the feature full.-string=New_York is always seen with the features contains (New) and The baseline method tags all entities as the most frequent class type (organization). contains (York) and is never seen with a feature such as contains (Group) ).</S>",['Method_Citation']
4,W99-0613,C02-1154,0,"Collins and Singer, 1999",0,"DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syn tactically analyzed corpus","DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syntactically analyzed corpus","['26', '4', '68', '163', '88']","<S sid =""26"" ssid = ""20"">We present two algorithms.</S><S sid =""4"" ssid = ""4"">We present two algorithms.</S><S sid =""68"" ssid = ""1"">The first unsupervised algorithm we describe is based on the decision list method from (Yarowsky 95).</S><S sid =""163"" ssid = ""30"">We now describe the CoBoost algorithm for the named entity problem.</S><S sid =""88"" ssid = ""21"">We can now compare this algorithm to that of (Yarowsky 95).</S>",['Aim_Citation']
5,W99-0613,C02-1154,0,"Collins and Singer, 1999",0,"(Collins and Singer, 1999) also makes use of competing categories (person, organization, and location), which cover 96% of all the instances itset out to classify","(Collins and Singer, 1999) also makes use of competing categories (person, organization, and location), which cover 96% of all the instances it set out to classify","['5', '31', '6', '97', '68']","<S sid =""5"" ssid = ""5"">The first method uses a similar algorithm to that of (Yarowsky 95)  with modifications motivated by (Blum and Mitchell 98).</S><S sid =""31"" ssid = ""25"">Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).</S><S sid =""6"" ssid = ""6"">The second algorithm extends ideas from boosting algorithms  designed for supervised learning tasks  to the framework suggested by (Blum and Mitchell 98).</S><S sid =""97"" ssid = ""30"">It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.</S><S sid =""68"" ssid = ""1"">The first unsupervised algorithm we describe is based on the decision list method from (Yarowsky 95).</S>",['Method_Citation']
6,W99-0613,W06-2204,0,"Collins and Singer, 1999",0,"In (Collins and Singer, 1999) Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification","In (Collins and Singer, 1999) Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification","['6', '31', '5', '204', '30']","<S sid =""6"" ssid = ""6"">The second algorithm extends ideas from boosting algorithms  designed for supervised learning tasks  to the framework suggested by (Blum and Mitchell 98).</S><S sid =""31"" ssid = ""25"">Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).</S><S sid =""5"" ssid = ""5"">The first method uses a similar algorithm to that of (Yarowsky 95)  with modifications motivated by (Blum and Mitchell 98).</S><S sid =""204"" ssid = ""71"">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid =""30"" ssid = ""24"">(Blum and Mitchell 98) offer a promising formulation of redundancy  also prove some results about how the use of unlabeled examples can help classification  and suggest an objective function when training with unlabeled examples.</S>",['Method_Citation']
8,W99-0613,W03-1022,0,"Collins and Singer, 1999",0,"Collins and Singer (1999) for example report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999)","Collins and Singer (1999) for example report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999)","['7', '2', '250', '3', '130']","<S sid =""7"" ssid = ""1"">Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision  in the form of labeled training examples.</S><S sid =""2"" ssid = ""2"">A large number of rules is needed for coverage of the domain  suggesting that a fairly large number of labeled examples should be required to train a classi- However  we show that the use of data can reduce the requirements for supervision to just 7 simple &quot;seed&quot; rules.</S><S sid =""250"" ssid = ""1"">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S><S sid =""3"" ssid = ""3"">The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.</S><S sid =""130"" ssid = ""63"">So the success of the algorithm may well be due to its success in maximizing the number of unlabeled examples on which the two decision lists agree.</S>",['Method_Citation']
9,W99-0613,E09-1018,0,"Collinsand Singer, 1999",0,"While EM has worked quite well for a few tasks, notably ma chine translations (starting with the IBM models 1-5 (Brown et al, 1993), it has not had success inmost others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collinsand Singer, 1999) and context-free-grammar induction (numerous attempts, too many to mention)","While EM has worked quite well for a few tasks, notably machine translations (starting with the IBM models 1-5 (Brown et al, 1993), it has not had success in most others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collinsand Singer, 1999) and context-free-grammar induction (numerous attempts, too many to mention)","['8', '27', '120', '40', '88']","<S sid =""8"" ssid = ""2"">Recent results (e.g.  (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.</S><S sid =""27"" ssid = ""21"">The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).</S><S sid =""120"" ssid = ""53"">(Blum and Mitchell 98) go on to give PAC results for learning in the cotraining case.</S><S sid =""40"" ssid = ""34"">(Berland and Charniak 99) describe a method for extracting parts of objects from wholes (e.g.  &quot;speedometer&quot; from &quot;car&quot;) from a large corpus using hand-crafted patterns.</S><S sid =""88"" ssid = ""21"">We can now compare this algorithm to that of (Yarowsky 95).</S>",['Results_Citation']
11,W99-0613,W07-1712,0,"Collins and Singer, 1999",0,"In addition, we would also like to explore the semi-supervised techniques such as co-training and self-training (Collins and Singer, 1999)","In addition, we would also like to explore the semi-supervised techniques such as co-training and self-training (Collins and Singer, 1999)","['1', '9', '250', '136', '163']","<S sid =""1"" ssid = ""1"">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid =""9"" ssid = ""3"">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid =""250"" ssid = ""1"">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S><S sid =""136"" ssid = ""3"">We then discuss how we adapt and generalize a boosting algorithm  AdaBoost  to the problem of named entity classification.</S><S sid =""163"" ssid = ""30"">We now describe the CoBoost algorithm for the named entity problem.</S>",['Method_Citation']
12,W99-0613,W09-2208,0,"Collins and Singer, 1999",0,"Collins et al (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky ?smethod (Yarowsky, 1995) and the framework suggested by (Blum and Mitchell, 1998)","Collins et al (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky's method (Yarowsky, 1995) and the framework suggested by (Blum and Mitchell, 1998)","['10', '147', '35', '224', '74']","<S sid =""10"" ssid = ""4"">The task is to learn a function from an input string (proper name) to its type  which we will assume to be one of the categories Person  Organization  or Location.</S><S sid =""147"" ssid = ""14"">The weak learner for two-class problems computes a weak hypothesis h from the input space into the reals (h : 2x -4 R)  where the sign4 of h(x) is interpreted as the predicted label and the magnitude I h(x)I is the confidence in the prediction: large numbers for I h(x)I indicate high confidence in the prediction  and numbers close to zero indicate low confidence.</S><S sid =""35"" ssid = ""29"">AdaBoost finds a weighted combination of simple (weak) classifiers  where the weights are chosen to minimize a function that bounds the classification error on a set of training examples.</S><S sid =""224"" ssid = ""3"">The model was parameterized such that the joint probability of a (label  feature-set) pair P(yi  xi) is written as The model assumes that (y  x) pairs are generated by an underlying process where the label is first chosen with some prior probability P(yi); the number of features mi is then chosen with some probability P(mi); finally the features are independently generated with probabilities P(xulyi).</S><S sid =""74"" ssid = ""7"">Output of the learning algorithm: a function h:Xxy [0  1] where h(x  y) is an estimate of the conditional probability p(y1x) of seeing label y given that feature x is present.</S>",['Method_Citation']
13,W99-0613,W06-2207,0,"Collins and Singer, 1999",0,"This approach was shown to perform well on real-world natural language problems (Collins and Singer, 1999)","This approach was shown to perform well on real-world natural language problems (Collins and Singer, 1999)","['11', '40', '23', '48', '55']","<S sid =""11"" ssid = ""5"">For example  a good classifier would identify Mrs. Frank as a person  Steptoe & Johnson as a company  and Honduras as a location.</S><S sid =""40"" ssid = ""34"">(Berland and Charniak 99) describe a method for extracting parts of objects from wholes (e.g.  &quot;speedometer&quot; from &quot;car&quot;) from a large corpus using hand-crafted patterns.</S><S sid =""23"" ssid = ""17"">For example  in ..  says Mr. Cooper  a vice president of.. both a spelling feature (that the string contains Mr.) and a contextual feature (that president modifies the string) are strong indications that Mr. Cooper is of type Person.</S><S sid =""48"" ssid = ""2"">For example  take ...  says Maury Cooper  a vice president at S.&P.</S><S sid =""55"" ssid = ""9"">In addition to the named-entity string (Maury Cooper or Georgia)  a contextual predictor was also extracted.</S>",['Method_Citation']
15,W99-0613,W06-2207,0,"Collins and Singer, 1999",0,"(6) Similarly to (Collins and Singer, 1999) we used T= 0.95 for all experiments reported here","(6) Similarly to (Collins and Singer, 1999) we used T= 0.95 for all experiments reported here","['12', '58', '22', '165', '55']","<S sid =""12"" ssid = ""6"">The approach uses both spelling and contextual rules.</S><S sid =""58"" ssid = ""12"">Having found (spelling  context) pairs in the parsed data  a number of features are extracted.</S><S sid =""22"" ssid = ""16"">In many cases  inspection of either the spelling or context alone is sufficient to classify an example.</S><S sid =""165"" ssid = ""32"">In the namedentity problem each example is a (spelling context) pair.</S><S sid =""55"" ssid = ""9"">In addition to the named-entity string (Maury Cooper or Georgia)  a contextual predictor was also extracted.</S>",['Aim_Citation']
16,W99-0613,P12-1065,0,1999,0,We use Collins and Singer (1999) for our exact specification of Yarowsky.2 It uses DL rule scores ?fj?| ?fj |+ |? f |+ L (1) where  is a smoothing constant,We use Collins and Singer (1999) for our exact specification of Yarowsky,"['13', '14', '61', '22', '165']","<S sid =""13"" ssid = ""7"">A spelling rule might be a simple look-up for the string (e.g.  a rule that Honduras is a location) or a rule that looks at words within a string (e.g.  a rule that any string containing Mr. is a person).</S><S sid =""14"" ssid = ""8"">A contextual rule considers words surrounding the string in the sentence in which it appears (e.g.  a rule that any proper name modified by an appositive whose head is president is a person).</S><S sid =""61"" ssid = ""15"">The following features were used: full-string=x The full string (e.g.  for Maury Cooper  full- s tring=Maury_Cooper). contains(x) If the spelling contains more than one word  this feature applies for any words that the string contains (e.g.  Maury Cooper contributes two such features  contains (Maury) and contains (Cooper) . allcapl This feature appears if the spelling is a single word which is all capitals (e.g.  IBM would contribute this feature). allcap2 This feature appears if the spelling is a single word which is all capitals or full periods  and contains at least one period.</S><S sid =""22"" ssid = ""16"">In many cases  inspection of either the spelling or context alone is sufficient to classify an example.</S><S sid =""165"" ssid = ""32"">In the namedentity problem each example is a (spelling context) pair.</S>",['Method_Citation']
