Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,W11-2123,W11-2138,0,"Heafield, 2011",0,"We used common tools for phrase-based translation? Moses (Koehn et al, 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modelling and GIZA++ (Och and Ney, 2000) for word alignments","We used common tools for phrase-based translation Moses (Koehn et al, 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modelling and GIZA++ (Och and Ney, 2000) for word alignments","['1', '274', '7', '259', '45']","<S sid =""1"" ssid = ""1"">We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.</S><S sid =""274"" ssid = ""1"">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S><S sid =""7"" ssid = ""2"">This paper presents methods to query N-gram language models  minimizing time and space costs.</S><S sid =""259"" ssid = ""1"">There any many techniques for improving language model speed and reducing memory consumption.</S><S sid =""45"" ssid = ""23"">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S>",['Aim_Citation']
2,W11-2123,P14-2022,0,"Heafield, 2011",0,"The language model was com piled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run","The language model was compiled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run","['2', '194', '23', '45', '120']","<S sid =""2"" ssid = ""2"">The structure uses linear probing hash tables and is designed for speed.</S><S sid =""194"" ssid = ""13"">Further  the probing hash table does only one random lookup per query  explaining why it is faster on large data.</S><S sid =""23"" ssid = ""1"">We implement two data structures: PROBING  designed for speed  and TRIE  optimized for memory.</S><S sid =""45"" ssid = ""23"">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S><S sid =""120"" ssid = ""24"">The compressed variant uses block compression and is rather slow as a result.</S>",['Method_Citation']
3,W11-2123,W12-3145,0,"Heafield, 2011",0,"Thus given afragment tf consisting of a sequence of target to kens, we compute LM scores for (i) < s& gt ;tf, (ii )tf and (iii )tf < /s& gt; and use the best score (only) for pruning.2 While this increases the number ofLM queries, we exploit the language model state in formation in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states","Thus given a fragment tf consisting of a sequence of target tokens, we compute LM scores for (i) < s& gt ;tf, (ii )tf and (iii )tf < /s& gt; and use the best score (only) for pruning. While this increases the number of LM queries, we exploit the language model state in formation in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states","['3', '275', '276', '128', '161']","<S sid =""3"" ssid = ""3"">Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S sid =""275"" ssid = ""2"">The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.</S><S sid =""276"" ssid = ""3"">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S><S sid =""128"" ssid = ""32"">These models generally outperform our memory consumption but are much slower  even when cached.</S><S sid =""161"" ssid = ""33"">In both cases  SRILM walks its trie an additional time to minimize context as mentioned in Section 4.1.</S>",['Method_Citation']
4,W11-2123,W12-3131,0,"Heafield, 2011",0,"Our translation system uses cdec (Dyer et al,2010), an implementation of the hierarchical phrase based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference","Our translation system uses cdec (Dyer et al,2010), an implementation of the hierarchical phrase based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference","['4', '279', '169', '282', '266']","<S sid =""4"" ssid = ""4"">Our code is thread-safe  and integrated into the Moses  cdec  and Joshua translation systems.</S><S sid =""279"" ssid = ""6"">The code is opensource  has minimal dependencies  and offers both C++ and Java interfaces for integration.</S><S sid =""169"" ssid = ""41"">In our case multi-threading is trivial because our data structures are read-only and uncached.</S><S sid =""282"" ssid = ""3"">Adam Pauls provided a pre-release comparison to BerkeleyLM and an initial Java interface.</S><S sid =""266"" ssid = ""8"">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S>",['Aim_Citation']
5,W11-2123,W12-3154,0,"Heafield, 2011",0,"The three data sets in use in this paper are summarised in Table 1.The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al, 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime","The three data sets in use in this paper are summarised in Table 1.The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al, 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime","['5', '1', '274', '26', '129']","<S sid =""5"" ssid = ""5"">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S sid =""1"" ssid = ""1"">We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.</S><S sid =""274"" ssid = ""1"">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S><S sid =""26"" ssid = ""4"">We use two common techniques  hash tables and sorted arrays  describing each before the model that uses the technique.</S><S sid =""129"" ssid = ""1"">In addition to the optimizations specific to each datastructure described in Section 2  we implement several general optimizations for language modeling.</S>",['Method_Citation']
6,W11-2123,P12-2058,0,"Heafield, 2011",0,"The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011)","The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011)","['6', '130', '184', '7', '259']","<S sid =""6"" ssid = ""1"">Language models are widely applied in natural language processing  and applications such as machine translation make very frequent queries.</S><S sid =""130"" ssid = ""2"">Applications such as machine translation use language model probability as a feature to assist in choosing between hypotheses.</S><S sid =""184"" ssid = ""3"">Sparse lookup is a key subproblem of language model queries.</S><S sid =""7"" ssid = ""2"">This paper presents methods to query N-gram language models  minimizing time and space costs.</S><S sid =""259"" ssid = ""1"">There any many techniques for improving language model speed and reducing memory consumption.</S>",['Method_Citation']
7,W11-2123,W11-2139,0,2011,0,Inference was carried out using the language modeling library described by Heafield (2011),Inference was carried out using the language modeling library described by Heafield (2011),"['7', '1', '274', '259', '45']","<S sid =""7"" ssid = ""2"">This paper presents methods to query N-gram language models  minimizing time and space costs.</S><S sid =""1"" ssid = ""1"">We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.</S><S sid =""274"" ssid = ""1"">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S><S sid =""259"" ssid = ""1"">There any many techniques for improving language model speed and reducing memory consumption.</S><S sid =""45"" ssid = ""23"">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S>",['Method_Citation']
8,W11-2123,P13-2003,0,"Heafield, 2011",0,"We used the MADA ATB segmentation for Arabic (Roth et al, 2008) and true casing for English, phrases of maximal length 7, KneserNey smoothing, and lexicalized reordering (Koehn et al, 2005), and a 5-gram language model, trained on GigaWordv.5 using KenLM (Heafield, 2011)","We used the MADA ATB segmentation for Arabic (Roth et al, 2008) and true casing for English, phrases of maximal length 7, KneserNey smoothing, and lexicalized reordering (Koehn et al, 2005), and a 5-gram language model, trained on GigaWordv.5 using KenLM (Heafield, 2011)","['8', '84', '88', '72', '46']","<S sid =""8"" ssid = ""3"">Queries take the form p(wn|wn−1 1 ) where wn1 is an n-gram.</S><S sid =""84"" ssid = ""62"">In a model we built with default settings  1.2% of n + 1-grams were missing their ngram suffix.</S><S sid =""88"" ssid = ""66"">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S><S sid =""72"" ssid = ""50"">We maintain a separate array for each length n containing all n-gram entries sorted in suffix order.</S><S sid =""46"" ssid = ""24"">Unigram lookup is dense so we use an array of probability and backoff values.</S>",['Results_Citation']
9,W11-2123,W12-3134,0,2011,0,The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language mod els by Heafield (2011) and Pauls and Klein (2011)? both language model implementations are now integrated with Joshua,The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language models by Heafield (2011) and Pauls and Klein (2011)? both language model implementations are now integrated with Joshua,"['9', '127', '136', '78', '154']","<S sid =""9"" ssid = ""4"">Backoff-smoothed models estimate this probability based on the observed entry with longest matching history wnf   returning where the probability p(wn|wn−1 f ) and backoff penalties b(wn−1 i ) are given by an already-estimated model.</S><S sid =""127"" ssid = ""31"">Minimal perfect hashing is used to find the index at which a quantized probability and possibly backoff are stored.</S><S sid =""136"" ssid = ""8"">We offer a state function s(wn1) = wn� where substring wn� is guaranteed to extend (to the right) in the same way that wn1 does for purposes of language modeling.</S><S sid =""78"" ssid = ""56"">Unigram records store probability  backoff  and an index in the bigram table.</S><S sid =""154"" ssid = ""26"">To optimize left-to-right queries  we extend state to store backoff information: where m is the minimal context from Section 4.1 and b is the backoff penalty.</S>",['Method_Citation']
10,W11-2123,W12-3134,0,2011,0,Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets,Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets,"['10', '25', '24', '93', '85']","<S sid =""10"" ssid = ""5"">The problem is to store these two values for a large and sparse set of n-grams in a way that makes queries efficient.</S><S sid =""25"" ssid = ""3"">An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.</S><S sid =""24"" ssid = ""2"">The set of n-grams appearing in a model is sparse  and we want to efficiently find their associated probabilities and backoff penalties.</S><S sid =""93"" ssid = ""71"">The cost of storing these averages  in bits  is Because there are comparatively few unigrams  we elected to store them byte-aligned and unquantized  making every query faster.</S><S sid =""85"" ssid = ""63"">This causes a problem for reverse trie implementations  including SRILM itself  because it leaves n+1-grams without an n-gram node pointing to them.</S>",['Method_Citation']
11,W11-2123,W12-3134,0,2011,0,"With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult to compile SRILM toolkit (Stolcke, 2002)","With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult to compile SRILM toolkit (Stolcke, 2002)","['11', '68', '266', '259', '45']","<S sid =""11"" ssid = ""6"">Many packages perform language model queries.</S><S sid =""68"" ssid = ""46"">The trie data structure is commonly used for language modeling.</S><S sid =""266"" ssid = ""8"">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S><S sid =""259"" ssid = ""1"">There any many techniques for improving language model speed and reducing memory consumption.</S><S sid =""45"" ssid = ""23"">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S>",['Method_Citation']
12,W11-2123,W12-3160,0,"Heafield, 2011",0,"This was used to create a KenLM (Heafield, 2011)","This was used to create a KenLM (Heafield, 2011)","['12', '13', '14', '103', '97']","<S sid =""12"" ssid = ""7"">Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke  2002) is a popular toolkit based on tries used in several decoders.</S><S sid =""13"" ssid = ""8"">IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.</S><S sid =""14"" ssid = ""9"">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S><S sid =""103"" ssid = ""7"">IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.</S><S sid =""97"" ssid = ""1"">SRILM (Stolcke  2002) is widely used within academia.</S>",['Aim_Citation']
13,W11-2123,W12-3706,0,"Heafield, 2011",0,"In the Opinum system we query the M p, M n mod els with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable fora web application","In the Opinum system we query the M p, M n models with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable for a web application","['13', '103', '15', '14', '262']","<S sid =""13"" ssid = ""8"">IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.</S><S sid =""103"" ssid = ""7"">IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.</S><S sid =""15"" ssid = ""10"">RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.</S><S sid =""14"" ssid = ""9"">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S><S sid =""262"" ssid = ""4"">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S>",['Method_Citation']
14,W11-2123,W11-2147,0,"Heafield, 2011",0,"Our base line is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al, 2007) for translation model training and decoding, GIZA++ (Ochand Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights","Our base line is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al, 2007) for translation model training and decoding, GIZA++ (Ochand Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights","['14', '15', '103', '97', '266']","<S sid =""14"" ssid = ""9"">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S><S sid =""15"" ssid = ""10"">RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.</S><S sid =""103"" ssid = ""7"">IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.</S><S sid =""97"" ssid = ""1"">SRILM (Stolcke  2002) is widely used within academia.</S><S sid =""266"" ssid = ""8"">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S>",['Method_Citation']
15,W11-2123,E12-1083,0,"Heafield, 2011",0,"For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011)","For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011)","['15', '14', '103', '13', '262']","<S sid =""15"" ssid = ""10"">RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.</S><S sid =""14"" ssid = ""9"">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S><S sid =""103"" ssid = ""7"">IRSTLM (Federico et al.  2008) is an open-source toolkit for building and querying language models.</S><S sid =""13"" ssid = ""8"">IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.</S><S sid =""262"" ssid = ""4"">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S>",['Method_Citation']
16,W11-2123,P12-1002,0,"Heafield, 2011",0,"Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Sima ?an,2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well.3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using kenlm (Heafield, 2011)","Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Simaan,2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well.3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using kenlm (Heafield, 2011)","['16', '15', '51', '262', '13']","<S sid =""16"" ssid = ""11"">BerkeleyLM revision 152 (Pauls and Klein  2011) implements tries based on hash tables and sorted arrays in Java with lossy quantization.</S><S sid =""15"" ssid = ""10"">RandLM 0.2 (Talbot and Osborne  2007) stores large-scale models in less memory using randomized data structures.</S><S sid =""51"" ssid = ""29"">This differs from other implementations (Stolcke  2002; Pauls and Klein  2011) that use hash tables as nodes in a trie  as explained in the next section.</S><S sid =""262"" ssid = ""4"">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid =""13"" ssid = ""8"">IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.</S>",['Method_Citation']
17,W11-2123,D12-1108,0,"Heafield, 2011",0,"n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3","n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3","['17', '18', '262', '215', '114']","<S sid =""17"" ssid = ""12"">Sheffield Guthrie and Hepple (2010) explore several randomized compression techniques  but did not release code.</S><S sid =""18"" ssid = ""13"">TPT Germann et al. (2009) describe tries with better locality properties  but did not release code.</S><S sid =""262"" ssid = ""4"">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S><S sid =""215"" ssid = ""34"">Another option is the closedsource data structures from Sheffield (Guthrie and Hepple  2010).</S><S sid =""114"" ssid = ""18"">Later  BerkeleyLM (Pauls and Klein  2011) described ideas similar to ours.</S>",['Method_Citation']
18,W11-2123,P12-2006,0,"Heafield, 2011",0,"Research efforts to increase search efficiency for phrase-based MT (Koehn et al, 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al, 2006) to additional early pruning techniques (Delaney et al, 2006), (Moore and Quirk, 2007) and more efficient language model (LM) querying (Heafield, 2011)","Research efforts to increase search efficiency for phrase-based MT (Koehn et al, 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al, 2006) to additional early pruning techniques (Delaney et al, 2006), (Moore and Quirk, 2007) and more efficient language model (LM) querying (Heafield, 2011)","['18', '17', '232', '114', '262']","<S sid =""18"" ssid = ""13"">TPT Germann et al. (2009) describe tries with better locality properties  but did not release code.</S><S sid =""17"" ssid = ""12"">Sheffield Guthrie and Hepple (2010) explore several randomized compression techniques  but did not release code.</S><S sid =""232"" ssid = ""51"">Memory usage is likely much lower than ours. fThe original paper (Germann et al.  2009) provided only 2s of query timing and compared with SRI when it exceeded available RAM.</S><S sid =""114"" ssid = ""18"">Later  BerkeleyLM (Pauls and Klein  2011) described ideas similar to ours.</S><S sid =""262"" ssid = ""4"">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S>",['Method_Citation']
19,W11-2123,P13-2073,0,"Heafield, 2011",0,"For English language modeling, we use English Giga word Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011)","For English language modeling, we use English Giga word Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011)","['19', '129', '266', '5', '150']","<S sid =""19"" ssid = ""14"">These packages are further described in Section 3.</S><S sid =""129"" ssid = ""1"">In addition to the optimizations specific to each datastructure described in Section 2  we implement several general optimizations for language modeling.</S><S sid =""266"" ssid = ""8"">Finally  other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.</S><S sid =""5"" ssid = ""5"">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S sid =""150"" ssid = ""22"">Section 4.1 explained that state s is stored by applications with partial hypotheses to determine when they can be recombined.</S>",['Method_Citation']
20,W11-2123,P13-1109,0,"Heafield, 2011",0,"For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with KneserNey smoothing","For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with KneserNey smoothing","['20', '128', '276', '1', '275']","<S sid =""20"" ssid = ""15"">We substantially outperform all of them on query speed and offer lower memory consumption than lossless alternatives.</S><S sid =""128"" ssid = ""32"">These models generally outperform our memory consumption but are much slower  even when cached.</S><S sid =""276"" ssid = ""3"">The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.</S><S sid =""1"" ssid = ""1"">We present KenLM  a library that implements two data structures for efficient language model queries  reducing both time and costs.</S><S sid =""275"" ssid = ""2"">The PROBING model is 2.4 times as fast as the fastest alternative  SRILM  and uses less memory too.</S>",['Aim_Citation']
