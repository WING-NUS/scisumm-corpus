Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
2,W06-2932,W06-2920,nan,"McDonald et al, 2006",0,"Introduce through post-processing ,e.g. through reattachment rules (Bick, 2006) or if the change increases overall parse tree probability (McDonald et al, 2006)","Introduce through post-processing ,e.g. through reattachment rules (Bick, 2006) or if the change increases overall parse tree probability (McDonald et al, 2006)","['1', '9', '107', '23', '27']","<S sid =""1"" ssid = ""1"">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S><S sid =""9"" ssid = ""5"">Nivre (2005) gives an introduction to dependency representations of sentences and recent developments in dependency parsing strategies.</S><S sid =""107"" ssid = ""4"">It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.</S><S sid =""23"" ssid = ""5"">That system uses MIRA  an online large-margin learning algorithm  to compute model parameters.</S><S sid =""27"" ssid = ""9"">We augmented this model to incorporate morphological features derived from each token.</S>",['Aim_Citation']
3,W06-2932,W06-2920,0,2006,0,Table 5 shows the official results for submitted parser outputs.31 The two participant groups with the highest total score are McDonald et al (2006) and Nivre et al (2006),Table 5 shows the official results for submitted parser outputs. The two participant groups with the highest total score are McDonald et al (2006) and Nivre et al (2006),"['2', '20', '14', '58', '25']","<S sid =""2"" ssid = ""2"">The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.</S><S sid =""20"" ssid = ""2"">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid =""14"" ssid = ""10"">The results are promising and show the language independence of our system under the assumption of a labeled dependency corpus in the target language.</S><S sid =""58"" ssid = ""6"">These results show that the discriminative spanning tree parsing framework (McDonald et al.  2005b; McDonald and Pereira  2006) is easily adapted across all these languages.</S><S sid =""25"" ssid = ""7"">For instance  the system of McDonald et al. (2005a) incorporates features over the part of speech of words occurring between and around a possible head-dependent relation.</S>",['Method_Citation']
4,W06-2932,W06-2920,0,2006,0,"Even though McDonald et al (2006) and Nivre et al (2006) obtained very similar overall scores, a more detailed look at their performance shows clear differences","Even though McDonald et al (2006) and Nivre et al (2006) obtained very similar overall scores, a more detailed look at their performance shows clear differences","['3', '19', '109', '10', '51']","<S sid =""3"" ssid = ""3"">The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.</S><S sid =""19"" ssid = ""1"">The first stage of our system creates an unlabeled parse y for an input sentence x.</S><S sid =""109"" ssid = ""6"">The current system simply includes all morphological bi-gram features.</S><S sid =""10"" ssid = ""6"">Dependency graphs also encode much of the deep syntactic information needed for further processing.</S><S sid =""51"" ssid = ""20"">Note that many of these features are beyond the scope of the edge based factorizations of the unlabeled parser.</S>",['Method_Citation']
5,W06-2932,W08-1007,0,2006,0,"The high est score on parsing German in the CoNLL-X shared task was obtained by the system of McDonald et al (2006) with a LAS of 87.34 based on the TIGER tree bank, but we want to stress that these results are not comparable due to different data sets (anda different policy regarding the inclusion of punctuation) .The constituency versions were evaluated according to the labeled recall (LR), labeled precision (LP) and labeled F-score (LF)","The highest score on parsing German in the CoNLL-X shared task was obtained by the system of McDonald et al (2006) with a LAS of 87.34 based on the TIGER tree bank, but we want to stress that these results are not comparable due to different data sets (and a different policy regarding the inclusion of punctuation). The constituency versions were evaluated according to the labeled recall (LR), labeled precision (LP) and labeled F-score (LF)","['4', '53', '44', '13', '56']","<S sid =""4"" ssid = ""4"">We report results on the CoNLL-X shared task (Buchholz et al.  2006) data sets and present an error analysis.</S><S sid =""53"" ssid = ""1"">We trained models for all 13 languages provided by the CoNLL organizers (Buchholz et al.  2006).</S><S sid =""44"" ssid = ""13"">We use the MIRA online learner to set the weights (Crammer and Singer  2003; McDonald et al.  2005a) since we found it trained quickly and provide good performance.</S><S sid =""13"" ssid = ""9"">We evaluate this parser on a diverse set of 13 languages using data provided by the CoNLL-X shared-task organizers (Buchholz et al.  2006; Hajiˇc et al.  2004; Simov et al.  2005; Simov and Osenova  2003; Chen et al.  2003; B¨ohmov´a et al.  2003; Kromann  2003; van der Beek et al.  2002; Brants et al.  2002; Kawata and Bartels  2000; Afonso et al.  2002; Dˇzeroski et al.  2006; Civit Torruella and MartiAntonin  2002; Nilsson et al.  2005; Oflazer et al.  2003; Atalay et al.  2003).</S><S sid =""56"" ssid = ""4"">Results on the test set are given in Table 1.</S>",['Aim_Citation']
6,W06-2932,W09-1210,0,2006,nan,McDonald et al (2006) use an additional algorithm,McDonald et al (2006) use an additional algorithm,"['5', '10', '88', '24', '107']","<S sid =""5"" ssid = ""1"">Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.</S><S sid =""10"" ssid = ""6"">Dependency graphs also encode much of the deep syntactic information needed for further processing.</S><S sid =""88"" ssid = ""10"">Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.</S><S sid =""24"" ssid = ""6"">Its power lies in the ability to define a rich set of features over parsing decisions  as well as surface level features relative to these decisions.</S><S sid =""107"" ssid = ""4"">It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.</S>",['Method_Citation']
7,W06-2932,W12-3407,0,"McDonald et al, 2006",0,"Regarding the data-driven parsers, we have made use of MaltParser (Nivre et al, 2007b) and MST Parser (McDonald et al, 2006), two state of the art dependency parsers representing two dominant approaches in data-driven dependency parsing, and that have been successfully applied to typologically different languages and tree banks (McDonald and Nivre, 2007)","Regarding the data-driven parsers, we have made use of MaltParser (Nivre et al, 2007b) and MST Parser (McDonald et al, 2006), two state of the art dependency parsers representing two dominant approaches in data-driven dependency parsing, and that have been successfully applied to typologically different languages and tree banks (McDonald and Nivre, 2007)","['6', '14', '7', '36', '8']","<S sid =""6"" ssid = ""2"">With the availability of resources such as the Penn WSJ Treebank  much of the focus in the parsing community had been on producing syntactic representations based on phrase-structure.</S><S sid =""14"" ssid = ""10"">The results are promising and show the language independence of our system under the assumption of a labeled dependency corpus in the target language.</S><S sid =""7"" ssid = ""3"">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'ˇcuk  1988).</S><S sid =""36"" ssid = ""5"">However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S><S sid =""8"" ssid = ""4"">This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.</S>",['Method_Citation']
8,W06-2932,I08-1012,0,"McDonald et al, 2006",0,"In fact, our approach can also be applied to other parsers, such as (Yamada and Matsumoto, 2003)? s parser, (McDonald et al., 2006)? s parser, and so on","In fact, our approach can also be applied to other parsers, such as (Yamada and Matsumoto, 2003)'s parser, (McDonald et al., 2006)'s parser, and so on","['7', '110', '11', '26', '8']","<S sid =""7"" ssid = ""3"">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'ˇcuk  1988).</S><S sid =""110"" ssid = ""7"">It is our hope that a better morphological feature set will help with both unlabeled parsing and labeling for highly inflected languages.</S><S sid =""11"" ssid = ""7"">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid =""26"" ssid = ""8"">These features are highly important to overall accuracy since they eliminate unlikely scenarios such as a preposition modifying a noun not directly to its left  or a noun modifying a verb with another verb occurring between them.</S><S sid =""8"" ssid = ""4"">This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.</S>",['Method_Citation']
11,W06-2932,N07-1050,0,"McDonald et al, 2006",0,"We have shown that, for languages with a7McDonald et al (2006) use post-processing for non projective dependencies and for labeling",McDonald et al (2006) use post-processing for non-projective dependencies and for labeling,"['8', '7', '30', '11', '6']","<S sid =""8"" ssid = ""4"">This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.</S><S sid =""7"" ssid = ""3"">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'ˇcuk  1988).</S><S sid =""30"" ssid = ""12"">These features play the obvious role of explicitly modeling consistencies and commonalities between a head and its dependents in terms of attributes like gender  case  or number.</S><S sid =""11"" ssid = ""7"">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid =""6"" ssid = ""2"">With the availability of resources such as the Penn WSJ Treebank  much of the focus in the parsing community had been on producing syntactic representations based on phrase-structure.</S>",['Results_Citation']
12,W06-2932,D07-1122,0,"McDonald et al, 2006",0,"As described in (McDonald et al, 2006), we treat the labeling of dependencies as a sequence labeling problem","As described in (McDonald et al, 2006), we treat the labeling of dependencies as a sequence labeling problem","['9', '20', '88', '5', '25']","<S sid =""9"" ssid = ""5"">Nivre (2005) gives an introduction to dependency representations of sentences and recent developments in dependency parsing strategies.</S><S sid =""20"" ssid = ""2"">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid =""88"" ssid = ""10"">Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.</S><S sid =""5"" ssid = ""1"">Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.</S><S sid =""25"" ssid = ""7"">For instance  the system of McDonald et al. (2005a) incorporates features over the part of speech of words occurring between and around a possible head-dependent relation.</S>",['Method_Citation']
14,W06-2932,D07-1015,0,2006,0,5It should be noted that McDonald et al (2006) use a richer feature set that is incomparable to our features,It should be noted that McDonald et al (2006) use a richer feature set that is incomparable to our features,"['10', '5', '24', '34', '107']","<S sid =""10"" ssid = ""6"">Dependency graphs also encode much of the deep syntactic information needed for further processing.</S><S sid =""5"" ssid = ""1"">Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.</S><S sid =""24"" ssid = ""6"">Its power lies in the ability to define a rich set of features over parsing decisions  as well as surface level features relative to these decisions.</S><S sid =""34"" ssid = ""3"">However  the parser is fundamentally limited by the scope of local factorizations that make inference tractable.</S><S sid =""107"" ssid = ""4"">It is our hypothesis that for languages with fine-grained label sets  joint parsing and labeling will improve performance.</S>",['Method_Citation']
18,W06-2932,D10-1004,0,2006,0,"Entries marked with? are the highest reported in the literature, to the best of our knowledge, beating (sometimes slightly) McDonald et al (2006), Martins et al (2008), Martins et al (2009), and, in the case of English Proj., also the third-order parser of Koo and Collins (2010), which achieves 93.04% on that dataset (their experiments in Czech are not comparable, since the datasets are different)","Entries marked with are the highest reported in the literature, to the best of our knowledge, beating (sometimes slightly) McDonald et al (2006), Martins et al (2008), Martins et al (2009), and, in the case of English Proj., also the third-order parser of Koo and Collins (2010), which achieves 93.04% on that dataset (their experiments in Czech are not comparable, since the datasets are different)","['11', '7', '110', '45', '8']","<S sid =""11"" ssid = ""7"">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid =""7"" ssid = ""3"">However  recently their has been a revived interest in parsing models that produce dependency graph representations of sentences  which model words and their arguments through directed edges (Hudson  1984; Mel'ˇcuk  1988).</S><S sid =""110"" ssid = ""7"">It is our hope that a better morphological feature set will help with both unlabeled parsing and labeling for highly inflected languages.</S><S sid =""45"" ssid = ""14"">Furthermore  it made the system homogeneous in terms of learning algorithms since that is what is used to train our unlabeled parser (McDonald and Pereira  2006).</S><S sid =""8"" ssid = ""4"">This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.</S>",['Method_Citation']
19,W06-2932,P08-1108,0,"McDonald et al, 2006",0,"The specific graph-based model studied in this work is that presented by McDonald et al (2006), which factors scores over pairs of arcs (instead of just single arcs) and uses near exhaustive search for unlabeled parsing coupled with a separate classifier to label each arc. We call this system MSTParser, or simply MST for short, which is also the name of the freely available implementation.2 2.3 Transition-Based Models","The specific graph-based model studied in this work is that presented by McDonald et al (2006), which factors scores over pairs of arcs (instead of just single arcs) and uses near exhaustive search for unlabeled parsing coupled with a separate classifier to label each arc. We call this system MSTParser, or simply MST for short, which is also the name of the freely available implementation.","['12', '95', '110', '69', '106']","<S sid =""12"" ssid = ""8"">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S sid =""95"" ssid = ""17"">Note the difference in error between the unlabeled parser and the edge labeler: the former makes mistakes on edges into prepositions  conjunctions and verbs  and the latter makes mistakes on edges into nouns (subject/objects).</S><S sid =""110"" ssid = ""7"">It is our hope that a better morphological feature set will help with both unlabeled parsing and labeling for highly inflected languages.</S><S sid =""69"" ssid = ""7"">However  if we only allow projective parses  do not use morphological features and label edges with a simple atomic classifier  the overall drop in performance becomes significant (row 5 versus row 1).</S><S sid =""106"" ssid = ""3"">First  we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.</S>",['Aim_Citation']
20,W06-2932,P08-1108,0,"McDonald et al, 2006",0,"More precisely, dependency arcs (or pairs of arcs) are first represented by a high dimensional feature vector f (i, j, l)? Rk, where f is typically a bi nary feature vector over properties of the arc as well as the surrounding input (McDonald et al, 2005a; McDonald et al, 2006)","More precisely, dependency arcs (or pairs of arcs) are first represented by a high dimensional feature vector f (i, j, l) Rk, where f is typically a binary feature vector over properties of the arc as well as the surrounding input (McDonald et al, 2005a; McDonald et al, 2006)","['13', '54', '4', '53', '14']","<S sid =""13"" ssid = ""9"">We evaluate this parser on a diverse set of 13 languages using data provided by the CoNLL-X shared-task organizers (Buchholz et al.  2006; Hajiˇc et al.  2004; Simov et al.  2005; Simov and Osenova  2003; Chen et al.  2003; B¨ohmov´a et al.  2003; Kromann  2003; van der Beek et al.  2002; Brants et al.  2002; Kawata and Bartels  2000; Afonso et al.  2002; Dˇzeroski et al.  2006; Civit Torruella and MartiAntonin  2002; Nilsson et al.  2005; Oflazer et al.  2003; Atalay et al.  2003).</S><S sid =""54"" ssid = ""2"">Based on performance from a held-out section of the training data  we used non-projective parsing algorithms for Czech  Danish  Dutch  German  Japanese  Portuguese and Slovene  and projective parsing algorithms for Arabic  Bulgarian  Chinese  Spanish  Swedish and Turkish.</S><S sid =""4"" ssid = ""4"">We report results on the CoNLL-X shared task (Buchholz et al.  2006) data sets and present an error analysis.</S><S sid =""53"" ssid = ""1"">We trained models for all 13 languages provided by the CoNLL organizers (Buchholz et al.  2006).</S><S sid =""14"" ssid = ""10"">The results are promising and show the language independence of our system under the assumption of a labeled dependency corpus in the target language.</S>",['Method_Citation']
