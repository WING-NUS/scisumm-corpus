Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,W06-3114,W06-3120,0,"Koehn and Monz, 2006",0,"The official results were slightly better because a lowercase evaluation was used, see (Koehn and Monz, 2006)","The official results were slightly better because a lowercase evaluation was used, see (Koehn and Monz, 2006)","['1', '136', '143', '129', '121']","<S sid =""1"" ssid = ""1"">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S sid =""136"" ssid = ""29"">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid =""143"" ssid = ""36"">For instance  for out-ofdomain English-French  Systran has the best BLEU and manual scores.</S><S sid =""129"" ssid = ""22"">All systems (except for Systran  which was not tuned to Europarl) did considerably worse on outof-domain training data.</S><S sid =""121"" ssid = ""14"">For the automatic scoring method BLEU  we can distinguish three quarters of the systems.</S>",['Aim_Citation']
2,W06-3114,D07-1092,0,"Koehn and Monz, 2006",0,"We are further focusing on the shared task of the workshop on Statistical Machine Translation, which took place last year (Koehn and Monz, 2006) and consisted in translating Spanish, German, and French texts from and to English","We are further focusing on the shared task of the workshop on Statistical Machine Translation, which took place last year (Koehn and Monz, 2006) and consisted in translating Spanish, German, and French texts from and to English","['2', '86', '150', '95', '136']","<S sid =""2"" ssid = ""2"">0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 •systran • ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency •systran •nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p � 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain •upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv 23 24 25 26 27 28 29 30 31 32 •upc-mr •utd •upc-jmc •uedin-birch •ntt •rali •uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy •upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency •ntt •nrc •rali •uedin-birch -0.2 -0.3 -0.5 •upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr • rali Fluency -0.4 •upc-mr utd •upc-jmc -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 •upv 0.5 0.4 •systran •upc-mr • •rali 0.3 •ntt 0.2 0.1 -0.0 -0.1 •systran •upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •upv •systran •upc-mr • Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 •systran •ntt</S><S sid =""86"" ssid = ""2"">The average fluency judgement per judge ranged from 2.33 to 3.67  the average adequacy judgement ranged from 2.56 to 4.13.</S><S sid =""150"" ssid = ""43"">For some language pairs (such as GermanEnglish) system performance is more divergent than for others (such as English-French)  at least as measured by BLEU.</S><S sid =""95"" ssid = ""11"">The judgement of 4 in the first case will go to a vastly better system output than in the second case.</S><S sid =""136"" ssid = ""29"">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S>",['Method_Citation']
3,W06-3114,C08-1074,0,"Koehn and Monz, 2006",0,"For our training and test data we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference","For our training and test data we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference","['3', '67', '29', '78', '168']","<S sid =""3"" ssid = ""1"">was done by the participants.</S><S sid =""67"" ssid = ""6"">Participants and other volunteers contributed about 180 hours of labor in the manual evaluation.</S><S sid =""29"" ssid = ""22"">About half of the participants of last year’s shared task participated again.</S><S sid =""78"" ssid = ""17"">Judges where excluded from assessing the quality of MT systems that were submitted by their institution.</S><S sid =""168"" ssid = ""61"">Annotators argued for the importance of having correct and even multiple references.</S>",['Method_Citation']
4,W06-3114,W07-0718,0,"Koehn and Monz, 2006",0,"The results of last year? s workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006)","The results of last year's workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006)","['4', '146', '168', '176', '134']","<S sid =""4"" ssid = ""2"">This revealed interesting clues about the properties of automatic and manual scoring.</S><S sid =""146"" ssid = ""39"">This data set of manual judgements should provide a fruitful resource for research on better automatic scoring methods.</S><S sid =""168"" ssid = ""61"">Annotators argued for the importance of having correct and even multiple references.</S><S sid =""176"" ssid = ""7"">Human judges also pointed out difficulties with the evaluation of long sentences.</S><S sid =""134"" ssid = ""27"">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S>",['Aim_Citation']
5,W06-3114,P07-1083,0,"Koehn and Monz, 2006",0,"For the bi text-based annotation, we use publicly available word alignments from the Europarl corpus, automatically generated by GIZA++ for FrenchEnglish (Fr), Spanish-English (Es) and GermanEnglish (De) (Koehn and Monz, 2006)","For the bi text-based annotation, we use publicly available word alignments from the Europarl corpus, automatically generated by GIZA++ for FrenchEnglish (Fr), Spanish-English (Es) and German-English (De) (Koehn and Monz, 2006)","['5', '6', '152', '170', '162']","<S sid =""5"" ssid = ""3"">• We evaluated translation from English  in addition to into English.</S><S sid =""6"" ssid = ""4"">English was again paired with German  French  and Spanish.</S><S sid =""152"" ssid = ""45"">The predominate focus of building systems that translate into English has ignored so far the difficult issues of generating rich morphology which may not be determined solely by local context.</S><S sid =""170"" ssid = ""1"">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid =""162"" ssid = ""55"">A few annotators suggested to break up long sentences into clauses and evaluate these separately.</S>",['Method_Citation']
6,W06-3114,W07-0738,0,2006,0,"Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MTquality indicator","Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MTquality indicator","['6', '5', '163', '79', '162']","<S sid =""6"" ssid = ""4"">English was again paired with German  French  and Spanish.</S><S sid =""5"" ssid = ""3"">• We evaluated translation from English  in addition to into English.</S><S sid =""163"" ssid = ""56"">Not every annotator was fluent in both the source and the target language.</S><S sid =""79"" ssid = ""18"">Sentences and systems were randomly selected and randomly shuffled for presentation.</S><S sid =""162"" ssid = ""55"">A few annotators suggested to break up long sentences into clauses and evaluate these separately.</S>",['Method_Citation']
7,W06-3114,W07-0738,0,2006,0,"For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001)","For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001)","['7', '165', '66', '47', '125']","<S sid =""7"" ssid = ""5"">We dropped  however  one of the languages  Finnish  partly to keep the number of tracks manageable  partly because we assumed that it would be hard to find enough Finnish speakers for the manual evaluation.</S><S sid =""165"" ssid = ""58"">However  ince we extracted the test corpus automatically from web sources  the reference translation was not always accurate — due to sentence alignment errors  or because translators did not adhere to a strict sentence-by-sentence translation (say  using pronouns when referring to entities mentioned in the previous sentence).</S><S sid =""66"" ssid = ""5"">In this shared task  we were also confronted with this problem  and since we had no funding for paying human judgements  we asked participants in the evaluation to share the burden.</S><S sid =""47"" ssid = ""13"">Because of this  we retokenized and lowercased submitted output with our own tokenizer  which was also used to prepare the training and test data.</S><S sid =""125"" ssid = ""18"">We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.</S>",['Method_Citation']
8,W06-3114,W07-0738,0,2006,0,Wepresenta comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006) (see Section 3),We present a comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006) (see Section 3),"['8', '57', '29', '31', '28']","<S sid =""8"" ssid = ""1"">The evaluation framework for the shared task is similar to the one used in last year’s shared task.</S><S sid =""57"" ssid = ""23"">We are therefore applying a different method  which has been used at the 2005 DARPA/NIST evaluation.</S><S sid =""29"" ssid = ""22"">About half of the participants of last year’s shared task participated again.</S><S sid =""31"" ssid = ""24"">Compared to last year’s shared task  the participants represent more long-term research efforts.</S><S sid =""28"" ssid = ""21"">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S>",['Results_Citation']
9,W06-3114,W07-0738,0,2006,0,Weanalyze some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006),We analyze some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006),"['9', '28', '170', '143', '21']","<S sid =""9"" ssid = ""2"">Training and testing is based on the Europarl corpus.</S><S sid =""28"" ssid = ""21"">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid =""170"" ssid = ""1"">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid =""143"" ssid = ""36"">For instance  for out-ofdomain English-French  Systran has the best BLEU and manual scores.</S><S sid =""21"" ssid = ""14"">The out-of-domain test set differs from the Europarl data in various ways.</S>",['Method_Citation']
10,W06-3114,D07-1030,0,"Koehn and Monz, 2006",0,"We use the same method described in (Koehn and Monz, 2006) to perform the significance test","We use the same method described in (Koehn and Monz, 2006) to perform the significance test","['10', '20', '134', '4', '120']","<S sid =""10"" ssid = ""3"">Figure 1 provides some statistics about this corpus.</S><S sid =""20"" ssid = ""13"">For statistics on this test set  refer to Figure 1.</S><S sid =""134"" ssid = ""27"">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid =""4"" ssid = ""2"">This revealed interesting clues about the properties of automatic and manual scoring.</S><S sid =""120"" ssid = ""13"">In Figure 4  we displayed the number of system comparisons  for which we concluded statistical significance.</S>",['Method_Citation']
11,W06-3114,D07-1030,0,"Koehn and Monz, 2016",0,"We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006)","We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006)","['11', '28', '161', '152', '31']","<S sid =""11"" ssid = ""4"">To lower the barrier of entrance to the competition  we provided a complete baseline MT system  along with data resources.</S><S sid =""28"" ssid = ""21"">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid =""161"" ssid = ""54"">Since all long sentence translation are somewhat muddled  even a contrastive evaluation between systems was difficult.</S><S sid =""152"" ssid = ""45"">The predominate focus of building systems that translate into English has ignored so far the difficult issues of generating rich morphology which may not be determined solely by local context.</S><S sid =""31"" ssid = ""24"">Compared to last year’s shared task  the participants represent more long-term research efforts.</S>",['Method_Citation']
12,W06-3114,W08-0406,0,"Koehn and Monz, 2017",0,"The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002)","The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002)","['12', '141', '8', '81', '151']","<S sid =""12"" ssid = ""5"">To summarize  we provided: The performance of the baseline system is similar to the best submissions in last year’s shared task.</S><S sid =""141"" ssid = ""34"">In-domain Systran scores on this metric are lower than all statistical systems  even the ones that have much worse human scores.</S><S sid =""8"" ssid = ""1"">The evaluation framework for the shared task is similar to the one used in last year’s shared task.</S><S sid =""81"" ssid = ""20"">This is less than the 694 judgements 2004 DARPA/NIST evaluation  or the 532 judgements in the 2005 DARPA/NIST evaluation.</S><S sid =""151"" ssid = ""44"">The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphological rich languages  as demonstrated by the results for English-German and English-French.</S>",['Aim_Citation']
13,W06-3114,W11-1002,0,2006,0,"Callison-Burch et al (2006 )andKoehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality","Callison-Burch et al (2006) and Koehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality","['13', '33', '43', '74', '42']","<S sid =""13"" ssid = ""6"">We are currently working on a complete open source implementation of a training and decoding system  which should become available over the summer. pus  from which also the in-domain test set is taken.</S><S sid =""33"" ssid = ""26"">While building a machine translation system is a serious undertaking  in future we hope to attract more newcomers to the field by keeping the barrier of entry as low as possible.</S><S sid =""43"" ssid = ""9"">At the very least  we are creating a data resource (the manual annotations) that may the basis of future research in evaluation metrics.</S><S sid =""74"" ssid = ""13"">While we had up to 11 submissions for a translation direction  we did decide against presenting all 11 system outputs to the human judge.</S><S sid =""42"" ssid = ""8"">It was our hope that this competition  which included the manual and automatic evaluation of statistical systems and one rulebased commercial system  will give further insight into the relation between automatic and manual evaluation.</S>",['Method_Citation']
14,W06-3114,D07-1091,0,"Koehn and Monz, 2006",0,"The English? German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006)","The English German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006)","['14', '165', '58', '151', '155']","<S sid =""14"" ssid = ""7"">There is twice as much language modelling data  since training data for the machine translation system is filtered against sentences of length larger than 40 words.</S><S sid =""165"" ssid = ""58"">However  ince we extracted the test corpus automatically from web sources  the reference translation was not always accurate — due to sentence alignment errors  or because translators did not adhere to a strict sentence-by-sentence translation (say  using pronouns when referring to entities mentioned in the previous sentence).</S><S sid =""58"" ssid = ""24"">We divide up each test set into blocks of 20 sentences (100 blocks for the in-domain test set  53 blocks for the out-of-domain test set)  check for each block  if one system has a higher BLEU score than the other  and then use the sign test.</S><S sid =""151"" ssid = ""44"">The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphological rich languages  as demonstrated by the results for English-German and English-French.</S><S sid =""155"" ssid = ""48"">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S>",['Method_Citation']
15,W06-3114,D07-1091,0,"Koehn and Monz, 2006",0,"We report results on the development test set, which is also the out-of-domain test set of the WMT06 workshop shared task (Koehn and Monz, 2006)","We report results on the development test set, which is also the out-of-domain test set of the WMT06 workshop shared task (Koehn and Monz, 2006)","['15', '21', '28', '142', '18']","<S sid =""15"" ssid = ""8"">Out-of-domain test data is from the Project Syndicate web site  a compendium of political commentary.</S><S sid =""21"" ssid = ""14"">The out-of-domain test set differs from the Europarl data in various ways.</S><S sid =""28"" ssid = ""21"">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid =""142"" ssid = ""35"">Surprisingly  this effect is much less obvious for out-of-domain test data.</S><S sid =""18"" ssid = ""11"">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S>",['Method_Citation']
16,W06-3114,P07-1108,0,"Koehn and Monz, 2006",0,"A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Ma chine Translation (Koehn and Monz, 2006)","A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Machine Translation (Koehn and Monz, 2006)","['16', '126', '18', '165', '14']","<S sid =""16"" ssid = ""9"">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid =""126"" ssid = ""19"">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid =""18"" ssid = ""11"">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S><S sid =""165"" ssid = ""58"">However  ince we extracted the test corpus automatically from web sources  the reference translation was not always accurate — due to sentence alignment errors  or because translators did not adhere to a strict sentence-by-sentence translation (say  using pronouns when referring to entities mentioned in the previous sentence).</S><S sid =""14"" ssid = ""7"">There is twice as much language modelling data  since training data for the machine translation system is filtered against sentences of length larger than 40 words.</S>",['Method_Citation']
18,W06-3114,E12-3010,0,"Koehn and Monz, 2006",0,"For the same reason, human evaluation metrics based on adequacy and fluency were not suitable either (Koehn and Monz, 2006)","For the same reason, human evaluation metrics based on adequacy and fluency were not suitable either (Koehn and Monz, 2006)","['17', '69', '162', '170', '19']","<S sid =""17"" ssid = ""10"">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid =""69"" ssid = ""8"">We settled on contrastive evaluations of 5 system outputs for a single test sentence.</S><S sid =""162"" ssid = ""55"">A few annotators suggested to break up long sentences into clauses and evaluate these separately.</S><S sid =""170"" ssid = ""1"">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid =""19"" ssid = ""12"">We aligned the texts at a sentence level across all four languages  resulting in 1064 sentence per language.</S>",['Method_Citation']
19,W06-3114,W09-0402,0,"Koehn and Monz, 2006",0,"The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008)","The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008)","['18', '126', '16', '15', '155']","<S sid =""18"" ssid = ""11"">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S><S sid =""126"" ssid = ""19"">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid =""16"" ssid = ""9"">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid =""15"" ssid = ""8"">Out-of-domain test data is from the Project Syndicate web site  a compendium of political commentary.</S><S sid =""155"" ssid = ""48"">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S>",['Method_Citation']
