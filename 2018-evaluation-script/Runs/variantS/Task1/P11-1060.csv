Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P11-1060,D11-1039,0,2011,0,"Clarkeet al (2010) and Liang et al (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al (2011) presents work on unsupervised learning","Clarke et al (2010) and Liang et al (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al (2011) presents work on unsupervised learning","['1', '141', '15', '6', '2']","<S sid =""1"" ssid = ""1"">Compositional question answering begins by mapping questions to logical forms  but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.</S><S sid =""141"" ssid = ""26"">Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.</S><S sid =""15"" ssid = ""11"">Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.</S><S sid =""6"" ssid = ""2"">Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).</S><S sid =""2"" ssid = ""2"">In this paper  we learn to map questions to answers via latent logical forms  which are induced automatically from question-answer pairs.</S>",['Aim_Citation']
2,P11-1060,P13-1092,0,2011,0,"In particular, Clarke et al (2010) and Liang et al (2011) proposed methods to learn from question answer pairs alone, which represents a significant advance","In particular, Clarke et al (2010) and Liang et al (2011) proposed methods to learn from question answer pairs alone, which represents a significant advance","['2', '3', '117', '100', '149']","<S sid =""2"" ssid = ""2"">In this paper  we learn to map questions to answers via latent logical forms  which are induced automatically from question-answer pairs.</S><S sid =""3"" ssid = ""3"">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid =""117"" ssid = ""2"">In each dataset  each sentence x is annotated with a Prolog logical form  which we use only to evaluate and get an answer y.</S><S sid =""100"" ssid = ""76"">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid =""149"" ssid = ""34"">Think of DCS as a higher-level Our system learns lexical associations between programming language tailored to natural language  words and predicates.</S>",['Method_Citation']
3,P11-1060,P13-1092,0,2011,0,"To handle syntax-semantics mismatch, GUSP introduces a novel dependency-based meaning representation 1Clarke et al (2010) and Liang et al (2011) used the annotated logical forms to compute answers for their experiments","To handle syntax-semantics mismatch, GUSP introduces a novel dependency-based meaning representation. Clarke et al (2010) and Liang et al (2011) used the annotated logical forms to compute answers for their experiments","['3', '48', '149', '2', '51']","<S sid =""3"" ssid = ""3"">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid =""48"" ssid = ""24"">In addition  trees enable efficient computation  thereby establishing a new connection between dependency syntax and efficient semantic evaluation.</S><S sid =""149"" ssid = ""34"">Think of DCS as a higher-level Our system learns lexical associations between programming language tailored to natural language  words and predicates.</S><S sid =""2"" ssid = ""2"">In this paper  we learn to map questions to answers via latent logical forms  which are induced automatically from question-answer pairs.</S><S sid =""51"" ssid = ""27"">It is impossible to represent the semantics of this phrase with just a CSP  so we introduce a new aggregate relation  notated E. Consider a tree hE:ci  whose root is connected to a child c via E. If the denotation of c is a set of values s  the parent’s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S>",['Method_Citation']
4,P11-1060,P13-1092,0,2011,0,"More recently, Liang et al (2011 )proposedDCS for dependency-based compositional semantics, which represents a semantic parse as a tree with nodes representing database elements and operations, and edges representing relational joins","More recently, Liang et al (2011) proposed DCS for dependency-based compositional semantics, which represents a semantic parse as a tree with nodes representing database elements and operations, and edges representing relational joins","['4', '24', '136', '139', '135']","<S sid =""4"" ssid = ""4"">On two stansemantic parsing benchmarks our system obtains the highest published accuracies  despite requiring no annotated logical forms.</S><S sid =""24"" ssid = ""20"">Our system outperforms all existing systems despite using no annotated logical forms.</S><S sid =""136"" ssid = ""21"">In fact  DCS performs comparably to even the version of SEMRESP trained using logical forms.</S><S sid =""139"" ssid = ""24"">All other systems require logical forms as training data  whereas ours does not.</S><S sid =""135"" ssid = ""20"">SEMRESP requires a lexicon of 1.42 words per non-value predicate  WordNet features  and syntactic parse trees; DCS requires only words for the domain-independent predicates (overall  around 0.5 words per non-value predicate)  POS tags  and very simple indicator features.</S>",['Aim_Citation']
5,P11-1060,P13-1092,0,"Liang et al, 2011",0,"GUSP represents meaning by a semantic tree, which is similar to DCS (Liang et al, 2011)","GUSP represents meaning by a semantic tree, which is similar to DCS (Liang et al, 2011)","['5', '52', '105', '62', '50']","<S sid =""5"" ssid = ""1"">What is the total population of the ten largest capitals in the US?</S><S sid =""52"" ssid = ""28"">The denotation of the middle node is {s}  where s is all major cities.</S><S sid =""105"" ssid = ""81"">The feature vector φ(x  z) is defined by sums of five simple indicator feature templates: (F1) a word triggers a predicate (e.g.  [city  city]); (F2) a word is under a relation (e.g.  [that  11]); (F3) a word is under a trace predicate (e.g.  [in  loc]); (F4) two predicates are linked via a relation in the left or right direction (e.g.  [city 11  loc  RIGHT]); and (F5) a predicate has a child relation (e.g.  [city  11]).</S><S sid =""62"" ssid = ""38"">Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes)  as well as information stored about each marked node.</S><S sid =""50"" ssid = ""26"">For example  consider the phrase number of major cities  and suppose that number corresponds to the count predicate.</S>",['Method_Citation']
6,P11-1060,W12-2802,0,2011,0,"Matuszek et al [2010], Liang et al [2011] and Chen and Mooney [2011] describe models that learn compositional semantics, but word meanings are symbolic structures rather than patterns of features in the external world","Matuszek et al [2010], Liang et al [2011] and Chen and Mooney [2011] describe models that learn compositional semantics, but word meanings are symbolic structures rather than patterns of features in the external world","['6', '169', '58', '48', '165']","<S sid =""6"" ssid = ""2"">Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).</S><S sid =""169"" ssid = ""54"">The combination rules are encoded in the tems  despite using no annotated logical forms.</S><S sid =""58"" ssid = ""34"">We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.</S><S sid =""48"" ssid = ""24"">In addition  trees enable efficient computation  thereby establishing a new connection between dependency syntax and efficient semantic evaluation.</S><S sid =""165"" ssid = ""50"">These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.</S>",['Method_Citation']
7,P11-1060,P13-2009,0,"Liang et al, 2011",0,"It is well-studied in NLP, and a wide variety of methods have been proposed to tackle it ,e.g. rule-based (Popescu et al, 2003), super vised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al, 2011)","It is well-studied in NLP, and a wide variety of methods have been proposed to tackle it ,e.g. rule-based (Popescu et al, 2003), super vised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al, 2011)","['7', '8', '166', '125', '161']","<S sid =""7"" ssid = ""3"">Supervised semantic parsers (Zelle and Mooney  1996; Tang and Mooney  2001; Ge and Mooney  2005; Zettlemoyer and Collins  2005; Kate and Mooney  2007; Zettlemoyer and Collins  2007; Wong and Mooney  2007; Kwiatkowski et al.  2010) rely on manual annotation of logical forms  which is expensive.</S><S sid =""8"" ssid = ""4"">On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.</S><S sid =""166"" ssid = ""51"">Our employed (Zettlemoyer and Collins  2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language—think grounded al.  2010). compositional semantics.</S><S sid =""125"" ssid = ""10"">We used the same training-test splits as Zettlemoyer and Collins (2005) (600+280 for GEO and 500+140 for JOBS).</S><S sid =""161"" ssid = ""46"">CCG (Steedman  2000)  in which semantic pars- The integration of natural language with denotaing is driven from the lexicon.</S>",['Method_Citation']
8,P11-1060,D12-1069,0,Liangetal2011,0,"One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Liangetal2011) or even a binary correct/incorrect signal (Clarke et al2010)","One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Lianget al 2011) or even a binary correct/incorrect signal (Clarke et al2010)","['8', '138', '9', '165', '18']","<S sid =""8"" ssid = ""4"">On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.</S><S sid =""138"" ssid = ""23"">Next  we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).</S><S sid =""9"" ssid = ""5"">As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S><S sid =""165"" ssid = ""50"">These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.</S><S sid =""18"" ssid = ""14"">CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).</S>",['Results_Citation']
9,P11-1060,N12-1049,0,2011,0,"For example, Liang et al (2011) constructs a latent parse similar in structure to a dependency grammar, but representing a logical form","For example, Liang et al (2011) constructs a latent parse similar in structure to a dependency grammar, but representing a logical form","['9', '58', '165', '26', '20']","<S sid =""9"" ssid = ""5"">As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S><S sid =""58"" ssid = ""34"">We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.</S><S sid =""165"" ssid = ""50"">These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.</S><S sid =""26"" ssid = ""2"">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid =""20"" ssid = ""16"">At the same time  representations such as FunQL (Kate et al.  2005)  which was used in Clarke et al. (2010)  are simpler but lack the full expressive power of lambda calculus.</S>",['Method_Citation']
10,P11-1060,P12-1045,0,2011,0,Clarke et al (2010) and Liang et al (2011) trained systems on question and answer pairs by automatically finding semantic interpretations of the questions that would generate the correct answers,Clarke et al (2010) and Liang et al (2011) trained systems on question and answer pairs by automatically finding semantic interpretations of the questions that would generate the correct answers,"['10', '58', '169', '37', '26']","<S sid =""10"" ssid = ""6"">However  we still model the logical form (now as a latent variable) to capture the complexities of language.</S><S sid =""58"" ssid = ""34"">We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.</S><S sid =""169"" ssid = ""54"">The combination rules are encoded in the tems  despite using no annotated logical forms.</S><S sid =""37"" ssid = ""13"">The basic version of DCS restricts R to join and aggregate relations (see Table 1).</S><S sid =""26"" ssid = ""2"">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S>",['Method_Citation']
11,P11-1060,P14-1008,0,"Liang et al,2011",0,"Dependency-based Compositional Semantics (DCS) provides an intuitive way to model semantics of questions, by using simple dependency-like trees (Liang et al, 2011)","Dependency-based Compositional Semantics (DCS) provides an intuitive way to model semantics of questions, by using simple dependency-like trees (Liang et al, 2011)","['11', '118', '31', '38', '29']","<S sid =""11"" ssid = ""7"">Figure 1 shows our probabilistic model: with respect to a world w (database of facts)  producing an answer y.</S><S sid =""118"" ssid = ""3"">This evaluation is done with respect to a world w. Recall that a world w maps each predicate p ∈ P to a set of tuples w(p).</S><S sid =""31"" ssid = ""7"">Conceptually  a world is a relational database where each predicate is a relation (possibly infinite).</S><S sid =""38"" ssid = ""14"">Let us start by considering a DCS tree z with only join relations.</S><S sid =""29"" ssid = ""5"">Let P be a set of predicates (e.g.  state  count ∈ P)  which are just symbols.</S>",['Method_Citation']
12,P11-1060,P14-1008,0,"Liang et al, 2011",0,"DCS trees has been proposed to represent natural language semantics with a structure similar to dependency trees (Liang et al, 2011) (Figure 1)","DCS trees has been proposed to represent natural language semantics with a structure similar to dependency trees (Liang et al, 2011) (Figure 1)","['12', '169', '45', '32', '85']","<S sid =""12"" ssid = ""8"">We represent logical forms z as labeled trees  induced automatically from (x  y) pairs.</S><S sid =""169"" ssid = ""54"">The combination rules are encoded in the tems  despite using no annotated logical forms.</S><S sid =""45"" ssid = ""21"">The logical forms in DCS are called DCS trees  where nodes are labeled with predicates  and edges are labeled with relations.</S><S sid =""32"" ssid = ""8"">Define a special predicate ø with w(ø) = V. We represent functions by a set of inputoutput pairs  e.g.  w(count) = {(S  n) : n = |S|}.</S><S sid =""85"" ssid = ""61"">Extraction allows us to return the set of consistent values of a marked non-root node.</S>",['Aim_Citation']
13,P11-1060,P14-1008,0,"Liang et al, 2011",0,"are explained in? 2.5. 5http: //nlp.stanford.edu/software/corenlp.shtml 6 In (Liang et al, 2011) DCS trees are learned from QApairs and database entries","In (Liang et al, 2011) DCS trees are learned from QA pairs and database entries","['13', '62', '12', '103', '108']","<S sid =""13"" ssid = ""9"">We want to induce latent logical forms z (and parameters 0) given only question-answer pairs (x  y)  which is much cheaper to obtain than (x  z) pairs.</S><S sid =""62"" ssid = ""38"">Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes)  as well as information stored about each marked node.</S><S sid =""12"" ssid = ""8"">We represent logical forms z as labeled trees  induced automatically from (x  y) pairs.</S><S sid =""103"" ssid = ""79"">To define the features  we technically need to augment each tree z ∈ ZL(x) with alignment information—namely  for each predicate in z  the span in x (if any) that triggered it.</S><S sid =""108"" ssid = ""84"">However  in order to learn  we need to sum over {z ∈ ZL(x) : JzKw = y}  and unfortunately  the additional constraint JzKw = y does not factorize.</S>",['Method_Citation']
14,P11-1060,P14-1008,0,"Liang et al, 2011",0,"as in the sentence? Tropi cal storm Debby is blamed for death?, which is a tropical storm, is Debby, etc. Technically, each germ in a DCS tree indicates a variable when the DCS tree is translated to a FOL formula, and the abstract denotation of the germ corresponds to the set of consistent values (Liang et al, 2011) of that variable","Technically, each germ in a DCS tree indicates a variable when the DCS tree is translated to a FOL formula, and the abstract denotation of the germ corresponds to the set of consistent values (Liang et al, 2011) of that variable","['14', '43', '107', '47', '59']","<S sid =""14"" ssid = ""10"">The core problem that arises in this setting is program induction: finding a logical form z (over an exponentially large space of possibilities) that produces the target answer y.</S><S sid =""43"" ssid = ""19"">Computation We can compute the denotation JzKw of a DCS tree z by exploiting dynamic programming on trees (Dechter  2003).</S><S sid =""107"" ssid = ""83"">Our model is arc-factored  so we can sum over all DCS trees in ZL(x) using dynamic programming.</S><S sid =""47"" ssid = ""23"">This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure  which will facilitate parsing.</S><S sid =""59"" ssid = ""35"">The key idea that allows us to give semanticallyscoped denotations to syntactically-scoped trees is as follows: We mark a node low in the tree with a mark relation (one of E  Q  or C).</S>",['Method_Citation']
15,P11-1060,D11-1140,0,2011,0,Clarke et al (2010) and Liang et al (2011) replace semantic annotations in the training set with target answers which are more easily available,Clarke et al (2010) and Liang et al (2011) replace semantic annotations in the training set with target answers which are more easily available,"['15', '100', '141', '142', '163']","<S sid =""15"" ssid = ""11"">Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.</S><S sid =""100"" ssid = ""76"">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid =""141"" ssid = ""26"">Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.</S><S sid =""142"" ssid = ""27"">This option is not available to us since we do not have annotated logical forms  so we must instead rely on lexical triggers to define the search space.</S><S sid =""163"" ssid = ""48"">Feedback from the context; for example  the lexical entry for borders world has been used to guide both syntactic parsing is S\NP/NP : Ay.Ax.border(x  y)  which means (Schuler  2003) and semantic parsing (Popescu et borders looks right for the first argument and left al.  2003; Clarke et al.  2010).</S>",['Method_Citation']
16,P11-1060,D11-1140,0,"Liang et al, 2011",0,"and Collins, 2005, 2007),? -WASP (Wong and Mooney, 2007), UBL (Kwiatkowski et al, 2010) systems and DCS (Liang et al, 2011)","WASP (Wong and Mooney, 2007), UBL (Kwiatkowski et al, 2010) systems and DCS (Liang et al, 2011)","['16', '144', '142', '95', '15']","<S sid =""16"" ssid = ""12"">Which one should we use?</S><S sid =""144"" ssid = ""29"">Intuitions How is our system learning?</S><S sid =""142"" ssid = ""27"">This option is not available to us since we do not have annotated logical forms  so we must instead rely on lexical triggers to define the search space.</S><S sid =""95"" ssid = ""71"">Our first question is: given notation of the DCS tree before execution is an utterance x  what trees z ∈ Z are permissible?</S><S sid =""15"" ssid = ""11"">Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.</S>",['Method_Citation']
17,P11-1060,P13-1007,0,2011,0,"In general, every plural NPpotentially introduces an implicit universal, ranging 1For example, Liang et al (2011) in their state-of-the-art statistical semantic parser within the domain of natural language queries to databases, explicitly devise quantifier scoping in the semantic model","For example, Liang et al (2011) in their state-of-the-art statistical semantic parser within the domain of natural language queries to databases, explicitly devise quantifier scoping in the semantic model","['17', '36', '37', '58', '25']","<S sid =""17"" ssid = ""13"">The dominant paradigm in compositional semantics is Montague semantics  which constructs lambda calculus forms in a bottom-up manner.</S><S sid =""36"" ssid = ""12"">It is this transparency between syntax and semantics provided by DCS which leads to a simple and streamlined compositional semantics suitable for program induction.</S><S sid =""37"" ssid = ""13"">The basic version of DCS restricts R to join and aggregate relations (see Table 1).</S><S sid =""58"" ssid = ""34"">We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.</S><S sid =""25"" ssid = ""1"">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS)  which captures the core idea of using trees to represent formal semantics.</S>",['Method_Citation']
18,P11-1060,D11-1022,0,2011,0,"DD-ADMM may be useful in other frameworks involving logical constraints, such as the models for compositional semantics presented by Liang et al (2011)","DD-ADMM may be useful in other frameworks involving logical constraints, such as the models for compositional semantics presented by Liang et al (2011)","['18', '165', '26', '20', '167']","<S sid =""18"" ssid = ""14"">CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).</S><S sid =""165"" ssid = ""50"">These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.</S><S sid =""26"" ssid = ""2"">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid =""20"" ssid = ""16"">At the same time  representations such as FunQL (Kate et al.  2005)  which was used in Clarke et al. (2010)  are simpler but lack the full expressive power of lambda calculus.</S><S sid =""167"" ssid = ""52"">In DCS  we start with lexical triggers  which are 6 Conclusion more basic than CCG lexical entries.</S>",['Method_Citation']
19,P11-1060,P12-1051,0,2011,0,"In fact, for any CFG G, it 1See Liang et al (2011) for work in representing lambda calculus expressions with trees","In fact, for any CFG G, it 1See Liang et al (2011) for work in representing lambda calculus expressions with trees","['19', '122', '51', '14', '9']","<S sid =""19"" ssid = ""15"">However  the logical forms there can become quite complex  and in the context of program induction  this would lead to an unwieldy search space.</S><S sid =""122"" ssid = ""7"">For JOBS  if we use the standard Jobs database  close to half the y’s are empty  which makes it uninteresting.</S><S sid =""51"" ssid = ""27"">It is impossible to represent the semantics of this phrase with just a CSP  so we introduce a new aggregate relation  notated E. Consider a tree hE:ci  whose root is connected to a child c via E. If the denotation of c is a set of values s  the parent’s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S><S sid =""14"" ssid = ""10"">The core problem that arises in this setting is program induction: finding a logical form z (over an exponentially large space of possibilities) that produces the target answer y.</S><S sid =""9"" ssid = ""5"">As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S>",['Method_Citation']
