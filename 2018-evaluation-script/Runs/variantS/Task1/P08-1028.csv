Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P08-1028,D08-1094,0,2008,0,"Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p+ a as a function f operating on four components: c= f (p, a, R, K) (3) R is the relation holding between p and a, and Kadditional knowledge","Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p+ a as a function f operating on four components: c= f (p, a, R, K) (3) R is the relation holding between p and a, and K additional knowledge","['1', '189', '43', '24', '34']","<S sid =""1"" ssid = ""1"">This paper proposes a framework for representing the meaning of phrases and sentences in vector space.</S><S sid =""189"" ssid = ""1"">In this paper we presented a general framework for vector-based semantic composition.</S><S sid =""43"" ssid = ""16"">Vector addition is by far the most common method for representing the meaning of linguistic sequences.</S><S sid =""24"" ssid = ""20"">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S><S sid =""34"" ssid = ""7"">Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.</S>",['Aim_Citation']
4,P08-1028,P14-1060,0,2008,0,"While works such asthe SDSM model suffer from the problem of sparsity in composing structures beyond bi grams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstetteand Sadrzadeh (2011) are restricted by significant model biases in representing semantic com position by generic algebraic operations","While works such as the SDSM model suffer from the problem of sparsity in composing structures beyond bigrams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstette and Sadrzadeh (2011) are restricted by significant model biases in representing semantic com position by generic algebraic operations","['2', '190', '25', '83', '26']","<S sid =""2"" ssid = ""2"">Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions.</S><S sid =""190"" ssid = ""2"">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S><S sid =""25"" ssid = ""21"">We present a general framework for vector-based composition which allows us to consider different classes of models.</S><S sid =""83"" ssid = ""31"">Although we have presented multiplicative and additive models separately  there is nothing inherent in our formulation that disallows their combination.</S><S sid =""26"" ssid = ""22"">Specifically  we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.</S>",['Method_Citation']
6,P08-1028,P10-1097,0,2008,0,"Mitchell and Lapata (2008), henceforth M& amp; L, propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression","Mitchell and Lapata (2008), henceforth M& amp; L, propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression","['3', '26', '52', '25', '24']","<S sid =""3"" ssid = ""3"">Under this framework  we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.</S><S sid =""26"" ssid = ""22"">Specifically  we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.</S><S sid =""52"" ssid = ""25"">Under this framework  we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S><S sid =""25"" ssid = ""21"">We present a general framework for vector-based composition which allows us to consider different classes of models.</S><S sid =""24"" ssid = ""20"">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>",['Method_Citation']
7,P08-1028,P10-1097,0,2008,0,"Interestingly, Mitchell and Lapata (2008) came to the same result in a different setting","Interestingly, Mitchell and Lapata (2008) came to the same result in a different setting","['4', '27', '176', '26', '174']","<S sid =""4"" ssid = ""4"">Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.</S><S sid =""27"" ssid = ""23"">Our results show that the multiplicative models are superior and correlate significantly with behavioral data.</S><S sid =""176"" ssid = ""10"">The multiplicative and combined models yield means closer to the human ratings.</S><S sid =""26"" ssid = ""22"">Specifically  we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.</S><S sid =""174"" ssid = ""8"">The simple additive model fails to distinguish between High and Low Similarity items.</S>",['Aim_Citation']
8,P08-1028,D11-1094,0,2008,0,"And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to combine the constituent vectors","And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to combine the constituent vectors","['5', '9', '20', '19', '8']","<S sid =""5"" ssid = ""1"">Vector-based models of word meaning (Lund and Burgess  1996; Landauer and Dumais  1997) have become increasingly popular in natural language processing (NLP) and cognitive science.</S><S sid =""9"" ssid = ""5"">In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess  1996; Landauer and Dumais  1997) and text comprehension (Landauer and Dumais  1997; Foltz et al.  1998).</S><S sid =""20"" ssid = ""16"">Computational models of semantics which use symbolic logic representations (Montague  1974) can account naturally for the meaning of phrases or sentences.</S><S sid =""19"" ssid = ""15"">While vector addition has been effective in some applications such as essay grading (Landauer and Dumais  1997) and coherence assessment (Foltz et al.  1998)  there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al.  1991; West and Stanovich  1986) and modulate cognitive behavior in sentence priming (Till et al.  1988) and inference tasks (Heit and Rubinstein  1994).</S><S sid =""8"" ssid = ""4"">Examples include automatic thesaurus extraction (Grefenstette  1994)  word sense discrimination (Sch¨utze  1998) and disambiguation (McCarthy et al.  2004)  collocation extraction (Schone and Jurafsky  2001)  text segmentation (Choi et al.  2001)   and notably information retrieval (Salton et al.  1975).</S>",['Method_Citation']
9,P08-1028,W11-0131,0,2008,0,"Mitchell and Lapata (2008) provide a general framework for semantic vector composition: p= f (u, v, R, K) (1) 295 where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base, and p is a resulting composed vector (or tensor)","Mitchell and Lapata (2008) provide a general framework for semantic vector composition: p= f (u, v, R, K) (1) 295 where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base, and p is a resulting composed vector (or tensor)","['6', '21', '138', '20', '29']","<S sid =""6"" ssid = ""2"">The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris  1968).</S><S sid =""21"" ssid = ""17"">Central in these models is the notion of compositionality — the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.</S><S sid =""138"" ssid = ""51"">Model Parameters Irrespectively of their form  all composition models discussed here are based on a semantic space for representing the meanings of individual words.</S><S sid =""20"" ssid = ""16"">Computational models of semantics which use symbolic logic representations (Montague  1974) can account naturally for the meaning of phrases or sentences.</S><S sid =""29"" ssid = ""2"">While neural networks can readily represent single distinct objects  in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.</S>",['Method_Citation']
10,P08-1028,W11-0131,0,2008,0,"As Mitchell and Lapata (2008) did, let us temporarily suspend discussion on what semantics populate our vectors for now","As Mitchell and Lapata (2008) did, let us temporarily suspend discussion on what semantics populate our vectors for now","['7', '43', '200', '1', '42']","<S sid =""7"" ssid = ""3"">A variety of NLP tasks have made good use of vector-based models.</S><S sid =""43"" ssid = ""16"">Vector addition is by far the most common method for representing the meaning of linguistic sequences.</S><S sid =""200"" ssid = ""12"">The applications of the framework discussed here are many and varied both for cognitive science and NLP.</S><S sid =""1"" ssid = ""1"">This paper proposes a framework for representing the meaning of phrases and sentences in vector space.</S><S sid =""42"" ssid = ""15"">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S>",['Method_Citation']
11,P08-1028,P13-2083,0,2008,0,"Mitchell and Lapata (2008) propose a framework to define the composition c= f (a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition","Mitchell and Lapata (2008) propose a framework to define the composition c= f (a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition","['8', '19', '5', '193', '20']","<S sid =""8"" ssid = ""4"">Examples include automatic thesaurus extraction (Grefenstette  1994)  word sense discrimination (Sch¨utze  1998) and disambiguation (McCarthy et al.  2004)  collocation extraction (Schone and Jurafsky  2001)  text segmentation (Choi et al.  2001)   and notably information retrieval (Salton et al.  1975).</S><S sid =""19"" ssid = ""15"">While vector addition has been effective in some applications such as essay grading (Landauer and Dumais  1997) and coherence assessment (Foltz et al.  1998)  there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al.  1991; West and Stanovich  1986) and modulate cognitive behavior in sentence priming (Till et al.  1988) and inference tasks (Heit and Rubinstein  1994).</S><S sid =""5"" ssid = ""1"">Vector-based models of word meaning (Lund and Burgess  1996; Landauer and Dumais  1997) have become increasingly popular in natural language processing (NLP) and cognitive science.</S><S sid =""193"" ssid = ""5"">Previous applications of vector addition to document indexing (Deerwester et al.  1990) or essay grading (Landauer et al.  1997) were more concerned with modeling the gist of a document rather than the meaning of its sentences.</S><S sid =""20"" ssid = ""16"">Computational models of semantics which use symbolic logic representations (Montague  1974) can account naturally for the meaning of phrases or sentences.</S>",['Results_Citation']
12,P08-1028,P13-2083,0,"Mitchell and Lapata, 2008",0,"As our final set of baselines, we extend two simple techniques proposed by (Mitchell and Lapata, 2008) that use element-wise addition and multiplication operators to perform composition","As our final set of baselines, we extend two simple techniques proposed by (Mitchell and Lapata, 2008) that use element-wise addition and multiplication operators to perform composition","['9', '5', '19', '148', '193']","<S sid =""9"" ssid = ""5"">In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess  1996; Landauer and Dumais  1997) and text comprehension (Landauer and Dumais  1997; Foltz et al.  1998).</S><S sid =""5"" ssid = ""1"">Vector-based models of word meaning (Lund and Burgess  1996; Landauer and Dumais  1997) have become increasingly popular in natural language processing (NLP) and cognitive science.</S><S sid =""19"" ssid = ""15"">While vector addition has been effective in some applications such as essay grading (Landauer and Dumais  1997) and coherence assessment (Foltz et al.  1998)  there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al.  1991; West and Stanovich  1986) and modulate cognitive behavior in sentence priming (Till et al.  1988) and inference tasks (Heit and Rubinstein  1994).</S><S sid =""148"" ssid = ""61"">In addition  Bullinaria and Levy (2007) found that these parameters perform well on a number of other tasks such as the synonymy task from the Test ofEnglish as a Foreign Language (TOEFL).</S><S sid =""193"" ssid = ""5"">Previous applications of vector addition to document indexing (Deerwester et al.  1990) or essay grading (Landauer et al.  1997) were more concerned with modeling the gist of a document rather than the meaning of its sentences.</S>",['Method_Citation']
13,P08-1028,P10-1021,0,2008,0,"Althoughthis model has been shown to successfully simulate single and multiple-word priming (McDonald and Brew 2004), it failed to predict processing costs in the Embra eye-tracking corpus (McDonald and Shillcock 2003) .In this work we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008)","Although this model has been shown to successfully simulate single and multiple-word priming (McDonald and Brew 2004), it failed to predict processing costs in the Embra eye-tracking corpus (McDonald and Shillcock 2003). In this work we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008)","['10', '141', '163', '191', '108']","<S sid =""10"" ssid = ""6"">Moreover  the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald  2000) and word association norms (Denhire and Lemaire  2004).</S><S sid =""141"" ssid = ""54"">The task involves examining the degree of linear relationship between the human judgments for two individual words and vector-based similarity values.</S><S sid =""163"" ssid = ""76"">We expect better models to yield a pattern of similarity scores like those observed in the human ratings (see Figure 2).</S><S sid =""191"" ssid = ""3"">Despite the popularity of additive models  our experimental results showed the superiority of models utilizing multiplicative combinations  at least for the sentence similarity task attempted here.</S><S sid =""108"" ssid = ""21"">Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).</S>",['Method_Citation']
14,P08-1028,P10-1021,0,2008,0,"Assuming that h is a linear function of the Cartesian product of u and v allows to specify additive models which are by far the most common method of vector combination in the literature: hi =ui+ vi (3) Alternatively, we can assume that h is a linear function of the tensor product of u and v, and thus derive models based on multiplication: hi =ui? vi (4) Mitchell and Lapata (2008) show that several additive and multiplicative models can be formulated under this framework, including the well known tensor products (Smolensky 1990) and circular convolution (Plate 1995)","Mitchell and Lapata (2008) show that several additive and multiplicative models can be formulated under this framework, including the well known tensor products (Smolensky 1990) and circular convolution (Plate 1995)","['11', '33', '6', '29', '95']","<S sid =""11"" ssid = ""7"">Despite their widespread use  vector-based models are typically directed at representing words in isolation and methods for constructing representations for phrases or sentences have received little attention in the literature.</S><S sid =""33"" ssid = ""6"">In contrast  symbolic representations can naturally handle the binding of constituents to their roles  in a systematic manner that avoids both these problems.</S><S sid =""6"" ssid = ""2"">The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris  1968).</S><S sid =""29"" ssid = ""2"">While neural networks can readily represent single distinct objects  in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.</S><S sid =""95"" ssid = ""8"">Focusing on a single compositional structure  namely intransitive verbs and their subjects  is a good point of departure for studying vector combination.</S>",['Method_Citation']
15,P08-1028,W11-0115,0,2008,0,"Mitchell and Lapata (2008) introduce a whole family of models of compositionality based on vector addition and point wise-multiplication (and a weighted combination of both), evaluated on a sentence similarity task inspired by Kintsch (2001)","Mitchell and Lapata (2008) introduce a whole family of models of compositionality based on vector addition and point wise-multiplication (and a weighted combination of both), evaluated on a sentence similarity task inspired by Kintsch (2001)","['12', '77', '43', '34', '39']","<S sid =""12"" ssid = ""8"">In fact  the commonest method for combining the vectors is to average them.</S><S sid =""77"" ssid = ""25"">For additive models  a natural way to achieve this is to include further vectors into the summation.</S><S sid =""43"" ssid = ""16"">Vector addition is by far the most common method for representing the meaning of linguistic sequences.</S><S sid =""34"" ssid = ""7"">Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.</S><S sid =""39"" ssid = ""12"">The compression is achieved by summing along the transdiagonal elements of the tensor product.</S>",['Aim_Citation']
16,P08-1028,W11-0115,0,2008,0,"The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression","The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression","['13', '6', '46', '93', '194']","<S sid =""13"" ssid = ""9"">Vector averaging is unfortunately insensitive to word order  and more generally syntactic structure  giving the same representation to any constructions that happen to share the same vocabulary.</S><S sid =""6"" ssid = ""2"">The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris  1968).</S><S sid =""46"" ssid = ""19"">However  since it is order independent  it cannot capture meaning differences that are modulated by differences in syntactic structure.</S><S sid =""93"" ssid = ""6"">The idea here is that an appropriate composition model when applied to horse and ran will yield a vector closer to the landmark gallop than dissolve.</S><S sid =""194"" ssid = ""6"">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S>",['Method_Citation']
17,P08-1028,W11-0115,0,2008,0,"For example, Mitchell and Lapata (2008) use their models to approximate the human ratings in their sentence similarity dataset","For example, Mitchell and Lapata (2008) use their models to approximate the human ratings in their sentence similarity dataset","['14', '175', '179', '55', '88']","<S sid =""14"" ssid = ""10"">This is illustrated in the example below taken from Landauer et al. (1997).</S><S sid =""175"" ssid = ""9"">We observe a similar pattern for the non compositional baseline model  the weighted additive model and Kintsch (2001).</S><S sid =""179"" ssid = ""13"">The results of our correlation analysis are also given in Table 2.</S><S sid =""55"" ssid = ""3"">A hypothetical semantic space is illustrated in Figure 1.</S><S sid =""88"" ssid = ""1"">We evaluated the models presented in Section 3 on a sentence similarity task initially proposed by Kintsch (2001).</S>",['Method_Citation']
18,P08-1028,W11-1310,0,2008,0,We use other WSM settings following Mitchell and Lapata (2008),We use other WSM settings following Mitchell and Lapata (2008),"['15', '22', '56', '79', '20']","<S sid =""15"" ssid = ""11"">Sentences (1-a) and (1-b) contain exactly the same set of words but their meaning is entirely different.</S><S sid =""22"" ssid = ""18"">Here  semantic analysis is guided by syntactic structure  and therefore sentences (1-a) and (1-b) receive distinct representations.</S><S sid =""56"" ssid = ""4"">Here  the space has only five dimensions  and the matrix cells denote the co-occurrence of the target words (horse and run) with the context words animal  stable  and so on.</S><S sid =""79"" ssid = ""27"">When modeling predicate-argument structures  Kintsch (2001) proposes including one or more distributional neighbors  n  of the predicate: Note that considerable latitude is allowed in selecting the appropriate neighbors.</S><S sid =""20"" ssid = ""16"">Computational models of semantics which use symbolic logic representations (Montague  1974) can account naturally for the meaning of phrases or sentences.</S>",['Method_Citation']
19,P08-1028,W11-1310,0,2008,0,Mitchell and Lapata (2008) observed that a simple multiplication function modelled compositionality better than addition,Mitchell and Lapata (2008) observed that a simple multiplication function modelled compositionality better than addition,"['16', '150', '15', '34', '22']","<S sid =""16"" ssid = ""12"">(1) a.</S><S sid =""150"" ssid = ""63"">First  the additive model in (7) weighs differentially the contribution of the two constituents.</S><S sid =""15"" ssid = ""11"">Sentences (1-a) and (1-b) contain exactly the same set of words but their meaning is entirely different.</S><S sid =""34"" ssid = ""7"">Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.</S><S sid =""22"" ssid = ""18"">Here  semantic analysis is guided by syntactic structure  and therefore sentences (1-a) and (1-b) receive distinct representations.</S>",['Method_Citation']
20,P08-1028,W11-1310,0,"Mitchell and Lapata, 2008",0,"We use the compositionality functions, simple addition and simple multiplication to build compositional vectors Vwr1+wr2 and Vwr1 ?wr2. These are as described in (Mitchell and Lapata, 2008)","We use the compositionality functions, simple addition and simple multiplication to build compositional vectors Vwr1+wr2 and Vwr1 ?wr2. These are as described in (Mitchell and Lapata, 2008)","['17', '18', '164', '174', '122']","<S sid =""17"" ssid = ""13"">It was not the sales manager who hit the bottle that day  but the office worker with the serious drinking problem. b.</S><S sid =""18"" ssid = ""14"">That day the office manager  who was drinking  hit the problem sales worker with a bottle  but it was not serious.</S><S sid =""164"" ssid = ""77"">A more scrupulous evaluation requires directly correlating all the individual participants’ similarity judgments with those of the models.6 We used Spearman’s p for our correlation analyses.</S><S sid =""174"" ssid = ""8"">The simple additive model fails to distinguish between High and Low Similarity items.</S><S sid =""122"" ssid = ""35"">The opposite is the case for the reference The face glowed.</S>",['Method_Citation']
