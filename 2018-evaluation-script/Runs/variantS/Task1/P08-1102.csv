Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P08-1102,C08-1049,0,2008,0,"Following Jiang et al (2008), we describe segmentation and Joint S& amp; T as below: For a given Chinese sentence appearing as a character sequence: C 1: n= C 1 C 2.","Following Jiang et al (2008), we describe segmentation and Joint S& amp; T as below: For a given Chinese sentence appearing as a character sequence: C 1: n= C 1 C 2.","['1', '130', '76', '24', '4']","<S sid =""1"" ssid = ""1"">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S sid =""130"" ssid = ""1"">We proposed a cascaded linear model for Chinese Joint S&T.</S><S sid =""76"" ssid = ""1"">Sequence segmentation and labelling problem can be solved through a viterbi style decoding procedure.</S><S sid =""24"" ssid = ""20"">It is a better idea to perform segmentation and POS tagging jointly in a uniform framework.</S><S sid =""4"" ssid = ""4"">On the Penn Chinese Treebank 5.0  we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.</S>",['Aim_Citation']
2,P08-1102,C08-1049,0,2008,0,"As described in Ng and Low (2004 )andJiang et al (2008), we use s indicating a single character word, while b, m and e indicating the be gin, middle and end of a word respectively","As described in Ng and Low (2004) and Jiang et al (2008), we use s indicating a single character word, while b, m and e indicating the be gin, middle and end of a word respectively","['2', '16', '133', '64', '134']","<S sid =""2"" ssid = ""2"">With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S><S sid =""16"" ssid = ""12"">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S><S sid =""133"" ssid = ""4"">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S><S sid =""64"" ssid = ""15"">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result  a 4-gram POS language model functioning as the product of statetransition probabilities in HMM  and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S><S sid =""134"" ssid = ""5"">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S>",['Method_Citation']
3,P08-1102,C08-1049,0,2008,0,plates called lexical-target in the column below areintroduced by Jiang et al (2008),plates called lexical-target in the column below are introduced by Jiang et al (2008),"['3', '21', '106', '1', '4']","<S sid =""3"" ssid = ""3"">Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.</S><S sid =""21"" ssid = ""17"">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S><S sid =""106"" ssid = ""17"">Note that the accuracy of Joint S&T means that a word-POS pair is recognized only if both the boundary tags and the POS’s are correctly labelled.</S><S sid =""1"" ssid = ""1"">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S sid =""4"" ssid = ""4"">On the Penn Chinese Treebank 5.0  we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.</S>",['Method_Citation']
4,P08-1102,P12-1110,0,2008,0,"For CTB-5, we refer to the split by Duan et al (2007) as CTB-5d, and to the split by Jiang et al (2008) as CTB-5j","For CTB-5, we refer to the split by Duan et al (2007) as CTB-5d, and to the split by Jiang et al (2008) as CTB-5j","['4', '127', '1', '123', '118']","<S sid =""4"" ssid = ""4"">On the Penn Chinese Treebank 5.0  we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.</S><S sid =""127"" ssid = ""38"">Finally  the word count penalty gives improvement to the cascaded model  0.13 points on segmentation and 0.16 points on Joint S&T.</S><S sid =""1"" ssid = ""1"">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S sid =""123"" ssid = ""34"">Without it  the F-measure on segmentation and Joint S&T both suffer a decrement of 0.2 points.</S><S sid =""118"" ssid = ""29"">We find that the cascaded model achieves a F-measure increment of about 0.5 points on segmentation and about 0.9 points on Joint S&T  over the perceptron-only model POS+.</S>",['Aim_Citation']
5,P08-1102,D12-1126,0,2008,0,Jiang et al (2008) proposes a cascaded linear model for joint Chinese word segmentation and POS tagging,Jiang et al (2008) proposes a cascaded linear model for joint Chinese word segmentation and POS tagging,"['5', '31', '1', '51', '114']","<S sid =""5"" ssid = ""1"">Word segmentation and part-of-speech (POS) tagging are important tasks in computer processing of Chinese and other Asian languages.</S><S sid =""31"" ssid = ""3"">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S><S sid =""1"" ssid = ""1"">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S sid =""51"" ssid = ""2"">Additional features most widely used are related to word or POS ngrams.</S><S sid =""114"" ssid = ""25"">We used SRI Language Modelling Toolkit (Stolcke and Andreas  2002) to train a 3gram word LM with modified Kneser-Ney smoothing (Chen and Goodman  1998)  and a 4-gram POS LM with Witten-Bell smoothing  and we trained a word-POS co-occurrence model simply by MLE without smoothing.</S>",['Method_Citation']
6,P08-1102,C10-1135,0,2008,0,"We use the feature templates the same as Jiang et al, (2008) to extract features form E model","We use the feature templates the same as Jiang et al, (2008) to extract features form E model","['6', '8', '134', '15', '45']","<S sid =""6"" ssid = ""2"">Several models were introduced for these problems  for example  the Hidden Markov Model (HMM) (Rabiner  1989)  Maximum Entropy Model (ME) (Ratnaparkhi and Adwait  1996)  and Conditional Random Fields (CRFs) (Lafferty et al.  2001).</S><S sid =""8"" ssid = ""4"">Another widely used discriminative method is the perceptron algorithm (Collins  2002)  which achieves comparable performance to CRFs with much faster training  so we base this work on the perceptron.</S><S sid =""134"" ssid = ""5"">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S><S sid =""15"" ssid = ""11"">To cope with this problem  we propose a cascaded linear model inspired by the log-linear model (Och and Ney  2004) widely used in statistical machine translation to incorporate different kinds of knowledge sources.</S><S sid =""45"" ssid = ""17"">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S>",['Method_Citation']
8,P08-1102,P12-1025,0,"Jiangetal., 2008a",0,"approach, where basic processing units are characters which compose words (Jiangetal., 2008a)","basic processing units are characters which compose words (Jiangetal., 2008a)","['7', '8', '100', '14', '31']","<S sid =""7"" ssid = ""3"">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S><S sid =""8"" ssid = ""4"">Another widely used discriminative method is the perceptron algorithm (Collins  2002)  which achieves comparable performance to CRFs with much faster training  so we base this work on the perceptron.</S><S sid =""100"" ssid = ""11"">Test results listed in Table 2 shows that this model obtains higher accuracy than the best of SIGHAN Bakeoff 2 in three corpora (AS  CityU and MSR).</S><S sid =""14"" ssid = ""10"">As a result  many theoretically useful features such as higherorder word or POS n-grams are difficult to be incorporated in the model efficiently.</S><S sid =""31"" ssid = ""3"">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S>",['Method_Citation']
9,P08-1102,C10-2096,0,2008b,0,"The solid lines show the 1-best result, which is wrong. Jiang et al (2008b) stress the problems in re ranking phase","The solid lines show the 1-best result, which is wrong. Jiang et al (2008b) stress the problems in re ranking phase","['8', '45', '49', '29', '115']","<S sid =""8"" ssid = ""4"">Another widely used discriminative method is the perceptron algorithm (Collins  2002)  which achieves comparable performance to CRFs with much faster training  so we base this work on the perceptron.</S><S sid =""45"" ssid = ""17"">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S><S sid =""49"" ssid = ""21"">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S><S sid =""29"" ssid = ""1"">The perceptron algorithm introduced into NLP by Collins (2002)  is a simple but effective discriminative training method.</S><S sid =""115"" ssid = ""26"">To obtain their corresponding weights  we adapted the minimum-error-rate training algorithm (Och  2003) to train the outside-layer model.</S>",['Results_Citation']
10,P08-1102,C10-2096,0,"Jiang et al, 2008a",0,"6.1.1 Baseline Forest-based System We first segment the Chinese sentences into the1-best segmentations using a state-of-the-art system (Jiang et al, 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results","We first segment the Chinese sentences into the 1-best segmentations using a state-of-the-art system (Jiang et al, 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results","['9', '23', '11', '80', '81']","<S sid =""9"" ssid = ""5"">To segment and tag a character sequence  there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T).</S><S sid =""23"" ssid = ""19"">We can see that segmentation and POS tagging task is to divide a character sequence into several subsequences and label each of them a POS tag.</S><S sid =""11"" ssid = ""7"">Compared to performing segmentation and POS tagging one at a time  Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging (Ng and Low  2004).</S><S sid =""80"" ssid = ""5"">At each position i  we enumerate all possible word-POS pairs by assigning each POS to each possible word formed from the character subsequence spanning length l = L. min(i  K) (K is assigned 20 in all our experiments) and ending at position i  then we derive all candidate results by attaching each word-POS pair p (of length l) to the tail of each candidate result at the prior position of p (position i−l)  and select for position i a N-best list of candidate results from all these candidates.</S><S sid =""81"" ssid = ""6"">When we derive a candidate result from a word-POS pair p and a candidate q at prior position of p  we calculate the scores of the word LM  the POS LM  the labelling probability and the generating probability  Algorithm 2 Decoding algorithm. as well as the score of the perceptron model.</S>",['Method_Citation']
11,P08-1102,C10-2096,0,"Jiang et al, 2008a",0,"6.1.2 Lattice-forest SystemWe first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al, 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm","We first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al, 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm","['10', '25', '23', '7', '106']","<S sid =""10"" ssid = ""6"">Since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a boundary tag (Xue and Shen  2003)  Joint S&T can be conducted in a labelling fashion by expanding boundary tags to include POS information (Ng and Low  2004).</S><S sid =""25"" ssid = ""21"">According to Ng and Low (2004)  the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S><S sid =""23"" ssid = ""19"">We can see that segmentation and POS tagging task is to divide a character sequence into several subsequences and label each of them a POS tag.</S><S sid =""7"" ssid = ""3"">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S><S sid =""106"" ssid = ""17"">Note that the accuracy of Joint S&T means that a word-POS pair is recognized only if both the boundary tags and the POS’s are correctly labelled.</S>",['Method_Citation']
12,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"However, when we repeat the work of (Jiang et al, 2008), which reports to achieve the state-of-art performance in the data-sets that we adopt, it has been found that some features (e.g., C0) are unnoticeably trained several times in their model (which are implicitly generated from different feature templates used in the paper)","However, when we repeat the work of (Jiang et al, 2008), which reports to achieve the state-of-art performance in the data-sets that we adopt, it has been found that some features (e.g., C0) are unnoticeably trained several times in their model (which are implicitly generated from different feature templates used in the paper)","['11', '9', '3', '26', '106']","<S sid =""11"" ssid = ""7"">Compared to performing segmentation and POS tagging one at a time  Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging (Ng and Low  2004).</S><S sid =""9"" ssid = ""5"">To segment and tag a character sequence  there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T).</S><S sid =""3"" ssid = ""3"">Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.</S><S sid =""26"" ssid = ""22"">In order to perform POS tagging at the same time  we expand boundary tags to include POS information by attaching a POS to the tail of a boundary tag as a postfix following Ng and Low (2004).</S><S sid =""106"" ssid = ""17"">Note that the accuracy of Joint S&T means that a word-POS pair is recognized only if both the boundary tags and the POS’s are correctly labelled.</S>",['Method_Citation']
13,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"Unicode/CP936 1.1M/55K 104K/13K 0.035 Table 3: Corpus statistics for the second SIGHAN Bakeoff appears twice, which is generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al, 2008), with n=0, generates [C0C0])","Table 3: Corpus statistics for the second SIGHAN Bakeoff appears twice, which is generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al, 2008), with n=0, generates [C0C0])","['12', '14', '54', '51', '82']","<S sid =""12"" ssid = ""8"">Besides the usual character-based features  additional features dependent on POS’s or words can also be employed to improve the performance.</S><S sid =""14"" ssid = ""10"">As a result  many theoretically useful features such as higherorder word or POS n-grams are difficult to be incorporated in the model efficiently.</S><S sid =""54"" ssid = ""5"">We noticed that the templates related to word unigrams and bigrams bring to the feature space an enlargement much rapider than the character-base ones  not to mention the higher-order grams such as trigrams or 4-grams.</S><S sid =""51"" ssid = ""2"">Additional features most widely used are related to word or POS ngrams.</S><S sid =""82"" ssid = ""7"">In addition  we add the score of the word count penalty as another feature to alleviate the tendency of LMs to favor shorter candidates.</S>",['Aim_Citation']
14,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"As all the features adopted in (Jiang et al, 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be? n?, at least in principle","As all the features adopted in (Jiang et al, 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be n, at least in principle","['13', '52', '55', '131', '14']","<S sid =""13"" ssid = ""9"">However  as such features are generated dynamically during the decoding procedure  two limitation arise: on the one hand  the amount of parameters increases rapidly  which is apt to overfit on training corpus; on the other hand  exact inference by dynamic programming is intractable because the current predication relies on the results of prior predications.</S><S sid =""52"" ssid = ""3"">However  such features are generated dynamically during the decoding procedure so that the feature space enlarges much more rapidly.</S><S sid =""55"" ssid = ""6"">In addition  even though these higher grams were managed to be used  there still remains another problem: as the current predication relies on the results of prior ones  the decoding procedure has to resort to approximate inference by maintaining a list of N-best candidates at each predication position  which evokes a potential risk to depress the training.</S><S sid =""131"" ssid = ""2"">Under this model  many knowledge sources that may be intractable to be incorporated into the perceptron directly  can be utilized effectively in the outside-layer linear model.</S><S sid =""14"" ssid = ""10"">As a result  many theoretically useful features such as higherorder word or POS n-grams are difficult to be incorporated in the model efficiently.</S>",['Method_Citation']
15,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"Inspired by (Jiang et al, 2008), we set the real d Although Table 5 has shown that the proposed all the value of C0 to be 2.0, the value of C-1C0anC0C1 to be 3.0, and the values of all other features to be 1.0 for the character-based discriminative-plus model","Inspired by (Jiang et al, 2008), we set the real d Although Table 5 has shown that the proposed all the value of C0 to be 2.0, the value of C-1C0anC0C1 to be 3.0, and the values of all other features to be 1.0 for the character-based discriminative-plus model","['14', '12', '131', '132', '54']","<S sid =""14"" ssid = ""10"">As a result  many theoretically useful features such as higherorder word or POS n-grams are difficult to be incorporated in the model efficiently.</S><S sid =""12"" ssid = ""8"">Besides the usual character-based features  additional features dependent on POS’s or words can also be employed to improve the performance.</S><S sid =""131"" ssid = ""2"">Under this model  many knowledge sources that may be intractable to be incorporated into the perceptron directly  can be utilized effectively in the outside-layer linear model.</S><S sid =""132"" ssid = ""3"">This is a substitute method to use both local and non-local features  and it would be especially useful when the training corpus is very large.</S><S sid =""54"" ssid = ""5"">We noticed that the templates related to word unigrams and bigrams bring to the feature space an enlargement much rapider than the character-base ones  not to mention the higher-order grams such as trigrams or 4-grams.</S>",['Method_Citation']
17,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"Last, (Jiang et al, 2008) 5 adds repeated features implicitly based on (Ng and Low, 2004)","Last, (Jiang et al, 2008) 5 adds repeated features implicitly based on (Ng and Low, 2004)","['15', '56', '49', '45', '8']","<S sid =""15"" ssid = ""11"">To cope with this problem  we propose a cascaded linear model inspired by the log-linear model (Och and Ney  2004) widely used in statistical machine translation to incorporate different kinds of knowledge sources.</S><S sid =""56"" ssid = ""7"">To alleviate the drawbacks  we propose a cascaded linear model.</S><S sid =""49"" ssid = ""21"">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S><S sid =""45"" ssid = ""17"">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S><S sid =""8"" ssid = ""4"">Another widely used discriminative method is the perceptron algorithm (Collins  2002)  which achieves comparable performance to CRFs with much faster training  so we base this work on the perceptron.</S>",['Method_Citation']
20,P08-1102,D12-1046,0,Jiang et al2008a,0,"Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a) ,perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b)","Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a), perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b)","['16', '65', '57', '2', '78']","<S sid =""16"" ssid = ""12"">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S><S sid =""65"" ssid = ""16"">As shown in Figure 1  the character-based perceptron is used as the inside-layer linear model and sends its output to the outside-layer.</S><S sid =""57"" ssid = ""8"">It has a two-layer architecture  with a perceptron as the core and another linear model as the outside-layer.</S><S sid =""2"" ssid = ""2"">With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S><S sid =""78"" ssid = ""3"">Given a Chinese character sequence C1:n  the decoding procedure can proceed in a left-right fashion with a dynamic programming approach.</S>",['Method_Citation']
