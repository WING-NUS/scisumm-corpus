Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,D09-1092,P14-1004,0,"Mimno et al, 2009",0,"This configuration is similar to PolyLDA (Mimno et al, 2009) or LinkLDA (Yano et al, 2009), such that utterances from different parties are treated as different languages or blog-post and comments pairs","This configuration is similar to PolyLDA (Mimno et al, 2009) or LinkLDA (Yano et al, 2009), such that utterances from different parties are treated as different languages or blog-post and comments pairs","['1', '25', '105', '194', '126']","<S sid =""1"" ssid = ""1"">Topic models are a useful tool for analyzing large text collections  but have previously been applied in only monolingual  or at most bilingual  contexts.</S><S sid =""25"" ssid = ""1"">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid =""105"" ssid = ""54"">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct  non-comparable documents in multiple languages.</S><S sid =""194"" ssid = ""3"">We also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in non-comparable corpora.</S><S sid =""126"" ssid = ""75"">These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.</S>",['Aim_Citation']
2,D09-1092,P10-1044,0,"Mimno et al, 2009",0,"Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009)","Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009)","['2', '16', '168', '196', '105']","<S sid =""2"" ssid = ""2"">Meanwhile  massive collections of interlinked documents in dozens of languages  such as Wikipedia  are now widely available  calling for tools that can characterize content in many languages.</S><S sid =""16"" ssid = ""12"">However  the growth of the internet  and in particular Wikipedia  has made vast corpora of topically comparable texts—documents that are topically similar but are not direct translations of one another—considerably more abundant than ever before.</S><S sid =""168"" ssid = ""2"">However  the growth of the web  and in particular Wikipedia  has made comparable text corpora – documents that are topically similar but are not direct translations of one another – considerably more abundant than true parallel corpora.</S><S sid =""196"" ssid = ""5"">When applied to comparable document collections such as Wikipedia  PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.</S><S sid =""105"" ssid = ""54"">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct  non-comparable documents in multiple languages.</S>",['Method_Citation']
3,D09-1092,P11-2084,0,"Mimno et al, 2009",0,"(Mimno et al, 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then add the Cartesian product of these sets for every topic to a set of candidate translations","(Mimno et al, 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then add the Cartesian product of these sets for every topic to a set of candidate translations","['3', '192', '18', '31', '20']","<S sid =""3"" ssid = ""3"">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid =""192"" ssid = ""1"">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S sid =""18"" ssid = ""14"">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid =""31"" ssid = ""7"">They also provide little analysis of the differences between polylingual and single-language topic models.</S><S sid =""20"" ssid = ""16"">We also explore how the characteristics of different languages affect topic model performance.</S>",['Method_Citation']
4,D09-1092,E12-1014,0,"Mimno et al, 2009",0,"Our Wikipedia-based topic similarity feature, w (f, e), is similar in spirit to polylingualtopic models (Mimno et al 2009), but it is scalable to full bilingual lexicon induction","Our Wikipedia-based topic similarity feature, w (f, e), is similar in spirit to polylingual topic models (Mimno et al 2009), but it is scalable to full bilingual lexicon induction","['4', '22', '19', '10', '126']","<S sid =""4"" ssid = ""4"">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid =""22"" ssid = ""18"">We use this corpus to explore the ability of the model both to infer similarities between vocabularies in different languages  and to detect differences in topic emphasis between languages.</S><S sid =""19"" ssid = ""15"">We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S><S sid =""10"" ssid = ""6"">We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).</S><S sid =""126"" ssid = ""75"">These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.</S>",['Aim_Citation']
5,D09-1092,D11-1086,0,"Mimno et al, 2009",0,"of English document and the second half of its aligned foreign language document (Mimno et al,2009)","For Europarl data sets, we artificially make them comparable by considering the first half of English document and the second half of its aligned foreign language document (Mimno et al,2009)","['5', '15', '18', '3', '6']","<S sid =""5"" ssid = ""1"">Statistical topic models have emerged as an increasingly useful analysis tool for large text collections.</S><S sid =""15"" ssid = ""11"">Previous work on bilingual topic modeling has focused on machine translation applications  which rely on sentence-aligned parallel translations.</S><S sid =""18"" ssid = ""14"">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid =""3"" ssid = ""3"">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid =""6"" ssid = ""2"">Topic models have been used for analyzing topic trends in research literature (Mann et al.  2006; Hall et al.  2008)  inferring captions for images (Blei and Jordan  2003)  social network analysis in email (McCallum et al.  2005)  and expanding queries with topically related words in information retrieval (Wei and Croft  2006).</S>",['Method_Citation']
6,D09-1092,N12-1007,0,"Mimno et al, 2009",0,"Since the PLTM is not a contribution of this paper, we refer the interested reader to (Mimno et al, 2009) for more details","Since the PLTM is not a contribution of this paper, we refer the interested reader to (Mimno et al, 2009) for more details","['6', '26', '5', '18', '29']","<S sid =""6"" ssid = ""2"">Topic models have been used for analyzing topic trends in research literature (Mann et al.  2006; Hall et al.  2008)  inferring captions for images (Blei and Jordan  2003)  social network analysis in email (McCallum et al.  2005)  and expanding queries with topically related words in information retrieval (Wei and Croft  2006).</S><S sid =""26"" ssid = ""2"">Tam  Lane and Schultz (Tam et al.  2007) also show improvements in machine translation using bilingual topic models.</S><S sid =""5"" ssid = ""1"">Statistical topic models have emerged as an increasingly useful analysis tool for large text collections.</S><S sid =""18"" ssid = ""14"">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid =""29"" ssid = ""5"">A recent extended abstract  developed concurrently by Ni et al. (Ni et al.  2009)  discusses a multilingual topic model similar to the one presented here.</S>",['Method_Citation']
7,D09-1092,N12-1007,0,2009,0,"Evaluation Corpus The automatic evaluation of cross-lingual co reference systems requires annotated 10Mimno et al (2009) showed that so long as the proportion of topically-aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly","Mimno et al (2009) showed that so long as the proportion of topically-aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly","['7', '15', '31', '18', '161']","<S sid =""7"" ssid = ""3"">Much of this work  however  has occurred in monolingual contexts.</S><S sid =""15"" ssid = ""11"">Previous work on bilingual topic modeling has focused on machine translation applications  which rely on sentence-aligned parallel translations.</S><S sid =""31"" ssid = ""7"">They also provide little analysis of the differences between polylingual and single-language topic models.</S><S sid =""18"" ssid = ""14"">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid =""161"" ssid = ""110"">As noted before  many of the documents in the EuroParl collection consist of short  formulaic sentences.</S>",['Method_Citation']
8,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"Similarly, Polylingual Topic Models (PLTM) (Mimno et al, 2009) generalized LDA to tuples of documents from multiple languages","Similarly, Polylingual Topic Models (PLTM) (Mimno et al, 2009) generalized LDA to tuples of documents from multiple languages","['8', '16', '168', '23', '108']","<S sid =""8"" ssid = ""4"">In an increasingly connected world  the ability to access documents in many languages has become both a strategic asset and a personally enriching experience.</S><S sid =""16"" ssid = ""12"">However  the growth of the internet  and in particular Wikipedia  has made vast corpora of topically comparable texts—documents that are topically similar but are not direct translations of one another—considerably more abundant than ever before.</S><S sid =""168"" ssid = ""2"">However  the growth of the web  and in particular Wikipedia  has made comparable text corpora – documents that are topically similar but are not direct translations of one another – considerably more abundant than true parallel corpora.</S><S sid =""23"" ssid = ""19"">The internet makes it possible for people all over the world to access documents from different cultures  but readers will not be fluent in this wide variety of languages.</S><S sid =""108"" ssid = ""57"">If we wish to perform topic-based bibliometric analysis  it is vital to be able to track the same topics across all languages.</S>",['Results_Citation']
9,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"Our baseline joint PLSA model (JPLSA) is closely related to the poly-lingual LDA model of (Mimno et al, 2009)","Our baseline joint PLSA model (JPLSA) is closely related to the poly-lingual LDA model of (Mimno et al, 2009)","['9', '192', '18', '3', '169']","<S sid =""9"" ssid = ""5"">In this paper  we present the polylingual topic model (PLTM).</S><S sid =""192"" ssid = ""1"">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S sid =""18"" ssid = ""14"">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid =""3"" ssid = ""3"">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid =""169"" ssid = ""3"">In this section  we explore two questions relating to comparable text corpora and polylingual topic modeling.</S>",['Method_Citation']
10,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"We describe the model for two languages, but it is straightforward to generalize to more than two languages, as in (Mimno et al, 2009)","We describe the model for two languages, but it is straightforward to generalize to more than two languages, as in (Mimno et al, 2009)","['10', '4', '19', '22', '111']","<S sid =""10"" ssid = ""6"">We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).</S><S sid =""4"" ssid = ""4"">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid =""19"" ssid = ""15"">We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S><S sid =""22"" ssid = ""18"">We use this corpus to explore the ability of the model both to infer similarities between vocabularies in different languages  and to detect differences in topic emphasis between languages.</S><S sid =""111"" ssid = ""60"">In order to simulate this scenario we create a set of variations of the EuroParl corpus by treating some documents as if they have no parallel/comparable texts – i.e.  we put each of these documents in a single-document tuple.</S>",['Method_Citation']
11,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"j=1 P (z2j|?) P (w 2 j| ?z2j) The difference between the JPLSA model and the poly-lingual topic model of (Mimno et al, 2009) is that we merge the vocabularies in the two languages and learn topic-specific word distributions over these merged vocabularies, instead of having pairs of topic-specific word distributions, one for each language, like in (Mimno et al, 2009)","The difference between the JPLSA model and the poly-lingual topic model of (Mimno et al, 2009) is that we merge the vocabularies in the two languages and learn topic-specific word distributions over these merged vocabularies, instead of having pairs of topic-specific word distributions, one for each language, like in (Mimno et al, 2009)","['11', '169', '3', '18', '31']","<S sid =""11"" ssid = ""7"">There are many potential applications for polylingual topic models.</S><S sid =""169"" ssid = ""3"">In this section  we explore two questions relating to comparable text corpora and polylingual topic modeling.</S><S sid =""3"" ssid = ""3"">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid =""18"" ssid = ""14"">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid =""31"" ssid = ""7"">They also provide little analysis of the differences between polylingual and single-language topic models.</S>",['Method_Citation']
12,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"Another difference between our model and the poly-lingual LDA model of (Mimno et al, 2009) is that we use maximum aposteriori (MAP) instead of Bayesian inference","Another difference between our model and the poly-lingual LDA model of (Mimno et al, 2009) is that we use maximum aposteriori (MAP) instead of Bayesian inference","['12', '161', '174', '7', '14']","<S sid =""12"" ssid = ""8"">Although research literature is typically written in English  bibliographic databases often contain substantial quantities of work in other languages.</S><S sid =""161"" ssid = ""110"">As noted before  many of the documents in the EuroParl collection consist of short  formulaic sentences.</S><S sid =""174"" ssid = ""8"">These versions of Wikipedia were selected to provide a diverse range of language families  geographic areas  and quantities of text.</S><S sid =""7"" ssid = ""3"">Much of this work  however  has occurred in monolingual contexts.</S><S sid =""14"" ssid = ""10"">Such analysis could be significant in tracking international research trends  where language barriers slow the transfer of ideas.</S>",['Aim_Citation']
13,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"For computing distance we used the L1-norm of the difference, which worked a bit better than the Jensen Shannon divergence between the topic vectors used in (Mimno et al, 2009)","For computing distance we used the L1-norm of the difference, which worked a bit better than the Jensen Shannon divergence between the topic vectors used in (Mimno et al, 2009)","['13', '108', '196', '126', '193']","<S sid =""13"" ssid = ""9"">To perform topic-based bibliometric analysis on these collections  it is necessary to have topic models that are aligned across languages.</S><S sid =""108"" ssid = ""57"">If we wish to perform topic-based bibliometric analysis  it is vital to be able to track the same topics across all languages.</S><S sid =""196"" ssid = ""5"">When applied to comparable document collections such as Wikipedia  PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.</S><S sid =""126"" ssid = ""75"">These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.</S><S sid =""193"" ssid = ""2"">We analyzed the characteristics of PLTM in comparison to monolingual LDA  and demonstrated that it is possible to discover aligned topics.</S>",['Method_Citation']
15,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"In previously reported work, (Mimno et al, 2009) evaluate parallel document retrieval using PLTM on Europarl speeches in English and Spanish, using training and test sets of size similar to ours","In previously reported work, (Mimno et al, 2009) evaluate parallel document retrieval using PLTM on Europarl speeches in English and Spanish, using training and test sets of size similar to ours","['14', '20', '170', '53', '68']","<S sid =""14"" ssid = ""10"">Such analysis could be significant in tracking international research trends  where language barriers slow the transfer of ideas.</S><S sid =""20"" ssid = ""16"">We also explore how the characteristics of different languages affect topic model performance.</S><S sid =""170"" ssid = ""4"">First  we explore whether comparable document tuples support the alignment of fine-grained topics  as demonstrated earlier using parallel documents.</S><S sid =""53"" ssid = ""2"">In this case  we can be confident that the topic distribution is genuinely shared across all languages.</S><S sid =""68"" ssid = ""17"">The third topic demonstrates differences in inflectional variation.</S>",['Method_Citation']
16,D09-1092,W12-3117,0,"Mimno et al, 2009",0,"We assume that a higher dimensionality can lead to a better repartitioning of the vocabulary over the topics. Multilingual LDA has been used before in natural language processing ,e.g .polylingual topic models (Mimno et al, 2009) or multilingual topic models for unaligned text (Boyd-Graber and Blei, 2009)","Multilingual LDA has been used before in natural language processing, e.g. polylingual topic models (Mimno et al, 2009) or multilingual topic models for unaligned text (Boyd-Graber and Blei, 2009)","['15', '52', '31', '3', '18']","<S sid =""15"" ssid = ""11"">Previous work on bilingual topic modeling has focused on machine translation applications  which rely on sentence-aligned parallel translations.</S><S sid =""52"" ssid = ""1"">Our first set of experiments focuses on document tuples that are known to consist of direct translations.</S><S sid =""31"" ssid = ""7"">They also provide little analysis of the differences between polylingual and single-language topic models.</S><S sid =""3"" ssid = ""3"">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid =""18"" ssid = ""14"">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S>",['Method_Citation']
17,D09-1092,W11-2133,0,2009,0,"ji =wjk? M j i? m k=1w j k, (1) where M j is the topic distribution of document j and wk is the number of occurrences of phrase pair Xk in document j. Mimno et al (2009) extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipediaarticles)","Mimno et al (2009) extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipedia articles)","['16', '168', '2', '54', '21']","<S sid =""16"" ssid = ""12"">However  the growth of the internet  and in particular Wikipedia  has made vast corpora of topically comparable texts—documents that are topically similar but are not direct translations of one another—considerably more abundant than ever before.</S><S sid =""168"" ssid = ""2"">However  the growth of the web  and in particular Wikipedia  has made comparable text corpora – documents that are topically similar but are not direct translations of one another – considerably more abundant than true parallel corpora.</S><S sid =""2"" ssid = ""2"">Meanwhile  massive collections of interlinked documents in dozens of languages  such as Wikipedia  are now widely available  calling for tools that can characterize content in many languages.</S><S sid =""54"" ssid = ""3"">Although direct translations in multiple languages are relatively rare (in contrast with comparable documents)  we use direct translations to explore the characteristics of the model.</S><S sid =""21"" ssid = ""17"">The second corpus  Wikipedia articles in twelve languages  contains sets of documents that are not translations of one another  but are very likely to be about similar concepts.</S>",['Method_Citation']
18,D09-1092,W11-2133,0,2009,0,Tuple-specific topic distributions arelearned using LDA with distinct topic-word concentration parameters? l. Mimno et al (2009) show that PLTM sufficiently aligns topics in parallel corpora,Mimno et al (2009) show that PLTM sufficiently aligns topics in parallel corpora,"['17', '126', '194', '105', '13']","<S sid =""17"" ssid = ""13"">We argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages.</S><S sid =""126"" ssid = ""75"">These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.</S><S sid =""194"" ssid = ""3"">We also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in non-comparable corpora.</S><S sid =""105"" ssid = ""54"">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct  non-comparable documents in multiple languages.</S><S sid =""13"" ssid = ""9"">To perform topic-based bibliometric analysis on these collections  it is necessary to have topic models that are aligned across languages.</S>",['Method_Citation']
19,D09-1092,P14-2110,0,"Mimno et al, 2009",0,"A good candidate for multilingual topic analyses are polylin gual topic models (Mimno et al, 2009), which learn topics for multiple languages, creating tuples of language specific distributions over monolingual vocabularies for each topic","A good candidate for multilingual topic analyses are polylingual topic models (Mimno et al, 2009), which learn topics for multiple languages, creating tuples of language specific distributions over monolingual vocabularies for each topic","['18', '169', '3', '31', '9']","<S sid =""18"" ssid = ""14"">In this paper  we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S><S sid =""169"" ssid = ""3"">In this section  we explore two questions relating to comparable text corpora and polylingual topic modeling.</S><S sid =""3"" ssid = ""3"">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid =""31"" ssid = ""7"">They also provide little analysis of the differences between polylingual and single-language topic models.</S><S sid =""9"" ssid = ""5"">In this paper  we present the polylingual topic model (PLTM).</S>",['Method_Citation']
20,D09-1092,P14-2110,0,2009,0,"3csLDATo train a polylingual topic model on social media, we make two modifications to the model of Mimno et al (2009): add a token specific language variable, and a process for identifying aligned top ics. First ,polylingual topic models require parallel or comparable corpora in which each document has an assigned language","To train a polylingual topic model on social media, we make two modifications to the model of Mimno et al (2009): add a token specific language variable, and a process for identifying aligned top ics","['19', '148', '131', '4', '126']","<S sid =""19"" ssid = ""15"">We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S><S sid =""148"" ssid = ""97"">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid =""131"" ssid = ""80"">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid =""4"" ssid = ""4"">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid =""126"" ssid = ""75"">These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.</S>",['Method_Citation']
