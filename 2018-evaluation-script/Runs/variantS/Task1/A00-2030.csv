Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,A00-2030,W01-0510,0,"Miller et al., 2000",0,"Section 5 compares our approach tooth ers in the literature, in particular that of (Miller et al., 2000)","Section 5 compares our approach too thiers in the literature, in particular that of (Miller et al., 2000)","['1', '10', '5', '43', '104']","<S sid =""1"" ssid = ""1"">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S><S sid =""10"" ssid = ""8"">Instead  our parsing algorithm  trained on the UPenn TREEBANK  was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S><S sid =""5"" ssid = ""3"">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid =""43"" ssid = ""3"">Thus  we did not consider simply adding semantic labels to the existing Penn TREEBANK  which is drawn from a single source â€” the Wall Street Journal â€” and is impoverished in articles about rocket launches.</S><S sid =""104"" ssid = ""1"">We have demonstrated  at least for one problem  that a lexicalized  probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>",['Aim_Citation']
2,A00-2030,W01-0510,0,"Miller et al, 2000",0,"The basic approach we described is very similar to the one presented in (Miller et al, 2000) however there are a few major di erences:  in our approach the augmentation of the syn tactic tags with semantic tags is straightforward due to the fact that the semantic constituents are matched exactly 5. The approach in (Miller","The basic approach we described is very similar to the one presented in (Miller et al, 2000) however there are a few major differences:  in our approach the augmentation of the syn tactic tags with semantic tags is straightforward due to the fact that the semantic constituents are matched exactly 5. The approach in (Miller","['2', '6', '28', '96', '26']","<S sid =""2"" ssid = ""2"">In this paper we report adapting a lexic al ized  probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S sid =""6"" ssid = ""4"">In this paper  we report adapting a lexicalized  probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S sid =""28"" ssid = ""11"">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S><S sid =""96"" ssid = ""1"">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S><S sid =""26"" ssid = ""9"">We were already using a generative statistical model for part-of-speech tagging (Weischedel et al. 1993)  and more recently  had begun using a generative statistical model for name finding (Bikel et al.</S>",['Method_Citation']
3,A00-2030,W01-0510,0,"Miller et al, 2000",0,"The semantic annotation required by our task is much simpler than that employed by (Miller et al, 2000)","The semantic annotation required by our task is much simpler than that employed by (Miller et al, 2000)","['3', '1', '104', '5', '96']","<S sid =""3"" ssid = ""1"">Since 1995  a few statistical parsing algorithms (Magerman  1995; Collins  1996 and 1997; Charniak  1997; Rathnaparki  1997) demonstrated a breakthrough in parsing accuracy  as measured against the University of Pennsylvania TREEBANK as a gold standard.</S><S sid =""1"" ssid = ""1"">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S><S sid =""104"" ssid = ""1"">We have demonstrated  at least for one problem  that a lexicalized  probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S><S sid =""5"" ssid = ""3"">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid =""96"" ssid = ""1"">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S>",['Method_Citation']
4,A00-2030,W01-0510,0,"Miller et al, 2000",0,"One possibly bene cial extension of our work suggested by (Miller et al, 2000) would be to add semantic tags describing relations between entities (slots), in which case the semantic constraints would not be structured strictly on the two levels used in the current approach, respectively frame and slot level","One possibly beneficial extension of our work suggested by (Miller et al, 2000) would be to add semantic tags describing relations between entities (slots), in which case the semantic constraints would not be structured strictly on the two levels used in the current approach, respectively frame and slot level","['4', '105', '23', '28', '108']","<S sid =""4"" ssid = ""2"">Yet  relatively few have embedded one of these algorithms in a task.</S><S sid =""105"" ssid = ""2"">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S><S sid =""23"" ssid = ""6"">An integrated model can limit the propagation of errors by making all decisions jointly.</S><S sid =""28"" ssid = ""11"">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S><S sid =""108"" ssid = ""5"">This simple semantic annotation was the only source of task knowledge used to configure the model.</S>",['Aim_Citation']
5,A00-2030,W01-0510,0,"Miller et al, 2000",0,"Similar to the approach in (Miller et al, 2000 )weinitialized the SLM statistics from the UPenn Tree bank parse trees (about 1Mwds of training data) at the rst training stage, see Section 3","Similar to the approach in (Miller et al, 2000) we initialized the SLM statistics from the UPenn Tree bank parse trees","['5', '106', '1', '28', '10']","<S sid =""5"" ssid = ""3"">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid =""106"" ssid = ""3"">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S><S sid =""1"" ssid = ""1"">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S><S sid =""28"" ssid = ""11"">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S><S sid =""10"" ssid = ""8"">Instead  our parsing algorithm  trained on the UPenn TREEBANK  was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S>",['Method_Citation']
6,A00-2030,P14-1078,0,"Miller et al, 2000",0,"Rule-based methods (Miller et al, 2000) employ a number of linguistic rules to capture relation patterns","Rule-based methods (Miller et al, 2000) employ a number of linguistic rules to capture relation patterns","['6', '104', '2', '51', '1']","<S sid =""6"" ssid = ""4"">In this paper  we report adapting a lexicalized  probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S sid =""104"" ssid = ""1"">We have demonstrated  at least for one problem  that a lexicalized  probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S><S sid =""2"" ssid = ""2"">In this paper we report adapting a lexic al ized  probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S sid =""51"" ssid = ""11"">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid =""1"" ssid = ""1"">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S>",['Method_Citation']
7,A00-2030,P05-1061,0,2000,0,"One interesting system that does not belong to the above class is that of Miller et al (2000), who take the view that relation extraction is just a form of probabilistic parsing where parse trees are augmented to identify all relations","One interesting system that does not belong to the above class is that of Miller et al (2000), who take the view that relation extraction is just a form of probabilistic parsing where parse trees are augmented to identify all relations","['7', '11', '28', '50', '107']","<S sid =""7"" ssid = ""5"">The technique was benchmarked in the Seventh Message Understanding Conference (MUC-7) in 1998.</S><S sid =""11"" ssid = ""1"">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh  1998).</S><S sid =""28"" ssid = ""11"">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S><S sid =""50"" ssid = ""10"">Figure 4 shows an example of the semantic annotation  which was the only type of manual annotation we performed.</S><S sid =""107"" ssid = ""4"">The semantic training corpus was produced by students according to a simple set of guidelines.</S>",['Method_Citation']
8,A00-2030,P05-1053,0,2000,0,"Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees","Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees","['8', '10', '5', '51', '1']","<S sid =""8"" ssid = ""6"">Several technical challenges confronted us and were solved: TREEBANK on Wall Street Journal adequately train the algorithm for New York Times newswire  which includes dozens of newspapers?</S><S sid =""10"" ssid = ""8"">Instead  our parsing algorithm  trained on the UPenn TREEBANK  was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S><S sid =""5"" ssid = ""3"">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid =""51"" ssid = ""11"">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid =""1"" ssid = ""1"">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S>",['Results_Citation']
9,A00-2030,P05-1053,0,2000,0,"Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model","Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model","['9', '106', '108', '107', '41']","<S sid =""9"" ssid = ""7"">Manually creating sourcespecific training data for syntax was not required.</S><S sid =""106"" ssid = ""3"">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S><S sid =""108"" ssid = ""5"">This simple semantic annotation was the only source of task knowledge used to configure the model.</S><S sid =""107"" ssid = ""4"">The semantic training corpus was produced by students according to a simple set of guidelines.</S><S sid =""41"" ssid = ""1"">To train our integrated model  we required a large corpus of augmented parse trees.</S>",['Method_Citation']
10,A00-2030,H05-1094,0,2000,0,"(Miller et al, 2000) have combined entity recognition, parsing, and relation extraction into a jointly-trained single statistical parsing model that achieves improved performance on all the subtasks. Part of the contribution of the current work is to suggest that joint decoding can be effective even when joint training is not possible because jointly-labeled data is unavailable","(Miller et al, 2000) have combined entity recognition, parsing, and relation extraction into a jointly-trained single statistical parsing model that achieves improved performance on all the subtasks. Part of the contribution of the current work is to suggest that joint decoding can be effective even when joint training is not possible because jointly-labeled data is unavailable","['10', '1', '43', '51', '5']","<S sid =""10"" ssid = ""8"">Instead  our parsing algorithm  trained on the UPenn TREEBANK  was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S><S sid =""1"" ssid = ""1"">Since 1995  a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy  as measured against the UPenn TREEBANK as a gold standard.</S><S sid =""43"" ssid = ""3"">Thus  we did not consider simply adding semantic labels to the existing Penn TREEBANK  which is drawn from a single source â€” the Wall Street Journal â€” and is impoverished in articles about rocket launches.</S><S sid =""51"" ssid = ""11"">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid =""5"" ssid = ""3"">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S>",['Method_Citation']
11,A00-2030,P04-1054,0,2000,0,Miller et al (2000) propose an integrated statistical parsing technique that augments parse trees with semantic labels denoting entity and relation types,Miller et al (2000) propose an integrated statistical parsing technique that augments parse trees with semantic labels denoting entity and relation types,"['11', '7', '101', '5', '96']","<S sid =""11"" ssid = ""1"">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh  1998).</S><S sid =""7"" ssid = ""5"">The technique was benchmarked in the Seventh Message Understanding Conference (MUC-7) in 1998.</S><S sid =""101"" ssid = ""6"">We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97)  and evaluated name finding accuracy on the MUC7 named entity test.</S><S sid =""5"" ssid = ""3"">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid =""96"" ssid = ""1"">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S>",['Method_Citation']
12,A00-2030,P04-1054,0,2000,0,"WhereasMiller et al (2000) use a generative model to produce parse information as well as relation information, we hypothesize that a technique discriminatively trained to classify relations will achieve better performance","Whereas Miller et al (2000) use a generative model to produce parse information as well as relation information, we hypothesize that a technique discriminatively trained to classify relations will achieve better performance","['12', '98', '16', '78', '34']","<S sid =""12"" ssid = ""2"">The Template Element (TE) task identifies organizations  persons  locations  and some artifacts (rocket and airplane-related artifacts).</S><S sid =""98"" ssid = ""3"">In both Template Entity (TE) and Template Relation (TR)  our system finished in second place among all entrants.</S><S sid =""16"" ssid = ""6"">For the following example  the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S><S sid =""78"" ssid = ""19"">If we generalize the tree components (constituent labels  words  tags  etc.) and treat them all as simply elements  e  and treat all the conditioning factors as the history  h  we can write:</S><S sid =""34"" ssid = ""2"">In these trees  the standard TREEBANK structures are augmented to convey semantic information  that is  entities and relations.</S>",['Aim_Citation']
13,A00-2030,W05-0602,0,"Miller et al, 2000",0,"The syntactic model in (Miller et al, 2000) is similar to Collins?, but doesnot use features like sub cat frames and distance measures","The syntactic model in (Miller et al, 2000) is similar to Collins', but does not use features like subcat frames and distance measures","['13', '14', '15', '58', '100']","<S sid =""13"" ssid = ""3"">For each organization in an article  one must identify all of its names as used in the article  its type (corporation  government  or other)  and any significant description of it.</S><S sid =""14"" ssid = ""4"">For each person  one must find all of the person's names within the document  his/her type (civilian or military)  and any significant descriptions (e.g.  titles).</S><S sid =""15"" ssid = ""5"">For each location  one must also give its type (city  province  county  body of water  etc.).</S><S sid =""58"" ssid = ""4"">Whenever a relation involves an entity that is not a direct descendant of that relation in the parse tree  semantic pointer labels are attached to all of the intermediate nodes.</S><S sid =""100"" ssid = ""5"">Given multiple constituents that cover identical spans in the chart  only those constituents with probabilities within a While our focus throughout the project was on TE and TR  we became curious about how well the model did at part-of-speech tagging  syntactic parsing  and at name finding.</S>",['Method_Citation']
14,A00-2030,N07-2041,0,"Miller et al, 2000",0,"Similar to the approach in (Miller et al, 2000) and (Kulick et al, 2004), our parser integrates both syntactic and semantic annotations into a single annotation as shown in Figure 2","Similar to the approach in (Miller et al, 2000) and (Kulick et al, 2004), our parser integrates both syntactic and semantic annotations into a single annotation as shown in Figure 2","['14', '13', '74', '15', '36']","<S sid =""14"" ssid = ""4"">For each person  one must find all of the person's names within the document  his/her type (civilian or military)  and any significant descriptions (e.g.  titles).</S><S sid =""13"" ssid = ""3"">For each organization in an article  one must identify all of its names as used in the article  its type (corporation  government  or other)  and any significant description of it.</S><S sid =""74"" ssid = ""15"">The categories for head constituents  clâ€ž are predicted based solely on the category of the parent node  cp: Modifier constituent categories  cm  are predicted based on their parent node  cp  the head constituent of their parent node  chp  the previously generated modifier  câ€ž _1  and the head word of their parent  wp.</S><S sid =""15"" ssid = ""5"">For each location  one must also give its type (city  province  county  body of water  etc.).</S><S sid =""36"" ssid = ""4"">The five key facts in this example are: Here  each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label.</S>",['Method_Citation']
15,A00-2030,W10-2924,0,2000,0,Miller et al (2000) adapt a probabilistic context-free parser for information extraction by augmenting syntactic labels with entity and relation labels,Miller et al (2000) adapt a probabilistic context-free parser for information extraction by augmenting syntactic labels with entity and relation labels,"['15', '78', '91', '82', '55']","<S sid =""15"" ssid = ""5"">For each location  one must also give its type (city  province  county  body of water  etc.).</S><S sid =""78"" ssid = ""19"">If we generalize the tree components (constituent labels  words  tags  etc.) and treat them all as simply elements  e  and treat all the conditioning factors as the history  h  we can write:</S><S sid =""91"" ssid = ""10"">The probability of generating a constituent of the specified category  starting at the topmost node.</S><S sid =""82"" ssid = ""1"">Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.</S><S sid =""55"" ssid = ""1"">syntactic modifier of the other  the inserted node serves to indicate the relation as well as the argument.</S>",['Method_Citation']
16,A00-2030,W06-0508,0,"Miller et al, 2000",0,"Most of the approaches for relation extraction rely on the mapping of syntactic dependencies, such as SVO, onto semantic relations, using either pattern matching or other strategies, such as probabilistic parsing for trees augmented with annotations for entities and relations (Miller et al 2000), or clustering of semantically similar syntactic dependencies, according to their selectional restrictions (Gamallo et al, 2002)","Most of the approaches for relation extraction rely on the mapping of syntactic dependencies, such as SVO, onto semantic relations, using either pattern matching or other strategies, such as probabilistic parsing for trees augmented with annotations for entities and relations (Miller et al 2000), or clustering of semantically similar syntactic dependencies, according to their selectional restrictions (Gamallo et al, 2002)","['16', '74', '98', '12', '94']","<S sid =""16"" ssid = ""6"">For the following example  the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S><S sid =""74"" ssid = ""15"">The categories for head constituents  clâ€ž are predicted based solely on the category of the parent node  cp: Modifier constituent categories  cm  are predicted based on their parent node  cp  the head constituent of their parent node  chp  the previously generated modifier  câ€ž _1  and the head word of their parent  wp.</S><S sid =""98"" ssid = ""3"">In both Template Entity (TE) and Template Relation (TR)  our system finished in second place among all entrants.</S><S sid =""12"" ssid = ""2"">The Template Element (TE) task identifies organizations  persons  locations  and some artifacts (rocket and airplane-related artifacts).</S><S sid =""94"" ssid = ""13"">Given a new sentence  the outcome of this search process is a tree structure that encodes both the syntactic and semantic structure of the sentence.</S>",['Method_Citation']
17,A00-2030,P07-1055,0,"Miller et al, 2000",0,"This includes parsing and relation extraction (Miller et al, 2000), entity labeling and relation extraction (Roth and Yih, 2004), and part-of-speech tagging and chunking (Sutton et al, 2004)","This includes parsing and relation extraction (Miller et al, 2000), entity labeling and relation extraction (Roth and Yih, 2004), and part-of-speech tagging and chunking (Sutton et al, 2004)","['17', '50', '35', '82', '56']","<S sid =""17"" ssid = ""7"">For the following example  the template relation in Figure 2 was to be generated: &quot;Donald M. Goldstein  a historian at the University of Pittsburgh who helped write...&quot;</S><S sid =""50"" ssid = ""10"">Figure 4 shows an example of the semantic annotation  which was the only type of manual annotation we performed.</S><S sid =""35"" ssid = ""3"">An example of an augmented parse tree is shown in Figure 3.</S><S sid =""82"" ssid = ""1"">Given a sentence to be analyzed  the search program must find the most likely semantic and syntactic interpretation.</S><S sid =""56"" ssid = ""2"">For example  in the phrase &quot;Lt. Cmdr.</S>",['Method_Citation']
18,A00-2030,W05-0636,0,2000,0,"For example, Miller et al (2000) showed that performing parsing and information extraction in a joint model improves performance on both tasks","For example, Miller et al (2000) showed that performing parsing and information extraction in a joint model improves performance on both tasks","['18', '19', '29', '5', '84']","<S sid =""18"" ssid = ""1"">Almost all approaches to information extraction â€” even at the sentence level â€” are based on the divide-and-conquer strategy of reducing a complex problem to a set of simpler ones.</S><S sid =""19"" ssid = ""2"">Currently  the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing  replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.</S><S sid =""29"" ssid = ""12"">Thus  each component of what would be the first three stages of our pipeline was based on the same general class of statistical model.</S><S sid =""5"" ssid = ""3"">Chiba  (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.</S><S sid =""84"" ssid = ""3"">Although mathematically the model predicts tree elements in a top-down fashion  we search the space bottom-up using a chartbased search.</S>",['Method_Citation']
19,A00-2030,N06-1037,0,2000,0,Miller et al (2000) address the task of relation extraction from the statistical parsing viewpoint,Miller et al (2000) address the task of relation extraction from the statistical parsing viewpoint,"['19', '96', '29', '24', '90']","<S sid =""19"" ssid = ""2"">Currently  the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing  replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.</S><S sid =""96"" ssid = ""1"">Our system for MUC-7 consisted of the sentential model described in this paper  coupled with a simple probability model for cross-sentence merging.</S><S sid =""29"" ssid = ""12"">Thus  each component of what would be the first three stages of our pipeline was based on the same general class of statistical model.</S><S sid =""24"" ssid = ""7"">For this reason  we focused on designing an integrated model in which tagging  namefinding  parsing  and semantic interpretation decisions all have the opportunity to mutually influence each other.</S><S sid =""90"" ssid = ""9"">Thus  the scores used in pruning can be considered as the product of: 1.</S>",['Method_Citation']
