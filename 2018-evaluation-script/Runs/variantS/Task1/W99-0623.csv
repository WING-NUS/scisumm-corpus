Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Reference Citation
1,W99-0623,A00-2005,0,1999,0,1 Introduct ion Henderson and Brill (1999) showed that independent human research efforts produce parsers that can be combined for an overall boost in accuracy,Henderson and Brill (1999) showed that independent human research efforts produce parsers that can be combined for an overall boost in accuracy,"['1', '72', '38', '129', '130']","<S sid =""1"" ssid = ""1"">Three state-of-the-art statistical parsers are combined to produce more accurate parses  as well as new bounds on achievable Treebank parsing accuracy.</S><S sid =""72"" ssid = ""1"">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank  leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S><S sid =""38"" ssid = ""24"">Under certain conditions the constituent voting and naïve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S><S sid =""129"" ssid = ""58"">In the interest of testing the robustness of these combining techniques  we added a fourth  simple nonlexicalized PCFG parser.</S><S sid =""130"" ssid = ""59"">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S>",['Aim_Citation']
2,W99-0623,A00-2005,0,1999,0,the collection of hypotheses ti =fi (Stest) using the unweighted constituent voting scheme of Henderson and Brill (1999),"Given a novel sentence Stest E Ctest, combine the collection of hypotheses ti = fi(Stest) using the unweighted constituent voting scheme of Henderson and Brill (1999)","['2', '59', '3', '85', '139']","<S sid =""2"" ssid = ""2"">Two general approaches are presented and two combination techniques are described for each approach.</S><S sid =""59"" ssid = ""45"">Once again we present both a non-parametric and a parametric technique for this task.</S><S sid =""3"" ssid = ""3"">Both parametric and non-parametric models are explored.</S><S sid =""85"" ssid = ""14"">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S><S sid =""139"" ssid = ""1"">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>",['Method_Citation']
4,W99-0623,N10-1091,0,"Henderson and Brill, 1999",0,"5 (Henderson and Brill, 1999) used a similar framework in the context of constituent parsing and only three base parsers","(Henderson and Brill, 1999) used a similar framework in the context of constituent parsing and only three base parsers","['3', '59', '2', '138', '136']","<S sid =""3"" ssid = ""3"">Both parametric and non-parametric models are explored.</S><S sid =""59"" ssid = ""45"">Once again we present both a non-parametric and a parametric technique for this task.</S><S sid =""2"" ssid = ""2"">Two general approaches are presented and two combination techniques are described for each approach.</S><S sid =""138"" ssid = ""67"">Surprisingly  the non-parametric switching technique also exhibited robust behaviour in this situation.</S><S sid =""136"" ssid = ""65"">The Bayes models were able to achieve significantly higher precision than their non-parametric counterparts.</S>",['Method_Citation']
5,W99-0623,W05-1518,0,"Henderson and Brill, 1999",0,"A successful application of voting and of a stacked classifier to constituent parsing followed in (Henderson and Brill, 1999)","A successful application of voting and of a stacked classifier to constituent parsing followed in (Henderson and Brill, 1999)","['4', '76', '85', '130', '134']","<S sid =""4"" ssid = ""4"">The resulting parsers surpass the best previously published performance results for the Penn Treebank.</S><S sid =""76"" ssid = ""5"">The standard measures for evaluating Penn Treebank parsing performance are precision and recall of the predicted constituents.</S><S sid =""85"" ssid = ""14"">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S><S sid =""130"" ssid = ""59"">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S><S sid =""134"" ssid = ""63"">As seen by the drop in average individual parser performance baseline  the introduced parser does not perform very well.</S>",['Aim_Citation']
6,W99-0623,W05-1518,0,"Henderson and Brill, 1999",0,"This approach roughly corresponds to (Henderson and Brill, 1999)? s Na ?ve Bayes parse hybridization","This approach roughly corresponds to (Henderson and Brill, 1999)'s Naive Bayes parse hybridization","['5', '6', '12', '13', '26']","<S sid =""5"" ssid = ""1"">The natural language processing community is in the strong position of having many available approaches to solving some of its most fundamental problems.</S><S sid =""6"" ssid = ""2"">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).</S><S sid =""12"" ssid = ""8"">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid =""13"" ssid = ""9"">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).</S><S sid =""26"" ssid = ""12"">This technique has the advantage of requiring no training  but it has the disadvantage of treating all parsers equally even though they may have differing accuracies or may specialize in modeling different phenomena.</S>",['Method_Citation']
7,W99-0623,W05-1518,0,1999,0,Henderson and Brill (1999) also reported that context did not help them to outperform simple voting,Henderson and Brill (1999) also reported that context did not help them to outperform simple voting,"['6', '11', '9', '12', '108']","<S sid =""6"" ssid = ""2"">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).</S><S sid =""11"" ssid = ""7"">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid =""9"" ssid = ""5"">Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).</S><S sid =""12"" ssid = ""8"">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid =""108"" ssid = ""37"">From this we see that a finer-grained model for parser combination  at least for the features we have examined  will not give us any additional power.</S>",['Method_Citation']
8,W99-0623,W05-1518,0,"Henderson and Brill, 1999",0,"(Henderson and Brill, 1999) improved their best parser? s F-measure of 89.7 to 91.3, using their na ?ve Bayes voting on the Penn TreeBank constituent structures (16% error reduction)","(Henderson and Brill, 1999) improved their best parser's F-measure of 89.7 to 91.3, using their naive Bayes voting on the Penn TreeBank constituent structures (16% error reduction)","['7', '30', '116', '38', '41']","<S sid =""7"" ssid = ""3"">Their theoretical finding is simply stated: classification error rate decreases toward the noise rate exponentially in the number of independent  accurate classifiers.</S><S sid =""30"" ssid = ""16"">This is equivalent to the assumption used in probability estimation for naïve Bayes classifiers  namely that the attribute values are conditionally independent when the target value is given.</S><S sid =""116"" ssid = ""45"">The maximum precision row is the upper bound on accuracy if we could pick exactly the correct constituents from among the constituents suggested by the three parsers.</S><S sid =""38"" ssid = ""24"">Under certain conditions the constituent voting and naïve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S><S sid =""41"" ssid = ""27"">IL+-1Proof: Assume a pair of crossing constituents appears in the output of the constituent voting technique using k parsers.</S>",['Method_Citation']
10,W99-0623,P01-1005,0,"Henderson and Brill, 1999",0,"Voting has proven to be an effective technique for improving classifier accuracy for many applications, including part-of-speech tagging (van Halteren, et al 1998), parsing (Henderson and Brill, 1999), and word sense disambiguation (Pederson, 2000)","Voting has proven to be an effective technique for improving classifier accuracy for many applications, including part-of-speech tagging (van Halteren, et al 1998), parsing (Henderson and Brill, 1999), and word sense disambiguation (Pederson, 2000)","['8', '36', '83', '121', '109']","<S sid =""8"" ssid = ""4"">The theory has also been validated empirically.</S><S sid =""36"" ssid = ""22"">The estimation of the probabilities in the model is carried out as shown in Equation 4.</S><S sid =""83"" ssid = ""12"">We performed three experiments to evaluate our techniques.</S><S sid =""121"" ssid = ""50"">Table 3 contains the results for evaluating our systems on the test set (section 22).</S><S sid =""109"" ssid = ""38"">The results in Table 2 were achieved on the development set.</S>",['Results_Citation']
11,W99-0623,D09-1161,0,1999,0,"Regarding the system combination study, Henderson and Brill (1999) propose two parser combination schemes, one that selects an entire tree from one of the parsers, and one that builds a new tree by selecting constituents suggested by the initial trees","Regarding the system combination study, Henderson and Brill (1999) propose two parser combination schemes, one that selects an entire tree from one of the parsers, and one that builds a new tree by selecting constituents suggested by the initial trees","['9', '55', '88', '21', '139']","<S sid =""9"" ssid = ""5"">Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).</S><S sid =""55"" ssid = ""41"">We have developed a general approach for combining parsers when preserving the entire structure of a parse tree is important.</S><S sid =""88"" ssid = ""17"">For example  one parser could be more accurate at predicting noun phrases than the other parsers.</S><S sid =""21"" ssid = ""7"">One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.</S><S sid =""139"" ssid = ""1"">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>",['Method_Citation']
12,W99-0623,D09-1161,0,1999,0,"Henderson and Brill (1999) combine three parsers and obtained an F1 score of 90.6, which is better than the score of 88.6 obtained by the best individual parser as reported in their paper","Henderson and Brill (1999) combine three parsers and obtained an F1 score of 90.6, which is better than the score of 88.6 obtained by the best individual parser as reported in their paper","['10', '136', '85', '141', '134']","<S sid =""10"" ssid = ""6"">In both cases the investigators were able to achieve significant improvements over the previous best tagging results.</S><S sid =""136"" ssid = ""65"">The Bayes models were able to achieve significantly higher precision than their non-parametric counterparts.</S><S sid =""85"" ssid = ""14"">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S><S sid =""141"" ssid = ""3"">All four of the techniques studied result in parsing systems that perform better than any previously reported.</S><S sid =""134"" ssid = ""63"">As seen by the drop in average individual parser performance baseline  the introduced parser does not perform very well.</S>",['Method_Citation']
13,W99-0623,D09-1161,0,"Henderson and Brill, 1999",0,"Besides the two model scores, we also adopt constituent count as an additional feature in spired by (Henderson and Brill 1999) and (Sagae and Lavie 2006)","Besides the two model scores, we also adopt constituent count as an additional feature in spired by (Henderson and Brill 1999) and (Sagae and Lavie 2006)","['11', '9', '6', '12', '13']","<S sid =""11"" ssid = ""7"">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid =""9"" ssid = ""5"">Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).</S><S sid =""6"" ssid = ""2"">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).</S><S sid =""12"" ssid = ""8"">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid =""13"" ssid = ""9"">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).</S>",['Method_Citation']
14,W99-0623,N06-2033,0,"Henderson and Brill, 1999",0,"Henderson and Brill (1999) proposed two parser combination schemes, one that picks an entire tree from one of the parsers, and one that, like ours, builds a new tree from constituents from the initial trees","Henderson and Brill (1999) proposed two parser combination schemes, one that picks an entire tree from one of the parsers, and one that, like ours, builds a new tree from constituents from the initial trees","['12', '13', '11', '6', '72']","<S sid =""12"" ssid = ""8"">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid =""13"" ssid = ""9"">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).</S><S sid =""11"" ssid = ""7"">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid =""6"" ssid = ""2"">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).</S><S sid =""72"" ssid = ""1"">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank  leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S>",['Aim_Citation']
15,W99-0623,N09-2064,0,"Henderson and Brill, 1999",0,"(Henderson and Brill, 1999) perform parse selection by maximizing the expected precision of the selected parse with respect to the set of parses being combined","(Henderson and Brill, 1999) perform parse selection by maximizing the expected precision of the selected parse with respect to the set of parses being combined","['13', '12', '4', '72', '130']","<S sid =""13"" ssid = ""9"">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).</S><S sid =""12"" ssid = ""8"">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid =""4"" ssid = ""4"">The resulting parsers surpass the best previously published performance results for the Penn Treebank.</S><S sid =""72"" ssid = ""1"">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank  leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S><S sid =""130"" ssid = ""59"">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S>",['Method_Citation']
16,W99-0623,N09-2064,0,"Henderson and Brill, 1999",0,"(Henderson and Brill, 1999) and (Sagae and Lavie, 2006) propose methods for parse hybridization by recombining constituents","(Henderson and Brill, 1999) and (Sagae and Lavie, 2006) propose methods for parse hybridization by recombining constituents","['14', '139', '85', '126', '140']","<S sid =""14"" ssid = ""10"">We used these three parsers to explore parser combination techniques.</S><S sid =""139"" ssid = ""1"">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S><S sid =""85"" ssid = ""14"">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S><S sid =""126"" ssid = ""55"">Table 4 shows how much the Bayes switching technique uses each of the parsers on the test set.</S><S sid =""140"" ssid = ""2"">For each experiment we gave an nonparametric and a parametric technique for combining parsers.</S>",['Method_Citation']
17,W99-0623,N09-2064,0,"Henderson and Brill, 1999",0,"output (Figure 3) .Second, the parse selection method of (Henderson and Brill, 1999) selects the parse with maximum expected precision; here, we present an efficient, linear-time algorithm for selecting the parse with maximum expected f-score within the Mini mum Bayes Risk (MBR) framework. Third, we extend these parser combination methods from 1-best outputs to n-best outputs","Second, the parse selection method of (Henderson and Brill, 1999) selects the parse with maximum expected precision; here, we present an efficient, linear-time algorithm for selecting the parse with maximum expected f-score within the Mini mum Bayes Risk (MBR) framework","['15', '55', '27', '85', '88']","<S sid =""15"" ssid = ""1"">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid =""55"" ssid = ""41"">We have developed a general approach for combining parsers when preserving the entire structure of a parse tree is important.</S><S sid =""27"" ssid = ""13"">Another technique for parse hybridization is to use a naïve Bayes classifier to determine which constituents to include in the parse.</S><S sid =""85"" ssid = ""14"">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S><S sid =""88"" ssid = ""17"">For example  one parser could be more accurate at predicting noun phrases than the other parsers.</S>",['Method_Citation']
18,W99-0623,P09-1065,0,"Henderson and Brill, 1999",0,"System combination has benefited various NLP tasks in recent years, such as products-of-experts (e.g., (Smith and Eisner, 2005)) and ensemble based parsing (e.g., (Henderson and Brill, 1999))","System combination has benefited various NLP tasks in recent years, such as products-of-experts (e.g., (Smith and Eisner, 2005)) and ensemble based parsing (e.g., (Henderson and Brill, 1999))","['16', '58', '23', '96', '139']","<S sid =""16"" ssid = ""2"">We call this approach parse hybridization.</S><S sid =""58"" ssid = ""44"">We call this approach parser switching.</S><S sid =""23"" ssid = ""9"">We call this technique constituent voting.</S><S sid =""96"" ssid = ""25"">We call such a constituent an isolated constituent.</S><S sid =""139"" ssid = ""1"">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>",['Method_Citation']
20,W99-0623,C10-1151,0,1999,0,Henderson and Brill (1999) performs parse selection by maximizing the expected precision of selected parse with respect to the set of parses to be combined,Henderson and Brill (1999) performs parse selection by maximizing the expected precision of selected parse with respect to the set of parses to be combined,"['17', '49', '117', '125', '95']","<S sid =""17"" ssid = ""3"">The substructures that are unanimously hypothesized by the parsers should be preserved after combination  and the combination technique should not foolishly create substructures for which there is no supporting evidence.</S><S sid =""49"" ssid = ""35"">In general  the lemma of the previous section does not ensure that all the productions in the combined parse are found in the grammars of the member parsers.</S><S sid =""117"" ssid = ""46"">Another way to interpret this is that less than 5% of the correct constituents are missing from the hypotheses generated by the union of the three parsers.</S><S sid =""125"" ssid = ""54"">The constituent voting and naïve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S><S sid =""95"" ssid = ""24"">One side of the decision making process is when we choose to believe a constituent should be in the parse  even though only one parser suggests it.</S>",['Method_Citation']
