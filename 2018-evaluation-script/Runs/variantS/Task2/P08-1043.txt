Morphological processes in Semitic languages deliver space-delimited words which introduce multiple  distinct  syntactic units into the structure of the input sentence.The input for the segmentation task is however highly ambiguous for Semitic languages  and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al.  2007; Adler and Elhadad  2006).Such ambiguities cause discrepancies between token boundaries (indexed as white spaces) and constituent boundaries (imposed by syntactic categories) with respect to a surface form.Lexical and Morphological Ambiguity The rich morphological processes for deriving Hebrew stems give rise to a high degree of ambiguity for Hebrew space-delimited tokens.Co-occurrences among the particles themselves are subject to further syntactic and lexical constraints relative to the stem.These words are in turn highly ambiguous  breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance.The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens  and the expected number of leaves in the syntactic analysis in not known in advance.Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.We claim that no particular morphological segmentation is a-priory more likely for surface forms before exploring the compositional nature of syntactic structures  including manifestations of various long-distance dependencies.In the current work morphological analyses and lexical probabilities are derived from a small Treebank  which is by no means the best way to go.Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.Employing a PCFG-based generative framework to make both syntactic and morphological disambiguation decisions is not only theoretically clean and linguistically justified and but also probabilistically apropriate and empirically sound.Better grammars are shown here to improve performance on both morphological and syntactic tasks  providing support for the advantage of a joint framework over pipelined or factorized ones.Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty  2006) and (Cohen and Smith  2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2Using a treebank grammar  a data-driven lexicon  and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined  integrated or factorized systems for Hebrew morphological and syntactic processing  yielding an error reduction of 12% over the best published results so far.In contrast  our morphological probabilities are based on a unigram  lexeme-based model  and all other (local and non-local) contextual considerations are delegated to the PCFG.The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth  ‘tokens’) that constitute the unanalyzed surface forms (utterances).This leads to word- and constituent-boundaries discrepancy  which breaks the assumptions underlying current state-of-the-art statistical parsers.In Semitic languages the situation is very different.A similar structure is used in speech recognition.This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.This is done using a simple PCFG which is lexemebased.However  there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context.In Modern Hebrew (Hebrew)  a Semitic language with very rich morphology  particles marking conjunctions  prepositions  complementizers and relativizers are bound elements prefixed to the word (Glinert  1989).Tsarfaty (2006) argues that for Semitic languages determining the correct morphological segmentation is dependent on syntactic context and shows that increasing information sharing between the morphological and the syntactic components leads to improved performance on the joint task.The Hebrew token ‘bcl’1  for example  stands for the complete prepositional phrase 'We adopt here the transliteration of (Sima’an et al.  2001).A less canonical representation of segmental morphology is triggered by a morpho-phonological process of omitting the definite article h when occurring after the particles b or l. This process triggers ambiguity as for the definiteness status of Nouns following these particles.We refer to such cases in which the concatenation of elements does not strictly correspond to the original surface form as super-segmental morphology.The form fmnh  for example  can be understood as the verb “lubricated”  the possessed noun “her oil”  the adjective “fat” or the verb “got fat”.When a token fmnh is to be interpreted as the lexeme sequence f/REL mnh/VB  the analysis introduces two distinct entities  the relativizer f (“that”) and the verb mnh (“counted”)  and not as the complex entity “that counted”.The aforementioned surface form bcl  for example  may also stand for the lexical item “onion”  a Noun.“in the shadow”.Oracle results).The latter arcs correspond to OOV words in English.The remaining arcs are marked OOV.This token may further embed into a larger utterance  e.g.  ‘bcl hneim’ (literally “in-the-shadow the-pleasant”  meaning roughly “in the pleasant shadow”) in which the dominated Noun is modified by a proceeding space-delimited adjective.Furthermore  some of the arcs represent lexemes not present in the input tokens (e.g. h/DT  fl/POS)  however these are parts of valid analyses of the token (cf. super-segmental morphology section 2).Such tag sequences are often treated as “complex tags” (e.g.Secondly  some segments in a proposed segment sequence may in fact be seen lexical events  i.e.  for some p tag Prf(p —* (s  p)) > 0  while other segments have never been observed as a lexical event before.It should be clear from the onset that the particle b (“in”) in ‘bcl’ may then attach higher than the bare noun cl (“shadow”).The same form fmnh can be segmented as f-mnh  f (“that”) functioning as a reletivizer with the form mnh.For brevity we omit the segments from the analysis  and so analysis of the form “fmnh” as f/REL mnh/VB is represented simply as REL VB.The form mnh itself can be read as at least three different verbs (“counted”  “appointed”  “was appointed”)  a noun (“a portion”)  and a possessed noun (“her kind”).The relativizer f(“that”) for example  may attach to an arbitrarily long relative clause that goes beyond token boundaries.One way to approach this discrepancy is to assume a preceding phase of morphological segmentation for extracting the different lexical items that exist at the token level (as is done  to the best of our knowledge  in all parsing related work on Arabic and its dialects (Chiang et al.  2006)).The current work treats both segmental and super-segmental phenomena  yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg  2008).For these models we limit the options provided for OOV words by not considering the entire token as a valid segmentation in case at least some prefix segmentation exists.To control for the effect of the HSPELL-based pruning  we also experimented with a morphological analyzer that does not perform this pruning.In such cases we use the non-pruned lattice including all (possibly ungrammatical) segmentation  and let the statistics (including OOV) decide.The additional morphological material in such cases appears after the stem and realizes the extended meaning.The same argument holds for resolving PP attachment of a prefixed preposition or marking conjunction of elements of any kind.An additional case of super-segmental morphology is the case of Pronominal Clitics.