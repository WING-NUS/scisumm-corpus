We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.This corresponds to an error reduction of 13% over the best previously published single parser results on this test set  those of Collins [9].The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.As noted above  the main methodological innovation presented here is our &quot;maximumentropy-inspired&quot; model for conditioning and smoothing.The model labeled &quot;Old&quot; attempts to recreate the Char97 system using the current program.First  it uses a clustering scheme on words to give the system a &quot;soft&quot; clustering of heads and sub-heads.This is as opposed to the &quot;Markovgrammar&quot; approach used in the current parser.We also present some partial results showing the effects of different conditioning information  including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.In Figure 2 we show that this one factor improves performance by nearly 2%.When we do so using our maximum-entropy-inspired conditioning  we get another 0.45% improvement in average precision/recall  as indicated in Figure 2 on the line labeled &quot;MaxEnt-Inspired'.In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.We use the gathered statistics for all observed words  even those with very low counts  though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.Following [5 10]  our parser is based upon a probabilistic generative model.As already noted  our best model uses a Markov-grammar approach.We created a parser based upon the maximumentropy-inspired model of the last section  smoothed using standard deleted interpolation.That is  for all sentences s and all parses 7r  the parser assigns a probability p(s   7r) = p(r)  the equality holding when we restrict consideration to 7r whose yield * This research was supported in part by NSF grant LIS SBR 9720368.The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c  t(c) (t for &quot;tag&quot;)  then the lexical head of c  h(c)  and then the expansion of c into further constituents e(c).That is  the parser implements the function arg maxrp(7r s) = arg maxirp(7r  s) = arg maxrp(w).For example  in computing the probability of the head's pre-terminal t we might want a feature schema f (t  1) that returns 1 if the observed pre-terminal of c = t and the label of c = 1  and zero otherwise.For example  when we first guess the lexical head we can move from computing p(r I 1 1p  h) to p(r I /  t  /p  h).The author would like to thank Mark Johnson and all the rest of the Brown Laboratory for Linguistic Information Processing. is s. Then for any s the parser returns the parse ir that maximizes this probability.We expect that as we experiment with other  more semantic conditioning information  the importance of this aspect of the model will increase.Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question.In the simple (content-free) form (Equation 6)  it is clear that Z(H) = 1.The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).In max-entropy models one can simply include features for all three events f1 (a  b  c)  f2 (a  b)  and f3(a  c) and combine them in the model according to Equation 3  or equivalently  Equation 4.What fundamentally distinguishes probabilistic generative parsers is how they compute p(r)  and it is to that topic we turn next.That is  suppose we want to compute a conditional probability p(a b  c)  but we are not sure that we have enough examples of the conditioning event b  c in the training corpus to ensure that the empirically obtained probability P (a I b  c) is accurate.The major advantage of using Equation 7 is that one can generally get away without computing the partition function Z(H).Ultimately it is this flexibility that let us try the various conditioning events  to move on to a Markov grammar approach  and to try several Markov grammars of different orders  without significant programming.The method that gives the best results  however  uses a Markov grammar — a method for assigning probabilities to any possible expansion using statistics gathered from the training corpus [6 10 15].As already noted  Char97 first guesses the lexical head of a constituent and then  given the head  guesses the PCFG rule used to expand the constituent in question.For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).In this notation the above equation takes the following form: Next we describe how we assign a probability to the expansion e of a constituent.The function Z(H)  called the partition function  is a normalizing constant (for fixed H)  so the probabilities over all a sum to one.Much of the interesting work is determining what goes into H (c).We make one more point on the connection of Equation 7 to a maximum entropy formulation.But let us look at how it works for a particular case in our parsing scheme.Something very much like this is done in [15].We have already noted the importance of conditioning on the parent label /p.Whenever it is clear to which constituent we are referring we omit the (c) in  e.g.  h(c).For example  it does not seem to make much sense to condition on  say  hp without first conditioning on ti .Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.It may not be obvious why this should make so great a difference  since most words are effectively unambiguous.We can now rewrite this in the form of Equation 5 as follows: Here we have sequentially conditioned on steadily increasing portions of c's history.Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.Now for our purposes it is useful to rewrite this as a sequence of multiplicative functions gi(a  H) for 0 < i < j: Here go(a  H) = 11Z (H) and gi(a  H) = eAi(a H) fi(° 11).