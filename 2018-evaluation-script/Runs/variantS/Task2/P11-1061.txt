We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language.Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).(2009) study related but different multilingual grammar and tagger induction tasks  where it is assumed that no labeled data at all is available.Unsupervised learning approaches appear to be a natural solution to this problem  as they require only unannotated text for training models.Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed)  making it applicable to a wide array of resource-poor languages.Unfortunately  the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al.  2010)  making its practical usability questionable at best.Because many foreign word types are not aligned to an English word (see Table 3)  and we do not run label propagation on the foreign side  we expect the projected information to have less coverage.Because the information flow in our graph is asymmetric (from English to the foreign language)  we use different types of vertices for each language.Of course  we are primarily interested in applying our techniques to languages for which no labeled resources are available.They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.The POS distributions over the foreign trigram types are used as features to learn a better unsupervised POS tagger (§5).To initialize the graph for label propagation we use a supervised English tagger to label the English side of the bitext.7 We then simply count the individual labels of the English tokens and normalize the counts to produce tag distributions over English word types.Across eight European languages  our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline  and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%)  and considerably bridges the gap to fully supervised POS tagging performance (96.6%).Overall  it gives improvements ranging from 1.1% for German to 14.7% for Italian  for an average improvement of 8.3% over the unsupervised feature-HMM model.For seven out of eight languages a threshold of 0.2 gave the best results for our final model  which indicates that for languages without any validation set  r = 0.2 can be used.For comparison  the completely unsupervised feature-HMM baseline accuracy on the universal POS tags for English is 79.4%  and goes up to 88.7% with a treebank dictionary.Supervised learning approaches have advanced the state-of-the-art on a variety of tasks in natural language processing  resulting in highly accurate systems.We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.First  we use a novel graph-based framework for projecting syntactic information across language boundaries.To make the projection practical  we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).Supervised part-of-speech (POS) taggers  for example  approach the level of inter-annotator agreement (Shen et al.  2007  97.3% accuracy for English).Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).Because there might be some controversy about the exact definitions of such universals  this set of coarse-grained POS categories is defined operationally  by collapsing language (or treebank) specific distinctions to a set of categories that exists across all languages.However  supervised methods rely on labeled training data  which is time-consuming and expensive to generate.Graph construction does not require any labeled data  but makes use of two similarity functions.To define a similarity function between the English and the foreign vertices  we rely on high-confidence word alignments.Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.Since our graph is built from a parallel corpus  we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g.  when the trigram type and the feature instantiation don’t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall  and we therefore extract only intersected high-confidence (> 0.9) alignments De�f.To bridge this gap  we consider a practically motivated scenario  in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest  but that we have access to parallel data with a resource-rich language.To this end  we construct a bilingual graph over word types to establish a connection between the two languages (§3)  and then use graph label propagation to project syntactic information from English to the foreign language (§4).As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.This scenario is applicable to a large set of languages and has been considered by a number of authors in the past (Alshawi et al.  2000; Xi and Hwa  2005; Ganchev et al.  2009).Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.We adopted this state-of-the-art model because it makes it easy to experiment with various ways of incorporating our novel constraint feature into the log-linear emission model.Naseem et al. (2009) and Snyder et al.More recently  Subramanya et al. (2010) defined a graph over the cliques in an underlying structured prediction model.For English POS tagging  BergKirkpatrick et al. (2010) found that this direct gradient method performed better (>7% absolute accuracy) than using a feature-enhanced modification of the Expectation-Maximization (EM) algorithm (Dempster et al.  1977).8 Moreover  this route of optimization outperformed a vanilla HMM trained with EM by 12%.This can be seen as a rough approximation of Yarowsky and Ngai (2001).The focus of this work is on building POS taggers for foreign languages  assuming that we have an English POS tagger and some parallel text between the two languages.Our work is closest to that of Yarowsky and Ngai (2001)  but differs in two important ways.We extend Subramanya et al.’s intuitions to our bilingual setup.In our experiments  we used the same set of features as BergKirkpatrick et al. (2010): an indicator feature based In a traditional Markov model  the emission distribution PΘ(Xi = xi  |Zi = zi) is a set of multinomials.We develop our POS induction model based on the feature-based HMM of Berg-Kirkpatrick et al. (2010).To establish a soft correspondence between the two languages  we use a second similarity function  which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data  our goal is to project syntactic information from the English side to the foreign side.Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.Given the bilingual graph described in the previous section  we can use label propagation to project the English POS labels to the foreign language.Second  we treat the projected labels as features in an unsupervised model (§5)  rather than using them directly for supervised training.This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.We use label propagation in two stages to generate soft labels on all the vertices in the graph.This is similar to stacking the different feature instantiations into long (sparse) vectors and computing the cosine similarity between them.We use the universal POS tagset of Petrov et al. (2011) in our experiments.10 This set C consists of the following 12 coarse-grained tags: NOUN (nouns)  VERB (verbs)  ADJ (adjectives)  ADV (adverbs)  PRON (pronouns)  DET (determiners)  ADP (prepositions or postpositions)  NUM (numerals)  CONJ (conjunctions)  PRT (particles)  PUNC (punctuation marks) and X (a catch-all for other categories such as abbreviations or foreign words).