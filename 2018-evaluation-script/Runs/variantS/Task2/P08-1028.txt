This paper proposes a framework for representing the meaning of phrases and sentences in vector space.In this paper we presented a general framework for vector-based semantic composition.Vector addition is by far the most common method for representing the meaning of linguistic sequences.In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions.We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.We present a general framework for vector-based composition which allows us to consider different classes of models.Although we have presented multiplicative and additive models separately  there is nothing inherent in our formulation that disallows their combination.Specifically  we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.Under this framework  we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.Under this framework  we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.Our results show that the multiplicative models are superior and correlate significantly with behavioral data.The multiplicative and combined models yield means closer to the human ratings.The simple additive model fails to distinguish between High and Low Similarity items.Vector-based models of word meaning (Lund and Burgess  1996; Landauer and Dumais  1997) have become increasingly popular in natural language processing (NLP) and cognitive science.In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess  1996; Landauer and Dumais  1997) and text comprehension (Landauer and Dumais  1997; Foltz et al.  1998).Computational models of semantics which use symbolic logic representations (Montague  1974) can account naturally for the meaning of phrases or sentences.While vector addition has been effective in some applications such as essay grading (Landauer and Dumais  1997) and coherence assessment (Foltz et al.  1998)  there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al.  1991; West and Stanovich  1986) and modulate cognitive behavior in sentence priming (Till et al.  1988) and inference tasks (Heit and Rubinstein  1994).Examples include automatic thesaurus extraction (Grefenstette  1994)  word sense discrimination (Sch¨utze  1998) and disambiguation (McCarthy et al.  2004)  collocation extraction (Schone and Jurafsky  2001)  text segmentation (Choi et al.  2001)   and notably information retrieval (Salton et al.  1975).The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris  1968).Central in these models is the notion of compositionality — the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.Model Parameters Irrespectively of their form  all composition models discussed here are based on a semantic space for representing the meanings of individual words.While neural networks can readily represent single distinct objects  in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.A variety of NLP tasks have made good use of vector-based models.The applications of the framework discussed here are many and varied both for cognitive science and NLP.This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.Previous applications of vector addition to document indexing (Deerwester et al.  1990) or essay grading (Landauer et al.  1997) were more concerned with modeling the gist of a document rather than the meaning of its sentences.In addition  Bullinaria and Levy (2007) found that these parameters perform well on a number of other tasks such as the synonymy task from the Test ofEnglish as a Foreign Language (TOEFL).Moreover  the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald  2000) and word association norms (Denhire and Lemaire  2004).The task involves examining the degree of linear relationship between the human judgments for two individual words and vector-based similarity values.We expect better models to yield a pattern of similarity scores like those observed in the human ratings (see Figure 2).Despite the popularity of additive models  our experimental results showed the superiority of models utilizing multiplicative combinations  at least for the sentence similarity task attempted here.Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).Despite their widespread use  vector-based models are typically directed at representing words in isolation and methods for constructing representations for phrases or sentences have received little attention in the literature.In contrast  symbolic representations can naturally handle the binding of constituents to their roles  in a systematic manner that avoids both these problems.Focusing on a single compositional structure  namely intransitive verbs and their subjects  is a good point of departure for studying vector combination.In fact  the commonest method for combining the vectors is to average them.For additive models  a natural way to achieve this is to include further vectors into the summation.The compression is achieved by summing along the transdiagonal elements of the tensor product.Vector averaging is unfortunately insensitive to word order  and more generally syntactic structure  giving the same representation to any constructions that happen to share the same vocabulary.However  since it is order independent  it cannot capture meaning differences that are modulated by differences in syntactic structure.The idea here is that an appropriate composition model when applied to horse and ran will yield a vector closer to the landmark gallop than dissolve.Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.This is illustrated in the example below taken from Landauer et al. (1997).We observe a similar pattern for the non compositional baseline model  the weighted additive model and Kintsch (2001).The results of our correlation analysis are also given in Table 2.A hypothetical semantic space is illustrated in Figure 1.We evaluated the models presented in Section 3 on a sentence similarity task initially proposed by Kintsch (2001).Sentences (1-a) and (1-b) contain exactly the same set of words but their meaning is entirely different.Here  semantic analysis is guided by syntactic structure  and therefore sentences (1-a) and (1-b) receive distinct representations.Here  the space has only five dimensions  and the matrix cells denote the co-occurrence of the target words (horse and run) with the context words animal  stable  and so on.When modeling predicate-argument structures  Kintsch (2001) proposes including one or more distributional neighbors  n  of the predicate: Note that considerable latitude is allowed in selecting the appropriate neighbors.(1) a.First  the additive model in (7) weighs differentially the contribution of the two constituents.It was not the sales manager who hit the bottle that day  but the office worker with the serious drinking problem. b.That day the office manager  who was drinking  hit the problem sales worker with a bottle  but it was not serious.A more scrupulous evaluation requires directly correlating all the individual participants’ similarity judgments with those of the models.6 We used Spearman’s p for our correlation analyses.The opposite is the case for the reference The face glowed.