An easy way to achieve this is to put the domain-specific LMs and TMs into the top-level log-linear model and learn optimal weights with MERT (Och  2003).Log-linear combination (loglin) improves on this in all cases  and also beats the pure IN system.In future work we plan to try this approach with more competitive SMT systems  and to extend instance weighting to other standard SMT components such as the LM  lexical phrase weights  and lexicalized distortion.(Thus the domain of the dev and test corpora matches IN.)Feature weights were set using Och’s MERT algorithm (Och  2003).There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan  2007; Wu et al.  2005)  and on dynamically choosing a dev set (Xu et al.  2007).This is appropriate in cases where it is sanctioned by Bayes’ law  such as multiplying LM and TM probabilities  but for adaptation a more suitable framework is often a mixture model in which each event may be generated from some domain.For instance  the sentence Similar improvements in haemoglobin levels were reported in the scientific literature for other epoetins would likely be considered domain-specific despite the presence of general phrases like were reported in.We will also directly compare with a baseline similar to the Matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences.We have already mentioned the closely related work by Matsoukas et al (2009) on discriminative corpus weighting  and Jiang and Zhai (2007) on (nondiscriminative) instance weighting.Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).It is also worth pointing out a connection with Daum´e’s (2007) work that splits each feature into domain-specific and general copies.Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.It is difficult to directly compare the Matsoukas et al results with ours  since our out-of-domain corpus is homogeneous; given heterogeneous training data  however  it would be trivial to include Matsoukas-style identity features in our instance-weighting model.This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.This is not unreasonable given the application to phrase pairs from OUT  but it suggests that an interesting alternative might be to use a plain log-linear weighting function exp(Ei Aifi(s  t))  with outputs in [0  oo].The idea of distinguishing between general and domain-specific examples is due to Daum´e and Marcu (2006)  who used a maximum-entropy model with latent variables to capture the degree of specificity.Apart from MERT difficulties  a conceptual problem with log-linear combination is that it multiplies feature probabilities  essentially forcing different features to agree on high-scoring candidates.Phrase-level granularity distinguishes our work from previous work by Matsoukas et al (2009)  who weight sentences according to sub-corpus and genre membership.This is consistent with the nature of these two settings: log-linear combination  which effectively takes the intersection of IN and OUT  does relatively better on NIST  where the domains are broader and closer together.We focus here instead on adapting the two most important features: the language model (LM)  which estimates the probability p(wIh) of a target word w following an ngram h; and the translation models (TM) p(slt) and p(t1s)  which give the probability of source phrase s translating to target phrase t  and vice versa.The first setting uses the European Medicines Agency (EMEA) corpus (Tiedemann  2009) as IN  and the Europarl (EP) corpus (www.statmt.org/europarl) as OUT  for English/French translation in both directions.To set β  we used the same criterion as for α  over a dev corpus: The MAP combination was used for TM probabilities only  in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney  1995).3 Motivated by information retrieval  a number of approaches choose “relevant” sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al.  2005; L¨u et al.  2007)  or individual target hypotheses (Zhao et al.  2004).The original OUT counts co(s  t) are weighted by a logistic function wλ(s  t): To motivate weighting joint OUT counts as in (6)  we begin with the “ideal” objective for setting multinomial phrase probabilities 0 = {p(s|t)  dst}  which is the likelihood with respect to the true IN distribution pi(s  t).Dropping the conditioning on 0 for brevity  and letting ¯cλ(s  t) = cλ(s  t) + yu(s|t)  and ¯cλ(t) = 4Note that the probabilities in (7) need only be evaluated over the support of ˜p(s  t)  which is quite small when this distribution is derived from a dev set.We extend the Matsoukas et al approach in several ways.Section 2 describes our baseline techniques for SMT adaptation  and section 3 describes the instance-weighting approach.A similar maximumlikelihood approach was used by Foster and Kuhn (2007)  but for language models only.For the LM  adaptive weights are set as follows: where α is a weight vector containing an element αi for each domain (just IN and OUT in our case)  pi are the corresponding domain-specific models  and ˜p(w  h) is an empirical distribution from a targetlanguage training corpus—we used the IN dev set for this.Linear weights are difficult to incorporate into the standard MERT procedure because they are “hidden” within a top-level probability that represents the linear combination.1 Following previous work (Foster and Kuhn  2007)  we circumvent this problem by choosing weights to optimize corpus loglikelihood  which is roughly speaking the training criterion used by the LM and TM themselves.We used it to score all phrase pairs in the OUT table  in order to provide a feature for the instance-weighting model.It is not immediately obvious how to formulate an equivalent to equation (1) for an adapted TM  because there is no well-defined objective for learning TMs from parallel corpora.For comparison to information-retrieval inspired baselines  eg (L¨u et al.  2007)  we select sentences from OUT using language model perplexities from IN.To approximate these baselines  we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.Maximizing (7) is thus much faster than a typical MERT run. where co(s  t) are the counts from OUT  as in (6).Our second contribution is to apply instance weighting at the level of phrase pairs.We used a standard one-pass phrase-based system (Koehn et al.  2003)  with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count.We obtained positive results using a very simple phrase-based system in two different adaptation settings: using English/French Europarl to improve a performance on a small  specialized medical domain; and using non-news portions of the NIST09 training material to improve performance on the news-related corpora.As mentioned above  it is not obvious how to apply Daum´e’s approach to multinomials  which do not have a mechanism for combining split features.This is a straightforward technique that is arguably better suited to the adaptation task than the standard method of treating representative IN sentences as queries  then pooling the match results.A final alternate approach would be to combine weighted joint frequencies rather than conditional estimates  ie: cI(s  t) + w \(s  t)co(  s  t)  suitably normalized.5 Such an approach could be simulated by a MAP-style combination in which separate 0(t) values were maintained for each t. This would make the model more powerful  but at the cost of having to learn to downweight OUT separately for each t  which we suspect would require more training data for reliable performance.Moving beyond directly related work  major themes in SMT adaptation include the IR (Hildebrand et al.  2005; L¨u et al.  2007; Zhao et al.  2004) and mixture (Finch and Sumita  2008; Foster and Kuhn  2007; Koehn and Schroeder  2007; L¨u et al.  2007) approaches for LMs and TMs described above  as well as methods for exploiting monolingual in-domain text  typically by translating it automatically and then performing self training (Bertoldi and Federico  2009; Ueffing et al.  2007; Schwenk and Senellart  2009).