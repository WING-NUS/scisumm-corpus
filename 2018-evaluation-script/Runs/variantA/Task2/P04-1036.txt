The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.From manual analysis  there are cases where the acquired first sense disagrees with SemCor  yet is intuitively plausible.Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al.  2001).It is not always intuitively clear which of the senses to expect as predominant sense for either a particular domain or for the BNC  but the first senses of words like division and goal shift towards the more specific senses (league and score respectively).A major benefit of our work  rather than reliance on hand-tagged training data such as SemCor  is that this method permits us to produce predominant senses for the domain and text type required.Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus  whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful  there is a strong case for obtaining a first  or predominant  sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.This is shown by the results of the English all-words task in SENSEVAL-2 (Cotton et al.  1998) in figure 1 below  where the first sense is that listed in WordNet for the PoS given by the Penn TreeBank (Palmer et al.  2001).It uses the glosses of semantically related (according to WordNet) senses too. jcn (Jiang and Conrath  1997) This score uses corpus data to populate classes (synsets) in the WordNet hierarchy with frequency counts.It would be useful however to combine our method of finding predominant senses with one which can automatically find new senses within text and relate these to WordNet synsets  as Ciaramita and Johnson (2003) do with unknown nouns.We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.There has been some related work on using automatic thesauruses for discovering word senses from corpora Pantel and Lin (2002).The figure distinguishes systems which make use of hand-tagged data (using HTD) such as SemCor  from those that do not (without HTD).Even given the difference in text type between SemCor and the BNC the results are encouraging  especially given that our results are for polysemous nouns.We are currently investigating the performance of the first sense heuristic  and this method  for other PoS on SENSEVAL-3 data (McCarthy et al.  2004)  although not yet with rankings from domain specific corpora.We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.Thus we rank each sense using: parser (Briscoe and Carroll  2002).We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness withinAdditionally  we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a  2000) which annotates WordNet synsets with domain labels.The results for 10 of the words from the qualitative experiment are summarized in table 3 with the WordNet sense number for each word supplied alongside synonyms or hypernyms from WordNet for readability.Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense  and is much more efficient than lesk  given the precompilation of the IC files.For the latter  we only take a first-sense where there is more than one occurrence of the noun in the test data and one sense has occurred more times than any of the others.Magnini and Cavagli`a (2000) have identified WordNet word senses with particular domains  and this has proven useful for high precision WSD (Magnini et al.  2001); indeed in section 5 we used these domain labels for evaluation.This is just 5% lower than results using the first sense in the manually labelled SemCor  and we obtain 67% precision on polysemous nouns that are not in SemCor.The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found.4 We calculate the accuracy of finding the predominant sense  when there is indeed one sense with a higher frequency than the others for this word in SemCor ( ).We chose the domains of SPORTS and FINANCE since there is sufficient material for these domains in this publically available corpus.We also calculate the WSD accuracy that would be obtained on SemCor  when using our first sense in all contexts ( ).We compare results using the first sense listed in SemCor  and the first sense according to the SENSEVAL-2 English all-words test data itself.In many cases the sense ranking provided in SemCor differs to that obtained automatically because we used the BNC to produce our thesaurus.This is a very promising result given that our method does not require any hand-tagged text  such as SemCor.In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).Jiang and Conrath specify a distance measure:   where the third class ( ) is the most informative  or most specific  superordinate synset of the two senses and .Lapata and Brew (2004) have recently also highlighted the importance of a good prior in WSD.In order to evaluate our method we use the data in SemCor as a gold-standard.We use an allwords task because the predominant senses will reflect the sense distributions of all nouns within the documents  rather than a lexical sample task  where the target words are manually determined and the results will depend on the skew of the words in the sample.We use an automatically acquired thesaurus and a WordNet Similarity measure.We use the WordNet Similarity Package 0.05 and WordNet version 1.6.In particular  we will use balanced and domain specific corpora to isolate words having very different neighbours  and therefore rankings  in the different corpora and to detect and target words for which there is a highly skewed sense distribution in these corpora.We then use the WordNet similarity package (Patwardhan and Pedersen  2003) to give us a semantic similarity measure (hereafter referred to as the WordNet similarity measure) to weight the contribution that each neighbour makes to the various senses of the target word.We are of course able to apply the method to other versions of WordNet. synset  is incremented with the frequency counts from the corpus of all words belonging to that synset  directly or via the hyponymy relation.However  the most accurate WSD systems are those which require manually sense tagged data in the first place  and their accuracy depends on the quantity of training examples (Yarowsky and Florian  2002) available.Assuming that one had an accurate WSD system then one could obtain frequency counts for senses and rank them with these counts.Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.They used syntactic evidence to find a prior distribution for verb classes  based on (Levin  1993)  and incorporate this in a WSD system.This is to be expected regardless of any inherent shortcomings of the ranking technique since the senses within SemCor will differ compared to those of the BNC.