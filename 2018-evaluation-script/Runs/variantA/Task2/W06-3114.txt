We assumed that such a contrastive assessment would be beneficial for an evaluation that essentially pits different systems against each other.We are therefore applying a different method  which has been used at the 2005 DARPA/NIST evaluation.Still  for about good number of sentences  we do have this direct comparison  which allows us to apply the sign test  as described in Section 2.2.The out-of-domain test set differs from the Europarl data in various ways.The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.This is can not be the only explanation  since the discrepancy still holds  for instance  for out-of-domain French-English  where Systran receives among the best adequacy and fluency scores  but a worse BLEU score than all but one statistical system.However  ince we extracted the test corpus automatically from web sources  the reference translation was not always accurate — due to sentence alignment errors  or because translators did not adhere to a strict sentence-by-sentence translation (say  using pronouns when referring to entities mentioned in the previous sentence).The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).However  it is also mostly political content (even if not focused on the internal workings of the European Union) and opinion.This work was supported in part under the GALE program of the Defense Advanced Research Projects Agency  Contract No.Also  the argument has been made that machine translation performance should be evaluated via task-based evaluation metrics  i.e. how much it assists performing a useful task  such as supporting human translators or aiding the analysis of texts.In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.To summarize  we provided: The performance of the baseline system is similar to the best submissions in last year’s shared task.Hence  we use the bootstrap resampling method described by Koehn (2004).The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphological rich languages  as demonstrated by the results for English-German and English-French.We dropped  however  one of the languages  Finnish  partly to keep the number of tracks manageable  partly because we assumed that it would be hard to find enough Finnish speakers for the manual evaluation.Given a set of n sentences  we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d  x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric  we want to be able to rank different systems against each other  for which we need assessments of statistical significance on the differences between a pair of systems.We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).This is demonstrated by average scores over all systems  in terms of BLEU  fluency and adequacy  as displayed in Figure 5.However  a recent study (Callison-Burch et al.  2006)  pointed out that this correlation may not always be strong.The scores and confidence intervals are detailed first in the Figures 7–10 in table form (including ranks)  and then in graphical form in Figures 11–16.While the Bootstrap method is slightly more sensitive  it is very much in line with the sign test on text blocks.Confidence Interval: Since BLEU scores are not computed on the sentence level  traditional methods to compute statistical significance and confidence intervals do not apply.To lower the barrier of entrance to the competition  we provided a complete baseline MT system  along with data resources.We are currently working on a complete open source implementation of a training and decoding system  which should become available over the summer. pus  from which also the in-domain test set is taken.English was again paired with German  French  and Spanish.The bootstrap method has been critized by Riezler and Maxwell (2005) and Collins et al. (2005)  as being too optimistic in deciding for statistical significant difference between systems.All systems (except for Systran  which was not tuned to Europarl) did considerably worse on outof-domain training data.While automatic measures are an invaluable tool for the day-to-day development of machine translation systems  they are only a imperfect substitute for human assessment of translation quality  or as the acronym BLEU puts it  a bilingual evaluation understudy.We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:One may argue with these efforts on normalization  and ultimately their value should be assessed by assessing their impact on inter-annotator agreement.We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.Hence  the different averages of manual scores for the different language pairs reflect the behaviour of the judges  not the quality of the systems on different language pairs.Automatic scores are computed on a larger tested than manual scores (3064 sentences vs. 300–400 sentences). collected manual judgements  we do not necessarily have the same sentence judged for both systems (judges evaluate 5 systems out of the 8–10 participating systems).