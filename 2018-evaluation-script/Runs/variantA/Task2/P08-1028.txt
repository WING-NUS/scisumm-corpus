In addition  Bullinaria and Levy (2007) found that these parameters perform well on a number of other tasks such as the synonymy task from the Test ofEnglish as a Foreign Language (TOEFL).To give a concrete example  circular convolution is an instance of the general multiplicative model which breaks this constraint by allowing uj to contribute to pi: For example  according to (5)  the addition of the two vectors representing horse and run in Figure 1 would yield horse + run = [1 14 6 14 4].Combining the multiplicative model with an additive model  which does not suffer from this problem  could mitigate this problem: pi = αui +βvi +γuivi (11) where α  β  and γ are weighting constants.Moreover  the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald  2000) and word association norms (Denhire and Lemaire  2004).Relaxing the assumption of symmetry in the case of the simple additive model produces a model which weighs the contribution of the two components differently: This allows additive models to become more syntax aware  since semantically important constituents can participate more actively in the composition.Examples include automatic thesaurus extraction (Grefenstette  1994)  word sense discrimination (Sch¨utze  1998) and disambiguation (McCarthy et al.  2004)  collocation extraction (Schone and Jurafsky  2001)  text segmentation (Choi et al.  2001)   and notably information retrieval (Salton et al.  1975).In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess  1996; Landauer and Dumais  1997) and text comprehension (Landauer and Dumais  1997; Foltz et al.  1998).The neighbors  Kintsch argues  can ‘strengthen features of the predicate that are appropriate for the argument of the predication’. animal stable village gallop jokey horse 0 6 2 10 4 run 1 8 4 4 0 Unfortunately  comparisons across vector composition models have been few and far between in the literature.We intend to assess the potential of our composition models on context sensitive semantic priming (Till et al.  1988) and inductive inference (Heit and Rubinstein  1994).We employed leave-one-out resampling (Weiss and Kulikowski  1991)  by correlating the data obtained from each participant with the ratings obtained from all other participants.In his study  Kintsch builds a model of how a verb’s meaning is modified in the context of its subject.Procedure and Subjects Participants first saw a set of instructions that explained the sentence similarity task and provided several examples.Unfortunately  Kintsch (2001) demonstrates how his own composition algorithm works intuitively on a few hand selected examples but does not provide a comprehensive test set.The tensor product u ® v is a matrix whose components are all the possible products uivj of the components of vectors u and v. A major difficulty with tensor products is their dimensionality which is higher than the dimensionality of the original vectors (precisely  the tensor product has dimensionality m x n).The construction of the semantic space depends on the definition of linguistic context (e.g.  neighbouring words can be documents or collocations)  the number of components used (e.g.  the k most frequent words in a corpus)  and their values (e.g.  as raw co-occurrence frequencies or ratios of probabilities).We formulate semantic composition as a function of two vectors  u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word’s vector typically represents its co-occurrence with neighboring words.In our experiments we selected parameters that Kintsch reports as optimal.Kintsch (2001) proposes a variation on the vector addition theme in an attempt to model how the meaning of a predicate (e.g.  run) varies depending on the arguments it operates upon (e.g  the horse ran vs. the color ran).In particular  the general class of multiplicative models (see equation (4)) appears to be a fruitful area to explore.For example  assuming that individual words are represented by vectors  we can compute the meaning of a sentence by taking their mean (Foltz et al.  1998; Landauer and Dumais  1997).Despite their widespread use  vector-based models are typically directed at representing words in isolation and methods for constructing representations for phrases or sentences have received little attention in the literature.The models considered so far assume that components do not ‘interfere’ with each other  i.e.  that It is also possible to re-introduce the dependence on K into the model of vector composition.We anticipate that more substantial correlations can be achieved by implementing more sophisticated models from within the framework outlined here.While vector addition has been effective in some applications such as essay grading (Landauer and Dumais  1997) and coherence assessment (Foltz et al.  1998)  there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al.  1991; West and Stanovich  1986) and modulate cognitive behavior in sentence priming (Till et al.  1988) and inference tasks (Heit and Rubinstein  1994).More evidence that this is not an easy task comes from Figure 2 where we observe some overlap in the ratings for High and Low similarity items.This is illustrated in the example below taken from Landauer et al. (1997).To quantify this shift  Kintsch proposes measuring similarity relative to other verbs acting as landmarks  for example gallop and dissolve.We evaluated the models presented in Section 3 on a sentence similarity task initially proposed by Kintsch (2001).We define a general class of models for this process of composition as: The expression above allows us to derive models for which p is constructed in a distinct space from u and v  as is the case for tensor products.Here  the space has only five dimensions  and the matrix cells denote the co-occurrence of the target words (horse and run) with the context words animal  stable  and so on.To derive specific models from this general framework requires the identification of appropriate constraints to narrow the space of functions being considered.