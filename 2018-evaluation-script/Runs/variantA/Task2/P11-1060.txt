These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.Our employed (Zettlemoyer and Collins  2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language—think grounded al.  2010). compositional semantics.There has been a fair amount of past work on no predicates)  confusion of Washington state with this topic: Liang et al. (2010) induces combinatory Washington D.C.  learning the wrong lexical asso- logic programs in a non-linguistic setting.5 Discussion Piantadosi et al. (2008) induces first-order formuA major focus of this work is on our semantic rep- lae using CCG in a small domain assuming observed resentation  DCS  which offers a new perspective lexical semantics.CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.To further reduce the search space  F imposes a few additional constraints  e.g.  limiting the number of marked nodes to 2 and only allowing trace predicates between arity 1 predicates.In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.This yields a more system is based on a new semantic representation  factorized and flexible representation that is easier DCS  which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.Think of DCS as a higher-level Our system learns lexical associations between programming language tailored to natural language  words and predicates.Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.Table 2 shows that our system using lexical triggers L (henceforth  DCS) outperforms SEMRESP (78.9% over 73.2%).Note that having lexical triggers is a much weaker requirement than having a CCG lexicon  and far easier to obtain than logical forms.There are three types of predicates in P: generic (e.g.  argmax)  data (e.g.  city)  and value (e.g.  CA).Feedback from the context; for example  the lexical entry for borders world has been used to guide both syntactic parsing is S\NP/NP : Ay.Ax.border(x  y)  which means (Schuler  2003) and semantic parsing (Popescu et borders looks right for the first argument and left al.  2003; Clarke et al.  2010).Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms  we hope to use our without becoming mired in the semantic formalism. techniques in developing even more accurate and Quantifiers and superlatives significantly compli- broader-coverage language understanding systems. cate scoping in lambda calculus  and often type rais- Acknowledgments We thank Luke Zettlemoyer ing needs to be employed.Supervised semantic parsers (Zelle and Mooney  1996; Tang and Mooney  2001; Ge and Mooney  2005; Zettlemoyer and Collins  2005; Kate and Mooney  2007; Zettlemoyer and Collins  2007; Wong and Mooney  2007; Kwiatkowski et al.  2010) rely on manual annotation of logical forms  which is expensive.For example  in Figure 4(a)  before execution  the denotation of the DCS tree is hh{[(CA  OR)  (OR)] ... }; ø; (E  Qhstatei�w  ø)ii; after applying X1  we have hh{[(OR)]  ... }; øii.We then superlative ambiguity based on where the scopeapply d.ci to these two sets (technically  denota- determining execute relation is placed. tions) and project away the first column: Xi(d) = 3 Semantic Parsing ((d.ci ./1 1 A) ./2 1 B) [−1].Next  we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.We used the same training-test splits as Zettlemoyer and Collins (2005) (600+280 for GEO and 500+140 for JOBS).The dominant paradigm in compositional semantics is Montague semantics  which constructs lambda calculus forms in a bottom-up manner.Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure  which will facilitate parsing.Figure 2(a) shows an example of a DCS tree.We tested our system on two standard datasets  GEO and JOBS.Aggregate relation DCS trees that only use join relations can represent arbitrarily complex compositional structures  but they cannot capture higherorder phenomena in language.The key idea that allows us to give semanticallyscoped denotations to syntactically-scoped trees is as follows: We mark a node low in the tree with a mark relation (one of E  Q  or C).For each data predicate p (e.g.  language)  we add each possible tuple (e.g.  (job37  Java)) to w(p) independently with probability 0.8.After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy pθ(T)(y  |x  z ∈ ˜ZL θ(T)).To contrast  consider et al. (2010)  which we discussed earlier.Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.It is this transparency between syntax and semantics provided by DCS which leads to a simple and streamlined compositional semantics suitable for program induction.In addition  trees enable efficient computation  thereby establishing a new connection between dependency syntax and efficient semantic evaluation.Figlar  the restrictor is A = E (d.bi) and the nu- ure 4(g) shows that we can naturally account for clear scope is B = E (d[i  −(i  0)]).For example  for d in Figure 5  d[1] keeps column 1  d[−ø] keeps column 2  and d[2  −2] swaps the two columns.As another example  w(average) = {(S  ¯x) : We write a DCS tree z as hp; r1 : c1; ... ; rm : cmi.There are three cases: Extraction (d.ri = E) In the basic version  the denotation of a tree was always the set of consistent values of the root node.A world w is mapping from each predicate p ∈ P to a set of tuples; for example  w(state) = {(CA)  (OR) ... }.