First  in section 4  we evaluate the graph transformation techniques in themselves  with data from the Prague Dependency Treebank and the Danish Dependency Treebank.Experiments using data from the Prague Dependency Treebank show that the combined system can handle nonprojective constructions with a precision sufficient to yield a significant improvement in overall parsing accuracy.First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).More details on the memory-based prediction can be found in Nivre et al. (2004) and Nivre and Scholz (2004).In the experiments below  we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs 3 previously tested on Swedish (Nivre et al.  2004) and English (Nivre and Scholz  2004).The prediction based on these features is a knearest neighbor classification  using the IB1 algorithm and k = 5  the modified value difference metric (MVDM) and class voting with inverse distance weighting  as implemented in the TiMBL software package (Daelemans et al.  2003).In addition  there are several approaches to non-projective dependency parsing that are still to be evaluated in the large (Covington  1990; Kahane et al.  1998; Duchier and Debusmann  2001; Holan et al.  2001; Hellwig  2003).Prague Dependency Treebank (Hajiˇc et al.  2001b)  Danish Dependency Treebank (Kromann  2003)  and the METU Treebank of Turkish (Oflazer et al.  2003)  which generally allow annotations with nonprojective dependency structures.Although the best published results for the Collins parser is 80% UAS (Collins  1999)  this parser reaches 82% when trained on the entire training data set  and an adapted version of Charniak’s parser (Charniak  2000) performs at 84% (Jan Hajiˇc  pers. comm.).Using the terminology of Kahane et al. (1998)  we say that jedna is the syntactic head of Z  while je is its linear head in the projectivized representation.In this paper  we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.Instead  we want to apply an inverse transformation to recover the underlying (nonprojective) dependency graph.However  this argument is only plausible if the formal framework allows non-projective dependency structures  i.e. structures where a head and its dependents may correspond to a discontinuous constituent.Projectivizing a dependency graph by lifting nonprojective arcs is a nondeterministic operation in the general case.In section 4 we evaluate these transformations with respect to projectivized dependency treebanks  and in section 5 they are applied to parser output.We have presented a new method for non-projective dependency parsing  based on a combination of data-driven projective dependency parsing and graph transformation techniques.Memory-based classifiers for the experiments were created using TiMBL (Daelemans et al.  2003).Table 5 shows the overall parsing accuracy attained with the three different encoding schemes  compared to the baseline (no special arc labels) and to training directly on non-projective dependency graphs.The first thing to note is that projectivizing helps in itself  even if no encoding is used  as seen from the fact that the projective baseline outperforms the non-projective training condition by more than half a percentage point on attachment score  although the gain is much smaller with respect to exact match.The main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy  especially with respect to the exact match criterion  leading to the best reported performance for robust non-projective parsing of Czech.It is sometimes claimed that one of the advantages of dependency grammar over approaches based on constituency is that it allows a more adequate treatment of languages with variable word order  where discontinuous syntactic constructions are more common than in languages like English (Mel’ˇcuk  1988; Covington  1990).Finally  since non-projective constructions often involve long-distance dependencies  the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson  2002; Dienes and Dubey  2003; Jijkoun and de Rijke  2004; Cahill et al.  2004; Levy and Manning  2004; Campbell  2004).This is true of the widely used link grammar parser for English (Sleator and Temperley  1993)  which uses a dependency grammar of sorts  the probabilistic dependency parser of Eisner (1996)  and more recently proposed deterministic dependency parsers (Yamada and Matsumoto  2003; Nivre et al.  2004).There exist a few robust broad-coverage parsers that produce non-projective dependency structures  notably Tapanainen and J¨arvinen (1997) and Wang and Harper (2004) for English  Foth et al. (2004) for German  and Holan (2004) for Czech.Compared to related work on the recovery of long-distance dependencies in constituency-based parsing  our approach is similar to that of Dienes and Dubey (2003) in that the processing of non-local dependencies is partly integrated in the parsing process  via an extension of the set of syntactic categories  whereas most other approaches rely on postprocessing only.We call this pseudoprojective dependency parsing  since it is based on a notion of pseudo-projectivity (Kahane et al.  1998).By contrast  when we turn to a comparison of the three encoding schemes it is hard to find any significant differences  and the overall impression is that it makes little or no difference which encoding scheme is used  as long as there is some indication of which words are assigned their linear head instead of their syntactic head by the projective parser.This may seem surprising  given the experiments reported in section 4  but the explanation is probably that the non-projective dependencies that can be recovered at all are of the simple kind that only requires a single lift  where the encoding of path information is often redundant.From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.In order to realize the full potential of dependency-based syntactic parsing  it is desirable to allow non-projective dependency structures.The last four columns in Table 3 show the distribution of nonprojective arcs with respect to the number of lifts required.This leads to the best reported performance for robust non-projective parsing of Czech.The dependency graph in Figure 1 satisfies all the defining conditions above  but it fails to satisfy the condition ofprojectivity (Kahane et al.  1998): The arc connecting the head jedna (one) to the dependent Z (out-of) spans the token je (is)  which is not dominated by jedna.The most informative scheme  Head+Path  gives the highest scores  although with respect to Head the difference is not statistically significant  while the least informative scheme  Path – with almost the same performance on treebank transformation – is significantly lower (p < 0.01).The baseline simply retains the original labels for all arcs  regardless of whether they have been lifted or not  and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme  called Head  we use a new label d↑h for each lifted arc  where d is the dependency relation between the syntactic head and the dependent in the non-projective representation  and h is the dependency relation that the syntactic head has to its own head in the underlying structure.Here we use a slightly different notion of lift  applying to individual arcs and moving their head upwards one step at a time: Intuitively  lifting an arc makes the word wk dependent on the head wi of its original head wj (which is unique in a well-formed dependency graph)  unless wj is a root in which case the operation is undefined (but then wj —* wk is necessarily projective if the dependency graph is well-formed).The second experiment is limited to data from PDT.5 The training part of the treebank was projectivized under different encoding schemes and used to train memory-based dependency parsers  which were run on the test part of the treebank  consisting of 7 507 sentences and 125 713 tokens.6 The inverse transformation was applied to the output of the parsers and the result compared to the gold standard test set.However  while Dienes and Dubey recognize empty categories in a pre-processing step and only let the parser find their antecedents  we use the parser both to detect dislocated dependents and to predict either the type or the location of their syntactic head (or both) and use post-processing only to transform the graph in accordance with the parser’s analysis.The details of the transformation procedure are slightly different depending on the encoding schemes: d↑h let the linear head be the syntactic head). target arc must have the form wl −→ wm; if no target arc is found  Head is used as backoff. must have the form wl −→ wm and no outgoing arcs of the form wm p'↓ −→ wo; no backoff.However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).It is also true of the adaptation of the Collins parser for Czech (Collins et al.  1999) and the finite-state dependency parser for Turkish by Oflazer (2003).It is worth noting that  although nonprojective constructions are less frequent in DDT than in PDT  they seem to be more deeply nested  since only about 80% can be projectivized with a single lift  while almost 95% of the non-projective arcs in PDT only require a single lift.We also see that the increase in the size of the label sets for Head and Head+Path is far below the theoretical upper bounds given in Table 1.Except for the left3The graphs satisfy all the well-formedness conditions given in section 2 except (possibly) connectedness.We assume that the goal in dependency parsing is to construct a labeled dependency graph of the kind depicted in Figure 1.To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.In the second part of the experiment  we applied the inverse transformation based on breadth-first search under the three different encoding schemes.In approaching this problem  a variety of different methods are conceivable  including a more or less sophisticated use of machine learning.