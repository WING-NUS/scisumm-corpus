Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
2,A00-2018,N10-1002,0,"Charniak, 2000",0,"As a benchmark VPC extraction system, we use the Charniak parser (Charniak, 2000)","As a benchmark VPC extraction system, we use the Charniak parser (Charniak, 2000)","['134', '113', '14', '92', '165']","<S sid =""134"" ssid = ""25"">As already noted  Char97 first guesses the lexical head of a constituent and then  given the head  guesses the PCFG rule used to expand the constituent in question.</S><S sid =""113"" ssid = ""4"">We take as our starting point the parser labled Char97 in Figure 1 [5]  as that is the program from which our current parser derives.</S><S sid =""14"" ssid = ""3"">Much of the interesting work is determining what goes into H (c).</S><S sid =""92"" ssid = ""3"">For runs with the generative model based upon Markov grammar statistics  the first pass uses the same statistics  but conditioned only on standard PCFG information.</S><S sid =""165"" ssid = ""56"">Note that we also tried including this information using a standard deleted-interpolation model.</S>",['Method_Citation']
3,A00-2018,W11-0610,0,"Charniak, 2000",0,"Each of these scores can be calculated from a provided syntactic parse tree, and to generate these we made use of the Charniakparser (Charniak, 2000), also trained on the Switch board tree bank","Each of these scores can be calculated from a provided syntactic parse tree, and to generate these we made use of the Charniak parser (Charniak, 2000), also trained on the Switch board tree bank","['54', '79', '18', '172', '49']","<S sid =""54"" ssid = ""23"">The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).</S><S sid =""79"" ssid = ""48"">We comment on this because in our example we can substantially speed up the process by choosing values picked so that  when the maximum-entropy equation is expressed in the form of Equation 4  the gi have as their initial values the values of the corresponding terms in Equation 7.</S><S sid =""18"" ssid = ""7"">The method that gives the best results  however  uses a Markov grammar — a method for assigning probabilities to any possible expansion using statistics gathered from the training corpus [6 10 15].</S><S sid =""172"" ssid = ""63"">As one can see in Figure 2  a firstorder Markov grammar (with all the aforementioned improvements) performs slightly worse than the equivalent tree-bank-grammar parser.</S><S sid =""49"" ssid = ""18"">First  as already implicit in our discussion  factoring the probability computation into a sequence of values corresponding to various &quot;features&quot; suggests that the probability model should be easily changeable — just change the set of features used.</S>",['Method_Citation']
4,A00-2018,W06-3119,0,"Charniak, 2000",0,"We then use Charniak? s parser (Charniak, 2000) to generate the most likely parse tree for each English target sentence in the training corpus","We then use Charniak's parser (Charniak, 2000) to generate the most likely parse tree for each English target sentence in the training corpus","['95', '172', '113', '134', '161']","<S sid =""95"" ssid = ""6"">We guess the preterminals of words that are not observed in the training data using statistics on capitalization  hyphenation  word endings (the last two letters)  and the probability that a given pre-terminal is realized using a previously unobserved word.</S><S sid =""172"" ssid = ""63"">As one can see in Figure 2  a firstorder Markov grammar (with all the aforementioned improvements) performs slightly worse than the equivalent tree-bank-grammar parser.</S><S sid =""113"" ssid = ""4"">We take as our starting point the parser labled Char97 in Figure 1 [5]  as that is the program from which our current parser derives.</S><S sid =""134"" ssid = ""25"">As already noted  Char97 first guesses the lexical head of a constituent and then  given the head  guesses the PCFG rule used to expand the constituent in question.</S><S sid =""161"" ssid = ""52"">We believe that this is mostly due to improvements in guessing the sub-constituent's pre-terminal and head.</S>",['Method_Citation']
5,A00-2018,N03-2024,0,"Charniak, 2000",0,"We were interested in the occurrence of features such as type and number of premodifiers, presence and type of post modifiers, and form of name reference for people. We constructed a large, automatically annotated corpus by merging the output of Charniak? s statistical parser (Charniak, 2000) with that of the IBM named entity recognition system Nominator (Wacholder et al,1997)","We were interested in the occurrence of features such as type and number of premodifiers, presence and type of post modifiers, and form of name reference for people. We constructed a large, automatically annotated corpus by merging the output of Charniak's statistical parser (Charniak, 2000) with that of the IBM named entity recognition system Nominator (Wacholder et al,1997)","['26', '12', '13', '63', '44']","<S sid =""26"" ssid = ""15"">Thus an expansion e(c) looks like: The expansion is generated by guessing first M  then in order L1 through L „.+1 (= A)  and similarly for RI through In a pure Markov PCFG we are given the left-hand side label 1 and then probabilistically generate the right-hand side conditioning on no information other than 1 and (possibly) previously generated pieces of the right-hand side itself.</S><S sid =""12"" ssid = ""1"">The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c  t(c) (t for &quot;tag&quot;)  then the lexical head of c  h(c)  and then the expansion of c into further constituents e(c).</S><S sid =""13"" ssid = ""2"">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question.</S><S sid =""63"" ssid = ""32"">As we discuss in more detail in Section 5  several different features in the context surrounding c are useful to include in H: the label  head pre-terminal and head of the parent of c (denoted as lp  tp  hp)  the label of c's left sibling (lb for &quot;before&quot;)  and the label of the grandparent of c (la).</S><S sid =""44"" ssid = ""13"">Now consider computing a conditional probability p(a H) with a set of features h that connect a to the history H. In a log-linear model the probability function takes the following form: Here the Ai are weights between negative and positive infinity that indicate the relative importance of a feature: the more relevant the feature to the value of the probability  the higher the absolute value of the associated A.</S>",['Method_Citation']
6,A00-2018,N06-1039,0,"Charniak, 2000",0,"After getting a set of basic clusters, we pass them to an existing statistical parser (Charniak, 2000) and rule-based tree normalizer to obtain a GLARFstructure for each sentence in every article","After getting a set of basic clusters, we pass them to an existing statistical parser (Charniak, 2000) and rule-based tree normalizer to obtain a GLARF structure for each sentence in every article","['172', '147', '39', '33', '149']","<S sid =""172"" ssid = ""63"">As one can see in Figure 2  a firstorder Markov grammar (with all the aforementioned improvements) performs slightly worse than the equivalent tree-bank-grammar parser.</S><S sid =""147"" ssid = ""38"">For example  when we first guess the lexical head we can move from computing p(r I 1 1p  h) to p(r I /  t  /p  h).</S><S sid =""39"" ssid = ""8"">In our work we assume that any feature can occur at most once  so features are boolean-valued: 0 if the pattern does not occur  1 if it does.</S><S sid =""33"" ssid = ""2"">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S><S sid =""149"" ssid = ""40"">For example  the tree-bank PCFG probability of the rule &quot;VP --+ vbg np&quot; is 0.0145  whereas once we condition on the fact that the lexical head is a vbg we get a probability of 0.214.</S>",['Method_Citation']
7,A00-2018,C04-1180,0,2000,0,"The levels of accuracy and robustness recently achieved by statistical parsers (e.g. Collins (1999), Charniak (2000)) have led to their use in a number of NLP applications, such as question-answering (Pasca and Harabagiu, 2001), machine translation (Charniak et al, 2003), sentence simplification (Carroll et al, 1999), and a linguist? s search engine (Resnik and Elkiss, 2003)","The levels of accuracy and robustness recently achieved by statistical parsers (e.g. Collins (1999), Charniak (2000)) have led to their use in a number of NLP applications, such as question-answering (Pasca and Harabagiu, 2001), machine translation (Charniak et al, 2003), sentence simplification (Carroll et al, 1999), and a linguist? s search engine (Resnik and Elkiss, 2003)","['26', '5', '63', '174', '13']","<S sid =""26"" ssid = ""15"">Thus an expansion e(c) looks like: The expansion is generated by guessing first M  then in order L1 through L „.+1 (= A)  and similarly for RI through In a pure Markov PCFG we are given the left-hand side label 1 and then probabilistically generate the right-hand side conditioning on no information other than 1 and (possibly) previously generated pieces of the right-hand side itself.</S><S sid =""5"" ssid = ""1"">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S><S sid =""63"" ssid = ""32"">As we discuss in more detail in Section 5  several different features in the context surrounding c are useful to include in H: the label  head pre-terminal and head of the parent of c (denoted as lp  tp  hp)  the label of c's left sibling (lb for &quot;before&quot;)  and the label of the grandparent of c (la).</S><S sid =""174"" ssid = ""1"">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S sid =""13"" ssid = ""2"">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question.</S>",['Method_Citation']
8,A00-2018,W05-0638,0,"Charniak, 2000",0,"In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000)","In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000)","['174', '101', '120', '128', '87']","<S sid =""174"" ssid = ""1"">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S sid =""101"" ssid = ""12"">In keeping with the standard methodology [5  9 10 15 17]  we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training  section 23 for testing  and section 24 for development (debugging and tuning).</S><S sid =""120"" ssid = ""11"">Second  Char97 uses unsupervised learning in that the original system was run on about thirty million words of unparsed text  the output was taken as &quot;correct&quot;  and statistics were collected on the resulting parses.</S><S sid =""128"" ssid = ""19"">The model labeled &quot;Old&quot; attempts to recreate the Char97 system using the current program.</S><S sid =""87"" ssid = ""56"">In a pure maximum-entropy model this is done by feature selection  as in Ratnaparkhi's maximum-entropy parser [17].</S>",['Method_Citation']
9,A00-2018,P05-1065,0,"Charniak, 2000",0,"We also use a standard statistical parser (Charniak, 2000) to provide syntactic analysis","We also use a standard statistical parser (Charniak, 2000) to provide syntactic analysis","['90', '87', '117', '169', '92']","<S sid =""90"" ssid = ""1"">We created a parser based upon the maximumentropy-inspired model of the last section  smoothed using standard deleted interpolation.</S><S sid =""87"" ssid = ""56"">In a pure maximum-entropy model this is done by feature selection  as in Ratnaparkhi's maximum-entropy parser [17].</S><S sid =""117"" ssid = ""8"">Also  the earlier parser uses two techniques not employed in the current parser.</S><S sid =""169"" ssid = ""60"">Up to this point all the models considered in this section are tree-bank grammar models.</S><S sid =""92"" ssid = ""3"">For runs with the generative model based upon Markov grammar statistics  the first pass uses the same statistics  but conditioned only on standard PCFG information.</S>",['Method_Citation']
10,A00-2018,P05-1065,0,"Charniak, 2000",0,"For each article, we calculated the per cent age of a) all word instances (tokens) and b) all unique words (types) not on these lists, resulting in three token OOV rate features and three type OOV rate features per article. The parse features are generated using the Charniak parser (Charniak, 2000) trained on the standard Wall Street Journal Treebank corpus","For each article, we calculated the percentage of a) all word instances (tokens) and b) all unique words (types) not on these lists, resulting in three token OOV rate features and three type OOV rate features per article. The parse features are generated using the Charniak parser (Charniak, 2000) trained on the standard Wall Street Journal Treebank corpus","['5', '26', '1', '12', '63']","<S sid =""5"" ssid = ""1"">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S><S sid =""26"" ssid = ""15"">Thus an expansion e(c) looks like: The expansion is generated by guessing first M  then in order L1 through L „.+1 (= A)  and similarly for RI through In a pure Markov PCFG we are given the left-hand side label 1 and then probabilistically generate the right-hand side conditioning on no information other than 1 and (possibly) previously generated pieces of the right-hand side itself.</S><S sid =""1"" ssid = ""1"">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid =""12"" ssid = ""1"">The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c  t(c) (t for &quot;tag&quot;)  then the lexical head of c  h(c)  and then the expansion of c into further constituents e(c).</S><S sid =""63"" ssid = ""32"">As we discuss in more detail in Section 5  several different features in the context surrounding c are useful to include in H: the label  head pre-terminal and head of the parent of c (denoted as lp  tp  hp)  the label of c's left sibling (lb for &quot;before&quot;)  and the label of the grandparent of c (la).</S>",['Results_Citation']
11,A00-2018,P04-1040,0,2000,0,"The evaluation of the transformed output of the parsers of Charniak (2000) and Collins (1999) gives 90 %unlabelled and 84 %labelled accuracy with respect to dependencies, when measured against a dependency corpus derived from the Penn Treebank.The paper is organized as follows","The evaluation of the transformed output of the parsers of Charniak (2000) and Collins (1999) gives 90 % unlabelled and 84 % labelled accuracy with respect to dependencies, when measured against a dependency corpus derived from the Penn Treebank. The paper is organized as follows","['174', '29', '96', '9', '175']","<S sid =""174"" ssid = ""1"">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S sid =""29"" ssid = ""18"">So  for example  in a second-order Markov PCFG  L2 would be conditioned on L1 and M. In our complete model  of course  the probability of each label in the expansions is also conditioned on other material as specified in Equation 1  e.g.  p(e t  h  H).</S><S sid =""96"" ssid = ""7"">As noted above  the probability model uses five smoothed probability distributions  one each for Li  M Ri t  and h. The equation for the (unsmoothed) conditional probability distribution for t is given in Equation 7.</S><S sid =""9"" ssid = ""5"">The author would like to thank Mark Johnson and all the rest of the Brown Laboratory for Linguistic Information Processing. is s. Then for any s the parser returns the parse ir that maximizes this probability.</S><S sid =""175"" ssid = ""2"">This corresponds to an error reduction of 13% over the best previously published single parser results on this test set  those of Collins [9].</S>",['Method_Citation']
12,A00-2018,P04-1040,0,2000,0,"Blaheta and Charniak (2000) presented the first method for assigning Pennfunctional tags to constituents identified by a parser. Pattern-matching approaches were used in (John son, 2002) and (Jijkoun, 2003) to recover non-local dependencies in phrase trees","Blaheta and Charniak (2000) presented the first method for assigning Penn functional tags to constituents identified by a parser. Pattern-matching approaches were used in (Johnson, 2002) and (Jijkoun, 2003) to recover non-local dependencies in phrase trees","['174', '29', '9', '129', '76']","<S sid =""174"" ssid = ""1"">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S sid =""29"" ssid = ""18"">So  for example  in a second-order Markov PCFG  L2 would be conditioned on L1 and M. In our complete model  of course  the probability of each label in the expansions is also conditioned on other material as specified in Equation 1  e.g.  p(e t  h  H).</S><S sid =""9"" ssid = ""5"">The author would like to thank Mark Johnson and all the rest of the Brown Laboratory for Linguistic Information Processing. is s. Then for any s the parser returns the parse ir that maximizes this probability.</S><S sid =""129"" ssid = ""20"">It makes no use of special maximum-entropyinspired features (though their presence made it much easier to perform these experiments)  it does not guess the pre-terminal before guessing the lexical head  and it uses a tree-bank grammar rather than a Markov grammar.</S><S sid =""76"" ssid = ""45"">This requires finding the appropriate Ais for Equation 3  which is accomplished using an algorithm such as iterative scaling [II] in which values for the Ai are initially &quot;guessed&quot; and then modified until they converge on stable values.</S>",['Method_Citation']
13,A00-2018,P04-1040,0,2000,0,"As an alternative to hard coded heuristics, Blaheta and Charniak (2000) proposed to recover the Penn functional tags automatically","As an alternative to hard coded heuristics, Blaheta and Charniak (2000) proposed to recover the Penn functional tags automatically","['92', '87', '108', '175', '85']","<S sid =""92"" ssid = ""3"">For runs with the generative model based upon Markov grammar statistics  the first pass uses the same statistics  but conditioned only on standard PCFG information.</S><S sid =""87"" ssid = ""56"">In a pure maximum-entropy model this is done by feature selection  as in Ratnaparkhi's maximum-entropy parser [17].</S><S sid =""108"" ssid = ""19"">As is typical  all of the standard measures tell pretty much the same story  with the new parser outperforming the other three parsers.</S><S sid =""175"" ssid = ""2"">This corresponds to an error reduction of 13% over the best previously published single parser results on this test set  those of Collins [9].</S><S sid =""85"" ssid = ""54"">As partition-function calculation is typically the major on-line computational problem for maximum-entropy models  this simplifies the model significantly.</S>",['Method_Citation']
17,A00-2018,N06-1022,0,2000,0,"The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents","The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents","['174', '180', '29', '13', '9']","<S sid =""174"" ssid = ""1"">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S sid =""180"" ssid = ""7"">From our perspective  perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head.</S><S sid =""29"" ssid = ""18"">So  for example  in a second-order Markov PCFG  L2 would be conditioned on L1 and M. In our complete model  of course  the probability of each label in the expansions is also conditioned on other material as specified in Equation 1  e.g.  p(e t  h  H).</S><S sid =""13"" ssid = ""2"">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question.</S><S sid =""9"" ssid = ""5"">The author would like to thank Mark Johnson and all the rest of the Brown Laboratory for Linguistic Information Processing. is s. Then for any s the parser returns the parse ir that maximizes this probability.</S>",['Method_Citation']
18,A00-2018,N06-1022,0,2000,0,"Most recently, McDonald et al (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000))","Most recently, McDonald et al (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000))","['174', '5', '29', '26', '176']","<S sid =""174"" ssid = ""1"">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S sid =""5"" ssid = ""1"">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S><S sid =""29"" ssid = ""18"">So  for example  in a second-order Markov PCFG  L2 would be conditioned on L1 and M. In our complete model  of course  the probability of each label in the expansions is also conditioned on other material as specified in Equation 1  e.g.  p(e t  h  H).</S><S sid =""26"" ssid = ""15"">Thus an expansion e(c) looks like: The expansion is generated by guessing first M  then in order L1 through L „.+1 (= A)  and similarly for RI through In a pure Markov PCFG we are given the left-hand side label 1 and then probabilistically generate the right-hand side conditioning on no information other than 1 and (possibly) previously generated pieces of the right-hand side itself.</S><S sid =""176"" ssid = ""3"">That the previous three best parsers on this test [5 9 17] all perform within a percentage point of each other  despite quite different basic mechanisms  led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training  and to conjecture that perhaps we were at it.</S>",['Method_Citation']
19,A00-2018,H05-1035,0,"Charniak, 2000",0,"The feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (Charniak, 2000), trees that will be improved by more accurate PP-attachment decisions","The feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (Charniak, 2000), trees that will be improved by more accurate PP-attachment decisions","['92', '87', '118', '76', '129']","<S sid =""92"" ssid = ""3"">For runs with the generative model based upon Markov grammar statistics  the first pass uses the same statistics  but conditioned only on standard PCFG information.</S><S sid =""87"" ssid = ""56"">In a pure maximum-entropy model this is done by feature selection  as in Ratnaparkhi's maximum-entropy parser [17].</S><S sid =""118"" ssid = ""9"">First  it uses a clustering scheme on words to give the system a &quot;soft&quot; clustering of heads and sub-heads.</S><S sid =""76"" ssid = ""45"">This requires finding the appropriate Ais for Equation 3  which is accomplished using an algorithm such as iterative scaling [II] in which values for the Ai are initially &quot;guessed&quot; and then modified until they converge on stable values.</S><S sid =""129"" ssid = ""20"">It makes no use of special maximum-entropyinspired features (though their presence made it much easier to perform these experiments)  it does not guess the pre-terminal before guessing the lexical head  and it uses a tree-bank grammar rather than a Markov grammar.</S>",['Method_Citation']
20,A00-2018,P04-1042,0,2000,0,"Note that the dependency figures of Dienes lag behind even the parsed results for Johnson? s model; this may well be due to the fact that Dienes built his model as an extension of Collins (1999), which lags behind Charniak (2000) by about 1.3-1.5% .Manual investigation of errors on English gold standard data revealed two major issues that suggest further potential for improvement in performance without further increase in algorithmic complexity or training set size","Note that the dependency figures of Dienes lag behind even the parsed results for Johnson's model; this may well be due to the fact that Dienes built his model as an extension of Collins (1999), which lags behind Charniak (2000) by about 1.3-1.5%. Manual investigation of errors on English gold standard data revealed two major issues that suggest further potential for improvement in performance without further increase in algorithmic complexity or training set size","['26', '5', '44', '63', '13']","<S sid =""26"" ssid = ""15"">Thus an expansion e(c) looks like: The expansion is generated by guessing first M  then in order L1 through L „.+1 (= A)  and similarly for RI through In a pure Markov PCFG we are given the left-hand side label 1 and then probabilistically generate the right-hand side conditioning on no information other than 1 and (possibly) previously generated pieces of the right-hand side itself.</S><S sid =""5"" ssid = ""1"">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S><S sid =""44"" ssid = ""13"">Now consider computing a conditional probability p(a H) with a set of features h that connect a to the history H. In a log-linear model the probability function takes the following form: Here the Ai are weights between negative and positive infinity that indicate the relative importance of a feature: the more relevant the feature to the value of the probability  the higher the absolute value of the associated A.</S><S sid =""63"" ssid = ""32"">As we discuss in more detail in Section 5  several different features in the context surrounding c are useful to include in H: the label  head pre-terminal and head of the parent of c (denoted as lp  tp  hp)  the label of c's left sibling (lb for &quot;before&quot;)  and the label of the grandparent of c (la).</S><S sid =""13"" ssid = ""2"">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question.</S>",['Method_Citation']
