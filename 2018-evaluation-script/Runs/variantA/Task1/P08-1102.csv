Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P08-1102,C08-1049,0,2008,0,"Following Jiang et al (2008), we describe segmentation and Joint S& amp; T as below: For a given Chinese sentence appearing as a character sequence: C 1: n= C 1 C 2.","Following Jiang et al (2008), we describe segmentation and Joint S& amp; T as below: For a given Chinese sentence appearing as a character sequence: C 1: n= C 1 C 2.","['45', '110', '16', '49', '139']","<S sid =""45"" ssid = ""17"">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S><S sid =""110"" ssid = ""21"">Similar trend appeared in experiments of Ng and Low (2004)  where they conducted experiments on CTB 3.0 and achieved Fmeasure 0.919 on Joint S&T  a ratio of 96% to the F-measure 0.952 on segmentation.</S><S sid =""16"" ssid = ""12"">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S><S sid =""49"" ssid = ""21"">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S><S sid =""139"" ssid = ""2"">The authors were supported by National Natural Science Foundation of China  Contracts 60736014 and 60573188  and 863 State Key Project No.</S>",['Method_Citation']
2,P08-1102,C08-1049,0,2008,0,"As described in Ng and Low (2004 )andJiang et al (2008), we use s indicating a single character word, while b, m and e indicating the be gin, middle and end of a word respectively","As described in Ng and Low (2004) and Jiang et al (2008), we use s indicating a single character word, while b, m and e indicating the be gin, middle and end of a word respectively","['31', '101', '26', '135', '110']","<S sid =""31"" ssid = ""3"">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S><S sid =""101"" ssid = ""12"">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S><S sid =""26"" ssid = ""22"">In order to perform POS tagging at the same time  we expand boundary tags to include POS information by attaching a POS to the tail of a boundary tag as a postfix following Ng and Low (2004).</S><S sid =""135"" ssid = ""6"">In addition  all knowledge sources we used in the core perceptron and the outside-layer linear model come from the training corpus  whereas many open knowledge sources (lexicon etc.) can be used to improve performance (Ng and Low  2004).</S><S sid =""110"" ssid = ""21"">Similar trend appeared in experiments of Ng and Low (2004)  where they conducted experiments on CTB 3.0 and achieved Fmeasure 0.919 on Joint S&T  a ratio of 96% to the F-measure 0.952 on segmentation.</S>",['Method_Citation']
3,P08-1102,C08-1049,0,2008,0,plates called lexical-target in the column below areintroduced by Jiang et al (2008),plates called lexical-target in the column below are introduced by Jiang et al (2008),"['38', '34', '43', '132', '82']","<S sid =""38"" ssid = ""10"">Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target.</S><S sid =""34"" ssid = ""6"">The feature templates we adopted are selected from those of Ng and Low (2004).</S><S sid =""43"" ssid = ""15"">Note that the templates of Ng and Low (2004) have already contained some lexical-target ones.</S><S sid =""132"" ssid = ""3"">This is a substitute method to use both local and non-local features  and it would be especially useful when the training corpus is very large.</S><S sid =""82"" ssid = ""7"">In addition  we add the score of the word count penalty as another feature to alleviate the tendency of LMs to favor shorter candidates.</S>",['Method_Citation']
4,P08-1102,P12-1110,0,2008,0,"For CTB-5, we refer to the split by Duan et al (2007) as CTB-5d, and to the split by Jiang et al (2008) as CTB-5j","For CTB-5, we refer to the split by Duan et al (2007) as CTB-5d, and to the split by Jiang et al (2008) as CTB-5j","['134', '10', '135', '43', '15']","<S sid =""134"" ssid = ""5"">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S><S sid =""10"" ssid = ""6"">Since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a boundary tag (Xue and Shen  2003)  Joint S&T can be conducted in a labelling fashion by expanding boundary tags to include POS information (Ng and Low  2004).</S><S sid =""135"" ssid = ""6"">In addition  all knowledge sources we used in the core perceptron and the outside-layer linear model come from the training corpus  whereas many open knowledge sources (lexicon etc.) can be used to improve performance (Ng and Low  2004).</S><S sid =""43"" ssid = ""15"">Note that the templates of Ng and Low (2004) have already contained some lexical-target ones.</S><S sid =""15"" ssid = ""11"">To cope with this problem  we propose a cascaded linear model inspired by the log-linear model (Och and Ney  2004) widely used in statistical machine translation to incorporate different kinds of knowledge sources.</S>",['Method_Citation']
5,P08-1102,D12-1126,0,2008,0,Jiang et al (2008) proposes a cascaded linear model for joint Chinese word segmentation and POS tagging,Jiang et al (2008) proposes a cascaded linear model for joint Chinese word segmentation and POS tagging,"['130', '3', '24', '1', '101']","<S sid =""130"" ssid = ""1"">We proposed a cascaded linear model for Chinese Joint S&T.</S><S sid =""3"" ssid = ""3"">Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.</S><S sid =""24"" ssid = ""20"">It is a better idea to perform segmentation and POS tagging jointly in a uniform framework.</S><S sid =""1"" ssid = ""1"">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S sid =""101"" ssid = ""12"">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S>",['Method_Citation']
6,P08-1102,C10-1135,0,2008,0,"We use the feature templates the same as Jiang et al, (2008) to extract features form E model","We use the feature templates the same as Jiang et al, (2008) to extract features form E model","['49', '86', '43', '34', '33']","<S sid =""49"" ssid = ""21"">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S><S sid =""86"" ssid = ""11"">Line 4 scans words of all possible lengths l (l = 1.. min(i  K)  where i points to the current considering character).</S><S sid =""43"" ssid = ""15"">Note that the templates of Ng and Low (2004) have already contained some lexical-target ones.</S><S sid =""34"" ssid = ""6"">The feature templates we adopted are selected from those of Ng and Low (2004).</S><S sid =""33"" ssid = ""5"">In following subsections  we describe the feature templates and the perceptron training algorithm.</S>",['Method_Citation']
8,P08-1102,P12-1025,0,"Jiangetal., 2008a",0,"approach, where basic processing units are characters which compose words (Jiangetal., 2008a)","basic processing units are characters which compose words (Jiangetal., 2008a)","['38', '43', '49', '34', '12']","<S sid =""38"" ssid = ""10"">Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target.</S><S sid =""43"" ssid = ""15"">Note that the templates of Ng and Low (2004) have already contained some lexical-target ones.</S><S sid =""49"" ssid = ""21"">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S><S sid =""34"" ssid = ""6"">The feature templates we adopted are selected from those of Ng and Low (2004).</S><S sid =""12"" ssid = ""8"">Besides the usual character-based features  additional features dependent on POS’s or words can also be employed to improve the performance.</S>",['Method_Citation']
9,P08-1102,C10-2096,0,2008b,0,"The solid lines show the 1-best result, which is wrong. Jiang et al (2008b) stress the problems in re ranking phase","The solid lines show the 1-best result, which is wrong. Jiang et al (2008b) stress the problems in re ranking phase","['88', '82', '100', '115', '3']","<S sid =""88"" ssid = ""13"">Line 8 considers each candidate result in N-best list at prior position of the current word.</S><S sid =""82"" ssid = ""7"">In addition  we add the score of the word count penalty as another feature to alleviate the tendency of LMs to favor shorter candidates.</S><S sid =""100"" ssid = ""11"">Test results listed in Table 2 shows that this model obtains higher accuracy than the best of SIGHAN Bakeoff 2 in three corpora (AS  CityU and MSR).</S><S sid =""115"" ssid = ""26"">To obtain their corresponding weights  we adapted the minimum-error-rate training algorithm (Och  2003) to train the outside-layer model.</S><S sid =""3"" ssid = ""3"">Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.</S>",['Method_Citation']
10,P08-1102,C10-2096,0,"Jiang et al, 2008a",0,"6.1.1 Baseline Forest-based System We first segment the Chinese sentences into the1-best segmentations using a state-of-the-art system (Jiang et al, 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results","We first segment the Chinese sentences into the 1-best segmentations using a state-of-the-art system (Jiang et al, 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results","['45', '81', '49', '66', '1']","<S sid =""45"" ssid = ""17"">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S><S sid =""81"" ssid = ""6"">When we derive a candidate result from a word-POS pair p and a candidate q at prior position of p  we calculate the scores of the word LM  the POS LM  the labelling probability and the generating probability  Algorithm 2 Decoding algorithm. as well as the score of the perceptron model.</S><S sid =""49"" ssid = ""21"">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S><S sid =""66"" ssid = ""17"">Besides the output of the perceptron  the outside-layer also receive the outputs of the word LM  the POS LM  the co-occurrence model and a word count penalty which is similar to the translation length penalty in SMT.</S><S sid =""1"" ssid = ""1"">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S>",['Results_Citation']
11,P08-1102,C10-2096,0,"Jiang et al, 2008a",0,"6.1.2 Lattice-forest SystemWe first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al, 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm","We first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al, 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm","['1', '49', '79', '115', '81']","<S sid =""1"" ssid = ""1"">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S sid =""49"" ssid = ""21"">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S><S sid =""79"" ssid = ""4"">By maintaining a stack of size N at each position i of the sequence  we can preserve the top N best candidate labelled results of subsequence C1:i during decoding.</S><S sid =""115"" ssid = ""26"">To obtain their corresponding weights  we adapted the minimum-error-rate training algorithm (Och  2003) to train the outside-layer model.</S><S sid =""81"" ssid = ""6"">When we derive a candidate result from a word-POS pair p and a candidate q at prior position of p  we calculate the scores of the word LM  the POS LM  the labelling probability and the generating probability  Algorithm 2 Decoding algorithm. as well as the score of the perceptron model.</S>",['Method_Citation']
12,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"However, when we repeat the work of (Jiang et al, 2008), which reports to achieve the state-of-art performance in the data-sets that we adopt, it has been found that some features (e.g., C0) are unnoticeably trained several times in their model (which are implicitly generated from different feature templates used in the paper)","However, when we repeat the work of (Jiang et al, 2008), which reports to achieve the state-of-art performance in the data-sets that we adopt, it has been found that some features (e.g., C0) are unnoticeably trained several times in their model (which are implicitly generated from different feature templates used in the paper)","['96', '25', '105', '55', '45']","<S sid =""96"" ssid = ""7"">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus  we randomly chosen 2  000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84  294 sentences)  then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S><S sid =""25"" ssid = ""21"">According to Ng and Low (2004)  the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S><S sid =""105"" ssid = ""16"">At the first step  we conducted a group of contrasting experiments on the core perceptron  the first concentrated on the segmentation regardless of the POS information and reported the F-measure on segmentation only  while the second performed Joint S&T using POS information and reported the F-measure both on segmentation and on Joint S&T.</S><S sid =""55"" ssid = ""6"">In addition  even though these higher grams were managed to be used  there still remains another problem: as the current predication relies on the results of prior ones  the decoding procedure has to resort to approximate inference by maintaining a list of N-best candidates at each predication position  which evokes a potential risk to depress the training.</S><S sid =""45"" ssid = ""17"">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S>",['Method_Citation']
13,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"Unicode/CP936 1.1M/55K 104K/13K 0.035 Table 3: Corpus statistics for the second SIGHAN Bakeoff appears twice, which is generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al, 2008), with n=0, generates [C0C0])","Table 3: Corpus statistics for the second SIGHAN Bakeoff appears twice, which is generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al, 2008), with n=0, generates [C0C0])","['22', '73', '25', '46', '45']","<S sid =""22"" ssid = ""18"">2 Segmentation and POS Tagging Given a Chinese character sequence: while the segmentation and POS tagging result can be depicted as: Here  Ci (i = L.n) denotes Chinese character  ti (i = L.m) denotes POS tag  and Cl:r (l < r) denotes character sequence ranges from Cl to Cr.</S><S sid =""73"" ssid = ""24"">For instance  if the word w appears N times in training corpus and is labelled as POS t for n times  the probability Pr(t|w) can be estimated by the formula below: The probability Pr(w|t) could be estimated through the same approach.</S><S sid =""25"" ssid = ""21"">According to Ng and Low (2004)  the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S><S sid =""46"" ssid = ""18"">Following Collins  we use a function GEN(x) generating all candidate results of an input x   a representation 4) mapping each training example (x  y) ∈ X × Y to a feature vector 4)(x  y) ∈ Rd  and a parameter vector α� ∈ Rd corresponding to the feature vector. d means the dimension of the vector space  it equals to the amount of features in the model.</S><S sid =""45"" ssid = ""17"">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S>",['Method_Citation']
14,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"As all the features adopted in (Jiang et al, 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be? n?, at least in principle","As all the features adopted in (Jiang et al, 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be n, at least in principle","['135', '35', '15', '73', '69']","<S sid =""135"" ssid = ""6"">In addition  all knowledge sources we used in the core perceptron and the outside-layer linear model come from the training corpus  whereas many open knowledge sources (lexicon etc.) can be used to improve performance (Ng and Low  2004).</S><S sid =""35"" ssid = ""7"">To compare with others conveniently  we excluded the ones forbidden by the close test regulation of SIGHAN  for example  Pu(C0)  indicating whether character C0 is a punctuation.</S><S sid =""15"" ssid = ""11"">To cope with this problem  we propose a cascaded linear model inspired by the log-linear model (Och and Ney  2004) widely used in statistical machine translation to incorporate different kinds of knowledge sources.</S><S sid =""73"" ssid = ""24"">For instance  if the word w appears N times in training corpus and is labelled as POS t for n times  the probability Pr(t|w) can be estimated by the formula below: The probability Pr(w|t) could be estimated through the same approach.</S><S sid =""69"" ssid = ""20"">Formally  an n-gram word LM approximates the probability of a word sequence W = w1:m with the following product: Notice that a bi-gram POS LM functions as the product of transition probabilities in HMM.</S>",['Method_Citation']
15,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"Inspired by (Jiang et al, 2008), we set the real d Although Table 5 has shown that the proposed all the value of C0 to be 2.0, the value of C-1C0anC0C1 to be 3.0, and the values of all other features to be 1.0 for the character-based discriminative-plus model","Inspired by (Jiang et al, 2008), we set the real d Although Table 5 has shown that the proposed all the value of C0 to be 2.0, the value of C-1C0anC0C1 to be 3.0, and the values of all other features to be 1.0 for the character-based discriminative-plus model","['25', '81', '45', '46', '28']","<S sid =""25"" ssid = ""21"">According to Ng and Low (2004)  the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S><S sid =""81"" ssid = ""6"">When we derive a candidate result from a word-POS pair p and a candidate q at prior position of p  we calculate the scores of the word LM  the POS LM  the labelling probability and the generating probability  Algorithm 2 Decoding algorithm. as well as the score of the perceptron model.</S><S sid =""45"" ssid = ""17"">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S><S sid =""46"" ssid = ""18"">Following Collins  we use a function GEN(x) generating all candidate results of an input x   a representation 4) mapping each training example (x  y) ∈ X × Y to a feature vector 4)(x  y) ∈ Rd  and a parameter vector α� ∈ Rd corresponding to the feature vector. d means the dimension of the vector space  it equals to the amount of features in the model.</S><S sid =""28"" ssid = ""24"">A subsequence of boundary-POS labelling result indicates a word with POS t only if the boundary tag sequence composed of its boundary part conforms to s or bm*e style  and all POS tags in its POS part equal to t. For example  a tag sequence b NN m NN e NN represents a threecharacter word with POS tag NN.</S>",['Method_Citation']
17,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"Last, (Jiang et al, 2008) 5 adds repeated features implicitly based on (Ng and Low, 2004)","Last, (Jiang et al, 2008) 5 adds repeated features implicitly based on (Ng and Low, 2004)","['43', '86', '101', '16', '141']","<S sid =""43"" ssid = ""15"">Note that the templates of Ng and Low (2004) have already contained some lexical-target ones.</S><S sid =""86"" ssid = ""11"">Line 4 scans words of all possible lengths l (l = 1.. min(i  K)  where i points to the current considering character).</S><S sid =""101"" ssid = ""12"">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S><S sid =""16"" ssid = ""12"">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S><S sid =""141"" ssid = ""4"">We would also like to Hwee-Tou Ng for sharing his code  and Yang Liu and Yun Huang for suggestions.</S>",['Method_Citation']
20,P08-1102,D12-1046,0,Jiang et al2008a,0,"Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a) ,perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b)","Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a), perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b)","['96', '25', '45', '110', '81']","<S sid =""96"" ssid = ""7"">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus  we randomly chosen 2  000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84  294 sentences)  then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S><S sid =""25"" ssid = ""21"">According to Ng and Low (2004)  the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S><S sid =""45"" ssid = ""17"">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S><S sid =""110"" ssid = ""21"">Similar trend appeared in experiments of Ng and Low (2004)  where they conducted experiments on CTB 3.0 and achieved Fmeasure 0.919 on Joint S&T  a ratio of 96% to the F-measure 0.952 on segmentation.</S><S sid =""81"" ssid = ""6"">When we derive a candidate result from a word-POS pair p and a candidate q at prior position of p  we calculate the scores of the word LM  the POS LM  the labelling probability and the generating probability  Algorithm 2 Decoding algorithm. as well as the score of the perceptron model.</S>",['Method_Citation']
