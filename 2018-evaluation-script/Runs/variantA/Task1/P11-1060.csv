Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P11-1060,D11-1039,0,2011,0,"Clarkeet al (2010) and Liang et al (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al (2011) presents work on unsupervised learning","Clarke et al (2010) and Liang et al (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al (2011) presents work on unsupervised learning","['165', '166', '156', '158', '18']","<S sid =""165"" ssid = ""50"">These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.</S><S sid =""166"" ssid = ""51"">Our employed (Zettlemoyer and Collins  2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language—think grounded al.  2010). compositional semantics.</S><S sid =""156"" ssid = ""41"">There has been a fair amount of past work on no predicates)  confusion of Washington state with this topic: Liang et al. (2010) induces combinatory Washington D.C.  learning the wrong lexical asso- logic programs in a non-linguistic setting.</S><S sid =""158"" ssid = ""43"">5 Discussion Piantadosi et al. (2008) induces first-order formuA major focus of this work is on our semantic rep- lae using CCG in a small domain assuming observed resentation  DCS  which offers a new perspective lexical semantics.</S><S sid =""18"" ssid = ""14"">CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).</S>",['Method_Citation']
2,P11-1060,P13-1092,0,2011,0,"In particular, Clarke et al (2010) and Liang et al (2011) proposed methods to learn from question answer pairs alone, which represents a significant advance","In particular, Clarke et al (2010) and Liang et al (2011) proposed methods to learn from question answer pairs alone, which represents a significant advance","['165', '156', '9', '166', '158']","<S sid =""165"" ssid = ""50"">These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.</S><S sid =""156"" ssid = ""41"">There has been a fair amount of past work on no predicates)  confusion of Washington state with this topic: Liang et al. (2010) induces combinatory Washington D.C.  learning the wrong lexical asso- logic programs in a non-linguistic setting.</S><S sid =""9"" ssid = ""5"">As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S><S sid =""166"" ssid = ""51"">Our employed (Zettlemoyer and Collins  2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language—think grounded al.  2010). compositional semantics.</S><S sid =""158"" ssid = ""43"">5 Discussion Piantadosi et al. (2008) induces first-order formuA major focus of this work is on our semantic rep- lae using CCG in a small domain assuming observed resentation  DCS  which offers a new perspective lexical semantics.</S>",['Method_Citation']
3,P11-1060,P13-1092,0,2011,0,"To handle syntax-semantics mismatch, GUSP introduces a novel dependency-based meaning representation 1Clarke et al (2010) and Liang et al (2011) used the annotated logical forms to compute answers for their experiments","To handle syntax-semantics mismatch, GUSP introduces a novel dependency-based meaning representation. Clarke et al (2010) and Liang et al (2011) used the annotated logical forms to compute answers for their experiments","['156', '99', '3', '166', '171']","<S sid =""156"" ssid = ""41"">There has been a fair amount of past work on no predicates)  confusion of Washington state with this topic: Liang et al. (2010) induces combinatory Washington D.C.  learning the wrong lexical asso- logic programs in a non-linguistic setting.</S><S sid =""99"" ssid = ""75"">To further reduce the search space  F imposes a few additional constraints  e.g.  limiting the number of marked nodes to 2 and only allowing trace predicates between arity 1 predicates.</S><S sid =""3"" ssid = ""3"">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid =""166"" ssid = ""51"">Our employed (Zettlemoyer and Collins  2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language—think grounded al.  2010). compositional semantics.</S><S sid =""171"" ssid = ""56"">This yields a more system is based on a new semantic representation  factorized and flexible representation that is easier DCS  which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.</S>",['Method_Citation']
4,P11-1060,P13-1092,0,2011,0,"More recently, Liang et al (2011 )proposedDCS for dependency-based compositional semantics, which represents a semantic parse as a tree with nodes representing database elements and operations, and edges representing relational joins","More recently, Liang et al (2011) proposed DCS for dependency-based compositional semantics, which represents a semantic parse as a tree with nodes representing database elements and operations, and edges representing relational joins","['166', '156', '158', '18', '149']","<S sid =""166"" ssid = ""51"">Our employed (Zettlemoyer and Collins  2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language—think grounded al.  2010). compositional semantics.</S><S sid =""156"" ssid = ""41"">There has been a fair amount of past work on no predicates)  confusion of Washington state with this topic: Liang et al. (2010) induces combinatory Washington D.C.  learning the wrong lexical asso- logic programs in a non-linguistic setting.</S><S sid =""158"" ssid = ""43"">5 Discussion Piantadosi et al. (2008) induces first-order formuA major focus of this work is on our semantic rep- lae using CCG in a small domain assuming observed resentation  DCS  which offers a new perspective lexical semantics.</S><S sid =""18"" ssid = ""14"">CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).</S><S sid =""149"" ssid = ""34"">Think of DCS as a higher-level Our system learns lexical associations between programming language tailored to natural language  words and predicates.</S>",['Method_Citation']
5,P11-1060,P13-1092,0,"Liang et al, 2011",0,"GUSP represents meaning by a semantic tree, which is similar to DCS (Liang et al, 2011)","GUSP represents meaning by a semantic tree, which is similar to DCS (Liang et al, 2011)","['18', '15', '133', '143', '119']","<S sid =""18"" ssid = ""14"">CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).</S><S sid =""15"" ssid = ""11"">Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.</S><S sid =""133"" ssid = ""18"">Table 2 shows that our system using lexical triggers L (henceforth  DCS) outperforms SEMRESP (78.9% over 73.2%).</S><S sid =""143"" ssid = ""28"">Note that having lexical triggers is a much weaker requirement than having a CCG lexicon  and far easier to obtain than logical forms.</S><S sid =""119"" ssid = ""4"">There are three types of predicates in P: generic (e.g.  argmax)  data (e.g.  city)  and value (e.g.  CA).</S>",['Method_Citation']
6,P11-1060,W12-2802,0,2011,0,"Matuszek et al [2010], Liang et al [2011] and Chen and Mooney [2011] describe models that learn compositional semantics, but word meanings are symbolic structures rather than patterns of features in the external world","Matuszek et al [2010], Liang et al [2011] and Chen and Mooney [2011] describe models that learn compositional semantics, but word meanings are symbolic structures rather than patterns of features in the external world","['166', '165', '156', '163', '18']","<S sid =""166"" ssid = ""51"">Our employed (Zettlemoyer and Collins  2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language—think grounded al.  2010). compositional semantics.</S><S sid =""165"" ssid = ""50"">These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.</S><S sid =""156"" ssid = ""41"">There has been a fair amount of past work on no predicates)  confusion of Washington state with this topic: Liang et al. (2010) induces combinatory Washington D.C.  learning the wrong lexical asso- logic programs in a non-linguistic setting.</S><S sid =""163"" ssid = ""48"">Feedback from the context; for example  the lexical entry for borders world has been used to guide both syntactic parsing is S\NP/NP : Ay.Ax.border(x  y)  which means (Schuler  2003) and semantic parsing (Popescu et borders looks right for the first argument and left al.  2003; Clarke et al.  2010).</S><S sid =""18"" ssid = ""14"">CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).</S>",['Method_Citation']
7,P11-1060,P13-2009,0,"Liang et al, 2011",0,"It is well-studied in NLP, and a wide variety of methods have been proposed to tackle it ,e.g. rule-based (Popescu et al, 2003), super vised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al, 2011)","It is well-studied in NLP, and a wide variety of methods have been proposed to tackle it ,e.g. rule-based (Popescu et al, 2003), super vised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al, 2011)","['163', '172', '7', '87', '93']","<S sid =""163"" ssid = ""48"">Feedback from the context; for example  the lexical entry for borders world has been used to guide both syntactic parsing is S\NP/NP : Ay.Ax.border(x  y)  which means (Schuler  2003) and semantic parsing (Popescu et borders looks right for the first argument and left al.  2003; Clarke et al.  2010).</S><S sid =""172"" ssid = ""57"">Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms  we hope to use our without becoming mired in the semantic formalism. techniques in developing even more accurate and Quantifiers and superlatives significantly compli- broader-coverage language understanding systems. cate scoping in lambda calculus  and often type rais- Acknowledgments We thank Luke Zettlemoyer ing needs to be employed.</S><S sid =""7"" ssid = ""3"">Supervised semantic parsers (Zelle and Mooney  1996; Tang and Mooney  2001; Ge and Mooney  2005; Zettlemoyer and Collins  2005; Kate and Mooney  2007; Zettlemoyer and Collins  2007; Wong and Mooney  2007; Kwiatkowski et al.  2010) rely on manual annotation of logical forms  which is expensive.</S><S sid =""87"" ssid = ""63"">For example  in Figure 4(a)  before execution  the denotation of the DCS tree is hh{[(CA  OR)  (OR)] ... }; ø; (E  Qhstatei�w  ø)ii; after applying X1  we have hh{[(OR)]  ... }; øii.</S><S sid =""93"" ssid = ""69"">We then superlative ambiguity based on where the scopeapply d.ci to these two sets (technically  denota- determining execute relation is placed. tions) and project away the first column: Xi(d) = 3 Semantic Parsing ((d.ci ./1 1 A) ./2 1 B) [−1].</S>",['Method_Citation']
8,P11-1060,D12-1069,0,Liangetal2011,0,"One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Liangetal2011) or even a binary correct/incorrect signal (Clarke et al2010)","One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Lianget al 2011) or even a binary correct/incorrect signal (Clarke et al2010)","['158', '156', '18', '138', '26']","<S sid =""158"" ssid = ""43"">5 Discussion Piantadosi et al. (2008) induces first-order formuA major focus of this work is on our semantic rep- lae using CCG in a small domain assuming observed resentation  DCS  which offers a new perspective lexical semantics.</S><S sid =""156"" ssid = ""41"">There has been a fair amount of past work on no predicates)  confusion of Washington state with this topic: Liang et al. (2010) induces combinatory Washington D.C.  learning the wrong lexical asso- logic programs in a non-linguistic setting.</S><S sid =""18"" ssid = ""14"">CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).</S><S sid =""138"" ssid = ""23"">Next  we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).</S><S sid =""26"" ssid = ""2"">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S>",['Method_Citation']
9,P11-1060,N12-1049,0,2011,0,"For example, Liang et al (2011) constructs a latent parse similar in structure to a dependency grammar, but representing a logical form","For example, Liang et al (2011) constructs a latent parse similar in structure to a dependency grammar, but representing a logical form","['3', '9', '18', '156', '21']","<S sid =""3"" ssid = ""3"">In tackling this challenging learning problem  we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.</S><S sid =""9"" ssid = ""5"">As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S><S sid =""18"" ssid = ""14"">CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).</S><S sid =""156"" ssid = ""41"">There has been a fair amount of past work on no predicates)  confusion of Washington state with this topic: Liang et al. (2010) induces combinatory Washington D.C.  learning the wrong lexical asso- logic programs in a non-linguistic setting.</S><S sid =""21"" ssid = ""17"">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S>",['Results_Citation']
10,P11-1060,P12-1045,0,2011,0,Clarke et al (2010) and Liang et al (2011) trained systems on question and answer pairs by automatically finding semantic interpretations of the questions that would generate the correct answers,Clarke et al (2010) and Liang et al (2011) trained systems on question and answer pairs by automatically finding semantic interpretations of the questions that would generate the correct answers,"['132', '166', '125', '156', '158']","<S sid =""132"" ssid = ""17"">Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.</S><S sid =""166"" ssid = ""51"">Our employed (Zettlemoyer and Collins  2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language—think grounded al.  2010). compositional semantics.</S><S sid =""125"" ssid = ""10"">We used the same training-test splits as Zettlemoyer and Collins (2005) (600+280 for GEO and 500+140 for JOBS).</S><S sid =""156"" ssid = ""41"">There has been a fair amount of past work on no predicates)  confusion of Washington state with this topic: Liang et al. (2010) induces combinatory Washington D.C.  learning the wrong lexical asso- logic programs in a non-linguistic setting.</S><S sid =""158"" ssid = ""43"">5 Discussion Piantadosi et al. (2008) induces first-order formuA major focus of this work is on our semantic rep- lae using CCG in a small domain assuming observed resentation  DCS  which offers a new perspective lexical semantics.</S>",['Method_Citation']
11,P11-1060,P14-1008,0,"Liang et al,2011",0,"Dependency-based Compositional Semantics (DCS) provides an intuitive way to model semantics of questions, by using simple dependency-like trees (Liang et al, 2011)","Dependency-based Compositional Semantics (DCS) provides an intuitive way to model semantics of questions, by using simple dependency-like trees (Liang et al, 2011)","['26', '138', '17', '88', '156']","<S sid =""26"" ssid = ""2"">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid =""138"" ssid = ""23"">Next  we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).</S><S sid =""17"" ssid = ""13"">The dominant paradigm in compositional semantics is Montague semantics  which constructs lambda calculus forms in a bottom-up manner.</S><S sid =""88"" ssid = ""64"">Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.</S><S sid =""156"" ssid = ""41"">There has been a fair amount of past work on no predicates)  confusion of Washington state with this topic: Liang et al. (2010) induces combinatory Washington D.C.  learning the wrong lexical asso- logic programs in a non-linguistic setting.</S>",['Method_Citation']
12,P11-1060,P14-1008,0,"Liang et al, 2011",0,"DCS trees has been proposed to represent natural language semantics with a structure similar to dependency trees (Liang et al, 2011) (Figure 1)","DCS trees has been proposed to represent natural language semantics with a structure similar to dependency trees (Liang et al, 2011) (Figure 1)","['18', '166', '149', '47', '156']","<S sid =""18"" ssid = ""14"">CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).</S><S sid =""166"" ssid = ""51"">Our employed (Zettlemoyer and Collins  2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language—think grounded al.  2010). compositional semantics.</S><S sid =""149"" ssid = ""34"">Think of DCS as a higher-level Our system learns lexical associations between programming language tailored to natural language  words and predicates.</S><S sid =""47"" ssid = ""23"">This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure  which will facilitate parsing.</S><S sid =""156"" ssid = ""41"">There has been a fair amount of past work on no predicates)  confusion of Washington state with this topic: Liang et al. (2010) induces combinatory Washington D.C.  learning the wrong lexical asso- logic programs in a non-linguistic setting.</S>",['Method_Citation']
13,P11-1060,P14-1008,0,"Liang et al, 2011",0,"are explained in? 2.5. 5http: //nlp.stanford.edu/software/corenlp.shtml 6 In (Liang et al, 2011) DCS trees are learned from QApairs and database entries","In (Liang et al, 2011) DCS trees are learned from QA pairs and database entries","['34', '158', '116', '26', '49']","<S sid =""34"" ssid = ""10"">Figure 2(a) shows an example of a DCS tree.</S><S sid =""158"" ssid = ""43"">5 Discussion Piantadosi et al. (2008) induces first-order formuA major focus of this work is on our semantic rep- lae using CCG in a small domain assuming observed resentation  DCS  which offers a new perspective lexical semantics.</S><S sid =""116"" ssid = ""1"">We tested our system on two standard datasets  GEO and JOBS.</S><S sid =""26"" ssid = ""2"">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid =""49"" ssid = ""25"">Aggregate relation DCS trees that only use join relations can represent arbitrarily complex compositional structures  but they cannot capture higherorder phenomena in language.</S>",['Method_Citation']
14,P11-1060,P14-1008,0,"Liang et al, 2011",0,"as in the sentence? Tropi cal storm Debby is blamed for death?, which is a tropical storm, is Debby, etc. Technically, each germ in a DCS tree indicates a variable when the DCS tree is translated to a FOL formula, and the abstract denotation of the germ corresponds to the set of consistent values (Liang et al, 2011) of that variable","Technically, each germ in a DCS tree indicates a variable when the DCS tree is translated to a FOL formula, and the abstract denotation of the germ corresponds to the set of consistent values (Liang et al, 2011) of that variable","['59', '124', '115', '119', '156']","<S sid =""59"" ssid = ""35"">The key idea that allows us to give semanticallyscoped denotations to syntactically-scoped trees is as follows: We mark a node low in the tree with a mark relation (one of E  Q  or C).</S><S sid =""124"" ssid = ""9"">For each data predicate p (e.g.  language)  we add each possible tuple (e.g.  (job37  Java)) to w(p) independently with probability 0.8.</S><S sid =""115"" ssid = ""91"">After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy pθ(T)(y  |x  z ∈ ˜ZL θ(T)).</S><S sid =""119"" ssid = ""4"">There are three types of predicates in P: generic (e.g.  argmax)  data (e.g.  city)  and value (e.g.  CA).</S><S sid =""156"" ssid = ""41"">There has been a fair amount of past work on no predicates)  confusion of Washington state with this topic: Liang et al. (2010) induces combinatory Washington D.C.  learning the wrong lexical asso- logic programs in a non-linguistic setting.</S>",['Method_Citation']
15,P11-1060,D11-1140,0,2011,0,Clarke et al (2010) and Liang et al (2011) replace semantic annotations in the training set with target answers which are more easily available,Clarke et al (2010) and Liang et al (2011) replace semantic annotations in the training set with target answers which are more easily available,"['166', '165', '156', '18', '125']","<S sid =""166"" ssid = ""51"">Our employed (Zettlemoyer and Collins  2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language—think grounded al.  2010). compositional semantics.</S><S sid =""165"" ssid = ""50"">These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.</S><S sid =""156"" ssid = ""41"">There has been a fair amount of past work on no predicates)  confusion of Washington state with this topic: Liang et al. (2010) induces combinatory Washington D.C.  learning the wrong lexical asso- logic programs in a non-linguistic setting.</S><S sid =""18"" ssid = ""14"">CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).</S><S sid =""125"" ssid = ""10"">We used the same training-test splits as Zettlemoyer and Collins (2005) (600+280 for GEO and 500+140 for JOBS).</S>",['Method_Citation']
16,P11-1060,D11-1140,0,"Liang et al, 2011",0,"and Collins, 2005, 2007),? -WASP (Wong and Mooney, 2007), UBL (Kwiatkowski et al, 2010) systems and DCS (Liang et al, 2011)","WASP (Wong and Mooney, 2007), UBL (Kwiatkowski et al, 2010) systems and DCS (Liang et al, 2011)","['166', '160', '165', '158', '9']","<S sid =""166"" ssid = ""51"">Our employed (Zettlemoyer and Collins  2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language—think grounded al.  2010). compositional semantics.</S><S sid =""160"" ssid = ""45"">To contrast  consider et al. (2010)  which we discussed earlier.</S><S sid =""165"" ssid = ""50"">These rules are often too stringent  cused on aligning text to a world (Liang et al.  2009)  and for complex utterances  especially in free word- using text in reinforcement learning (Branavan et al.  order languages  either disharmonic combinators are 2009; Branavan et al.  2010)  and many others.</S><S sid =""158"" ssid = ""43"">5 Discussion Piantadosi et al. (2008) induces first-order formuA major focus of this work is on our semantic rep- lae using CCG in a small domain assuming observed resentation  DCS  which offers a new perspective lexical semantics.</S><S sid =""9"" ssid = ""5"">As in Clarke et al. (2010)  we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S>",['Method_Citation']
17,P11-1060,P13-1007,0,2011,0,"In general, every plural NPpotentially introduces an implicit universal, ranging 1For example, Liang et al (2011) in their state-of-the-art statistical semantic parser within the domain of natural language queries to databases, explicitly devise quantifier scoping in the semantic model","For example, Liang et al (2011) in their state-of-the-art statistical semantic parser within the domain of natural language queries to databases, explicitly devise quantifier scoping in the semantic model","['156', '166', '21', '141', '158']","<S sid =""156"" ssid = ""41"">There has been a fair amount of past work on no predicates)  confusion of Washington state with this topic: Liang et al. (2010) induces combinatory Washington D.C.  learning the wrong lexical asso- logic programs in a non-linguistic setting.</S><S sid =""166"" ssid = ""51"">Our employed (Zettlemoyer and Collins  2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language—think grounded al.  2010). compositional semantics.</S><S sid =""21"" ssid = ""17"">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid =""141"" ssid = ""26"">Rather than using lexical triggers  several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.</S><S sid =""158"" ssid = ""43"">5 Discussion Piantadosi et al. (2008) induces first-order formuA major focus of this work is on our semantic rep- lae using CCG in a small domain assuming observed resentation  DCS  which offers a new perspective lexical semantics.</S>",['Method_Citation']
18,P11-1060,D11-1022,0,2011,0,"DD-ADMM may be useful in other frameworks involving logical constraints, such as the models for compositional semantics presented by Liang et al (2011)","DD-ADMM may be useful in other frameworks involving logical constraints, such as the models for compositional semantics presented by Liang et al (2011)","['58', '143', '36', '17', '48']","<S sid =""58"" ssid = ""34"">We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.</S><S sid =""143"" ssid = ""28"">Note that having lexical triggers is a much weaker requirement than having a CCG lexicon  and far easier to obtain than logical forms.</S><S sid =""36"" ssid = ""12"">It is this transparency between syntax and semantics provided by DCS which leads to a simple and streamlined compositional semantics suitable for program induction.</S><S sid =""17"" ssid = ""13"">The dominant paradigm in compositional semantics is Montague semantics  which constructs lambda calculus forms in a bottom-up manner.</S><S sid =""48"" ssid = ""24"">In addition  trees enable efficient computation  thereby establishing a new connection between dependency syntax and efficient semantic evaluation.</S>",['Method_Citation']
19,P11-1060,P12-1051,0,2011,0,"In fact, for any CFG G, it 1See Liang et al (2011) for work in representing lambda calculus expressions with trees","In fact, for any CFG G, it 1See Liang et al (2011) for work in representing lambda calculus expressions with trees","['92', '76', '33', '84', '30']","<S sid =""92"" ssid = ""68"">Figlar  the restrictor is A = E (d.bi) and the nu- ure 4(g) shows that we can naturally account for clear scope is B = E (d[i  −(i  0)]).</S><S sid =""76"" ssid = ""52"">For example  for d in Figure 5  d[1] keeps column 1  d[−ø] keeps column 2  and d[2  −2] swaps the two columns.</S><S sid =""33"" ssid = ""9"">As another example  w(average) = {(S  ¯x) : We write a DCS tree z as hp; r1 : c1; ... ; rm : cmi.</S><S sid =""84"" ssid = ""60"">There are three cases: Extraction (d.ri = E) In the basic version  the denotation of a tree was always the set of consistent values of the root node.</S><S sid =""30"" ssid = ""6"">A world w is mapping from each predicate p ∈ P to a set of tuples; for example  w(state) = {(CA)  (OR) ... }.</S>",['Results_Citation']
