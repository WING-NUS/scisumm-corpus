Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,W11-2123,W11-2138,0,"Heafield, 2011",0,"We used common tools for phrase-based translation? Moses (Koehn et al, 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modelling and GIZA++ (Och and Ney, 2000) for word alignments","We used common tools for phrase-based translation Moses (Koehn et al, 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modelling and GIZA++ (Och and Ney, 2000) for word alignments","['62', '199', '268', '205', '21']","<S sid =""62"" ssid = ""40"">If the key distribution’s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |− 1] to estimate pivots; this optimization alone led to a 24% speed improvement.</S><S sid =""199"" ssid = ""18"">For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid =""268"" ssid = ""10"">For example  syntactic decoders (Koehn et al.  2007; Dyer et al.  2010; Li et al.  2009) perform dynamic programming parametrized by both backward- and forward-looking state.</S><S sid =""205"" ssid = ""24"">We evaluate the time and memory consumption of each data structure by computing perplexity on 4 billion tokens from the English Gigaword corpus (Parker et al.  2009).</S><S sid =""21"" ssid = ""16"">Performance improvements transfer to the Moses (Koehn et al.  2007)  cdec (Dyer et al.  2010)  and Joshua (Li et al.  2009) translation systems where our code has been integrated.</S>",['Method_Citation']
2,W11-2123,P14-2022,0,"Heafield, 2011",0,"The language model was com piled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run","The language model was compiled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run","['223', '152', '171', '200', '108']","<S sid =""223"" ssid = ""42"">The binary language model from Section 5.2 and text phrase table were forced into disk cache before each run.</S><S sid =""152"" ssid = ""24"">All language model queries issued by machine translation decoders follow a left-to-right pattern  starting with either the begin of sentence token or null context for mid-sentence fragments.</S><S sid =""171"" ssid = ""43"">Along with IRSTLM and TPT  our binary format is memory mapped  meaning the file and in-memory representation are the same.</S><S sid =""200"" ssid = ""19"">The model was built with open vocabulary  modified Kneser-Ney smoothing  and default pruning settings that remove singletons of order 3 and higher.</S><S sid =""108"" ssid = ""12"">Compared with SRILM  IRSTLM adds several features: lower memory consumption  a binary file format with memory mapping  caching to increase speed  and quantization.</S>",['Method_Citation']
3,W11-2123,W12-3145,0,"Heafield, 2011",0,"Thus given afragment tf consisting of a sequence of target to kens, we compute LM scores for (i) < s& gt ;tf, (ii )tf and (iii )tf < /s& gt; and use the best score (only) for pruning.2 While this increases the number ofLM queries, we exploit the language model state in formation in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states","Thus given a fragment tf consisting of a sequence of target tokens, we compute LM scores for (i) < s& gt ;tf, (ii )tf and (iii )tf < /s& gt; and use the best score (only) for pruning. While this increases the number of LM queries, we exploit the language model state in formation in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states","['156', '62', '233', '199', '142']","<S sid =""156"" ssid = ""28"">As noted in Section 1  our code finds the longest matching entry wnf for query p(wn|s(wn−1 f ) The probability p(wn|wn−1 f ) is stored with wnf and the backoffs are immediately accessible in the provided state s(wn−1 When our code walks the data structure to find wnf   it visits wnn  wnn−1  ...   wnf .</S><S sid =""62"" ssid = ""40"">If the key distribution’s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |− 1] to estimate pivots; this optimization alone led to a 24% speed improvement.</S><S sid =""233"" ssid = ""52"">The authors provided us with a ratio between TPT and SRI under different conditions. aLossy compression with the same weights. bLossy compression with retuned weights. ditions make the value appropriate for estimating repeated run times  such as in parameter tuning.</S><S sid =""199"" ssid = ""18"">For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid =""142"" ssid = ""14"">Language models that contain wi must also contain prefixes wi for 1 G i G k. Therefore  when the model is queried for p(wnjwn−1 1 ) but the longest matching suffix is wnf   it may return state s(wn1) = wnf since no longer context will be found.</S>",['Method_Citation']
4,W11-2123,W12-3131,0,"Heafield, 2011",0,"Our translation system uses cdec (Dyer et al,2010), an implementation of the hierarchical phrase based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference","Our translation system uses cdec (Dyer et al,2010), an implementation of the hierarchical phrase based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference","['16', '76', '199', '21', '152']","<S sid =""16"" ssid = ""11"">BerkeleyLM revision 152 (Pauls and Klein  2011) implements tries based on hash tables and sorted arrays in Java with lossy quantization.</S><S sid =""76"" ssid = ""54"">This technique was introduced by Clarkson and Rosenfeld (1997) and is also implemented by IRSTLM and BerkeleyLM’s compressed option.</S><S sid =""199"" ssid = ""18"">For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid =""21"" ssid = ""16"">Performance improvements transfer to the Moses (Koehn et al.  2007)  cdec (Dyer et al.  2010)  and Joshua (Li et al.  2009) translation systems where our code has been integrated.</S><S sid =""152"" ssid = ""24"">All language model queries issued by machine translation decoders follow a left-to-right pattern  starting with either the begin of sentence token or null context for mid-sentence fragments.</S>",['Method_Citation']
5,W11-2123,W12-3154,0,"Heafield, 2011",0,"The three data sets in use in this paper are summarised in Table 1.The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al, 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime","The three data sets in use in this paper are summarised in Table 1.The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al, 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime","['199', '156', '233', '62', '21']","<S sid =""199"" ssid = ""18"">For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid =""156"" ssid = ""28"">As noted in Section 1  our code finds the longest matching entry wnf for query p(wn|s(wn−1 f ) The probability p(wn|wn−1 f ) is stored with wnf and the backoffs are immediately accessible in the provided state s(wn−1 When our code walks the data structure to find wnf   it visits wnn  wnn−1  ...   wnf .</S><S sid =""233"" ssid = ""52"">The authors provided us with a ratio between TPT and SRI under different conditions. aLossy compression with the same weights. bLossy compression with retuned weights. ditions make the value appropriate for estimating repeated run times  such as in parameter tuning.</S><S sid =""62"" ssid = ""40"">If the key distribution’s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |− 1] to estimate pivots; this optimization alone led to a 24% speed improvement.</S><S sid =""21"" ssid = ""16"">Performance improvements transfer to the Moses (Koehn et al.  2007)  cdec (Dyer et al.  2010)  and Joshua (Li et al.  2009) translation systems where our code has been integrated.</S>",['Method_Citation']
6,W11-2123,P12-2058,0,"Heafield, 2011",0,"The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011)","The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011)","['152', '105', '165', '115', '94']","<S sid =""152"" ssid = ""24"">All language model queries issued by machine translation decoders follow a left-to-right pattern  starting with either the begin of sentence token or null context for mid-sentence fragments.</S><S sid =""105"" ssid = ""9"">Their default variant implements a forward trie  in which words are looked up in their natural left-to-right order.</S><S sid =""165"" ssid = ""37"">The PROBING model can perform optimistic searches by jumping to any n-gram without needing state and without any additional memory.</S><S sid =""115"" ssid = ""19"">Most similar is scrolling queries  wherein left-to-right queries that add one word at a time are optimized.</S><S sid =""94"" ssid = ""72"">Unigrams also have 64-bit overhead for vocabulary lookup.</S>",['Method_Citation']
7,W11-2123,W11-2139,0,2011,0,Inference was carried out using the language modeling library described by Heafield (2011),Inference was carried out using the language modeling library described by Heafield (2011),"['66', '76', '152', '19', '231']","<S sid =""66"" ssid = ""44"">This has the effect of randomly permuting vocabulary identifiers  meeting the requirements of interpolation search when vocabulary identifiers are used as keys.</S><S sid =""76"" ssid = ""54"">This technique was introduced by Clarkson and Rosenfeld (1997) and is also implemented by IRSTLM and BerkeleyLM’s compressed option.</S><S sid =""152"" ssid = ""24"">All language model queries issued by machine translation decoders follow a left-to-right pattern  starting with either the begin of sentence token or null context for mid-sentence fragments.</S><S sid =""19"" ssid = ""14"">These packages are further described in Section 3.</S><S sid =""231"" ssid = ""50"">The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI’s speed reported in Guthrie and Hepple (2010) under different conditions.</S>",['Method_Citation']
8,W11-2123,P13-2003,0,"Heafield, 2011",0,"We used the MADA ATB segmentation for Arabic (Roth et al, 2008) and true casing for English, phrases of maximal length 7, KneserNey smoothing, and lexicalized reordering (Koehn et al, 2005), and a 5-gram language model, trained on GigaWordv.5 using KenLM (Heafield, 2011)","We used the MADA ATB segmentation for Arabic (Roth et al, 2008) and true casing for English, phrases of maximal length 7, KneserNey smoothing, and lexicalized reordering (Koehn et al, 2005), and a 5-gram language model, trained on GigaWordv.5 using KenLM (Heafield, 2011)","['199', '156', '233', '69', '21']","<S sid =""199"" ssid = ""18"">For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid =""156"" ssid = ""28"">As noted in Section 1  our code finds the longest matching entry wnf for query p(wn|s(wn−1 f ) The probability p(wn|wn−1 f ) is stored with wnf and the backoffs are immediately accessible in the provided state s(wn−1 When our code walks the data structure to find wnf   it visits wnn  wnn−1  ...   wnf .</S><S sid =""233"" ssid = ""52"">The authors provided us with a ratio between TPT and SRI under different conditions. aLossy compression with the same weights. bLossy compression with retuned weights. ditions make the value appropriate for estimating repeated run times  such as in parameter tuning.</S><S sid =""69"" ssid = ""47"">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid =""21"" ssid = ""16"">Performance improvements transfer to the Moses (Koehn et al.  2007)  cdec (Dyer et al.  2010)  and Joshua (Li et al.  2009) translation systems where our code has been integrated.</S>",['Method_Citation']
9,W11-2123,W12-3134,0,2011,0,The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language mod els by Heafield (2011) and Pauls and Klein (2011)? both language model implementations are now integrated with Joshua,The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language models by Heafield (2011) and Pauls and Klein (2011)? both language model implementations are now integrated with Joshua,"['199', '233', '231', '81', '221']","<S sid =""199"" ssid = ""18"">For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid =""233"" ssid = ""52"">The authors provided us with a ratio between TPT and SRI under different conditions. aLossy compression with the same weights. bLossy compression with retuned weights. ditions make the value appropriate for estimating repeated run times  such as in parameter tuning.</S><S sid =""231"" ssid = ""50"">The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI’s speed reported in Guthrie and Hepple (2010) under different conditions.</S><S sid =""81"" ssid = ""59"">Values in the trie are minimally sized at the bit level  improving memory consumption over trie implementations in SRILM  IRSTLM  and BerkeleyLM.</S><S sid =""221"" ssid = ""40"">SRILM’s compact variant has an incredibly expensive destructor  dwarfing the time it takes to perform translation  and so we also modified Moses to avoiding the destructor by calling exit instead of returning normally.</S>",['Results_Citation']
10,W11-2123,W12-3134,0,2011,0,Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets,Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets,"['16', '76', '231', '102', '51']","<S sid =""16"" ssid = ""11"">BerkeleyLM revision 152 (Pauls and Klein  2011) implements tries based on hash tables and sorted arrays in Java with lossy quantization.</S><S sid =""76"" ssid = ""54"">This technique was introduced by Clarkson and Rosenfeld (1997) and is also implemented by IRSTLM and BerkeleyLM’s compressed option.</S><S sid =""231"" ssid = ""50"">The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI’s speed reported in Guthrie and Hepple (2010) under different conditions.</S><S sid =""102"" ssid = ""6"">The PROBING model was designed to improve upon SRILM by using linear probing hash tables (though not arranged in a trie)  allocating memory all at once (eliminating the need for full pointers)  and being easy to compile.</S><S sid =""51"" ssid = ""29"">This differs from other implementations (Stolcke  2002; Pauls and Klein  2011) that use hash tables as nodes in a trie  as explained in the next section.</S>",['Method_Citation']
11,W11-2123,W12-3134,0,2011,0,"With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult to compile SRILM toolkit (Stolcke, 2002)","With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult to compile SRILM toolkit (Stolcke, 2002)","['199', '21', '16', '51', '264']","<S sid =""199"" ssid = ""18"">For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid =""21"" ssid = ""16"">Performance improvements transfer to the Moses (Koehn et al.  2007)  cdec (Dyer et al.  2010)  and Joshua (Li et al.  2009) translation systems where our code has been integrated.</S><S sid =""16"" ssid = ""11"">BerkeleyLM revision 152 (Pauls and Klein  2011) implements tries based on hash tables and sorted arrays in Java with lossy quantization.</S><S sid =""51"" ssid = ""29"">This differs from other implementations (Stolcke  2002; Pauls and Klein  2011) that use hash tables as nodes in a trie  as explained in the next section.</S><S sid =""264"" ssid = ""6"">For even larger models  storing counts (Talbot and Osborne  2007; Pauls and Klein  2011; Guthrie and Hepple  2010) is a possibility.</S>",['Method_Citation']
12,W11-2123,W12-3160,0,"Heafield, 2011",0,"This was used to create a KenLM (Heafield, 2011)","This was used to create a KenLM (Heafield, 2011)","['76', '163', '281', '12', '181']","<S sid =""76"" ssid = ""54"">This technique was introduced by Clarkson and Rosenfeld (1997) and is also implemented by IRSTLM and BerkeleyLM’s compressed option.</S><S sid =""163"" ssid = ""35"">Further  it needs extra pointers in the trie  increasing model size by 40%.</S><S sid =""281"" ssid = ""2"">Hieu Hoang named the code “KenLM” and assisted with Moses along with Barry Haddow.</S><S sid =""12"" ssid = ""7"">Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke  2002) is a popular toolkit based on tries used in several decoders.</S><S sid =""181"" ssid = ""53"">This is similar to using the Linux MAP POPULATE flag that is our default loading mechanism.</S>",['Method_Citation']
13,W11-2123,W12-3706,0,"Heafield, 2011",0,"In the Opinum system we query the M p, M n mod els with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable fora web application","In the Opinum system we query the M p, M n models with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable for a web application","['250', '205', '233', '67', '62']","<S sid =""250"" ssid = ""69"">The BerkeleyLM direct-mapped cache is in principle faster than caches implemented by RandLM and by IRSTLM  so we may write a C++ equivalent implementation as future work.</S><S sid =""205"" ssid = ""24"">We evaluate the time and memory consumption of each data structure by computing perplexity on 4 billion tokens from the English Gigaword corpus (Parker et al.  2009).</S><S sid =""233"" ssid = ""52"">The authors provided us with a ratio between TPT and SRI under different conditions. aLossy compression with the same weights. bLossy compression with retuned weights. ditions make the value appropriate for estimating repeated run times  such as in parameter tuning.</S><S sid =""67"" ssid = ""45"">While sorted arrays could be used to implement the same data structure as PROBING  effectively making m = 1  we abandoned this implementation because it is slower and larger than a trie implementation.</S><S sid =""62"" ssid = ""40"">If the key distribution’s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |− 1] to estimate pivots; this optimization alone led to a 24% speed improvement.</S>",['Method_Citation']
14,W11-2123,W11-2147,0,"Heafield, 2011",0,"Our base line is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al, 2007) for translation model training and decoding, GIZA++ (Ochand Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights","Our base line is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al, 2007) for translation model training and decoding, GIZA++ (Ochand Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights","['156', '199', '233', '62', '21']","<S sid =""156"" ssid = ""28"">As noted in Section 1  our code finds the longest matching entry wnf for query p(wn|s(wn−1 f ) The probability p(wn|wn−1 f ) is stored with wnf and the backoffs are immediately accessible in the provided state s(wn−1 When our code walks the data structure to find wnf   it visits wnn  wnn−1  ...   wnf .</S><S sid =""199"" ssid = ""18"">For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid =""233"" ssid = ""52"">The authors provided us with a ratio between TPT and SRI under different conditions. aLossy compression with the same weights. bLossy compression with retuned weights. ditions make the value appropriate for estimating repeated run times  such as in parameter tuning.</S><S sid =""62"" ssid = ""40"">If the key distribution’s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |− 1] to estimate pivots; this optimization alone led to a 24% speed improvement.</S><S sid =""21"" ssid = ""16"">Performance improvements transfer to the Moses (Koehn et al.  2007)  cdec (Dyer et al.  2010)  and Joshua (Li et al.  2009) translation systems where our code has been integrated.</S>",['Method_Citation']
15,W11-2123,E12-1083,0,"Heafield, 2011",0,"For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011)","For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011)","['268', '12', '214', '169', '264']","<S sid =""268"" ssid = ""10"">For example  syntactic decoders (Koehn et al.  2007; Dyer et al.  2010; Li et al.  2009) perform dynamic programming parametrized by both backward- and forward-looking state.</S><S sid =""12"" ssid = ""7"">Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke  2002) is a popular toolkit based on tries used in several decoders.</S><S sid =""214"" ssid = ""33"">For even larger models  we recommend RandLM; the memory consumption of the cache is not expected to grow with model size  and it has been reported to scale well.</S><S sid =""169"" ssid = ""41"">In our case multi-threading is trivial because our data structures are read-only and uncached.</S><S sid =""264"" ssid = ""6"">For even larger models  storing counts (Talbot and Osborne  2007; Pauls and Klein  2011; Guthrie and Hepple  2010) is a possibility.</S>",['Method_Citation']
16,W11-2123,P12-1002,0,"Heafield, 2011",0,"Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Sima ?an,2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well.3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using kenlm (Heafield, 2011)","Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Simaan,2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well.3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using kenlm (Heafield, 2011)","['156', '199', '62', '233', '245']","<S sid =""156"" ssid = ""28"">As noted in Section 1  our code finds the longest matching entry wnf for query p(wn|s(wn−1 f ) The probability p(wn|wn−1 f ) is stored with wnf and the backoffs are immediately accessible in the provided state s(wn−1 When our code walks the data structure to find wnf   it visits wnn  wnn−1  ...   wnf .</S><S sid =""199"" ssid = ""18"">For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid =""62"" ssid = ""40"">If the key distribution’s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |− 1] to estimate pivots; this optimization alone led to a 24% speed improvement.</S><S sid =""233"" ssid = ""52"">The authors provided us with a ratio between TPT and SRI under different conditions. aLossy compression with the same weights. bLossy compression with retuned weights. ditions make the value appropriate for estimating repeated run times  such as in parameter tuning.</S><S sid =""245"" ssid = ""64"">Along with locking and background kernel operations such as prefaulting  this explains why wall time is not one-eighth that of the single-threaded case. aLossy compression with the same weights. bLossy compression with retuned weights. the non-lossy options.</S>",['Method_Citation']
17,W11-2123,D12-1108,0,"Heafield, 2011",0,"n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3","n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3","['16', '171', '186', '267', '215']","<S sid =""16"" ssid = ""11"">BerkeleyLM revision 152 (Pauls and Klein  2011) implements tries based on hash tables and sorted arrays in Java with lossy quantization.</S><S sid =""171"" ssid = ""43"">Along with IRSTLM and TPT  our binary format is memory mapped  meaning the file and in-memory representation are the same.</S><S sid =""186"" ssid = ""5"">For sorted lookup  we compare interpolation search  standard C++ binary search  and standard C++ set based on red-black trees.</S><S sid =""267"" ssid = ""9"">While we have minimized forward-looking state in Section 4.1  machine translation systems could also benefit by minimizing backward-looking state.</S><S sid =""215"" ssid = ""34"">Another option is the closedsource data structures from Sheffield (Guthrie and Hepple  2010).</S>",['Method_Citation']
18,W11-2123,P12-2006,0,"Heafield, 2011",0,"Research efforts to increase search efficiency for phrase-based MT (Koehn et al, 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al, 2006) to additional early pruning techniques (Delaney et al, 2006), (Moore and Quirk, 2007) and more efficient language model (LM) querying (Heafield, 2011)","Research efforts to increase search efficiency for phrase-based MT (Koehn et al, 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al, 2006) to additional early pruning techniques (Delaney et al, 2006), (Moore and Quirk, 2007) and more efficient language model (LM) querying (Heafield, 2011)","['199', '233', '156', '21', '62']","<S sid =""199"" ssid = ""18"">For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid =""233"" ssid = ""52"">The authors provided us with a ratio between TPT and SRI under different conditions. aLossy compression with the same weights. bLossy compression with retuned weights. ditions make the value appropriate for estimating repeated run times  such as in parameter tuning.</S><S sid =""156"" ssid = ""28"">As noted in Section 1  our code finds the longest matching entry wnf for query p(wn|s(wn−1 f ) The probability p(wn|wn−1 f ) is stored with wnf and the backoffs are immediately accessible in the provided state s(wn−1 When our code walks the data structure to find wnf   it visits wnn  wnn−1  ...   wnf .</S><S sid =""21"" ssid = ""16"">Performance improvements transfer to the Moses (Koehn et al.  2007)  cdec (Dyer et al.  2010)  and Joshua (Li et al.  2009) translation systems where our code has been integrated.</S><S sid =""62"" ssid = ""40"">If the key distribution’s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |− 1] to estimate pivots; this optimization alone led to a 24% speed improvement.</S>",['Method_Citation']
19,W11-2123,P13-2073,0,"Heafield, 2011",0,"For English language modeling, we use English Giga word Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011)","For English language modeling, we use English Giga word Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011)","['268', '205', '12', '186', '171']","<S sid =""268"" ssid = ""10"">For example  syntactic decoders (Koehn et al.  2007; Dyer et al.  2010; Li et al.  2009) perform dynamic programming parametrized by both backward- and forward-looking state.</S><S sid =""205"" ssid = ""24"">We evaluate the time and memory consumption of each data structure by computing perplexity on 4 billion tokens from the English Gigaword corpus (Parker et al.  2009).</S><S sid =""12"" ssid = ""7"">Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke  2002) is a popular toolkit based on tries used in several decoders.</S><S sid =""186"" ssid = ""5"">For sorted lookup  we compare interpolation search  standard C++ binary search  and standard C++ set based on red-black trees.</S><S sid =""171"" ssid = ""43"">Along with IRSTLM and TPT  our binary format is memory mapped  meaning the file and in-memory representation are the same.</S>",['Results_Citation']
20,W11-2123,P13-1109,0,"Heafield, 2011",0,"For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with KneserNey smoothing","For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with KneserNey smoothing","['199', '250', '12', '268', '233']","<S sid =""199"" ssid = ""18"">For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid =""250"" ssid = ""69"">The BerkeleyLM direct-mapped cache is in principle faster than caches implemented by RandLM and by IRSTLM  so we may write a C++ equivalent implementation as future work.</S><S sid =""12"" ssid = ""7"">Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke  2002) is a popular toolkit based on tries used in several decoders.</S><S sid =""268"" ssid = ""10"">For example  syntactic decoders (Koehn et al.  2007; Dyer et al.  2010; Li et al.  2009) perform dynamic programming parametrized by both backward- and forward-looking state.</S><S sid =""233"" ssid = ""52"">The authors provided us with a ratio between TPT and SRI under different conditions. aLossy compression with the same weights. bLossy compression with retuned weights. ditions make the value appropriate for estimating repeated run times  such as in parameter tuning.</S>",['Method_Citation']
