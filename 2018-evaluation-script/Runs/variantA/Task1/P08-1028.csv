Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P08-1028,D08-1094,0,2008,0,"Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p+ a as a function f operating on four components: c= f (p, a, R, K) (3) R is the relation holding between p and a, and Kadditional knowledge","Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p+ a as a function f operating on four components: c= f (p, a, R, K) (3) R is the relation holding between p and a, and K additional knowledge","['148', '66', '87', '10', '73']","<S sid =""148"" ssid = ""61"">In addition  Bullinaria and Levy (2007) found that these parameters perform well on a number of other tasks such as the synonymy task from the Test ofEnglish as a Foreign Language (TOEFL).</S><S sid =""66"" ssid = ""14"">To give a concrete example  circular convolution is an instance of the general multiplicative model which breaks this constraint by allowing uj to contribute to pi: For example  according to (5)  the addition of the two vectors representing horse and run in Figure 1 would yield horse + run = [1 14 6 14 4].</S><S sid =""87"" ssid = ""35"">Combining the multiplicative model with an additive model  which does not suffer from this problem  could mitigate this problem: pi = αui +βvi +γuivi (11) where α  β  and γ are weighting constants.</S><S sid =""10"" ssid = ""6"">Moreover  the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald  2000) and word association norms (Denhire and Lemaire  2004).</S><S sid =""73"" ssid = ""21"">Relaxing the assumption of symmetry in the case of the simple additive model produces a model which weighs the contribution of the two components differently: This allows additive models to become more syntax aware  since semantically important constituents can participate more actively in the composition.</S>",['Method_Citation']
4,P08-1028,P14-1060,0,2008,0,"While works such asthe SDSM model suffer from the problem of sparsity in composing structures beyond bi grams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstetteand Sadrzadeh (2011) are restricted by significant model biases in representing semantic com position by generic algebraic operations","While works such as the SDSM model suffer from the problem of sparsity in composing structures beyond bigrams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstette and Sadrzadeh (2011) are restricted by significant model biases in representing semantic com position by generic algebraic operations","['8', '9', '49', '10', '148']","<S sid =""8"" ssid = ""4"">Examples include automatic thesaurus extraction (Grefenstette  1994)  word sense discrimination (Sch¨utze  1998) and disambiguation (McCarthy et al.  2004)  collocation extraction (Schone and Jurafsky  2001)  text segmentation (Choi et al.  2001)   and notably information retrieval (Salton et al.  1975).</S><S sid =""9"" ssid = ""5"">In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess  1996; Landauer and Dumais  1997) and text comprehension (Landauer and Dumais  1997; Foltz et al.  1998).</S><S sid =""49"" ssid = ""22"">The neighbors  Kintsch argues  can ‘strengthen features of the predicate that are appropriate for the argument of the predication’. animal stable village gallop jokey horse 0 6 2 10 4 run 1 8 4 4 0 Unfortunately  comparisons across vector composition models have been few and far between in the literature.</S><S sid =""10"" ssid = ""6"">Moreover  the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald  2000) and word association norms (Denhire and Lemaire  2004).</S><S sid =""148"" ssid = ""61"">In addition  Bullinaria and Levy (2007) found that these parameters perform well on a number of other tasks such as the synonymy task from the Test ofEnglish as a Foreign Language (TOEFL).</S>",['Method_Citation']
6,P08-1028,P10-1097,0,2008,0,"Mitchell and Lapata (2008), henceforth M& amp; L, propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression","Mitchell and Lapata (2008), henceforth M& amp; L, propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression","['148', '10', '201', '133', '89']","<S sid =""148"" ssid = ""61"">In addition  Bullinaria and Levy (2007) found that these parameters perform well on a number of other tasks such as the synonymy task from the Test ofEnglish as a Foreign Language (TOEFL).</S><S sid =""10"" ssid = ""6"">Moreover  the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald  2000) and word association norms (Denhire and Lemaire  2004).</S><S sid =""201"" ssid = ""13"">We intend to assess the potential of our composition models on context sensitive semantic priming (Till et al.  1988) and inductive inference (Heit and Rubinstein  1994).</S><S sid =""133"" ssid = ""46"">We employed leave-one-out resampling (Weiss and Kulikowski  1991)  by correlating the data obtained from each participant with the ratings obtained from all other participants.</S><S sid =""89"" ssid = ""2"">In his study  Kintsch builds a model of how a verb’s meaning is modified in the context of its subject.</S>",['Method_Citation']
7,P08-1028,P10-1097,0,2008,0,"Interestingly, Mitchell and Lapata (2008) came to the same result in a different setting","Interestingly, Mitchell and Lapata (2008) came to the same result in a different setting","['148', '133', '118', '201', '98']","<S sid =""148"" ssid = ""61"">In addition  Bullinaria and Levy (2007) found that these parameters perform well on a number of other tasks such as the synonymy task from the Test ofEnglish as a Foreign Language (TOEFL).</S><S sid =""133"" ssid = ""46"">We employed leave-one-out resampling (Weiss and Kulikowski  1991)  by correlating the data obtained from each participant with the ratings obtained from all other participants.</S><S sid =""118"" ssid = ""31"">Procedure and Subjects Participants first saw a set of instructions that explained the sentence similarity task and provided several examples.</S><S sid =""201"" ssid = ""13"">We intend to assess the potential of our composition models on context sensitive semantic priming (Till et al.  1988) and inductive inference (Heit and Rubinstein  1994).</S><S sid =""98"" ssid = ""11"">Unfortunately  Kintsch (2001) demonstrates how his own composition algorithm works intuitively on a few hand selected examples but does not provide a comprehensive test set.</S>",['Method_Citation']
8,P08-1028,D11-1094,0,2008,0,"And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to combine the constituent vectors","And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to combine the constituent vectors","['148', '133', '201', '118', '10']","<S sid =""148"" ssid = ""61"">In addition  Bullinaria and Levy (2007) found that these parameters perform well on a number of other tasks such as the synonymy task from the Test ofEnglish as a Foreign Language (TOEFL).</S><S sid =""133"" ssid = ""46"">We employed leave-one-out resampling (Weiss and Kulikowski  1991)  by correlating the data obtained from each participant with the ratings obtained from all other participants.</S><S sid =""201"" ssid = ""13"">We intend to assess the potential of our composition models on context sensitive semantic priming (Till et al.  1988) and inductive inference (Heit and Rubinstein  1994).</S><S sid =""118"" ssid = ""31"">Procedure and Subjects Participants first saw a set of instructions that explained the sentence similarity task and provided several examples.</S><S sid =""10"" ssid = ""6"">Moreover  the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald  2000) and word association norms (Denhire and Lemaire  2004).</S>",['Method_Citation']
9,P08-1028,W11-0131,0,2008,0,"Mitchell and Lapata (2008) provide a general framework for semantic vector composition: p= f (u, v, R, K) (1) 295 where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base, and p is a resulting composed vector (or tensor)","Mitchell and Lapata (2008) provide a general framework for semantic vector composition: p= f (u, v, R, K) (1) 295 where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base, and p is a resulting composed vector (or tensor)","['35', '66', '8', '54', '53']","<S sid =""35"" ssid = ""8"">The tensor product u ® v is a matrix whose components are all the possible products uivj of the components of vectors u and v. A major difficulty with tensor products is their dimensionality which is higher than the dimensionality of the original vectors (precisely  the tensor product has dimensionality m x n).</S><S sid =""66"" ssid = ""14"">To give a concrete example  circular convolution is an instance of the general multiplicative model which breaks this constraint by allowing uj to contribute to pi: For example  according to (5)  the addition of the two vectors representing horse and run in Figure 1 would yield horse + run = [1 14 6 14 4].</S><S sid =""8"" ssid = ""4"">Examples include automatic thesaurus extraction (Grefenstette  1994)  word sense discrimination (Sch¨utze  1998) and disambiguation (McCarthy et al.  2004)  collocation extraction (Schone and Jurafsky  2001)  text segmentation (Choi et al.  2001)   and notably information retrieval (Salton et al.  1975).</S><S sid =""54"" ssid = ""2"">The construction of the semantic space depends on the definition of linguistic context (e.g.  neighbouring words can be documents or collocations)  the number of components used (e.g.  the k most frequent words in a corpus)  and their values (e.g.  as raw co-occurrence frequencies or ratios of probabilities).</S><S sid =""53"" ssid = ""1"">We formulate semantic composition as a function of two vectors  u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word’s vector typically represents its co-occurrence with neighboring words.</S>",['Method_Citation']
10,P08-1028,W11-0131,0,2008,0,"As Mitchell and Lapata (2008) did, let us temporarily suspend discussion on what semantics populate our vectors for now","As Mitchell and Lapata (2008) did, let us temporarily suspend discussion on what semantics populate our vectors for now","['118', '10', '133', '201', '159']","<S sid =""118"" ssid = ""31"">Procedure and Subjects Participants first saw a set of instructions that explained the sentence similarity task and provided several examples.</S><S sid =""10"" ssid = ""6"">Moreover  the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald  2000) and word association norms (Denhire and Lemaire  2004).</S><S sid =""133"" ssid = ""46"">We employed leave-one-out resampling (Weiss and Kulikowski  1991)  by correlating the data obtained from each participant with the ratings obtained from all other participants.</S><S sid =""201"" ssid = ""13"">We intend to assess the potential of our composition models on context sensitive semantic priming (Till et al.  1988) and inductive inference (Heit and Rubinstein  1994).</S><S sid =""159"" ssid = ""72"">In our experiments we selected parameters that Kintsch reports as optimal.</S>",['Method_Citation']
11,P08-1028,P13-2083,0,2008,0,"Mitchell and Lapata (2008) propose a framework to define the composition c= f (a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition","Mitchell and Lapata (2008) propose a framework to define the composition c= f (a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition","['148', '47', '133', '10', '201']","<S sid =""148"" ssid = ""61"">In addition  Bullinaria and Levy (2007) found that these parameters perform well on a number of other tasks such as the synonymy task from the Test ofEnglish as a Foreign Language (TOEFL).</S><S sid =""47"" ssid = ""20"">Kintsch (2001) proposes a variation on the vector addition theme in an attempt to model how the meaning of a predicate (e.g.  run) varies depending on the arguments it operates upon (e.g  the horse ran vs. the color ran).</S><S sid =""133"" ssid = ""46"">We employed leave-one-out resampling (Weiss and Kulikowski  1991)  by correlating the data obtained from each participant with the ratings obtained from all other participants.</S><S sid =""10"" ssid = ""6"">Moreover  the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald  2000) and word association norms (Denhire and Lemaire  2004).</S><S sid =""201"" ssid = ""13"">We intend to assess the potential of our composition models on context sensitive semantic priming (Till et al.  1988) and inductive inference (Heit and Rubinstein  1994).</S>",['Method_Citation']
12,P08-1028,P13-2083,0,"Mitchell and Lapata, 2008",0,"As our final set of baselines, we extend two simple techniques proposed by (Mitchell and Lapata, 2008) that use element-wise addition and multiplication operators to perform composition","As our final set of baselines, we extend two simple techniques proposed by (Mitchell and Lapata, 2008) that use element-wise addition and multiplication operators to perform composition","['198', '44', '11', '76', '197']","<S sid =""198"" ssid = ""10"">In particular  the general class of multiplicative models (see equation (4)) appears to be a fruitful area to explore.</S><S sid =""44"" ssid = ""17"">For example  assuming that individual words are represented by vectors  we can compute the meaning of a sentence by taking their mean (Foltz et al.  1998; Landauer and Dumais  1997).</S><S sid =""11"" ssid = ""7"">Despite their widespread use  vector-based models are typically directed at representing words in isolation and methods for constructing representations for phrases or sentences have received little attention in the literature.</S><S sid =""76"" ssid = ""24"">The models considered so far assume that components do not ‘interfere’ with each other  i.e.  that It is also possible to re-introduce the dependence on K into the model of vector composition.</S><S sid =""197"" ssid = ""9"">We anticipate that more substantial correlations can be achieved by implementing more sophisticated models from within the framework outlined here.</S>",['Results_Citation']
13,P08-1028,P10-1021,0,2008,0,"Althoughthis model has been shown to successfully simulate single and multiple-word priming (McDonald and Brew 2004), it failed to predict processing costs in the Embra eye-tracking corpus (McDonald and Shillcock 2003) .In this work we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008)","Although this model has been shown to successfully simulate single and multiple-word priming (McDonald and Brew 2004), it failed to predict processing costs in the Embra eye-tracking corpus (McDonald and Shillcock 2003). In this work we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008)","['8', '9', '19', '49', '53']","<S sid =""8"" ssid = ""4"">Examples include automatic thesaurus extraction (Grefenstette  1994)  word sense discrimination (Sch¨utze  1998) and disambiguation (McCarthy et al.  2004)  collocation extraction (Schone and Jurafsky  2001)  text segmentation (Choi et al.  2001)   and notably information retrieval (Salton et al.  1975).</S><S sid =""9"" ssid = ""5"">In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess  1996; Landauer and Dumais  1997) and text comprehension (Landauer and Dumais  1997; Foltz et al.  1998).</S><S sid =""19"" ssid = ""15"">While vector addition has been effective in some applications such as essay grading (Landauer and Dumais  1997) and coherence assessment (Foltz et al.  1998)  there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al.  1991; West and Stanovich  1986) and modulate cognitive behavior in sentence priming (Till et al.  1988) and inference tasks (Heit and Rubinstein  1994).</S><S sid =""49"" ssid = ""22"">The neighbors  Kintsch argues  can ‘strengthen features of the predicate that are appropriate for the argument of the predication’. animal stable village gallop jokey horse 0 6 2 10 4 run 1 8 4 4 0 Unfortunately  comparisons across vector composition models have been few and far between in the literature.</S><S sid =""53"" ssid = ""1"">We formulate semantic composition as a function of two vectors  u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word’s vector typically represents its co-occurrence with neighboring words.</S>",['Method_Citation']
14,P08-1028,P10-1021,0,2008,0,"Assuming that h is a linear function of the Cartesian product of u and v allows to specify additive models which are by far the most common method of vector combination in the literature: hi =ui+ vi (3) Alternatively, we can assume that h is a linear function of the tensor product of u and v, and thus derive models based on multiplication: hi =ui? vi (4) Mitchell and Lapata (2008) show that several additive and multiplicative models can be formulated under this framework, including the well known tensor products (Smolensky 1990) and circular convolution (Plate 1995)","Mitchell and Lapata (2008) show that several additive and multiplicative models can be formulated under this framework, including the well known tensor products (Smolensky 1990) and circular convolution (Plate 1995)","['148', '133', '201', '47', '10']","<S sid =""148"" ssid = ""61"">In addition  Bullinaria and Levy (2007) found that these parameters perform well on a number of other tasks such as the synonymy task from the Test ofEnglish as a Foreign Language (TOEFL).</S><S sid =""133"" ssid = ""46"">We employed leave-one-out resampling (Weiss and Kulikowski  1991)  by correlating the data obtained from each participant with the ratings obtained from all other participants.</S><S sid =""201"" ssid = ""13"">We intend to assess the potential of our composition models on context sensitive semantic priming (Till et al.  1988) and inductive inference (Heit and Rubinstein  1994).</S><S sid =""47"" ssid = ""20"">Kintsch (2001) proposes a variation on the vector addition theme in an attempt to model how the meaning of a predicate (e.g.  run) varies depending on the arguments it operates upon (e.g  the horse ran vs. the color ran).</S><S sid =""10"" ssid = ""6"">Moreover  the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald  2000) and word association norms (Denhire and Lemaire  2004).</S>",['Method_Citation']
15,P08-1028,W11-0115,0,2008,0,"Mitchell and Lapata (2008) introduce a whole family of models of compositionality based on vector addition and point wise-multiplication (and a weighted combination of both), evaluated on a sentence similarity task inspired by Kintsch (2001)","Mitchell and Lapata (2008) introduce a whole family of models of compositionality based on vector addition and point wise-multiplication (and a weighted combination of both), evaluated on a sentence similarity task inspired by Kintsch (2001)","['47', '148', '133', '201', '10']","<S sid =""47"" ssid = ""20"">Kintsch (2001) proposes a variation on the vector addition theme in an attempt to model how the meaning of a predicate (e.g.  run) varies depending on the arguments it operates upon (e.g  the horse ran vs. the color ran).</S><S sid =""148"" ssid = ""61"">In addition  Bullinaria and Levy (2007) found that these parameters perform well on a number of other tasks such as the synonymy task from the Test ofEnglish as a Foreign Language (TOEFL).</S><S sid =""133"" ssid = ""46"">We employed leave-one-out resampling (Weiss and Kulikowski  1991)  by correlating the data obtained from each participant with the ratings obtained from all other participants.</S><S sid =""201"" ssid = ""13"">We intend to assess the potential of our composition models on context sensitive semantic priming (Till et al.  1988) and inductive inference (Heit and Rubinstein  1994).</S><S sid =""10"" ssid = ""6"">Moreover  the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald  2000) and word association norms (Denhire and Lemaire  2004).</S>",['Method_Citation']
16,P08-1028,W11-0115,0,2008,0,"The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression","The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression","['47', '201', '49', '10', '133']","<S sid =""47"" ssid = ""20"">Kintsch (2001) proposes a variation on the vector addition theme in an attempt to model how the meaning of a predicate (e.g.  run) varies depending on the arguments it operates upon (e.g  the horse ran vs. the color ran).</S><S sid =""201"" ssid = ""13"">We intend to assess the potential of our composition models on context sensitive semantic priming (Till et al.  1988) and inductive inference (Heit and Rubinstein  1994).</S><S sid =""49"" ssid = ""22"">The neighbors  Kintsch argues  can ‘strengthen features of the predicate that are appropriate for the argument of the predication’. animal stable village gallop jokey horse 0 6 2 10 4 run 1 8 4 4 0 Unfortunately  comparisons across vector composition models have been few and far between in the literature.</S><S sid =""10"" ssid = ""6"">Moreover  the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald  2000) and word association norms (Denhire and Lemaire  2004).</S><S sid =""133"" ssid = ""46"">We employed leave-one-out resampling (Weiss and Kulikowski  1991)  by correlating the data obtained from each participant with the ratings obtained from all other participants.</S>",['Method_Citation']
17,P08-1028,W11-0115,0,2008,0,"For example, Mitchell and Lapata (2008) use their models to approximate the human ratings in their sentence similarity dataset","For example, Mitchell and Lapata (2008) use their models to approximate the human ratings in their sentence similarity dataset","['11', '10', '137', '201', '14']","<S sid =""11"" ssid = ""7"">Despite their widespread use  vector-based models are typically directed at representing words in isolation and methods for constructing representations for phrases or sentences have received little attention in the literature.</S><S sid =""10"" ssid = ""6"">Moreover  the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald  2000) and word association norms (Denhire and Lemaire  2004).</S><S sid =""137"" ssid = ""50"">More evidence that this is not an easy task comes from Figure 2 where we observe some overlap in the ratings for High and Low similarity items.</S><S sid =""201"" ssid = ""13"">We intend to assess the potential of our composition models on context sensitive semantic priming (Till et al.  1988) and inductive inference (Heit and Rubinstein  1994).</S><S sid =""14"" ssid = ""10"">This is illustrated in the example below taken from Landauer et al. (1997).</S>",['Method_Citation']
18,P08-1028,W11-1310,0,2008,0,We use other WSM settings following Mitchell and Lapata (2008),We use other WSM settings following Mitchell and Lapata (2008),"['118', '92', '88', '133', '201']","<S sid =""118"" ssid = ""31"">Procedure and Subjects Participants first saw a set of instructions that explained the sentence similarity task and provided several examples.</S><S sid =""92"" ssid = ""5"">To quantify this shift  Kintsch proposes measuring similarity relative to other verbs acting as landmarks  for example gallop and dissolve.</S><S sid =""88"" ssid = ""1"">We evaluated the models presented in Section 3 on a sentence similarity task initially proposed by Kintsch (2001).</S><S sid =""133"" ssid = ""46"">We employed leave-one-out resampling (Weiss and Kulikowski  1991)  by correlating the data obtained from each participant with the ratings obtained from all other participants.</S><S sid =""201"" ssid = ""13"">We intend to assess the potential of our composition models on context sensitive semantic priming (Till et al.  1988) and inductive inference (Heit and Rubinstein  1994).</S>",['Method_Citation']
19,P08-1028,W11-1310,0,2008,0,Mitchell and Lapata (2008) observed that a simple multiplication function modelled compositionality better than addition,Mitchell and Lapata (2008) observed that a simple multiplication function modelled compositionality better than addition,"['148', '201', '98', '47', '10']","<S sid =""148"" ssid = ""61"">In addition  Bullinaria and Levy (2007) found that these parameters perform well on a number of other tasks such as the synonymy task from the Test ofEnglish as a Foreign Language (TOEFL).</S><S sid =""201"" ssid = ""13"">We intend to assess the potential of our composition models on context sensitive semantic priming (Till et al.  1988) and inductive inference (Heit and Rubinstein  1994).</S><S sid =""98"" ssid = ""11"">Unfortunately  Kintsch (2001) demonstrates how his own composition algorithm works intuitively on a few hand selected examples but does not provide a comprehensive test set.</S><S sid =""47"" ssid = ""20"">Kintsch (2001) proposes a variation on the vector addition theme in an attempt to model how the meaning of a predicate (e.g.  run) varies depending on the arguments it operates upon (e.g  the horse ran vs. the color ran).</S><S sid =""10"" ssid = ""6"">Moreover  the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald  2000) and word association norms (Denhire and Lemaire  2004).</S>",['Method_Citation']
20,P08-1028,W11-1310,0,"Mitchell and Lapata, 2008",0,"We use the compositionality functions, simple addition and simple multiplication to build compositional vectors Vwr1+wr2 and Vwr1 ?wr2. These are as described in (Mitchell and Lapata, 2008)","We use the compositionality functions, simple addition and simple multiplication to build compositional vectors Vwr1+wr2 and Vwr1 ?wr2. These are as described in (Mitchell and Lapata, 2008)","['58', '66', '87', '56', '60']","<S sid =""58"" ssid = ""6"">We define a general class of models for this process of composition as: The expression above allows us to derive models for which p is constructed in a distinct space from u and v  as is the case for tensor products.</S><S sid =""66"" ssid = ""14"">To give a concrete example  circular convolution is an instance of the general multiplicative model which breaks this constraint by allowing uj to contribute to pi: For example  according to (5)  the addition of the two vectors representing horse and run in Figure 1 would yield horse + run = [1 14 6 14 4].</S><S sid =""87"" ssid = ""35"">Combining the multiplicative model with an additive model  which does not suffer from this problem  could mitigate this problem: pi = αui +βvi +γuivi (11) where α  β  and γ are weighting constants.</S><S sid =""56"" ssid = ""4"">Here  the space has only five dimensions  and the matrix cells denote the co-occurrence of the target words (horse and run) with the context words animal  stable  and so on.</S><S sid =""60"" ssid = ""8"">To derive specific models from this general framework requires the identification of appropriate constraints to narrow the space of functions being considered.</S>",['Method_Citation']
