Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,J01-2004,W05-0104,0,2001,0,"Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark, see Roark (2001))","Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark, see Roark (2001))","['282', '103', '399', '376', '396']","<S sid =""282"" ssid = ""38"">Like the nonlexicalized parser in Roark and Johnson (1999)  we found that the search efficiency  in terms of number of rule expansions considered or number of analyses advanced  also improved as we increased the amount of conditioning.</S><S sid =""103"" ssid = ""7"">There are efficient algorithms in the literature (Jelinek and Lafferty 1991; Stolcke 1995) for calculating exact string prefix probabilities given a PCFG.</S><S sid =""399"" ssid = ""12"">In addition  as mentioned above  we would like to further test our language model in speech recognition tasks  to see if the perplexity improvement that we have seen can lead to significant reductions in word error rate.</S><S sid =""376"" ssid = ""132"">This contrasts with our perplexity results reported above  as well as with the recognition experiments in Chelba (2000)  where the best results resulted from interpolated models.</S><S sid =""396"" ssid = ""9"">Second  there are advantages to top-down parsing that have not been examined to date  e.g.  empty categories.</S>",['Method_Citation']
2,J01-2004,P08-1013,0,2001,0,"Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition. These models have in common that they explicitly or implicitly use a context-free grammar induced from a tree bank, with the exception of Chelba and Jelinek (2000)",Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition,"['97', '18', '209', '16', '134']","<S sid =""97"" ssid = ""1"">There have been attempts to jump over adjacent words to words farther back in the left context  without the use of dependency links or syntactic structure  for example Saul and Pereira (1997) and Rosenfeld (1996  1997).</S><S sid =""18"" ssid = ""6"">Some of the recent literature investigating approaches that use syntactic structure in an attempt to capture long-distance dependencies for language modeling will be reviewed.</S><S sid =""209"" ssid = ""113"">This parser is essentially a stochastic version of the top-down parser described in Aho  Sethi  and Ullman (1986).</S><S sid =""16"" ssid = ""4"">While language models built around shallow local dependencies are still the standard in state-of-the-art speech recognition systems  there is reason to hope that better language models can and will be developed by computational linguists for this task.</S><S sid =""134"" ssid = ""38"">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S>",['Method_Citation']
4,J01-2004,P04-1015,0,"Roark, 2001a",0,"Theperceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn tree bank","The perceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn treebank","['209', '138', '76', '99', '141']","<S sid =""209"" ssid = ""113"">This parser is essentially a stochastic version of the top-down parser described in Aho  Sethi  and Ullman (1986).</S><S sid =""138"" ssid = ""42"">Our approach is found to yield very accurate parses efficiently  and  in addition  to lend itself straightforwardly to estimating word probabilities on-line  that is  in a single pass from left to right.</S><S sid =""76"" ssid = ""34"">The leftfactorization transform that we use is identical to what is called right binarization in Roark and Johnson (1999).</S><S sid =""99"" ssid = ""3"">These can be divided into two rough groups: those that use the grammar as a language model  and those that use a parser to uncover phrasal heads standing in an important relation (c-command) to the current word.</S><S sid =""141"" ssid = ""45"">We will then present empirical results in two domains: one to compare with previous work in the parsing literature  and the other to compare with previous work using parsing for language modeling for speech recognition  in particular with the Chelba and Jelinek results mentioned above.</S>",['Method_Citation']
5,J01-2004,P04-1015,0,"Roark, 2001a",0,"We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the per ceptron model gives performance competitive to that of the generative model on parsing the Penn tree bank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search","We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the perceptron model gives performance competitive to that of the generative model on parsing the Penn treebank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search","['133', '151', '317', '100', '402']","<S sid =""133"" ssid = ""37"">Statistically based heuristic best-first or beam-search strategies (Caraballo and Charniak 1998; Charniak  Goldwater  and Johnson 1998; Goodman 1997) have yielded an enormous improvement in the quality and speed of parsers  even without any guarantee that the parse returned is  in fact  that with the maximum likelihood for the probability model.</S><S sid =""151"" ssid = ""55"">Notice  however  that in this case the annotations on the righthand side are predictable from the annotation on the left-hand side (unlike  for example  bilexical grammars)  so that the relative frequency estimator yields conditional probability distributions of the original rules  given the parent of the left-hand side.</S><S sid =""317"" ssid = ""73"">Note  however  that this renormalization factor is necessarily less than one  and thus would uniformly increase each word's probability under the model  that is  any perplexity results reported below will be higher than the &quot;true&quot; perplexity that would be assigned with a properly normalized distribution.</S><S sid =""100"" ssid = ""4"">The approach that we will subsequently present uses the probabilistic grammar as its language model  but only includes probability mass from those parses that are found  that is  it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S><S sid =""402"" ssid = ""15"">In fact  left-corner parsing can be simulated by a top-down parser by transforming the grammar  as was done in Roark and Johnson (1999)  and so an approach very similar to the one outlined here could be used in that case.</S>",['Method_Citation']
6,J01-2004,P04-1015,0,2001a,0,"One way around this problem is to adopt a two-pass approach, where GEN (x) is the top N analyses under some initial model, as in the re ranking approach of Collins (2000) .In the current paper we explore alternatives to rerank ing approaches, namely heuristic methods for finding the arg max, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999)","In the current paper we explore alternatives to reranking approaches, namely heuristic methods for finding the argmax, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999)","['134', '282', '399', '263', '97']","<S sid =""134"" ssid = ""38"">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid =""282"" ssid = ""38"">Like the nonlexicalized parser in Roark and Johnson (1999)  we found that the search efficiency  in terms of number of rule expansions considered or number of analyses advanced  also improved as we increased the amount of conditioning.</S><S sid =""399"" ssid = ""12"">In addition  as mentioned above  we would like to further test our language model in speech recognition tasks  to see if the perplexity improvement that we have seen can lead to significant reductions in word error rate.</S><S sid =""263"" ssid = ""19"">Following standard practice  we will be reporting scores only for non-part-of-speech constituents  which are called labeled recall (LR) and labeled precision (LP).</S><S sid =""97"" ssid = ""1"">There have been attempts to jump over adjacent words to words farther back in the left context  without the use of dependency links or syntactic structure  for example Saul and Pereira (1997) and Rosenfeld (1996  1997).</S>",['Method_Citation']
7,J01-2004,P04-1015,0,2001a,0,"approach The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights","The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights","['209', '175', '192', '294', '112']","<S sid =""209"" ssid = ""113"">This parser is essentially a stochastic version of the top-down parser described in Aho  Sethi  and Ullman (1986).</S><S sid =""175"" ssid = ""79"">This is a sort of decision tree for a tree-walking algorithm to decide what value to return  for a given partial tree and a given depth.</S><S sid =""192"" ssid = ""96"">The exception is the function at level 5 along the left branch of the tree in Figure 4.</S><S sid =""294"" ssid = ""50"">Figure 6 shows the observed time at our standard base beam of 10-11 with the full conditioning regimen  alongside an approximation of the reported observed (linear) time in Ratnaparkhi (1997).</S><S sid =""112"" ssid = ""16"">Goddeau (1992) used a robust deterministic shift-reduce parser to condition word probabilities by extracting a specified number of stack entries from the top of the current state  and conditioning on those entries in a way similar to an n-gram.</S>",['Method_Citation']
9,J01-2004,P04-1015,0,2001a,0,"Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word","Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word","['399', '282', '75', '242', '306']","<S sid =""399"" ssid = ""12"">In addition  as mentioned above  we would like to further test our language model in speech recognition tasks  to see if the perplexity improvement that we have seen can lead to significant reductions in word error rate.</S><S sid =""282"" ssid = ""38"">Like the nonlexicalized parser in Roark and Johnson (1999)  we found that the search efficiency  in terms of number of rule expansions considered or number of analyses advanced  also improved as we increased the amount of conditioning.</S><S sid =""75"" ssid = ""33"">In effect  this is an underspecification of some of the predictions that our top-down parser is making about the rest of the string.</S><S sid =""242"" ssid = ""146"">As mentioned in Section 2.1  we left-factor the grammar  so that all productions are binary  except those with a single terminal on the right-hand side and epsilon productions.</S><S sid =""306"" ssid = ""62"">Recall the discussion of the grammar models above  and our definition of the set of partial derivations Dw  with respect to a prefix string wil) (see Equations 2 and 7).</S>",['Method_Citation']
10,J01-2004,P05-1022,0,"Roark, 2001",0,"A good example of this is the Roark parser (Roark, 2001) which works left-to right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning","A good example of this is the Roark parser (Roark, 2001) which works left-to-right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning","['79', '345', '108', '399', '291']","<S sid =""79"" ssid = ""37"">This underspecification of the nonterminal predictions (e.g.  VP-VBD in the example in Figure 2  as opposed to NP)  allows lexical items to become part of the left context  and so be used to condition production probabilities  even the production probabilities of constituents that dominate them in the unfactored tree.</S><S sid =""345"" ssid = ""101"">This is not surprising  since our conditioning information is in many ways orthogonal to that of the trigram  insofar as it includes the probability mass of the derivations; in contrast  their model in some instances is very close to the trigram  by conditioning on two words in the prefix string  which may happen to be the two adjacent words.</S><S sid =""108"" ssid = ""12"">Another approach that uses syntactic structure for language modeling has been to use a shift-reduce parser to &quot;surface&quot; c-commanding phrasal headwords or part-of-speech (POS) tags from arbitrarily far back in the prefix string  for use in a trigram-like model.</S><S sid =""399"" ssid = ""12"">In addition  as mentioned above  we would like to further test our language model in speech recognition tasks  to see if the perplexity improvement that we have seen can lead to significant reductions in word error rate.</S><S sid =""291"" ssid = ""47"">Also  the parser returns a set of candidate parses  from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence)  we find an average labeled precision/recall of 94.1  for sentences of length < 100.</S>",['Method_Citation']
11,J01-2004,P05-1022,0,"Roark, 2001",0,"At the end one has a beam-width? s number of best parses (Roark, 2001) .The Collins parser (Collins, 1997) does use dynamic programming in its search","At the end one has a beam-width's number of best parses (Roark, 2001)","['309', '323', '258', '356', '283']","<S sid =""309"" ssid = ""65"">Let Ht be the priority queue H  before any processing has begun with word w  in the look-ahead.</S><S sid =""323"" ssid = ""79"">They trained and tested the SLM on a modified  more &quot;speech-like&quot; version of the Penn Treebank.</S><S sid =""258"" ssid = ""14"">For example  in Figure 1(a)  there is a VP that spans the words &quot;chased the ball&quot;.</S><S sid =""356"" ssid = ""112"">The DARPA '93 HUB1 test setup consists of 213 utterances read from the Wall Street Journal  a total of 3 446 words.</S><S sid =""283"" ssid = ""39"">Unlike the Roark and Johnson parser  however  our coverage did not substantially drop as the amount of conditioning information increased  and in some cases  coverage improved slightly.</S>",['Results_Citation']
12,J01-2004,P05-1022,0,"Roark, 2001",0,"To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses","To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses","['399', '263', '121', '282', '242']","<S sid =""399"" ssid = ""12"">In addition  as mentioned above  we would like to further test our language model in speech recognition tasks  to see if the perplexity improvement that we have seen can lead to significant reductions in word error rate.</S><S sid =""263"" ssid = ""19"">Following standard practice  we will be reporting scores only for non-part-of-speech constituents  which are called labeled recall (LR) and labeled precision (LP).</S><S sid =""121"" ssid = ""25"">When there is no more parser work to be done (or  in their case  when the beam is full)  the following word is predicted.</S><S sid =""282"" ssid = ""38"">Like the nonlexicalized parser in Roark and Johnson (1999)  we found that the search efficiency  in terms of number of rule expansions considered or number of analyses advanced  also improved as we increased the amount of conditioning.</S><S sid =""242"" ssid = ""146"">As mentioned in Section 2.1  we left-factor the grammar  so that all productions are binary  except those with a single terminal on the right-hand side and epsilon productions.</S>",['Method_Citation']
13,J01-2004,P04-1006,0,"Roark, 2001",0,"We ran the first stage parser with 4-timesoverparsing for each string in 7The n? best lists were provided by Brian Roark (Roark, 2001) 8A local-tree is an explicit expansion of an edge and its children","The n-best lists were provided by Brian Roark (Roark, 2001)","['338', '396', '192', '356', '263']","<S sid =""338"" ssid = ""94"">Table 4 compares the perplexity of our model with Chelba and Jelinek (1998a  1998b) on the same training and testing corpora.</S><S sid =""396"" ssid = ""9"">Second  there are advantages to top-down parsing that have not been examined to date  e.g.  empty categories.</S><S sid =""192"" ssid = ""96"">The exception is the function at level 5 along the left branch of the tree in Figure 4.</S><S sid =""356"" ssid = ""112"">The DARPA '93 HUB1 test setup consists of 213 utterances read from the Wall Street Journal  a total of 3 446 words.</S><S sid =""263"" ssid = ""19"">Following standard practice  we will be reporting scores only for non-part-of-speech constituents  which are called labeled recall (LR) and labeled precision (LP).</S>",['Method_Citation']
14,J01-2004,P05-1063,0,"Roark, 2001a",0,"Incremental top down and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models","Incremental top-down and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models","['0', '134', '402', '97', '43']","<S sid =""0"" ssid = ""0"">Probabilistic Top-Down Parsing and Language Modeling</S><S sid =""134"" ssid = ""38"">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid =""402"" ssid = ""15"">In fact  left-corner parsing can be simulated by a top-down parser by transforming the grammar  as was done in Roark and Johnson (1999)  and so an approach very similar to the one outlined here could be used in that case.</S><S sid =""97"" ssid = ""1"">There have been attempts to jump over adjacent words to words farther back in the left context  without the use of dependency links or syntactic structure  for example Saul and Pereira (1997) and Rosenfeld (1996  1997).</S><S sid =""43"" ssid = ""1"">This section will introduce probabilistic (or stochastic) context-free grammars (PCFGs)  as well as such notions as complete and partial parse trees  which will be important in defining our language model later in the paper.'</S>",['Method_Citation']
15,J01-2004,W10-2009,0,"Roark, 2001",0,"Levy, on the other hand ,argued that studies of probabilistic parsing reveal that typically a small number of analyses are as signed the majority of probability mass (Roark, 2001)","Levy, on the other hand, argued that studies of probabilistic parsing reveal that typically a small number of analyses are assigned the majority of probability mass (Roark, 2001)","['376', '209', '299', '103', '396']","<S sid =""376"" ssid = ""132"">This contrasts with our perplexity results reported above  as well as with the recognition experiments in Chelba (2000)  where the best results resulted from interpolated models.</S><S sid =""209"" ssid = ""113"">This parser is essentially a stochastic version of the top-down parser described in Aho  Sethi  and Ullman (1986).</S><S sid =""299"" ssid = ""55"">Furthermore  this is quite a large beam (see discussion below)  so that very large improvements in efficiency can be had at the expense of the number of analyses that are retained.</S><S sid =""103"" ssid = ""7"">There are efficient algorithms in the literature (Jelinek and Lafferty 1991; Stolcke 1995) for calculating exact string prefix probabilities given a PCFG.</S><S sid =""396"" ssid = ""9"">Second  there are advantages to top-down parsing that have not been examined to date  e.g.  empty categories.</S>",['Method_Citation']
17,J01-2004,D09-1034,0,2001,0,"For example, in Demberg and Keller (2008), trials were run deriving surprisal from the Roark (2001) parser under two different conditions: fully lex icalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags)","For example, in Demberg and Keller (2008), trials were run deriving surprisal from the Roark (2001) parser under two different conditions: fully lexicalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags)","['79', '108', '345', '291', '245']","<S sid =""79"" ssid = ""37"">This underspecification of the nonterminal predictions (e.g.  VP-VBD in the example in Figure 2  as opposed to NP)  allows lexical items to become part of the left context  and so be used to condition production probabilities  even the production probabilities of constituents that dominate them in the unfactored tree.</S><S sid =""108"" ssid = ""12"">Another approach that uses syntactic structure for language modeling has been to use a shift-reduce parser to &quot;surface&quot; c-commanding phrasal headwords or part-of-speech (POS) tags from arbitrarily far back in the prefix string  for use in a trigram-like model.</S><S sid =""345"" ssid = ""101"">This is not surprising  since our conditioning information is in many ways orthogonal to that of the trigram  insofar as it includes the probability mass of the derivations; in contrast  their model in some instances is very close to the trigram  by conditioning on two words in the prefix string  which may happen to be the two adjacent words.</S><S sid =""291"" ssid = ""47"">Also  the parser returns a set of candidate parses  from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence)  we find an average labeled precision/recall of 94.1  for sentences of length < 100.</S><S sid =""245"" ssid = ""1"">The empirical results will be presented in three stages: (i) trials to examine the accuracy and efficiency of the parser; (ii) trials to examine its effect on test corpus perplexity and recognition performance; and (iii) trials to examine the effect of beam variation on these performance measures.</S>",['Method_Citation']
18,J01-2004,D09-1034,0,2001,0,"We modified the Roark (2001) parser to calculate the discussed measures 1, and the empirical results in? 4 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time","We modified the Roark (2001) parser to calculate the discussed measures, and the empirical results in ?4 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time","['115', '93', '145', '83', '277']","<S sid =""115"" ssid = ""19"">The structured language model (SLM) used in Chelba and Jelinek (1998a  1998b  1999)  Jelinek and Chelba (1999)  and Chelba (2000) is similar to that of Goddeau  except that (i) their shift-reduce parser follows a nondeterministic beam search  and (ii) each stack entry contains  in addition to the nonterminal node label  the headword of the constituent.</S><S sid =""93"" ssid = ""51"">The standard language model used in many speech recognition systems is the trigram model  i.e.  a Markov model of order 2  which can be characterized by the following equation: To smooth the trigram models that are used in this paper  we interpolate the probability estimates of higher-order Markov models with lower-order Markov models (Jelinek and Mercer 1980).</S><S sid =""145"" ssid = ""49"">Examples of this are bilexical grammars—such as Eisner and Satta (1999)  Charniak (1997)  Collins (1997)—where the lexical heads of each constituent are annotated on both the right- and left-hand sides of the context-free rules  under the constraint that every constituent inherits the lexical head from exactly one of its children  and the lexical head of a POS is its terminal item.</S><S sid =""83"" ssid = ""41"">First  it is easily reversible  i.e.  every parse tree built with Gf corresponds to a unique parse tree built with G. Second  if we use the relative frequency estimator for our production probabilities  the probability of a tree built with Gf is identical to the probability of the corresponding tree built with G. Finally  let us introduce the term c-command.</S><S sid =""277"" ssid = ""33"">The first set of results looks at the performance of the parser on the standard corpora for statistical parsing trials: Sections 2-21 (989 860 words  39 832 sentences) of the Penn Treebank (Marcus  Santorini  and Marcinkiewicz 1993) served as the training data  Section 24 (34 199 words  1 346 sentences) as the held-out data for parameter estimation  and Section 23 (59 100 words  2 416 sentences) as the test data.</S>",['Method_Citation']
19,J01-2004,D09-1034,0,2001,0,"In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here","In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here","['399', '40', '132', '299', '408']","<S sid =""399"" ssid = ""12"">In addition  as mentioned above  we would like to further test our language model in speech recognition tasks  to see if the perplexity improvement that we have seen can lead to significant reductions in word error rate.</S><S sid =""40"" ssid = ""28"">The following section will provide some background in probabilistic context-free grammars and language modeling for speech recognition.</S><S sid =""132"" ssid = ""36"">The rest of this paper will present our parsing model  its application to language modeling for speech recognition  and empirical results.</S><S sid =""299"" ssid = ""55"">Furthermore  this is quite a large beam (see discussion below)  so that very large improvements in efficiency can be had at the expense of the number of analyses that are retained.</S><S sid =""408"" ssid = ""21"">The improvement that we derived from interpolating the different models above indicates that using multiple models may be the most fruitful path in the future.</S>",['Method_Citation']
20,J01-2004,D09-1034,0,2001,0,"At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is re quired to calculate such measures. Let H (D) be the entropy over a set of derivations D, calculated as follows: H (D)=? X D? D? (D) P D?? D? (D?) log? (D) P D?? D? (D?) (10) If the set of derivations D= D (G, W [1, i]) is a set of partial derivations for string W [1, i], then H (D) is a measure of uncertainty over the partial derivations ,i.e., the uncertainty regarding the correct analysis of what has already been processed","At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is required to calculate such measures","['399', '75', '96', '176', '269']","<S sid =""399"" ssid = ""12"">In addition  as mentioned above  we would like to further test our language model in speech recognition tasks  to see if the perplexity improvement that we have seen can lead to significant reductions in word error rate.</S><S sid =""75"" ssid = ""33"">In effect  this is an underspecification of some of the predictions that our top-down parser is making about the rest of the string.</S><S sid =""96"" ssid = ""54"">This interpolation is recursively applied to the smaller-order n-grams until the bigram is finally interpolated with the unigram  i.e.  Ao = 1.</S><S sid =""176"" ssid = ""80"">For example  if the algorithm is asked for the value at level 0  it will return A  the left-hand side of the rule being expanded.&quot; Suppose the algorithm is asked for the value at level 4.</S><S sid =""269"" ssid = ""25"">In such a model  it is possible to commit to a set of partial analyses at a particular point that cannot be completed given the rest of the input string (i.e.  the parser can &quot;garden path&quot;).</S>",['Method_Citation']
