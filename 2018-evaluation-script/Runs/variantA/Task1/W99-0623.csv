Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Reference Citation
1,W99-0623,A00-2005,0,1999,0,1 Introduct ion Henderson and Brill (1999) showed that independent human research efforts produce parsers that can be combined for an overall boost in accuracy,Henderson and Brill (1999) showed that independent human research efforts produce parsers that can be combined for an overall boost in accuracy,"['9', '143', '6', '141', '11']","<S sid =""9"" ssid = ""5"">Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).</S><S sid =""143"" ssid = ""5"">Through parser combination we have reduced the precision error rate by 30% and the recall error rate by 6% compared to the best previously published result.</S><S sid =""6"" ssid = ""2"">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).</S><S sid =""141"" ssid = ""3"">All four of the techniques studied result in parsing systems that perform better than any previously reported.</S><S sid =""11"" ssid = ""7"">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S>",['Method_Citation']
2,W99-0623,A00-2005,0,1999,0,the collection of hypotheses ti =fi (Stest) using the unweighted constituent voting scheme of Henderson and Brill (1999),"Given a novel sentence Stest E Ctest, combine the collection of hypotheses ti = fi(Stest) using the unweighted constituent voting scheme of Henderson and Brill (1999)","['32', '48', '13', '41', '30']","<S sid =""32"" ssid = ""18"">In Equations 1 through 3 we develop the model for constructing our parse using naïve Bayes classification.</S><S sid =""48"" ssid = ""34"">• Similarly  when the naïve Bayes classifier is configured such that the constituents require estimated probabilities strictly larger than 0.5 to be accepted  there is not enough probability mass remaining on crossing brackets for them to be included in the hypothesis.</S><S sid =""13"" ssid = ""9"">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).</S><S sid =""41"" ssid = ""27"">IL+-1Proof: Assume a pair of crossing constituents appears in the output of the constituent voting technique using k parsers.</S><S sid =""30"" ssid = ""16"">This is equivalent to the assumption used in probability estimation for naïve Bayes classifiers  namely that the attribute values are conditionally independent when the target value is given.</S>",['Method_Citation']
4,W99-0623,N10-1091,0,"Henderson and Brill, 1999",0,"5 (Henderson and Brill, 1999) used a similar framework in the context of constituent parsing and only three base parsers","(Henderson and Brill, 1999) used a similar framework in the context of constituent parsing and only three base parsers","['6', '9', '143', '13', '141']","<S sid =""6"" ssid = ""2"">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).</S><S sid =""9"" ssid = ""5"">Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).</S><S sid =""143"" ssid = ""5"">Through parser combination we have reduced the precision error rate by 30% and the recall error rate by 6% compared to the best previously published result.</S><S sid =""13"" ssid = ""9"">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).</S><S sid =""141"" ssid = ""3"">All four of the techniques studied result in parsing systems that perform better than any previously reported.</S>",['Method_Citation']
5,W99-0623,W05-1518,0,"Henderson and Brill, 1999",0,"A successful application of voting and of a stacked classifier to constituent parsing followed in (Henderson and Brill, 1999)","A successful application of voting and of a stacked classifier to constituent parsing followed in (Henderson and Brill, 1999)","['19', '141', '38', '40', '71']","<S sid =""19"" ssid = ""5"">The precision and recall measures (described in more detail in Section 3) used in evaluating Treebank parsing treat each constituent as a separate entity  a minimal unit of correctness.</S><S sid =""141"" ssid = ""3"">All four of the techniques studied result in parsing systems that perform better than any previously reported.</S><S sid =""38"" ssid = ""24"">Under certain conditions the constituent voting and naïve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S><S sid =""40"" ssid = ""26"">Lemma: If the number of votes required by constituent voting is greater than half of the parsers under consideration the resulting structure has no crossing constituents.</S><S sid =""71"" ssid = ""57"">It is chosen such that the decisions it made in including or excluding constituents are most probable under the models for all of the parsers.</S>",['Method_Citation']
6,W99-0623,W05-1518,0,"Henderson and Brill, 1999",0,"This approach roughly corresponds to (Henderson and Brill, 1999)? s Na ?ve Bayes parse hybridization","This approach roughly corresponds to (Henderson and Brill, 1999)'s Naive Bayes parse hybridization","['6', '143', '9', '123', '19']","<S sid =""6"" ssid = ""2"">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).</S><S sid =""143"" ssid = ""5"">Through parser combination we have reduced the precision error rate by 30% and the recall error rate by 6% compared to the best previously published result.</S><S sid =""9"" ssid = ""5"">Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).</S><S sid =""123"" ssid = ""52"">The difference in precision between similarity and Bayes switching techniques is significant  but the difference in recall is not.</S><S sid =""19"" ssid = ""5"">The precision and recall measures (described in more detail in Section 3) used in evaluating Treebank parsing treat each constituent as a separate entity  a minimal unit of correctness.</S>",['Method_Citation']
7,W99-0623,W05-1518,0,1999,0,Henderson and Brill (1999) also reported that context did not help them to outperform simple voting,Henderson and Brill (1999) also reported that context did not help them to outperform simple voting,"['6', '9', '128', '124', '143']","<S sid =""6"" ssid = ""2"">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).</S><S sid =""9"" ssid = ""5"">Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).</S><S sid =""128"" ssid = ""57"">Ties are rare in Bayes switching because the models are fine-grained — many estimated probabilities are involved in each decision.</S><S sid =""124"" ssid = ""53"">This is the first set that gives us a fair evaluation of the Bayes models  and the Bayes switching model performs significantly better than its non-parametric counterpart.</S><S sid =""143"" ssid = ""5"">Through parser combination we have reduced the precision error rate by 30% and the recall error rate by 6% compared to the best previously published result.</S>",['Method_Citation']
8,W99-0623,W05-1518,0,"Henderson and Brill, 1999",0,"(Henderson and Brill, 1999) improved their best parser? s F-measure of 89.7 to 91.3, using their na ?ve Bayes voting on the Penn TreeBank constituent structures (16% error reduction)","(Henderson and Brill, 1999) improved their best parser's F-measure of 89.7 to 91.3, using their naive Bayes voting on the Penn TreeBank constituent structures (16% error reduction)","['6', '13', '143', '19', '120']","<S sid =""6"" ssid = ""2"">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).</S><S sid =""13"" ssid = ""9"">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).</S><S sid =""143"" ssid = ""5"">Through parser combination we have reduced the precision error rate by 30% and the recall error rate by 6% compared to the best previously published result.</S><S sid =""19"" ssid = ""5"">The precision and recall measures (described in more detail in Section 3) used in evaluating Treebank parsing treat each constituent as a separate entity  a minimal unit of correctness.</S><S sid =""120"" ssid = ""49"">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser  and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>",['Method_Citation']
10,W99-0623,P01-1005,0,"Henderson and Brill, 1999",0,"Voting has proven to be an effective technique for improving classifier accuracy for many applications, including part-of-speech tagging (van Halteren, et al 1998), parsing (Henderson and Brill, 1999), and word sense disambiguation (Pederson, 2000)","Voting has proven to be an effective technique for improving classifier accuracy for many applications, including part-of-speech tagging (van Halteren, et al 1998), parsing (Henderson and Brill, 1999), and word sense disambiguation (Pederson, 2000)","['11', '9', '120', '124', '26']","<S sid =""11"" ssid = ""7"">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid =""9"" ssid = ""5"">Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).</S><S sid =""120"" ssid = ""49"">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser  and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S><S sid =""124"" ssid = ""53"">This is the first set that gives us a fair evaluation of the Bayes models  and the Bayes switching model performs significantly better than its non-parametric counterpart.</S><S sid =""26"" ssid = ""12"">This technique has the advantage of requiring no training  but it has the disadvantage of treating all parsers equally even though they may have differing accuracies or may specialize in modeling different phenomena.</S>",['Method_Citation']
11,W99-0623,D09-1161,0,1999,0,"Regarding the system combination study, Henderson and Brill (1999) propose two parser combination schemes, one that selects an entire tree from one of the parsers, and one that builds a new tree by selecting constituents suggested by the initial trees","Regarding the system combination study, Henderson and Brill (1999) propose two parser combination schemes, one that selects an entire tree from one of the parsers, and one that builds a new tree by selecting constituents suggested by the initial trees","['72', '6', '7', '117', '48']","<S sid =""72"" ssid = ""1"">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank  leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S><S sid =""6"" ssid = ""2"">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).</S><S sid =""7"" ssid = ""3"">Their theoretical finding is simply stated: classification error rate decreases toward the noise rate exponentially in the number of independent  accurate classifiers.</S><S sid =""117"" ssid = ""46"">Another way to interpret this is that less than 5% of the correct constituents are missing from the hypotheses generated by the union of the three parsers.</S><S sid =""48"" ssid = ""34"">• Similarly  when the naïve Bayes classifier is configured such that the constituents require estimated probabilities strictly larger than 0.5 to be accepted  there is not enough probability mass remaining on crossing brackets for them to be included in the hypothesis.</S>",['Results_Citation']
12,W99-0623,D09-1161,0,1999,0,"Henderson and Brill (1999) combine three parsers and obtained an F1 score of 90.6, which is better than the score of 88.6 obtained by the best individual parser as reported in their paper","Henderson and Brill (1999) combine three parsers and obtained an F1 score of 90.6, which is better than the score of 88.6 obtained by the best individual parser as reported in their paper","['13', '143', '124', '9', '141']","<S sid =""13"" ssid = ""9"">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).</S><S sid =""143"" ssid = ""5"">Through parser combination we have reduced the precision error rate by 30% and the recall error rate by 6% compared to the best previously published result.</S><S sid =""124"" ssid = ""53"">This is the first set that gives us a fair evaluation of the Bayes models  and the Bayes switching model performs significantly better than its non-parametric counterpart.</S><S sid =""9"" ssid = ""5"">Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).</S><S sid =""141"" ssid = ""3"">All four of the techniques studied result in parsing systems that perform better than any previously reported.</S>",['Method_Citation']
13,W99-0623,D09-1161,0,"Henderson and Brill, 1999",0,"Besides the two model scores, we also adopt constituent count as an additional feature in spired by (Henderson and Brill 1999) and (Sagae and Lavie 2006)","Besides the two model scores, we also adopt constituent count as an additional feature in spired by (Henderson and Brill 1999) and (Sagae and Lavie 2006)","['126', '124', '13', '6', '19']","<S sid =""126"" ssid = ""55"">Table 4 shows how much the Bayes switching technique uses each of the parsers on the test set.</S><S sid =""124"" ssid = ""53"">This is the first set that gives us a fair evaluation of the Bayes models  and the Bayes switching model performs significantly better than its non-parametric counterpart.</S><S sid =""13"" ssid = ""9"">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).</S><S sid =""6"" ssid = ""2"">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).</S><S sid =""19"" ssid = ""5"">The precision and recall measures (described in more detail in Section 3) used in evaluating Treebank parsing treat each constituent as a separate entity  a minimal unit of correctness.</S>",['Method_Citation']
14,W99-0623,N06-2033,0,"Henderson and Brill, 1999",0,"Henderson and Brill (1999) proposed two parser combination schemes, one that picks an entire tree from one of the parsers, and one that, like ours, builds a new tree from constituents from the initial trees","Henderson and Brill (1999) proposed two parser combination schemes, one that picks an entire tree from one of the parsers, and one that, like ours, builds a new tree from constituents from the initial trees","['6', '9', '124', '143', '64']","<S sid =""6"" ssid = ""2"">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).</S><S sid =""9"" ssid = ""5"">Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).</S><S sid =""124"" ssid = ""53"">This is the first set that gives us a fair evaluation of the Bayes models  and the Bayes switching model performs significantly better than its non-parametric counterpart.</S><S sid =""143"" ssid = ""5"">Through parser combination we have reduced the precision error rate by 30% and the recall error rate by 6% compared to the best previously published result.</S><S sid =""64"" ssid = ""50"">Furthermore  we know one of the original parses will be the hypothesized parse  so the direct method of determining which one is best is to compute the probability of each of the candidate parses using the probabilistic model we developed in Section 2.1.</S>",['Method_Citation']
15,W99-0623,N09-2064,0,"Henderson and Brill, 1999",0,"(Henderson and Brill, 1999) perform parse selection by maximizing the expected precision of the selected parse with respect to the set of parses being combined","(Henderson and Brill, 1999) perform parse selection by maximizing the expected precision of the selected parse with respect to the set of parses being combined","['139', '141', '143', '124', '19']","<S sid =""139"" ssid = ""1"">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S><S sid =""141"" ssid = ""3"">All four of the techniques studied result in parsing systems that perform better than any previously reported.</S><S sid =""143"" ssid = ""5"">Through parser combination we have reduced the precision error rate by 30% and the recall error rate by 6% compared to the best previously published result.</S><S sid =""124"" ssid = ""53"">This is the first set that gives us a fair evaluation of the Bayes models  and the Bayes switching model performs significantly better than its non-parametric counterpart.</S><S sid =""19"" ssid = ""5"">The precision and recall measures (described in more detail in Section 3) used in evaluating Treebank parsing treat each constituent as a separate entity  a minimal unit of correctness.</S>",['Method_Citation']
16,W99-0623,N09-2064,0,"Henderson and Brill, 1999",0,"(Henderson and Brill, 1999) and (Sagae and Lavie, 2006) propose methods for parse hybridization by recombining constituents","(Henderson and Brill, 1999) and (Sagae and Lavie, 2006) propose methods for parse hybridization by recombining constituents","['6', '9', '125', '11', '13']","<S sid =""6"" ssid = ""2"">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).</S><S sid =""9"" ssid = ""5"">Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).</S><S sid =""125"" ssid = ""54"">The constituent voting and naïve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S><S sid =""11"" ssid = ""7"">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid =""13"" ssid = ""9"">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).</S>",['Method_Citation']
17,W99-0623,N09-2064,0,"Henderson and Brill, 1999",0,"output (Figure 3) .Second, the parse selection method of (Henderson and Brill, 1999) selects the parse with maximum expected precision; here, we present an efficient, linear-time algorithm for selecting the parse with maximum expected f-score within the Mini mum Bayes Risk (MBR) framework. Third, we extend these parser combination methods from 1-best outputs to n-best outputs","Second, the parse selection method of (Henderson and Brill, 1999) selects the parse with maximum expected precision; here, we present an efficient, linear-time algorithm for selecting the parse with maximum expected f-score within the Mini mum Bayes Risk (MBR) framework","['64', '124', '48', '120', '70']","<S sid =""64"" ssid = ""50"">Furthermore  we know one of the original parses will be the hypothesized parse  so the direct method of determining which one is best is to compute the probability of each of the candidate parses using the probabilistic model we developed in Section 2.1.</S><S sid =""124"" ssid = ""53"">This is the first set that gives us a fair evaluation of the Bayes models  and the Bayes switching model performs significantly better than its non-parametric counterpart.</S><S sid =""48"" ssid = ""34"">• Similarly  when the naïve Bayes classifier is configured such that the constituents require estimated probabilities strictly larger than 0.5 to be accepted  there is not enough probability mass remaining on crossing brackets for them to be included in the hypothesis.</S><S sid =""120"" ssid = ""49"">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser  and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S><S sid =""70"" ssid = ""56"">In this case we are interested in finding' the maximum probability parse  ri  and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers.</S>",['Method_Citation']
18,W99-0623,P09-1065,0,"Henderson and Brill, 1999",0,"System combination has benefited various NLP tasks in recent years, such as products-of-experts (e.g., (Smith and Eisner, 2005)) and ensemble based parsing (e.g., (Henderson and Brill, 1999))","System combination has benefited various NLP tasks in recent years, such as products-of-experts (e.g., (Smith and Eisner, 2005)) and ensemble based parsing (e.g., (Henderson and Brill, 1999))","['11', '6', '124', '9', '13']","<S sid =""11"" ssid = ""7"">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid =""6"" ssid = ""2"">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).</S><S sid =""124"" ssid = ""53"">This is the first set that gives us a fair evaluation of the Bayes models  and the Bayes switching model performs significantly better than its non-parametric counterpart.</S><S sid =""9"" ssid = ""5"">Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).</S><S sid =""13"" ssid = ""9"">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al.  1993).</S>",['Method_Citation']
20,W99-0623,C10-1151,0,1999,0,Henderson and Brill (1999) performs parse selection by maximizing the expected precision of selected parse with respect to the set of parses to be combined,Henderson and Brill (1999) performs parse selection by maximizing the expected precision of selected parse with respect to the set of parses to be combined,"['9', '143', '124', '6', '19']","<S sid =""9"" ssid = ""5"">Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).</S><S sid =""143"" ssid = ""5"">Through parser combination we have reduced the precision error rate by 30% and the recall error rate by 6% compared to the best previously published result.</S><S sid =""124"" ssid = ""53"">This is the first set that gives us a fair evaluation of the Bayes models  and the Bayes switching model performs significantly better than its non-parametric counterpart.</S><S sid =""6"" ssid = ""2"">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert  1992; Heath et al.  1996).</S><S sid =""19"" ssid = ""5"">The precision and recall measures (described in more detail in Section 3) used in evaluating Treebank parsing treat each constituent as a separate entity  a minimal unit of correctness.</S>",['Method_Citation']
