Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,W06-3114,W06-3120,0,"Koehn and Monz, 2006",0,"The official results were slightly better because a lowercase evaluation was used, see (Koehn and Monz, 2006)","The official results were slightly better because a lowercase evaluation was used, see (Koehn and Monz, 2006)","['73', '57', '107', '21', '126']","<S sid =""73"" ssid = ""12"">We assumed that such a contrastive assessment would be beneficial for an evaluation that essentially pits different systems against each other.</S><S sid =""57"" ssid = ""23"">We are therefore applying a different method  which has been used at the 2005 DARPA/NIST evaluation.</S><S sid =""107"" ssid = ""23"">Still  for about good number of sentences  we do have this direct comparison  which allows us to apply the sign test  as described in Section 2.2.</S><S sid =""21"" ssid = ""14"">The out-of-domain test set differs from the Europarl data in various ways.</S><S sid =""126"" ssid = ""19"">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S>",['Method_Citation']
2,W06-3114,D07-1092,0,"Koehn and Monz, 2006",0,"We are further focusing on the shared task of the workshop on Statistical Machine Translation, which took place last year (Koehn and Monz, 2006) and consisted in translating Spanish, German, and French texts from and to English","We are further focusing on the shared task of the workshop on Statistical Machine Translation, which took place last year (Koehn and Monz, 2006) and consisted in translating Spanish, German, and French texts from and to English","['145', '165', '136', '24', '177']","<S sid =""145"" ssid = ""38"">This is can not be the only explanation  since the discrepancy still holds  for instance  for out-of-domain French-English  where Systran receives among the best adequacy and fluency scores  but a worse BLEU score than all but one statistical system.</S><S sid =""165"" ssid = ""58"">However  ince we extracted the test corpus automatically from web sources  the reference translation was not always accurate — due to sentence alignment errors  or because translators did not adhere to a strict sentence-by-sentence translation (say  using pronouns when referring to entities mentioned in the previous sentence).</S><S sid =""136"" ssid = ""29"">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid =""24"" ssid = ""17"">However  it is also mostly political content (even if not focused on the internal workings of the European Union) and opinion.</S><S sid =""177"" ssid = ""1"">This work was supported in part under the GALE program of the Defense Advanced Research Projects Agency  Contract No.</S>",['Method_Citation']
3,W06-3114,C08-1074,0,"Koehn and Monz, 2006",0,"For our training and test data we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference","For our training and test data we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference","['145', '165', '64', '18', '12']","<S sid =""145"" ssid = ""38"">This is can not be the only explanation  since the discrepancy still holds  for instance  for out-of-domain French-English  where Systran receives among the best adequacy and fluency scores  but a worse BLEU score than all but one statistical system.</S><S sid =""165"" ssid = ""58"">However  ince we extracted the test corpus automatically from web sources  the reference translation was not always accurate — due to sentence alignment errors  or because translators did not adhere to a strict sentence-by-sentence translation (say  using pronouns when referring to entities mentioned in the previous sentence).</S><S sid =""64"" ssid = ""3"">Also  the argument has been made that machine translation performance should be evaluated via task-based evaluation metrics  i.e. how much it assists performing a useful task  such as supporting human translators or aiding the analysis of texts.</S><S sid =""18"" ssid = ""11"">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S><S sid =""12"" ssid = ""5"">To summarize  we provided: The performance of the baseline system is similar to the best submissions in last year’s shared task.</S>",['Method_Citation']
4,W06-3114,W07-0718,0,"Koehn and Monz, 2006",0,"The results of last year? s workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006)","The results of last year's workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006)","['64', '12', '49', '151', '7']","<S sid =""64"" ssid = ""3"">Also  the argument has been made that machine translation performance should be evaluated via task-based evaluation metrics  i.e. how much it assists performing a useful task  such as supporting human translators or aiding the analysis of texts.</S><S sid =""12"" ssid = ""5"">To summarize  we provided: The performance of the baseline system is similar to the best submissions in last year’s shared task.</S><S sid =""49"" ssid = ""15"">Hence  we use the bootstrap resampling method described by Koehn (2004).</S><S sid =""151"" ssid = ""44"">The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphological rich languages  as demonstrated by the results for English-German and English-French.</S><S sid =""7"" ssid = ""5"">We dropped  however  one of the languages  Finnish  partly to keep the number of tracks manageable  partly because we assumed that it would be hard to find enough Finnish speakers for the manual evaluation.</S>",['Method_Citation']
5,W06-3114,P07-1083,0,"Koehn and Monz, 2006",0,"For the bi text-based annotation, we use publicly available word alignments from the Europarl corpus, automatically generated by GIZA++ for FrenchEnglish (Fr), Spanish-English (Es) and GermanEnglish (De) (Koehn and Monz, 2006)","For the bi text-based annotation, we use publicly available word alignments from the Europarl corpus, automatically generated by GIZA++ for FrenchEnglish (Fr), Spanish-English (Es) and German-English (De) (Koehn and Monz, 2006)","['145', '103', '165', '136', '140']","<S sid =""145"" ssid = ""38"">This is can not be the only explanation  since the discrepancy still holds  for instance  for out-of-domain French-English  where Systran receives among the best adequacy and fluency scores  but a worse BLEU score than all but one statistical system.</S><S sid =""103"" ssid = ""19"">Given a set of n sentences  we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d  x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric  we want to be able to rank different systems against each other  for which we need assessments of statistical significance on the differences between a pair of systems.</S><S sid =""165"" ssid = ""58"">However  ince we extracted the test corpus automatically from web sources  the reference translation was not always accurate — due to sentence alignment errors  or because translators did not adhere to a strict sentence-by-sentence translation (say  using pronouns when referring to entities mentioned in the previous sentence).</S><S sid =""136"" ssid = ""29"">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid =""140"" ssid = ""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>",['Method_Citation']
6,W06-3114,W07-0738,0,2006,0,"Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MTquality indicator","Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MTquality indicator","['38', '151', '64', '130', '145']","<S sid =""38"" ssid = ""4"">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S><S sid =""151"" ssid = ""44"">The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphological rich languages  as demonstrated by the results for English-German and English-French.</S><S sid =""64"" ssid = ""3"">Also  the argument has been made that machine translation performance should be evaluated via task-based evaluation metrics  i.e. how much it assists performing a useful task  such as supporting human translators or aiding the analysis of texts.</S><S sid =""130"" ssid = ""23"">This is demonstrated by average scores over all systems  in terms of BLEU  fluency and adequacy  as displayed in Figure 5.</S><S sid =""145"" ssid = ""38"">This is can not be the only explanation  since the discrepancy still holds  for instance  for out-of-domain French-English  where Systran receives among the best adequacy and fluency scores  but a worse BLEU score than all but one statistical system.</S>",['Method_Citation']
7,W06-3114,W07-0738,0,2006,0,"For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001)","For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001)","['136', '39', '64', '165', '140']","<S sid =""136"" ssid = ""29"">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid =""39"" ssid = ""5"">However  a recent study (Callison-Burch et al.  2006)  pointed out that this correlation may not always be strong.</S><S sid =""64"" ssid = ""3"">Also  the argument has been made that machine translation performance should be evaluated via task-based evaluation metrics  i.e. how much it assists performing a useful task  such as supporting human translators or aiding the analysis of texts.</S><S sid =""165"" ssid = ""58"">However  ince we extracted the test corpus automatically from web sources  the reference translation was not always accurate — due to sentence alignment errors  or because translators did not adhere to a strict sentence-by-sentence translation (say  using pronouns when referring to entities mentioned in the previous sentence).</S><S sid =""140"" ssid = ""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>",['Method_Citation']
8,W06-3114,W07-0738,0,2006,0,Wepresenta comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006) (see Section 3),We present a comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006) (see Section 3),"['140', '64', '145', '109', '165']","<S sid =""140"" ssid = ""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S><S sid =""64"" ssid = ""3"">Also  the argument has been made that machine translation performance should be evaluated via task-based evaluation metrics  i.e. how much it assists performing a useful task  such as supporting human translators or aiding the analysis of texts.</S><S sid =""145"" ssid = ""38"">This is can not be the only explanation  since the discrepancy still holds  for instance  for out-of-domain French-English  where Systran receives among the best adequacy and fluency scores  but a worse BLEU score than all but one statistical system.</S><S sid =""109"" ssid = ""2"">The scores and confidence intervals are detailed first in the Figures 7–10 in table form (including ranks)  and then in graphical form in Figures 11–16.</S><S sid =""165"" ssid = ""58"">However  ince we extracted the test corpus automatically from web sources  the reference translation was not always accurate — due to sentence alignment errors  or because translators did not adhere to a strict sentence-by-sentence translation (say  using pronouns when referring to entities mentioned in the previous sentence).</S>",['Method_Citation']
9,W06-3114,W07-0738,0,2006,0,Weanalyze some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006),We analyze some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006),"['140', '39', '49', '122', '18']","<S sid =""140"" ssid = ""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S><S sid =""39"" ssid = ""5"">However  a recent study (Callison-Burch et al.  2006)  pointed out that this correlation may not always be strong.</S><S sid =""49"" ssid = ""15"">Hence  we use the bootstrap resampling method described by Koehn (2004).</S><S sid =""122"" ssid = ""15"">While the Bootstrap method is slightly more sensitive  it is very much in line with the sign test on text blocks.</S><S sid =""18"" ssid = ""11"">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S>",['Results_Citation']
10,W06-3114,D07-1030,0,"Koehn and Monz, 2006",0,"We use the same method described in (Koehn and Monz, 2006) to perform the significance test","We use the same method described in (Koehn and Monz, 2006) to perform the significance test","['140', '57', '49', '48', '11']","<S sid =""140"" ssid = ""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S><S sid =""57"" ssid = ""23"">We are therefore applying a different method  which has been used at the 2005 DARPA/NIST evaluation.</S><S sid =""49"" ssid = ""15"">Hence  we use the bootstrap resampling method described by Koehn (2004).</S><S sid =""48"" ssid = ""14"">Confidence Interval: Since BLEU scores are not computed on the sentence level  traditional methods to compute statistical significance and confidence intervals do not apply.</S><S sid =""11"" ssid = ""4"">To lower the barrier of entrance to the competition  we provided a complete baseline MT system  along with data resources.</S>",['Method_Citation']
11,W06-3114,D07-1030,0,"Koehn and Monz, 2016",0,"We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006)","We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006)","['130', '38', '140', '109', '57']","<S sid =""130"" ssid = ""23"">This is demonstrated by average scores over all systems  in terms of BLEU  fluency and adequacy  as displayed in Figure 5.</S><S sid =""38"" ssid = ""4"">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S><S sid =""140"" ssid = ""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S><S sid =""109"" ssid = ""2"">The scores and confidence intervals are detailed first in the Figures 7–10 in table form (including ranks)  and then in graphical form in Figures 11–16.</S><S sid =""57"" ssid = ""23"">We are therefore applying a different method  which has been used at the 2005 DARPA/NIST evaluation.</S>",['Method_Citation']
12,W06-3114,W08-0406,0,"Koehn and Monz, 2017",0,"The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002)","The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002)","['18', '130', '136', '13', '140']","<S sid =""18"" ssid = ""11"">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S><S sid =""130"" ssid = ""23"">This is demonstrated by average scores over all systems  in terms of BLEU  fluency and adequacy  as displayed in Figure 5.</S><S sid =""136"" ssid = ""29"">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid =""13"" ssid = ""6"">We are currently working on a complete open source implementation of a training and decoding system  which should become available over the summer. pus  from which also the in-domain test set is taken.</S><S sid =""140"" ssid = ""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>",['Method_Citation']
13,W06-3114,W11-1002,0,2006,0,"Callison-Burch et al (2006 )andKoehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality","Callison-Burch et al (2006) and Koehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality","['140', '39', '151', '6', '56']","<S sid =""140"" ssid = ""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S><S sid =""39"" ssid = ""5"">However  a recent study (Callison-Burch et al.  2006)  pointed out that this correlation may not always be strong.</S><S sid =""151"" ssid = ""44"">The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphological rich languages  as demonstrated by the results for English-German and English-French.</S><S sid =""6"" ssid = ""4"">English was again paired with German  French  and Spanish.</S><S sid =""56"" ssid = ""22"">The bootstrap method has been critized by Riezler and Maxwell (2005) and Collins et al. (2005)  as being too optimistic in deciding for statistical significant difference between systems.</S>",['Method_Citation']
14,W06-3114,D07-1091,0,"Koehn and Monz, 2006",0,"The English? German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006)","The English German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006)","['129', '140', '130', '13', '18']","<S sid =""129"" ssid = ""22"">All systems (except for Systran  which was not tuned to Europarl) did considerably worse on outof-domain training data.</S><S sid =""140"" ssid = ""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S><S sid =""130"" ssid = ""23"">This is demonstrated by average scores over all systems  in terms of BLEU  fluency and adequacy  as displayed in Figure 5.</S><S sid =""13"" ssid = ""6"">We are currently working on a complete open source implementation of a training and decoding system  which should become available over the summer. pus  from which also the in-domain test set is taken.</S><S sid =""18"" ssid = ""11"">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S>",['Method_Citation']
15,W06-3114,D07-1091,0,"Koehn and Monz, 2006",0,"We report results on the development test set, which is also the out-of-domain test set of the WMT06 workshop shared task (Koehn and Monz, 2006)","We report results on the development test set, which is also the out-of-domain test set of the WMT06 workshop shared task (Koehn and Monz, 2006)","['13', '57', '18', '140', '11']","<S sid =""13"" ssid = ""6"">We are currently working on a complete open source implementation of a training and decoding system  which should become available over the summer. pus  from which also the in-domain test set is taken.</S><S sid =""57"" ssid = ""23"">We are therefore applying a different method  which has been used at the 2005 DARPA/NIST evaluation.</S><S sid =""18"" ssid = ""11"">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S><S sid =""140"" ssid = ""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S><S sid =""11"" ssid = ""4"">To lower the barrier of entrance to the competition  we provided a complete baseline MT system  along with data resources.</S>",['Method_Citation']
16,W06-3114,P07-1108,0,"Koehn and Monz, 2006",0,"A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Ma chine Translation (Koehn and Monz, 2006)","A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Machine Translation (Koehn and Monz, 2006)","['18', '62', '38', '170', '13']","<S sid =""18"" ssid = ""11"">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S><S sid =""62"" ssid = ""1"">While automatic measures are an invaluable tool for the day-to-day development of machine translation systems  they are only a imperfect substitute for human assessment of translation quality  or as the acronym BLEU puts it  a bilingual evaluation understudy.</S><S sid =""38"" ssid = ""4"">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S><S sid =""170"" ssid = ""1"">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid =""13"" ssid = ""6"">We are currently working on a complete open source implementation of a training and decoding system  which should become available over the summer. pus  from which also the in-domain test set is taken.</S>",['Method_Citation']
18,W06-3114,E12-3010,0,"Koehn and Monz, 2006",0,"For the same reason, human evaluation metrics based on adequacy and fluency were not suitable either (Koehn and Monz, 2006)","For the same reason, human evaluation metrics based on adequacy and fluency were not suitable either (Koehn and Monz, 2006)","['84', '107', '100', '125', '138']","<S sid =""84"" ssid = ""23"">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid =""107"" ssid = ""23"">Still  for about good number of sentences  we do have this direct comparison  which allows us to apply the sign test  as described in Section 2.2.</S><S sid =""100"" ssid = ""16"">One may argue with these efforts on normalization  and ultimately their value should be assessed by assessing their impact on inter-annotator agreement.</S><S sid =""125"" ssid = ""18"">We can check  what the consequences of less manual annotation of results would have been: With half the number of manual judgements  we can distinguish about 40% of the systems  10% less.</S><S sid =""138"" ssid = ""31"">Hence  the different averages of manual scores for the different language pairs reflect the behaviour of the judges  not the quality of the systems on different language pairs.</S>",['Method_Citation']
19,W06-3114,W09-0402,0,"Koehn and Monz, 2006",0,"The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008)","The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008)","['103', '145', '165', '106', '136']","<S sid =""103"" ssid = ""19"">Given a set of n sentences  we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d  x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric  we want to be able to rank different systems against each other  for which we need assessments of statistical significance on the differences between a pair of systems.</S><S sid =""145"" ssid = ""38"">This is can not be the only explanation  since the discrepancy still holds  for instance  for out-of-domain French-English  where Systran receives among the best adequacy and fluency scores  but a worse BLEU score than all but one statistical system.</S><S sid =""165"" ssid = ""58"">However  ince we extracted the test corpus automatically from web sources  the reference translation was not always accurate — due to sentence alignment errors  or because translators did not adhere to a strict sentence-by-sentence translation (say  using pronouns when referring to entities mentioned in the previous sentence).</S><S sid =""106"" ssid = ""22"">Automatic scores are computed on a larger tested than manual scores (3064 sentences vs. 300–400 sentences). collected manual judgements  we do not necessarily have the same sentence judged for both systems (judges evaluate 5 systems out of the 8–10 participating systems).</S><S sid =""136"" ssid = ""29"">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S>",['Method_Citation']
