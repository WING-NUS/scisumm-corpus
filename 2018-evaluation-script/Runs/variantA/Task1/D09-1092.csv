Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,D09-1092,P14-1004,0,"Mimno et al, 2009",0,"This configuration is similar to PolyLDA (Mimno et al, 2009) or LinkLDA (Yano et al, 2009), such that utterances from different parties are treated as different languages or blog-post and comments pairs","This configuration is similar to PolyLDA (Mimno et al, 2009) or LinkLDA (Yano et al, 2009), such that utterances from different parties are treated as different languages or blog-post and comments pairs","['155', '33', '110', '191', '162']","<S sid =""155"" ssid = ""104"">Finally  for each pair of languages (“query” and “target”) we calculate the difference between the topic distribution for each held-out document in the query language and the topic distribution for each held-out document in the target language.</S><S sid =""33"" ssid = ""9"">They find  for example  that English blog posts about the Nintendo Wii often relate to a hack  which cannot be mentioned in Japanese posts due to Japanese intellectual property law.</S><S sid =""110"" ssid = ""59"">Continuing with the example above  one might extract a set of connected Wikipedia articles related to the focus of the journal and then train PLTM on a joint corpus consisting of journal papers and Wikipedia articles.</S><S sid =""191"" ssid = ""25"">Finally  historical topics  such as the Byzantine and Ottoman empires (right) are strong in all languages  but show geographical variation: interest centers around the empires.</S><S sid =""162"" ssid = ""111"">Restricting the query/target pairs to only those with query and target documents that are both longer than 50 words results in significant improvement and reduced variance: the average proportion of query documents for which the true translation is ranked highest goes from 53.9% to 72.7%.</S>",['Method_Citation']
2,D09-1092,P10-1044,0,"Mimno et al, 2009",0,"Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009)","Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009)","['6', '143', '173', '172', '55']","<S sid =""6"" ssid = ""2"">Topic models have been used for analyzing topic trends in research literature (Mann et al.  2006; Hall et al.  2008)  inferring captions for images (Blei and Jordan  2003)  social network analysis in email (McCallum et al.  2005)  and expanding queries with topically related words in information retrieval (Wei and Croft  2006).</S><S sid =""143"" ssid = ""92"">We also do not count morphological variants: the model finds EN “rules” and DE “vorschriften ” but the lexicon contains only “rule” and “vorschrift.” Results remain strong as we increase K. With K = 3  T = 800  1349 of the 7200 candidate pairs for Spanish appeared in the lexicon. topic in different languages translations of each other?</S><S sid =""173"" ssid = ""7"">We downloaded XML copies of all Wikipedia articles in twelve different languages: Welsh  German  Greek  English  Farsi  Finnish  French  Hebrew  Italian  Polish  Russian and Turkish.</S><S sid =""172"" ssid = ""6"">Second  because comparable texts may not use exactly the same topics  it becomes crucially important to be able to characterize differences in topic prevalence at the document level (do different languages have different perspectives on the same article?) and at the language-wide level (which topics do particular languages focus on?).</S><S sid =""55"" ssid = ""4"">The EuroParl corpus consists of parallel texts in eleven western European languages: Danish  German  Greek  English  Spanish  Finnish  French  Italian  Dutch  Portuguese and Swedish.</S>",['Method_Citation']
3,D09-1092,P11-2084,0,"Mimno et al, 2009",0,"(Mimno et al, 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then add the Cartesian product of these sets for every topic to a set of candidate translations","(Mimno et al, 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then add the Cartesian product of these sets for every topic to a set of candidate translations","['129', '10', '19', '139', '148']","<S sid =""129"" ssid = ""78"">We therefore evaluate the ability of the PLTM to generate bilingual lexica  similar to other work in unsupervised translation modeling (Haghighi et al.  2008).</S><S sid =""10"" ssid = ""6"">We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).</S><S sid =""19"" ssid = ""15"">We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S><S sid =""139"" ssid = ""88"">Results for K = 1  that is  considering only the single most probable word for each language  are shown in figure 6.</S><S sid =""148"" ssid = ""97"">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S>",['Method_Citation']
4,D09-1092,E12-1014,0,"Mimno et al, 2009",0,"Our Wikipedia-based topic similarity feature, w (f, e), is similar in spirit to polylingualtopic models (Mimno et al 2009), but it is scalable to full bilingual lexicon induction","Our Wikipedia-based topic similarity feature, w (f, e), is similar in spirit to polylingual topic models (Mimno et al 2009), but it is scalable to full bilingual lexicon induction","['196', '35', '192', '25', '174']","<S sid =""196"" ssid = ""5"">When applied to comparable document collections such as Wikipedia  PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.</S><S sid =""35"" ssid = ""1"">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S><S sid =""192"" ssid = ""1"">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S sid =""25"" ssid = ""1"">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid =""174"" ssid = ""8"">These versions of Wikipedia were selected to provide a diverse range of language families  geographic areas  and quantities of text.</S>",['Method_Citation']
5,D09-1092,D11-1086,0,"Mimno et al, 2009",0,"of English document and the second half of its aligned foreign language document (Mimno et al,2009)","For Europarl data sets, we artificially make them comparable by considering the first half of English document and the second half of its aligned foreign language document (Mimno et al,2009)","['137', '54', '42', '129', '85']","<S sid =""137"" ssid = ""86"">For every topic t we select a small number K of the most probable words in English (e) and in each “translation” language (E): Wte and Wtt  respectively.</S><S sid =""54"" ssid = ""3"">Although direct translations in multiple languages are relatively rare (in contrast with comparable documents)  we use direct translations to explore the characteristics of the model.</S><S sid =""42"" ssid = ""8"">Given a corpus of training and test document tuples—W and W'  respectively—two possible inference tasks of interest are: computing the probability of the test tuples given the training tuples and inferring latent topic assignments for test documents.</S><S sid =""129"" ssid = ""78"">We therefore evaluate the ability of the PLTM to generate bilingual lexica  similar to other work in unsupervised translation modeling (Haghighi et al.  2008).</S><S sid =""85"" ssid = ""34"">Smoothed histograms of inter−language JS divergence A topic model specifies a probability distribution over documents  or in the case of PLTM  document tuples.</S>",['Method_Citation']
6,D09-1092,N12-1007,0,"Mimno et al, 2009",0,"Since the PLTM is not a contribution of this paper, we refer the interested reader to (Mimno et al, 2009) for more details","Since the PLTM is not a contribution of this paper, we refer the interested reader to (Mimno et al, 2009) for more details","['96', '104', '110', '139', '54']","<S sid =""96"" ssid = ""45"">It is important to note  however  that these results do not imply that LDA should be preferred over PLTM—that choice depends upon the needs of the modeler.</S><S sid =""104"" ssid = ""53"">This result is important: informally  we have found that increasing the granularity of topics correlates strongly with user perceptions of the utility of a topic model.</S><S sid =""110"" ssid = ""59"">Continuing with the example above  one might extract a set of connected Wikipedia articles related to the focus of the journal and then train PLTM on a joint corpus consisting of journal papers and Wikipedia articles.</S><S sid =""139"" ssid = ""88"">Results for K = 1  that is  considering only the single most probable word for each language  are shown in figure 6.</S><S sid =""54"" ssid = ""3"">Although direct translations in multiple languages are relatively rare (in contrast with comparable documents)  we use direct translations to explore the characteristics of the model.</S>",['Method_Citation']
7,D09-1092,N12-1007,0,2009,0,"Evaluation Corpus The automatic evaluation of cross-lingual co reference systems requires annotated 10Mimno et al (2009) showed that so long as the proportion of topically-aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly","Mimno et al (2009) showed that so long as the proportion of topically-aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly","['140', '172', '129', '184', '36']","<S sid =""140"" ssid = ""89"">Precision at this level is relatively high  above 50% for Spanish  French and Italian with T = 400 and 800.</S><S sid =""172"" ssid = ""6"">Second  because comparable texts may not use exactly the same topics  it becomes crucially important to be able to characterize differences in topic prevalence at the document level (do different languages have different perspectives on the same article?) and at the language-wide level (which topics do particular languages focus on?).</S><S sid =""129"" ssid = ""78"">We therefore evaluate the ability of the PLTM to generate bilingual lexica  similar to other work in unsupervised translation modeling (Haghighi et al.  2008).</S><S sid =""184"" ssid = ""18"">Interestingly  we find that almost all languages in our corpus  including several pairs that have historically been in conflict  show average JS divergences of between approximately 0.08 and 0.12 for T = 400  consistent with our findings for EuroParl translations.</S><S sid =""36"" ssid = ""2"">Each tuple is a set of documents that are loosely equivalent to each other  but written in different languages  e.g.  corresponding Wikipedia articles in French  English and German.</S>",['Method_Citation']
8,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"Similarly, Polylingual Topic Models (PLTM) (Mimno et al, 2009) generalized LDA to tuples of documents from multiple languages","Similarly, Polylingual Topic Models (PLTM) (Mimno et al, 2009) generalized LDA to tuples of documents from multiple languages","['0', '25', '129', '131', '195']","<S sid =""0"" ssid = ""0"">Polylingual Topic Models</S><S sid =""25"" ssid = ""1"">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid =""129"" ssid = ""78"">We therefore evaluate the ability of the PLTM to generate bilingual lexica  similar to other work in unsupervised translation modeling (Haghighi et al.  2008).</S><S sid =""131"" ssid = ""80"">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid =""195"" ssid = ""4"">Additionally  PLTM can support the creation of bilingual lexica for low resource language pairs  providing candidate translations for more computationally intense alignment processes without the sentence-aligned translations typically used in such tasks.</S>",['Method_Citation']
9,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"Our baseline joint PLSA model (JPLSA) is closely related to the poly-lingual LDA model of (Mimno et al, 2009)","Our baseline joint PLSA model (JPLSA) is closely related to the poly-lingual LDA model of (Mimno et al, 2009)","['35', '193', '85', '192', '129']","<S sid =""35"" ssid = ""1"">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S><S sid =""193"" ssid = ""2"">We analyzed the characteristics of PLTM in comparison to monolingual LDA  and demonstrated that it is possible to discover aligned topics.</S><S sid =""85"" ssid = ""34"">Smoothed histograms of inter−language JS divergence A topic model specifies a probability distribution over documents  or in the case of PLTM  document tuples.</S><S sid =""192"" ssid = ""1"">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S sid =""129"" ssid = ""78"">We therefore evaluate the ability of the PLTM to generate bilingual lexica  similar to other work in unsupervised translation modeling (Haghighi et al.  2008).</S>",['Results_Citation']
10,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"We describe the model for two languages, but it is straightforward to generalize to more than two languages, as in (Mimno et al, 2009)","We describe the model for two languages, but it is straightforward to generalize to more than two languages, as in (Mimno et al, 2009)","['28', '129', '3', '81', '54']","<S sid =""28"" ssid = ""4"">We take a simpler approach that is more suitable for topically similar document tuples (where documents are not direct translations of one another) in more than two languages.</S><S sid =""129"" ssid = ""78"">We therefore evaluate the ability of the PLTM to generate bilingual lexica  similar to other work in unsupervised translation modeling (Haghighi et al.  2008).</S><S sid =""3"" ssid = ""3"">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S sid =""81"" ssid = ""30"">As with the previous figure  there are a small number of documents that contain only one topic in all languages  and thus have zero divergence.</S><S sid =""54"" ssid = ""3"">Although direct translations in multiple languages are relatively rare (in contrast with comparable documents)  we use direct translations to explore the characteristics of the model.</S>",['Method_Citation']
11,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"j=1 P (z2j|?) P (w 2 j| ?z2j) The difference between the JPLSA model and the poly-lingual topic model of (Mimno et al, 2009) is that we merge the vocabularies in the two languages and learn topic-specific word distributions over these merged vocabularies, instead of having pairs of topic-specific word distributions, one for each language, like in (Mimno et al, 2009)","The difference between the JPLSA model and the poly-lingual topic model of (Mimno et al, 2009) is that we merge the vocabularies in the two languages and learn topic-specific word distributions over these merged vocabularies, instead of having pairs of topic-specific word distributions, one for each language, like in (Mimno et al, 2009)","['78', '172', '143', '39', '6']","<S sid =""78"" ssid = ""27"">Although the model does not distinguish between topic assignment variables within a given document tuple (so it is technically incorrect to speak of different posterior distributions over topics for different documents in a given tuple)  we can nevertheless divide topic assignment variables between languages and use them to estimate a Dirichlet-multinomial posterior distribution for each language in each tuple.</S><S sid =""172"" ssid = ""6"">Second  because comparable texts may not use exactly the same topics  it becomes crucially important to be able to characterize differences in topic prevalence at the document level (do different languages have different perspectives on the same article?) and at the language-wide level (which topics do particular languages focus on?).</S><S sid =""143"" ssid = ""92"">We also do not count morphological variants: the model finds EN “rules” and DE “vorschriften ” but the lexicon contains only “rule” and “vorschrift.” Results remain strong as we increase K. With K = 3  T = 800  1349 of the 7200 candidate pairs for Spanish appeared in the lexicon. topic in different languages translations of each other?</S><S sid =""39"" ssid = ""5"">Additionally  PLTM assumes that each “topic” consists of a set of discrete distributions over words—one for each language l = 1  ...   L. In other words  rather than using a single set of topics Φ = {φ1  ...   φT}  as in LDA  there are L sets of language-specific topics  Φ1  ...   ΦL  each of which is drawn from a language-specific symmetric Dirichlet with concentration parameter βl.</S><S sid =""6"" ssid = ""2"">Topic models have been used for analyzing topic trends in research literature (Mann et al.  2006; Hall et al.  2008)  inferring captions for images (Blei and Jordan  2003)  social network analysis in email (McCallum et al.  2005)  and expanding queries with topically related words in information retrieval (Wei and Croft  2006).</S>",['Method_Citation']
12,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"Another difference between our model and the poly-lingual LDA model of (Mimno et al, 2009) is that we use maximum aposteriori (MAP) instead of Bayesian inference","Another difference between our model and the poly-lingual LDA model of (Mimno et al, 2009) is that we use maximum aposteriori (MAP) instead of Bayesian inference","['95', '193', '86', '85', '129']","<S sid =""95"" ssid = ""44"">Additionally  the predictive ability of PLTM is consistently slightly worse than that of (monolingual) LDA.</S><S sid =""193"" ssid = ""2"">We analyzed the characteristics of PLTM in comparison to monolingual LDA  and demonstrated that it is possible to discover aligned topics.</S><S sid =""86"" ssid = ""35"">Given a set of training document tuples  PLTM can be used to obtain posterior estimates of Φ'  ...   ΦL and αm.</S><S sid =""85"" ssid = ""34"">Smoothed histograms of inter−language JS divergence A topic model specifies a probability distribution over documents  or in the case of PLTM  document tuples.</S><S sid =""129"" ssid = ""78"">We therefore evaluate the ability of the PLTM to generate bilingual lexica  similar to other work in unsupervised translation modeling (Haghighi et al.  2008).</S>",['Method_Citation']
13,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"For computing distance we used the L1-norm of the difference, which worked a bit better than the Jensen Shannon divergence between the topic vectors used in (Mimno et al, 2009)","For computing distance we used the L1-norm of the difference, which worked a bit better than the Jensen Shannon divergence between the topic vectors used in (Mimno et al, 2009)","['79', '118', '139', '77', '182']","<S sid =""79"" ssid = ""28"">For each tuple we can then calculate the JensenShannon divergence (the average of the KL divergences between each distribution and a mean distribution) between these distributions.</S><S sid =""118"" ssid = ""67"">We calculate the Jensen-Shannon divergence between the topic distributions for each pair of individual documents in S that were originally part of the same tuple prior to separation.</S><S sid =""139"" ssid = ""88"">Results for K = 1  that is  considering only the single most probable word for each language  are shown in figure 6.</S><S sid =""77"" ssid = ""26"">Maximum topic probability in document Although the posterior distribution over topics for each tuple is not concentrated on one topic  it is worth checking that this is not simply because the model is assigning a single topic to the 1We use the R density function. tokens in each of the languages.</S><S sid =""182"" ssid = ""16"">As with EuroParl  we can calculate the JensenShannon divergence between pairs of documents within a comparable document tuple.</S>",['Method_Citation']
15,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"In previously reported work, (Mimno et al, 2009) evaluate parallel document retrieval using PLTM on Europarl speeches in English and Spanish, using training and test sets of size similar to ours","In previously reported work, (Mimno et al, 2009) evaluate parallel document retrieval using PLTM on Europarl speeches in English and Spanish, using training and test sets of size similar to ours","['129', '133', '130', '10', '29']","<S sid =""129"" ssid = ""78"">We therefore evaluate the ability of the PLTM to generate bilingual lexica  similar to other work in unsupervised translation modeling (Haghighi et al.  2008).</S><S sid =""133"" ssid = ""82"">We collected bilingual lexica mapping English words to German  Greek  Spanish  French  Italian  Dutch and Swedish.</S><S sid =""130"" ssid = ""79"">In the early statistical translation model work at IBM  these representations were called “cepts ” short for concepts (Brown et al.  1993).</S><S sid =""10"" ssid = ""6"">We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).</S><S sid =""29"" ssid = ""5"">A recent extended abstract  developed concurrently by Ni et al. (Ni et al.  2009)  discusses a multilingual topic model similar to the one presented here.</S>",['Method_Citation']
16,D09-1092,W12-3117,0,"Mimno et al, 2009",0,"We assume that a higher dimensionality can lead to a better repartitioning of the vocabulary over the topics. Multilingual LDA has been used before in natural language processing ,e.g .polylingual topic models (Mimno et al, 2009) or multilingual topic models for unaligned text (Boyd-Graber and Blei, 2009)","Multilingual LDA has been used before in natural language processing, e.g. polylingual topic models (Mimno et al, 2009) or multilingual topic models for unaligned text (Boyd-Graber and Blei, 2009)","['129', '36', '17', '172', '35']","<S sid =""129"" ssid = ""78"">We therefore evaluate the ability of the PLTM to generate bilingual lexica  similar to other work in unsupervised translation modeling (Haghighi et al.  2008).</S><S sid =""36"" ssid = ""2"">Each tuple is a set of documents that are loosely equivalent to each other  but written in different languages  e.g.  corresponding Wikipedia articles in French  English and German.</S><S sid =""17"" ssid = ""13"">We argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages.</S><S sid =""172"" ssid = ""6"">Second  because comparable texts may not use exactly the same topics  it becomes crucially important to be able to characterize differences in topic prevalence at the document level (do different languages have different perspectives on the same article?) and at the language-wide level (which topics do particular languages focus on?).</S><S sid =""35"" ssid = ""1"">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S>",['Method_Citation']
17,D09-1092,W11-2133,0,2009,0,"ji =wjk? M j i? m k=1w j k, (1) where M j is the topic distribution of document j and wk is the number of occurrences of phrase pair Xk in document j. Mimno et al (2009) extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipediaarticles)","Mimno et al (2009) extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipedia articles)","['129', '29', '32', '186', '54']","<S sid =""129"" ssid = ""78"">We therefore evaluate the ability of the PLTM to generate bilingual lexica  similar to other work in unsupervised translation modeling (Haghighi et al.  2008).</S><S sid =""29"" ssid = ""5"">A recent extended abstract  developed concurrently by Ni et al. (Ni et al.  2009)  discusses a multilingual topic model similar to the one presented here.</S><S sid =""32"" ssid = ""8"">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid =""186"" ssid = ""20"">Although we find that if Wikipedia contains an article on a particular subject in some language  the article will tend to be topically similar to the articles about that subject in other languages  we also find that across the whole collection different languages emphasize topics to different extents.</S><S sid =""54"" ssid = ""3"">Although direct translations in multiple languages are relatively rare (in contrast with comparable documents)  we use direct translations to explore the characteristics of the model.</S>",['Method_Citation']
18,D09-1092,W11-2133,0,2009,0,Tuple-specific topic distributions arelearned using LDA with distinct topic-word concentration parameters? l. Mimno et al (2009) show that PLTM sufficiently aligns topics in parallel corpora,Mimno et al (2009) show that PLTM sufficiently aligns topics in parallel corpora,"['192', '129', '193', '13', '54']","<S sid =""192"" ssid = ""1"">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S sid =""129"" ssid = ""78"">We therefore evaluate the ability of the PLTM to generate bilingual lexica  similar to other work in unsupervised translation modeling (Haghighi et al.  2008).</S><S sid =""193"" ssid = ""2"">We analyzed the characteristics of PLTM in comparison to monolingual LDA  and demonstrated that it is possible to discover aligned topics.</S><S sid =""13"" ssid = ""9"">To perform topic-based bibliometric analysis on these collections  it is necessary to have topic models that are aligned across languages.</S><S sid =""54"" ssid = ""3"">Although direct translations in multiple languages are relatively rare (in contrast with comparable documents)  we use direct translations to explore the characteristics of the model.</S>",['Method_Citation']
19,D09-1092,P14-2110,0,"Mimno et al, 2009",0,"A good candidate for multilingual topic analyses are polylin gual topic models (Mimno et al, 2009), which learn topics for multiple languages, creating tuples of language specific distributions over monolingual vocabularies for each topic","A good candidate for multilingual topic analyses are polylingual topic models (Mimno et al, 2009), which learn topics for multiple languages, creating tuples of language specific distributions over monolingual vocabularies for each topic","['17', '105', '129', '32', '172']","<S sid =""17"" ssid = ""13"">We argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages.</S><S sid =""105"" ssid = ""54"">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct  non-comparable documents in multiple languages.</S><S sid =""129"" ssid = ""78"">We therefore evaluate the ability of the PLTM to generate bilingual lexica  similar to other work in unsupervised translation modeling (Haghighi et al.  2008).</S><S sid =""32"" ssid = ""8"">Outside of the field of topic modeling  Kawaba et al. (Kawaba et al.  2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S><S sid =""172"" ssid = ""6"">Second  because comparable texts may not use exactly the same topics  it becomes crucially important to be able to characterize differences in topic prevalence at the document level (do different languages have different perspectives on the same article?) and at the language-wide level (which topics do particular languages focus on?).</S>",['Method_Citation']
20,D09-1092,P14-2110,0,2009,0,"3csLDATo train a polylingual topic model on social media, we make two modifications to the model of Mimno et al (2009): add a token specific language variable, and a process for identifying aligned top ics. First ,polylingual topic models require parallel or comparable corpora in which each document has an assigned language","To train a polylingual topic model on social media, we make two modifications to the model of Mimno et al (2009): add a token specific language variable, and a process for identifying aligned top ics","['24', '129', '54', '13', '137']","<S sid =""24"" ssid = ""20"">By linking topics across languages  polylingual topic models can increase cross-cultural understanding by providing readers with the ability to characterize the contents of collections in unfamiliar languages and identify trends in topic prevalence.</S><S sid =""129"" ssid = ""78"">We therefore evaluate the ability of the PLTM to generate bilingual lexica  similar to other work in unsupervised translation modeling (Haghighi et al.  2008).</S><S sid =""54"" ssid = ""3"">Although direct translations in multiple languages are relatively rare (in contrast with comparable documents)  we use direct translations to explore the characteristics of the model.</S><S sid =""13"" ssid = ""9"">To perform topic-based bibliometric analysis on these collections  it is necessary to have topic models that are aligned across languages.</S><S sid =""137"" ssid = ""86"">For every topic t we select a small number K of the most probable words in English (e) and in each “translation” language (E): Wte and Wtt  respectively.</S>",['Results_Citation']
