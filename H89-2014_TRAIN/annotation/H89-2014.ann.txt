Citance Number: 1 | Reference Article:  H89-2014.xml | Citing Article:  W93-0111.xml |  Citation Marker Offset:  ['178'] | Citation Marker:  Kupiec, 1989 | Citation Offset:  ['178'] | Citation Text:  <S sid ="178" ssid = "178">This approach is similar in spirit to the iterative computational approaches of the Hidden Markov Models (Kupiec, 1989</S> | Reference Offset:  ['27'] | Reference Text:  <S sid ="27" ssid = "27">The work described here also makes use of a hidden Markov model.</S> | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka, NTU |


Citance Number: 2 | Reference Article:  H89-2014.xml | Citing Article:  A92-1018.xml | Citation Marker Offset:  ['47'] | Citation Marker:  Kupiec, 1989a | Citation Offset:  ['47'] | Citation Text:  <S sid ="47" ssid = "47">In [Kupiec, 1989a], networks are used to selectively augment the context in a basic first- order model, rather than using uniformly second-order dependencies.</S> | Reference Offset:  ['124', '125'] | Reference Text:  <S sid ="124" ssid = "124">A model containing all of the refinements described, was tested using a magazine article containing 146 sentences (3,822 words).</S><S sid ="125" ssid = "125">A 30,000 word dictionary was used, supplemented by inflectional analysis for words not found directly in the dictionary.</S> | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka, NTU |


Citance Number: 3 | Reference Article:  H89-2014.xml | Citing Article:  A92-1018.xml | Citation Marker Offset:  ['108'] | Citation Marker:  Kupiec, 1989a | Citation Offset:  ['108'] | Citation Text:  <S sid ="108" ssid = "108">adequate training requires processing from tens of thousands to hundreds of thousands of tokens [Kupiec, 1989a].</S> | Reference Offset:  ['87', '88', '89'] | Reference Text:  <S sid ="87" ssid = "87">An alternative to uniformly increasing the order of the conditioning is to extend it selectively.</S><S sid ="88" ssid = "88">Mixed higher- order context can be modeled by introducing explicit state sequences.</S><S sid ="89" ssid = "89">In the arrangement the basic first-order network remains, permitting all possible category sequences, and modeling first-order dependency.</S> | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka, NTU |


Citance Number: 4 | Reference Article:  H89-2014.xml | Citing Article:  J93-2006.xml | Citation Marker Offset:  ['41'] | Citation Marker:  Kupiec 1989 | Citation Offset:  ['40','41'] | Citation Text:  <S sid ="40" ssid = "40">We report in Section 2 on our experiments on the assignment of part of speech to words in text.</S><S sid ="41" ssid = "41">The effectiveness of such models is well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)</S> | Reference Offset:  ['1'] | Reference Text:  <S sid ="1" ssid = "1">The paper describes refinements that are currently being investigated in a model for part-of-speech assignment to words in unrestricted text.</S> | Discourse Facet:  Aim_Citation | Annotator:  Kokil Jaidka, NTU |


Citance Number: 5 | Reference Article:  H89-2014.xml | Citing Article:  H91-1046.xml | Citation Marker Offset:  ['21'] | Citation Marker:  1989 | Citation Offset:  ['21'] | Citation Text:  <S sid ="21" ssid = "21">Kupiec (1989) has experimented with the inclusion of networks to model mixed-order dependencies.</S> |  Reference Offset:  ['88', '90'] | Reference Text:  <S sid ="88" ssid = "88">Mixed higher- order context can be modeled by introducing explicit state sequences.</S><S sid ="90" ssid = "90">The basic network is then augmented with the extra state sequences which model certain category sequences in more detail.</S> | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka, NTU |


Citance Number: 6 | Reference Article:  H89-2014.xml | Citing Article:  H91-1046.xml | Citation Marker Offset:  ['60'] | Citation Marker:  Kupiec, 1989 | Citation Offset:  ['60'] | Citation Text:  <S sid ="60" ssid = "60">The vocabulary entry may be a word or an equivalence class based on categories (Kupiec, 1989).</S> | Reference Offset:  ['29', '30'] | Reference Text:  <S sid ="29" ssid = "29">In this regard, word equivalence classes were used (Kupiec, 1989).</S><S sid ="30" ssid = "30">There it is assumed that the distribution of the use of a word depends on the set of categories it can assume, and words are partitioned accordingly.</S> | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka, NTU |


Citance Number: 7 | Reference Article:  H89-2014.xml | Citing Article:  C00-1081.xml | Citation Marker Offset:  ['85'] | Citation Marker:  Kupiec, 1989 | Citation Offset:  ['85'] | Citation Text:  <S sid ="85" ssid = "55">In a practical tagger (Kupiec, 1989), only the most frequent 100 words are lexicalized.</S> | Reference Offset:  ['69', '70'] | Reference Text:  <S sid ="69" ssid = "69">In a ranked list of words in the corpus the most frequent 100 words account for approximately 50% of the total tokens in the corpus, and thus data is available to estimate them reliably.</S><S sid ="70" ssid = "70">The most frequent 100 words of the corpus were assigned individually in the model, thereby enabling them to have different distributions over their categories.</S> | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka, NTU |


Citance Number: 8 | Reference Article:  H89-2014.xml | Citing Article:  H92-1022.xml | Citation Marker Offset: ['18'] | Citation Marker: 11 | Citation Offset:  ['18'] | Citation Text:  <S sid ="18" ssid = "18">The parameters of the model can be estimated from tagged (1, 3, 4, 6, 12] or untagged [2, 9, 11] text.</S> | Reference Offset:  ['2'] | Reference Text:  <S sid ="2" ssid = "2">The model has the advantage that a pre-tagged training corpus is not required.</S> | Discourse Facet:  Implication_Citation | Annotator:  Kokil Jaidka, NTU |


Citance Number: 9 | Reference Article: H89-2014.xml | Citing Article: H92-1022.xml | Citation Marker Offset: ['9'] | Citation Marker: 11 | Citation Offset: ['9'] | Citation Text:  <S sid ="9" ssid = "9">One area in which the statistical approach has done par­ ticularly well is automatic part of speech tagging, as­ signing each word in an input sentence its proper part of speech (1, 2, 3, 4, 6, 9, 11, 12].</S> | Reference Offset:  ['145'] | Reference Text:  <S sid ="145" ssid = "145">A stochastic method for assigning part-of-speech categories to unrestricted English text has been described.</S> | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka, NTU |


Citance Number: 10 | Reference Article:  H89-2014.xml | Citing Article:  C92-1060.xml | Citation Marker Offset:  ['124'] | Citation Marker:  Kupiec, 1989 | Citation Offset:  ['124'] | Citation Text:  <S sid ="124" ssid = "19">Instead, only common words are represented individually; the rest of the words in the dictionary are partitioned into word equivalence classes (Kupiec, 1989)</S> | Reference Offset:  ['29', '30'] | Reference Text:  <S sid ="29" ssid = "29">In this regard, word equivalence classes were used (Kupiec, 1989).</S><S sid ="30" ssid = "30">There it is assumed that the distribution of the use of a word depends on the set of categories it can assume, and words are partitioned accordingly.</S> | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka, NTU |


