Citance Number: 0 | Reference Article:  P98-1081.txt | Citing Article:  W01-0712.txt | Citation Marker Offset:  17292-17296 | Citation Marker:  1998 | Citation Offset:  17217-17297 | Citation Text:  Nine combination methods were originally suggested by Van Halteren et al. (1998) | Reference Offset: ['17041-17128'] | Reference Text:  In principle, this could remove the restric- tion of gain only in 2-2 and 1-1-1-1 cases | Discourse Facet: Method_Citation |  Annotator:  a |
Citance Number: 1 | Reference Article:  P98-1081.txt | Citing Article:  W01-0712.txt | Citation Marker Offset:  17966-17991 | Citation Marker:  van Halteren et al., 1998 | Citation Offset:  17843-17992 | Citation Text:  TAGPAIR uses classification pair weights based on the probability of a classification for some predicted classification pair (van Halteren et al., 1998) | Reference Offset: ['18778-19002'] | Reference Text:  9 The explanation for this can be found when we examine the differences within the Memory- Based general strategy: the more feature infor- mation is stored, the higher the accuracy on Tune, but the lower the accuracy on Test | Discourse Facet: Method_Citation |  Annotator:  a |
Citance Number: 2 | Reference Article:  P98-1081.txt | Citing Article:  W01-0712.txt | Citation Marker Offset:  18252-18256 | Citation Marker:  1998 | Citation Offset:  18226-18297 | Citation Text:  Like Van Halteren et al. (1998), we evaluated two features combinations | Reference Offset: ['8020-8093'] | Reference Text:  A beam search is then used to find the highest probability tag se- quence | Discourse Facet: Method_Citation |  Annotator:  a |
Citance Number: 3 | Reference Article:  P98-1081.txt | Citing Article:  W02-1004-parscit-section.txt | Citation Marker Offset:  23491-23516 | Citation Marker:  van Halteren et al., 1998 | Citation Offset:  23341-23785 | Citation Text:  An alternative method for estimating the parameters is to approximate them with the performance of the th classifier (a performance-based combiner) (van Halteren et al., 1998; ... therefore giving more weight to classifiers that have a smaller classification error (the method will be referred to as PB) | Reference Offset: ['3393-3633', '3635-3671'] | Reference Text:  It has been shown that, when the errors are uncorrelated to a sufficient degree, the resulting combined classifier will often per- form better than all the individual systems (Ali and Pazzani 1996; Chan and Stolfo 1995; Tumer and Gosh 1996)...The underlying assumption is twofold | Discourse Facet: Method_Citation |  Annotator:  a |
Citance Number: 4 | Reference Article:  P98-1081.txt | Citing Article:  W02-1004-parscit-section.txt | Citation Marker Offset:  26097-26222 | Citation Marker:  1998 | Citation Offset:  26050-26075 | Citation Text:  In the experiments presented in van Halteren et al. (1998), this method was the best performer among the presented methods | Reference Offset: ['5271-5518'] | Reference Text:  The first and oldest system uses a tradi-tional trig-ram model (Steetskamp 1995; hence- forth tagger T, for Trigrams), based on context statistics P(ti[ti-l,ti-2) and lexical statistics P(tilwi) directly estimated from relative cor-pus frequencies | Discourse Facet: Method_Citation |  Annotator:  a |
Citance Number: 5 | Reference Article:  P98-1081.txt | Citing Article:  W02-1004-parscit-section.txt | Citation Marker Offset:  25959-26222 | Citation Marker:  van Halteren et al., 1998 | Citation Offset:  25959-26222 | Citation Text:  A very common technique for combination in such a case is by voting(...van Halteren et al., 1998...). In the simplest model, each classifier votes for its classification and the sense that receives the most number of votes wins... | Reference Offset: ['4726-4925'] | Reference Text:  Since the compo- nent taggers all used n-gram statistics to model context probabilities and the knowledge repre- sentation was hence fundamentally the same in each component, the results were limited | Discourse Facet: Method_Citation |  Annotator:  a |
Citance Number: 6 | Reference Article:  P98-1081.txt | Citing Article:  W02-1004-parscit-section.txt | Citation Marker Offset: 26817-26821 | Citation Marker: 1998 | Citation Offset: 26796-27474 | Citation Text: Van Halteren et al. (1998) introduce a modified version of voting called TagPair...Each classifier votes for its classification and every pair of classifiers votes for the sense that is most likely given the joint classification. | Reference Offset: ['960-1109'] | Reference Text:  All combination taggers outperform their best component, with the best combina- tion showing a 19.1% lower error rate than the best individual tagger | Discourse Facet: Method_Citation |  Annotator:  a |
Citance Number: 7 | Reference Article:  P98-1081.txt | Citing Article:  E99-1025-parscit-section.txt | Citation Marker Offset:  19413-19417 | Citation Marker:  1998 | Citation Offset:  19343-19588 | Citation Text:  We consider three voting strategies suggested by van Halteren et al. (1998): equal vote, where each classifier's vote is weighted equally, overall accuracy, where the weight depends on the overall accuracy of a classifier, and pair-wise voting | Reference Offset: ['960-1109', '1124-1293'] | Reference Text:  All combination taggers outperform their best component, with the best combina- tion showing a 19.1% lower error rate than the best individual tagger...In all Natural Language Processing (NLP) systems, we find one or more language models which are used to predict, classify and/or interpret language related observa-tions | Discourse Facet: Method_Citation |  Annotator:  a |
Citance Number: 8 | Reference Article:  P98-1081.txt | Citing Article:  W03-1728.txt | Citation Marker Offset:  7228-7253 | Citation Marker:  van Halteren et al., 1998 | Citation Offset:  7211-7254 | Citation Text:  pairwise voting (van Halteren et al., 1998) | Reference Offset: ['21209-21431'] | Reference Text:  A major factor in the quality of the combi- nation results is obviously the quality of the best component: all combinations with E score higher than those without E (although M, R and T together are able to beat E alone11) | Discourse Facet: Method_Citation |  Annotator:  a |
Citance Number: 9 | Reference Article:  P98-1081.txt | Citing Article:  P06-2060-parscit-section.txt | Citation Marker Offset:  6390-6394 | Citation Marker:  1998 | Citation Offset:  6374-6523 | Citation Text:  Halteren et al (1998) compare a number of voting methods including a Majority Vote scheme with other combination methods for part of speech tagging | Reference Offset: ['4726-4925'] | Reference Text:  Since the compo- nent taggers all used n-gram statistics to model context probabilities and the knowledge repre- sentation was hence fundamentally the same in each component, the results were limited | Discourse Facet: Method_Citation |  Annotator:  a |
Citance Number: 10 | Reference Article:  P98-1081.txt | Citing Article:  A00-1024-parscit-section.txt | Citation Marker Offset:  5797-5822 | Citation Marker:  van Halteren et al., 1998 | Citation Offset:  5791-5917 | Citation Text:  (cf. (van Halteren et al., 1998) who found that combining the results of several part of speech taggers increased performance) | Reference Offset: ['4726-4925'] | Reference Text:  Since the compo- nent taggers all used n-gram statistics to model context probabilities and the knowledge repre- sentation was hence fundamentally the same in each component, the results were limited | Discourse Facet: Method_Citation |  Annotator:  a |
Citance Number: 11 | Reference Article:  P98-1081.txt | Citing Article:  W05-1518-parscit-section.txt | Citation Marker Offset:  2088-2098 | Citation Marker:  van Halteren et ... al., 1998 | Citation Offset:  1618-1714 | Citation Text:  Combination techniques have been successfully applied to part of speech tagging (van Halteren et ... al., 1998; ... the investigators were able to achieve significant improvements over the previous best tagging results | Reference Offset: ['11274-11461'] | Reference Text:  As far as we know this is also one of the first rigorous measurements of the relative quality of different tagger generators, using a single tagset and dataset and identical circumstances | Discourse Facet: Method_Citation |  Annotator:  a |
Citance Number: 12 | Reference Article:  P98-1081.txt | Citing Article:  W05-1518-parscit-section.txt | Citation Marker Offset:  1699-1714 | Citation Marker:  Van Halteren et al. (1998) have generalized this approach for higher number of classifiers in their TotPrecision voting method. The vote of each classifier (parser) is weighted by their respective accuracy | Citation Offset:  2161-2265 | Citation Text:  al., 1998; | Reference Offset: ['15980-16058'] | Reference Text:  So far, we have only used information on the performance of individual taggers | Discourse Facet: Method_Citation |  Annotator:  a |
Citance Number: 13 | Reference Article:  P98-1081.txt | Citing Article:  W05-1518-parscit-section.txt | Citation Marker Offset:  2088-2098 | Citation Marker:  Parallel to (van Halteren et al., 1998), we ran experiments with two stacked classifiers, Memory-Based, and Decision-Tree-Based | Citation Offset:  2088-2097 | Citation Text:  1998 | Reference Offset: ['154-184'] | Reference Text:  of Computational Linguistics   | Discourse Facet: Method_Citation |  Annotator:  a |
Citance Number: 14 | Reference Article:  P98-1081.txt | Citing Article:  W05-1518-parscit-section.txt | Citation Marker Offset:  11761-11765 | Citation Marker:  In all experiments, the TotPrecision voting scheme of (van Halteren et al., 1998) has been used | Citation Offset:  11740-11945 | Citation Text:  van Halteren et al., 1998 | Reference Offset:  ['19647-19719', '20775-20901', '20970-21036'] | Reference Text:  The first choice for this is to use a Memory-Based second level learner ... To examine if the overtraining effects are specific to this particular second level classifier, we also used the C5.0 system ... for the induction of decision trees, on the same training material | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka, NTU | Reference Offset: ['4726-4925'] | Reference Text:  Since the compo- nent taggers all used n-gram statistics to model context probabilities and the knowledge repre- sentation was hence fundamentally the same in each component, the results were limited | Discourse Facet: Method_Citation |  Annotator:  a |
Citance Number: 15 | Reference Article:  P98-1081.txt | Citing Article:  W00-0733-parscit-section.txt | Citation Marker Offset:  3906-4120 | Citation Marker:  The most advanced voting method examines output values of pairs of classifiers and assigns weights to tags based on how often they appear with this pair in the tuning data (Tag-Pair, Van Halteren et al., (1998)) | Citation Offset:  2948-3221 | Citation Text:  We will evaluate nine different methods for combining the output of our five chunkers (Van Halteren et al., 1998). Five are so-called voting methods. They assign weights to the output of the individual systems and use these weights to determine the most probable output tag | Reference Offset:  ['17328-17533'] | Reference Text:  A next step is to examine them in pairs. We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka, NTU | Reference Offset: ['960-1109'] | Reference Text:  All combination taggers outperform their best component, with the best combina- tion showing a 19.1% lower error rate than the best individual tagger | Discourse Facet: Method_Citation |  Annotator:  a |
Citance Number: 16 | Reference Article:  P98-1081.txt | Citing Article:  W00-0733-parscit-section.txt | Citation Marker Offset:  3035-3060 | Citation Marker:  Van Halteren et al., 1998 | Citation Offset:  4114-4118 | Citation Text:  1998 | Reference Offset:  ['13877-14023', '14461-14555'] | Reference Text:  There are many ways in which the results of the component taggers can be combined, selecting a single tag from the set proposed by these taggers ... However, it appears more useful to give more weight to taggers which have proved their quality | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka, NTU | Reference Offset: ['154-184'] | Reference Text:  of Computational Linguistics   | Discourse Facet: Method_Citation |  Annotator:  a |
Citance Number: 17 | Reference Article:  P98-1081.txt | Citing Article:  W00-0733-parscit-section.txt | Citation Marker Offset:  4941-4966 | Citation Marker:  Van Halteren et al., 1998 | Citation Offset:  4811-4967 | Citation Text:  For this purpose we have used the part-of-speech tag of the current word as compressed representation of the first stage input (Van Halteren et al., 1998) | Reference Offset:  ['19512-19645'] | Reference Text:  The second stage can be provided with the first level outputs, and with additional information, e.g. about the original input pattern | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka, NTU | Reference Offset: ['5271-5518'] | Reference Text:  The first and oldest system uses a tradi-tional trig-ram model (Steetskamp 1995; hence- forth tagger T, for Trigrams), based on context statistics P(ti[ti-l,ti-2) and lexical statistics P(tilwi) directly estimated from relative cor-pus frequencies | Discourse Facet: Method_Citation |  Annotator:  a |
Citance Number: 18 | Reference Article:  P98-1081.txt | Citing Article:  J01-2002-parscit-section.txt | Citation Marker Offset:  6962-7224 | Citation Marker:  van Halteren, Zavrel, and Daelemans 1998 | Citation Offset:  6943-7182 | Citation Text:  First experiments (van Halteren, Zavrel, and Daelemans 1998...) demonstrated the basic validity of the approach for tagging, with the error rate of the best combiner being 19.1% lower than that of the best individual tagger (van Halteren, Zavrel, and Daelemans 1998). | Reference Offset:  ['22735-23104'] | Reference Text:  The most important observation is that every combination (significantly) outperforms the combination of any strict subset of its components. Also of note is the improvement yielded by the best combination. The pairwise voting system, using all four individual taggers, scores 97.92% correct on Test, a 19.1% reduction in error rate over the best individual system | Discourse Facet:  Results_Citation | Annotator:  Kokil Jaidka, NTU | Reference Offset: ['1124-1293'] | Reference Text:  In all Natural Language Processing (NLP) systems, we find one or more language models which are used to predict, classify and/or interpret language related observa-tions | Discourse Facet: Method_Citation |  Annotator:  a |
Citance Number: 19 | Reference Article:  P98-1081.txt | Citing Article:  J01-2002-parscit-section.txt | Citation Marker Offset:  34169-34279 | Citation Marker:  van Halteren, Zavrel, and Daelemans 1998. | Citation Offset:  103207-103211 | Citation Text:  (van Halteren, Zavrel, and Daelemans 1998). However, these experiments were restricted to a single language, a single tagset and, more importantly, a limited amount of training data for the combiners | Reference Offset:  ['25304-25479'] | Reference Text:  extend our investigation of combination, starting with additional component taggers and selection strategies, and going on to shifts to other tagsets and/or languages | Discourse Facet:  Implication_Citation | Annotator:  Kokil Jaidka, NTU | Reference Offset: ['20941-21006'] | Reference Text:  Also of note is the improvement yielded by the best combi- nation | Discourse Facet: Method_Citation |  Annotator:  a |
Citance Number: 20 | Reference Article:  P98-1081.txt | Citing Article:  J01-2002-parscit-section.txt | Citation Marker Offset:  103139-103262 | Citation Marker:  van Halteren, Zavrel, and Daelemans (1998) | Citation Offset:  102860-102864 | Citation Text:  Compare this to the "tune" set in van Halteren, Zavrel, and Daelemans (1998). This consisted of 114K ... tokens, but, because of a 92.5% agreement over all four taggers, it yielded less than 9K tokens of useful training material to resolve disagreements. This was suspected to be the main reason for the relative lack of performance by the more sophisticated combiners | Reference Offset:  ['10878-10967', '13373-13378', '13455-13488', '20566-20773'] | Reference Text:  The second part, Tune, consists of 10% of the data (every ninth utterance, 114479 tokens) ... 92.49 ... Table 1: Tagger agreement on Tune ... Tune is probably too small to collect case bases which can leverage the stacking effect convincingly, especially since only 7.51% of the second stage material shows disagreement between the featured tags | Discourse Facet:  Implication_Citation | Annotator:  Kokil Jaidka, NTU | Reference Offset: ['10399-10545', '10658-10932'] | Reference Text:  The third and final part, Test, consists of the remaining 10% (.115101 tokens) and is used for the final performance measure- ments of all tuggers...The data in Train (for individual tuggers) and Tune (for combination tuggers) is to be the only information used in tagger construction: all components of all tuggers (lexicon, context statistics, etc.) are to be entirely data driven and no manual adjustments are to be done | Discourse Facet: Method_Citation |  Annotator:  a |
Citance Number: 21 | Reference Article:  P98-1081.txt | Citing Article:  J01-2002-parscit-section.txt | Citation Marker Offset: 102860-102864 | Citation Marker: 1998 | Citation Offset:  102684-103066 | Citation Text:  For part-of-speech tagging, a significant increase in accuracy through combining the output of different taggers was first demonstrated in van Halteren, Zavrel, and Daelemans (1998) ... In both approaches, different tagger generators were applied to the same training data and their predictions combined using different combination methods, including stacking | Reference Offset:  ['182-243', '945-1206', '18852-18873'] | Reference Text:  Improving Data Driven Wordclass Tagging by System Combination ...  Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data. After comparison, their outputs are combined using several voting strategies and second stage classifiers ... 6 Stacked classifiers | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka, NTU | Reference Offset: ['21765-21827', '21829-22057'] | Reference Text:  the differ-ences within the group ME/ER/ET are not significant...nation scheme is the fact that for the most suc- cessful combination schemes, one has to reserve a non-trivial portion (in the experiment 10% of the total material) of the annotated data to set the parameters for the combination | Discourse Facet: Method_Citation |  Annotator:  a |
Citance Number: 22 | Reference Article:  P98-1081.txt | Citing Article:  J01-2002-parscit-section.txt | Citation Marker Offset:  17833-17925 | Citation Marker:  van Halteren, Zavrel, and Daelemans 1998 | Citation Offset:  6961-7003 | Citation Text:  One of the best methods for tagger combination in (van Halteren, Zavrel, and Daelemans 1998) is the TagPair method. It looks at all situations where one tagger suggests tag 1 and the other tag 2 and estimates the probability that in this situation the tag should actually be tag x. Although it is presented as a variant of voting in that paper, it is in fact also a stacked classifier, because it does not necessarily select one of the tags suggested by the component taggers | Reference Offset:  ['17328-17533', '19148-19243', '19469-19645', '20211-20294'] | Reference Text:  A next step is to examine them in pairs. We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx ... The practice of feeding the outputs of a number of classifiers as features for a next learner ... is usually called stacking (Wolpert 1992). The second stage can be provided with the first level outputs, and with additional information, e.g. about the original input pattern ... Surprisingly, none of the Memory-Based  based methods reaches the quality of TagPair | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka, NTU | Reference Offset: ['16505-16655'] | Reference Text:  When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each pos- sible tag, i.e | Discourse Facet: Method_Citation |  Annotator:  a |
Citance Number: 23 | Reference Article:  P98-1081.txt | Citing Article:  J01-2002-parscit-section.txt | Citation Marker Offset:  56632-56636 | Citation Marker:  van Halteren,Zavrel, and Daelemans 1998 | Citation Offset:  33144-33184 | Citation Text:  In previous work (van Halteren,Zavrel, and Daelemans 1998), we were unable to confirm the latter half of the hypothesis unequivocally. As we judged this to be due to insufficient training data for proper training of the second-level classifiers | Reference Offset:  ['20566-20773', '25304-25479'] | Reference Text:  Tune is probably too small to collect case bases which can leverage the stacking effect convincingly, especially since only 7.51% of the second stage material shows disagreement between the featured tags ... extend our investigation of combination, starting with additional component taggers and selection strategies, and going on to shifts to other tagsets and / or languages | Discourse Facet:  Implication_Citation | Annotator:  Kokil Jaidka, NTU | Reference Offset: ['19257-19518', '-1-165'] | Reference Text:  To examine if the overtraining effects are spe- cific to this particular second level classifier, we also used the C5.0 system, a commercial version of the well-known program C4.5 (Quinlan 1993) for the induction of decision trees, on the same training material...1?Because C5.0 prunes the decision tree, the overfitting of training material (Tune) is less than with Memory-Based learn- ing, but the results on Test are also worse | Discourse Facet: Method_Citation |  Annotator:  a |
Citance Number: 24 | Reference Article:  P98-1081.txt | Citing Article:  J01-2002-parscit-section.txt | Citation Marker Offset:  89248-89252 | Citation Marker:  1998 | Citation Offset:  89112-89367 | Citation Text:  The most important result that has undergone a change between van Halteren, Zavrel, and Daelemans (1998) and our current experiments is the relative accuracy of TagPair and stacked systems such as MBL | Reference Offset:  ['18640-18850', '20211-20294'] | Reference Text:  When used on Test, the pairwise voting strategy (TagPair) clearly outperforms the other voting strategies, 8 but does not yet approach the level where all tying majority votes are handled correctly (98.31%) ... Surprisingly, none of the Memory-Based based methods reaches the quality of TagPair | Discourse Facet:  Results_Citation | Annotator:  Kokil Jaidka, NTU | Reference Offset: ['20796-20939'] | Reference Text:  The most impor- tant observation is that every combination (sig- nificantly) outperforms the combination of any strict subset of its components | Discourse Facet: Method_Citation |  Annotator:  a |
Citance Number: 25 | Reference Article:  P98-1081.txt | Citing Article:  J01-2002-parscit-section.txt | Citation Marker Offset:  33048-33075 | Citation Marker:  van Halteren, Zavrel, and Daelemans 1998 | Citation Offset:  17883-17925 | Citation Text:  The first is the LOB corpus ... which we used in the earlier experiments as well (van Halteren, Zavrel, and Daelemans 1998) and which has proved to be a good testing ground. | Reference Offset:  ['8871-8956'] | Reference Text:  The data we use for our experiment consists of the tagged LOB corpus (Johansson 1986) | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka, NTU | Reference Offset: ['12647-12747'] | Reference Text:  The pat- terns between the brackets give the distribution of correct/incorrect tags over the systems | Discourse Facet: Method_Citation |  Annotator:  a |
Citance Number: 26 | Reference Article:  P98-1081.txt | Citing Article:  J01-2002-parscit-section.txt | Citation Marker Offset:  56592-56764 | Citation Marker:  van Halteren, Zavrel, and Daelemans 1998 | Citation Offset:  33094-33240 | Citation Text:  In van Halteren, Zavrel, and Daelemans (1998) we used a straightforward implementation of HMM's, which turned out to have the worst accuracy of the four competing methods | Reference Offset:  ['946-1095', '15601-15602', '15922-15985'] | Reference Text:  Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data ... T ... Table 2: Accuracy of individual taggers and combination methods | Discourse Facet:  Results_Citation | Annotator:  Kokil Jaidka, NTU | Reference Offset: ['13299-13412'] | Reference Text:  Each tagger is allowed to vote for the tag of its choice and the tag with the highest number of votes is selected | Discourse Facet: Method_Citation |  Annotator:  a |
Citance Number: 27 | Reference Article:  P98-1081.txt | Citing Article:  J01-2002-parscit-section.txt | Citation Marker Offset:  7452-7491 | Citation Marker:  van Halteren,Zavrel, and Daelemans 1998 | Citation Offset:  7434-7673 | Citation Text:  With LOB and a single 114K tune set (van Halteren, Zavrel, and Daelemans 1998), both MBL and Decision Trees degraded significantly when adding context, and MBL degraded when adding the word | Reference Offset:  ['20775-20901', '21167-21205', '19647-19719', '20211-20294'] | Reference Text:  The first choice for this is to use a Memory-Based second level learner ... Surprisingly, none of the Memory-Based based methods reaches the quality of TagPair ... To examine if the overtraining effects are specific to this particular second level classifier, we also used the C5.0 system ... but the results on Test are also worse | Discourse Facet:  Results_Citation | Annotator:  Kokil Jaidka, NTU | Reference Offset: ['18513-18684'] | Reference Text:  For the first two the similarity metric used during tagging is a straightforward overlap count; for the third we need to use an Information Gain weighting (Daelemans ct al | Discourse Facet: Method_Citation |  Annotator:  a |
Citance Number: 28 | Reference Article:  P98-1081.txt | Citing Article:  J01-2002-parscit-section.txt | Citation Marker Offset:  102891-103066 | Citation Marker:  van Halteren, Zavrel, and Daelemans (1998) | Citation Offset:  102684-102865 | Citation Text:  As we now apply the methods of van Halteren, Zavrel, and Daelemans (1998) to WSJ as well, it is easier to make a comparison | Reference Offset: ['182-243'] | Reference Text: Improving Data Driven Wordclass Tagging by System Combination| Discourse Facet:  Aim_Citation | Annotator:  Kokil Jaidka, NTU | Reference Offset: ['960-1109'] | Reference Text:  All combination taggers outperform their best component, with the best combina- tion showing a 19.1% lower error rate than the best individual tagger | Discourse Facet: Method_Citation |  Annotator:  a |
