* Data Preprocessing *

The original SciSumm corpus contained data for 10 papers from the ACL Anthology network. For each of these papers, citing sentences were extracted from all its citing papers. Each citing sentence was then matched to a text segment in the original paper creating the final annotated dataset. The original source text for the papers in the SciSumm corpus was not sentence segmented, which made it difficult to compute evaluation metrics.

We pre-processed the SciSumm corpus in the following way. We first sentence segmented the source paper text for each of the 10 papers provided in the original SciSumm corpus. We then matched each of these source sentences to the SciSumm annotation files. This provided us a with a fixed set of source sentences from the original files, a subset of which were matched to each citing sentence. In this way given, a citing sentence, we can compare the matching sentences from the source paper returned by our system to the gold standard sentences matched from the source paper and compute precision/recall. 

The average number of source sentences matched for each citing sentence is 1.28 (with standard deviation 1.92). The maximum number of source sentences matched for a citing sentence is 7. Given that the total number of source sentences for papers ranges between from 100 to 600, this makes it a very challenging classification problem. 


* Baseline System *

We first created a baseline system based on TF*IDF cosine similarity. The IDF's were computed over all sentences for each source paper, thus the IDF values differed across each of the 10 source papers. For any citing sentence, we computed the TF*IDF cosine similarity with all the sentences in the source paper and any sentences that had a cosine similarity higher than a given threshold were added to the matched sentences. The following table shows the precision/recall for different values of the cosine threshold:

|----------------------+-----------+--------+----------|
| similarity threshold | precision | recall | F1-score |
|----------------------+-----------+--------+----------|
|                 0.01 |     0.027 |  0.641 |    0.051 |
|                 0.05 |     0.048 |  0.426 |    0.087 |
|                  0.1 |     0.060 |  0.235 |    0.095 |
|                  0.2 |     0.079 |  0.081 |    0.080 |
|                  0.3 |     0.062 |  0.032 |    0.042 |
|                  0.4 |     0.022 |  0.085 |    0.012 |
|                  0.5 |     0.007 |  0.002 |    0.003 |
|----------------------+-----------+--------+----------|

The F1-score seems to reach a maxima at a similarity threshold of about 0.1. The recall at the threshold of 0.1 is about 0.23, while the precision is only 0.06. This suggests that initial progress can be made on this problem by first removing these spurious matches that have high lexical similarity. We present some error analysis in the next section.

* Error Analysis for the Baseline System * 

A number of errors made by the baseline system are due to source sentences that match the words but differ slightly in their information content. Here is an example.

--------------------
Citing text: use the BNC to build a co-occurrence graph for nouns, based on a co-occurrence frequency threshold
--------------------

--------------------
True positives:
--------------------
1. Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1.
--------------------


--------------------
False positives:
--------------------
1. Based on the intuition that nouns which co-occur in a list are often semantically related, we extract contexts of the form Noun, Noun,... and/or Noun, e.g. "genomic DNA from rat, mouse and dog".
2. To detect the different areas of meaning in our local graphs, we use a cluster algorithm for graphs (Markov clustering, MCL) developed by van Dongen (2000).
3. The algorithm is based on a graph model representing words and relationships between them.
--------------------

Even though the false positive sentences contain the same lexical items (nouns, co-occurrence, graph), they differ slightly in the facts presented. Detection of such subtle differences in meaning might be challenging for an automated system.

Another set of difficult sentences is when the citing sentence says something that is implied by the sentence in the source paper. For example:

--------------------
Citing text: The line of our argument below follows a proof provided in ... for the maximum likelihood estimator based on nite tree distributions
--------------------

--------------------
False negatives:
--------------------
1. We will show that in both cases the estimated probability is tight.
--------------------

Here, the citing text mentions a proof from source paper, but to match the sentence in the source paper, the system needs to understand that the act of showing something in a scientific paper constitutes a proof.

* Supervised System * 

Based on our error analysis, we are currently working on creating a supervised system with knowledge based features derived from WordNet, syntactic dependency based features, and distributional features in addition to the simple lexical features like cosine similarity. We have implemented a subset of these features, but the current system does not outperform our baseline system.
++++++++++++++++++++++++++++


Here are the average ROUGE-1 scores for each of the methods as you asked. The ROUGE-1 scores were computed for each citing sentence in each annotation file separately and then averaged. 

Cosine similarity baseline (threshold = 0.1) : 0.44
Lexical features : 0.64
Lexical + Knowledge based features: 0.63
Lexical + Knowledge based + syntactic features: 0.64