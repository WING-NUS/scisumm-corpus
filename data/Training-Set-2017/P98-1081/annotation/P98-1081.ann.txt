Citance Number: 2 | Reference Article:  P98-1081.xml | Citing Article:  W01-0712.xml | Citation Marker Offset:  ['130'] | Citation Marker:  van Halteren et al., 1998 | Citation Offset:  ['130'] | Citation Text:  <S sid ="130" ssid = "116">And .nally, TAGPAIR uses classi.cation pair weights based on the probability of a classi.cation for some predicted classi.cation pair (van Halteren et al., 1998).</S> | Reference Offset:  ['116', '117'] | Reference Text:  <S sid ="116" ssid = "37">A next step is to examine them in pairs.</S><S sid ="117" ssid = "38">We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx.</S> | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka, NTU |


Citance Number: 3 | Reference Article:  P98-1081.xml | Citing Article:  W01-0712.xml | Citation Marker Offset:  ['135'] | Citation Marker:  1998 | Citation Offset:  ['134','135'] | Citation Text:  <S sid ="134" ssid = "120">Like Van Halteren et al.</S><S sid ="135" ssid = "121">(1998), we evaluated two features combinations.</S> | Reference Offset:  ['80', '89'] | Reference Text:  <S sid ="80" ssid = "1">In order to see whether combination of the component tuggers is likely to lead to improvements of tagging quality, we first examine the results of the individual taggers when applied to Tune.</S><S sid ="89" ssid = "10">We accept that we are measuring quality in relation to a specific tagging</S> | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka, NTU |


Citance Number: 5 | Reference Article:  P98-1081.xml | Citing Article:  W02-1004.xml | Citation Marker Offset:  ['105'] | Citation Marker:  1998 | Citation Offset:  ['105','106','107','108','109','110','111'] | Citation Text:  <S sid ="105" ssid = "48">Van Halteren et al.</S><S sid ="106" ssid = "49">(1998) introduce a modi.ed version of voting called TagPair.</S><S sid ="107" ssid = "50">Under this model, the conditional probability that the word sense is s given that classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), is com­puted on development data, and the posterior prob­ability is estimated as N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid ="108" ssid = "51">j where sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid ="109" ssid = "52">Each classi.er votes for its classi.cation and every pair of classi.ers votes for the sense that is most likely given the joint classi.cation.</S><S sid ="110" ssid = "53">In the experi­ments presented in van Halteren et al.</S><S sid ="111" ssid = "54">(1998), this method was the best performer among the presented methods.</S> | Reference Offset:  ['116','117','124'] | Reference Text:  <S sid ="116" ssid = "37">A next step is to examine them in pairs.</S><S sid ="117" ssid = "38">We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx.</S><S sid ="124" ssid = "45">When used on Test, the pairwise voting strategy (TagPair) clearly outperforms the other voting strategies, 8 but does not yet approach the level where all tying majority votes are handled correctly (98.31%).</S> | Discourse Facet:  Results_Citation | Annotator:  Kokil Jaidka, NTU |


Citance Number: 7 | Reference Article:  P98-1081.xml | Citing Article:  E99-1025.xml | Citation Marker Offset:  ['118'] | Citation Marker:  1998 | Citation Offset:  ['117','118'] | Citation Text:  <S sid ="117" ssid = "73">We consider three voting strategies suggested by van Halteren et al.</S><S sid ="118" ssid = "74">(1998): equal vote, where each classifier&apos;s vote is weighted equally, overall accuracy, where the weight depends on the overall accuracy of a classifier, and pair&apos;wise voting.</S> | Reference Offset:  ['100', '102', '116', '118'] | Reference Text:  <S sid ="100" ssid = "21">The most democratic option is to give each tagger one vote (Majority).</S><S sid ="102" ssid = "23">This can be general quality, e.g. each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g. each tagger votes its precision on the suggested tag (Tag- Precision).</S><S sid ="116" ssid = "37">A next step is to examine them in pairs.</S><S sid ="118" ssid = "39">When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers.</S> | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka, NTU |


Citance Number: 9 | Reference Article:  P98-1081.xml | Citing Article:  P06-2060.xml | Citation Marker Offset:  ['35'] | Citation Marker:  1998 | Citation Offset:  ['35'] | Citation Text:  <S sid ="35" ssid = "13">Halteren et al (1998) compare a number of voting methods including a Majority Vote scheme with other combination methods for part of speech tagging.</S> | Reference Offset:  ['100'] | Reference Text:  <S sid ="100" ssid = "21">The most democratic option is to give each tagger one vote (Majority).</S> | Discourse Facet:  Aim_Citation | Annotator:  Kokil Jaidka, NTU |


Citance Number: 10 | Reference Article:  P98-1081.xml | Citing Article:  A00-1024.xml | Citation Marker Offset:  ['43'] | Citation Marker:  van Halteren et al., 1998 | Citation Offset:  ['42','43'] | Citation Text:  <S sid ="42" ssid = "17">Thirdly, this approach is compatible with in­ corporating multiple components of the same type to improve performance (cf.</S><S sid ="43" ssid = "18">(van Halteren et al., 1998) who found that combining the results of several part of speech taggers increased performance).</S> | Reference Offset:  ['158'] | Reference Text:  <S sid ="158" ssid = "79">Our experiment shows that, at least for the task at hand, combination of several different systems allows us to raise the performance ceiling for data driven systems.</S> | Discourse Facet:  Results_Citation | Annotator:  Kokil Jaidka, NTU |


Citance Number: 11 | Reference Article:  P98-1081.xml | Citing Article:  W05-1518.xml | Citation Marker Offset:  ['10'] | Citation Marker:  van Halteren et ... al., 1998 | Citation Offset:  ['10','11'] | Citation Text:  <S sid ="10" ssid = "10">Combination techniques have been successfully applied to part of speech tagging (van Halteren et al., 1998; Brill and Wu, 1998; van Halteren et al., 2001).</S><S sid ="11" ssid = "11">In both cases the investigators were able to achieve significant improvements over the previous best tagging results.</S> | Reference Offset:  ['101', '102'] | Reference Text:  <S sid ="101" ssid = "22">However, it appears more useful to give more weight to taggers which have proved their quality.</S><S sid ="102" ssid = "23">This can be general quality, e.g. each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g. each tagger votes its precision on the suggested tag (Tag- Precision).</S> | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka, NTU |


Citance Number: 12 | Reference Article:  P98-1081.xml | Citing Article:  W05-1518.xml | Citation Marker Offset:  ['82'] | Citation Marker: 1998 | Citation Offset:  ['81','82','83'] | Citation Text:  <S sid ="81" ssid = "19">Van Halteren et al.</S><S sid ="82" ssid = "20">(1998) have generalized this approach for higher number of classifiers in their TotPrecision voting method.</S><S sid ="83" ssid = "21">The vote of each classifier (parser) is weighted by their respective accuracy.</S> | Reference Offset:  ['145', '146', '147'] | Reference Text:  <S sid ="145" ssid = "66">The most important observation is that every combination (significantly) outperforms the combination of any strict subset of its components.</S><S sid ="146" ssid = "67">Also of note is the improvement yielded by the best combination.</S><S sid ="147" ssid = "68">The pairwise voting system, using all four individual taggers, scores 97.92% correct on Test, a 19.1% reduction in error rate over the best individual system, viz.</S> | Discourse Facet:  Results_Citation | Annotator:  Kokil Jaidka, NTU |


Citance Number: 13 | Reference Article:  P98-1081.xml | Citing Article:  W05-1518.xml | Citation Marker Offset:  ['90'] | Citation Marker: van Halteren et al., 1998 | Citation Offset:  ['90'] | Citation Text:  <S sid ="90" ssid = "28">Parallel to (van Halteren et al., 1998), we ran experiments with two stacked classifiers, Memory-Based, and Decision-Tree-Based.</S> | Reference Offset:  ['101', '102'] | Reference Text:  <S sid ="101" ssid = "22">However, it appears more useful to give more weight to taggers which have proved their quality.</S><S sid ="102" ssid = "23">This can be general quality, e.g. each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g. each tagger votes its precision on the suggested tag (Tag- Precision).</S> | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka, NTU |


Citance Number: 14 | Reference Article:  P98-1081.xml | Citing Article:  W05-1518.xml | Citation Marker Offset:  ['111'] | Citation Marker:  van Halteren et al., 1998 | Citation Offset:  ['111'] | Citation Text: <S sid ="111" ssid = "3">In all experiments, the TotPrecision voting scheme of (van Halteren et al., 1998) has been used.</S> | Reference Offset:  ['130', '137'] | Reference Text:  <S sid ="130" ssid = "51">The first choice for this is to use a Memory- Based second level learner.</S><S sid ="137" ssid = "58">To examine if the overtraining effects are specific to this particular second level classifier, we also used the C5.0 system, a commercial version of the well-known program C4.5 (Quinlan 1993) for the induction of decision trees, on the same training material.</S> | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka, NTU |


Citance Number: 15 | Reference Article:  P98-1081.xml | Citing Article:  W00-0733.xml | Citation Marker Offset:  ['26'] | Citation Marker:  1998 | Citation Offset:  ['26'] | Citation Text: <S sid ="26" ssid = "23">The most advanced voting method ex­ amines output values of pairs of classifiers and assigns weights to tags based on how often they appear with this pair in the tuning data (Tag­ Pair, Van Halteren et al., (1998)).</S> | Reference Offset: ['116', '117'] | Reference Text:  <S sid ="116" ssid = "37">A next step is to examine them in pairs.</S><S sid ="117" ssid = "38">We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx.</S> | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka, NTU |


Citation Number: 16 | Reference Article: P98-1081.xml | Citing Article: W00-0733.xml | Citation Marker Offset:  ['18'] | Citation Marker:  Van Halteren et al., 1998 | Citation Offset:  ['18','19','20'] | Citation Text: <S sid ="18" ssid = "15">We will evaluate nine different methods for combining the output of our five chunkers (Van Halteren et al., 1998).</S><S sid ="19" ssid = "16">Five are so-called voting methods.</S><S sid ="20" ssid = "17">They assign weights to the output of the individual systems and use these weights to determine the most probable output tag.</S> |  Reference Offset:  ['94', '101'] | Reference Text:  <S sid ="94" ssid = "15">Simple Voting There are many ways in which the results of the component taggers can be combined, selecting a single tag from the set proposed by these taggers.</S><S sid ="101" ssid = "22">However, it appears more useful to give more weight to taggers which have proved their quality.</S> | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka, NTU |


Citance Number: 17 | Reference Article:  P98-1081.xml | Citing Article:  W00-0733.xml | Citation Marker Offset:  ['32'] | Citation Marker:  Van Halteren et al., 1998 | Citation Offset:  ['32'] | Citation Text:  <S sid ="32" ssid = "29">For this purpose we have used the part-of-speech tag of the cur­ rent word as compressed representation of the first stage input (Van Halteren et al., 1998).</S> | Reference Offset:  ['129'] | Reference Text:  <S sid ="129" ssid = "50">The second stage can be provided with the first level outputs, and with additional information, e.g. about the original input pattern.</S> | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka, NTU |


Citance Number: 18 | Reference Article:  P98-1081.xml | Citing Article:  J01-2002.xml | Citation Marker Offset:  ['40'] | Citation Marker:  van Halteren, Zavrel, and Daelemans 1998 | Citation Offset:  ['40','41','42'] | Citation Text:  <S sid ="40" ssid = "40">First experiments (van Halteren, Zavrel, and Daelemans 1998; Brill and Wu 1998) demonstrated the basic validity of the approach for tagging, with the error rate of the best combiner being 19.1% lower than that of the best individual tagger (van Halteren, Zavrel, and Daelemans 1998).</S><S sid ="41" ssid = "41">However, these experiments were restricted to a single language, a single tagset and, more importantly, a limited amount of training data for the combiners.</S><S sid ="42" ssid = "42">This led us to perform further, more extensive, 1In previous work (van Halteren, Zavrel, and Daelemans 1998), we were unable to confirm the latter half of the hypothesis unequivocally.</S> | Reference Offset:  ['136','145', '146', '147','161'] | Reference Text:  <S sid ="136" ssid = "57">This is most likely an overtraining effect: Tune is probably too small to collect case bases which can leverage the stacking effect convincingly, especially since only 7.51% of the second stage material shows disagreement between the featured tags.</S><S sid ="145" ssid = "66">The most important observation is that every combination (significantly) outperforms the combination of any strict subset of its components.</S><S sid ="146" ssid = "67">Also of note is the improvement yielded by the best combination.</S><S sid ="147" ssid = "68">The pairwise voting system, using all four individual taggers, scores 97.92% correct on Test, a 19.1% reduction in error rate over the best individual system, viz.</S><S sid ="161" ssid = "82">Regardless of such closer investigation, we feel that our results are encouraging enough to extend our investigation of combination, starting with additional component taggers and selection strategies, and going on to shifts to other tagsets and/or languages.</S> | Discourse Facet:  Results_Citation | Annotator:  Kokil Jaidka, NTU |


Citance Number: 19 | Reference Article:  P98-1081.xml | Citing Article:  J01-2002.xml | Citation Marker Offset:  ['183'] | Citation Marker:  1998 | Citation Offset:  ['183','184','185','186'] | Citation Text:  <S sid ="183" ssid = "33">Compare this to the &quot;tune&quot; set in van Halteren, Zavrel, and Daelemans (1998).</S><S sid ="184" ssid = "34">This consisted of 114K.</S><S sid ="185" ssid = "35">tokens, but, because of a 92.5% agreement over all four taggers, it yielded less than 9K tokens of useful training material to resolve disagreements.</S><S sid ="186" ssid = "36">This was suspected to be the main reason for the relative lack of performance by the more sophisticated combiners.</S> | Reference Offset:  ['75', '90', '136'] | Reference Text:  <S sid ="75" ssid = "13">The second part, Tune, consists of 10% of the data (every ninth utterance, 114479 tokens) and is used to select the best tagger parameters where applicable and to develop the combination methods.</S><S sid ="90" ssid = "11">All Taggers Correct 92.49 Majority Correct (31,211) 4.34 Correct Present, No Majority 1.37 (22,1111) Minority Correct (13,121) 1.01 All Taggers Wrong 0.78 Table 1: Tagger agreement on Tune.</S><S sid ="136" ssid = "57">This is most likely an overtraining effect: Tune is probably too small to collect case bases which can leverage the stacking effect convincingly, especially since only 7.51% of the second stage material shows disagreement between the featured tags.</S> | Discourse Facet:  Implication_Citation | Annotator:  Kokil Jaidka, NTU |


Citance Number: 20 | Reference Article:  P98-1081.xml | Citing Article:  J01-2002.xml | Citation Marker Offset: ['570'] | Citation Marker: 1998 | Citation Offset:  ['570','571','573'] | Citation Text: <S sid ="570" ssid = "7">For part-of-speech tagging, a significant increase in accuracy through combining the output of different taggers was first demonstrated in van Halteren, Zavrel, and Daelemans (1998) and Brill and Wu (1998).</S><S sid ="571" ssid = "8">In both approaches, different tagger gen­ erators were applied to the same training data and their predictions combined using different combination methods, including stacking.</S><S sid ="573" ssid = "10">As we now apply the methods of van Halteren, Zavrel, and Daelemans (1998) to WSJ as well, it is easier to make a comparison.</S> | Reference Offset:  ['3', '4', '125'] | Reference Text: <S sid ="3" ssid = "3">Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data.</S><S sid ="4" ssid = "4">After comparison, their outputs are combined using several voting strategies and second stage classifiers.</S><S sid ="125" ssid = "46">Stacked classifiers From the measurements so far it appears that the use of more detailed information leads to a better accuracy improvement.</S> | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka, NTU |


Citance Number: 21 | Reference Article:  P98-1081.xml | Citing Article:  J01-2002.xml | Citation Marker Offset:  ['102'] | Citation Marker:  van Halteren, Zavrel, and Daelemans 1998 | Citation Offset:  ['102','103','104'] | Citation Text:  <S sid ="102" ssid = "52">One of the best methods for tagger combination in (van Halteren, Zavrel, and Daele­ mans 1998) is the TagPair method.</S><S sid ="103" ssid = "53">It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx.</S><S sid ="104" ssid = "54">Although it is presented as a variant of voting in that paper, it is in fact also a stacked classifier, because it does not necessarily select one of the tags suggested by the component taggers.</S> | Reference Offset:  ['116', '117', '127', '128', '134'] | Reference Text:  <S sid ="116" ssid = "37">A next step is to examine them in pairs.</S><S sid ="117" ssid = "38">We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx.</S><S sid ="127" ssid = "48">The practice of feeding the outputs of a number of classifiers as features for a next learner sit is significantly better than the runner-up (Precision-Recall) with p=0.</S><S sid ="128" ssid = "49">is usually called stacking (Wolpert 1992).</S><S sid ="134" ssid = "55">Surprisingly, none of the Memory-Based based methods reaches the quality of TagPair.</S> | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka, NTU |


Citance Number: 22 | Reference Article:  P98-1081.xml | Citing Article:  J01-2002.xml | Citation Marker Offset:  ['487'] | Citation Marker:  1998 | Citation Offset:  ['487','488'] | Citation Text:  <S sid ="487" ssid = "81">The most important result that has undergone a change between van Halteren, Zavrel, and Daelemans (1998) and our current experiments is the relative accuracy of TagPair and stacked systems such as MBL.</S><S sid ="488" ssid = "82">Where TagPair used to be significantly better than MBL, the roles are now well reversed.</S> | Reference Offset:  ['124', '134'] | Reference Text:  <S sid ="124" ssid = "45">When used on Test, the pairwise voting strategy (TagPair) clearly outperforms the other voting strategies, 8 but does not yet approach the level where all tying majority votes are handled correctly (98.31%).</S><S sid ="134" ssid = "55">Surprisingly, none of the Memory-Based based methods reaches the quality of TagPair.</S> | Discourse Facet:  Results_Citation | Annotator:  Kokil Jaidka, NTU |


Citance Number: 23 | Reference Article:  P98-1081.xml | Citing Article:  J01-2002.xml | Citation Marker Offset:  ['176'] | Citation Marker:  van Halteren, Zavrel, and Daelemans 1998 | Citation Offset:  ['176'] | Citation Text:  <S sid ="176" ssid = "26">The first is the LOB corpus (Johansson 1986), which we used in the earlier experiments as well (van Halteren, Zavrel, and Daelemans 1998) and which has proved to be a good testing ground.</S> | Reference Offset:  ['63'] | Reference Text:  <S sid ="63" ssid = "1">The data we use for our experiment consists of the tagged LOB corpus (Johansson 1986).</S> | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka, NTU |


Citance Number: 24 | Reference Article:  P98-1081.xml | Citing Article:  J01-2002.xml | Citation Marker Offset:  ['309'] | Citation Marker: 1998 | Citation Offset:  ['309'] | Citation Text:  <S sid ="309" ssid = "159">In van Halteren, Zavrel, and Daelemans (1998) we used a straightforward im­ plementation of HMM&apos;s, which turned out to have the worst accuracy of the four competing methods.</S> | Reference Offset:  ['3', '107'] | Reference Text:  <S sid ="3" ssid = "3">Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data.</S><S sid ="107" ssid = "28">Table 2: Accuracy of individual taggers and combination methods.</S> | Discourse Facet:  Results_Citation | Annotator:  Kokil Jaidka, NTU |


Citance Number: 25 | Reference Article:  P98-1081.xml | Citing Article:  J01-2002.xml | Citation Marker Offset:  ['398'] | Citation Marker:  van Halteren,Zavrel, and Daelemans 1998 | Citation Offset:  ['398'] | Citation Text:  <S sid ="398" ssid = "83">With LOB and a single 114K tune set (van Halteren, Zavrel, and Daelemans 1998), both MBL and Decision Trees degraded significantly when adding context, and MBL degraded when adding the word</S> | Reference Offset:  ['130', '134', '137', '138'] | Reference Text:  <S sid ="130" ssid = "51">The first choice for this is to use a Memory- Based second level learner.</S><S sid ="134" ssid = "55">Surprisingly, none of the Memory-Based based methods reaches the quality of TagPair.</S><S sid ="137" ssid = "58">To examine if the overtraining effects are specific to this particular second level classifier, we also used the C5.0 system, a commercial version of the well-known program C4.5 (Quinlan 1993) for the induction of decision trees, on the same training material.</S><S sid ="138" ssid = "59">1° Because C5.0 prunes the decision tree, the overfitting of training material (Tune) is less than with Memory-Based learning, but the results on Test are also worse.</S> | Discourse Facet:  Results_Citation | Annotator:  Kokil Jaidka, NTU |


