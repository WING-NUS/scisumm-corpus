Empirical evaluations of animacy annotation



Lilja Øvrelid Department 
of Linguistics University 
of Potsdam Germany
lilja@ling.uni-potsdam.de








Abstract

This article presents empirical evaluations 
of aspects of annotation for the linguis- 
tic property of animacy in Swedish, rang- 
ing from manual human annotation, auto- 
matic classification and, finally, an exter- 
nal evaluation in the task of syntactic pars- 
ing. We show that a treatment of animacy 
as a lexical semantic property of noun 
types enables generalization over distri- 
butional properties of these nouns which 
proves beneficial in automatic classifica- 
tion and furthermore gives significant im- 
provements in terms of parsing accuracy 
for Swedish, compared to a state-of-the- 
art baseline parser with gold standard ani- 
macy information.

1   Introduction

The property of animacy influences linguistic phe- 
nomena in a range of different languages, such 
as case marking (Aissen, 2003) and argument re- 
alization (Bresnan et al., 2005;  de Swart et al.,
2008), and has been shown to constitute an im- 
portant factor in the production and comprehen- 
sion of syntactic structure (Branigan et al., 2008; 
Weckerly and Kutas, 1999).1   In computational 
linguistic work, animacy has been shown to pro- 
vide important information in anaphora resolution 
(Ora˘san and Evans, 2007), argument disambigua- 
tion (Dell’Orletta et al., 2005) and syntactic pars- 
ing in general (Øvrelid and Nivre, 2007).
The  dimension  of  animacy  roughly  distin-
guishes between entities which are alive and en- 
tities which are not, however, other distinctions

  1 Parts of the research reported in this paper has been sup- 
ported by the Deutsche Forschungsgemeinschaft (DFG, Son- 
derforschungsbereich 632, project D4).


are also relevant and the animacy dimension is of- 
ten viewed as a continuum ranging from humans 
to inanimate objects. Following Silverstein (1976) 
several animacy hierarchies have been proposed in 
typological studies, focusing on the linguistic cat- 
egory of animacy, i.e., the distinctions which are 
relevant for linguistic phenomena. An example of 
an animacy hierarchy, taken from (Aissen, 2003), 
is provided in (1):

(1)   Human > Animate > Inanimate

Clearly, non-human animates, like animals, are 
not less animate than humans in a biological sense, 
however, humans and animals show differing lin- 
guistic behaviour.
Empirical studies of animacy require human an-
notation efforts, and, in particular, a well-defined 
annotation task.  However, annotation studies of 
animacy differ distinctly in their treatment of ani- 
macy as a type or token-level phenomenon, as well 
as in terms of granularity of categories.  The use 
of the annotated data as a computational resource 
furthermore poses requirements on the annotation 
which do not necessarily agree with more theo- 
retical considerations.  Methods for the induction 
of animacy information for use in practical appli- 
cations require the resolution of issues of level of 
representation, as well as granularity.
This article addresses these issues through em-
pirical and experimental evaluation.  We present 
an in-depth study of a manually annotated data 
set which indicates that animacy may be treated 
as a lexical semantic property at the type level. 
We then evaluate this proposal through supervised 
machine learning of animacy information and fo- 
cus on an in-depth error analysis of the resulting 
classifier, addressing issues of granularity of the 
animacy dimension. Finally, the automatically an-



  Proceedings of the 12th Conference of the European Chapter of the ACL, pages 630–638, 
Athens, Greece, 30 March – 3 April 2009. Qc 2009 Association for Computational Linguistics


notated data set is employed in order to train a syn- 
tactic parser and we investigate the effect of the an- 
imacy information and contrast the automatically 
acquired features with gold standard ones.
The rest of the article is structured as follows. In
section 2, we briefly discuss annotation schemes 
for animacy, the annotation strategies and cate- 
gories proposed there. We go on to describe anno- 
tation for the binary distinction of ‘human refer- 
ence’ found in a Swedish dependency treebank in 
section 3 and we perform an evaluation of the con- 
sistency of the human annotation in terms of lin- 
guistic level. In section 4, we present experiments 
in lexical acquisition of animacy based on mor- 
phosyntactic features extracted from a consider- 
ably larger corpus. Section 5 presents experiments 
with the acquired animacy information applied in 
the data-driven dependency parsing of Swedish. 
Finally, section 6 concludes the article and pro- 
vides some suggestions for future research.

2   Animacy annotation

Annotation for animacy is not a common compo- 
nent of corpora or treebanks. However, following 
from the theoretical interest in the property of an- 
imacy, there have been some initiatives directed at


the animacy of their referent in the particular con- 
text.  Animacy is thus treated as a token level 
property, however, has also been proposed as a 
lexical semantic property of nouns (Yamamoto,
1999).  The indirect encoding of animacy in lex- 
ical resources, such as WordNet (Fellbaum, 1998) 
can also be seen as treating animacy as a type- 
level property. We may thus distinguish between a 
purely type level annotation strategy and a purely 
token level one. Type level properties hold for lex- 
emes and are context-independent, i.e., indepen- 
dent of the particular linguistic context, whereas 
token-level properties are determined in context 
and hold for referring expressions, rather than lex- 
emes.

3   Human reference in Swedish

Talbanken05 is a Swedish treebank which was 
created  in  the  1970’s  and  which  has  recently 
been converted to dependency format (Nivre et 
al., 2006b) and made freely available.  The writ- 
ten sections of the treebank consist of profes- 
sional prose and student essays and amount to
197,123 running tokens, spread over 11,431 sen- 
tences.  Figure 2 shows the labeled dependency 
graph of example (2), taken from Talbanken05.


animacy annotation of corpus data.
Corpus studies of animacy (Yamamoto, 1999;



(2)	Samma
same



erfarenhet
experience



gjorde
made



engelsma¨ nnen
englishmen-D E F


Dahl and Fraurud, 1996) have made use of an-
notated data, however they differ in the extent to 
which the annotation has been explicitly formu- 
lated as an annotation scheme.   The annotation


‘The same experience, the Englishmen had’


DT	OO


ROOT	SS


study presented in Zaenen et. al. (2004) makes use
of a coding manual designed for a project study- 
ing genitive modification (Garretson et al., 2004) 
and presents an explicit annotation scheme for an-


_   Samma
_	PO
_	KP


erfarenhet
NN
_



gjorde VV PT


engelsmannen
NN
DD|HH


imacy, illustrated by figure 1. The main class dis- 
tinction for animacy is three-way, distinguishing 
Human, Other animate and Inanimate, with sub- 
classes under two of the main classes. The ‘Other 
animate’ class further distinguishes Organizations 
and Animals. Within the group of inanimates, fur- 
ther distinctions are made between concrete and 
non-concrete inanimate, as well as time and place 
nominals.2
The annotation scheme described in Zaenen et.
al. (2004) annotates the markables according to

  2 The fact that the study focuses on genitival modification 
has clearly influenced the categories distinguished, as these 
are all distinctions which have been claimed to influence the 
choice of genitive construction. For instance, temporal nouns 
are frequent in genitive constructions, unlike the other inani- 
mate nouns.


Figure 2: Dependency representation of example
(2) from Talbanken05.

In addition to information on part-of-speech, de- 
pendency head and relation, and various mor- 
phosyntactic properties such as definiteness, the 
annotation expresses a distinction for nominal el- 
ements between reference to human and non- 
human.  The annotation manual (Teleman, 1974) 
states that a markable should be tagged as human 
(H H) if it may be replaced by the interrogative pro- 
noun vem ‘who’ and be referred to by the personal 
pronouns han ‘he’ or hon ‘she’.
There are clear similarities between the anno-
tation for human reference found in Talbanken05 
and the annotation scheme for animacy discussed



HUM



 Other animate



Inanimate






ORG


ANIM

CONC 	NCONC 	TIME PLACE



Figure 1: Animacy classification scheme (Zaenen et al., 2004).




above. The human/non-human contrast forms the 
central distinction in the animacy dimension and, 
in this respect, the annotation schemes do not con- 
flict. If we compare the annotation found in Tal- 
banken05 with the annotation proposed in Zaenen 
et. al. (2004), we find that the schemes differ pri- 
marily in the granularity of classes distinguished. 
The main source of variation in class distinctions 
consists in the annotation of collective nouns, in- 
cluding organizations, as well as animals.


tic contexts, we may group the nouns which were 
assigned to both classes, into the following cate- 
gories:that ‘H H’ is the tag for

Abstract nouns   Nouns with underspecified or 
vague type level properties with respect to ani- 
macy, such as quantifying nouns, e.g. ha¨ lft ‘half’, 
miljon ‘million’, as well as nouns which may be 
employed with varying animacy, e.g. element ‘el- 
ement’, part ‘party’, as in (3) and (4):



3.1	Level of annotation


(3)	. . . ocksa˚
. . . also


den
the


andra
other


partenH H
party-D E F


sta˚ r
stands


utanfo¨ r
outside


We distinguished above between type and token


‘. . . also the other party is left outside’


level annotation strategies, where a type level an- 
notation strategy entails that an element consis- 
tently be assigned to only one class. A token level


(4)	I in lika


ett
a
 

fo¨ rha˚ llande relationship starka


a¨ r
are


aldrig
never


ba¨ gge
both


parter
parties


same


strong


strategy, in contrast, does not impose this restric-
tion on the annotation and class assignment may 
vary depending on the specific context. Garretson 
et. al (2004) propose a token level annotation strat- 
egy and state that “when coding for animacy [. . . ] 
we are not considering the nominal per se (e.g., the 
word ‘church’), but rather the entity that is the ref- 
erent of that nominal (e.g. some particular thing in 
the real world)”. This indicates that for all possible 
markables, a referent should be determinable.
The brief instruction with respect to annotation
for human reference in the annotation manual for 
Talbanken05 (Teleman, 1974, 223) gives leeway 
for interpretation in the annotation and does not 
clearly state that it should be based on token level
reference in context.  It may thus be interesting



‘In a relationship, both parties are never equally 
strong’

We also find that nouns which denote abstract con- 
cepts regarding humans show variable annotation, 
e.g.   individ ‘individual’, adressat ‘addressee’, 
medlem ‘member’, kandidat ‘candidate’, repre- 
sentant ‘representative’, auktoritet ‘authority’

Reference shifting contexts  These are nouns 
whose type level animacy is clear but which are 
employed in a specific context which shifts their 
reference. Examples include metonymic usage of 
nouns, as in (5) and nouns occurring in derefer- 
encing constructions, such as predicative construc- 
tions (6), titles (7) and idioms (8):


to examine the extent to which this manual an-


(5)	. . . daghemmensH H
. . . kindergarten-D E F.G E N


otillra¨ ckliga
inadequate


resurser
resources


notation is consistent across lexemes or whether
we observe variation.  We manually examine the


‘. . . the kindergarten’s inadequate resources’


intersection of the two classes of noun lemmas


(6)	. . . fo¨ r
. . . for


att
to


bli
become


en   bra
a	good


soldat
soldier


in the written sections of Talbanken, i.e., the set
of nouns which have been assigned both classes


‘. . . in order to become a good soldier’


by the annotators.  It contains 82 noun lemmas,


(7)	. . . menar
. . . thinks


biskop
bishop


Hellsten
Hellsten


which corresponds to only 1.1% of the total num-
ber of noun lemmas in the treebank (7554 lem-



‘thinks bishop Hellsten’


mas all together).  After a manual inspection of 
the intersective elements along with their linguis-


(8)	ta
take


studenten
student-D E F


‘graduate from highschool (lit. take the student)’



It is interesting to note that the main variation in 
annotation stems precisely from difficulties in de- 
termining reference, either due to bleak type level 
properties such as for the abstract nouns, or due to 
properties of the context, as in the reference shift- 
ing constructions. The small amount of variation 
in the human annotation for animacy clearly sup- 
ports a type-level approach to animacy, however, 
underline the influence of the linguistic context on 
the conception of animacy, as noted in the litera- 
ture (Zaenen et al., 2004; Rosenbach, 2008).

4   Lexical acquisition of animacy

Even though knowledge about the animacy of a 
noun clearly has some interesting implications, lit- 
tle work has been done within the field of lexical 
acquisition in order to automatically acquire ani- 
macy information. Ora˘san and Evans (2007) make 
use of hyponym-relations taken from the Word- 
Net resource in order to classify animate referents. 
However, such a method is clearly restricted to 
languages for which large scale lexical resources, 
such as the WordNet, are available.  The task of 
animacy classification bears some resemblance to 
the task of named entity recognition (NER) which 
usually makes reference to a ‘person’ class. How- 
ever, whereas most NER systems make extensive 
use of orthographic, morphological or contextual 
clues (titles, suffixes) and gazetteers, animacy for 
nouns is not signaled overtly in the same way.
Following  a  strategy  in  line  with  work  on
verb classification (Merlo and Stevenson, 2001; 
Stevenson and Joanis, 2003), we set out to clas- 
sify common noun lemmas based on their mor- 
phosyntactic distribution in a considerably larger 
corpus.  This is thus equivalent to treatment of 
animacy as a lexical semantic property and the 
classification strategy is based on generalization 
of morphosyntactic behaviour of common nouns 
over large quantities of data. Due to the small size 
of the Talbanken05 treebank and the small amount 
of variation, this strategy was pursued for the ac- 
quisition of animacy information.
In the animacy classification of common nouns
we exploit well-documented correlations between 
morphosyntactic realization and semantic proper- 
ties of nouns. For instance, animate nouns tend to 
be realized as agentive subjects, inanimate nouns 
do not (Dahl and Fraurud, 1996). Animate nouns 
make good ‘possessors’, whereas inanimate nouns 
are more likely ‘possessees’ (Rosenbach, 2008). 
Table 1 presents an overview of the animacy data




Clas
s
Ani
mat
e
Type
s Tok
6
4
4
ens 
cover
ed
6
0
1
0
Inan
imat
e
691
0
3
4
8
2
2
Tota
l
755
4
4
0
8
3
2

Table 1: The animacy data set from Talbanken05; 
number of noun lemmas (Types) and tokens in 
each class.





for common nouns in Talbanken05. It is clear that 
the data is highly skewed towards the non-human 
class, which accounts for 91.5% of the type in- 
stances.   For classification we organize the data 
into accumulated frequency bins, which include 
all nouns with frequencies above a certain thresh- 
old.  We here approximate the class of ‘animate’ 
to ‘human’ and the class of ‘inanimate’ to ‘non- 
human’. Intersective elements, see section 3.1, are 
assigned to their majority class.3

4.1   Features for animacy classification
We define a feature space, which makes use of 
distributional data regarding the general syntactic 
properties of a noun, as well as various morpho- 
logical properties.  It is clear that in order for a 
syntactic environment to be relevant for animacy 
classification it must be, at least potentially, nom- 
inal. We define the nominal potential of a depen- 
dency relation as the frequency with which it is 
realized by a nominal element (noun or pronoun) 
and determine empirically a threshold of .10. The 
syntactic and morphological features in the feature 
space are presented below:

Syntactic features A  feature  for  each  depen- 
dency relation with nominal potential: (tran- 
sitive) subject (SU B J), object (O B J), preposi- 
tional complement (PA),  root (RO OT)4 , ap- 
position  (A PP),  conjunct  (C C),  determiner 
(D ET),  predicative  (PR D),  complement  of 
comparative subjunction (U K).  We also in- 
clude a feature for the head of a genitive mod- 
ifier, the so-called ‘possessee’, (G EN H D).

Morphological features A feature for each mor- 
phological  distinction  relevant  for  a  noun

  3 When there is no majority class, i.e. in the case of ties, 
the noun is removed from the data set. 12 lemmas were con- 
sequently removed.
    4 Nominal elements may be assigned the root relation of 
the dependency graph in sentence fragments which do not 
contain a finite verb.



in   Swedish:     gender   (N EU /U TR),   num- 
ber (SIN/PLU), definiteness (D EF/IN D), case 
(N O M/G EN).   Also, the part-of-speech tags 
distinguish   dates   (DAT)   and   quantifying 
nouns (SET),  e.g.   del, rad ‘part, row’, so 
these are also included as features.

For extraction of distributional data for the set of 
Swedish nouns we make use of the Swedish Pa- 
role corpus of 21.5M tokens.5 To facilitate feature 
extraction, we part-of-speech tag the corpus and 
parse it with MaltParser6 , which assigns a depen- 
dency analysis.7

4.2   Experimental methodology
For machine learning, we make use of the Tilburg 
Memory-Based Learner (TiMBL) (Daelemans et 
al., 2004).8    Memory-based learning is a super- 
vised machine learning method characterized by 
a lazy learning algorithm which postpones learn- 
ing until classification time, using the k-nearest 
neighbor algorithm for the classification of unseen 
instances. For animacy classification, the TiMBL 
parameters are optimized on a subset of the full 
data set.9
For training and testing of the classifiers, we
make use of leave-one-out cross-validation.  The 
baseline represents assignment of the majority 
class (inanimate) to all nouns in the data set. Due 
to the skewed distribution of classes, as noted 
above, the baseline accuracy is very high, usu- 
ally around 90%.Clearly, however, the class-based 
measures of precision and recall, as well as the 
combined F-score measure are more informative 
for these results. The baseline F-score for the ani- 
mate class is thus 0, and a main goal is to improve 
on the rate of true positives for animates, while 
limiting the trade-off in terms of performance for

5 Parole is freely available at http://spraakbanken.gu.se
6 http://www.maltparser.org
7 For part-of-speech tagging, we employ the MaltTagger
– a HMM part-of-speech tagger for Swedish (Hall, 2003). 
For parsing, we employ MaltParser (Nivre et al., 2006a), 
a language-independent system for data-driven dependency 
parsing , with the pretrained model for Swedish, which has 
been trained on the tags output by the tagger.
8 http://ilk.uvt.nl/software.html
    9 For     parameter     optimization     we     employ     the 
paramsearch tool,  supplied with TiMBL, see 
http://ilk.uvt.nl/software.html.      Paramsearch  implements 
a hill climbing search for the optimal settings on iteratively 
larger parts of the supplied data.  We performed parameter 
optimization on 20% of the total data set, where we balanced 
the data with respect to frequency. The resulting settings are 
k = 11, GainRatio feature weighting and Inverse Linear (IL) 
class voting weights.


Bin	Instances Baseline MBL SVM
>1000	291	89.3   97.3   95.2
>500 	597	88.9   97.3   97.1
>100 	1668	90.5   96.8   96.9
>50 	2278	90.6   96.1   96.0
>10 	3786	90.8   95.4   95.1
>0 	5481	91.3   93.9   93.7

Table 2: Accuracy for MBL and SVM classifiers 
on Talbanken05 nouns in accumulated frequency 
bins by Parole frequency.





the majority class of inanimates, which start out 
with F-scores approaching 100. For calculation of 
the statistical significance of differences in the per- 
formance of classifiers tested on the same data set, 
McNemar’s test (Dietterich, 1998) is employed.

4.3   Results
Column four (MBL) in table 2 shows the accu- 
racy obtained with all features in the general fea- 
ture space.  We observe a clear improvement on 
all data sets (p<.0001),  compared to the respec- 
tive baselines. As we recall, the data sets are suc- 
cessively larger, hence it seems fair to conclude 
that the size of the data set partially counteracts 
the lower frequency of the test nouns.  It is not 
surprising, however, that a method based on dis- 
tributional features suffers when the absolute fre- 
quencies approach 1.  We obtain results for ani- 
macy classification, ranging from 97.3% accuracy 
to 93.9% depending on the sparsity of the data. 
With an absolute frequency threshold of 10, we 
obtain an accuracy of 95.4%, which constitutes a
50% reduction of error rate.
Table 3 presents the experimental results rela-
tive to class. We find that classification of the inan- 
imate class is quite stable throughout the experi- 
ments, whereas the classification of the minority 
class of animate nouns suffers from sparse data. It 
is an important point, however, that it is largely re- 
call for the animate class which goes down with 
increased sparseness, whereas precision remains 
quite stable. All of these properties are clearly ad- 
vantageous in the application to realistic data sets, 
where a more conservative classifier is to be pre- 
ferred.

4.4   Error analysis
The human reference annotation of the Tal- 
banken05 nouns distinguishes only the classes cor- 
responding to ‘human’ and ‘inanimate’ along the




A
n
i
m
a
t
e	Inanimate

Prec
isio
n 
Rec
all 
Fsc
ore
Prec
isio
n 
Rec
all 
Fsco
re
>10
00
>50
0
>10
0
>50
>10
>0
8
9
.
7
	
8
3
.
9
	
8
6
.
7
8
9
.
1
	
8
6
.
4
	
8
7
.
7
8
7
.
7
	
7
6
.
6
	
8
1
.
8
8
5
.
8
	
7
0
.
2
	
7
7
.
2
8
1
.
9
	
6
4
.
0
	
7
1
.
8
7
5
.
7
	
4
4
.
9
	
5
6
.
4
9
8
.
1
	
9
8
.
8
	
9
8
.
5
9
8
.
3
	
9
8
.
7
	
9
8
.
5
9
7
.
6
	
9
8
.
9
	
9
8
.
2
9
7
.
0
	
9
8
.
9
	
9
7
.
9
9
6
.
4
	
9
8
.
6
	
9
7
.
5
9
4
.
9
	
9
8
.
6
	
9
6
.
7

Table 3: Precision, recall and F-scores for the two classes in MBL-experiments with a general feature 
space.




>10 nouns
(a)	(b) ← classified as
222   125 (a) class animate
49 3390 (b) class inanimate

Table 4: Confusion matrix for the MBL-classifier 
with a general feature space on the >10 data set 
on Talbanken05 nouns.





animacy dimension.  An interesting question is 
whether the errors show evidence of the gradi- 
ence in categories discussed earlier and explic- 
itly expressed in the annotation scheme by Zaenen 
et.al. (2004) in figure 1.  If so, we would expect 
erroneously classified inanimate nouns to contain 
nouns of intermediate animacy, such as animals 
and organizations.
The error analysis examines the performance of
the MBL-classifier employing all features on the
> 10 data set in order to abstract away from the 
most serious effects of data sparseness.  Table 4 
shows a confusion matrix for the classification of 
the nouns. If we examine the errors for the inan- 
imate class we indeed find evidence of gradience 
within this category.  The errors contain a group 
of nouns referring to animals and other living be- 
ings (bacteria, algae), as listed in (9), as well as 
one noun referring to an “intelligent machine”, in- 
cluded in the intermediate animacy category in Za- 
enen et al. (2004).  Collective nouns with human 
reference and organizations are also found among 
the errors, listed in (11). We also find some nouns 
among the errors with human denotation, listed in 
(12).  These are nouns which typically occur in 
dereferencing contexts, such as titles, e.g.   herr
‘mister’, biskop ‘bishop’ and which were anno- 
tated as non-human referring by the human an- 
notators.10    Finally, a group of abstract, human-

  10In fact, both of these showed variable annotation in the 
treebank and were assigned their majority class – inanimate


denoting nouns are also found among the errors, as 
listed in (13). In summary, we find that nouns with 
gradient animacy properties account for 53.1% of 
the errors for the inanimate class.
(9)    Animals/living beings:
alg ‘algae’, apa ‘monkey’, bakterie ‘bacteria’, bjo¨ rn
‘bear’, djur ‘animal’, fa˚ gel ‘bird’, fladdermo¨ ss ‘bat’,
myra ‘ant’, ma˚ s ‘seagull’, parasit ‘parasite’

(10)  Intelligent machines:
robot ‘robot’

(11)  Collective nouns, organizations:
myndighet ‘authority’, nation ‘nation’, fo¨ retagsledning
‘corporate-board’, personal ‘personell’, stiftelse
‘foundation’, idrottsklubb ‘sport-club’

(12)  Human-denoting nouns:
biskop ‘bishop’, herr ‘mister’, nationalist
‘nationalist’, tolk ‘interpreter’

(13)  Abstract, human nouns:
fo¨ rlorare ‘loser’, huvudpart ‘main-party’, konkurrent
‘competitor’, majoritet ‘majority’, va¨ rd ‘host’

It   is   interesting   to   note   that   both   the   hu- 
man and automatic annotation showed difficul- 
ties  in  ascertaining  class  for  a  group  of  ab- 
stract, human-denoting nouns, like individ ‘indi- 
vidual’, motsta˚ ndare ‘opponent’, kandidat ‘candi- 
date’, representant ‘representative’.  These were 
all assigned to the animate majority class dur- 
ing extraction, but were misclassified as inanimate 
during classification.

4.5   SVM classifiers
In order to evaluate whether the classification 
method generalizes to a different machine learn- 
ing algorithm, we design an identical set of experi- 
ments to the ones presented above, but where clas- 
sification is performed with Support Vector Ma- 
chines (SVMs) instead of MBL. We use the LIB- 
SVM package (Chang and Lin, 2001) with a RBF 
kernel (C = 8.0, γ = 0.5).11

– in the extraction of training data.
   11 As in the MBL-experiment, parameter optimization, i.e., 
choice of kernel function, C and γ values, is performed on
20% of the total data set with the easy.py tool, supplied 
with LIBSVM.



  As column 5 (SVM) in table 2 shows, the clas- 
sification results are very similar to the results ob- 
tained with MBL.12  We furthermore find a very 
similar set of errors, and in particular, we find that
51.0 % of the errors for the inanimate class are 
nouns with the gradient animacy properties pre- 
sented in (9)-(13) above.

5   Parsing with animacy information

As an external evaluation of our animacy classi- 
fier, we apply the induced information to the task 
of syntactic parsing.  Seeing that we have a tree- 
bank with gold standard syntactic information and 
gold standard as well as induced animacy informa- 
tion, it should be possible to study the direct effect 
of the added animacy information in the assign- 
ment of syntactic structure.

5.1   Experimental methodology
We use the freely available MaltParser system, 
which is a language-independent system for data- 
driven dependency parsing (Nivre, 2006; Nivre et 
al., 2006c).  A set of parsers are trained on Tal- 
banken05, both with and without additional an- 
imacy information, the origin of which is either 
the manual annotation described in section 3 or 
the automatic animacy classifier described in sec- 
tion 4.2- 4.4 (MBL). The common nouns in the 
treebank are classified for animacy using leave- 
one-out training and testing.   This ensures that 
the training and test instances are disjoint at all 
times.  Moreover, the fact that the distributional 
data is taken from a separate data set ensures non- 
circularity since we are not basing the classifica- 
tion on gold standard parses.
All parsing experiments are performed using
10-fold cross-validation for training and testing on 
the entire written part of Talbanken05.  Overall 
parsing accuracy will be reported using the stan- 
dard metrics of labeled attachment score (LAS) 
and unlabeled attachment score (UAS).13  Statis- 
tical significance is checked using Dan Bikel’s 
randomized parsing evaluation comparator.14   As 
our baseline, we use the settings optimized for 
Swedish in the CoNLL-X shared task (Buchholz

  12The SVM-classifiers generally show slightly lower re- 
sults, however, only performance on the >1000  data set is 
significantly lower (p<.05).
   13LAS and UAS report the percentage of tokens that are as- 
signed the correct head with (labeled) or without (unlabeled) 
the correct dependency label.
14http://www.cis.upenn.edu/∼dbikel/software.html


Gold standard   Automatic
UAS	LAS  UAS   LAS
Baseline 89.87	84.92 89.87 84.92
Anim	89.81	84.94 89.87 84.99

Table 5:  Overall results in experiments with au- 
tomatic features compared to gold standard fea- 
tures, expressed as unlabeled and labeled attach- 
ment scores.





and Marsi, 2006), where this parser was the best 
performing parser for Swedish.

5.2   Results
The addition of automatically assigned animacy 
information for common nouns (Anim) causes a 
small, but significant improvement in overall re- 
sults (p<.04) compared to the baseline, as well 
as the corresponding gold standard experiment 
(p<.04).  In the gold standard experiment, the re- 
sults are not significantly better than the baseline 
and the main, overall, improvement from the gold 
standard animacy information reported in Øvrelid 
and Nivre (2007) and Øvrelid (2008) stems largely 
from the animacy annotation of pronouns.15  This 
indicates that the animacy information for com- 
mon nouns, which has been automatically ac- 
quired from a considerably larger corpus, captures 
distributional distinctions which are important for 
the general effect of animacy and furthermore that 
the differences from the gold standard annotation 
prove beneficial for the results.
We see from Table 5, that the improvement in
overall parse results is mainly in terms of depen- 
dency labeling, reflected in the LAS score.   A 
closer error analysis shows that the performance 
of the two parsers employing gold and automatic 
animacy information is very similar with respect 
to dependency relations and we observe an im- 
proved analysis for subjects, (direct and indirect) 
objects and subject predicatives with only minor 
variations.  This in itself is remarkable, since the 
covered set of animate instances is notably smaller 
in the automatically annotated data set.  We fur- 
thermore find that the main difference between the 
gold standard and automatic Anim-experiments

  15 Recall that the Talbanken05 treebank contains animacy 
information for all nominal elements – pronouns, proper and 
common nouns.  When the totality of this information is 
added the overall parse results are significantly improved 
(p<.0002) (Øvrelid and Nivre, 2007; Øvrelid, 2008).


does not reside in the analysis of syntactic argu- 
ments, but rather of non-arguments.   One rela- 
tion for which performance deteriorates with the 
added information in the gold Anim-experiment 
is the nominal postmodifier relation (ET) which 
is employed for relative clauses and nominal PP- 
attachment. With the automatically assigned fea- 
ture, in contrast, we observe an improvement in 
the performance for the ET relation, compared to 
the gold standard experiment, from a F-score in 
the latter of 76.14 to 76.40 in the former.  Since 
this is a quite common relation, with a frequency 
of 5% in the treebank as a whole, the improvement 
has a clear effect on the results.
The parser’s analysis of postnominal modifica-
tion is influenced by the differences in the added 
animacy annotation for the nominal head, as well 
as the internal dependent. If we examine the cor- 
rected errors in the automatic experiment, com- 
pared to the gold standard experiment, we find ele- 
ments with differing annotation. Preferences with 
respect to the animacy of prepositional comple- 
ments vary.  In (14), the automatic annotation of 
the noun djur ‘animal’ as animate results in cor- 
rect assignment of the ET relation to the prepo- 
sition hos ‘among’, as well as correct nominal, 
as opposed to verbal, attachment.  This preposi- 
tion is one of the few with a preference for an- 
imate complements in the treebank.  In contrast, 
the example in (15) illustrates an error where the 
automatic classification of barn ‘children’ as inan- 
imate causes a correct analysis of the head prepo- 
sition om ‘about’.16


6   Conclusion

This article has dealt with an empirical evaluation 
of animacy annotation in Swedish, where the main 
focus has been on the use of such annotation for 
computational purposes.
We have seen that human annotation for ani-
macy shows little variation at the type-level for 
a  binary animacy distinction.    Following from 
this  observation,  we  have  shown  how  a  type- 
level induction strategy based on morphosyntac- 
tic distributional features enables automatic ani- 
macy classification for noun lemmas which fur- 
thermore generalizes to different machine learning 
algorithms (MBL, SVM). We obtain results for an- 
imacy classification, ranging from 97.3% accuracy 
to 93.9% depending on the sparsity of the data. 
With an absolute frequency threshold of 10, we 
obtain an accuracy of 95.4%, which constitutes a
50% reduction of error rate. A detailed error anal-
ysis revealed some interesting results and we saw 
that more than half of the errors performed by the 
animacy classifier for the large class of inanimate 
nouns actually included elements which have been 
assigned an intermediate animacy status in theo- 
retical work, such as animals and collective nouns.
The application of animacy annotation in the
task of syntactic parsing provided a test bed for 
the applicability of the annotation, where we could 
contrast the manually assigned classes with the 
automatically acquired ones.  The results showed 
that the automatically acquired information gives 
a slight, but significant improvement of overall 
parse results where the gold standard annotation


(14)  . . . samha¨ llsbildningar
. . . societies


hos
among


olika
different


djur
animals


does not, despite a 
considerably lower 
coverage.
This is a suprising result 
which highlights impor-


‘. . . social organizations among different animals’


tant properties of the annotation.  First of all, 
the


(15)  Fo¨ ra¨ ldrar
parents


har
have


va˚ rdnaden
custody-D E F


om   sina
of	their


barn
children


automatic 
annotation is 
completely 
consistent at 
the type 
level.	Second, the automatic animacy


‘Parents have the custody of their children’

A more thorough analysis of the different factors 
involved in PP-attachment is a complex task which 
is clearly beyond the scope of the present study. 
We may note, however, that the distinctions in- 
duced by the animacy classifier based purely on 
linguistic evidence proves useful for the analysis 
of both arguments and non-arguments.

  16Recall that the classification is based purely on linguistic 
evidence and in this respect children largely pattern with the 
inanimate nouns.  A child is probably more like a physical 
object in the sense that it is something one possesses and oth- 
erwise reacts to, rather than being an agent that acts upon its 
surroundings.


classifier captures important distributional proper- 
ties of the nouns, exemplified by the case of nom- 
inal postmodifiers in PP-attachment.   The auto- 
matic annotation thus captures a purely linguistic 
notion of animacy and abstracts over contextual 
influence in particular instances.
Animacy has been shown to be an important
property in a range of languages, hence animacy 
classification of other languages constitutes an in- 
teresting line of work for the future, where empir- 
ical evaluations may point to similarities and dif- 
ferences in the linguistic expression of animacy.


References
Judith Aissen. 2003. Differential Object Marking: Iconicity 
vs. economy.  Natural Language and Linguistic Theory,
21(3):435–483.

Holly P. Branigan, Martin J. Pickering, and Mikihiro Tanaka.
2008.    Contributions of animacy to grammatical func- 
tion  assignment and  word  order production.	Lingua,
118(2):172–189.

Joan Bresnan, Anna Cueni, Tatiana Nikitina, and Harald 
Baayen. 2005. Predicting the dative alternation. In Gosse 
Bouma, Irene Kraemer, and Joost Zwarts, editors, Cog- 
nitive foundations of interpretation, pages 69–94. Royal 
Netherlands Academy of Science, Amsterdam.

Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared 
task on multilingual dependency parsing. In Proceedings 
of the Tenth Conference on Computational Natural Lan- 
guage Learning (CoNLL-X), pages 149–164.

Chih-Chung Chang and Chih-Jen Lin.  2001.  LIBSVM: A

library for support vector machines. Software available at 
http://www.csie.ntu.edu.tw/∼cjlin/libsvm.

Walter Daelemans, Jakub Zavrel, Ko Van der Sloot, and An- 
tal Van den Bosch. 2004. TiMBL: Tilburg Memory Based 
Learner, version 5.1, Reference Guide.  Technical report, 
ILK Technical Report Series 04-02.

O¨ sten Dahl and Kari Fraurud.   1996.   Animacy in gram- 
mar and discourse. In Thorstein Fretheim and Jeanette K. 
Gundel, editors, Reference and referent accessibility, 
pages 47–65. John Benjamins, Amsterdam.

Peter de Swart, Monique Lamers, and Sander Lestrade. 2008.
Animacy, argument structure and argument encoding: In- 
troduction  to  the  special  issue  on  animacy.	Lingua,
118(2):131–140.

Felice Dell’Orletta, Alessandro Lenci, Simonetta Monte- 
magni, and Vito Pirrelli.  2005.   Climbing the path to 
grammar:  A maximum entropy model of subject/object 
learning.   In Proceedings of the 2nd Workshop on Psy- 
chocomputational Models of Human Language Acquisi- 
tion, pages 72–81.

Thomas G. Dietterich. 1998. Approximate statistical test for 
comparing supervised classification learning algorithms. 
Neural Computation, 10(7):1895–1923.

Christiane Fellbaum, editor.  1998.  WordNet: an electronic 
lexical database. MIT Press, Cambridge, MA.

Gregory Garretson, M. Catherine O’Connor, Barbora 
Skarabela, and Marjorie Hogan, 2004.   Optimal Typol- 
ogy of Determiner Phrases Coding Manual.    Boston 
University, version 3.2 edition.     Downloaded from 
http://people.bu.edu/depot/coding manual.html            on
02/15/2006.

Johan Hall.   2003.   A probabilistic part-of-speech tagger 
with suffix probabilities.  Master’s thesis, Va¨xjo¨ Univer- 
sity, Sweden.

Paola Merlo and Suzanne Stevenson. 2001. Automatic verb 
classification based on statistical distributions of argument 
structure. Computational Linguistics, 27(3):373–408.



Joakim Nivre, Johan Hall, and Jens Nilsson.  2006a.  Malt- 
parser:  A data-driven parser-generator for dependency 
parsing.  In Proceedings of the Fifth International Con- 
ference on Language Resources and Evaluation (LREC), 
pages 2216–2219.

Joakim Nivre, Jens Nilsson, and Johan Hall.  2006b.   Tal- 
banken05: A Swedish treebank with phrase structure and 
dependency annotation. In Proceedings of the fifth Inter- 
national Conference on Language Resources and Evalua- 
tion (LREC), pages 1392–1395.

Joakim Nivre, Jens Nilsson, Johan Hall, Gu¨ ls¸en Eryigˇ it, and 
Svetoslav Marinov.   2006c.   Labeled pseudo-projective 
dependency parsing with Support Vector Machines.  In 
Proceedings of the Conference on Computational Natural 
Language Learning (CoNLL).

Joakim  Nivre.	2006.	Inductive  Dependency  Parsing.
Springer, Dordrecht.

Constantin Ora˘san and Richard Evans.  2007.  NP animacy 
resolution for anaphora resolution.   Journal of Artificial 
Intelligence Research, 29:79–103.

Lilja Øvrelid and Joakim Nivre. 2007. When word order and 
part-of-speech tags are not enough – Swedish dependency 
parsing with rich linguistic features. In Proceedings of the 
International Conference on Recent Advances in Natural 
Language Processing (RANLP), pages 447–451.

Lilja Øvrelid.   2008.   Linguistic features in data-driven 
dependency parsing.   In Proceedings of the Conference 
on Computational Natural Language Learning (CoNLL
2008).

Anette Rosenbach.  2008.  Animacy and grammatical vari- 
ation - findings from English genitive variation.  Lingua,
118(2):151–171.

Michael Silverstein.  1976.  Hierarchy of features and erga- 
tivity.  In Robert M.W. Dixon, editor, Grammatical cat- 
egories in Australian Languages, pages 112–171. Aus- 
tralian Institute of Aboriginal Studies, Canberra.

Suzanne Stevenson and Eric Joanis. 2003. Semi-supervised 
verb class discovery using noisy features. In Proceedings 
of the Conference on Computational Natural Language 
Learning (CoNLL), pages 71–78.

Ulf Teleman. 1974. Manual fo¨ r grammatisk beskrivning av 
talad och skriven svenska. Studentlitteratur, Lund.

J. Weckerly and M. Kutas.  1999.  An electrophysiological 
analysis of animacy effects in the processing of object rel- 
ative sentences. Psychophysiology, 36:559–570.

Mutsumi Yamamoto. 1999. Animacy and Reference: A cog- 
nitive approach to corpus linguistics.   John Benjamins, 
Amsterdam.

Annie Zaenen, Jean Carletta, Gregory Garretson, Joan 
Bresnan,  Andrew  Koontz-Garboden,  Tatiana  Nikitina, 
M. Catherine O’Connor, and Tom Wasow.  2004.   Ani- 
macy encoding in English: why and how.  In Donna By- 
ron and Bonnie Webber, editors, Proceedings of the ACL 
Workshop on Discourse Annotation.

