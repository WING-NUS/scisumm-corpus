Document-level Automatic MT Evaluation based on Discourse Representations



Jesu´ s Gime´nez and 
Llu´ıs Ma` rquez 
TALP UPC 
Barcelona, Spain
{jgimenez, lluism}
@lsi.upc.edu


Elisabet Comelles and 
Irene Castello´ n Universitat 
de Barcelona Barcelona, 
Spain
{elicomelles, 
icastellon} @ub.edu


Victoria Arranz 
ELDA/ELRA 
Paris, France
arranz@elda.org






Abstract



  This paper describes the joint submission of 
Universitat Polite`cnica de Catalunya and Uni- 
versitat   de   Barcelona   to   the   Metrics   MaTr
2010 evaluation challenge, in collaboration with 
ELDA/ELRA. Our work is aimed at widening the 
scope of current automatic evaluation measures 
from sentence to document level. Preliminary ex- 
periments, based on an extension of the metrics by 
Gime´nez and Ma`rquez (2009) operating over dis- 
course representations, are presented.

1   Introduction

Current automatic similarity measures for Ma- 
chine Translation (MT) evaluation operate all, 
without exception, at the segment level.  Trans- 
lations are analyzed on a segment-by-segment1 
fashion,  ignoring the text structure.   Document 
and system scores are obtained using aggregate 
statistics over individual segments.  This strategy 
presents the main disadvantage of ignoring cross- 
sentential/discursive phenomena.
  In this work we suggest widening the scope 
of evaluation methods.  We have defined genuine 
document-level measures which are able to ex- 
ploit the structure of text to provide more informed 
evaluation scores. For that purpose we take advan- 
tage of two coincidental facts. First, test beds em- 
ployed in recent MT evaluation campaigns include 
a document structure grouping sentences related 
to the same event, story or topic (Przybocki et al.,
2008; Przybocki et al., 2009; Callison-Burch et al.,
2009).  Second, we count on automatic linguistic 
processors which provide very detailed discourse- 
level representations of text (Curran et al., 2007).
  Discourse representations allow us to focus on 
relevant pieces of information, such as the agent

1 A segment typically consists of one or two sentences.


(who), location (where), time (when), and theme 
(what), which may be spread all over the text. 
Counting on a means of discerning the events, the 
individuals taking part in each of them, and their 
role, is crucial to determine the semantic equiva- 
lence between a reference document and a candi- 
date translation.
  Moreover, the discourse analysis of a document 
is not a mere concatenation of the analyses of its 
individual sentences.  There are some phenom- 
ena which may go beyond the scope of a sen- 
tence and can only be explained within the con- 
text of the whole document.  For instance, in a 
newspaper article, facts and entities are progres- 
sively added to the discourse and then referred 
to anaphorically later on.  The following extract 
from the development set illustrates the impor- 
tance of such a phenomenon in the discourse anal- 
ysis: ‘Among the current or underlying crises in 
the Middle East, Rod Larsen mentioned the Arab- 
Israeli conflict and the Iranian nuclear portfolio, 
as well as the crisis between Lebanon and Syria. 
He stated: “All this leads us back to crucial val- 
ues and opinions, which render the situation prone 
at any moment to getting out of control, more so 
than it was in past days.”’.  The subject pronoun 
“he” works as an anaphoric pronoun whose an- 
tecedent is the proper noun “Rod Larson”.  The 
anaphoric relation established between these two 
elements can only be identified by analyzing the 
text as a whole, thus considering the gender agree- 
ment between the third person singular masculine 
subject pronoun “he” and the masculine proper 
noun “Rod Larson”.  However, if the two sen- 
tences were analyzed separately, the identification 
of this anaphoric relation would not be feasible 
due to the lack of connection between the two ele- 
ments. Discourse representations allow us to trace 
links across sentences between the different facts 
and entities appearing in them. Therefore, provid- 
ing an approach to the text more similar to that of


a human, which implies taking into account the 
whole text structure instead of considering each 
sentence separately.
  The rest of the paper is organized as follows. 
Section 2 describes our evaluation methods and 
the linguistic theory upon which they are based. 
Experimental results are reported and discussed in 
Section 3. Section 4 presents the metric submitted 
to the evaluation challenge.  Future work is out- 
lined in Section 5.
  As an additional result, document-level metrics 
generated in this study have been incorporated to 
the IQMT package for automatic MT evaluation2 .

2   Metric Description

This section provides a brief description of our ap- 
proach. First, in Section 2.1, we describe the un- 
derlying theory and give examples on its capabili- 
ties. Then, in Section 2.2, we describe the associ- 
ated similarity measures.

2.1   Discourse Representations
As previously mentioned in Section 1, a document 
has some features which need to be analyzed con- 
sidering it as a whole instead of dividing it up 
into sentences.   The anaphoric relation between 
a subject pronoun and a proper noun has already 
been exemplified.  However, this is not the only 
anaphoric relation which can be found inside a 
text, there are some others which are worth men- 
tioning:
• the connection between a possessive adjec- 
tive  and  a  proper  noun  or  a  subject  pro- 
noun, as exemplified in the sentences “Maria 
bought a new sweater.  Her new sweater is
blue.”, where the possessive feminine adjec- 
tive “her” refers to the proper noun “Maria”.
• the link between a demonstrative pronoun 
and its referent, which is exemplified in the 
sentences “He developed a new theory on
grammar.  However, this is not the only the- 
ory he developed”.  In the second sentence, 
the demonstrative pronoun ”this” refers back 
to the noun phrase “new theory on grammar” 
which occurs in the previous sentence.
• the relation between a main verb and an aux- 
iliary verb in certain contexts, as illustrated in 
the following pair of sentences “Would you

2 http://www.lsi.upc.edu/˜nlp/IQMT


like more sugar?  Yes, I would”.  In this ex- 
ample, the auxiliary verb “would” used in 
the short answer substitutes the verb phrase 
“would like”.

  In addition to anaphoric relations, other features 
need to be highlighted, such as the use of discourse 
markers which help to give cohesion to the text, 
link parts of a discourse and show the relations es- 
tablished between them.  Below, some examples 
are given:
• “Moreover”,  “Furthermore”,  “In  addition” 
indicate that the upcoming sentence adds 
more information.
• “However”,  “Nonetheless”,  “Nevertheless”
show contrast with previous ideas.
• “Therefore”, “As a result”, “Consequently”
show a cause and effect relation.
• “For instance”, “For example” clarify or il- 
lustrate the previous idea.

  It is worth noticing that anaphora, as well as dis- 
course markers, are key features in the interface 
between syntax, semantics and pragmatics. Thus, 
when dealing with these phenomena at a text level 
we are not just looking separately at the different 
language levels, but we are trying to give a com- 
plete representation of both the surface and the 
deep structures of a text.

2.2   Definition of Similarity Measures
In this work, as a first proposal, instead of elabo- 
rating on novel similarity measures, we have bor- 
rowed and extended the Discourse Representation 
(DR) metrics defined by Gime´nez and Ma`rquez 
(2009).   These metrics analyze similarities be- 
tween automatic and reference translations by 
comparing their respective discourse representa- 
tions over individual sentences.
  For the discursive analysis of texts, DR met- 
rics rely on the C&C Tools (Curran et al., 2007), 
specifically on the Boxer component (Bos, 2008). 
This software is based on the Discourse Represen- 
tation Theory (DRT) by Kamp and Reyle (1993). 
DRT is a theoretical framework offering a rep- 
resentation language for the examination of con- 
textually dependent meaning in discourse. A dis- 
course is represented in a discourse representation 
structure (DRS), which is essentially a variation of 
first-order predicate calculus —its forms are pairs


of first-order formulae and the free variables that 
occur in them.
  DRSs are viewed as semantic trees, built 
through the application of two types of DRS con- 
ditions:

basic conditions: one-place    properties    (pred- 
icates), two-place properties (relations), 
named entities, time-expressions, cardinal 
expressions and equalities.

complex conditions: disjunction,      implication, 
negation, question, and propositional attitude 
operations.

  For instance, the DRS representation for the 
sentence “Every man loves Mary.” is as follows:
∃y  named(y, mary, per)  ∧ (∀x  man(x) 	→
∃z	love(z)   ∧   event(z)   ∧   agent(z, x)   ∧
patient(z, y)). 	DR  integrates  three  different
kinds of metrics:

DR-STM These metrics are similar to the Syntac- 
tic Tree Matching metric defined by Liu and 
Gildea (2005), in this case applied to DRSs 
instead of constituent trees. All semantic sub- 
paths in the candidate and reference trees are 
retrieved. The fraction of matching subpaths 
of a given length (l=4 in our experiments) is 
computed.
DR-Or (⋆)  Average lexical overlap between dis- 
course representation structures of the same 
type.  Overlap is measured according to the 
formulae  and  definitions by  Gime´nez  and
Ma`rquez (2007).

DR-Orp(⋆) Average morphosyntactic overlap, 
i.e., between grammatical categories –parts- 
of-speech– associated to lexical items, be- 
tween discourse representation structures of
the same type.

  We have extended these metrics to operate at 
document level. For that purpose, instead of run- 
ning the C&C Tools in a sentence-by-sentence 
fashion,  we run  them  document by  document. 
This is as simple as introducing a “<META>” tag 
at the beginning of each document to denote doc- 
ument boundaries3 .

  3 Details  on  the  advanced  use  of  Boxer  are  avail- 
able at http://svn.ask.it.usyd.edu.au/trac/ 
candc/wiki/BoxerComplex.


3   Experimental Work

In this section, we analyze the behavior of the new 
DR metrics operating at document level with re- 
spect to their sentence-level counterparts.

3.1   Settings
We have used the ‘mt06’ part of the development 
set provided by the Metrics MaTr 2010 organiza- 
tion, which corresponds to a subset of 25 docu- 
ments from the NIST 2006 Open MT Evaluation 
Campaign Arabic-to-English translation.  The to- 
tal number of segments is 249. The average num- 
ber of segments per document is, thus, 9.96. The 
number of segments per document varies between
2 and 30. For the purpose of automatic evaluation,
4 human reference translations and automatic out- 
puts by 8 different MT systems are available.  In 
addition, we count on the results of a process of 
manual evaluation. Each translation segment was 
assessed by two judges. After independently and 
completely assessing the entire set, the judges re- 
viewed their individual assessments together and 
settled on a single final score. Average system ad- 
equacy is 5.38.
  In our experiments, metrics are evaluated in 
terms of their correlation with human assess- 
ments.   We have computed Pearson, Spearman 
and Kendall correlation coefficients between met- 
ric scores and adequacy assessments. Document- 
level and system-level assessments have been ob- 
tained by averaging over segment-level assess- 
ments. We have computed correlation coefficients 
and confidence intervals applying bootstrap re- 
sampling at a 99% statistical significance (Efron 
and Tibshirani, 1986; Koehn, 2004).   Since the 
cost of exhaustive resampling was prohibitive, we 
have limited to 1,000 resamplings. Confidence in- 
tervals, not shown in the tables, are in all cases 
lower than 10−3 .

3.2   Metric Performance
Table 1 shows correlation coefficients at the docu- 
ment level for several DR metric representatives, 
and their document-level counterparts (DRdoc). 
For the sake of comparison, the performance of 
the METEOR metric is also reported4 .
  Contrary to our expectations, DRdoc variants 
obtain lower levels of correlation than their DR

  4 We have used METEOR version 1.0 with default param- 
eters optimized by its developers over adequacy and fluency 
assessments. The METEOR metric is publicly available at 
http://www.cs.cmu.edu/˜alavie/METEOR/


Metric










Table 1: Meta-evaluation results at document level

Metric















Table 2: Meta-evaluation results at system level




counterparts.   There are three different factors 
which could provide a possible explanation for 
this negative result. First, the C&C Tools, like any 
other automatic linguistic processor are not per- 
fect.  Parsing errors could be causing the metric 
to confer less informed scores. This is especially 
relevant taking into account that candidate transla- 
tions are not always well-formed.  Secondly, we 
argue that the way in which we have obtained 
document-level quality assessments, as an average 
of segment-level assessments, may be biasing the 
correlation.  Thirdly, perhaps the similarity mea- 
sures employed are not able to take advantage of 
the document-level features provided by the dis- 
course analysis.  In the following subsection we 
show some error analysis we have conducted by 
inspecting particular cases.

  Table 2 shows correlation coefficients at system 
level. In the case of DR and DRdoc metrics, sys- 
tem scores are computed by simple average over 
individual documents.  Interestingly, in this case 
DRdoc variants seem to obtain higher correlation 
than their DR counterparts.  The improvement is 
especially substantial in terms of Spearman and 
Kendall coefficients, which do not consider ab- 
solute values but ranking positions.  However, it 
could be the case that it was just an average ef-


fect. While DR metrics compute system scores as 
an average of segment scores, DRdoc metrics av- 
erage directly document scores. In order to clarify 
this result, we have modified DR metrics so as to 
compute system scores as an average of document 
scores (DR′  variants, the last three rows in the ta- 
ble).  It can be observed that DR’ variants out- 
perform their DRdoc counterparts, thus confirming 
our suspicion about the averaging effect.

3.3   Analysis
It is worth noting that DRdoc metrics are able to 
detect and deal with several linguistic phenomena 
related to both syntax and semantics at sentence 
and document level.  Below, several examples il- 
lustrating the potential of this metric are presented.

Control structures. Control   structures   (either 
subject or object control) are always a 
difficult issue as they mix both syntactic and 
semantic knowledge. In Example 1 a couple 
of   control  structures   must  be  identified 
and DRdoc metrics deal correctly with the 
argument structure of all the verbs involved. 
Thus, in the first part of the sentence, a 
subject control verb can be identified being 
“the minister” the agent of both verb forms 
“go” and “say”.  On the other hand, in the


quoted question, the verb “invite” works as 
an object control verb because its patient 
“Chechen representatives” is also the agent 
of the verb visit.

Example 1:  The minister went on to say, 
“What would Moscow say if we were to invite 
Chechen representatives to visit Jerusalem?”

Anaphora and pronoun resolution. Whenever 
there  is  a  pronoun  whose antecedent  is  a 
named  entity  (NE),  the  metric  identifies 
correctly  its  antecedent.	This  feature  is 
highly valuable because a relationship be- 
tween syntax and semantics is established. 
Moreover,	when   dealing   with   Semantic 
Roles the roles of Agent or Patient are given 
to the antecedents instead of the pronouns. 
Thus,  in Example 2 the antecedent of the 
relative pronoun “who” is the NE “Putin” 
and the patient of the verb “classified” is 
also the NE “Putin” instead of the relative 
pronoun “who”.

Example 2:  Putin, who was not classified 
as his country Hamas as “terrorist organiza- 
tions”, recently said that the European Union 
is “a big mistake” if it decided to suspend fi- 
nancial aid to the Palestinians.

Nevertheless, although Boxer was expected 
to deal with long-distance anaphoric relations 
beyond the sentence, after analyzing several 
cases, results show that it did not succeed in 
capturing this type of relations as shown in 
Example 3.  In this example, the antecedent 
of the pronoun “he” in the second sentence 
is the NE “Roberto Calderoli” which ap- 
pears in the first sentence.   DRdoc metrics 
should be capable of showing this connec- 
tion.   However, although the proper noun 
“Roberto Calderoli” is identified as a NE, it
does not share the same reference as the third 
person singular pronoun “he”.

Example 3:  Roberto Calderoli does not in- 
tend to apologize.  The newspaper Corriere 
Della  Sera  reported  today,  Saturday,  that 
he said “I don’t feel responsible for those 
deaths.”

4	Our Submission

Instead of participating with individual metrics, 
we have combined them by averaging their scores


as described in (Gime´nez and Ma`rquez, 2008). 
This strategy has proven as an effective means of 
combining the scores conferred by different met- 
rics (Callison-Burch et al., 2008; Callison-Burch 
et al., 2009). Metrics submitted are:

DRdoc an arithmetic mean over a heuristically- 
defined set of DRdoc metric variants, respec- 
tively computing lexical overlap, morphosyn- 
tactic  overlap,  and  semantic  tree  match-
ing (M = {‘DRdoc-Or (⋆)’, ‘DRdoc-Orp (⋆)’, ‘DRdoc-
STM4 ’}).  Since DRdoc metrics do not operate
over individual segments, we have assigned
each segment the score of the document in 
which it is contained.

DR a measure analog to DRdoc but using the de- 
fault version of DR metrics operating at the 
segment level (M = {‘DR-Or (⋆)’,  ‘DR-Orp (⋆)’,
‘DR-STM4 ’}).

ULCh an arithmetic mean over a heuristically- 
defined set  of  metrics  operating  at  differ- 
ent linguistic levels, including lexical met- 
rics, and measures of overlap between con- 
stituent parses, dependency parses, seman-
tic roles, and discourse representations (M =
{‘ROUGEW ’, ‘METEOR’, ‘DP-HWCr ’, ‘DP-Oc (⋆)’,
‘DP-Ol (⋆)’,   ‘DP-Or (⋆)’,   ‘CP-STM4 ’,  ‘SR-Or (⋆)’,
‘SR-Orv ’,  ‘DR-Orp(⋆)’}).	This metric corre-
sponds exactly to the metric submitted in our
previous participation.

  The performance of these metrics at the docu- 
ment and system levels is shown in Table 3.

5   Conclusions and Future Work

We have presented a modified version of the DR 
metrics by Gime´nez and Ma`rquez (2009) which, 
instead of limiting their scope to the segment level, 
are able to capture and exploit document-level fea- 
tures.  However, results in terms of correlation 
with human assessments have not reported any im- 
provement of these metrics over their sentence- 
level counterparts as document and system quality 
predictors. It must be clarified whether the prob- 
lem is on the side of the linguistic tools, in the 
similarity measure, or in the way in which we have 
built document-level human assessments.
  For future work, we plan to continue the er- 
ror analysis to clarify why DRdoc metrics do not 
outperform their DR counterparts at the document 
level, and how to improve their behavior.   This




Metric





Table 3: Meta-evaluation results at document and system level for submitted metrics




may imply defining new metrics possibly using 
alternative linguistic processors.  In addition, we 
plan to work on the identification and analysis 
of discourse markers.  Finally, we plan to repeat 
this experiment over other test beds with docu- 
ment structure, such as those from the 2009 Work- 
shop on Statistical Machine Translation shared 
task (Callison-Burch et al., 2009) and the 2009
NIST MT Evaluation Campaign (Przybocki et al.,
2009).  In the case that document-level assess- 
ments are not provided, we will also explore the 
possibility of producing them ourselves.

Acknowledgments

This work has been partially funded by the 
Spanish Government (projects OpenMT-2, 
TIN2009-14675-C03,   and  KNOW,  TIN-2009-
14715-C0403) and the European Community’s 
Seventh Framework Programme (FP7/2007-2013) 
under grant agreement numbers 247762 (FAUST 
project, FP7-ICT-2009-4-247762) and 247914 
(MOLTO project, FP7-ICT-2009-4-247914).  We 
are also thankful to anonymous reviewers for their 
comments and suggestions.


References

Johan Bos.    2008.   Wide-coverage semantic analy- 
sis with boxer.   In Johan Bos and Rodolfo Del- 
monte, editors, Semantics in Text Processing. STEP
2008 Conference Proceedings, Research in Compu- 
tational Semantics, pages 277–286. College Publi-
cations.

Chris Callison-Burch, Cameron Fordyce, Philipp 
Koehn, Christof Monz, and Josh Schroeder.  2008. 
Further meta-evaluation of machine translation.  In 
Proceedings of the Third Workshop on Statistical 
Machine Translation, pages 70–106.

Chris Callison-Burch, Philipp Koehn, Christof Monz, 
and Josh Schroeder.  2009.  Findings of the 2009
Workshop on Statistical Machine Translation.   In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1–28.

James Curran, Stephen Clark, and Johan Bos.  2007.
Linguistically motivated large-scale nlp with c&c


and boxer. In Proceedings of the 45th Annual Meet- 
ing of the Association for Computational Linguistics 
Companion Volume Proceedings of the Demo and 
Poster Sessions, pages 33–36.

Bradley Efron and Robert Tibshirani. 1986. Bootstrap 
Methods for Standard Errors, Confidence Intervals, 
and Other Measures of Statistical Accuracy. Statis- 
tical Science, 1(1):54–77.

Jesu´ s Gime´nez and Llu´ıs Ma`rquez.   2007.   Linguis- 
tic Features for Automatic Evaluation of Heteroge- 
neous MT Systems.   In Proceedings of the ACL 
Workshop on Statistical Machine Translation, pages
256–264.

Jesu´ s Gime´nez and Llu´ıs Ma`rquez.  2008.  A Smor- 
gasbord of Features for Automatic MT Evaluation. 
In Proceedings of the Third Workshop on Statistical 
Machine Translation, pages 195–198.

Jesu´ s Gime´nez and Llu´ıs Ma`rquez. 2009. On the Ro- 
bustness of Syntactic and Semantic Features for Au- 
tomatic MT Evaluation.  In Proceedings of the 4th 
Workshop on Statistical Machine Translation (EACL
2009).

Hans Kamp and Uwe Reyle. 1993. From Discourse to 
Logic: An Introduction to Modeltheoretic Semantics 
of Natural Language, Formal Logic and Discourse 
Representation Theory. Dordrecht: Kluwer.

Philipp Koehn.   2004.   Statistical Significance Tests 
for Machine Translation Evaluation. In Proceedings 
of the Conference on Empirical Methods in Natural 
Language Processing (EMNLP), pages 388–395.

Ding Liu and Daniel Gildea. 2005. Syntactic Features 
for Evaluation of Machine Translation. In Proceed- 
ings of ACL Workshop on Intrinsic and Extrinsic 
Evaluation Measures for MT and/or Summarization, 
pages 25–32.

Mark Przybocki, Kay Peterson, and Se´bastien Bron- 
sart.  2008.  NIST Metrics for Machine Translation
2008 Evaluation (MetricsMATR08).  Technical re- 
port, National Institute of Standards and Technol- 
ogy.

Mark Przybocki, Kay Peterson, and Se´bastien Bron- 
sart.  2009.  NIST Open Machine Translation 2009
Evaluation (MT09).  Technical report, National In-
stitute of Standards and Technology.


