<PAPER>
	<ABSTRACT>
	<SECTION title="Why Parse Word Structures?. " number = "1">
			<S sid ="1" ssid = "1">Research in Chinese word segmentation has progressed tremendously in recent years, with state of JJf 䉂 NNf 擌奒 the art performing at around 97% in precision and recall (Xue, 2003; Gao et al., 2005; Zhang and Clark, 2007; Li and Sun, 2009).</S>
			<S sid ="2" ssid = "2">However, virtually all these systems focus exclusively on recognizing the word boundaries, giving no consideration to the internal structures of many words.</S>
			<S sid ="3" ssid = "3">Though it has been the standard practice for many years, we argue that this paradigm is inadequate both in theory and in practice, for at least the following four reasons.</S>
			<S sid ="4" ssid = "4">The first reason is that if we confine our definition of word segmentation to the identification of word boundaries, then people tend to have divergent opinions as to whether a linguistic unit is a word or not (Sproat et al., 1996). This has led to many different annotation standards for Chinese word segmentation. Even worse, this could cause inconsistency in the same corpus. For instance, 䉂擌奒 ‘vice president’ is considered to be one word in the Penn Chinese Treebank (Xue et al., 2005), but is split into two words by the Peking University corpus in the SIGHAN Bakeoffs (Sproat and Emerson, 2003). Meanwhile, 䉂䀓惼‘vice director’ and 䉂 䚲䡮‘deputy manager’ are both segmented into two words in the same Penn Chinese Treebank. In fact, all these words are composed of the prefix 䉂‘vice’ and a root word. Thus the structure of 䉂擌奒‘vice president’ can be represented with the tree in Figure 1. Figure 1: Example of a word with internal structure.</S>
			<S sid ="5" ssid = "5">Without a doubt, there is complete agreement on the correctness of this structure among native Chinese speakers.</S>
			<S sid ="6" ssid = "6">So if instead of annotating only word boundaries, we annotate the structures of every word, 1 then the annotation tends to be more 1 Here it is necessary to add a note on terminology used in this paper.</S>
			<S sid ="7" ssid = "7">Since there is no universally accepted definition of the “word” concept in linguistics and especially in Chinese, whenever we use the term “word” we might mean a linguistic unit such as 䉂擌奒 ‘vice president’ whose structure is shown as the tree in Figure 1, or we might mean a smaller unit such as 擌奒 ‘president’ which is a substructure of that tree.</S>
			<S sid ="8" ssid = "8">Hopefully, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1405–1414, Portland, Oregon, June 1924, 2011.</S>
			<S sid ="9" ssid = "9">Qc 2011 Association for Computational Linguistics consistent and there could be less duplication of efforts in developing the expensive annotated corpus.</S>
			<S sid ="10" ssid = "10">The second reason is applications have different requirements for granularity of words.</S>
			<S sid ="11" ssid = "11">Take the per sonal name 撱嗤吼 ‘Zhou Shuren’ as an example.</S>
			<S sid ="12" ssid = "12">It’s considered to be one word in the Penn Chinese Treebank, but is segmented into a surname and a given name in the Peking University corpus.</S>
			<S sid ="13" ssid = "13">For some applications such as information extraction, the former segmentation is adequate, while for others like machine translation, the later finer-grained output is more preferable.</S>
			<S sid ="14" ssid = "14">If the analyzer can produce a structure as shown in Figure 4(a), then every application can extract what it needs from this tree.</S>
			<S sid ="15" ssid = "15">A solution with tree output like this is more el structures is related to head driven statistical parsers (Collins, 2003).</S>
			<S sid ="16" ssid = "16">To illustrate this, note that in thePenn Chinese Treebank, the word 戽 䊂 䠽 吼 ‘En glish People’ does not occur at all.</S>
			<S sid ="17" ssid = "17">Hence constituents headed by such words could cause some difficulty for head driven models in which out-of- vocabulary words need to be treated specially both when they are generated and when they are conditioned upon.</S>
			<S sid ="18" ssid = "18">But this word is in turn headed by its suffix 吼 ‘people’, and there are 2,233 such words in Penn Chinese Treebank.</S>
			<S sid ="19" ssid = "19">If we annotate the structure of every compound containing this suffix (e.g. Figure 3), such data sparsity simply goes away.</S>
			<S sid ="20" ssid = "20">NN ✧❜ egant than approaches which try to meet the needs of different applications in post-processing (Gao et al., 2004).The third reason is that traditional word segmen ✧ NRf 戽䊂䠽 ❜ NNf 吼 tation has problems in handling many phenomena in Chinese.</S>
			<S sid ="21" ssid = "21">For example, the telescopic compound 㦌撥怂惆 ‘universities, middle schools and primary schools’ is in fact composed of three coordinating elements 㦌惆 ‘university’, 撥惆 ‘middle school’ and 怂惆 ‘primary school’.</S>
			<S sid ="22" ssid = "22">Regarding it as one flat word loses this important information.</S>
			<S sid ="23" ssid = "23">Another example is separable words like 扩 扙 ‘swim’.</S>
			<S sid ="24" ssid = "24">With a lin ear segmentation, the meaning of ‘swimming’ as in 扩 堑 扙 ‘after swimming’ cannot be properly represented, since 扩扙 ‘swim’ will be segmented into discontinuous units.</S>
			<S sid ="25" ssid = "25">These language usages lie at the boundary between syntax and morphology, and are not uncommon in Chinese.</S>
			<S sid ="26" ssid = "26">They can be adequately represented with trees (Figure 2).</S>
			<S sid ="27" ssid = "27">Figure 3: Structure of the out-of-vocabulary word 戽䊂 䠽吼 ‘English People’.</S>
			<S sid ="28" ssid = "28">Had there been only a few words with internal structures, current Chinese word segmentation paradigm would be sufficient.</S>
			<S sid ="29" ssid = "29">We could simply recover word structures in post-processing.</S>
			<S sid ="30" ssid = "30">But this is far from the truth.</S>
			<S sid ="31" ssid = "31">In Chinese there is a large number of such words.</S>
			<S sid ="32" ssid = "32">We just name a few classes of these words and give one example for each class (a dot is used to separate roots from affixes): personal name: 㡿増·揽 ‘Nagao Makoto’ location name: 凝挕·撲 ‘New York State’ noun with a suffix: 䆩䡡·勬 ‘classifier’ noun with a prefix: 敏·䧥䧥 ‘mother-to-be’ verb with a suffix: 敧䃄·䑺 ‘automatize’ (a) NN ✟✟✟❍❍❍ (b) VV ✟ ✟❍❍❍ verb with a prefix: 䆓·噙 ‘waterproof’ adjective with a suffix: 䉅䏜·怮 ‘composite’ JJ ✟❍❍❍ NNf VV NNf ✚❩ adjective with a prefix: 䆚·搔喪 ‘informal’ ✟✟ JJf 㦌 JJf 撥 JJf 惆 怂 ✚ VVf 扩 ❩ VVf 扙 堑 pronoun with a prefix: 䊈·墠 ‘everybod y’ time expression: 憘䛊䛊壊· 兣 ‘the year 1995’ ordinal number: 䀱·喛憘 ‘eleventh’ retroflex suffixation: 䑳䃹·䄎 ‘flower’ Figure 2: Example of telescopic compound (a) and separable word (b).</S>
			<S sid ="33" ssid = "33">The last reason why we should care about word the context will always make it clear what is being referred to with the term “word”.</S>
			<S sid ="34" ssid = "34">This list is not meant to be complete, but we can get a feel of how extensive the words with nontrivial structures can be.</S>
			<S sid ="35" ssid = "35">With so many productive suffixes and prefixes, analyzing word structures in post- processing is difficult, because a character may or may not act as an affix depending on the context.</S>
			<S sid ="36" ssid = "36">For example, the character 吼 ‘people’ in 撇 嗤 吼 ‘the one who plants’ is a suffix, but in the personal name 撱嗤吼 ‘Zhou Shuren’ it isn’t. The structures of these two words are shown in Figure 4.</S>
			<S sid ="37" ssid = "37">In this paper, we propose a new paradigm for Chinese word segmentation in which not only word boundaries are identified but the internal structures of words are recovered (Section 3).</S>
			<S sid ="38" ssid = "38">To achieve this, we design a joint morphological and syntactic pars (a) NR ✚✚❩❩ (b) NN ✚✚❩❩ ing model of Chinese (Section 4).</S>
			<S sid ="39" ssid = "39">Our generative story describes the complete process from sentence NFf 撱 NGf 嗤吼 VVf 撇嗤 NNf 吼 and word structures to the surface string of characters in a top down fashion.</S>
			<S sid ="40" ssid = "40">With this probability model, we give an algorithm to find the parse Figure 4: Two words that differ only in one character, but have different internal structures.</S>
			<S sid ="41" ssid = "41">The character 吼 ‘people’ is part of a personal name in tree (a), but is a suffix in (b).</S>
			<S sid ="42" ssid = "42">A second reason why generally we cannot recover word structures in post-processing is that some words have very complex structures.</S>
			<S sid ="43" ssid = "43">For example, the tree of 壃 搕 䈿 擌 懂 揶 ‘anarchist’ is shown in Figure 5.</S>
			<S sid ="44" ssid = "44">Parsing this structure correctly without a principled method is difficult and messy, if not impossible.</S>
			<S sid ="45" ssid = "45">NN ✦✦✦❛❛❛ tree of a raw sentence with the highest probability (Section 5).</S>
			<S sid ="46" ssid = "46">The output of our parser incorporates word structures naturally.</S>
			<S sid ="47" ssid = "47">Evaluation shows that the model can learn much of the regularity of word structures, and also achieves reasonable accuracy in parsing higher level constituent structures (Section 6).</S>
	</SECTION>
	<SECTION title="Related Work. " number = "2">
			<S sid ="48" ssid = "1">The necessity of parsing word structures has been noticed by Zhao (2009), who presented a character- level dependency scheme as an alternative to the linear representation of words.</S>
			<S sid ="49" ssid = "2">Although our work is based on the same notion, there are two key dif NN ✟✟✟❍❍❍ NNf ferences.</S>
			<S sid ="50" ssid = "3">The first one is that part-of-speech tags and constituent labels are fundamental for our pars VV ✚✚❩❩ NNf 揶 ing model, while Zhao focused on unlabeled depen dencies between characters in a word, and part-of VVf 壃 NNf 擌懂 搕䈿 speech information was not utilized.</S>
			<S sid ="51" ssid = "4">Secondly, we distinguish explicitly the generation of flat words such as 䑵喏䃮 ‘Washington’ and words with inter Figure 5: An example word which has very complex structures.</S>
			<S sid ="52" ssid = "5">Finally, it must be mentioned that we cannot store all word structures in a dictionary, as the word formation process is very dynamic and productive innature.</S>
			<S sid ="53" ssid = "6">Take 䌬 ‘hall’ as an example.</S>
			<S sid ="54" ssid = "7">Standard Chi nese dictionaries usually contain 埣 嗖 䌬 ‘library’,but not many other words such as 䎰 愒 䌬 ‘aquar ium’ generated by this same character.</S>
			<S sid ="55" ssid = "8">This is understandable since the character 䌬 ‘hall’ is so pro ductive that it is impossible for a dictionary to list every word with this character as a suffix.</S>
			<S sid ="56" ssid = "9">The same thing happens for natural language processing systems.</S>
			<S sid ="57" ssid = "10">Thus it is necessary to have a dynamic mechanism for parsing word structures.</S>
			<S sid ="58" ssid = "11">nal structures.</S>
			<S sid ="59" ssid = "12">Our parsing algorithm also has to be adapted accordingly.</S>
			<S sid ="60" ssid = "13">Such distinction was not made in Zhao’s parsing model and algorithm.</S>
			<S sid ="61" ssid = "14">Many researchers have also noticed the awkwardness and insufficiency of current boundary-only Chinese word segmentation paradigm, so they tried to customize the output to meet the requirements of various applications (Wu, 2003; Gao et al., 2004).</S>
			<S sid ="62" ssid = "15">In a related research, Jiang et al.</S>
			<S sid ="63" ssid = "16">(2009) presented a strategy to transfer annotated corpora between different segmentation standards in the hope of saving some expensive human labor.</S>
			<S sid ="64" ssid = "17">We believe the best solution to the problem of divergent standards and requirements is to annotate and analyze word structures.</S>
			<S sid ="65" ssid = "18">Then applications can make use of these structures according to their own convenience.</S>
			<S sid ="66" ssid = "19">Since the distinction between morphology and syntax in Chinese is somewhat blurred, our model for word structure parsing is integrated with constituent parsing.</S>
			<S sid ="67" ssid = "20">There has been many efforts to integrate Chinese word segmentation, part-of-speech tagging and parsing (Wu and Zixin, 1998; Zhou and Su, 2003; Luo, 2003; Fung et al., 2004).</S>
			<S sid ="68" ssid = "21">However, in these research all words were considered to be flat, and thus word structures were not parsed.</S>
			<S sid ="69" ssid = "22">This is a crucial difference with our work.</S>
			<S sid ="70" ssid = "23">Specifically, consider the word 碾 碜 扨 ‘olive oil’.</S>
			<S sid ="71" ssid = "24">Our parser output tree Figure 6(a), while Luo (2003) output tree of head-driven generative models (Charniak, 1997; Bikel and Chiang, 2000) .</S>
	</SECTION>
	<SECTION title="The New Paradigm. " number = "3">
			<S sid ="72" ssid = "1">Given a raw Chinese sentence like 䤕 撓 䏓 喴 敯 䋳 㢧 喓, a traditional word segmentation system would output some result like 䤕撓䏓 喴 敯䋳㢧 喓(‘Lin Zhihao’, ‘is’, ‘chief engineer’).</S>
			<S sid ="73" ssid = "2">In our new paradigm, the output should at least be a linear sequence of trees representing the structures of each word as in Figure 7.</S>
			<S sid ="74" ssid = "3">(b), giving no hint to the structure of this word since the result is the same with a real flat word 䧢哫膝 NR VV ✑◗ NN ✟✟❍❍ ‘Los Angeles’(c).</S>
			<S sid ="75" ssid = "4">✑ NFf ◗ NGf ✟ VVf JJ ❍ NN ✚✚❩❩ (a) NN ✚✚❩❩ (b) NN (c) NR 䤕 撓䏓 喴 JJf NNf NNf NNf NNf NNf NRf 敯 䋳㢧 喓 碾碜 扨 碾碜扨 䧢哫膝 Figure 7: Proposed output for the new Chinese word segmentation paradigm.</S>
			<S sid ="76" ssid = "5">Figure 6: Difference between our output (a) of parsing the word 碾 碜 扨 ‘olive oil’ and the output (b) of Luo (2003).</S>
			<S sid ="77" ssid = "6">In (c) we have a true flat word, namely the location name 䧢哫膝 ‘Los Angeles’.</S>
			<S sid ="78" ssid = "7">The benefits of joint modeling has been noticed by many.</S>
			<S sid ="79" ssid = "8">For example, Li et al.</S>
			<S sid ="80" ssid = "9">(2010) reported that a joint syntactic and semantic model improved the accuracy of both tasks, while Ng and Low (2004) showed it’s beneficial to integrate word segmentation and part-of-speech tagging into one model.</S>
			<S sid ="81" ssid = "10">The later result is confirmed by many others (Zhang and Clark, 2008; Jiang et al., 2008; Kruengkrai et al., 2009).</S>
			<S sid ="82" ssid = "11">Goldberg and Tsarfaty (2008) showed that a single model for morphological segmentation and syntactic parsing of Hebrew yielded an error reduc Note that in the proposed output, all words are annotated with their part-of-speech tags.</S>
			<S sid ="83" ssid = "12">This is necessary since part-of-speech plays an important role in the generation of compound words.</S>
			<S sid ="84" ssid = "13">For example, 揶 ‘person’ usually combines with a verb to form a compound noun such as 唗䕏揶 ‘designer’.</S>
			<S sid ="85" ssid = "14">In this paper, we will actually design an integrated morphological and syntactical parser trained with a treebank.</S>
			<S sid ="86" ssid = "15">Therefore, the real output of our system looks like Figure 8.</S>
			<S sid ="87" ssid = "16">It’s clear that besides all S ✏✏✏✏ NP VP ✦✦✦❛❛❛ tion of 12% over the best pipelined models.</S>
			<S sid ="88" ssid = "17">This is because an integrated approach can effectively take NR ✚✚❩❩ VV NN ✟✟✟❍❍❍ into account more information from different levels of analysis.</S>
			<S sid ="89" ssid = "18">NFf NGf VVf JJ NN ✚✚❩❩ Parsing of Chinese word structures can be reduced to the usual constituent parsing, for which there has been great progress in the past several 䤕 撓䏓 喴 JJf 敯 NNf 䋳㢧 NNf 喓 years.</S>
			<S sid ="90" ssid = "19">Our generative model for unified word and phrase structure parsing is a direct adaptation of the model presented by Collins (2003).</S>
			<S sid ="91" ssid = "20">Many other approaches of constituent parsing also use this kind Figure 8: The actual output of our parser trained with a fully annotated treebank.</S>
			<S sid ="92" ssid = "21">the information of the proposed output for the new paradigm, our model’s output also includes higher- level syntactic parsing results.</S>
			<S sid ="93" ssid = "22">(a) NN (b) NN ✱✱❧❧ 3.1 Training Data.</S>
			<S sid ="94" ssid = "23">NNf 憞䠮䞎 JJf 卣 NNf 敯埚 We employ a statistical model to parse phrase and word structures as illustrated in Figure 8.</S>
			<S sid ="95" ssid = "24">The currently available treebank for us is the Penn Chinese Treebank (CTB) 5.0 (Xue et al., 2005).</S>
			<S sid ="96" ssid = "25">Because our model belongs to the family of head-driven statistical parsing models (Collins, 2003), we use the head- finding rules described by Sun and Jurafsky (2004).</S>
			<S sid ="97" ssid = "26">Unfortunately, this treebank or any other tree- banks for that matter, does not contain annotations of word structures.</S>
			<S sid ="98" ssid = "27">Therefore, we must annotate these structures by ourselves.</S>
			<S sid ="99" ssid = "28">The good news is that the annotation is not too complicated.</S>
			<S sid ="100" ssid = "29">First, we extract all words in the treebank and check each of them manually.</S>
			<S sid ="101" ssid = "30">Words with nontrivial structures are thus annotated.</S>
			<S sid ="102" ssid = "31">Finally, we install these small trees of words into the original treebank.</S>
			<S sid ="103" ssid = "32">Whether a word has structures or not is mostly context independent, so we only have to annotate each word once.</S>
			<S sid ="104" ssid = "33">There are two noteworthy issues in this process.</S>
			<S sid ="105" ssid = "34">Firstly, as we’ll see in Section 4, flat words and non-flat words will be modeled differently, thus it’s important to adapt the part-of-speech tags to facilitate this modeling strategy.</S>
			<S sid ="106" ssid = "35">For example, the tag fornouns is NN as in 憞䠮䞎 ‘Iraq’ and 卣敯埚 ‘for mer president’.</S>
			<S sid ="107" ssid = "36">After annotation, the former is flat, but the later has a structure (Figure 9).</S>
			<S sid ="108" ssid = "37">So we change the POS tag for flat nouns to NNf, then during bottom up parsing, whenever a new constituent ending with ‘f’ is found, we can assign it a probability in a way different from a structured word or phrase.</S>
			<S sid ="109" ssid = "38">Secondly, we should record the head position of each word tree in accordance with the requirements of head driven parsing models.</S>
			<S sid ="110" ssid = "39">As an example, the right tree in Figure 9 has the context free rule “NN → JJf NNf”, the head of which should be the rightmost NNf.</S>
			<S sid ="111" ssid = "40">Therefore, in 卣敯埚 ‘former president’ the head is 敯埚 ‘president’.</S>
			<S sid ="112" ssid = "41">In passing, the readers should note the fact that in Figure 9, we have to add a parent labeled NN to the flat word 憞䠮䞎 ‘Iraq’ so as not to change the context-free rules contained inherently in the original treebank.</S>
			<S sid ="113" ssid = "42">Figure 9: Example word structure annotation.</S>
			<S sid ="114" ssid = "43">We add an ‘f’ to the POS tags of words with no further structures.</S>
	</SECTION>
	<SECTION title="The Model. " number = "4">
			<S sid ="115" ssid = "1">Given an observed raw sentences S, our generative model tells a story about how this surface sequence of Chinese characters is generated with a linguistically plausible morphological and syntactical process, thereby defining a joint probability Pr(T , S) where T is a parse tree carrying word structures as well as phrase structures.</S>
			<S sid ="116" ssid = "2">With this model, the pars ing problem is to search for the tree T ∗ such that T ∗ = arg max Pr(T , S) (1) T The generation of S is defined in a top down fashion, which can be roughly summarized as follows.</S>
			<S sid ="117" ssid = "3">First, the lexicalized constituent structures are generated, then the lexicalized structure of each word is generated.</S>
			<S sid ="118" ssid = "4">Finally, flat words with no structures are generated.</S>
			<S sid ="119" ssid = "5">As soon as this is done, we get a tree whose leaves are Chinese characters and can be concatenated to get the surface character sequence S. 4.1 Generation of Constituent Structures.</S>
			<S sid ="120" ssid = "6">Each node in the constituent tree corresponds to a lexicalized context free rule P → Ln Ln−1 · · · L1H R1 R2 · · · Rm (2) where P , Li, Ri and H are lexicalized nonterminals and H is the head.</S>
			<S sid ="121" ssid = "7">To generate this constituent, first P is generated, then the head child H is generated conditioned on P , and finally each Li and Rj are generated conditioned on P and H and a distance metric.</S>
			<S sid ="122" ssid = "8">This breakdown of lexicalized PCFG rules is essentially the Model 2 defined by Collins (1999).</S>
			<S sid ="123" ssid = "9">We refer the readers to Collins’ thesis for further details.</S>
			<S sid ="124" ssid = "10">4.2 Generation of Words with Internal.</S>
			<S sid ="125" ssid = "11">Structures Words with rich internal structures can be described using a context-free grammar formalism as wo rd → ro ot ( 3 ) wo rd → wo rd su ffix ( 4 ) wo rd → pr efi x wo rd ( 5 ) Here the root is any word without interesting internal structures, and the prefixes and suffixes are not lim ited to single characters.</S>
			<S sid ="126" ssid = "12">For example, 擌懂 ‘ism’ as in 她㦓擌懂 ‘modernism’ is a well known and very productive suffix.</S>
			<S sid ="127" ssid = "13">Also, we can see that rules (4) and (5) are recursive and hence can handle words with very complex structures.</S>
			<S sid ="128" ssid = "14">By (3)–(5), the generation of word structures is exactly the same as that of ordinary phrase structures.</S>
			<S sid ="129" ssid = "15">Hence the probabilities of these words can be defined in the same way as higher level constituents in (2).</S>
			<S sid ="130" ssid = "16">Note that in our case, each word with structures is naturally lexicalized, since in the annotation process we have been careful to record the head position of each complex word.</S>
			<S sid ="131" ssid = "17">As an example, consider a word w = R(r) S(s) where R is the root part-of-speech headed by the word r, and S is the suffix part-of-speech headed by s. If the head of this word is its suffix, then we can define the probability of w by Pr(w) = Pr(S, s) · Pr(R, r|S, s) (6) trivial and deterministic since every phrase and word structure rules are lexicalized.</S>
			<S sid ="132" ssid = "18">However, the generation of unknown flat words is a different story.</S>
			<S sid ="133" ssid = "19">During training, words that occur less than 6 times are substituted with the symbol UNKNOWN.</S>
			<S sid ="134" ssid = "20">In testing, unknown words are generated after the generation of symbol UNKNOWN, and we define their probability by a first-order Markov model.</S>
			<S sid ="135" ssid = "21">That is, given a flat word w = c1c2 · · · cn not seen in training, we define its probability conditioned with the part-of-speech p as n+1 Pr(w|p) = n Pr(ci|ci−1, p) (7) i=1 where c0 is taken to be a START symbol indicating the left boundary of a word and cn+1 is the STOP symbol to indicate the right boundary.</S>
			<S sid ="136" ssid = "22">Note that the generation of w is only conditioned on its part-of- speech p, ignoring the larger constituent or word in which w occurs.</S>
			<S sid ="137" ssid = "23">We use a back-off strategy to smooth the probabilities in (7): P˜r(ci|ci−1, p) = λ1 · Pˆr(ci|ci−1, p) + λ2 · Pˆr(ci|ci−1) +λ3 · Pˆr(ci) (8) where λ1 + λ2 + λ3 = 1 to ensure the conditional probability is well formed.</S>
			<S sid ="138" ssid = "24">These λs will be estimated with held-out data.</S>
			<S sid ="139" ssid = "25">The probabilities on the right side of (8) can be estimated with simple counts: This is equivalent to saying that to generate w, we Pˆr(ci|ci 1, p) = COUNT(c i−1 ci, p) (9) first generate its head S(s), then conditioned on this head, other components of this word are generated.</S>
			<S sid ="140" ssid = "26">In actual parsing, because a word always occurs in some contexts, the above probability should also be conditioned on these contexts, such as its parent and the parent’s head word.</S>
			<S sid ="141" ssid = "27">4.3 Generation of Flat Words.</S>
			<S sid ="142" ssid = "28">We say a word is flat if it contains only one morpheme such as 憞䠮䞎 ‘Iraq’, or if it is a compound like 䝭䅵 ‘develop’ which does not have a produc tive component we are currently interested in.</S>
			<S sid ="143" ssid = "29">Depending on whether a flat word is known or not, − COUNT(ci−1, p) The other probabilities can be estimated in the same way.</S>
			<S sid ="144" ssid = "30">4.4 Summary of the Generative Story.</S>
			<S sid ="145" ssid = "31">We make a brief summary of our generative story for the integrated morphological and syntactic parsing model.</S>
			<S sid ="146" ssid = "32">For a sentence S and its parse tree T , if we denote the set of lexicalized phrase structures in T by C, the set of lexicalized word structures by W , and the set of unknown flat words by F , then the joint probability Pr(T , S) according to our model istheir generative probabilities are computed also dif Pr(T , S) = n Pr(c) n Pr(w) n Pr(f ) (10) ferently.</S>
			<S sid ="147" ssid = "33">Generation of flat words seen in training is c∈C w∈W f ∈F In practice, the logarithm of this probability can be calculated instead to avoid numerical difficulties.</S>
	</SECTION>
	<SECTION title="The Parsing Algorithm. " number = "5">
			<S sid ="148" ssid = "1">To find the parse tree with highest probability we use a chart parser adapted from Collins (1999).</S>
			<S sid ="149" ssid = "2">Two key changes must be made to the search process, though.</S>
			<S sid ="150" ssid = "3">Firstly, because we are proposing a new paradigm for Chinese word segmentation, the input to the parser must be raw sentences by definition.</S>
			<S sid ="151" ssid = "4">Hence to use the bottom-up parser, we need a lexicon of all characters together with what roles they can play in a flat word.</S>
			<S sid ="152" ssid = "5">We can get this lexicon from the treebank.</S>
			<S sid ="153" ssid = "6">For example, from the word 撥愊/NNf ‘center’, we can extract a role bNNf for character 撥 ‘middle’ and a role eNNf for character 愊 ‘center’.</S>
			<S sid ="154" ssid = "7">The role bNNf means the beginning of the flat label NNf, while eNNf stands for the end of the label NNf.</S>
			<S sid ="155" ssid = "8">This scheme was first proposed by Luo (2003) in his character-based Chinese parser, and we find it quite adequate for our purpose here.</S>
			<S sid ="156" ssid = "9">Secondly, in the bottom-up parser for head driven models, whenever a new edge is found, we must assign it a probability and a head word.</S>
			<S sid ="157" ssid = "10">If the newly discovered constituent is a flat word (its label ends with ‘f’), then we set its head word to be the concatenation of all its child characters, i.e. the word itself.</S>
			<S sid ="158" ssid = "11">If it is an unknown word, we use (7) to assign the probability, otherwise its probability is set to be 1.</S>
			<S sid ="159" ssid = "12">On the other hand, if the new edge is a phrase or.</S>
			<S sid ="160" ssid = "13">word with internal structures, the probability is set according to (2), while the head word is found with the appropriate head rules.</S>
			<S sid ="161" ssid = "14">In this bottom-up way, the probability for a complete parse tree is known as soon as it is completed.</S>
			<S sid ="162" ssid = "15">This probability includes both word generation probabilities and constituent probabilities.</S>
	</SECTION>
	<SECTION title="Evaluation. " number = "6">
			<S sid ="163" ssid = "1">For several reasons, it is a little tricky to evaluate the accuracy of our model for integrated morphological and syntactic parsing.</S>
			<S sid ="164" ssid = "2">First and foremost, we currently know of no other same effort in parsing the structures of Chinese words, and we have to annotate word structures by ourselves.</S>
			<S sid ="165" ssid = "3">Hence there is no baseline performance to compare with.</S>
			<S sid ="166" ssid = "4">Secondly, simply reporting the accuracy of labeled precision and recall is not very informative because our parser takes raw sentences as input, and its output includes a lot of easy cases like word segmentation and part- of-speech tagging results.</S>
			<S sid ="167" ssid = "5">Despite these difficulties, we note that higher- level constituent parsing results are still somewhat comparable with previous performance in parsing Penn Chinese Treebank, because constituent parsing does not involve word structures directly.</S>
			<S sid ="168" ssid = "6">Having said that, it must be pointed out that the comparison is meaningful only in a limited sense, as in previous literatures on Chinese parsing, the input is always word segmented or even part-of-speech tagged.</S>
			<S sid ="169" ssid = "7">That is, the bracketing in our case is around characters instead of words.</S>
			<S sid ="170" ssid = "8">Another observation is we can still evaluate Chinese word segmentation and part- of-speech tagging accuracy, by reading off the corresponding result from parse trees.</S>
			<S sid ="171" ssid = "9">Again because we split the words with internal structures into their components, comparison with other systems should be viewed with that in mind.</S>
			<S sid ="172" ssid = "10">Based on these discussions, we divide the labels of all constituents into three categories: Phrase labels are the labels in Peen Chinese Tree- bank for nonterminal phrase structures, including NP, VP, PP, etc. POS labels represent part-of-speech tags such as NN, VV, DEG, etc. Flat labels are generated in our annotation for words with no interesting structures.</S>
			<S sid ="173" ssid = "11">Recall that they always end with an ‘f’ such as NNf, VVf and DEGf, etc. With this classification, we report our parser’s accuracy for phrase labels, which is approximately the accuracy of constituent parsing of Penn Chinese Treebank.</S>
			<S sid ="174" ssid = "12">We report our parser’s word segmentation accuracy based on the flat labels.</S>
			<S sid ="175" ssid = "13">This accuracy is in fact the joint accuracy of segmentation and part-of-speech tagging.</S>
			<S sid ="176" ssid = "14">Most importantly, we can report our parser’s accuracy in recovering word structures based on POS labels and flat labels, since word structures may contain only these two kinds of labels.</S>
			<S sid ="177" ssid = "15">With the standard split of CTB 5.0 data into training, development and test sets (Zhang and Clark, 2009), the result are summarized in Table 1.</S>
			<S sid ="178" ssid = "16">For all label categories, the PARSEEVAL measures (Abney et al., 1991) are used in computing the labeled pre correctly analyzed it into a root 㦌斊 ‘masterpiece’ and a suffix 䕛 ‘expert’, as in Figure 10(b).</S>
			<S sid ="179" ssid = "17">This cision and recall.</S>
			<S sid ="180" ssid = "18">(a) NP ✱✱❧❧ (b) NN ✚✚❩❩ JJ JJf 㦌 NN NNf 斊䕛 NNf 㦌斊 NNf 䕛 Table 1: Labeled precision and recall for the three types of labels.</S>
			<S sid ="181" ssid = "19">The line labeled ‘Flat*’ is for unlabeled metrics of flat words, which is effectively the ordinary word segmentation accuracy.</S>
			<S sid ="182" ssid = "20">Though not directly comparable, we can make some remarks to the accuracy of our model.</S>
			<S sid ="183" ssid = "21">For constituent parsing, the best result on CTB 5.0 is reported to be 78% F1 measure for unlimited sentences with automatically assigned POS tags (Zhang and Clark, 2009).</S>
			<S sid ="184" ssid = "22">Our result for phrase labels is close to this accuracy.</S>
			<S sid ="185" ssid = "23">Besides, the result for flat labels compares favorably with the state of the art accuracy of about 93% F1 for joint word segmentation and part-of-speech tagging (Jiang et al., 2008; Kruengkrai et al., 2009).</S>
			<S sid ="186" ssid = "24">For ordinary word segmentation, the best result is reported to be around 97% F1 on CTB 5.0 (Kruengkrai et al., 2009), while our parser performs at 97.3%, though we should remember that the result concerns flat words only.</S>
			<S sid ="187" ssid = "25">Finally, we see the performance of word structure recovery is almost as good as the recognition of flat words.</S>
			<S sid ="188" ssid = "26">This means that parsing word structures accurately is possible with a generative model.</S>
			<S sid ="189" ssid = "27">It is interesting to see how well the parser does in recognizing the structure of words that were not seen during training.</S>
			<S sid ="190" ssid = "28">For this, we sampled 100 such words including those with prefixes or suffixes and personal names.</S>
			<S sid ="191" ssid = "29">We found that for 82 of these words, our parser can correctly recognize their structures.</S>
			<S sid ="192" ssid = "30">This means our model has learnt something that generalizes well to unseen words.</S>
			<S sid ="193" ssid = "31">In error analysis, we found that the parser tends to over generalize for prefix and suffix characters.</S>
			<S sid ="194" ssid = "32">For example, 㦌斊䕛 ‘great writer’ is a noun phrase consisting of an adjective 㦌 ‘great’ and a noun 斊䕛‘writer’, as shown in Figure 10(a), but our parser in Figure 10: Example of parser error.</S>
			<S sid ="195" ssid = "33">Tree (a) is correct, and (b) is the wrong result by our parser.</S>
			<S sid ="196" ssid = "34">is because the character 䕛 ‘expert’ is a very productive suffix, as in 䑺惆䕛 ‘chemist’ and 堉䘂䕛 ‘diplomat’.</S>
			<S sid ="197" ssid = "35">This observation is illuminating because most errors of our parser follow this pattern.</S>
			<S sid ="198" ssid = "36">Currently we don’t have any non-ad hoc way of preventing such kind of over generalization.</S>
	</SECTION>
	<SECTION title="Conclusion and Discussion. " number = "7">
			<S sid ="199" ssid = "1">In this paper we proposed a new paradigm for Chinese word segmentation in which not only flat words were identified but words with structures were also parsed.</S>
			<S sid ="200" ssid = "2">We gave good reasons why this should be done, and we presented an effective method showing how this could be done.</S>
			<S sid ="201" ssid = "3">With the progress in statistical parsing technology and the development of large scale treebanks, the time has now come for this paradigm shift to happen.</S>
			<S sid ="202" ssid = "4">We believe such a new paradigm for word segmentation is linguistically justified and pragmatically beneficial to real world applications.</S>
			<S sid ="203" ssid = "5">We showed that word structures can be recovered with high precision, though there’s still much room for improvement, especially for higher level constituent parsing.</S>
			<S sid ="204" ssid = "6">Our model is generative, but discriminative models such as maximum entropy technique (Berger et al., 1996) can be used in parsing word structures too.</S>
			<S sid ="205" ssid = "7">Many parsers using these techniques have been proved to be quite successful (Luo, 2003; Fung et al., 2004; Wang et al., 2006).</S>
			<S sid ="206" ssid = "8">Another possible direction is to combine generative models with discriminative reranking to enhance the accuracy (Collins and Koo, 2005; Charniak and Johnson, 2005).</S>
			<S sid ="207" ssid = "9">Finally, we must note that the use of flat labels such as “NNf” is less than ideal.</S>
			<S sid ="208" ssid = "10">The most impor tant reason these labels are used is we want to compare the performance of our parser with previous results in constituent parsing, part-of-speech tagging and word segmentation, as we did in Section 6.</S>
			<S sid ="209" ssid = "11">The problem with this approach is that word structures and phrase structures are then not treated in a truly unified way, and besides the 33 part-of-speech tags originally contained in Penn Chinese Treebank, another 33 tags ending with ‘f’ are introduced.</S>
			<S sid ="210" ssid = "12">We leave this problem open for now and plan to address it in future work.</S>
	</SECTION>
	<SECTION title="Acknowledgments">
			<S sid ="211" ssid = "13">I would like to thank Professor Maosong Sun for many helpful discussions on topics of Chinese morphological and syntactic analysis.</S>
			<S sid ="212" ssid = "14">The author is supported by NSFC under Grant No. 60873174.</S>
			<S sid ="213" ssid = "15">Heartfelt thanks also go to the reviewers for many pertinent comments which have greatly improved the presentation of this paper.</S>
	</SECTION>
</PAPER>
