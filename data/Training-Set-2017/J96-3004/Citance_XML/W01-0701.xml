<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">This paper presents a novel method that allows a machine learning algorithm following the transformation-based learning paradigm (Brill, 1995) to be applied to multiple classification tasks by training jointly and simultaneously on all fields.</S>
		<S sid ="2" ssid = "2">The motivation for constructing such a system stemsfrom the observation that many tasks in natural language processing are naturally composed of multiple subtasks which need to be resolved simultaneously; also tasks usually learned in isolation can possibly benefit from being learned in a joint framework, asthe signals for the extra tasks usually constitute inductive bias.The proposed algorithm is evaluated in two experiments: in one, the system is used to jointly predict the part-of-speech and text chunks/baseNP chunks of an English corpus; and in the second it is used tolearn the joint prediction of word segment boundaries and part-of-speech tagging for Chinese.</S>
		<S sid ="3" ssid = "3">Theresults show that the simultaneous learning of multiple tasks does achieve an improvement in each taskupon training the same tasks sequentially.</S>
		<S sid ="4" ssid = "4">The part- of-speech tagging result of 96.63% is state-of-the-art for individual systems on the particular train/test split.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="5" ssid = "5">Transformation-based learning (TBL) (Brill, 1995) is one of the most successful rule-based machine learning algorithms.</S>
			<S sid ="6" ssid = "6">It is a flexible and powerful method which is easily extended to various tasks and domains, and it has been applied to a wide variety of NLP tasks, including part of speech tagging (Brill, 1995), parsing (Brill, 1996) and phrase chunking (Ramshaw and Marcus, 1994; Florian et al., 2000).</S>
			<S sid ="7" ssid = "7">It achieves state-of-the-art performance on several tasks, and has been shown to be fairly resistant to overtraining (Ramshaw and Marcus, 1994).</S>
			<S sid ="8" ssid = "8">The processing of natural language text is usually done through a pipeline of well defined tasks, each extracting specific information.</S>
			<S sid ="9" ssid = "9">For instance, one possible sequence of actions performed could be: 1.</S>
			<S sid ="10" ssid = "10">Tokenize the text;.</S>
	</SECTION>
	<SECTION title="Associate part-of-speech tags with each word;. " number = "2">
			<S sid ="11" ssid = "1">4.</S>
			<S sid ="12" ssid = "2">Identify and label named entities;.</S>
			<S sid ="13" ssid = "3">5.</S>
			<S sid ="14" ssid = "4">Resolve anaphora..</S>
			<S sid ="15" ssid = "5">In the above scenario, each task is well-defined in itself and is often performed independently and in a specific order.</S>
			<S sid ="16" ssid = "6">There are NLP tasks, however, which consist of closely-related sub-tasks, where the orderand independence is hard to determine for ex ample, the task of part-of-speech (POS) tagging in highly inflective languages such as Czech.</S>
			<S sid ="17" ssid = "7">A POS tag in Czech consists of several sub-tags, includingthe main part-of-speech (e.g. noun, verb), a de tailed part-of-speech (e.g. past tense verb, genitive noun, etc), gender, case, number and some other11 sub-tags.</S>
			<S sid ="18" ssid = "8">Allowing a system to learn the sub tasks jointly is beneficial in this case, as it eliminates the need to define a learning order, and it allows the true dependencies between the sub-tasks to be modeled, while not imposing artificial dependencies among them.The multi-task classification approach we are pre senting in this paper is very similar to the one proposed by Caruana et al.</S>
			<S sid ="19" ssid = "9">(1997).</S>
			<S sid ="20" ssid = "10">Instead of using neural network learning, we are modifyingthe transformation-based learning to able to perform multiple-task classification.</S>
			<S sid ="21" ssid = "11">The new frame work is tested by performing joint POS tagging and base noun-phrase (baseNP) chunking on the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993), and simultaneous word segmentation and POS tagging on the Chinese Treebank&amp;apos;s (Xia et al., 2000).</S>
			<S sid ="22" ssid = "12">In both experiments the jointly trained system outperforms the sequentially-trained system combination.The remainder of the paper is organized as fol lows: Section 2 briefly presents previous approaches to multi-task classification; Section 3 describes thegeneral TBL framework and the proposed modifica tions to it; Section 4 describes the experiments and the results; Section 5 does a qualitative analysis of the behavior of the system, and Section 6 concludes the paper.</S>
			<S sid ="23" ssid = "13">2 Previous Work.</S>
			<S sid ="24" ssid = "14">Multitask Learning(Caruana et al., 1997) analyzes in depth the multitask learning (MTL) paradigm, where individual related tasks are trained together by sharing a com mon representation of knowledge, and shows thatsuch a strategy obtains better results than a single task learning strategy.</S>
			<S sid ="25" ssid = "15">The algorithm of choice there is a backpropagation neural network, and the paradigm is tested on several machine learningtasks, including 1DALVINN (road-following prob lem), 1D-DOORS (locate doorknobs to recognizedoor types) and pneumonia prediction.</S>
			<S sid ="26" ssid = "16">It also con tains an excellent discussion on how and why the MTL paradigm is superior to single-task learning.</S>
			<S sid ="27" ssid = "17">Combining ClassificationsA straightforward way of addressing a multiple clas sification problem is to create a new label for eachobserved combination of the original sub-tags; Ha jic and Hladka (1998) describes such an approach for performing POS tagging in Czech.</S>
			<S sid ="28" ssid = "18">While it has the advantage of not modifying the structure of the original algorithm, it does have some drawbacks:• By increasing the range of possible classifica tions, each individual tag label will have fewer samples associated with it, resulting in datasparseness.</S>
			<S sid ="29" ssid = "19">For example, in Czech, &amp;amp;quot;glueing&amp;amp;quot; to gether the subtags results in 1291 part-of-speechtags, a considerably larger number than in num ber of POS tags in English 55.</S>
			<S sid ="30" ssid = "20">It is arguable that one would need one order or magnitude more Czech data than English data to estimate similarly well the same model parameters.</S>
			<S sid ="31" ssid = "21">• No new class labels will be generated, even if it should be possible to assign a label consisting of sub-parts that were observed in the training set, but whose combination wasn&amp;apos;t actually seen.</S>
			<S sid ="32" ssid = "22">N-Best Rescoring Another trend in a 2-task classification is to use asingle-task classifier for the first task to output n best lists and then use a classifier trained on the joint tasks to select the best candidate that maximizes the joint likelihood.</S>
			<S sid ="33" ssid = "23">Xun et al.</S>
			<S sid ="34" ssid = "24">(2000) performs a jointPOS tagging / baseNP classification by using a sta tistical POS tagger to generate n-best lists of POS tags, and then a Viterbi algorithm to determine thebest candidate that maximizes the joint probabil ity of POS tag/ baseNP chunk.</S>
			<S sid ="35" ssid = "25">Chang and Chen(1993) uses a similar technique to perform word segmentation and POS tagging in Chinese texts.</S>
			<S sid ="36" ssid = "26">Inboth approaches, the joint search obtains better re sults than the ones obtained when the search was performed independently.</S>
	</SECTION>
	<SECTION title="Multi-task Training with. " number = "3">
			<S sid ="37" ssid = "1">Transformation-Based Learning The multidimensional training method presented in this paper learns multiple related tasks in parallel, by using the domain specific signals present in eachtraining stream.</S>
			<S sid ="38" ssid = "2">The tasks share a common repre sentation, and rules are allowed to correct any of the errors present in the streams, without imposing ordering restrictions on the type of the individualerrors (i.e. learn POS tagging before baseNP chunk ing).Transformation-based learning (TBL) is well suited to perform in such a framework: • Partial classifications are easily accommodated in the TBL paradigm, as features of the samples (e.g. word, gender, number); • The system can learn rules that correct onesub-classification, then use the corrected sub classification to correct the other classifications, in a seemingly interspersed fashion, as dictated by the data; • The objective function used in TBL usually isthe evaluation measure of the task (e.g. number of errors, F-Measure).</S>
			<S sid ="39" ssid = "3">This allows the al gorithm to work directly toward optimizing its evaluation function.&amp;apos; 3.1 The Standard TBL Algorithm.</S>
			<S sid ="40" ssid = "4">The central idea behind transformation-based learning is to induce an ordered list of rules which progres sively improve upon the current state of the training set.</S>
			<S sid ="41" ssid = "5">An initial assignment is made based on simplestatistics, and rules are then greedily learned to cor rect the existing mistakes until no net improvement can be made.</S>
			<S sid ="42" ssid = "6">The use of a transformation-based system assumes the existence of the following: • An initial state generator;• A set of allowable transform types, or tem plates; • An objective function for learning typically the evaluation function.</S>
			<S sid ="43" ssid = "7">Before learning begins, the training corpus is passed through the initial state generator which assigns to each instance some initial classification.</S>
			<S sid ="44" ssid = "8">The learner then iteratively learns an ordered sequence of rules: 1.</S>
			<S sid ="45" ssid = "9">For each possible transformation, or rule, r that.</S>
			<S sid ="46" ssid = "10">can be applied to the corpus: This distinguishes TBL as a error-driven approach from other feature-based methods such as maximum entropy which adjust parameters to maximize the likelihood of the data; the latter may not be perfectly correlated with the classifier performance.</S>
			<S sid ="47" ssid = "11">(a) Apply the rule to a copy of the current state of the corpus,(b) Score the resulting corpus with the objec tive function; compute the score associated with the applied rule (f (r)) (usually the difference in performance) 2.</S>
			<S sid ="48" ssid = "12">If no rule with a positive objective function.</S>
			<S sid ="49" ssid = "13">score exists, learning is finished.</S>
			<S sid ="50" ssid = "14">Stop.</S>
			<S sid ="51" ssid = "15">3.</S>
			<S sid ="52" ssid = "16">Select the rule with the best score; append it to.</S>
			<S sid ="53" ssid = "17">the list of learned rules; 5.</S>
			<S sid ="54" ssid = "18">Repeat from Step 1..</S>
			<S sid ="55" ssid = "19">At evaluation time, the test data is first initialized by passing it through the initial state generator.</S>
			<S sid ="56" ssid = "20">The rules are then applied to the data in the order in which they were learned.</S>
			<S sid ="57" ssid = "21">The final state of the test data after all the rules have been applied constitutes the output of the system.</S>
			<S sid ="58" ssid = "22">3.2 Multi-task Rule Evaluation Function.</S>
			<S sid ="59" ssid = "23">The algorithm for the multidimensional transformation-based learner (mTBL) can be derived easily from the standard algorithm.</S>
			<S sid ="60" ssid = "24">The only change needed is modifying the objective function to take into account the current state of all the subtags (the classifications of the various sub-tasks): f (r) = 8 sample i=1 where • r is a rule • r (s) is the result of applying rule r to sample s • n is the number of tasks• Si (s) is the score on sub-classification i of sam ple s (1 if is correct and 0 otherwise).</S>
			<S sid ="61" ssid = "25">• wi represent weights that can be assigned to tasks to impose priorities for specific sub-tasks.</S>
			<S sid ="62" ssid = "26">In the experiments, all the weights were set to 1.</S>
	</SECTION>
	<SECTION title="Experiments. " number = "4">
	</SECTION>
</PAPER>
