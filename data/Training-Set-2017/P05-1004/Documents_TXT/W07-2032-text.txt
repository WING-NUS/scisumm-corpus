Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval -2007), pages 157?160, Prague, June 2007. c ? 2007 Association for Computational Linguistics GPLSI: Word Coarse-grained Disambiguation aided by Basic Level Concepts? Rube? n Izquierdo Armando Sua? rez GPLSI Group, DLSI University of Alicante Spain {ruben, armando}@dlsi.ua.es German Rigau IXA NLP Group EHU/UPV Donostia, Basque Country german.rigau@ehu.es Abstract We present a corpus-based supervised lear.ning system for coarse-grained sense disam.biguation. In addition to usual features for training in word sense disambiguation, our system also uses Base Level Concepts au.tomatically obtained from WordNet. Base Level Concepts are some synsets that gene.ralize a hyponymy sub?hierarchy, and pro.vides an extra level of abstraction as well as relevant information about the context of a word to be disambiguated. Our experiments proved that using this type of features re.sults on a significant improvement of preci.sion. Our system has achieved almost 0.8 F1 (fifth place) in the coarse? grained English all-words task using a very simple set of fea.tures plus Base Level Concepts annotation. 1 Introduction The GPLSI system in SemEval? s task 7, coarse? grained English all-words, consists of a corpus.based supervised-learning method which uses lo.cal context information. The system uses Base Le.vel Concepts (BLC) (Rosch, 1977) as features. In short, BLC are synsets of WordNet (WN) (Fell.baum, 1998) that are representative of a certain hy.ponymy sub?hierarchy. The synsets that are se.lected to be BLC must accomplish certain condi.tions that will be explained in next section. BLC ?This paper has been supported by the European Union un.der the project QALL-ME (FP6 IST-033860) and the Spanish Government under the project Text-Mess (TIN2006-15265.C06-01) and KNOW (TIN2006-15049-C03-01) are slightly different from Base Concepts of Eu.roWordNet1 (EWN) (Vossen et al, 1998), Balkanet2 or Meaning Project3 because of the selection crite.ria but also because our method is capable to define them automatically. This type of features helps our system to achieve 0.79550 F1 (over the First?Sense baseline, 0.78889) while only four systems outper.formed ours being the F1 of the best one 0.83208. WordNet has been widely criticised for being a sense repository that often offers too fine?grained sense distinctions for higher level applications like Machine Translation or Question & Answering. In fact, WSD at this level of granularity, has resisted all attempts of inferring robust broad-coverage mo.dels. It seems that many word? sense distinctions are too subtle to be captured by automatic systems with the current small volumes of word? sense annotated examples. Possibly, building class -based classifiers would allow to avoid the data sparseness problem of the word-based approach. Thus, some research has been focused on deri.ving different sense groupings to overcome the fine? grained distinctions of WN (Hearst and Schu? tze, 1993) (Peters et al, 1998) (Mihalcea and Moldo.van, 2001) (Agirre et al, 2003) and on using predefi.ned sets of sense-groupings for learning class -based classifiers for WSD (Segond et al, 1997) (Ciaramita and Johnson, 2003) (Villarejo et al, 2005) (Curran, 2005) (Ciaramita and Altun, 2006). However, most of the later approaches used the original Lexico.graphical Files of WN (more recently called Super.1http://www.illc.uva.nl/EuroWordNet/ 2http://www.ceid.upatras.gr/Balkanet 3http://www.lsi.upc.es/ nlp/meaning 157 senses) as very coarse? grained sense distinctions. However, not so much attention has been paid on learning class -based classifiers from other available sense? groupings such as WordNet Domains (Mag.nini and Cavaglia, 2000), SUMO labels (Niles and Pease, 2001), EuroWordNet Base Concepts or Top Concept Ontology labels (Atserias et al, 2004). Ob.viously, these resources relate senses at some level of abstraction using different semantic criteria and properties that could be of interest for WSD. Pos .sibly, their combination could improve the overall results since they offer different semantic perspecti.ves of the data. Furthermore, to our knowledge, to date no comparative evaluation have been performed exploring different sense? groupings. This paper is organized as follows. In section 2, we present a method for deriving fully automatica.lly a number of Base Level Concepts from any WN version. Section 3 shows the details of the whole system and finally, in section 4 some concluding re.marks are provided. 2 Automatic Selection of Base Level Concepts The notion of Base Concepts (hereinafter BC) was introduced in EWN. The BC are supposed to be the concepts that play the most important role in the va.rious wordnets4 (Fellbaum, 1998) of different lan.guages. This role was measured in terms of two main criteria: ? A high position in the semantic hierarchy; ? Having many relations to other concepts; Thus, the BC are the fundamental building blocks for establishing the relations in a wordnet and give information about the dominant lexicalization pat.terns in languages. BC are generalizations of featu.res or semantic components and thus apply to a ma.ximum number of concepts. Thus, the Lexicografic Files (or Supersenses) of WN could be considered the most basic set of BC. Basic Level Concepts (Rosch, 1977) should not be confused with Base Concepts. BLC are the result of a compromise between two conflicting principles of characterization: 4http://wordnet.princeton.edu #rel. synset 18 group 1,grouping 1 19 social group 1 37 organisation 2,organization 1 10 establishment 2,institution 1 12 faith 3,religion 2 5 Christianity 2,church 1,Christian church 1 #rel. synset 14 entity 1,something 1 29 object 1,physical object 1 39 artifact 1,artefact 1 63 construction 3,structure 1 79 building 1,edifice 1 11 place of worship 1, ... 19 church 2,church building 1 #rel. synset 20 act 2,human action 1,human activity 1 69 activity 1 5 ceremony 3 11 religious ceremony 1,religious ritual 1 7 service 3,religious service 1,divine service 1 1 church 3,church service 1 Table 1: Possible Base Level Concepts for the noun Church ? Represent as many concepts as possible; ? Represent as many features as possible; As a result of this, Basic Level Concepts typically occur in the middle of hierarchies and less than the maximum number of relations. BC mostly involve the first principle of the Basic Level Concepts only. Our work focuses on devising simple methods for selecting automatically an accurate set of Basic Le.vel Concepts from WN. In particular, our method se.lects the appropriate BLC of a particular synset con .sidering the relative number of relations encoded in WN of their hypernyms. The process follows a bottom-up approach using the chain of hypernym relations. For each synset in WN, the process selects as its Base Level Con.cept the first local maximum according to the rela.tive number of relations. For synsets having multi.ple hypernyms, the path having the local maximum with higher number of relations is selected. Usually, this process finishes having a number of ? fake? Base Level Concepts. That is, synsets having no descen.dants (or with a very small number) but being the first local maximum according to the number of re.lations considered. Thus, the process finishes che.cking if the number of concepts subsumed by the 158 Senses BLC SuperSenses Nouns 4.92 4.10 3.01 Verbs 11.00 8.67 1.03 Nouns + Verbs 7.66 6.16 3.47 Table 2: Polysemy degree over SensEval? 3 preliminary list of BLC is higher than a certain th.reshold. For those BLC not representing enough concepts according to a certain threshold, the pro.cess selects the next local maximum following the hypernym hierarchy. An example is provided in table 1. This table shows the possible BLC for the noun ?church? using WN1.6. The table presents the hypernym chain for each synset together with the number of relations en .coded in WN for the synset. The local maxima along the hypernym chain of each synset appears in bold. Table 2 presents the polysemy degree for nouns and verbs of the different words when grouping its senses with respect the different semantic classes on SensEval?3. Senses stand for the WN senses, BLC for the Automatic BLC derived using a threshold of 20 and SuperSenses for the Lexicographic Files of WN. 3 The GPLSI system The GPLSI system uses a publicly available imple.mentation of Support Vector Machines, SVMLight5 (Joachims, 2002), and Semcor as learning corpus. Semcor has been properly mapped and labelled with both BLC6 and sense-clusters. Actually, the process of training-classification has two phases: first, one classifier is trained for each possible BLC class and then the SemEval test data is classified and enriched with them, and second, a classifier for each target word is built using as addi.tional features the BLC tags in Semcor and SemE.val?s test. Then, the features used for training the classifiers are: lemmas, word forms, PoS tags7, BLC tags, and first sense class of target word (S1TW). All features 5http://svmlight.joachims.org/ 6Because BLC are automatically defined from WN, some tu.ning must be performed due to the nature of the task 7. We have not enough room to present the complete study but threshold 20 has been chosen, using SENSEVAL-3 English all-words as test data. Moreover, our tests showed roughly 5% of improvement against not using these features. 7TreeTagger (Schmid, 1994) was used were extracted from a window [? 3.. + 3] except for the last type (S1TW). The reason of using S1TW features is to assure the learning of the baseline. It is well known that Semcor presents a higher frequency on first senses (and it is also the baseline of the task finally provided by the organizers). Besides, these are the same features for both first and second phases (obviously except for S1TW be.cause of the different target set of classes). Nevert.heless, the training in both cases are quite different: the first phase is class -based while the second is word-based. By word-based we mean that the lear.ning is performed using just the examples in Semcor that contains the target word. We obtain one classi.fier per polysemous word are in the SemEval test corpus. The output of these classifiers is a sense.cluster. In class -based learning all the examples in Semcor are used, tagging those ones belonging to a specific class (BLC in our case) as positive exam.ples while the rest are tagged as negatives. We ob.tain so many binary classifiers as BLC are in Se.mEval test corpus. The output of these classifiers is true or false, ?the example belongs to a class ? or not. When dealing with a concrete target word, only those BLC classifiers that are related to it are ?activated? (i.e, ? animal? classifier will be not used to classify ? church? ), ensuring that the word will be tagged with coherent labels. In order to avoid statis.tical bias because of very large set of negative exam.ples, the features are defined from positive examples only (although they are obviously used to characte.rize all the examples). 4 Conclusions and further work The WSD task seems to have reached its maxi.mum accuracy figures with the usual framework. Some of its limitations could come from the sense? granularity of WN. In particular, SemEval? s coarse.grained English all-words task represents a solution in this direction. Nevertheless, the task still remains oriented to words rather than classes. Then, other problems arise like data sparseness just because the lack of adequate and enough examples. Changing the set of classes could be a solution to enrich training corpora with many more examples Another option seems to be incorporating more semantic information. 159 
Base Level Concepts (BLC) are concepts that are representative for a set of other concepts. A simple method for automatically selecting BLC from WN based on the hypernym hierarchy and the number of stored relationships between synsets have been used to define features for training a supervised system. Although in our system BLC play a simple role aiding to the disambiguation just as additional fea.tures, the good results achieved with such simple features confirm us that an appropriate set of BLC will be a better semantic discriminator than senses or even sense-clusters. References E. Agirre, I. Aldezabal, and E. Pociello. 2003. A pi.lot study of english selectional preferences and their cross-lingual compatibility with basque. In Procee.dings of the International Conference on Text Speech and Dialogue (TSD? 2003), CeskBudojovice, Czech Republic. J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Carroll, B. Magnini, and P. Vossen. 2004. The meaning mul.tilingual central repository. In Proceedings of Global WordNet Conference (GWC? 04), Brno, Czech Repu.blic. M. Ciaramita and Y. Altun. 2006. Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger. In Proceedings of the Conference on Empirical Methods in Natural Lan.guage Processing (EMNLP? 06), pages 594?602, Syd.ney, Australia. ACL. M. Ciaramita and M. Johnson. 2003. Supersense tagging of unknown nouns in wordnet. In Proceedings of the Conference on Empirical methods in natural language processing (EMNLP? 03), pages 168?175. ACL. J. Curran. 2005. Supersense tagging of unknown nouns using semantic similarity. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL? 05), pages 26? 33. ACL. C. Fellbaum, editor. 1998. WordNet. An Electronic Lexi.cal Database. The MIT Press. M. Hearst and H. Schu? tze. 1993. Customizing a lexicon to better suit a computational task. In Proceedingns of the ACL SIGLEX Workshop on Lexical Acquisition, Stuttgart, Germany. Thorsten Joachims. 2002. Learning to Classify Text Using Support Vector Machines. Kluwer Academic Publishers. B. Magnini and G. Cavaglia. 2000. Integrating subject fields codes into wordnet. In Proceedings of the Se.cond International Conference on Language Resour.ces and Evaluation (LREC? 00). R. Mihalcea and D. Moldovan. 2001. Automatic ge .neration of coarse grained wordnet. In Proceding of the NAACL workshop on WordNet and Other Lexical Resources: Applications, Extensions and Customiza.tions, Pittsburg, USA. I. Niles and A. Pease. 2001. Towards a standard up.per ontology. In Proceedings of the 2nd International Conference on Formal Ontology in Information Sys.tems (FOIS -2001), pages 17?19. Chris Welty and Ba.rry Smith, eds. W. Peters, I. Peters, and P. Vossen. 1998. Automatic sense clustering in eurowordnet. In First Internatio.nal Conference on Language Resources and Evalua.tion (LREC?98), Granada, Spain. E. Rosch. 1977. Human categorisation. Studies in Cross-Cultural Psychology, I(1):1?49. Helmut Schmid. 1994. Probabilistic part-of-speech tag.ging using decision trees. In Proceedings of NemLap.94, pages 44? 49, Manchester, England. F. Segond, A. Schiller, G. Greffenstette, and J. Chanod. 1997. An experiment in semantic tagging using hid.den markov model tagging. In ACL Workshop on Au.tomatic Information Extraction and Building of Lexi.cal Semantic Resources for NLP Applications, pages 78?81. ACL, New Brunswick, New Jersey. L. Villarejo, L. Ma`rquez, and G. Rigau. 2005. Explo.ring the construction of semantic class classifiers for wsd. In Proceedings of the 21th Annual Meeting of Sociedad Espaola para el Procesamiento del Lenguaje Natural SEPLN? 05, pages 195? 202, Granada, Spain, September. ISSN 1136-5948. P. Vossen, L. Bloksma, H. Rodriguez, S. Climent, N. Cal.zolari, A. Roventini, F. Bertagna, A. Alonge, and W. Peters. 1998. The eurowordnet base concepts and top ontology. Technical report, Paris, France, France. 160 