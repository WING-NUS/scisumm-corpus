<PAPER>
	<ABSTRACT>
	</ABSTRACT>
	<SECTION title="The Named Entity task. " number = "1">
			<S sid ="1" ssid = "1">There is currently much interest, in both research and com­ mercial arenas, in natural language processing systems which can perform multilingual information extraction (IE), the task of automatically identifying the various aspects of a text that are of interest to specific users.</S>
			<S sid ="2" ssid = "2">An example of IE is the Named Entity (NE) task, which has become established as the important first step in many other IE tasks, provid­ ing information useful for coreference and template filling.</S>
			<S sid ="3" ssid = "3">Named Entity evaluation began as a part of recent Message Understanding Conferences (MUC), whose objective was to standardize the evaluation of IE tasks (Sundheim, 1995b).</S>
			<S sid ="4" ssid = "4">Several organized evaluations have been held to determine the state-of-the-art in NE systems, and there are commercial systems available.</S>
			<S sid ="5" ssid = "5">The goal of the NE task is to automatically identify the boundaries of a variety of phrases in a raw text, and then to categorize the phrases identified.</S>
			<S sid ="6" ssid = "6">There are three categories of named-entities defined by the guidelines: TIMEX, NUMEX, and ENAMEX.</S>
			<S sid ="7" ssid = "7">TIMEX phrases are temporal expressions, which are subdivided into date expressions ( April 7) and time expressions ( noon EST).</S>
			<S sid ="8" ssid = "8">NUMEX phrases are numeric expres­ sions, which are subdivided into percent expressions (3.2%) and money expressions ( $180 million).</S>
			<S sid ="9" ssid = "9">ENAMEX phrases are proper names, representing references in a text to persons ( Jeffre y H. Birnbaum), locations ( New York), and organiza­ tions ( Northwest Airlines ).</S>
			<S sid ="10" ssid = "10">Evaluation of system performance for the NE task is done using an automatic scoring program (Chinchor, 1995), with the scores based on two measures - recall and precision.</S>
			<S sid ="11" ssid = "11">Recall is the percent of the &quot;correct&quot; named-entities that the system identifies; precision is the percent of the phrases that the system identifies that are actually correct NE phrases.</S>
			<S sid ="12" ssid = "12">The component recall and precision scores are then used to calculate a balanced F-measure (Rijsbergen, 1979), where F = 2PR/ ( P + R).</S>
			<S sid ="13" ssid = "13">Human performance on the NE task has been determined to be quite high, with F-measures better than 96% (Sund­ heim, 1995b).</S>
			<S sid ="14" ssid = "14">Despite the fact that some systems in recent evaluations have performance approaching this human perfor­ mance, it is important to note that named-entity recognition is by no means a &quot;solved problem.&quot;</S>
			<S sid ="15" ssid = "15">The fact that existing sys­ tems perform extremely well on mixed case English newswire corpora is certainly related to the years of research (and or­ ganized evaluations) on this specific task in this language.</S>
			<S sid ="16" ssid = "16">Although performance by MUC6 and MET systems is en­ couraging, it is not clear what resources are required to adapt systems to new languages.</S>
			<S sid ="17" ssid = "17">It is also unknown how the exist­ ing high scoring systems would perform on less well-behaved texts, such as single-case texts, non-newswire texts, or texts obtained via optical character recognition (OCR).</S>
			<S sid ="18" ssid = "18">There has been little discussion of the linguistic signifi­ cance of performing NE recognition, or of how much linguistic knowledge is required to perform well on such an evaluation.</S>
			<S sid ="19" ssid = "19">However, any given language task should be examined care­ fully to establish a baseline of performance which should be attainable by any system; only then can we adequately de­ termine the significance of the results reported on that task.</S>
			<S sid ="20" ssid = "20">In this paper we give the results of an analysis of NE cor­ pora in six languages from the point of view of a system with no knowledge of the languages; that is, we performed an analysis based purely on the strings of characters composing the texts and the named-entity phrases.</S>
			<S sid ="21" ssid = "21">The performance of such a straw-man system, which did not use language-specific lexicons or word lists or even information about tokeniza­ tion/segmentation or part-of speech, can serve as a baseline score for comparison of more sophisticated systems.</S>
	</SECTION>
	<SECTION title="The Corpora. " number = "2">
			<S sid ="22" ssid = "1">The definition of the NE task we discuss in this paper was taken from the guidelines for the Sixth Message Understand­ ing Conferences (MUC6) (Sundheim, 1995a) and the recent Multilingual Entity Task (MET, May 1996), both sponsored by the TIPSTER program.</S>
			<S sid ="23" ssid = "2">MUC6 evaluated English NE systems, and MET evaluated Spanish, Japanese, and Chinese NE systems.</S>
			<S sid ="24" ssid = "3">The Spanish, Japanese, and Chinese corpora we analyzed each consisted of the MET training documents; sim­ ilarly, the English corpus contains 60 Wall Street Journal ar­ ticles prepared for the MUC6 dry-run and official evaluation.</S>
			<S sid ="25" ssid = "4">In addition to the four corpora available from the recent orga­ nized NE evaluations, we analyzed similar-sized French and 190 Portuguese corpora1 which were prepared according to the MET guidelines.</S>
			<S sid ="26" ssid = "5">Table 1 shows the sources for the corpora.</S>
			<S sid ="27" ssid = "6">I language, as well as a breakdown of total phrases into the three individual categories.</S>
			<S sid ="28" ssid = "7">Table 1: Corpora sources.</S>
			<S sid ="29" ssid = "8">All six corpora consisted of a collection of newswire articles, and none of the articles in any language was a translation of an article in another language.</S>
			<S sid ="30" ssid = "9">There were important differ­ ences in the makeup of these individual corpora that affected this analysis.</S>
			<S sid ="31" ssid = "10">The French corpus, for example, contained a wide range of articles from a single issue of Le Monde, so the topics of the articles ranged from world politics to the Paris fashion scene.</S>
			<S sid ="32" ssid = "11">The articles in the English and Spanish corpora were specifically selected (by the MUC6 and MET evaluation organizers) because they contained references to press conferences.</S>
			<S sid ="33" ssid = "12">While the content was more homogeneous in the English corpus, the articles were nevertheless drawn from a range of several months of the Wall Street Journal, so the specific topics (and constituent Named Entities) were very diverse.</S>
			<S sid ="34" ssid = "13">The Chinese Xinhua corpus was, in contrast, ex­ tremely homogeneous.</S>
			<S sid ="35" ssid = "14">These differences demonstrate a num­ ber of difficulties presented by corpora in different languages.</S>
			<S sid ="36" ssid = "15">In order to estimate the complexity of the NE task, we first determined the vocabulary size of the corpora involved (i.e. &quot;count the words&quot;), in terms of individual lexemes of the lan­ guage.</S>
			<S sid ="37" ssid = "16">For our analysis of the European-language corpora, we considered a token to be any sequence of characters de­ limited by white space, and we ignored the case of all letters.</S>
			<S sid ="38" ssid = "17">The Japanese corpus was segmented using NEWJUMAN, the Chinese corpus with a segmenter made available by New Mex­ ico State University.</S>
			<S sid ="39" ssid = "18">This segmentation information was used only to estimate the corpora sizes and was not used in any of the other portions of our analysis.</S>
			<S sid ="40" ssid = "19">Language Lexeme Tokens Lexeme Types Token/ Type Chinese 34782 4584 7.6 English 24797 5764 4.3 French 35997 8691 4.1 Japanese 21484 3655 5.9 Portuguese 42621 7756 5.5 Spanish 31991 7850 4.1 Table 2: Corpora size by lexeme.</S>
			<S sid ="41" ssid = "20">Table 3 shows the total number of NE phrases for each 1The French corpus was prepared by Marc Vilain; the Por­ tuguese corpus was prepared by Sasha Caskey.</S>
			<S sid ="42" ssid = "21">Table 3: NE phrases, by subcategory.</S>
			<S sid ="43" ssid = "22">2.1 NUMEX and TIMEX phrases.</S>
			<S sid ="44" ssid = "23">From Table 3 we see that TIMEX and NUMEX phrases to­ gether composed only 2030% of all NE phrases in each lan­ guage.</S>
			<S sid ="45" ssid = "24">Furthermore, these phrases were the easiest to recog­ nize, because they could be represented by very few simple patterns.</S>
			<S sid ="46" ssid = "25">Upon inspection of the corpora, for example, we were able to represent nearly all NUMEX phrases in each of the six corpora with just 5 patterns.2 Similarly, given a simple list of the basic temporal phrase words for a lan­ guage (months, days of the week, seasons, etc.), it was possi­ ble to construct a series of patterns to represent most of the TIMEX phrases.</S>
			<S sid ="47" ssid = "26">3 We were able to represent at least 95% of all TIMEX in each language in similar ways with just a few patterns (less than 30 per language), constructed in a few hours.</S>
			<S sid ="48" ssid = "27">Since we found most NUMEX and TIMEX phrases to be easy to recognize, we therefore restricted our further analysis of the corpora to ENAMEX phrases, which proved to be significantly more complex.</S>
			<S sid ="49" ssid = "28">2.2 ENAMEX phrases.</S>
			<S sid ="50" ssid = "29">Table 4 shows the numbers of ENAMEX phrases tokens con­ tained by the six corpora.</S>
			<S sid ="51" ssid = "30">The average occurrence of each token in each language was quite low (much lower than the av­ erage occurrence of each lexeme), which indicated that many phrases occurred very infrequently in the corpus.</S>
			<S sid ="52" ssid = "31">ou ld ypes . OC A­ d be 2 An example of a NUMEX pattern representing a Spanish PERCENT would be a sequence of digits followed by either the percent sign (%) or the words &quot;por ciento&quot;.</S>
			<S sid ="53" ssid = "32">3An example of a NUMEX pattern representing a Spanish DATE would be the name of a month (or its abbreviation) followed by a sequence of digits (the day), optionally followed by a comma and another sequence of digits (the year).</S>
			<S sid ="54" ssid = "33">accounted for by the three common Chinese words for China.</S>
			<S sid ="55" ssid = "34">Figure 1 shows a graph of the cumulative percentage of all phrases of the corresponding category represented by the x most frequently-occurring phrases of that type in the given language.</S>
			<S sid ="56" ssid = "35">Figure 1: Graph of the cumulative % of phrase tokens provided by % of phrase types.</S>
			<S sid ="57" ssid = "36">The graph shows a similar shape for all subcategories of ENAMEX phrases in all the languages investigated, although the rate of increase varies slightly.</S>
			<S sid ="58" ssid = "37">It is clear from the classic Zipfian distribution (d.</S>
			<S sid ="59" ssid = "38">(Zipf, 1932; Zipf, 1949}) shown by the graph that a significant percentage of the ENAMEX phrase tokens could be represented by a small amount of frequently­ occurring phrase types.</S>
			<S sid ="60" ssid = "39">However, Zipf &apos;s law also tells us that a nontrivial percentage of the phrases (those in the tail of the graph) are very infrequent, most likely never occurring in any amount of training data.</S>
			<S sid ="61" ssid = "40">Unlike the distribution of the overall NE phrases, the rel­ ative proportion of constituent ENAMEX phrase subcate­ gories (PERSON, LOCATION, and ORGANIZATION) var­ ied greatly by language.</S>
			<S sid ="62" ssid = "41">The breakdown by ENAMEX phrase subcategory is shown in Table 5.</S>
			<S sid ="63" ssid = "42">Since high performance on training texts is meaningless if a system performs poorly on new, unseen texts, we estimated the performance of a simple memorization algorithm on un­ seen data.</S>
			<S sid ="64" ssid = "43">For our simple system, the answer to the question depended on the vocabulary transfer rate of the corpus, the percentage of phrases occurring in the training corpus which also occurred in the test corpus.</S>
			<S sid ="65" ssid = "44">To measure the vocab­ ulary transfer rate for the six corpora, we randomly divided each corpus into a training set and a test set, with each test set containing about 450 ENAMEX phrases, and each train­ ing set containing all remaining phrases.</S>
			<S sid ="66" ssid = "45">We then examined the ENAMEX phrases in the training set to determine how many also occurred in the test set.</S>
			<S sid ="67" ssid = "46">The results of this experiment showed that, to a certain extent, a word list built from the training set provided rea­ sonable performance.</S>
			<S sid ="68" ssid = "47">Just as some frequent phrase types comprised a large percentage of the phrase tokens within a corpus, a small number of phrase types from the training set accounted for many tokens in the test set.</S>
			<S sid ="69" ssid = "48">As shown by the transfer curve for the six languages in Figure 2, the transfer rate varied dramatically depending on the language, but the graph has the same shape for each, even though the six cor­ pora contained different amounts of training data (thus the lines of different length).</S>
			<S sid ="70" ssid = "49">Figure 2: Graph of the cumulative test phrase tokens (%) covered by training phrase types.</S>
			<S sid ="71" ssid = "50">Table 5: ENAMEX phrases by subcategory.</S>
			<S sid ="72" ssid = "51">The significance of this result is that each ENAMEX phrase subcategory had to be treated as equivalent.</S>
			<S sid ="73" ssid = "52">It was not pos­ sible to focus on a particular subcategory to obtain a con­ sistently high score.</S>
			<S sid ="74" ssid = "53">In other words, a strategy that focuses on locations would do well on the Chinese corpus where loca­ tions comprise 59.8% of the ENAMEX phrases, but would do poorly on the English corpus, where locations are only 14.5% of the ENAMEX.</S>
	</SECTION>
	<SECTION title="Training and ambiguity. " number = "3">
			<S sid ="75" ssid = "1">A logical question to pose is, &quot;How well can our system per­ form if it simply memorizes the phrases in the training texts?&quot;</S>
			<S sid ="76" ssid = "2">In each language, the transfer rate for the most frequent phrase types (the steep part of the graph) was quite high; however, the graph rapidly peaks and leaves a large per­ centage of the phrases uncovered by the training phrases.</S>
			<S sid ="77" ssid = "3">The remaining &quot;uncovered&quot; phrases can only be recognized by means other than &quot;memorization,&quot; such as by examining contextual clues.</S>
			<S sid ="78" ssid = "4">Table 6 shows the transfer rates of phrase tokens.</S>
			<S sid ="79" ssid = "5">The accuracy of the pure memorization can be reduced by two forms of ambiguity.</S>
			<S sid ="80" ssid = "6">Phrases or parts of phrases can oc­ cur within two or more named-entity categories, such as the string Boston, which by itself is a location but within Boston Red Sox is an organization.</S>
			<S sid ="81" ssid = "7">In most cases this ambiguity can be resolved using a simple longest-match heuristic.</S>
			<S sid ="82" ssid = "8">Another source of ambiguity occurs when a string can occur both as a Language Overall ENAMEX Org Loe Pers Chinese 73.2% 46.9&apos;7o 87.1% 42.6% English 21.2% 17.7% 42.7% 13.3% French 23.6% 13.4% 45.9% 11.2% Japanese 59.2% 56.2% 72.7% 37.5% Portuguese 61.3% 56.4% 57.4% 47.9% Spanish 48.1% 49.8% 71.4% 13.7% Table 6: Vocabulary transfer (tokens).</S>
			<S sid ="83" ssid = "9">NE phrase and as a non-phrase, such as Apple, which would sometimes refer to the computer company (and thus be tagged an organization) and sometimes refer to the fruit (and thus not be tagged at all).</S>
			<S sid ="84" ssid = "10">Such cases, although infrequent, would result in precision errors which we do not factor into the fol­ lowing estimation of a recall lower bound.</S>
	</SECTION>
	<SECTION title="Estimating  a lower bound. " number = "4">
			<S sid ="85" ssid = "1">Given the above statistical analysis, we estimated a baseline score for our straw-man algorithm on the NE task, a score which should easily be attainable by any system attempting to perform the task.</S>
			<S sid ="86" ssid = "2">First, we estimated that any system should be able to recognize a large percentage of NUMEX and TIMEX phrases; our experience indicates that 95% is possible due to the small number of patterns which compose most of these phrases.</S>
			<S sid ="87" ssid = "3">In order to estimate a lower bound for ENAMEX recogni­ tion, we relied on the transfer graph in Figure 2.</S>
			<S sid ="88" ssid = "4">It is clear from the graph that the contribution of the training data has leveled off in ea.ch language by the time the number of training types is roughly equal to the size of the test data (450 in this case).</S>
			<S sid ="89" ssid = "5">Selecting this point on the graph allowed us to directly compare memorization performance for the six languages.</S>
			<S sid ="90" ssid = "6">An ideal memorization-based algorithm would be able to recog­ nize phrases according to the transfer rate corresponding to this amount of training data.</S>
			<S sid ="91" ssid = "7">Our lower bound formula.</S>
			<S sid ="92" ssid = "8">would thus be ( ( N N uM E X + NT I M E x ) * a) + ( N EN AM E X * TEN AM E X ) where a = 0.95 (in our experience) Neat = Percentage of NE phrases represented by category (from Table 3) TEN AM E X = ENAMEX transfer rate (from Figure 2) The resulting lower bound scores, shown in Table 7, were surprisingly high, indicating that a very simple NE system could easily achieve a recall above 70 for some languages.</S>
			<S sid ="93" ssid = "9">The range of lower bound scores can partly be attributed to the differences in corpus makeup discussed in Section 3, but the range also illustrates the large score differences which are possible from one corpus to the next.</S>
			<S sid ="94" ssid = "10">The upper bounds of memorization algorithms implied by the preceding analysis do not require that a. deeper under­ standing of the linguistic phenomena of a target language is necessary to generalize NE recognition in unseen test data..</S>
			<S sid ="95" ssid = "11">Contextual clues can improve the expected score of a base­ line system without requiring extensive linguistic knowledge.</S>
			<S sid ="96" ssid = "12">Just as most of the TIMEX and NUMEX phrases in any lan­ guage can be recognized upon inspection using simple pattern Table 7: Estimated lower bounds.</S>
			<S sid ="97" ssid = "13">matching, a large percentage of the ENAMEX phrases could be codified given an adequate analysis of the phrasal contexts in the training documents.</S>
			<S sid ="98" ssid = "14">Furthermore, lists of titles, ge­ ographic units, and corporate designators would assist this contextual analysis and improve the expected baseline.</S>
			<S sid ="99" ssid = "15">In­ deed, such simple strategies drive most current NE systems.</S>
	</SECTION>
	<SECTION title="Discussion. " number = "5">
			<S sid ="100" ssid = "1">The results of this analysis indicate that it is possible to per­ form much of the task of named-entity recognition with a. very simple analysis of the strings composing the NE phrases; even more is possible with an additional inspection of the common phrasal contexts.</S>
			<S sid ="101" ssid = "2">The underlying principle is Zipf &apos;s Law; due to the prevalence of very frequent phenomena, a little effort goes a long way and very high scores can be achieved directly from the training data.</S>
			<S sid ="102" ssid = "3">Yet according to the same Law that gives us that initial high score, incremental advances above the baseline can be arduous and very language specific.</S>
			<S sid ="103" ssid = "4">Such improvement can most certainly only be achieved with a cer­ tain amount of well-placed linguistic intuition.</S>
			<S sid ="104" ssid = "5">The analysis also demonstrated the large differences in lan­ guages for the NE task, suggesting that we need to not only examine the overall score but also the ability to surpass the limitations of word lists, especially since extensive lists are available in very few languages.</S>
			<S sid ="105" ssid = "6">It is particularly important to evaluate system performance beyond a lower bound, such as that proposed in Section 4.</S>
			<S sid ="106" ssid = "7">Since the baseline scores will differ for different languages and corpora, scores for different corpora that appear equal may not necessarily be comparable.</S>
	</SECTION>
</PAPER>
