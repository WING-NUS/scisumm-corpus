Measuring the Impact of Sense Similarity on Word Sense Induction
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 113–123,Edinburgh, Scotland, UK, July 27–31, 2011. c©2011 Association for Computational Linguistics
Measuring the Impact of Sense Similarity on Word Sense Induction
David Jurgens1,21HRL Laboratories, LLCMalibu, California, USA
jurgens@cs.ucla.edu
Keith Stevens22University of California, Los Angeles
Los Angeles, California, USAkstevens@cs.ucla.edu
Abstract
Word Sense Induction (WSI) is an unsuper-vised learning approach to discovering the dif-ferent senses of a word from its contextualuses. A core challenge to WSI approachesis distinguishing between related and possiblysimilar senses of a word. Current WSI evalu-ation techniques have yet to analyze the spe-cific impact of similarity on accuracy. There-fore, we present a new WSI evaluation thatquantifies the relationship between the relat-edness of a word’s senses and the ability of aWSI algorithm to distinguish between them.Furthermore, we perform an analysis on senseconfusions in SemEval-2 WSI task accordingto sense similarity. Both analyses for a rep-resentative selection of clustering-based WSIapproaches reveals that performance is mostsensitive to the clustering algorithm and notthe lexical features used.
1 IntroductionMany words in a language have several distinctmeanings. For example, “earth” may refer to theplanet Earth, dirt, or solid ground, depending on thecontext. The goal of Word Sense Induction (WSI) isto automatically discover the different senses by ex-amining how a word is used. This unsupervised dis-covery process produces a sense inventory where thenumber of senses is corpus-driven and where sensesmay reflect additional usages not present in a pre-defined sense inventory, such as those for medicineor law (Dorow and Widdows, 2003). Furthermore,these discovered senses can be used to automati-cally expand lexical resources such as WordNet orFrameNet (Klapaftis and Manandhar, 2010).
Discovering the multiple senses is frequently
confounded by the relationships between a word’ssenses. While homonyms such as “bass” or “bank”have unrelated senses, many polysemous wordshave interrelated senses, with lexicographers of-ten in disagreement for the number of fine-grainedsenses (Palmer et al., 2007). For example, the mostfrequent four senses for “law” according to Word-Net, shown in Table 1, are similar in several aspectsand could be ascribed interchangeably in some con-texts. The difficulty of automatically distinguishingtwo senses is proportional to their similarity becauseof the increasing likelihood of the two senses shar-ing similar contexts.
While the issue distinguishing between relatedsenses is a recognized issue for Word Sense Dis-ambiguation (Chugur et al., 2002; McCarthy, 2006),which uses supervised training to learn sense dis-tinctions, measuring the impact of sense related-ness on the harder problem of WSI remains unad-dressed. The recent SemEval WSI tasks (Agirre andSoroa, 2007; Manandhar and Klapaftis, 2009) haveprovided a standard framework for evaluating WSIsystems, with a controlled training corpus designedto limit sense ambiguity in the example contexts.However, given the potential relatedness of a word’ssenses, we view it necessary to consider how WSImethods perform relative to the degree of contextualambiguity. Our goal is therefore to quantify the sim-ilarity at which a WSI approach is unable to distin-guish between two senses, which reflects the sensegranularity at which the approach operates.
We propose two new evaluations. The first, de-scribed in Section 4, uses a similarity-based pseudo-word discrimination task to measure the discrimi-nation capability for related senses along a gradedscale of similarity. As a second evaluation, in
113
1 the collection of rules imposed by authority2 legal document setting forth rules governing a particular
kind of activity3 a rule or body of rules of conduct inherent in human
nature and essential to or binding upon human society4 a generalization that describes recurring facts or events
in nature
Table 1: Definitions for the top four senses of “law”according to WordNet
Section 5 we perform an error analysis using theSemEval-2010 WSI task, examining sense confu-sion relative to the sense similarities. For both evalu-ations, we examine twenty different WSI clustering-based models through combining five feature typesand four clustering algorithms. These models wereselected to be representative of a wide class of exist-ing algorithms as a way of influence future algorith-mic directions based on the current model’s perfor-mance.
2 Clustering Contexts to Discover SensesFrequently, WSI is treated as an unsupervised clus-tering problem: The contexts in which a word ap-pears are clustered in order to discover its senses(Navigli, 2009). We selected four diverse cluster-ing algorithms for evaluation based on three crite-ria: (1) the ability to automatically determine the fi-nal number of clusters given an upper bound or aset of parameters, (2) an efficient run time, and (3)high quality results in either WSI or other fields re-lated to text analysis. The first criteria is essentialfor WSI; the final number of senses must be derivedwithout supervision in order to reflect the true num-ber of senses present in the corpus.
K-Means K-Means builds clusters based on thesimilarity between two data points. Clusters growby assigning data points to the cluster with the mostsimilar centroid. After every data point is assigned,each cluster’s centroid is recalculated to be the av-erage of all the data points assigned to the cluster.This process repeats until the centroids converge toa fixed point. We choose initial seeds at random anduse the H2 criterion function (Zhao and Karypis,2001). Although K-Means is efficient and widelyused, it requires the number of clusters to be spec-ified a priori. Therefore, we follow the WSI model
of Pedersen and Kulkarni (2006) and use the GapStatistic (Tibshirani et al., 2000) to automatically de-termine the number of clusters.
The Gap Statistic runs K-Means repeatedly withdifferent values of K , ranging from 1 to some sen-sible maximum. The Gap Statistic first induces adata model from the feature distributions of the ini-tial dataset and then for each K , creates a set of arti-ficial datasets by sampling from the derived model.K is increased until the “gap”, i.e. the distance be-tween the objective function of the original datasetand the average objective function of the artificialdatasets, is larger then the gap for the previous Kvalue. We calculate the gap using 10 artificial datasets sampled from the model.
Spectral Clustering Spectral Clustering inter-prets a dataset’s elements as vertices in graph withedges based on their similarity (Ng et al., 2001).Clusters are found by identifying the graph parti-tion that produces the minimum conductance be-tween every partition. This can be thought of astrying to find small islands that are connected by asfew bridges as possible. We refer the reader to (vonLuxburg, 2007) for further technical details. To ourknowledge, only He et al. (2010) have applied spec-tral clustering to WSI, which was performed on aChinese dataset. However, the algorithm used by Heet al. requires the number of clusters to be specified.
We instead use a hybrid spectral clustering algo-rithm, first applied to information retrieval (Chenget al., 2006), that automatically selects the numberof clusters. This algorithm recursively partitions adataset in half by finding the cut that produces theminimum conductance, which builds a tree of par-titions. This split is done until either every datapoint is in its own partition or a maximum number ofpartitions is found. Partitions are then dynamicallymerged, starting at leaf partitions, based on a cluster-ing criteria. We use the relaxed correlation criteria(Cheng et al., 2006), which tries to maximize bothinter cluster similarity and intra cluster dissimilarity.The final cluttering generated is then the best tree-respecting partition of the original data set.
Clustering By Committee Pantel and Lin (2002)found that K-Means clustering folded all featuresfound in a cluster into the centroid, many of whichare not useful for identifying the desired word sense.
114
To overcome this, they proposed a novel cluster-ing algorithm for WSI, Clustering by Committee(CBC), which includes only the most distinguishingfeatures for a cluster into the centroid.
For each context, an initial set of “committees”is formed by clustering the most similar contexts toeach context, with the resulting committees rankedto prefer larger, highly similar clusters. The finalset of committees (sense clusters) are selected by re-cursively identifying the highest ranking committeesthat are dissimilar to each other and then repeatingthe process for any contexts not similar to existingcommittees. In essence, CBC aims to find the clus-ters that are similar to the largest set of contexts,while keeping clusters dissimilar from each other.CBC’s recursion ensures that contexts dissimilar tothe large committees are still grouped into their ownsmaller committees, which enables the discovery ofinfrequent senses with distinct contexts. We use ahard sense assignment for each context, i.e., a con-text is labeled with only one sense according to themost similar cluster.
Streaming K-Means As WSI moves into induc-ing senses from Web-scale amounts of data, exist-ing clustering algorithms that keep all contexts inmemory become impractical. Jurgens and Stevens(2010a) proposed an on-line hybrid clustering so-lution using on-line K-Means and Hierarchical Ag-glomerative Clustering, which automatically de-cided the number of clusters without retaining allthe contexts. To the best of our knowledge, theirsis the only work using an on-line approach. Weextend this work by applying a more theoreticallysound online K-Means algorithm, called StreamingK-Means (Braverman et al., 2011), to WSI. We useStreaming K-Means to conduct a direct algorithmiccomparison with K-Means in the hopes that onlineapproaches can be made just as effective as off-lineapproaches.
Streaming K-Means processes each data pointonly once, thus reducing the memory overhead dra-matically. Instead of recording each data point, itimmediately assigns each data point to a cluster andmaintains K·C clusters. C varies as the algorithmruns, initially being set to 0. When assigning a datapoint, it is only assigned to an existing cluster whentheir similar is above some threshold, otherwise the
data point becomes the centroid of a new cluster.Once C reaches a threshold, based on an estimate ofthe number of data points, or the overall K-Meansclustering cost reaches some limit, the centroids aretreated as new data points and re-clustered, with thegoal of merging some centroids. We follow (Jur-gens and Stevens, 2010a) and cluster the final cen-troids with Hierarchical Agglomerative Clustering,with the average link criteria as suggested by (Ped-ersen and Bruce, 1997).3 Modeling ContextFor each clustering algorithm, we consider five con-text models that represent the types of lexical fea-tures used by the majority of WSI approaches.Co-Occurrence Contexts formed from word co-occurrence are the most common in WSI algorithms.For each occurrence of a word, those words withina certain range are counted as features. Prior workhas used a variety of context sizes, e.g. words inthe same sentence (Bordag, 2006), in nearby lexi-cal positions (Gauch and Futrelle, 1993), or within aparagraph-sized context window (Pedersen, 2010).
We consider two co-occurrence context models:a 5-word and a 25-word window. We note that inco-occurrence-based word space algorithms, smallercontext sizes have shown to better capture paradag-matic similarity, while larger sizes capture semanticassociativity (Peirsman et al., 2008; Utsumi, 2010).Dependency-Relations Dependency parsing cre-ates a syntax tree where words are directly linkedaccording to their relation. These links refine co-occurrence based contexts by utilizing syntactic in-dications of how words are related. Dependencyparsed features have proven highly effective forword representations in many NLP applications,e.g., (Pado´ and Lapata, 2007; Baroni et al., 2010).We follow Pantel and Lin (2002) and Dorow andWiddows (2003) using the sentence as contexts andall words with a dependency path of length 3 or less,with the last word and its relation as a feature. Wenote that recently Kern et al. (2010) achieved goodWSI performance with only a small, manually-tunedsubset of all relations as context.
Word Ordering Word ordering can provide amild form of syntactic information (Jones et al.,2006; Sahlgren et al., 2008). While other syntac-
115
tic features may provide significantly more informa-tion, word ordering is efficient to compute and pro-vides an alternative source of syntactic informationfor knowledge-lean systems or for languages whereNLP tools are not readily available.
Because we treat word ordering as a syntactic fea-ture, we limit the context to words occurring in thesame sentence. A feature is the combination of aco-occurring word and its relative position, i.e. thesame word in different positions is treated as twoseparate features.
Parts of Speech Part of speech tagging can pro-vide a preliminary coarse-grained sense disambigua-tion of a word’s contextual features, where a wordmay have as many senses as it does parts of speech.For example, consider an occurrence of “house” inthe context of “address” as a noun and verb: “I wentto his house address,” and “I heard the legislator ad-dress the house.” Labeling “address” with its partof speech provides for more semantic informationon its meaning, which further constrains the senseof “house.” Prior work (Pedersen and Bruce, 1997)has suggested that this information can improve per-formance, but to our knowledge, the impact of POSfeatures has not been evaluated in isolation.
Each context is formed from the containing sen-tence; a feature is a combination of each word and itspart of speech, e.g., “board-NOUN” is distinct from“board-VERB.”
4 WSI Performance on Related SensesThe proposed methodology measures the ability of aWSI approach to distinguish between related senses.However, generating a large corpus with manu-ally labeled sense assignments and sense similarityjudgements is prohibitively expensive. Therefore,we employ a pseudo-word discrimination task wherea base word and a second word, its confounder,are replaced throughout the corpus with a pseudo-word. The objective is then to determine which ofthe words was originally present given the contextof an occurrence of the pseudo-word. Due to notrequiring manual annotation, this type of task wasinitially proposed as a substitute for word sense dis-ambiguation (Schu¨tze, 1992; Gale et al., 1992) andfor selectional preferences (Clark and Weir, 2002).
Following the suggestions of Chambers and Ju-
festival laws
offices 0.13660 interests 0.18289play 0.13751 politics 0.20440convention 0.20296 governments 0.29125tournament 0.29007 regulations 0.40761concerts 0.48348 legislation 0.56112
Table 2: Example confounders for “festival” and“laws” and their similarities
rafsky (2010) on designing pseudo-words, pseudo-words were created from words with the same partof speech and equal frequency in the training cor-pus. We selected nouns occurring more than 5,000times in a 2009 Wikipedia snapshot and then drew5,000 contexts for each. The snapshot was taggedwith the Stanford Part of Speech Tagger (Toutanovaet al., 2003) and parsed with the Malt Parser (Nivreet al., 2006).
To evaluate the impact of sense similarity, pseudo-words were created from word pairs with a broadrange of lexical similarities. We selected lexicalsimilarity as an approximation of sense similarityin order to model the hypothesis that similar sensesmay appear in similar contexts. Similarity scoreswere calculated using cosine similarity on contex-tual distributions built from a sliding ±2 word win-dow over the Wikipedia corpus. Table 2 highlightsseveral example confounders and their similaritieswith the base term. In total, we generated 5000 term-confounder pairs from 98 base terms, with a mean of51 confounders per term.
All clustering parameters were chosen using thedefault values provided in the original papers. K-means and Streaming K-Means were both set witha maximum of 15 clusters, with the final number ofclusters being determined by the data itself.
4.1 Evaluation
The pseudo-word’s senses are induced from a train-ing segment using each feature and clustering com-bination. Given that both words making up thepseudo-word may be polysemous, more than twosenses may be induced. Each sense cluster is la-beled according to which of the original words waspresent in the majority of its contexts. For testing,each instance of the pseudo-word in a previouslyunseen context is assigned the label of the cluster
116
0.40
0.50
0.60
0.70
0.80
0.90
1.00
0.0 0.10 0.20 0.30 0.40 0.50
Pseu
do W
ord 
Disc
rimin
atio
n Ac
cura
cy
Base word and Confounder Similarity
(a) 5-Word Co-Occurrence
0.40
0.50
0.60
0.70
0.80
0.90
1.00
0.0 0.10 0.20 0.30 0.40 0.50
Pseu
do W
ord 
Disc
rimin
atio
n Ac
cura
cy
Base word and Confounder Similarity
(b) 25-Word Co-Occurrence
0.40
0.50
0.60
0.70
0.80
0.90
1.00
0.0 0.10 0.20 0.30 0.40 0.50
Pseu
do W
ord 
Disc
rimin
atio
n Ac
cura
cy
Base word and Confounder Similarity
(c) Dependency Relations
0.40
0.50
0.60
0.70
0.80
0.90
1.00
0.0 0.10 0.20 0.30 0.40 0.50
Pseu
do W
ord 
Disc
rimin
atio
n Ac
cura
cy
Base word and Confounder Similarity
(d) Word Order
0.40
0.50
0.60
0.70
0.80
0.90
1.00
0.0 0.10 0.20 0.30 0.40 0.50
Pseu
do W
ord 
Disc
rimin
atio
n Ac
cura
cy
Base word and Confounder Similarity
(e) Parts of Speech (f)
Figure 1: Pseudo-word discrimination performance
to which it is most similar. We perform five-foldcross-validation, using 4,000 contexts for trainingand 1,000 contexts for testing. Discrimination ac-curacy is reported as the average of all five runs.Since an equal number of contexts are used for eachterm, the base line accuracy of a most frequent sensemodel is 50% for each pseudo-word.
4.2 Results and Discussion
Figure 1 shows the discrimination accuracy relativeto the similarity of a base pair and confounder, foreach feature and clustering algorithm combination.Similarity values were binned at the 0.01 level witha mean of 39.0 scores per bin (median=11). Be-cause most word pairs are not related, the distri-bution of similarity values is biased towards lowervalues. Therefore, we omit similarity ranges above0.5, as too few confounders occurred in that range todraw reliable conclusions. The standard error (notshown) is < 1 for all measurements.
The general trends suggests that the clustering al-gorithm impacts the sense discriminatory ability farmore than the lexical feature choice. Furthermore,sense similarity affects most clustering algorithms,with most systems seeing a noticeable performancedrop when pseudo-word similarity is increased just
beyond 0. Performance at high similarity becomesmore variable for all algorithms and features.
For each clustering algorithm, we see dramati-cally different trends. Streaming K-Means performswell with co-occurrence based features and it doespoorly when either contexts have too many features,as in the 25 Window Co-Occurrence feature space,or the feature space overall is too sparse, as in theParts of Speech and Ordering feature spaces.
K-Means with the gap statistic converges to themost frequent sense baseline for nearly every con-founder pair. We note that this behavior significantlydiffers from that seen in (Pedersen and Kulkarni,2006), which clustered second-order co-occurrencevectors rather than the first-order features that weuse. Our analysis showed that the H2 criterion wasresponsible for this behavior. A subsequent analy-sis revealed that K-Means still converged to MFSfor the E1, E2, I1, and I2 criterion functions (Zhaoand Karypis, 2001) as well as when the number ofartificial datasets was increased up to 100. How-ever, additional tests using the same features on theSemEval-1 WSI task did not converge to MFS. Fur-ther investigation is needed to identify the cause ofconvergence and what types of data are appropriate
117
the Gap Statistic.Clustering by Committee performs well on most
models, but significantly worse on dependency re-lation features. A subsequent analysis showed thatCBC generates significantly more clusters than allother models. For the POS, 5 word window, and 25window Co-Occurrence feature spaces, CBC gener-ated between 205 and 247 clusters on average, perword. With the order feature space, CBC generated1087 clusters per word. However, when paired withdependency relation features, the number of clustersdrops to only 78 per word.
Spectral Clustering is most affected by sensesimilarity, performing competitively for unrelatedsenses but dropping significantly when words be-come even slightly similar. This performance dropis seen across all features. Performance is thereforelow, with the exception of dependency relations.
Overall, these results suggest that sense related-ness is a important factor in WSI performance andits impact should be considered in future WSI eval-uations. A potential next step is to vary the pro-portion of contexts from the confounder. The cur-rent method intentionally uses a uniform distribu-tion to avoid potential bias; however, word sense dis-tributions are rarely equal, and a varied distributionwould more closely model real world distributions.Similarly, the current method tested only two senses,whereas an n-way disambiguation between multipleconfounders should also provide further insight intoa WSI approach’s discriminatory abilities.
5 Sense Confusion in SemEval-2 Task 14As a second experiment, we analyze incorrect senseassignments on SemEval-2 Task 14 (Manandhar etal., 2010) to measure whether sense-relatedness bi-ases which sense was incorrectly selected. For WSIsystems, a similarity bias would indicate that similarsenses are more likely to be incorrectly identified asa single sense.
We summarize Task 14 as follows. Systems areprovided with an unlabeled training corpus con-sisting of 879,807 multi-sentence contexts for 100polysemous words, comprised of 50 nouns and 50verbs. Systems induce sense representations for tar-get words from the training corpus and then usethose representations to label the senses of the tar-get words in unseen contexts from a test corpus.
The induced senses are then evaluated against thegold standard labels OntoNotes (Hovy et al., 2006)senses labels for the test corpus. For our evaluation,we use both the two contrasting unsupervised mea-sures, the paired FScore (Artiles et al., 2009) and theV-Measure (Rosenberg and Hirschberg, 2007), anda supervised measure. For each metric, we use theevaluation framework provided by the organizers ofSemEval-2 Task 14.1
The V-Measure rates the homogeneity and com-pleteness of a clustering solution. Solutions thathave word clusters formed from one gold-standardsense are homogeneous; completeness measures thedegree to which a gold-standard sense’s instancesare assigned to a single cluster. The paired FScoremeasures two types of overlap of a solution and thegold standard in cluster assignments for all in pair-wise combination of instances. This score tendsto penalize solutions with many small clusters andhighly heterogeneous clusters (Manandhar and Kla-paftis, 2009).
The supervised evaluation measures the recallwhen building a Word Sense Disambiguation classi-fier from the induced senses. The WSI system labelsthe entire corpus, which is then divided into train-ing and test portions. The sense labels in the train-ing portion are used to construct a mapping from in-duced senses to the gold standard OntoNotes labels.This mapping is then evaluated for the induced la-bels in the test. We report the scores for the 80%training and 20% testing scenario.5.1 EvaluationWe expect that if sense similarity is a factor in senseconfusion, the probability of confusion will increasewith sense similarity. Therefore, we measure theprobability of labeling an instance with the incorrectOntoNotes sense relative to the sense similarity withthe gold standard sense.
In order to calculate the incorrect assignments,the induced senses must be mapped to OntoNotessenses. Each induced sense, si, is mapped to theOntoNotes sense that occurs most frequently amongthe instances in the test corpus that are assigned in-duced sense si. We note that this labeling processis only an approximate solution to assigning goldstandard labels to induced senses. A more robust
1http://www.cs.york.ac.uk/semeval2010_WSI/
118
 0
 50
 100
 150
 200
 250
 300
 350
 0.05  0.1  0.15  0.2  0.25  0.3
Freq
uenc
y of
 Sen
se C
onfu
sion
Ontonote Sense Similarity
Actual (avg)Baseline (avg)
Actual (max)Baseline (max)
(a) Streaming K-means
 0 20 40 60 80
 100 120 140 160
 0.05  0.1  0.15  0.2  0.25  0.3
Freq
uenc
y of
 Sen
se C
onfu
sion
Ontonote Sense Similarity
Actual (avg)Baseline (avg)
Actual (max)Baseline (max)
(b) CBC
 0
 50
 100
 150
 200
 250
 300
 350
 0.05  0.1  0.15  0.2  0.25  0.3
Freq
uenc
y of
 Sen
se C
onfu
sion
Ontonote Sense Similarity
Actual (avg)Baseline (avg)
Actual (max)Baseline (max)
(c) Spectral Clustering
 0 50
 100 150 200 250 300 350 400
 0.05  0.1  0.15  0.2  0.25  0.3
Freq
uenc
y of
 Sen
se C
onfu
sion
Ontonote Sense Similarity
Actual (avg)Baseline (avg)
Actual (max)Baseline (max)
(d) K-means
Figure 2: The error frequency distributions for confusing the correct sense with another sense of the givensimilarity when using a 5-word co-occurrence window as context. Dashed lines indicate the null models.
labeling could take into account the distribution ofgold standard senses labels in the corpus from whichthe senses are induced; however, such labels are notavailable in the Task 14 training corpus.
For each incorrect sense assignment, we mea-sure the similarity of the confused sense to thecorrect sense. To our knowledge, no work hasbeen done on calculating sense similarity within theOntoNotes sense hierarchy.2 Therefore, we approxi-mate OntoNotes sense similarity by using sense sim-ilarity in the WordNet ontology, on which has manysimilarity measures have been defined. FollowingBudanitsky and Hirst (2006), we estimate the Word-Net sense similarity using the method proposed byJiang and Conrath (1997).
Each OntoNotes sense si is mapped to a set ofWordNet 3.0 senses Si = {wn1, . . . , wnn} using
2We suspect that this is in part because a word’s OntoNotessenses have been designed to minimize sense confusion.
the sense mapping provided by the CoNLL sharedtask.3 The sense similarity for two OntoNotessenses is computed using one of two methods:
sim =1
|S1||S2|
?
wni?S1,wnj?S2
JCN(wni, wnj),
(1)or
sim = argmaxwni?S1,wnj?S2
JCN(wni, wnj), (2)
where JCN indicates the Jiang-Conrath similar-ity of two WordNet senses, calculated using Word-Net::Similarity (Pedersen et al., 2004). Eq. 1 com-putes similarity as the average similarity of all pair-wise WordNet sense combinations, while Eq. 2 usesthe highest similarity. The resulting OntoNote sensesimilarities range from 0 to 1, with 1 being maxi-mally similar. We excluded 10 words from the test
3http://conll.bbn.com/index.php/data.html
119
Context Feature Clustering V-Measure F-Score Recall # Clusters Purity GoF p-Value
5-Word Co-Occurrence
Streaming 6.7 55.5 54.8 4.74 0.103 p < 2.07e-37Spectral 10.8 39.2 54.3 8.41 0.194 p < 1.11e-25
CBC 23.9 8.2 39.5 39.7 0.665 p < 0.916K-Means 2.5 61.8 55.6 1.68 0.020 p < 1.20e-37
25-Word Co-Occurrence
Streaming 2.6 61.7 55.5 1.7 0.020 p < 1.20e-37Spectral 5.0 48.6 55.9 3.3 0.083 p < 4.36e-32
CBC 21.3 11.6 45.0 32.2 0.561 p < 0.011K-Means 2.5 61.8 55.6 1.68 0.020 p < 1.20e-37
Dependency Relations
Streaming 3.0 61.5 55.6 1.9 0.022 p < 7.33e-38Spectral 8.5 46.8 55.3 5.9 0.134 p < 5.45e-14
CBC 12.9 31.3 52.4 11.4 0.259 p < 4.07e-12K-Means 2.5 61.8 55.6 1.6 0.020 p < 1.20e-37
Word Order
Streaming 10.8 43.1 54.2 10.8 0.300 p < 4.46e-24Spectral 12.2 32.4 53.7 10.0 0.26 p < 3.27e-20
CBC 27.2 11.8 30.3 54.9 0.857 p < 0.999K-Means 2.5 61.8 55.6 1.6 0.020 p < 1.20e-37
Parts of Speech
Streaming 6.6 53.0 54.5 4.7 0.117 p < 1.06e-39Spectral 10.9 39.4 53.7 8.3 0.201 p < 2.38e-13
CBC 23.8 08.0 40.1 39.7 0.678 p < 1.04e-2K-Means 2.5 61.8 55.6 1.6 0.020 p < 1.20e-37
SemEval-2 Most Frequent Sense 0.0 63.4 58.6 1.0 0.0 p < 4.244e-23
Best SemEval-2 FScore 0.0 63.3 58.6 1.0 0.0 p < 2.893e-23
Best SemEval-2 VMeasure 16.2 26.7 58.3 10.7 0.367 p < 1.956e-14
Best SemEval-2 Supervised Recall 15.7 49.7 62.4 11.5 0.187 p < 8.910e-19
Table 3: Unsupervised and Supervised scores on the SemEval-2010 WSI Task for each feature and clusteringmodels, with reference scores for the top performing systems for each evaluation shown below.
set that did not have mappings from OntoNotes toWordNet 3.0 senses, and additional 23 words thatonly had two senses, which prevented testing fora similarity bias. The remaining 67 words yielded4,097 test instances for evaluation.
Each instance of the test corpus was tested forsense confusion, recording the similarity of the in-correctly assigned sense and the gold standard sense.The resulting incorrect assignments are transformedinto an error distribution according by accumulatingerror counts into similarity bins where each bin has arange of 0.02. We analyze the WSI systems definedin section 4 as well as the results of three systemsthat participated in Task 14 and scored the higheston the paired FScore, V-measure, or Supervised Re-call evaluations.
To quantify the impact, we compare each system’serror distribution against a null model over the set ofincorrect test instances missed by that system. In
the null model, the incorrect sense for each instanceis selected with uniform probability from the avail-able senses. This behavior produces a distributionwith no similarity bias. The cumulative error dis-tribution for the null model is not uniform due tomultiple sense pairings having the same similarity.4To quantify the difference between a system’s errordistribution and corresponding null model, we cal-culate the G-test as a measure of Goodness of Fit(GoF). The resulting p-values reflect the probabilityof observing the system’s error distribution if therewas no bias from sense-similarity.
5.2 Results and Discussion
We compare the error analysis against the evalua-tion measures of Task 14. Table 3 displays the eval-
4Verb senses often have a JCN similarity of 0 due to hav-ing no shared parent within the WordNet verb sense hierarchy,which results in high frequency distribution around 0.
120
uation measures. We also report the average num-ber of clusters per word, the cluster purity, and thep-value when using Eq. 2 to measure sense similar-ity. Figure 2 visualizes the error distributions for thefour clustering algorithms on 5-word co-occurrencefeatures. The distributions in Figure 2 are represen-tative of those of the other context models, which weomit due to space. Each plot reflects the frequencyat which a sense with the specified similarity wasconfused for the correct sense.
The low p-values in Table 3 indicate a significantdeviation from the null model. Examining the shapeof the error distribution in Figure 2 reveals a no-ticeable skew towards higher similarity when an in-correct sense assignment is made. This distributionskew is also consistent for both similarity measures.
Comparing the Task 14 results in Table 3 to thesense confusion trends in in Figure 2 highlights aninteresting pattern among the various models: as thenumber of induced sense clusters increases, the er-ror distribution better approximates the null model.Specifically, the GoF for all models was well corre-lated with cluster purity (?=0.66), and the number ofclusters (?=0.76). CBC generated the highest num-ber of clusters and has a sense confusion distributionthat closely matches the null model, indicating thatit is less affected by sense similarity. In compari-son, all of the Streaming K-Means models, whichhave the fewest clusters, differ noticeably from thenull model. Spectral Clustering, which also gener-ates fewer clusters than CBC, has an observed con-fusion rate that differs from the baseline. K-Meansagain reduces to the MFS baseline.
When comparing along the feature sets, we seethat on average Word Order features generate thehighest V-Measure scores, highest purity, and high-est p-values for Streaming K-Means and CBC. Thisresult correlates well with the average number offeatures seen per context: Word Order contexts used0.03% of the feature space while contexts in otherfeature spaces used between 0.07% and 0.12% ofthe feature space, suggesting that the SemEval mea-sures are determined in part by feature space den-sity. Similarly, 25-word co-occurrence features hadthe highest percentage of features used per context,0.12%, and generated the lowest V-Measure, purityscore, and p-value for 3 clustering models.
These scores support another known trend in the
SemEval-2 evaluation: the performance on the V-Measure is proportional to the number of inducedsense clusters, while the paired FScore is inverselyproportional. But what is surprising is that modelswhich perform well against the V-Measure also ex-hibit a smaller sense similarity bias, suggesting thatCBC and similar clustering methods are suitable forsituations where competing senses of a word have ahigh degree of overlap.
As a final comparison, we also computed thesense bias for the top 3 SemEval systems under eachmeasure. The best of these models are listed in Table3. We did not find any consistent trends between theV-Measure, purity, and p-value among these mod-els. The top F-Scoring models all used either a firstor second order co-occurrence feature space similarto ours (Kern et al., 2010; Pedersen, 2010), whereasthe top supervised score was achieved by a graph-based system (Klapaftis and Manandhar, 2008).6 Future Work and ConclusionWe presented a two evaluation for WSI approachesand examined the performance of a wide range ofalgorithms. The results raise a potential issue forclustering-based WSI approaches: sense discrimi-nation degrades notably as the sense relatedness in-creases. We highlight three potential avenues forfuture research. First, this methodology should beapplied to additional WSI models, such as graph-based (Klapaftis and Manandhar, 2008; Navigli andCrisafulli, 2010) and probabilistic models (Dinu andLapata, 2010; Elshamy et al., 2010). Second, weplan to extend the analysis to different sense dis-tributions, varying number of senses, and for hu-man annotated sense similarity data. Third, thisevaluation makes the simplifying assumption of onesense per instance; however, Erk et al. (2009) notethat the relations between senses may cause a singleword instance to evoke multiple senses within thesame context. Therefore, a future experiment shouldconsider how WSI systems might address learningsenses given the presence of multiple, similar sensesfor a single instance.
All models, associated data sets, testing frame-work, and scores have been released as a part of theopen-source S-Space Package (Jurgens and Stevens,2010b).5
5http://code.google.com/p/airhead-research/
121
References
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 task02: Evaluating word sense induction and discrimina-tion systems. In Proceedings of the Fourth Interna-tional Workshop on Semantic Evaluations, pages 7–12.ACL, June.
Javier Artiles, Enrique Amigo´, and Julio Gonzalo. 2009.The role of named entities in web people search. InProceedings of EMNLP, pages 534–542. ACL.
Marco Baroni, Brian Murphy, Eduard Barbu, and Mas-simo Poesio. 2010. Strudel: A corpus-based semanticmodel based on properties and types. Cognitive Sci-ence, 34(2):222–254.
Stefan Bordag. 2006. Word sense induction: Triplet-based clustering and automatic evaluation. In Pro-ceedings of the 11th EACL, pages 137–144.
Vladimir Braverman, Adam Meyerson, Rafail Ostrovsky,Alan Roytman, Michael Shindler, and Brian Tagiku.2011. Streaming k-means on Well-Clusterable Data.In Proceedings of SODA 2011.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-uating wordnet-based measures of semantic distance.Computational Linguistics, 32(1):13–47, March.
Nathanael Chambers and Dan Jurafsky. 2010. Improvingthe Use of Pseudo-Words for Evaluating SelectionalPreferences. In ACL 2010.
David Cheng, Ravi Kannan, Santosh Vempala, and GrantWang. 2006. A divide-and-merge methodology forclustering. ACM Transactions on Database Systems(TODS), 31(4):1499–1525.
Irina Chugur, Julio Gonzalo, and Felisa Verdejo. 2002.Polysemy and sense proximity in the senseval-2 testsuite. In Proceedings of the ACL-02 workshop onWord sense disambiguation: recent successes and fu-ture directions - Volume 8, WSD ’02, pages 32–39,Stroudsburg, PA, USA. Association for ComputationalLinguistics.
Stephen Clark and David Weir. 2002. Class-based prob-ability estimation using a semantic hierarchy. Compu-tational Linguistics, 28(2):187–206.
Georgiana Dinu and Mirella Lapata. 2010. Measuringdistributional similarity in context. In Proceedings ofthe 2010 Conference on Empirical Methods in NaturalLanguage Processing, pages 1162–1172. Associationfor Computational Linguistics.
Beate Dorow and Dominic Widdows. 2003. Discover-ing corpus-specific word senses. In Proceedings of the10th EACL, pages 79–82.
Wesam Elshamy, Doina Caragea, and William H. Hsu.2010. KSU KDD: Word sense induction by cluster-ing in topic space. In Proceedings of the 5th Interna-tional Workshop on Semantic Evaluation, pages 367–370. Association for Computational Linguistics.
Katrin Erk, Diana McCarthy, and Nicholas Gaylord.2009. Investigations on word senses and word us-ages. In Proceedings of the Joint Conference of the47th Annual Meeting of the ACL and the 4th Interna-tional Joint Conference on Natural Language Process-ing of the AFNLP: Volume 1, pages 10–18. Associationfor Computational Linguistics.
William A. Gale, Kenneth W. Church, and DavidYarowsky. 1992. Work on statistical methods for wordsense disambiguation. In AAAI Fall Symposium onProbabilistic Approaches to Natural Language, pages54–60.
Susan Gauch and Robert P. Futrelle. 1993. Experimentsin automatic word class and word sense identificationfor information retrieval. In Proceedings of the 3rdAnnual Symposium on Document Analysis and Infor-mation Retrieval, pages 425–434.
Zhengyan He, Yang Song, and Houfeng Wang. 2010.Applying Spectral Clustering for Chinese Word SenseInduction. In Proceedings of the CIPS-SIGHAN JointConference on Chinese Language Processing.
Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel. 2006. OntoNotes:the 90% solution. In Proceedings of the Human Lan-guage Technology Conference of the NAACL, pages57–60. Association for Computational Linguistics.
Jay J. Jiang and David W. Conrath. 1997. Semantic Sim-ilarity Based on Corpus Statistics and Lexical Taxon-omy. In Proceedings of International Conference onResearch in Computational Linguistics,.
Michael N. Jones, Walter Kintsch, and Doughlas J. K.Mewhort. 2006. High-dimensional semantic space ac-counts of priming. Journal of Memory and Language,55:534–552.
David Jurgens and Keith Stevens. 2010a. HERMIT: Us-ing word ordering applied to the Sense Induction taskof SemEval-2. In Proceedings of the 5th InternationalWorkshop on Semantic Evaluations. Association forComputational Linguistics.
David Jurgens and Keith Stevens. 2010b. The S-SpacePackage: An Open Source Package for Word SpaceModels. In Proceedings of the ACL 2010 SystemDemonstrations.
Roman Kern, Markus Muhr, and Michael Granitzer.2010. KCDC: Word sense induction by using gram-matical dependencies and sentence phrase structure.In Proceedings of the 5th International Workshop onSemantic Evaluation, pages 351–354. Association forComputational Linguistics.
Ioannis P. Klapaftis and Suresh Manandhar. 2008. Wordsense induction using graphs of collocations. In Pro-ceeding of the 2008 conference on ECAI 2008: 18thEuropean Conference on Artificial Intelligence, pages298–302.
122
Ioannis P. Klapaftis and Suresh Manandhar. 2010. Tax-onomy learning using word sense induction. In Hu-man Language Technologies: The 2010 Annual Con-ference of the North American Chapter of the Associ-ation for Computational Linguistics, HLT ’10, pages82–90, Morristown, NJ, USA. Association for Com-putational Linguistics.
Suresh Manandhar and Ioannis P. Klapaftis. 2009.SemEval-2010 Task 14: Evaluation Setting for WordSense Induction & Disambiguation Systems. InNAACL-HLT 2009 Workshop on Semantic Evalua-tions: Recent Achievements and Future Directions.
Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dli-gach, and Sameer S. Pradhan. 2010. SemEval-2010 task 14: Word sense induction & disambigua-tion. In Proceedings of the 5th International Workshopon Semantic Evaluation, pages 63–68. Association forComputational Linguistics.
Diana McCarthy. 2006. Relating WordNet senses forword sense disambiguation. Making Sense of Sense:Bringing Psycholinguistics and Computational Lin-guistics Together, page 17.
Roberto Navigli and Giuseppe Crisafulli. 2010. Inducingword senses to improve web search result clustering.In Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, pages 116–126. Association for Computational Linguistics.
Roberto Navigli. 2009. Word sense disambiguation: asurvey. ACM Computing Surveys, 41(2):1–69.
Andrew Ng, Michael Jordan, and Yair Weiss. 2001. Onspectral clustering: Analysis and an algorithm. In Ad-vances in Neural Information Processing Systems 14:Proceeding of the 2001 Conference, pages 849–856.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-Parser: A data-driven parser-generator for dependencyparsing. In Proceedings of LREC, pages 2216–2219.
Sebastian Pado´ and Mirella Lapata. 2007. Dependency-Based Construction of Semantic Space Models. Com-putational Linguistics, 33(2):161–199.
Martha Palmer, Hoa Trang Dang, and Christiane Fell-baum. 2007. Making fine-grained and coarse-grainedsense distinctions, both manually and automatically.Natural Language Engineering, 13(02):137–163.
Patrick Pantel and Dekang Lin. 2002. Discovering wordsenses from text. In Proceedings of ACM Conferenceon Knowledge Discovery and Data Mining (KDD-02),pages 613–619.
Ted Pedersen and Rebecca Bruce. 1997. Distinguishingword senses in untagged text. In Proceedings of theSecond Conference on Empirical Methods in NaturalLanguage Processing, pages 197–207, August.
Ted Pedersen and Anagha Kulkarni. 2006. Automaticcluster stopping with criterion functions and the gapstatistic. In In Proceedings of the Demo Session of
HLT/NAACL, pages 276–279.Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet:: Similarity: measuring therelatedness of concepts. In Demonstration Papers atHLT-NAACL 2004 on XX, pages 38–41. Associationfor Computational Linguistics.
Ted Pedersen. 2010. Duluth-WSI: SenseClusters Ap-plied to the Sense Induction Task of SemEval-2. InProceedings of the SemEval 2010 Workshop : the5th International Workshop on Semantic Evaluations,pages 363–366, July.
Yves Peirsman, Kris Heylen, and Dirk Geeraerts. 2008.Size matters. Tight and loose context definitions in En-glish word space models. In ESSLLI Workshop on Dis-tributional Lexical Semantics, pages 34–41.
Andrew Rosenberg and Julia Hirschberg. 2007. V-measure: A conditional entropy-based external clusterevaluation measure. In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL). ACL, June.
Magnus Sahlgren, Anders Holst, and Pentti Kanerva.2008. Permutations as a means to encode order inword space. In Proceedings of the 30th Annual Meet-ing of the Cognitive Science Society (CogSci’08).
Hinrich Schu¨tze, 1992. Context Space, pages 113–120.AAAI Press, Menlo Park, CA.
Robert Tibshirani, Guenther Walther, and Trevor Hastie.2000. Estimating the number of clusters in a datasetvia the gap statistic. Journal Royal Statistics SocietyB, 63:411–423.
Kristina Toutanova, Dan Klein, Christopher Manning,and Yoram Singer. 2003. Feature-rich part-of-speechtagging with a cyclic dependency network. pages 252–259.
Akira Utsumi. 2010. Exploring the Relationship be-tween Semantic Spaces and Semantic Relations. InProceedings of the 7th International Conference onLanguage Resources and Evaluation (LREC’2010),pages 257–262.
Ulrike von Luxburg. 2007. A tutorial on spectral cluster-ing. Statistics and Computing, 17(4):395–416.
Ying Zhao and George Karypis. 2001. Criterion func-tions for document clustering: Experiments and analy-sis. Technical Report UMN CS 01-040, University ofMinnesota.
123
