Word Sense Induction  Disambiguation Using Hierarchical Random Graphs
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 745–755,MIT, Massachusetts, USA, 9-11 October 2010. c©2010 Association for Computational Linguistics
Word Sense Induction & Disambiguation UsingHierarchical Random Graphs
Ioannis P. KlapaftisDepartment of Computer Science
University of YorkUnited Kingdom
giannis@cs.york.ac.uk
Suresh ManandharDepartment of Computer Science
University of YorkUnited Kingdom
suresh@cs.york.ac.uk
Abstract
Graph-based methods have gained attention inmany areas of Natural Language Processing(NLP) including Word Sense Disambiguation(WSD), text summarization, keyword extrac-tion and others. Most of the work in these ar-eas formulate their problem in a graph-basedsetting and apply unsupervised graph cluster-ing to obtain a set of clusters. Recent studiessuggest that graphs often exhibit a hierarchi-cal structure that goes beyond simple flat clus-tering. This paper presents an unsupervisedmethod for inferring the hierarchical group-ing of the senses of a polysemous word. Theinferred hierarchical structures are applied tothe problem of word sense disambiguation,where we show that our method performs sig-nificantly better than traditional graph-basedmethods and agglomerative clustering yield-ing improvements over state-of-the-art WSDsystems based on sense induction.
1 Introduction
A number of NLP problems can be cast into a graph-based framework, in which entities are representedas vertices in a graph and relations between them aredepicted by weighted or unweighted edges. For in-stance, in unsupervised WSD a number of methods(Widdows and Dorow, 2002; Ve´ronis, 2004; Agirreet al., 2006) have constructed word co-occurrencegraphs for a target polysemous word and appliedgraph-clustering to obtain the clusters (senses) ofthat word.
Similarly in text summarization, Mihalcea (2004)developed a method, in which sentences are rep-
resented as vertices in a graph and edges betweenthem are drawn according to their common tokensor words of a given POS category, e.g. nouns.Graph-based ranking algorithms, such as PageRank(Brin and Page, 1998), were then applied in orderto determine the significance of sentences. In thesame vein, graph-based methods have been appliedto other problems such as determining semantic sim-ilarity of text (Ramage et al., 2009).
Recent studies (Clauset et al., 2006; Clauset etal., 2008) suggest that graphs exhibit a hierarchi-cal structure (e.g. a binary tree), in which verticesare divided into groups that are further subdividedinto groups of groups, and so on, until we reach theleaves. This hierarchical structure provides addi-tional information as opposed to flat clustering byexplicitly including organisation at all scales of agraph (Clauset et al., 2008). In this paper, we presentan unsupervised method for inferring the hierarchi-cal structure (binary tree) of a graph, in which ver-tices are the contexts of a polysemous word andedges represent the similarity between contexts. Themethod that we use to infer that hierarchical struc-ture is the Hierarchical Random Graphs (HRGs) al-gorithm due to Clauset et al. (2008).
The binary tree produced by our method groupsthe contexts of a polysemous word at differentheights of the tree. Thus, it induces the senses ofthat word at different levels of sense granularity. Toevaluate our method, we apply it to the problem ofnoun sense disambiguation showing that inferringthe hierarchical structure using HRGs provides ad-ditional information from the observed graph lead-ing to improved WSD performance compared to: (1)
745
Figure 1: Stages of the proposed method.
simple flat clustering, and (2) traditional agglomera-tive clustering. Finally, we compare our results withstate-of-the-art sense induction systems and showthat our method yields improvements. Figure 1shows the different stages of the proposed methodthat we describe in the following sections.
2 Related work
Typically, graph-based methods, when applied tounsupervised sense disambiguation represent eachword wi co-occurring with the target word tw as avertex. Two vertices are connected via an edge ifthey co-occur in one or more contexts of tw. Oncethe co-occurrence graph of tw has been constructed,different graph clustering algorithms are applied toinduce the senses. Each cluster (induced sense) con-sists of a set of words that are semantically related tothe particular sense. Figure 2 shows an example ofa graph for the target word paper that appears withtwo different senses scholarly article and newspa-per.
Ve´ronis (2004) has shown that co-occurrencegraphs are small-world networks that contain highlydense subgraphs representing the different clusters(senses) of the target word (Ve´ronis, 2004). To iden-tify these dense regions Ve´ronis’s algorithm itera-tively finds their hubs, where a hub is a vertex with avery high degree. The degree of a vertex is defined tobe the number of edges incident to that vertex. Theidentified hub is then deleted along with its directneighbours from the graph producing a new cluster.
For example, in Figure 2 the highest degree ver-tex, news, is the first hub, which would be deletedalong with its direct neighbours. The deleted re-gion corresponds to the newspaper sense of the tar-get word paper. Ve´ronis (2004) further processedthe identified clusters (senses), in order to assign therest of graph vertices to the identified clusters by
utilising the minimum spanning tree of the originalgraph.
In Agirre et al. (2006), the algorithm of Ve´ronis(2004) is analysed and assessed on the SensEval-3dataset (Snyder and Palmer, 2004), after optimis-ing its parameters on the SensEval-2 dataset (Ed-monds and Dorow, 2001). The results show that theWSD F-Score outperforms the Most Frequent Sense(MFS) baseline by approximately 10%, while induc-ing a large number of clusters (with averages of 60to 70).
Another graph-based method is presented in(Dorow and Widdows, 2003). They extract onlynoun neighbours that appear in conjunctions or dis-junctions with the target word. Additionally, theyextract second-order co-occurrences. Nouns are rep-resented as vertices, while edges between verticesare drawn, if their associated nouns co-occur in con-junctions or disjunctions more than a given num-ber of times. This co-occurrence frequency is alsoused to weight the edges. The resulting graph isthen pruned by removing the target word and ver-tices with a low degree. Finally, the MCL algorithm(Dongen, 2000) is used to cluster the graph and pro-duce a set of clusters (senses) each one consisting ofa set of contextually related words.
Chinese Whispers (CW) (Biemann, 2006) is aparameter-free1 graph clustering method that hasbeen applied in sense induction to cluster the co-occurrence graph of a target word (Biemann, 2006),as well as a graph of collocations related to the tar-get word (Klapaftis and Manandhar, 2008). Theevaluation of the collocational-graph method in theSemEval-2007 sense induction task (Agirre andSoroa, 2007) showed promising results.
All the described methods for sense induction ap-1One needs to specify only the number of iterations. The
number of clusters is generated automatically.
746
Figure 2: Graph of words for the target word paper.Numbers inside vertices correspond to their degree.
Figure 3: Running example of graph creation
ply flat graph clustering methods to derive the clus-ters (senses) of a target word. As a result, they ne-glect the fact that their constructed graphs often ex-hibit a hierarchical structure that is useful in severaltasks including word sense disambiguation.
3 Building a graph of contexts
This section describes the process of creating agraph of contexts for a polysemous target word. Fig-ure 3 provides a running example of the differentstages of our method. In the example, the targetword paper appears with the scholarly article sensein the contexts A, B, and with the newspaper sensein the contexts C and D.
3.1 Corpus preprocessing
Let bc denote the base corpus consisting of the con-texts containing the target word tw. In our work,
a context is defined as a paragraph2 containing thetarget word.
The aim of this stage is to capture nouns contex-tually related to tw. Initially, the target word is re-moved from bc and part-of-speech tagging is appliedto each context. Following the work in (Ve´ronis,2004; Agirre et al., 2006) only nouns are kept andlemmatised. In the next step, the distribution of eachnoun in the base corpus is compared to the distri-bution of the same noun in a reference corpus3 us-ing the log-likelihood ratio (G2) (Dunning, 1993).Nouns with a G2 below a pre-specified threshold(parameter p1) are removed from each paragraph ofthe base corpus. The upper left part of Figure 3shows the words kept as a result of this stage.
3.2 Graph creationGraph vertices: To create the graph of vertices, werepresent each context ci as a vertex in a graph G.Graph edges: Edges between the vertices of thegraph are drawn based on their similarity, definedin Equation 1, where simcl(ci, cj) is the colloca-tional weight of contexts ci, cj and simwd(ci, cj)is their bag-of-words weight. If the edge weightW (ci, cj) is above a prespecified threshold (param-eter p3), then an edge is drawn between the corre-sponding vertices in the graph.
W (ci, cj) =12(simcl(ci, cj) + simwd(ci, cj)) (1)
Collocational weight: The limited polysemy of col-locations can be exploited to compute the similaritybetween contexts ci and cj . In our setting, a colloca-tion is a juxtaposition of two nouns within the samecontext. Thus, given a context ci, each of its nounsis combined with any other noun yielding a total of(N2
)collocations for a context with N nouns. Each
collocation, clij is weighted using the log-likelihoodratio (G2) (Dunning, 1993) and is filtered out if theG2 is below a prespecified threshold (parameter p2).At the end of this process, each context ci of tw isassociated with a vector of collocations (vi). Theupper right part of Figure 3 shows the collocationsassociated with each context of our example.
2Our definition of context is equivalent to an instance of thetarget word in the SemEval-2007 sense induction task dataset(Agirre and Soroa, 2007).
3The British National Corpus, 2001, Distributed by OxfordUniversity Computing Services.
747
Given two contexts ci and cj , we calculate theircollocational weight using the Jaccard coefficienton the collocational vectors, i.e. simcl(ci, cj) =|vinvj ||vi?vj | . The selection of Jaccard is based on the workof Weeds et al. (2004), who analyzed the variationin a word’s distributionally nearest neighbours withrespect to a variety of similarity measures. Theiranalysis showed that there are three classes of mea-sures, i.e. those selecting distributionally more gen-eral neighbours (e.g. cosine), those selecting distri-butionally less general neighbours (e.g. AMCRM-Precision (Weeds et al., 2004)) and those without abias towards the distributional generality of a neigh-bour (e.g. Jaccard). In our setting, we are interestedin calculating the similarity between two contextswithout any bias. We selected Jaccard, since the restof that class’s measures are based on pointwise mu-tual information that assigns high weights to infre-quent events.Bag-of-words weight: Estimating context similar-ity using collocations may provide reliable estimatesregarding the existence of an edge in the graph, how-ever, it also suffers from data sparsity. For this rea-son, we also employ a bag-of-words model. Specif-ically, each context ci is associated with a vector githat contains the nouns kept as result of the corpuspreprocessing stage. The upper left part of Figure3 shows the words associated with each context ofour example. Given two contexts ci and cj , we cal-culate their bag-of-words weight using the Jaccardcoefficient on the word vectors, i.e. simwd(ci, cj) =|gingj ||gi?gj | .
The collocational weight and bag-of-wordsweight are averaged to derive the edge weight be-tween two contexts as defined in Equation 1. Theresulting graph of our running example is shown onthe bottom of Figure 3. This graph is the input to thehierarchical random graphs method (Clauset et al.,2008) described in the next section.
4 Hierarchical Random Graphs for senseinduction
In this section, we describe the process of inferringthe hierarchical structure of the graph of contextsusing hierarchical random graphs (Clauset et al.,2008).
Figure 4: Two dendrograms for the graph in Figure 3.
4.1 The Hierarchical Random Graph model
A dendrogram is a binary tree with n leaves andn - 1 parents. Figure 4 shows an example of twodendrograms with 4 leaves and 3 parents. Given aset of n contexts that we need to arrange hierarchi-cally, let us denote by G = (V,E) the graph of con-texts, where V = {v0, v1 . . . vn} is the set of ver-tices, E = {e0, e1 . . . em} is the set of edges andek = {vi, vj}.
Given an undirected graph G, each of its n ver-tices is a leaf in a dendrogram, while the internalnodes of that dendrogram indicate the hierarchicalrelationships among the leaves. We denote this or-ganisation byD = {D1, D2, . . . Dn-1}, where eachDk is an internal node. Every pair of nodes (vi, vj)is associated with a unique Dk, which is their low-est common ancestor in the tree. In this manner Dpartitions the edges that exist in G.
The primary assumption in the hierarchical ran-dom graph model is that edges in G exist indepen-dently, but with a probability that is not identicallydistributed. In particular, the probability that an edge{vi, vj} exists in G is given by a parameter ?k asso-ciated with Dk, the lowest common ancestor of viand vj in D. In this manner, the topological struc-ture D and the vector of probabilities ~? define theHRG given by H(D, ~?) (Clauset et al., 2008).
748
4.2 HRG parameterisationAssuming a uniform prior over all HRGs, the targetis to identify the parameters of D and ~?, so that thechosen HRG is statistically similar to G. Let Dk bean internal node of dendrogram D and f(Dk) be thenumber of edges between the vertices of the subtreesof the subtree rooted at Dk that actually exist in G.For example, in Figure 4(A), f(D2) = 1, becausethere is one edge in G connecting vertices B and C.Let l(Dk) be the number of leaves in the left subtreeof Dk, and r(Dk) be the number of leaves in theright subtree. For example in Figure 4(A), l(D2) =2 and r(D2) = 2. The likelihood of the hierarchicalrandom graph (D, ~?) is defined in Equation 2, whereA(Dk) = l(Dk)r(Dk)- f(Dk).
L(D, ~?) =?Dk?D
?f(Dk)k (1- ?k)A(Dk) (2)
The probabilities ?k that maximise the likelihoodof a dendrogram D can be easily estimated usingthe method of MLE i.e ?k =
f(Dk)l(Dk)r(Dk)
. Substi-tuting this into Equation 2 yields Equation 3. Fornumerical reasons, it is more convenient to workwith the logarithm of the likelihood which is definedin Equation 4, where h(?k) = -?k log ?k - (1 -?k) log (1- ?k).
L(D) =?Dk?D
[??kk (1- ?k)1-?k ]l(Dk)r(Dk) (3)
logL(D) = -?Dk?D
h(?k)l(Dk)r(Dk) (4)
As can be observed, each term -l(Dk)r(Dk)h(?k)is maximised when ?k approaches 0 or 1. Thismeans that high-likelihood dendrograms partitionvertices into subtrees, such that the connectionsamong their vertices in the observed graph are eithervery rare or very common (Clauset et al., 2008). Forexample, consider the two dendrograms in Figures4(A) and 4(B). We observe that 4(A) is more likelythan 4(B), since it provides a better division of thenetwork leaves. Particularly, the likelihood of 4(A)is L(D1) = (11 · (1- 1)1) · (11 · (1- 1)1) · (0.251 ·(1 - 0.25)3) = 0.105, while the likelihood of 4(B)is L(D2) = (00 · (1- 0)1) · (11 · (1- 1)1) · (0.52 ·(1- 0.5)2) = 0.062.
4.2.1 MCMC samplingFinding the values of ?k using the MLE method
is straightforward. However, this is not the casefor maximising the likelihood function over thespace of all possible dendrograms. Given a graphwith n vertices, i.e. n leaves in each dendrogram,the total number of different dendrograms is super-exponential ((2n- 3)!! ˜ v2(2n)n-1e-n) (Clausetet al., 2006).
To deal with this problem, we use a MarkovChain Monte Carlo (MCMC) method that samplesdendrograms from the space of dendrogram mod-els with probability proportional to their likelihood.Each time MCMC samples a dendrogram with anew highest likelihood, that dendrogram is stored.Hence, our goal is to choose the highest likelihooddendrogram once MCMC has converged.
Following the work in (Clauset et al., 2008),we pick a set of transitions between dendrograms,where a transition is a re-arrangement of the sub-trees of a dendrogram. In particular, given a currentdendrogram Dcurr, each internal node Dk of Dcurris associated with three subtrees of Dcurr. For in-stance, in Figure 5A, the subtrees st1 and st2 arederived from the two children of Dk and the thirdst3 from its sibling. Given a current dendrogram,Dcurr, the algorithm proceeds as follows:
1. Choose an internal node, Dk ? Dcurr uni-formly.
2. Generate two possible new configurations ofthe subtrees of Dk (See Figure 5).
3. Choose one of the configurations uniformly togenerate a new dendrogram, Dnext.
4. Accept or reject Dnext according toMetropolis-Hastings (MH) rule.
5. If transition is accepted, then Dcurr = Dnext.
6. GOTO 1.
According to MH rule (Newman and Barkema,1999), a transition is accepted if logL(Dnext) =logL(Dcurr); otherwise the transition is acceptedwith probability L(Dnext)L(Dcurr) . These transitions definean ergodic Markov chain, hence its stationary distri-bution can be reached (Clauset et al., 2008).
749
Figure 5: (A) current configuration for internal node Dk and its associated subtrees (B) first alternative configuration,(C) second alternative configuration. Note that swapping st1, st2 in (A) results in an equivalent tree. Hence, thisconfiguration is excluded.
In our experiments, we noticed that the algorithmconverged relatively quickly. The same behaviour(roughly O(n2) steps) was also noticed in Clauset etal. (2008), when considering graphs with thousandsof vertices.
5 HRGs for sense disambiguation
5.1 Sense mappingThe output of HRG learning is a dendrogramD withn leaves (contexts) and n-1 internal nodes. To per-form sense disambiguation, we mapped the internalnodes to gold standard senses using a sense-taggedcorpus. Such a sense-tagged corpus is needed wheninduced word senses need to be mapped to a goldstandard sense inventory.
Instead of using a hard mapping from the den-drogram internal nodes to the Gold Standard (GS)senses, we use a soft probabilistic mapping and cal-culateP (sk|Di), i.e the probability of sense sk givennode Di. Let F (Di) be the set of training contextsgrouped by internal node Di. Let F '(sk) be the setof training contexts that are tagged with sense sk.Then the conditional probability, P (sk|Di), is de-fined in Equation 5.
P (sk|Di) = |F (Di) n F'(sk)|
|F (Di)| (5)
Table 1 provides a sense-tagged corpus for therunning example of Figure 3. Using this corpusand the tree in Figure 4(A), P (s1|D2) = 23 andP (s2|D2) = 13 . In Figure 4(A) the rest of the calcu-lated conditional probabilities are given.
5.2 Sense taggingFor evaluation we compared the proposed methodagainst the current state-of-the-art sense induction
GS sense Context ID Context wordss1 A journal, scholar, observation
science, papers1 B scholar, scholar, author,
publication, papers2 D times, guardian,
journalist, paper
Table 1: Sense-tagged corpus for the example in Figure 3
systems in the WSD task. We followed the settingof SemEval-2007 sense induction task (Agirre andSoroa, 2007). In this setting, the base corpus (bc)(Section 3.1) for a target word consists both of thetraining and testing corpus. As a result, a testingcontext cj of tw is a leaf in the generated dendro-gram. The process of disambiguating cj is straight-forward exploiting the structural information pro-vided by HRGs.
w(sk, cj) =?
Di?H(cj)P (sk|Di) · ?i (6)
w(s*, cj) = argmax sk(w(sk, cj)) (7)
Let H(cj) denote the set of parents for context cj .Then, the weight assigned to sense sk is the sum ofweighted scores provided by each identified parent.This is shown in Equation 6, where ?i is the proba-bility associated with each internal nodeDi from thehierarchical random graph (see Figure 4(A)). Thisprobability reflects the discriminating ability of in-ternal nodes.
Finally, the highest weight determines the win-ning sense for context cj (Equation 7). In our ex-ample (Figure 4(A)), w(s1, C) = (0 ·1+ 23 ·0.25) =0.16 andw(s2, C) = (1·1+ 13 ·0.25) = 1.08. Hence,s2 is the winning sense.
750
Parameter RangeG2 word threshold (p1) 15,25,35,45G2 collocation threshold (p2) 10,15,20Edge similarity threshold (p3) 0.05,0.09,0.13
Table 2: Parameter values used in the evaluation.
6 Evaluation
6.1 Evaluation setting & baselinesWe evaluate our method on the nouns of theSemEval-2007 word sense induction task (Agirreand Soroa, 2007) under the second evaluation settingof that task, i.e. supervised evaluation. Specifically,we use the standard WSD measures of precision andrecall in order to produce their harmonic mean (F-Score). The official scoring software of that task hasbeen used in our evaluation. Note that the unsuper-vised measures of that task are not directly applica-ble to our induced hierarchies, since they focus onassessing flat clustering methods.
The first aim of our evaluation is to test whetherinferring the hierarchical structure of the constructedgraphs improves WSD performance. For that reasonour first baseline, Chinese Whispers Unweightedversion (CWU), takes as input the same unweightedgraph of contexts as HRGs in order to produce aflat clustering. The set of produced clusters is thenmapped to GS senses using the training dataset andperformance is then measured on the testing dataset.We followed the same sense mapping method as inthe SemEval-2007 sense induction task (Agirre andSoroa, 2007).
Our second baseline, Chinese Whispers Weightedversion (CWW), is similar to the previous one, withthe difference that the edges of the input graphare weighted using Equation 1. For clustering thegraphs of CWU and CWW we employ, ChineseWhispers4 (Biemann, 2006).
The second aim of our evaluation is to assesswhether the hierarchical structure inferred by HRGsis more informative than the hierarchical struc-ture inferred by traditional Hierarchical Clustering(HAC). Hence, our third baseline, takes as input asimilarity matrix of the graph vertices and performsbottom-up clustering with average-linkage, whichhas already been used in WSI in (Pantel and Lin,
4The number of iterations for CW was set to 200.
2003) and was shown to have superior or similar per-formance to single-linkage and complete-linkage inthe related problem of learning a taxonomy of senses(Klapaftis and Manandhar, 2010).
To calculate the similarity matrix of vertices wefollow a process similar to the one used in Sec-tion 4.2 for calculating the probability of an inter-nal node. The similarity between two vertices iscalculated according to the degree of connected-ness among their direct neighbours. Specifically,we would like to assign high similarity to pairs ofvertices, whose neighbours are close to forming aclique.
Given two vertices (contexts) ci and cj , letN(ci, cj) be the set of their neighbours andK(ci, cj)be the set of edges between the vertices inN(ci, cj).The maximum number of edges that could exist be-tween vertices in N(ci, cj) is
(|N(ci,cj)|2
). Thus, the
similarity of ci, cj is set equal to the number ofedges that actually exist in that neighbourhood di-vided by the total number of edges that could exist( |K(ci,cj)|(|N(ci,cj)|
2)).
The disambiguation process using the HAC treeis identical to the one presented in Section 5.2 withthe only difference that the internal probability, ?i,in Equation 6 does not exist for HAC. Hence, we re-placed it with the factor 1|H(Di)| , whereH(Di) is theset of children of internal node Di. This factor pro-vides lower weights for nodes high in the tree, sincetheir discriminating ability will possibly be lower.
6.2 Results & discussion
Table 2 shows the parameter values used in the eval-uation. Figure 6(A) shows the performance of theproposed method against the baselines for p3 = 0.05and different p1 and p2 values. Figure 6(B) il-lustrates the results of the same experiment usingp3 = 0.09. In both figures, we observe that HRGsoutperform the CWU baseline under all parametercombinations. In particular, all of the 12 perfor-mance differences for p3 = 0.09 are statisticallysignificant using McNemar’s test at 95% confidencelevel, while for p3 = 0.05 only 2 out of the 12 per-formance differences were not judged as significantfrom the test.
The picture is the same for p3 = 0.13, whereCWU performs significantly worse than for p3 =
751
Figure 6: Performance analysis of HRGs, CWU, CWW & HAC for different parameter combinations (Table 2). (A)All combinations of p1, p2 and p3 = 0.05. (B) All combinations of p1, p2 and p3 = 0.09.
0.05 and p3 = 0.09. Specifically, the largest perfor-mance difference between HRGs and CWU is 9.4%at p1 = 25, p2 = 10 and p3 = 0.13. Setting the ver-tex similarity threshold (p3) equal to 0.13 leads tomore sparse and disconnected graphs, which causesChinese Whispers to produce a large number of clus-ters. This leads to sparsity problems and unreliablemapping of clusters to GS senses due to the lack ofadequate training data. In contrast, HRGs suffer lessat this high threshold, although their performancewhen p3<0.13 is better.
This picture does not change for the weighted ver-sion of Chinese Whispers (CWW) which performsworse than CWU. This is because CWW producesa smaller number of clusters than CWU that con-flate the target word senses. It seems that usingweighted edges creates a bias towards the MFS, ineffect missing rare senses of a target word. Thismeans that a number of words in the bag-of-wordscontext vectors and collocations in the collocationalcontext vectors (Section 3.2) are associated to morethan one sense of the target word and most stronglyassociated to the MFS. As a result, increasing the p1threshold to 25 and 35 leads to a higher performancefor CWW, since many of these words and colloca-tions are filtered out.
Overall, the comparison of HRGs against theCWU and CWW baselines has shown that inferringthe hierarchical structure of observed graphs leadsto improved WSD performance as opposed to usingflat clustering. This is because HRGs are able to in-
fer both the hierarchical structure of the graph andinclude the probabilities, ?k, associated with eachinternal node. These probabilities reflect the dis-criminating ability of each node, offering informa-tion missed by flat clustering.
In Figures 6(A) and 6(B) we observe that HRGsperform significantly better than HAC. In particular,all of their performance differences are statisticallysignificant for these parameter values. The largestperformance difference is 6.0% at p1 = 45, p2 = 10and p3 = 0.05. However, this picture is not the samewhen considering a higher context similarity thresh-old (p3 = 0.13) as Figure 7 shows. In particular,HRGs and HAC perform similarly for p3 = 0.13,while the majority of performance differences arenot statistically significant.
The similar behaviour of HRGs and HAC at thisthreshold is caused both by the worse performanceof HRGs and the improved performance of HAC asopposed to lower p3 values. As it has been men-tioned, setting p3 = 0.13 leads to sparse and dis-connected graphs. Additionally, the likelihood func-tion (Equation 3) is maximised when the probabil-ity, ?k, of an internal node, Dk, approaches 0 or 1.This creates a bias towards dendrograms, in which alarge number of internal nodes have zero probabil-ity. These dendrograms might be a good-fit to theobserved graph, but not to the GS.
In contrast, HAC is less affected, because it neverconsiders creating an internal node, when the max-imum similarity among any pair of two candidate
752
Figure 7: Performance of HRGs and HAC for differentparameter combinations (Table 2). All combinations ofp1, p2 and p3 = 0.13.
subtrees is zero. Additionally, our experiments showthat HAC is unable to deal with noise when con-sidering sparse graphs (p3<0.13). For that reason,the F-Score of HAC increases as the edge similaritythreshold decreases.
To further investigate this issue and test whetherHAC is able to achieve a higher F-Score than HRGsin higher p3 values, we executed two more experi-ments for HAC and HRGs increasing p3 to 0.17 and0.21 respectively. In the first case we observed thatthe performance of HAC remained relatively stablecompared to p3 = 0.13, while in the second case theperformance of HAC decreased as Figure 7 shows.In both cases, HAC performed significantly betterthan HRGs.
Overall, the comparison of HRGs against HAChas shown that HRGs perform significantly betterthan HAC when considering connected or less sparsegraphs (p3<0.13). This is due to the fact that HACcreates dendrograms, in which connections withinthe clusters are dense, while connections betweenthe clusters are sparse, i.e. it only considers assorta-tive structures. In contrast, HRGs also consider dis-assortative dendrograms, i.e. dendrograms in whichvertices are less likely to be connected on smallscales than on large ones, as well as mixtures ofassortative and disassortative (Clauset et al., 2008).This is achieved by allowing the probability ?k ofa node k to vary arbitrarily throughout the dendro-gram.HAC performs similarly or better than HRGs for
largely disconnected and sparse graphs, becauseHRGs become biased towards disassortative treeswhich are not a good fit to the GS (Figure 7). De-spite that, our evaluation has also shown that the bestperformance of HAC (F-Score = 86.0% at p1 = 15,p2 = 10, p3 = 0.13) is significantly lower thanthe best performance of HRGs (F-Score = 87.6% atp1 = 35, p2 = 10, p3 = 0.09).
6.3 Comparison to state-of-the-art methodsTable 3 compares the best performing parametercombination of our method against state-of-the-artmethods. Table 3 also includes the best performanceof our baselines, i.e HAC, CWU and CWW.
Brody & Lapata (2009) presented a sense induc-tion method that is related to Latent Dirichlet Al-location (Blei et al., 2003). In their work, theymodel the target word instances as samples from amultinomial distribution over senses which are suc-cessively characterized as distributions over words(Brody and Lapata, 2009). A significant advantageof their method is the inclusion of more than onelayer in the LDA setting, where each layer corre-sponds to a different feature type e.g. dependencyrelations, bigrams, etc. The inclusion of differentfeature types as separate models in the sense in-duction process can easily be modeled in our set-ting, by inferring a different hierarchy of target wordinstances according to each feature type, and thencombining all of them to a consensus tree. In thiswork, we have focused on extracting a single hierar-chy combining word co-occurrence and bigram fea-tures.
Niu et al. (2007) developed a vector-basedmethod that performs sense induction by group-ing the contexts of a target word using three typesof features, i.e. POS tags of neighbouring words,word co-occurrences and local collocations. The se-quential information bottleneck algorithm (Slonimet al., 2002) is applied for clustering. HRGs performslightly better than the methods of Brody & Lap-ata (2009) and Niu et al. (2007), although the dif-ferences are not significant (McNemar’s test at 95%confidence level).
Klapaftis & Manandhar (2008) developed agraph-based sense induction method, in which ver-tices correspond to collocations related to the tar-get word and edges between vertices are drawn ac-
753
System Performance (%)HRGs 87.6(Brody and Lapata, 2009) 87.3(Niu et al., 2007) 86.8(Klapaftis and Manandhar, 2008) 86.4HAC 86.0CWU 85.1CWW 84.7(Pedersen, 2007) 84.5MFS 80.9
Table 3: HRGs against recent methods & baselines.
cording to the co-occurrence frequency of the cor-responding collocations. The constructed graph issmoothed to identify more edges between verticesand then clustered using Chinese Whispers (Bie-mann, 2006). This method is related to the basicinputs of our presented method. Despite that, it isa flat clustering method that ignores the hierarchicalstructure exhibited by observed graphs. The previ-ous section has shown that inferring the hierarchicalstructure of graphs leads to superior WSD perfor-mance.
Pedersen (2007) presented SenseClusters, avector-based method that clusters second order co-occurrence vectors using k-means, where k is auto-matically determined using the Adapted Gap Statis-tic (Pedersen and Kulkarni, 2006). As can be ob-served, HRGs perform significantly better than themethods of Pedersen (2007) and Klapaftis & Man-andhar (2008) (McNemar’s test at 95% confidencelevel).
Finally, Table 3 shows that the best performingparameter combination of HRGs achieves a signifi-cantly higher F-Score than the best performing pa-rameter combination of HAC, CWU and CWW. Fur-thermore, HRGs outperform the most frequent sensebaseline by 6.7%.
7 Conclusion & future work
We presented an unsupervised method for inferringthe hierarchical grouping of the senses of a polyse-mous word. Our method creates a graph, in whichvertices correspond to contexts of a polysemous tar-get word and edges between them are drawn ac-cording to their similarity. The hierarchical randomgraphs algorithm (Clauset et al., 2008) was applied
to the constructed graph in order to infer its hierar-chical structure, i.e. binary tree.
The learned tree provides an induction of thesenses of a given word at different levels of sensegranularity and was applied to the problem of WSD.The WSD process mapped the tree’s internal nodesto GS senses using a sense tagged corpus, and thentagged new instances by exploiting the structural in-formation provided by the tree.
Our experimental results have shown that ourgraphs exhibit hierarchical organisation that canbe captured by HRGs, in effect providing im-proved WSD performance compared to flat cluster-ing. Additionally, our comparison against hierarchi-cal agglomerative clustering with average-linkagehas shown that HRGs perform significantly betterthan HAC when the graphs do not suffer from spar-sity (disconnected graphs). The comparison withstate-of-the-art sense induction systems has shownthat our method yields improvements.
Our future work focuses on using different featuretypes, e.g. dependency relations, second-order co-occurrences, named entities and others to constructour undirected graphs and then applying HRGs, inorder to measure the impact of each feature typeon the induced hierarchical structures within a WSDsetting. Moreover, following the work in (Clauset etal., 2008), we are also working on using MCMC inorder to sample more than one dendrogram at equi-librium, and then combine them to a consensus tree.This consensus tree might be able to express a largeramount of topological features of the initial undi-rected graph.
Finally in terms of evaluation, our future workalso focuses on evaluating HRGs using a fine-grained sense inventory, extending the evaluation onthe SemEval-2010 WSI task dataset (Manandhar etal., 2010) as well as applying HRGs to other relatedtasks such as taxonomy learning.
Acknowledgements
This work is supported by the European Com-mission via the EU FP7 INDECT project, GrantNo.218086, Research area: SEC-2007-1.2-01 Intel-ligent Urban Environment Observation System. Theauthors would like to thank the anonymous review-ers for their useful comments.
754
References
Eneko Agirre and Aitor. Soroa. 2007. Semeval-2007Task 02: Evaluating Word Sense Induction and Dis-crimination Systems. In Proceedings of SemEval-2007, pages 7–12, Prague, Czech Republic.
Eneko Agirre, David Marti´nez, Oier Lo´pez de Lacalle,and Aitor Soroa. 2006. Two Graph-based Algorithmsfor State-of-the-art WSD. In Proceedings of EMNLP-2006, pages 585–593, Sydney, Australia.
Chris Biemann. 2006. Chinese Whispers - An EfficientGraph Clustering Algorithm and its Application toNatural Language Processing Problems. In Proceed-ings of TextGraphs, pages 73–80, New York, USA.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003. Latent Dirichlet Allocation. J. Mach. Learn.Res., 3:993–1022.
Sergey Brin and Lawrence Page. 1998. The Anatomyof a Large-Scale Hypertextual Web Search Engine.Comput. Netw. ISDN Syst., 30(1-7):107–117.
Samuel Brody and Mirella Lapata. 2009. Bayesian WordSense Induction. In Proceedings of EACL-2009, pages103–111, Athens, Greece. ACL.
Aaron Clauset, Cristopher Moore, and Mark E. J. New-man. 2006. Structural Inference of Hierarchies in Net-works. In Proceedings of the ICML-2006 Workshopon Social Network Analysis, pages 1–13, Pittsburgh,USA.
Aaron Clauset, Cristopher Moore, and Mark E. J. New-man. 2008. Hierarchical Structure and the Predictionof Missing Links in Networks. Nature, 453(7191):98–101.
Stijn Dongen. 2000. Performance Criteria for GraphClustering and Markov Cluster Experiments. Tech-nical report, CWI (Centre for Mathematics and Com-puter Science), Amsterdam, The Netherlands.
Beate Dorow and Dominic Widdows. 2003. DiscoveringCorpus-specific Word Senses. In Proceedings of theEACL-2003, pages 79–82, Budapest, Hungary.
Ted Dunning. 1993. Accurate Methods for the Statisticsof Surprise and Coincidence. Computational Linguis-tics, 19(1):61–74.
Phil Edmonds and Beate Dorow. 2001. Senseval-2:Overview. In Proceedings of SensEval-2, pages 1–5,Toulouse, France.
Ioannis P. Klapaftis and Suresh Manandhar. 2008. WordSense Induction Using Graphs of Collocations. InProceedings of ECAI-2008, pages 298–302, Patras,Greece.
Ioannis P. Klapaftis and Suresh Manandhar. 2010. Tax-onomy Learning Using Word Sense Induction. In Pro-ceedings of NAACL-HLT-2010, pages 82–90, Los An-geles, California, June. ACL.
Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dli-gach, and Sameer S. Pradhan. 2010. Semeval-2010Task 14: Word Sense Induction & Disambiguation. InProceedings of SemEval-2, Uppsala, Sweden. ACL.
Rada Mihalcea. 2004. Graph-based Ranking Algorithmsfor Sentence Extraction, Applied to Text Summariza-tion. In Proceedings of the ACL 2004 on Interactiveposter and demonstration sessions, page 20, Morris-town, NJ, USA.
Mark Newman and Gerard Barkema. 1999. Monte CarloMethods in Statistical Physics. Oxford: ClarendonPress, New York, USA.
Zheng-Yu Niu, Dong-Hong Ji, and Chew-Lim Tan. 2007.I2R: Three Systems for Word Sense Discrimination,Chinese Word Sense Disambiguation, and EnglishWord Sense Disambiguation. In Proceedings ofSemEval-2007, pages 177–182, Prague, Czech Repub-lic.
Patrick Pantel and Dekang Lin. 2003. AutomaticallyDiscovering Word Senses. In Proceedings of NAACL-HLT-2003, pages 21–22, Morristown, NJ, USA.
Ted Pedersen and Anagha Kulkarni. 2006. AutomaticCluster Stopping With Criterion Functions and the gapStatistic. In Proceedings of the 2006 Conference of theNorth American Chapter of the ACL on Human Lan-guage Technology, pages 276–279, Morristown, NJ,USA.
Ted Pedersen. 2007. UMND2 : Senseclusters Applied tothe Sense Induction Task of Senseval-4. In Proceed-ings of SemEval-2007, pages 394–397, Prague, CzechRepublic.
Daniel Ramage, Anna N. Rafferty, and Christopher D.Manning. 2009. Random Walks for Text SemanticSimilarity. In Proceedings of TextGraphs-4, Suntec,Singapore, August.
Noam Slonim, Nir Friedman, and Naftali Tishby. 2002.Unsupervised Document Classification Using Sequen-tial Information Maximization. In SIGIR 2002, pages129–136, New York, NY, USA. ACM.
Benjamin Snyder and Martha Palmer. 2004. The En-glish All-words Task. In Rada Mihalcea and Phil Ed-monds, editors, In Proceedings of Senseval-3, pages41–43, Barcelona, Spain.
Jean Ve´ronis. 2004. Hyperlex: Lexical Cartography forInformation Retrieval. Computer Speech & Language,18(3):223–252.
Julie Weeds, David Weir, and Diana McCarthy. 2004.Characterising Measures of Lexical DistributionalSimilarity. In Proceedings of COLING-2004, pages10–15, Morristown, NJ, USA.
Dominic Widdows and Beate Dorow. 2002. A GraphModel for Unsupervised Lexical Acquisition. In Pro-ceedings of Coling-2002, pages 1–7, Morristown, NJ,USA.
755
