KnowNet: A Proposal for Building Highly Connected and Dense Knowledge Bases from the Web 
Montse Cuadros TALP Research Center, UPC, Barcelona (Spain) 
email: cuadros@lsi.upc.edu 
German Rigau IXA NLP Group, UPV/EHU, Donostia (Spain) 
email: german.rigau@ehu.es 
Abstract 
This paper presents a new fully automatic method for building highly denseand accurateknowledgebasesfromexisting semanticresources. Basically, the method uses a wide-coverage and accurate knowledge­basedWordSenseDisambiguation algorithm to assign the most appro­priatesensestolargesets of topically related wordsacquiredfromthe web. KnowNet, the resultingknowledge-base which connectslarge sets of semantically-relatedconceptsis a major steptowardsthe autonomous acquisition ofknowledgefrom raw corpora. Infact,KnowNetis several times larger than any available knowledge resource encoding relations between synsets, andtheknowledgethatKnowNet contains outperform anyotherresourcewhenempiricallyevaluatedin a commonmultilingual framework. 
71 
1 Introduction 
Usinglarge-scaleknowledgebases,suchasWordNet(Fellbaum,1998),hasbecomea usual, often necessary,practicefor most currentNaturalLanguageProcessing(NLP) systems. Even now, building large and rich enough knowledge bases for broad– coverage semantic processing takes a greatdeal of expensive manual effortinvolv­ing large research groups during long periods of development. In fact, hundreds ofperson-yearshavebeeninvestedin thedevelopment of wordnetsfor variouslan­guages(Vossen,1998). For example,in morethan tenyears ofmanual construction (from1995to2006,thatisfromversion1.5to3.0),WordNetpassedfrom103,445to 235,402semanticrelations1.Butthisdatadoesnotseemstoberichenoughtosupport advancedconcept-basedNLPapplicationsdirectly.It seemsthat applicationswill not scale upto workingin opendomains without moredetailed and richgeneral-purpose (andalsodomain-speci.c)semanticknowledgebuiltbyautomaticmeans.Obviously, thisfacthas severelyhamperedthe state-of-the-artofadvancedNLP applications. 
However, thePrincetonWordNetisbyfar the most widely-usedknowledgebase (Fellbaum,1998). Infact,WordNetisbeing used world-wideforanchoringdiffer­enttypesofsemanticknowledgeincludingwordnetsforlanguagesotherthanEnglish (Atserias etal.,2004),domainknowledge(Magnini andCavaglià,2000) or ontolo­gieslikeSUMO(Niles andPease,2001)ortheEuroWordNetTopConceptOntology (Álvez etal.,2008). It contains manually codedinformation about nouns, verbs, ad­jectives andadverbs in English and is organised around the notion of a synset.A synsetisasetofwordswiththesamepart-of-speechthatcanbeinterchangedinacer­tain context. For example, <party, political_party> form a synsetbecause theycan be usedto refertothe same concept.Asynsetis oftenfurtherdescribedby agloss,in thiscase: "anorganisationtogainpoliticalpower"andby explicit semanticrelations to other synsets. 
Fortunately,duringthelastyearstheresearchcommunityhasdevisedalargesetof innovative methods andtoolsforlarge-scale automatic acquisition oflexicalknowl­edgefrom structured and unstructured corpora. Among others we can mention eX­tendedWordNet(Mihalcea andMoldovan,2001),large collections of semanticpref­erences acquiredfromSemCor(Agirre andMartinez,2001,2002) or acquiredfrom British National Corpus(BNC)(McCarthy,2001), large-scaleTopicSignatures for eachsynset acquiredfromtheweb(AgirreanddelaCalle,2004)orknowledgeabout individualsfromWikipedia(Suchaneketal.,2007).Obviously,allthese semantic re­sourceshavebeenacquiredusing averydifferentset ofprocesses(Snowetal.,2006), tools and corpora. Infact, each semantic resourcehasdifferent volume and accuracy .gures when evaluatedin a common and controlledframework(Cuadros andRigau, 2006). 
However,notallavailablelarge-scaleresourcesencodesemanticrelationsbetween synsets.Insomecases, onlyrelationsbetweensynsets and wordshavebeen acquired. Thisis the case oftheTopicSignatures(Agirre et al.,2000) acquiredfrom the web (AgirreanddelaCalle,2004).Thisis oneofthelargestsemanticresourceseverbuilt with around onehundred million relationsbetween synsets and semantically related 
1Symmetric relations are countedonly once. 
words.2 
A knowledge net or KnowNet, is an extensible, large and accurate knowledge base, whichhasbeenderivedby semanticallydisambiguating theTopicSignatures acquiredfromthe web. Basically,the method uses a robust and accurateknowledge­basedWordSenseDisambiguationalgorithmto assignthemost appropriatesenses to the topic words associated to a particular synset. The resulting knowledge-base whichconnectslargesets oftopically-relatedconceptsis a majorsteptowardsthe au­tonomous acquisition ofknowledgefrom rawtext. Infact,KnowNetis severaltimes largerthanWordNetandtheknowledgecontainedinKnowNetoutperformsWordNet when empiricallyevaluatedin a commonframework. 
Table1comparesthedifferent volumes of semantic relationsbetween synsetpairs of availableknowledgebases andthe newly createdKnowNets3. 
Table1:Numberofsynset relations 

Princeton WN3.0  235,402  
SelectionalPreferences from SemCor  203,546  
eXtendedWN  550,922  
Co-occurringrelations from SemCor  932,008  
New KnowNet-5  231,163  
New KnowNet-10  689,610  
New KnowNet-15  1,378,286  
New KnowNet-20  2,358,927  

Varyingfrom.vetotwentythenumberofprocessedwordsfromeachTopicSigna­ture,wecreatedautomaticallyfourdifferentKnowNetswithmillionsofnewsemantic relationsbetweensynsets. 
Afterthisintroduction,Section2describestheTopicSignatures acquiredfromthe web.Section3presentsthe approachweplantofollowforbuildinghighlydense and accurateknowledgebases. Section4describesthe methods wefollowedforbuilding KnowNet.InSection5, wepresentthe evaluationframeworkusedinthis study.Sec­tion6describestheresultswhenevaluatingdifferentversionsofKnowNetand.nally, Section7presents some concludingremarksandfuturework. 
2 Topic Signatures 
TopicSignatures(TS) are word vectors related to aparticular topic(Lin andHovy, 2000). TopicSignatures arebuiltby retrieving context words of a target topicfrom large corpora. In our case, we consider wordsenses astopics. Basically,the acquisi­tion ofTSconsists of: 
• 	acquiringthebestpossiblecorpusexamplesforaparticularwordsense(usually characterisingeachwordsenseasaqueryandperformingasearchonthecorpus 
2Available athttp://ixa.si.ehu.es/Ixa/resources/sensecorpus 
3TheseKnowNetversions canbedownloadedfrom http://adimen.si.ehu.es 

tammany#n  0.0319  
alinement#n  0.0316  
federalist#n  0.0315  
whig#n  0.0300  
missionary#j  0.0229  
Democratic#n  0.0218  
nazi#j  0.0202  
republican#n  0.0189  
constitutional#n  0.0186  
organization#n  0.0163  

forthose examplesthatbestmatchthequeries) 
• 	buildingtheTSbyderivingthecontextwordsthatbestrepresentthewordsense fromthe selectedcorpora. 
TheTopicSignatures acquiredfromthe web(hereinafterTSWEB)constitutes one ofthelargestavailablesemanticresourceswitharound100millionrelations(between synsets and words)(Agirre anddelaCalle,2004). Inspiredby the work ofLeacock et al.(1998),TSWEB was constructed using monosemous relativesfromWN(syn­onyms,hypernyms,directandindirecthyponyms,andsiblings),queryingGoogleand retrieving up toonethousand snippetsperquery(thatis, aword sense), extracting the salient words withdistinctivefrequency usingTFIDF.Thus,TSWEB consist of a large orderedlistof words with weights associatedto each ofthe senses ofthepoly­semous nouns ofWordNet1.6.The number of constructedtopic signaturesis35,250 withanaveragesizepersignatureof6,877words.WhenevaluatingTSWEB,weused atmaximumthe .rst700wordswhileforbuildingKnowNetweusedatmaximumthe .rst20 words. 
Forexample,Table2presentthe .rstwords(lemmasandpart-of-speech) and weights oftheTopicSignatureacquiredforparty#n#1. 
3 Building highly connected and dense knowledge bases 
Itis ourbelief, that accurate semanticprocessing(such asWSD) would rely not only on sophisticated algorithms but on knowledge intensive approaches. In fact, the cyclingarquitecture oftheMEANING4projectdemonstratedthatacquiringbet­terknowledge allow toperformbetterWordSenseDisambiguation(WSD) and that havingimprovedWSDsystems we areableto acquirebetterknowledge(Rigau et al., 2002). 
Thus, we plan to acquire by fully automatic means highly connected and dense knowledgebasesfromlargecorporaorthewebbyusingtheknowledgealreadyavail­able,increasingthe total number of relationsfromless than one million(the current numberof available relations)to millions. 
4http://www.lsi.upc.edu/~nlp/meaning 
The currentproposalconsistof: 
• 	
tofollowCuadroset al.(2005) andCuadrosandRigau(2006)foracquiring highly accurateTopicSignaturesfor all monosemous wordsin WordNet(for instance,usingInfoMap(DorowandWiddows,2003)). Thatis,toacquire wordvectorscloselyrelatedtoaparticularmonosemousword(forinstance,air­port#n#1)fromBNC or otherlarge text collectionslikeGigaWord,Wikipedia orthe web. 

• 	
to applya veryaccurateknowledge–basedall–wordsdisambiguationalgorithm totheTopicSignaturesin orderto obtain sense vectorsinstead of word vectors (forinstance,usingaversionofStructuralSemanticInterconnectionsalgorithm (SSI)(NavigliandVelardi,2005)). 


Forinstance,considerthe .rsttenweighted words(withPart-of-Speech) appear­ingin theTopicSignature(TS) of the word sense airport#n#1 correspondingto the monosemous word airport, as shown in Table 3. This TS has been obtained from BNC usingInfoMap. FromthetenwordsappearingintheTS,twoof themdonot appearinWN(correspondingto theproper namesheathrow#n andgatwick#n),four words are monosemous(airport#n,air.eld#n,travelling#nandpassenger#n)andfour otherarepolysemous(.ight#n,train#n,station#n andferry#n). 
Table 3: First ten words with weigths and number of senses in WN of the Topic Signaturefor airport#n#1obtainedfromBNCusingInfoMap 

airport#n  1.000000  1  
heathrow#n  0.843162  0  
gatwick#n  0.768215  0  
.ight#n  0.765804  9  
air.eld#n  0.740861  1  
train#n  0.739805  6  
travelling#n  0.732794  1  
passenger#n  0.722912  1  
station#n  0.722364  4  
ferry#n  0.717653  2  

SSI-Dijkstra 
WehaveimplementedaversionoftheStructuralSemanticInterconnectionsalgorithm (SSI), aknowledge-basediterative approachtoWordSenseDisambiguation(Navigli andVelardi,2005).TheSSI algorithmis very simple and consists of aninitialisation stepandasetofiterativesteps.GivenW,anorderedlistofwordstobedisambiguated, theSSIalgorithmperformsasfollows.Duringtheinitialisation step,all monosemous wordsareincludedintothesetI of alreadyinterpreted words, and thepolysemous words areincludedinP(all ofthempendingtobedisambiguated). At each step,the 
Table4:Minimumdistancesfromairport#n#1 
Synsets  Distance  
4  6  
4530  5  
64713  4  
29767  3  
597  2  
20  1  
1  0  

setIis usedtodisambiguate one word ofP, selectingthe word sense whichis closer tothe setIof alreadydisambiguated words.Once a senseisdisambiguated,the word senseisremovedfromP andincludedintoI.Thealgorithm .nisheswhennomore pendingwords remaininP. 
Initially,thelistIofinterpretedwordsshouldincludethesensesofthemonosemous wordsinW,ora.xedsetofwordsenses5.However,inthiscase,whendisambiguating aTSderivedfrom a monosemous word m, thelistIincludes since thebeginning at leastthe sense ofthe monosemouswordm (in ourexample,airport#n#1). 
Inordertomeasuretheproximityofonesynset(ofthewordtobedisambiguatedat each step)to a set of synsets(those word senses alreadyinterpretedinI),the original SSIuses anin-houseknowledgebasederived semi-automatically whichintegrates a variety of online resources(Navigli,2005).This very richknowledge-baseis usedto calculategraphdistancesbetweensynsets.Inordertoavoidtheexponentialexplosion ofpossibilities, notallpathsareconsidered. They used acontext-freegrammarof relationstrainedonSemCorto .lter-outinappropriatepathsandtoprovideweightsto the appropriatepaths. 
Instead, we usepartoftheknowledge already availabletobuild a verylarge con­nectedgraphwith99,635nodes(synsets)and636,077edges(thesetofdirectrelations between synsetsgatheredfromWordNetand eXtendedWordNet).Onthatgraph, we used a very ef.cientgraphlibrary to computetheDijkstra algorithm.6 TheDijkstra algorithmis agreedy algorithmthat computesthe shortestpathdistancebetween one node an the rest of nodes of a graph. In that way, we can compute very ef.ciently the shortestdistancebetween anytwogiven nodesofagraph.This versionoftheSSI algorithmis called SSI-Dijkstra. 
Forinstance,Table4showsthevolumesoftheminimumdistancesfromairport#n#1 to the rest of the synsets of thegraph. Interestingly,from airport#n#1 all synsets of the graph are accessible following paths of at maximum six edges. While there is only one synset atdistance zero(airport#n#1) andtwenty synsetsdirectly connected to airport#n#1,95%ofthetotalgraphis accessible atdistancefourorless. 
SSI-Dijkstrahasveryinterestingproperties.Forinstance,SSI-Dijkstraalwayspro­
5Ifno monosemous words arefound orif noinitial senses areprovided, the algorithm could make an initialguessbasedonthe mostprobable sense oftheless ambiguous word ofW. 
6Seehttp://www.boost.org 
vides an answer when comparingthedistancesbetweenthe synsets of a word and all the synsets alreadyinterpretedin I.Thatis, theDijkstra algorithm always provides an answerbeingthe minimumdistance close orfar7. At each step, theSSI-Dijkstra algorithmselectsthe synset whichis closertoI(the set of alreadyinterpretedwords). 
Table5presentsthe resultoftheword–sensedisambiguationprocesswiththeSSI-Dijkstra algorithm on the TS presented in Table 38. Now, part of the TS obtained fromBNC usingInfoMaphavebeendisambiguated at a synsetlevel resulting on a word–sensedisambiguatedTS.ThosewordsnotpresentinWN1.6havebeenignored (heathrow andgatwick). Some others,beingmonosemousinWordNet were consid­ered alreadydisambiguated(travelling,passenger, airport and air.eld). But the rest, havebeen correctlydisambiguated(.ightwith nine senses,train with six senses, sta­tion withfourandferrywithtwo). 
Table5:SensedisambiguatedTSfor airport#n#1obtainedfromBNCusingInfoMap andSSI-Dijkstra 
. 
Word  Offset-WN  Weight  Gloss  
.ight#n  00195002n  0.017  a scheduled tripbyplane between designated airports  
travelling#n  00191846n  0  the act ofgoingfrom one place to another  
train#n  03528724n  0.012  a line of railway cars coupledtogether anddrawn bya locomotive  
passenger#n  07460409n  0  a person travellingin a vehicle (a boat or bus or car or plane or train etc) who is not operatingit  
station#n  03404271n  0.019  a building equipped with special equipment and personnelfor a particular purpose  
airport#n  02175180n  0  an air.eld equipped with controltower andhangers as well as accommodations for passengers and cargo  
ferry#n  02671945n  0.010  a boat that transports people or vehicles across a bodyof water and operates on a regular schedule  
air.eld#n  02171984n  0  a place where planes take off andland  

This sensedisambiguatedTS representssevendirect newsemanticrelationsbe­tweenairport#n#1andthe .rst wordsoftheTS.It couldbedirectlyintegratedintoa newknowledgebase(forinstance,airport#n#1–related–> .ight#n#9),butalsoallthe indirectrelationsofthedisambiguatedTS(forinstance,.ight#n#9 –related–> trav­elling#n#1). In that way,having n disambiguatedword senses, a total of (n 2 - n)/2 relations couldbe created. Thatis,fortheteninitial words oftheTS of airport#n#1, twenty-eightnewdirectrelationsbetween synsets couldbe created. 
Thisprocess couldbe repeatedfor all monosemous words ofWordNet appearing inthe selectedcorpus. Thetotal number of monosemous wordsinWN1.6is98,953. Obviously, not all these monosemous words are expected to appear in the corpus. However, we expectto obtainin thatway several millions of new semantic relations between synsets. This methodwill allow toderivebyfully automatic means ahuge knowledgebase withmillions of new semanticrelations. 
7In contrast, the originalSSIalgorithm not alwaysprovides apathdistancebecauseitdepends on the grammar. 
8Ittook4.6secondstodisambiguate theTS on a modernpersonal computer. 
Furthermore,this approachis completelylanguageindependent. It couldbere­peatedfor anylanguagehavingwords connectedtoWordNet. 
It remainsforfurther study and research,how to convert the relations createdin thatwayto morespeci.c andinformedrelations. 
4 Building KnowNet 
As aproof of concept, wedevelopedKnowNet(KN), alarge-scaleand extensible knowledgebase obtainedbyapplyingtheSSI-Dijkstra algorithmto eachtopic signa­turefromTSWEB.Thatis,insteadofusingInfoMapandalargecorporaforacquiring newTopicSignaturesforallthemonosemoustermsinWN,weusedthealreadyavail­ableTSWEB.Wehavegeneratedfourdifferent versions ofKonwNet applyingSSI-Dijkstratothe .rst5,10,15 and20 wordsforeachTS.SSI-Dijkstraused only the knowledgepresentinWordNetand eXtendedWordNet which consist of a verylarge connectedgraphwith99,635 nodes(synsets) and636,077edges(semanticrelations). 
WegeneratedeachKnowNetby applyingtheSSI-Dijkstra algorithmto the whole TSWEB(processingthe .rstwordsof each ofthe35,250topicsignatures).Foreach TS, we obtainedthedirectandindirect relationsfromthetopic(a word sense)to the disambiguatedword senses oftheTS.Then, as explainedinSection3, we alsogen­eratedtheindirectrelationsforeachTS.Finally,we removedsymmetricandrepeated relations. 
Table6shows thepercentage ofthe overlappingbetween eachKnowNet with re­specttheknowledgecontainedintoWordNetandeXtendedWordNet,thetotalnumber of relations and synsets of each resource.Forinstance,only an8,6% ofthetotal rela­tionsincludedintoWN+XWN are alsopresentinKN-20.This meansthatthe rest of relationsfromKN-20 are new.Thistablealso showsthedifferentKnowNetvolumes. 
As expected, eachKnowNetis verylarge, rangingfromhundreds ofthousandsto millions of new semantic relationsbetween synsets amongincreasing sets of synsets. Surprisingly, the overlapping between the semantic relations of KnowNet and the knowledgebasesusedforbuilding theSSI-Dijkstragraph(WordNet and eXtended WordNet)is verysmall,possiblyindicatingdisjuncttypes ofknowledge. 
Table6:Size andpercentageofoverlappingrelationsbetweenKnowNetversionsand WN+XWN 
KB  WN+XWN  #relations  #synsets  
KN-5  3.2%  231,164  39,837  
KN-10  5.4%  689,610  45,770  
KN-15  7.0%  1,378,286  48,461  
KN-20  8.6%  2,358,927  50,705  

Table 7 presents the percentage of overlapping relations between KnowNet ver­sions. The uppertriangularpart of the matrixpresentsthe overlappingpercentage coveredbylargerKnowNet versions.Thatis, most of theknowledgefromKN-5is also containedinlargerversionsofKnowNet.Interestingly,theknowledgecontained into KN-10 is only partially covered by KN-15 and KN-20. The lower triangular 
KnowNet:AProposalforBuildingKnowledgeBasesfromtheWeb part of the matrixpresents the overlappingpercentage coveredby smallerKnowNet versions. Table7:PercentageofoverlappingrelationsbetweenKnowNet versions 
overlapping  KN-5  KN-10  KN-15  KN-20  
KN-5 KN-10 KN-15 KN-20  100 31,2 16,4 9,5  93,3 100 44,4 26,0  97,7 88,5 100 56,7  97,2 88,9 97.14 100  

5 Evaluation framework 
In orderto empiricallyestablish the relativequality oftheseKnowNet versions with respect already available semantic resources, we usedthe noun-set ofSenseval-3En­glishLexicalSampletaskwhich consists of20 nouns. 
Tryingtobeasneutralaspossiblewithrespecttotheresourcesstudied,weapplied systematicallythe samedisambiguation methodto all ofthem. Recall that our main goalistoestablishafaircomparisonoftheknowledgeresourcesratherthanproviding thebestdisambiguationtechniquefor aparticularresource.Thus,allthe semanticre­sources studied are evaluated asTopicSignatures.Thatis, word vectors with weights associatedto aparticular synset(topic) which are obtainedby collecting those word senses appearinginthe synsetsdirectly relatedtothetopics. 
A commonWSD methodhasbeen applied to allknowledge resources. A simple word overlapping counting is performed between the Topic Signature and the test example9.The synsethavinghigheroverlappingword countsis selected.Infact,this is a verysimpleWSD method which only considers the topicalinformation around the wordtobedisambiguated. Allperformances are evaluated on thetestdata using the .ne-grainedscoringsystemprovidedbytheorganisers.Finally,weshouldremark that the results are not skewed(forinstance,for resolvingties)by the mostfrequent senseinWN or any other statisticallypredictedknowledge. 
5.1 Baselines 
Wehavedesigneda number ofbaselinesin order to establish a complete evaluation frameworkfor comparingtheperformance ofeach semantic resource on theEnglish WSDtask. 
RANDOM:For eachtarget word,this method selects a random sense. Thisbase­line canbe consideredas alower-bound. 
SEMCOR-MFS:Thisbaseline selects the mostfrequent sense ofthe target word inSemCor. 
WN-MFS:Thisbaselineis obtainedby selectingthe mostfrequentsense(the .rst senseinWN1.6)ofthetargetword.WordNetword-senseswererankedusingSemCor andothersense-annotatedcorpora.Thus,WN-MFSandSemCor-MFSaresimilar,but not equal. 
9We also considerthe multiwordterms. 
TRAIN-MFS:Thisbaseline selectsthe mostfrequentsenseinthetraining corpus ofthetargetword. 
TRAIN:Thisbaseline usesthetraining corpustodirectlybuild aTopicSignature usingTFIDFmeasureforeachwordsense.NotethatinWSDevaluationframeworks, thisisaverybasicbaseline.However,inourevaluationframework,this "WSDbase­line" couldbe consideredas an upper-bound.Wedo not expectto obtainbettertopic signaturesfor aparticularsensethanfromits own annotatedcorpus. 

5.2 Large-scale Knowledge Resources 
In orderto measurethe relativequalityofthe new resources, weincludeinthe evalu­ation a wide rangeoflarge-scaleknowledgeresources connectedtoWordNet. 
WN (Fellbaum,1998):This resource usesthedifferentdirectrelations encodedin WN1.6 andWN2.0. WealsotestedWN2 using relationsatdistance1 and2,WN3 using relationsatdistances1to3 andWN4 usingrelations atdistances1to4. 
XWN (Mihalcea andMoldovan,2001):This resource usesthedirectrelations en­codedin eXtendedWN. 
WN+XWN:ThisresourceusesthedirectrelationsincludedinWNandXWN.We alsotested(WN+XWN)2(usingeitherWNorXWN relations atdistances1 and2). 
spBNC (McCarthy,2001):This resourcecontains707,618selectionalpreferences acquiredforsubjects and objectsfromBNC. 
spSemCor (Agirre and Martinez, 2002): This resource contains the selectional preferencesacquiredforsubjects andobjectsfromSemCor. 
MCR (Atserias etal.,2004):This resource usesthedirect relations ofWN,XWN and spSemCor(we excludedspBNCbecause ofitspoorperformance). 
TSSEM (Cuadros et al., 2007): These Topic Signatures have been constructed using the part of SemCor having all words tagged by PoS, lemmatized and sense taggedaccordingtoWN1.6totalizing192,639words.Foreachword-senseappearing inSemCor, wegather all sentencesfor that word sense,building aTS usingTFIDF for allword-senses co-occurringinthose sentences. 
6 KnowNet Evaluation 
We evaluatedKnowNetusingtheframeworkofSection5,thatis,the nounpartofthe testsetfromtheSenseval-3Englishlexical sampletask. 
Table8presents orderedbyF1 measure,theperformanceinterms ofprecision (P), recall(R)andF1 measure(F1,harmonic mean of recall andprecision) of each knowledgeresourceonSenseval-3andits averagesize oftheTSperword-sense.The differentKnowNetversionsappearmarkedinboldandthebaselinesappearinitalics. Inthistable,TRAINhasbeencalculatedwithavectorsizeofatmaximum450words. Asexpected,RANDOMbaselineobtainsthepoorestresult.Themostfrequentsenses obtainedfromSemCor(SEMCOR-MFS) andWN(WN-MFS) arebothbelowthe mostfrequent sense of the training corpus(TRAIN-MFS).However, all of them are farbelowtotheTopicSignaturesacquiredusingthetraining corpus(TRAIN). 
The best resources would be those obtainingbetter performances with a smaller number of related wordsper synset. Thebest results are obtainedbyTSSEM(with F1 of52.4). Thelowest resultis obtainedby theknowledgedirectlygatheredfrom WN mainlybecauseofitspoorcoverage(R of18.4 andF1 of26.1). Interestingly, theknowledgeintegratedintheMCR althoughpartlyderivedby automaticmeans performs muchbetterin terms ofprecision, recallandF1 measures than using them separately(F1 with18.4pointshigherthanWN,9.1thanXWN and3.7than spSem-Cor). 
Despiteitssmallsize,theresourcesderivedfromSemCorobtainbetterresultsthan its counterparts usingmuchlarger corpora(TSSEM vs. TSWEB and spSemCor vs. spBNC). 
Regarding the baselines, all knowledge resources surpass RANDOM, but none achieves neither WN-MFS, TRAIN-MFS nor TRAIN. Only TSSEM obtains better resultsthanSEMCOR-MFSandisveryclosetothemostfrequentsenseofWN(WN­MFS)andthetraining(TRAIN-MFS). 
ThedifferentversionsofKnowNetconsistently obtainbetterperformancesasthey increase the window size of processed words of TSWEB. As expected, KnowNet­5obtainthelowerresults. However,itperformsbetterthanWN(and allitsex­tensions) and spBNC. Interestingly, from KnowNet-10, all KnowNet versions sur­pass theknowledge resources usedfor their construction(WN,XWN,TSWEB and WN+XWN). 
Furthermore, the integration of WN+XWN+KN-20 performs better than MCR andsimilarlytoMCR2(havinglessthan50timesitssize).Itisalsointerestingtonote that WN+XWN+KN-20 has a better performance than their individual resources, indicatinga complementaryknowledge.Infact,WN+XWN+KN-20performsmuch betterthanthe resourcesfromwhichitderives(WN,XWNandTSWEB). 
These initial results seem to be very promising. If we do not consider the re­sourcesderivedfrommanuallysenseannotateddata(spSemCor,MCR,TSSEM,etc.), KnowNet-10performsbetter thatanyknowledge resourcederivedby manual or au­tomatic means.Infact,KnowNet-15andKnowNet-20 outperformsspSemCor which wasderivedfrom manually annotated corpora. Thisis a veryinteresting result since these KnowNet versions have been derived only with the knowledge coming from WN and the web(thatis, TSWEB), andWN andXWN as aknowledge sourcefor SSI-Dijkstra(eXtendedWordNetonlyhas17,185manuallylabelled senses). 
7 Conclusions and future research 
The initial results obtained for the different versions of KnowNet seem to be very promising, since they seem tobe of abetterquality than other availableknowledge resources encoding relationsbetween synsetsderivedfrom non-annotated sense cor­pora. 
We testedall these resources andthedifferent versions ofKnowNet onSemEval­2007EnglishLexicalSampleTask(CuadrosandRigau,2008a). When comparing the ranking of thedifferentknowledge resources, thedifferent versions ofKnowNet seem tobe more robust and stable across corpora changesthan the rest of resources. Furthermore,we alsotestedtheperformanceofKnowNetwhenportedtoSpanish(as theSpanishWordNetis also integratedinto theMCR).StartingfromKnowNet-10, allKnowNet versionsperformbetter than any otherknowledge resource onSpanish derivedby manual or automatic means(including theMCR)(Cuadros andRigau, 2008b). 
Table8:P,R andF1 .ne-grained resultsfortheresourcesevaluated atSenseval-3, EnglishLexicalSampleTask 
KB  P  R  F1  Av. Size  
TRAIN  65.1  65.1  65.1  450  
TRAIN-MFS  54.5  54.5  54.5  
WN-MFS  53.0  53.0  53.0  
TSSEM  52.5  52.4  52.4  103  
SEMCOR-MFS  49.0  49.1  49.0  
MCR2  45.1  45.1  45.1  26,429  
WN+XWN+KN-20  44.8  44.8  44.8  671  
MCR  45.3  43.7  44.5  129  
KnowNet-20  44.1  44.1  44.1  610  
KnowNet-15  43.9  43.9  43.9  339  
spSemCor  43.1  38.7  40.8  56  
KnowNet-10  40.1  40.0  40.0  154  
(WN+XWN)2  38.5  38.0  38.3  5,730  
WN+XWN  40.0  34.2  36.8  74  
TSWEB  36.1  35.9  36.0  1,721  
XWN  38.8  32.5  35.4  69  
KnowNet-5  35.0  35.0  35.0  44  
WN3  35.0  34.7  34.8  503  
WN4  33.2  33.1  33.2  2,346  
WN2  33.1  27.5  30.0  105  
spBNC  36.3  25.4  29.9  128  
WN  44.9  18.4  26.1  14  
RANDOM  19.1  19.1  19.1  

In sum, thisis apreliminarystep towardsimprovedKnowNets weplan to obtain exploitingtheTopicSignaturesderivedfrommonosemouswordsasexplainedinSec­tion3. 
Acknowledgments 
We wanttothankAitorSoroaforhistechnicalsupport andthe anonymousreviewers fortheircomments.This workhasbeen supportedbyKNOW(TIN2006-15049-C03­01)andKYOTO(ICT-2007-211423). 
References 
Agirre,E.,O.Ansa,D.Martinez,andE.Hovy(2000).Enrichingverylargeontologies withtopicsignatures.In Proceedings of ECAI’00 workshop on Ontology Learning, Berlin,Germany. 
Agirre,E.andO.L.delaCalle(2004). Publicly availabletopicsignaturesforall wordnetnominalsenses. In Proceedings of LREC,Lisbon,Portugal. 
Agirre,E.andD.Martinez(2001).Learningclass-to-classselectionalpreferences.In Proceedings of CoNLL,Toulouse,France. 
Agirre,E. andD.Martinez(2002). Integratingselectionalpreferencesin wordnet. In Proceedings of GWC,Mysore,India. 
Álvez,J.,J.Atserias,J.Carrera,S.Climent,A.Oliver, andG.Rigau(2008). Consis­tent annotation of eurowordnet with the top concept ontology. In Proceedings of Fourth International WordNet Conference (GWC’08). 
Atserias,J.,L.Villarejo,G.Rigau,E.Agirre,J.Carroll,B.Magnini, andP.Vossen (2004). The meaning multilingual central repository. In Proceedings of GWC, Brno,CzechRepublic. 
Cuadros,M.,L.Padró, andG.Rigau(2005). Comparingmethodsfor automatic ac­quisition oftopic signatures.InProceedings of RANLP,Borovets,Bulgaria. 
Cuadros,M. andG.Rigau(2006). Qualityassessment oflarge scaleknowledge re­sources. In Proceedings of the EMNLP. 
Cuadros,M. andG.Rigau(2008a). KnowNet:BuildingaL´
nargeNet ofKnowledge fromtheWeb. InProceedings of COLING. 
Cuadros,M.andG.Rigau(2008b). MultilingualEvaluationofKnowNet. In Pro­ceedings of SEPLN. 
Cuadros,M.,G.Rigau, andM.Castillo(2007). Evaluatinglarge-scaleknowledge resources acrosslanguages.In Proceedings of RANLP. 
Dorow,B.andD.Widdows(2003). Discovering corpus-speci.cword senses. In EACL,Budapest. 
Fellbaum,C.(1998). WordNet. An Electronic Lexical Database.TheMITPress. 
Leacock,C.,M.Chodorow,andG.Miller(1998).UsingCorpusStatistics andWord-NetRelationsforSenseIdenti.cation. Computational Linguistics 24(1),147–166. 
Lin,C.andE.Hovy(2000). Theautomated acquisitionof topicsignaturesfortext summarization.In Proceedings of COLING. Strasbourg,France. 
Magnini,B. andG.Cavaglià(2000). Integratingsubject.eld codesinto wordnet. In Proceedings of LREC,Athens.Greece. 
McCarthy,D.(2001). Lexical Acquisition at the Syntax-Semantics Interface: Diathe­sis Aternations, Subcategorization Frames and Selectional Preferences. Ph. D. thesis,UniversityofSussex. 
Mihalcea,R.andD.Moldovan(2001). extended wordnet:Progressreport. In Pro­ceedings of NAACL Workshop on WordNet and Other Lexical Resources, Pitts­burgh,PA. 
Navigli,R.(2005). Semi-automaticextensionoflarge-scalelinguisticknowledge bases. In Proc. of 18th FLAIRS International Conference (FLAIRS),Clearwater Beach,Florida. 
Navigli,R.andP.Velardi(2005).Structuralsemanticinterconnections:aknowledge­basedapproachtowordsensedisambiguation. IEEE Transactions on Pattern Anal­ysis and Machine Intelligence (PAMI) 27(7),1063–1074. 
Niles,I. andA.Pease(2001). Towards a standardupper ontology. InC.Welty and 
B.Smith(Eds.),Proc. of the 2nd International Conference on Formal Ontology in Information Systems (FOIS-2001),pp.17–19. 
Rigau,G.,B.Magnini,E.Agirre,P.Vossen,andJ.Carroll(2002). Meaning:A roadmaptoknowledgetechnologies. In Proceedings of COLING’2002 Workshop on A Roadmap for Computational Linguistics,Taipei,Taiwan. 
Snow,R.,D.Jurafsky,andA.Y.Ng(2006). Semantictaxonomyinductionfrom heterogenousevidence.InProceedings of COLING-ACL. 
Suchanek,F.M.,G.Kasneci, andG.Weikum(2007). Yago:ACoreofSemantic Knowledge.In16th international World Wide Web conference (WWW 2007),New York,NY,USA.ACMPress. 
Vossen, P.(1998). EuroWordNet: A Multilingual Database with Lexical Semantic Networks. KluwerAcademicPublishers. 


