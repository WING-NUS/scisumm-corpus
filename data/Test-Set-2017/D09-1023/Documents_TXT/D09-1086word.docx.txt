Parser Adaptation and Projection
with Quasi-Synchronous Grammar Features∗



David A. Smith Department of 
Computer Science University  of 
Massachusetts Amherst Amherst, MA 
01003, USA 
dasmith@cs.umass.edu


Jason Eisner Department of 
Computer Science Johns Hopkins 
University Baltimore, MD 21218, 
USA jason@cs.jhu.edu






Abstract


We connect two scenarios in structured 
learning: adapting   a  parser  trained on 
one corpus to another annotation style, and 
projecting syntactic annotations from one 
language to another. We propose quasi- 
synchronous grammar (QG) features for 
these structured learning tasks. That is, we 
score a aligned  pair of source and target 
trees based on local features of the trees 
and the alignment. Our quasi-synchronous 
model assigns positive probability  to any 
alignment of any trees, in contrast to a syn- 
chronous grammar, which would insist on 
some form of structural parallelism.

In monolingual  dependency parser adap- 
tation, we achieve high accuracy in trans- 
lating among  multiple annotation  styles 
for  the same  sentence.  On the more 
difficult problem of cross-lingual parser 
projection, we learn a dependency parser 
for  a   target language  by  using bilin- 
gual text, an English parser,  and auto- 
matic word alignments.  Our experiments 
show that unsupervised QG projection im- 
proves on parses trained using only high- 
precision projected  annotations  and far 
outperforms, by more than 35% absolute 
dependency accuracy,  learning  an unsu- 
pervised parser from raw target-language 
text alone. When  a few target-language 
parse trees are available,  projection  gives 
a boost equivalent  to doubling the number 
of target-language trees.

   ∗The first author would like to thank the Center for Intel- 
ligent Information Retrieval at UMass Amherst. We would 
also like to thank Noah Smith and Rebecca Hwa for helpful 
discussions and the anonymous reviewers for their sugges- 
tions for improving the paper.


1   Introduction

1.1   Parser Adaptation

Consider the problem of learning  a dependency 
parser, which must produce a directed tree whose 
vertices are the words of a given sentence. There 
are many differing conventions  for representing 
syntactic relations in dependency trees. Say that 
we wish to output parses in the Prague style and 
so have  annotated  a small target corpus—e.g.,
100 sentences—with those conventions.  A parser 
trained on those hundred  sentences will  achieve 
mediocre dependency accuracy (the proportion  of 
words that attach to their correct parent).
  But what if we also had a large number of trees 
in the CoNLL style (the source corpus)?   Ide- 
ally they should help train our parser.  But unfor- 
tunately,  a parser that learned to produce perfect 
CoNLL-style  trees would, for example, get both 
links “wrong” when its coordination constructions 
were evaluated  against a Prague-style  gold stan- 
dard (Figure 1).
  If it were just a matter of this one construction, 
the obvious solution would be to write a few rules 
by hand to transform the large source training cor- 
pus into the target style. Suppose, however, that 
there were many more ways that our corpora dif- 
fered. Then we would like to learn a statistical 
model to transform one style of tree into another.
  We may not possess hand-annotated  training 
data for this tree-to-tree transformation task. That 
would require the two corpora to annotate some of 
the same sentences in different styles.
  But fortunately, we can automatically  obtain a 
noisy form of the necessary paired-tree  training 
data. A parser trained on the source corpus can 
parse the sentences in our target corpus, yielding 
trees (or more generally, probability distributions 
over trees) in the source style. We will then learn 
a tree transformation  model  relating  these noisy 
source trees to our known trees in the target style.




822

Proceedings of the 2009 Conference on Empirical  Methods in Natural  Language Processing, pages 822–831, 
Singapore, 6-7 August 2009. Qc 2009 ACL and AFNLP





( 	 
(


( 	 (


now or never	now or never


Prague	Mel’cˇuk




now  (	 
(


( 	 (


or never	now or never


CoNLL	MALT



Figure 1:   Four of the five logically possible  schemes for 
annotating coordination show up in human-produced depen- 
dency treebanks. (The other possibility is a reverse Mel’cˇuk 
scheme.) These treebanks also differ on other conventions.



This model should enable us to convert the orig- 
inal large source corpus to target style, giving us 
additional training data in the target style.

1.2   Parser Projection

For many target languages, however,  we do not 
have  the luxury of a  large parsed “source  cor- 
pus” in the language, even one in a different  style 
or domain  as above.   Thus, we may seek other 
forms of data to augment our small target corpus. 
One option would be to leverage unannotated text 
(McClosky  et al., 2006; Smith and Eisner, 2007). 
But we can also try to transfer syntactic informa- 
tion from a parsed source corpus in another lan- 
guage.  This is an extreme  case of out-of-domain 
data. This leads to the second task of this paper: 
learning a statistical model to transform a syntac- 
tic analysis of a sentence in one language into an 
analysis of its translation.
  Tree  transformations   are often modeled with 
synchronous grammars.  Suppose we are given  a 
sentence wl in the “source” language and its trans- 
lation w into the “target” language. Their syn- 
tactic parses tl and t are presumably not indepen- 
dent, but will tend to have some parallel or at least 
correlated structure. So we could jointly model 
the parses tl, t and the alignment a between them,
with a model of the form p(t, a, tl | w, wl).
Such  a joint model captures how t, a, tl mu-
tually constrain  each other, so that even partial 
knowledge of some of these three variables  can 
help us to recover the others when training or de- 
coding on bilingual text.  This idea underlies  a 
number of recent papers on syntax-based align- 
ment (using t and tl to better recover a), grammar 
induction from bitext (using a to better recover t 
and tl), parser projection (using tl and a to better



Figure 2:  With the English tree and alignment provided 
by a parser and aligner at test time, the Chinese parser finds 
the correct dependencies (see §6). A monolingual parser’s 
incor- rect edges are shown with dashed lines.



recover t), as well as full joint parsing (Smith 
and
Smith, 2004; Burkett and Klein, 
2008).
  In this paper, we condition on the 1-best 
source tree tl.   As for the alignment a, our 
models ei- ther condition on the 1-best alignment 
or integrate the alignment out.  Our models  are 
thus of the
form p(t | w, wl, tl, a) or, in the generative 
case,
p(w, t, a | wl, tl). We intend to consider other 
for-
mulations in future 
work.
  So far, this is very similar to the 
monolingual parser adaptation scenario, but there 
are a few key differences.  Since the source and 
target sentences in the bitext are in different 
languages, there is no longer  a trivial 
alignment between the words of the source and 
target trees. Given  word align- ments, we could 
simply try to project dependency links in the 
source tree onto the target text.  A link-by-
link projection,  however, could result in 
invalid trees on the target side, with cycles or 
dis- connected words. Instead, our models 
learn the necessary transformations that align 
and transform a source tree into a target tree by 
means of quasi- synchronous grammar (QG) 
features.
  Figure 2 shows an example of bitext 
helping disambiguation  when a parser is trained 
with only a small number of Chinese trees. 
With the help of the English tree and 
alignment,  the parser is able to recover  the  
correct Chinese  dependen- cies using QG 
features.  Incorrect edges  from the 
monolingual   parser are shown  with dashed 
lines. (The bilingual  parser corrects additional 
er- rors in the second half of this sentence, which 
has been removed to improve legibility.)  The 
parser is able to recover the long-distance 
dependency from the first Chinese word 
(China) to the last (begun), while skipping over 
the intervening noun


phrase that confused the undertrained monolin- 
gual parser. Although,  due to the auxiliary verb, 
“China” and “begun” are siblings  in English and 
not in direct dependency,  the QG features still 
leverage this indirect projection.

1.3   Plan of the Paper

We  start by describing  the features  we use  to 
augment conditional  and generative parsers when
scoring pairs of trees (§2). Then we discuss in turn
monolingual  (§3)  and cross-lingual  (§4)  parser
adaptation. Finally, we present experiments  on
cross-lingual parser projection in conditions when 
no target language trees are available for training
(§5) and when some trees are available (§6).

2   Form of the Model

What should our model of source and target trees 
look like?   In our view, traditional approaches 
based on synchronous grammar are problematic 
both computationally  and linguistically. Full in- 
ference takes O(n6) time or worse (depending on 
the grammar formalism). Yet synchronous mod- 
els only consider a limited hypothesis space: e.g., 
parses must be projective,  and alignments must de- 
compose according  to the recursive  parse struc- 
ture.  (For example,  two nodes can be aligned 
only if their respective  parents are also aligned.) 
The synchronous model’s probability  mass func- 
tion is also restricted to decompose in this way, 
so it makes certain conditional  independence as- 
sumptions; put another way, it can evaluate only 
certain properties of the triple (t, a, tl).
We instead model (t, a, tl) as an arbitrary graph
that includes dependency links among the words 
of each  sentence  as well as arbitrary alignment 
links between  the words of the two sentences. 
This permits non-synchronous and many-to-many 
alignments.  The only hard constraint we impose 
is that the dependency links within each sentence 
must constitute  a valid monolingual  parse—a di- 
rected projective spanning tree.1
Given the two sentences w, wl,  our probabil-
ity distribution  over possible graphs considers lo- 
cal features of the parses, the alignment,  and both 
jointly. Thus, we learn what local syntactic con- 
figurations tend to occur in each language and how 
they correspond across languages. As a result, we 
might  learn that parses are “mostly synchronous,” 
but that there are some systematic cross-linguistic

1 Non-projective  parsing would also be possible.


divergences  and some instances of sloppy (non- 
parallel or inexact) translation. Our model is thus a 
form of quasi-synchronous grammar (QG) (Smith 
and Eisner, 2006a). In that paper, QG was applied 
to word alignment  and has since found applica- 
tions in question answering (Wang et al., 2007), 
paraphrase detection (Das and Smith,  2009), and 
machine translation (Gimpel and Smith, 2009).
  All the models in this paper are conditioned on 
the source tree tl.   Conditionally-trained  models 
of adaptation and projection also condition on the 
target string w and its alignment a to wl  and thus
have the form p(t | w, wl, tl, a); the unsupervised, 
generative projection models in §5 have the form 
p(w, t, a | wl, tl).
  The score s of a given tuple of trees, words, and 
alignment  can thus be written  as a dot product of 
weights w with features f and g:
s(t, tl, a, w, wl) = ) wifi(t, w)
i
+ ) wj gj (t, tl, a, w, wl)
j


The features f look only at target words and de- 
pendencies.  In the conditional  models of §3 and
§6, these features  are those  of an edge-factored
dependency parser (McDonald  et al., 2005). In 
the generative models of §5, f has the form of a
dependency model with valence (Klein and Man- 
ning, 2004). All models, for instance, have a fea- 
ture template that considers the parts of speech of 
a potential parent-child relation.
  In order to benefit from the source language, we 
also need to include bilingual features g.  When 
scoring  a candidate target dependency link from
word x → y, these features consider the relation-
ship of their corresponding source words xl and
yl.  (The correspondences are determined  by the 
alignment a.) For instance, the source tree tl may
contain the link xl → yl, which  would  cause a fea- 
ture for monotonic projection to fire for the x → y 
edge. If, on the other hand, yl   → xl  ∈  tl,  a
head-swapping feature fires. If xl  = yl, i.e.  x
and y align to the same word, the same-word fea- 
ture fires. Similar features fire when xl and yl are 
in grandparent-grandchild, sibling, c-command, or 
none-of-the above relationships, or when y aligns 
to NULL. These alignment  classes are called con- 
figurations (Smith and Eisner, 2006a, and follow- 
ing). When training is conditioned on the target
words  (see §3 and §6 below), we conjoin these


configuration features with the part of speech and 
coarse part of speech of one or both of the source 
and target words, i.e. the feature template has from 
one to four tags.
  In  conditional training,	the  exponentiated 
scores  s are normalized  by a  constant: Z   =
 


(McDonald  et al., 2005) on “source” domain data 
that followed  one set of dependency conventions. 
We then trained an edge-factored parser with QG 
features on a  small amount of “target” domain 
data. The source parser outputs were produced for 
all target data, both training  and test, so that fea-


t exp[s(t, tl, a, w, wl)].	For  the generative
model, the locally normalized  generative process 
is explained in §5.3.4.
  Previous  researchers have written fix-up rules 
to massage the projected links after the fact and 
learned  a parser from the resulting trees (Hwa et 
al., 2005). Instead, our models learn the necessary 
transformations  that align and transform a source 
tree into a target tree. Other researchers have tack- 
led the interesting task of learning  parsers from 
unparsed bitext alone (Kuhn, 2004; Snyder et al.,
2009); our methods take advantage of investments 
in high-resource   languages such as English. In 
work most closely related to this paper, Ganchev et 
al. (2009) constrain the posterior distribution over 
target-language  dependencies to align to source 
dependencies some “reasonable” proportion of the
time (≈ 70%, cf.  Table 2 in this paper). This
approach performs well but cannot directly learn
regular cross-language non-isomorphisms; for in- 
stance, some fixup rules for auxiliary verbs need 
to be introduced. Finally, Huang et al. (2009) 
use features, somewhat like QG configurations, on 
the shift-reduce actions in a monolingual,  target- 
language parser.

3   Adaptation
As discussed in §1,  the adaptation scenario is a 
special  case of parser projection  where the word 
alignments  are one-to-one and observed.  To test 
our handling of QG features, we performed ex-
periments in which training saw the correct parse 
trees in both source and target domains, and the 
mapping  between them was simple  and regular. 
We also performed experiments where the source 
trees were replaced by the noisy output of a trained 
parser,  making the mapping more complex and 
harder to learn.
  We used the subset of the Penn Treebank from 
the CoNLL 2007 shared task and converted it to 
dependency representation while varying two pa- 
rameters: (1) CoNLL vs.  Prague coordination 
style (Figure 1), and (2) preposition  the head vs. 
the child of its nominal object.
We trained an edge-factored dependency parser


tures for the target parser could refer to them.

  In this task, we know what the gold-standard 
source  language  parses are  for any given text, 
since we can produce them from the original Penn 
Treebank.  We can thus measure the contribution 
of adaptation  loss alone, and the combined loss 
of imperfect source-domain parsing with adapta- 
tion (Table 1). When no target domain  trees are 
available, we simply have the performance of the 
source domain parser on this out-of-domain  data. 
Training  a target-domain  parser on as few as 10 
sentences shows substantial improvements  in ac- 
curacy. In the “gold” conditions, where the target 
parser starts with perfect  source trees, accuracy 
approaches 100%;  in the realistic “parse” condi- 
tions, where the target-domain  parser gets noisy 
source-domain  parses, the improvements  are quite 
significant but approach  a lower ceiling imposed 
by the performance of the source parser.2

  The adaptation problem in this section is a sim- 
ple proof of concept of the QG approach; however, 
more complex and realistic  adaptation problems 
exist. Monolingual  adaptation is perhaps most ob- 
viously  useful when the source parser is a black- 
box or rule-based system or is trained on unavail- 
able data. One might still want to use such a parser 
in some new context, which might require new 
data or a new annotation standard.

  We are also interested in scenarios where we 
want to avoid expensive retraining on large rean- 
notated treebanks. We would like a linguist  to be 
able to annotate  a few trees according  to a hy- 
pothesized theory and then quickly use QG adap- 
tation to get a parser for that theory. One example 
would be adapting  a constituency  parser to pro- 
duce dependency parses.  We have concentrated 
here on adapting between two dependency  parse 
styles, in order to line up with the cross-lingual 
tasks to which we now turn.



  2 In the diagonal cells, source and target styles match, so 
training the QG parser amounts  to a “stacking” technique 
(Martins et al., 2008). The small training size and overreg- 
ularization of the QG parser mildly hurts in-domain parsing 
performance.




%
 
D
e
p
e
n
d
e
n
c
y
 
A
c
c
u
r
a
c
y
 
o
n
 
T
a
r
g
e
t

C
o
N
LL
-
Pr
ep
H
ea
d
C
o
N
LL
-
Pr
ep
C
hil
d
Pr
a
g
u
e-
Pr
e
p
H
e
a
d
Pr
a
g
u
e-
Pr
e
p
C
hil
d
So
urc
e
0
	10	100
0
	10	100
0
	10	100
0
	10	100
Go
ld 
Co
NL
L-
Pr
ep
He
ad
Par
se 
Co
NL
L-
Pr
ep
He
ad
1
0
0	99.6	99.6
89
.5	88.9	89.0
79.
5	96.9	97.8
71.
4	85.9	87.9
90.
5	95.0	98.1
82.
5	84.3	87.8
71.
0	92.7	95.4
65.
2	82.2	86.1
Go
ld 
Co
NL
L-
Pr
ep
Ch
ild
Par
se 
Co
NL
L-
Pr
ep
Ch
ild
79.
5	96.6	97.3
71.
0	84.2	86.8
1
0
0	99.6	99.6
88
.1	87.5	88.0
71.
0	91.3	95.5
64.
9	80.7	84.9
89.
9	94.5	97.9
80.
9	83.5	86.1
Go
ld 
Pr
ag
ue
-
Pr
ep
He
ad
Par
se 
Pr
ag
ue
-
Pr
ep
He
ad
90.
5	95.5	96.7
83.
0	87.1	87.4
71.
0	92.0	94.2
65.
6	84.2	85.9
1
0
0	99.6	99.6
88
.5	88.3	88.0
79.
6	97.4	98.1
70.
7	86.4	86.8
Go
ld 
Pr
ag
ue
-
Pr
ep
Ch
ild
Par
se 
Pr
ag
ue
-
Pr
ep
Ch
ild
71.
0	91.6	93.8
65.
3	81.7	84.6
89.
9	95.6	96.4
81.
2	84.5	86.1
79.
6	96.0	97.1
70.
4	83.2	85.3
1
0
0	99.6	99.6
86
.9	86.1	86.8

Table 1:  Adapting  a parser to a new annotation  style. We learn to parse in a “target” style (wide column label) given some 
number (narrow column label) of supervised target-style training  sentences. As a font of additional features, all training and 
test sentences have already been augmented with parses in some “source”  style (row label): either gold-standard parses (an 
oracle experiment) or else the output of a parser trained on 18k source trees (more realistic).  If we have 0 training  sentences, we 
simply output the source-style parse. But with 10 or 100 target-style training  sentences, each off-diagonal  block learns to adapt, 
mostly closing the gap with the diagonal block in the same column.  In the diagonal blocks, source and target styles match, and 
the QG parser degrades performance  when acting as a “stacked” parser.




4   Cross-Lingual Projection: Background

As in the adaptation scenario above, many syn- 
tactic structures can be transferred from one lan- 
guage to another.  In this section, we evaluate the 
extent of this direct projection on a small hand-
annotated corpus. In §5, we will use a QG genera-
tive model to learn dependency parsers from bitext

when there are no annotations in the target lan- 
guage. Finally, in §6,we show  how QG features
can augment a target-language parser trained on a 
small set of labeled trees.
  For syntactic annotation projection to work at 
all, we must hypothesize, or observe, that at least 
some syntactic structures are preserved in transla- 
tion. Hwa et al. (2005) have called this intuition 
the Direct Correspondence Assumption (DCA, 
with slight notational changes):

Given a pair of sentences w and wl  that 
are translations of each other with syn- 
tactic structure t and tl, if nodes xl and 
yl of tl are aligned with nodes x and y of 
t, respectively, and if syntactic relation- 
ship R(xl, yl) holds in tl, then R(x, y) 
holds in t.

The validity of this assumption clearly depends 
on the node-to-node alignment of the two trees. 
We again work in a dependency framework, where 
syntactic nodes are simply  lexical  items. This al- 
lows us to use existing work on word alignment.
  Hwa et al. (2005)  tested the DCA under ide- 
alized conditions by obtaining hand-corrected de- 
pendency  parse trees of a few hundred sentences 
of Spanish-English  and Chinese-English  bitext. 
They also used human-produced  word alignments.



C
or
pu
s
Pr
ec
.[
%]	Rec.[%]
S
pa
ni
sh
(n
o 
pu
nc
.)
6
4
.
3
	
2
8
.
4
7
2
.
0
	
3
0
.
8
C
hi
ne
se
(n
o 
pu
nc
.)
6
5
.
1
	
1
1
.
1
6
8
.
2
	
1
1
.
5

Table 2: Precision and recall of direct dependency projection 
via one-to-one links alone.



Since their word alignments could be many-to- 
many, they gave a heuristic Direct Projection Al- 
gorithm (DPA) for resolving them into component 
dependency relations.  It should be noted that this 
process introduced empty words into the projected 
target language tree and left words that are un- 
aligned to English detached from the tree; as a re- 
sult, they measured performance in dependency F- 
score rather than accuracy.  With manual English 
parses and word alignments, this DPA achieved
36.8% F-score in Spanish and 38.1% in Chinese. 
With Collins-model  English  parses and GIZA++ 
word alignments, F-score was 33.9% for Spanish 
and 26.3% for Chinese. Compare this to the Span- 
ish attach-left baseline of 31.0% and the Chinese 
attach-right  baselines of 35.9%. These discour- 
agingly low numbers led them to write language- 
specific transformation rules to fix up the projected 
trees. After these rules were applied to the pro- 
jections of automatic English parses, F-score was
65.7% for English and 52.4% for Chinese.
  While these F-scores were low, it is useful to 
look at a subset  of the alignment:  dependencies 
projected across one-to-one alignments before the 
heuristic fix-ups  had a much higher precision,  if 
lower recall, than Hwa et al.’s final results. Us-


ing Hwa et al.’s data, we calculated that the preci- 
sion of projection to Spanish and Chinese via these
one-to-one links was ≈ 65% (Table 2). There is
clearly more information in these direct links than
one would think from the F-scores. To exploit this 
information,  however, we need to overcome the 
problems of (1) learning from partial trees, when 
not all target words are attached, and (2) learning 
in the presence of the still considerable noise in the 
projected one-to-one dependencies—e.g., at least
28% error for Spanish non-punctuation dependen- 
cies.
  What does this noise consist of? Some errors 
reflect fairly arbitrary annotation conventions in 
treebanks,  e.g.  should the auxiliary verb gov- 
ern the main verb or vice versa.  (Examples like 
this suggest that the projection problem contains 
the adaptation problem above.) Other errors arise 
from divergences in the complements required of 
certain head words. In the German-English trans- 
lation pair, with co-indexed words aligned,


t p(t, w, a |  tl, wl).   We hope that this condi-
tional EM training will drive the model to posit ap-
propriate syntactic relationships in the latent vari- 
able t, because—thanks to the structure of the QG 
model—that is the easiest way for it to exploit the 
extra information in tl, wl  to help predict w.4  At 
test time, tl, wl  are not made available, so we just
use the trained model to find argmaxt p(t |  w),
backing off from the conditioning on tl, wl  and
summing over a.

  Below, we present the specific generative model 
(§5.1) and some details of training (§5.2). We will 
then compare three approaches (§5.3):

§5.3.2  a  straight EM baseline  (which does  not 
condition on tl, wl at all)

§5.3.3  a “hard” projection baseline (which naively 
projects tl, wl  to derive direct supervision in 
the target language)
§5.3.4  our conditional EM approach above (which


[an [den Libanon1]] denken2 ↔ remember2 Lebanon1


makes tl, wl


available to the learner for “soft”



we would prefer that the preposition an attach 
to denken, even though  the preposition’s  object 
Libanon aligns to a  direct child of remember. 
In other words, we would like the grandparent-
parent-child chain of denken → an → Libanon
to align to the parent-child pair of remember →
Lebanon. Finally, naturally occurring bitexts con-
tain some number  of free or erroneous transla- 
tions. Machine  translation  researchers often seek 
to strike  these examples from their training cor- 
pora; “free” translations are not usually welcome 
from an MT system.

5  Unsupervised Cross-Lingual Projection

First, we consider the problem of parser projection 
when  there are zero target-language  trees avail- 
able.  As in much other work on unsupervised 
parsing, we try to learn a generative  model  that 
can predict target-language sentences. Our novel 
contribution is to condition the probabilities of the 
generative actions on the dependency  parse of a 
source-language translation.  Thus, our generative 
model is a quasi-synchronous grammar, exactly as 
in (Smith and Eisner, 2006a).3
  When training on target sentences  w, there- 
fore, we tune the model parameters  to maxi- 
mize not    t p(t, w) as in ordinary EM, but rather
3 Our task here is new; they used it for alignment.


indirect supervision via QG)

5.1   Generative Models

Our base models  of target-language  syntax  are 
generative dependency models that have achieved 
state-of-the art results in unsupervised dependency 
structure induction. The simplest version, called 
Dependency Model with Valence (DMV), has been 
used in isolation  and in combination with other 
models (Klein and Manning, 2004; Smith and Eis- 
ner, 2006b). The DMV  generates the right chil- 
dren, and then independently the left children, for 
each node in the dependency tree. Nodes corre- 
spond to words, which are represented  by their 
part-of-speech tags. At each step of generation, 
the DMV  stochastically  chooses whether  to stop 
generating, conditioned on the currently generat- 
ing head; whether it is generating to the right or 
left; and whether it has yet generated any chil- 
dren on that side. If it chooses to continue, it then

  4 The contrastive estimation of Smith and Eisner (2005) 
also used  a form of conditional EM, with similar motiva- 
tion.  They suggested that EM grammar induction, which 
learns to predict w, unfortunately learns mostly to predict lex- 
ical topic or other properties of the training  sentences that do 
not strongly require syntactic latent variables. To focus EM 
on modeling the syntactic relationships, they conditioned the 
prediction of w on almost complete knowledge of the lexi- 
cal items. Similarly, we condition on a source translation  of 
w. Furthermore, our QG model structure makes it easy for 
EM to learn to exploit the (explicitly represented) syntactic 
properties of that translation when predicting w.


stochastically  generates the tag of a  new child, 
conditioned on the head. The parameters of the 
model are thus of the form
p(stop | head, dir, adj)	(1)
p(child | head, dir)	(2)
where  head and child  are part-of-speech  tags,
dir ∈ {left, right}, and adj, stop ∈ {true, false}.
ROOT is stipulated to generate a single right child.
Bilingual configurations that condition on tl, wl
(§2) are incorporated  into the generative process
as in Smith and Eisner (2006a). When the model
is generating a new child for word x, aligned to xl, 
it first chooses a configuration and then chooses a 
source word yl in that configuration. The child y is 
then generated, conditioned  on its parent x, most 
recent sibling a, and its source analogue yl.

5.2   Details of EM Training
As in previous work on grammar induction,  we 
learn the DMV  from part-of-speech-tagged target- 
language text. We use expectation maximization 
(EM) to maximize the likelihood of the data. Since 
the likelihood function is nonconvex in the unsu- 
pervised case, our choice of initial parameters can 
have a significant effect on the outcome. Although 
we could also try many random starting points, the 
initializer in Klein and Manning (2004) performs 
quite well.
  The base dependency parser generates the right 
dependents of a head separately  from the left de- 
pendents, which allows O(n3) dynamic program- 
ming for an n-word  target sentence. Since the QG 
annotates nonterminals  of the grammar with sin- 
gle nodes of tl, and we consider two nodes of tl 
when evaluating the above dependency configura- 
tions, QG parsing runs in O(n3m2) for an m-word 
source sentence. If, however, we restrict candidate 
senses for a target  child c to come from links in 
an IBM Model 4 Viterbi alignment, we achieve 
O(n3k2),  where k is the maximum  number of 
possible words aligned to a given target language
word. In practice, k « m, and parsing is not ap-
preciably slower than in the monolingual setting.
  If  all  configurations  were equiprobable,  the 
source sentence would provide no information to 
the target.  In our QG experiments,  therefore, 
we started with a bias towards direct parent–child 
links and a very small probability for breakages 
of locality. The values of other configuration pa- 
rameters seem, experimentally,  less important  for 
insuring accurate learning.


5.3   Experiments

Our experiments compare learning on target lan- 
guage text to learning on parallel text. In the lat- 
ter case, we compare learning  from high-precision 
one-to-one alignments alone, to learning from all 
alignments using a QG.

5.3.1  Corpora
Our development  and test data were drawn from 
the German TIGER and Spanish Cast3LB  tree- 
banks as converted to projective  dependencies for 
the CoNLL 2007 Shared Task (Brants et al., 2002; 
Civit Torruella and Mart´ı Anton´ın, 2002).5
Our training data were subsets  of the 2006
Statistical Machine Translation Workshop Shared 
Task, in  particular from  the German-English 
and Spanish-English  Europarl parallel corpora 
(Koehn, 2002). The Shared Task provided  pre- 
built automatic GIZA++ word alignments, which 
we used  to facilitate replicability.  Since these 
word alignments do not contain posterior proba- 
bilities or null links, nor do they distinguish which 
links are in the IBM Model intersection, we treated 
all links as equally  likely when learning the QG. 
Target  language words unaligned to any source 
language words were the only nodes allowed  to 
align to NULL in QG derivations.
  We parsed the English side of the bitext with the 
projective  dependency parser described by Mc- 
Donald et al. (2005) trained on the Penn Treebank
§§2–20.    Much previous work on unsupervised
grammar  induction  has used gold-standard  part-
of-speech  tags (Smith and Eisner,  2006b; Klein 
and Manning,  2004; Klein and Manning,  2002). 
While there are no gold-standard  tags for the Eu- 
roparl bitext, we did train a conditional  Markov

  5 We made one change to the annotation conventions in 
German: in the dependencies provided, words in a  noun 
phrase governed  by a preposition  were all attached to that 
preposition. This meant that in the phrase das Kind (“the 
child”) in, say, subject position,  das was the child of Kind; 
but, in fu¨ r das Kind (“for the child”), das was the child of 
fu¨ r. This seems to be a strange choice in converting from the 
TIGER  constituency format, which does in fact annotate NPs 
inside PPs; we have standardized prepositions to govern only 
the head of the noun phrase.  We did not change any other 
annotation conventions to make them more like English. In 
the Spanish treebank, for instance, control verbs are the chil- 
dren of their verbal complements: in quiero decir (“I want to 
say”=“I mean”), quiero is the child of decir. In German co- 
ordinations, the coordinands all attach to the first, but in En- 
glish, they all attach to the last. These particular divergences 
in annotation style hurt all of our models equally (since none 
of them have access to labeled trees). These annotation diver- 
gences are one motivation for experiments below that include 
some target trees.




B
as
eli
ne
s
Depen
dency 
accura
cy [%]

G
er
m
an
S
p
a
n
i
s
h
M
od
ify 
pr
ev
.
M
od
ify 
ne
xt
1
8
.
2
2
7
.
5
28
.5
21
.4
E
M
Ha
rd 
pr
oj.
H
ar
d 
p
r
oj
. 
w
/
E
M
 
Q
G 
w
/
E
M
3
0
.
2
6
6
.
2
5
8
.
6
6
8
.
5
25
.6
59
.1
53
.0
64
.8

Table 3: Test accuracy with unsupervised training methods


model tagger on a few thousand tagged sentences. 
This is the only supervised data we used in the tar- 
get. We created versions of each training  corpus 
with the first thousand, ten thousand, and hundred 
thousand sentence pairs, each a prefix  of the next. 
Since the target-language-only baseline converged 
much more slowly, we used a version  of the cor- 
pora with sentences 15 target words or fewer.

5.3.2  Fully Unsupervised EM
Using the target side of the bitext as training  data, 
we initialized our model parameters as described
in §5.2 and ran EM. We checked convergence on
a development set and measured unlabeled depen-
dency accuracy on held-out  test data. We com- 
pare performance  to simple attach-right and at- 
tach left baselines (Table 3).  For mostly head- 
final German, the “modify next” baseline is bet- 
ter; for mostly head-initial Spanish, “modify pre- 
vious” wins. Even after several hundred iterations, 
performance was slightly, but not significantly  bet- 
ter than the baseline for German. EM training did 
not beat the baseline for Spanish.6

5.3.3  Hard Projection, Semi-Supervised EM
The simplest approach to using the high-precision 
one-to-one word alignments is labeled “hard pro- 
jection” in the table. We filtered the training cor- 
pus to find sentences where  enough  links were 
projected to completely   determine  a target  lan- 
guage tree. Of course, we needed to filter more 
than 1000 sentences  of  bitext to output 1000 
training sentences in this way.  We simply per- 
form supervised training with this subset, which
is still quite noisy (§4), and performance quickly

  6 While these results are worse than those obtained previ- 
ously for this model, the experiments in Klein and Manning 
(2004) and only used sentences of 10 words or fewer, without 
punctuation, and with gold-standard tags. Punctuation in par- 
ticular  seems to trip up the initializer: since a sentence-final 
periods appear in most sentences, EM often decides to make 
it the head.



plateaus. Still, this method substantially improves 
over the baselines and unsupervised EM.
  Restricting ourselves to fully projected trees 
seems a waste of information. We can also sim- 
ply take all one-to-one projected links, impute ex- 
pected counts for the remaining dependencies with 
EM, and update our models. This approach (“hard 
projection with EM”), however, performed worse 
than using only the fully projected trees.  In fact, 
only the first iteration of EM with this method 
made any improvement; afterwards, EM degraded 
accuracy further from the numbers in Table 3.

5.3.4  Soft Projection: QG & Conditional EM

The quasi-synchronous  model used  all  of  the 
alignments in re-estimating its parameters and per- 
formed significantly  better than hard projection. 
Unlike EM on the target language alone, the QG’s 
performance  does not depend on a clever  initial- 
izer for initial model weights—all  parameters of 
the generative model except for the QG configura- 
tion features were initialized  to zero. Setting the 
prior to prefer direct correspondence provides the 
necessary bias to initialize learning.
  Error analysis showed that certain types of de- 
pendencies eluded the QG’s ability to learn from 
bitext. The Spanish treebank treats some verbal 
complements  as the heads of main verbs and aux- 
iliary verbs as the children  of participles; the QG, 
following the English, learned the opposite de- 
pendency direction.  Spanish treebank conventions 
for punctuation were also a common source of er- 
rors. In both German and Spanish, coordinations 
(a common  bugbear for dependency grammars) 
were often mishandled: both treebanks attach the 
later coordinands and any conjunctions to the first 
coordinand; the reverse is true in English. Finally, 
in both German and Spanish, preposition  attach- 
ments often led to errors, which is not surprising 
given the unlexicalized  target-language grammars. 
Rather than trying to adjudicate which dependen- 
cies are “mere” annotation conventions, it would 
be useful to test learned dependency models on 
some extrinsic  task such as relation  extraction  or 
machine translation.

6   Supervised Cross-Lingual Projection

Finally, we consider the problem of parser projec- 
tion when some target language trees are available.
As in the adaptation  case (§3), we train a condi-
tional model (not a generative  DMV) of the target


tree given the target sentence, using the monolin- 
gual and bilingual  QG features, including config-
urations conjoined with tags, outlined above (§2).
For these  experiments,   we used  the LDC’s
English-Chinese Parallel Treebank (ECTB). Since 
manual word alignments also exist for a part of 
this corpus, we were able to measure the loss in 
accuracy (if any) from the use of an automatic



language English  dependency parser was trained



Target only
 	+Gold alignments
+Source text


on the Wall Street Journal, where it achieved 91%


+Gold parses, alignments
+Gold parses





ever, it was only 80.3% accurate when applied to 
our task, the English side of the ECTB.7
  After parsing the source side of the bitext, we 
train a parser on the annotated target side, using
QG features described above (§2). Both the mono-
lingual target-language  parser and the projected
parsers are trained  to optimize conditional likeli- 
hood of the target trees tl with ten iterations of 
stochastic gradient ascent.
  In Figure 3, we plot the performance of the 
target-language  parser  on held-out bitext.   Al- 
though projection performance is, not surprisingly, 
better if we know the true source trees at training 
and test time, even with the 1-best output of the 
source parser, QG features help produce a parser 
as accurate asq one trained  on twice the amount 
of monolingual  data. In ablation experiments, we 
included bilingual features only for directly pro- 
jected links, with no features for head-swapping, 
grandparents,  etc.   When using 1-best English 
parses, parsers trained only with direct-projection 
and monolingual  features performed worse; when 
using gold English parses,  parsers  with direct- 
projection-only   features performed  better when 
trained with more Chinese trees.

7   Discussion

The two related problems of parser adaptation and 
projection are often approached in different ways. 
Many adaptation methods operate by simple aug- 
mentations of the target feature space, as we have 
done here (Daume III, 2007).  Parser projection, on 
the other hand, often uses a multi-stage  pipeline

7 It would be useful to explore whether the techniques of
§3 above could be used to improve English accuracy by do- 
main adaptation. In theory a model with QG features trained 
to perform well on Chinese should not suffer from an inaccu- 
rate, but consistent, English parser, but the results in Figure 3 
indicate a significant  benefit to be had from better English 
parsing or from joint Chinese-English inference.


10 	20 	50 	100 	200 	500     1000    2000

Training examples


Figure 3:  Parser projection  with target trees. Using the true 
or 1-best parse trees in the source language is equivalent to 
having twice as much data in the target language. Note that 
the penalty for using automatic alignments instead of gold 
alignments is negligible; in fact, using Source text alone is 
often higher than +Gold  alignments. Using gold source trees, 
however, significantly outperforms using 1-best source trees.

(Hwa et al., 2005). The methods presented here 
move parser projection  much closer in efficiency 
and simplicity  to monolingual parsing.
  We showed that augmenting a target parser with 
quasi-synchronous features can lead to significant 
improvements—first in experiments with adapt- 
ing to different  dependency representations in En- 
glish, and then in cross-language  parser projec- 
tion. As with many domain adaptation problems, 
it is quite helpful to have  some  annotated  tar- 
get data, especially when annotation styles vary 
(Dredze et al., 2007). Our experiments show that 
unsupervised QG projection improves on parsers 
trained using only high-precision projected anno- 
tations and far outperforms, by more than 35% 
absolute dependency accuracy, unsupervised EM. 
When a  small number of target-language  parse 
trees is available,  projection  gives a boost equiv- 
alent to doubling the number of target trees.
  The loss in performance from conditioning only 
on noisy 1-best source parses points to some nat- 
ural avenues  for improvement. We  are explor- 
ing methods that incorporate  a packed parse for- 
est on the source side and similar  representations 
of uncertainty about alignments. Building on our 
recent belief propagation work (Smith and Eis- 
ner, 2008), we can jointly infer two dependency 
trees and their alignment,  under a joint distribu-
tion p(t, a, tl | w, wl) that evaluates the full graph
of dependency and alignment edges.


References

S. Brants, S. Dipper, S. Hansen,  W. Lezius, and
G. Smith. 2002. The TIGER treebank. In TLT.

David Burkett and Dan Klein.    2008.   Two lan- 
guages are better than one (for syntactic parsing). In 
EMNLP.

M. Civit Torruella  and M. A. Mart´ı Anton´ın.  2002.
Design principles for a Spanish treebank.  In TLT.

Dipanjan Das and Noah A. Smith. 2009. Paraphrase 
identification as   probabilistic quasi-synchronous 
recognition. In ACL-IJCNLP.

Hal Daume III. 2007. Frustratingly easy domain adap- 
tation. In ACL, pages 256–263.

Mark  Dredze, John Blitzer,  Partha  Pratim Taluk- 
dar, Kuzman  Ganchev, Joa˜o Graca, and Fernando 
Pereira. 2007.  Frustratingly hard domain  adap- 
tation for dependency parsing. In Proceedings of 
the CoNLL  Shared Task Session of EMNLP-CoNLL
2007, pages 1051–1055.

Kuzman Ganchev, Jennifer Gillenwater, and Ben 
Taskar. 2009. Dependency grammar induction via 
bitext projection constraints. In ACL-IJCNLP.

Kevin Gimpel and Noah A. Smith. 2009. Feature-rich 
translation by quasi-synchronous lattice parsing. In 
EMNLP.

Liang Huang, Wenbin Jiang, and Qun Liu. 	2009.
Bilingually-constrained  (monolingual) shift-reduce 
parsing. In EMNLP.

Rebecca Hwa,  Philip Resnik, Amy Weinberg,  Clara 
Cabezas, and Okan Kolak.  2005.  Bootstrapping 
parsers via syntactic projection  across parallel texts. 
Natural Language Engineering, 11:311–325.

Dan Klein and Christopher D. Manning.  2002.  A 
generative constituent-context  model for improved 
grammar induction. In ACL, pages 128–135.

Dan Klein  and Christopher D.  Manning.	2004.
Corpus-based induction  of syntactic structure: Mod- 
els of dependency and constituency.  In ACL, pages
479–486.

Philipp Koehn.	2002.	Europarl: A  multilingual 
corpus for  evaluation   of  machine translation. 
http://www.iccs.informatics.ed.ac.uk/˜pkoehn/- 
publications/europarl.ps.

Jonas Kuhn. 2004. Experiments in parallel-text  based 
grammar induction. In ACL, pages 470–477.

Andre´ F. T. Martins, Dipanjan Das, Noah A. Smith, and 
Eric P. Xing.  2008. Stacking dependency parsers. 
In EMNLP.

David McClosky, Eugene Charniak,  and Mark John- 
son. 2006. Reranking and self-training  for parser 
adaptation. In ACL, pages 337–344.



Ryan McDonald,  Koby  Crammer, and Fernando 
Pereira.  2005. Online large-margin training of de- 
pendency parsers. In ACL, pages 91–98.

Noah A. Smith and Jason Eisner.  2005. Guiding  unsu- 
pervised grammar induction  using contrastive esti- 
mation. In International  Joint Conference on Artifi- 
cial Intelligence (IJCAI) Workshop on Grammatical 
Inference Applications, Edinburgh, July.

David A. Smith and Jason  Eisner.  2006a.  Quasi- 
synchronous grammars: Alignment  by soft projec- 
tion of syntactic  dependencies.  In Proceedings of 
the HLT-NAACL  Workshop on Statistical Machine 
Translation, pages 23–30.

Noah A. Smith and Jason Eisner. 2006b. Annealing 
structural bias in multilingual  weighted grammar in- 
duction. In ACL-COLING, pages 569–576.

David A. Smith and Jason Eisner. 2007. Bootstrap- 
ping feature-rich  dependency parsers with entropic 
priors. In EMNLP-CoNLL, pages 667–677.

David A. Smith and Jason Eisner.  2008. Dependency 
parsing by belief propagation. In EMNLP, pages
145–156.

David A. Smith and Noah A. Smith. 2004. Bilingual 
parsing with factored estimation: Using English to 
parse Korean. In EMNLP,  pages 49–56.

Benjamin  Snyder, Tahira Naseem, and Regina Barzi- 
lay. 2009. Unsupervised multilingual grammar in- 
duction. In ACL-IJCNLP.

Mengqiu Wang, Noah A. Smith, and Teruko Mita- 
mura. 2007. What is the Jeopardy model? a quasi- 
synchronous grammar for QA. In EMNLP-CoNLL, 
pages 22–32.


