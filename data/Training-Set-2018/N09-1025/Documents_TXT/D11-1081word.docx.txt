Fast Generation of Translation Forest

for Large-Scale SMT Discriminative Training



Xinyan Xiao, Yang Liu,  Qun Liu,  and Shouxun Lin 
Key Laboratory of Intelligent Information Processing 
Institute of Computing Technology
Chinese Academy of Sciences
{xiaoxinyan,yliu,liuqun,sxlin}@ict.ac.cn






Abstract

Although discriminative training guarantees to 
improve statistical machine translation by in- 
corporating  a large amount of overlapping fea- 
tures, it is hard to scale up to large data due to 
decoding complexity. We propose a new al- 
gorithm to generate translation forest of train- 
ing data in linear time with the help of word 
alignment. Our algorithm  also alleviates the 
oracle selection problem  by ensuring that a 
forest always contains derivations that exactly 
yield the reference translation.  With millions 
of features trained on 519K  sentences in 0.03 
second per sentence, our system achieves sig- 
nificant improvement by 0.84 BLEU over the 
baseline system on the NIST Chinese-English 
test sets.


1   Introduction

Discriminative  model (Och and Ney, 2002) can 
easily incorporate non-independent and overlapping 
features, and has been dominating  the research field 
of statistical machine translation (SMT) in the last 
decade. Recent work have shown that SMT benefits 
a lot from exploiting large amount of features (Liang 
et al., 2006; Tillmann and Zhang, 2006; Watanabe 
et al., 2007; Blunsom et al., 2008; Chiang et al.,
2009). However, the training of the large number 
of features was always restricted in fairly small data 
sets. Some systems limit the number of training ex- 
amples, while  others use short sentences to maintain 
efficiency.
  Overfitting problem often comes when training 
many features  on a  small data (Watanabe  et al.,
880


2007; Chiang et al., 2009). Obviously, using much 
more data can alleviate such problem. Furthermore, 
large data also enables us to globally train millions 
of sparse lexical  features which  offer accurate clues 
for SMT. Despite  these advantages, to the best of 
our knowledge, no previous discriminative training 
paradigms scale up to use a large amount of training 
data. The main obstacle comes from the complexity 
of packed forests or n-best lists generation which 
requires to search through all possible translations 
of each training example, which is computationally 
prohibitive in practice for SMT.
  To make normalization efficient, contrastive esti- 
mation (Smith and Eisner, 2005; Poon et al., 2009) 
introduce neighborhood for unsupervised log-linear 
model, and has presented positive  results in various 
tasks. Motivated by these work,  we use a translation 
forest (Section 3) which contains both “reference” 
derivations that potentially  yield the reference trans- 
lation and also neighboring “non-reference” deriva- 
tions that fail to produce the reference translation.1
However, the complexity of generating this transla-
tion forest is up to O(n6), because we still need bi-
parsing to create the reference derivations.
  Consequently, we propose a method to fast gener- 
ate a subset of the forest. The key idea (Section 4) 
is to initialize  a reference derivation tree with maxi- 
mum score by the help of word alignment, and then 
traverse the tree to generate the subset forest in lin- 
ear time. Besides the efficiency  improvement, such a 
forest allows us to train the model without resort-

  1 Exactly, there are no reference derivations, since derivation 
is a latent variable  in SMT. We call them reference derivation 
just for convenience.




Proceedings of the 2011 Conference on Empirical  Methods in Natural  Language Processing, pages 880–888, 
Edinburgh, Scotland, UK, July 27–31, 2011. Qc 2011 Association for Computational Linguistics



0,4
1	2


hyper- edge



rule








0,1
5

0 	1





2,4
3





2 	3





4


3,4
6

4


e1 	r1   X ⇒ 
⟨X1  bei X2, X1  
was X2⟩
e2 	r2   X ⇒ 
⟨qiangshou bei 
X1,
t
h
e
 
g
u
n
m
a
n
 
w
a
s
 
X
1
⟩
e3 	r3   X ⇒ 
⟨jingfang X1, X1  
by the police⟩
e4 	r4   X ⇒ 
⟨jingfang X1, 
police X1  ⟩
e5 	r5   X ⇒ 
⟨qiangshou, the 
gunman⟩
e6 	r6   X ⇒ 
⟨jibi, shot dead⟩



Figure 1: A translation forest which is the running example throughout this paper. The reference translation is “the 
gunman was killed by the police”. (1) Solid hyperedges denote a “reference”  derivation tree t1 which exactly yields 
the reference translation. (2) Replacing e3 in t1 with e4 results a competing non-reference derivation t2 , which fails to 
swap the order of X3,4 . (3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3 . Generally, 
this is done by deleting a node X0,1 .





ing to constructing the oracle reference (Liang et al.,
2006; Watanabe et al., 2007; Chiang et al., 2009), 
which is non-trivial for SMT and needs to be deter- 
mined experimentally.  Given such forests, we glob- 
ally learn a log-linear model using stochastic gradi- 
ent descend (Section 5). Overall, both the generation 
of forests and the training  algorithm  are scalable, en- 
abling us to train millions of features on large-scale 
data.
To show the effect of our framework, we globally


quence of SCFG rules {ri}.  Translation forest (Mi
et al., 2008; Li and Eisner, 2009) is a compact repre-
sentation of all the derivations for a given  sentence 
under an SCFG (see Figure 1). A tree t in the forest 
corresponds to a derivation.  In our paper, tree means 
the same as derivation.
More formally,  a forest is a pair ⟨V, E⟩, where V
is the set of nodes, E is the set of hyperedge.  For
a given source sentence f = f n, Each node v ∈ V
is in the form       , which denotes the recognition


train millions of word level context features moti-
vated by word sense disambiguation  (Chan  et al.,
2007) together with the features used in traditional 
SMT system (Section 6). Training on 519K sentence 
pairs in 0.03 seconds per sentence, we achieve sig- 
nificantly improvement over the traditional pipeline 
by 0.84 BLEU.

2   Synchronous Context Free Grammar

We  work on synchronous  context free grammar 
(SCFG) (Chiang,  2007) based translation.  The el- 
ementary structures in an SCFG are rewrite rules of 
the form:
X ⇒ ⟨γ, α⟩
where γ and α are strings of terminals and nonter-
minals. We call γ and α as the source side and the 
target side of rule respectively.  Here a rule means a 
phrase translation (Koehn et al., 2003) or a transla- 
tion pair that contains nonterminals.
We  call a  sequence  of  translation   steps  as  a
derivation. In context of SCFG, a derivation  is a se-


Xi,j
of nonterminal X spanning the substring from the i
through j (that is fi+1...fj ). Each hyperedge e ∈ E
connects  a set of antecedent to a single consequent
node and corresponds to an SCFG rule r(e).

3   Our Translation Forest

We use a translation forest that contains both “ref- 
erence” derivations that potentially yield the refer- 
ence translation  and also some neighboring  “non- 
reference” derivations that fail to produce the ref- 
erence translation.  Therefore, our forest only repre- 
sents some of the derivations for a sentence given an 
SCFG rule table. The motivation of using such a for- 
est is efficiency.  However,  since this space contains 
both “good” and “bad” translations, it still provides 
evidences for discriminative training.
  First see the example in Figure 1. The derivation 
tree t1 represented by solid hyperedges is a reference 
derivation.  We can construct a non-reference deriva- 
tion by making small change to t1. By replacing the 
e3  of t1 with e4, we obtain a non-reference deriva-


tion tree t2. Considering the rules in each derivation, 
the difference between t1 and t2 lies in r3 and r4. Al- 
though r3 has a same source side with r4, it produces 
a different  translation. While r3 provides  a swap- 
ping translation, r4 generates a monotone transla- 
tion. Thus, the derivation t2 fails to move the sub- 
ject “police” to the behind of verb “shot dead”, re- 
sulting a wrong translation “the gunman was police 
shot dead”. Given such derivations,  we hope that 
the discriminative model is capable to explain why 
should use a reordering rule in this context.
Generally,  our forest contains all the reference
derivations RT  for a sentence given a rule table, and 
some neighboring  non-reference derivations  N T , 
which can be defined from RT .


Algorithm 1 Forest Generation
1: procedure GENERATE(t)
2:	list ← t
3:	for v ∈ t in post order do
4:	e ← incoming  edge of v
5:	append C (t, e) to list;
6:	for u ∈ child(v) from left to right do
7:	tn ← OPERATE(t, u)
8:	if tn ̸= t then
9:	append tn to list
10:	for e′  ∈ tn ∧ e′  ∈/ t do
11:	append C (tn,e′ ) to list
12:	if SCORE(t) < SCORE(tn) then
13:	t ← tn
14:	return t,list


More formally, we call two hyperedges e1  and e2	 	
are competing  hyperedges, if their corresponding


rules r(e1) = ⟨γ1, α1⟩ and r(e2) = ⟨γ2, α2⟩ :
γ1 = γ2 ∧ α1 ̸= α2 	(1)
This means  they give different translations for a
same source side. We use C (e) to represent the set 
of competing hyperedges of e.
Two  derivations t1	=  ⟨V 1, E1⟩   and t2 	=
⟨V 2, E2⟩ are competing derivations if there exists
e1 ∈ E1 and e2 ∈ E2: 2
V 1 = V 2 ∧ E1 − e1 = E2 − e2
∧ e2 ∈ C (e1) 	(2)
In other words, derivations t1 and t2 only differ in
e1  and e2, and these two hyperedges are competing 
hyperedges.  We use C (t) to represent the set of com- 
peting derivations of tree t, and C (t,e) to represent 
the set of competing derivations of t if the competi- 
tion occurs in hyperedge e in t.
Given a rule table, the set of reference derivations
RT   for a sentence is determined.  Then, the set of 
non-reference derivations N T  can be defined from 
RT :
∪t∈RT C (t) 	(3)
Overall, our forest is the compact representation of
RT   and N T .
2 The definition of derivation tree is similar to forest, except
that the tree contains exactly one tree while forest contains ex- 
ponentially trees. In tree, the hyperedge degrades to edge.


4   Fast Generation

It is still slow to calculate the entire forest defined 
in Section 3, therefore we use a greedy decoding for 
fast generating  a subset of the forest. Starting form 
a reference derivation,  we try to slightly change the 
derivation into a new reference derivation. During 
this process, we collect the competing derivations 
of reference derivations.  We describe the details of 
local operators for changing a derivation  in section
4.1, and then introduce the creation of initial refer- 
ence derivation  with max score in Section 4.2.
  For example, given derivation t1,  we delete the 
node X0,1  and the related hyperedge e1 and e5. Fix- 
ing the other nodes and edges, we try to add a new 
edge e2 to create a new reference translation.  In this 
case, if rule r2 really exists in our rule table, we get 
a new reference derivation  t3. After constructing t3, 
we first collect the new tree and C (t3, e2). Then, we 
will move to t3, if the score of t3 is higher than t2. 
Notably, if r2 does not exist in the rule table, we fail 
to create a new reference derivation. In such case, 
we keep the origin derivation unchanged.
Algorithm 1 shows the process of generation.3
The input is a reference derivation  t, and the out- 
put is a new derivation  and the generated derivations.

  3 For simplicity, we list all the trees, and do not compress 
them into a forest  in practice.  It is straight to extent the algo- 
rithm to get a compact forest for those generated derivations. 
Actually,  instead of storing the derivations, we call the generate 
function twice to calculate gradient of log-linear model.



0,4



0,4



0,4






0,1 	2,4


2,4


0,2


2,4







Figure 2: Lexicalize  and generalize operators over t1 (part) in Figure 1. Although here only shows the nodes, we also 
need to change relative edges actually.  (1) Applying lexicalize operator on the non-terminal node X0,1  in (a) results a 
new derivation shown in (b). (2) When visiting bei in (b), the generalize operator changes the derivation  into (c).





The list used for storing forest is initialized with the 
input tree (line 2). We visit the nodes in t in post- 
order (line 3). For each node v, we first append the 
competing derivations C (t,e) to list, where e is in- 
coming edge of v (lines 4-5). Then, we apply oper- 
ators on the child nodes of v from left to right (lines
6-13). The operators returns a reference derivation
tn (line 7). If it is new (line 8), we collect both the tn 
(line 9), and also the competing derivations C (tn, e′) 
of the new derivation on those edges e′ which only
occur in the new derivation (lines 10-11). Finally, if 
the new derivation has a larger score, we will replace 
the origin derivation with new one (lines 12-13).
  Although there is a  two-level loop for visiting 
nodes (line 3 and 6), each node is visited only one 
time in the inner loops. Thus, the complexity is 
linear with the number of nodes #node.  Consid- 
ering that the number of source word (also leaf node 
here) is less than the total number of nodes and is
more than ⌈(#node + 1)/2⌉, the time complexity of
the process is also linear with the number of source
word.

4.1   Lexicalize and Generalize

The function OPERATE  in Algorithm 1 uses two op- 
erators to change a node:  lexicalize  and generalize. 
Figure 2 shows the effects of the two operators. The 
lexicalize operator works on nonterminal nodes. It 
moves away  a nonterminal  node and attaches the 
children of current node to its parent. In Figure 2(b), 
the node X0,1  is deleted, requiring  a more lexical- 
ized rule to be applied to the parent node X0,4  (one 
more terminal in the source side). We constrain the 
lexicalize  operator to apply on pre-terminal  nodes 
whose children  are all terminal nodes. In contrast, 
the generalize operator works on terminal  nodes and


inserts a nonterminal  node between current node and 
its parent node. This operator generalizes over the 
continuous terminal sibling  nodes left to the current 
node (including  the current node). Generalizing the 
node bei in Figure 2(b) results Figure 2(c). A new 
node X0,2  is inserted  as the parent of node qiang- 
shou and node bei.
  Notably,  there are two steps when apply an oper- 
ator. Suppose we want to lexicalize  the node X0,1 
in t1 of Figure 1, we first delete the node X0,1  and 
related edge e1  and e5, then we try to add the new 
edge e2.  Since rule table is fixed, the second step 
is a process of decoding.  Therefore, sometimes we 
may fail to create a new reference derivation  (like 
r2 may not exist in the rule table). In such case, we 
keep the origin derivation unchanged.
  The changes made by the two operators are local. 
Considering the change of rules, the lexicalize oper- 
ator deletes two rules and adds one new rule, while 
the generalize operator deletes one rule and adds two 
new rules. Such local changes provide us with a way 
to incrementally  calculate the scores of new deriva- 
tions. We use this method motivated by Gibbs Sam- 
pler (Blunsom et al., 2009) which has been used for 
efficiently learning rules. The different lies in that 
we use the operator for decoding where the rule ta- 
ble is fixing.

4.2   Initialize a Reference Derivation

The generation  starts from  an initial  reference 
derivation with max score. This requires bi-parsing 
(Dyer, 2010) over the source sentence f and the ref- 
erence translation e. In practice, we may face three 
problems.
First is efficiency problem.	Exhaustive search
over the space under  SCFG  requires  O(|f |3|e|3).


To parse quickly, we only visit the tight consistent 
(Zhang et al., 2008) bi-spans with the help of word 
alignment a.  Only visiting tight consistent  spans 
greatly speeds  up bi-parsing. Besides efficiency,


Algorithm 2 Training
1: procedure TRAIN(S )
2:	Training Data S  = {f n, en, an  N
}n=1
3:	Derivations T  = {}N


adoption of this constraint receives support from the
fact that heuristic SCFG rule extraction only extracts 
tight consistent initial phrases (Chiang,  2007).
  Second is degenerate problem. If we only use 
the features  as traditional SCFG  systems, the bi- 
parsing may end with a derivation  consists of some 
giant rules or rules with rare source/target  sides, 
which is called degenerate solution  (DeNero et al.,
2006). That is because the translation  rules with rare 
source/target sides always receive a very high trans- 
lation probability.  We add a prior score log(#rule) 
for each rule, where #rule is the number of occur- 
rence of a rule, to reward frequent reusable rules and 
derivations with more rules.
  Finally, we may fail to create reference deriva- 
tions due to the limitation in rule extraction.   We
create minimum  trees for (f , e, a) using shift-reduce


4:	for n = 1 to N do
5:	tn ← INITIAL(f n, en, an)
6:	i ← 0
7:	for m = 0 to M do
8:	for n = 0 to N do
9:	η ← LEARNRATE(i)
10:	(∆L(wi, tn), tn) ←GENERATE(tn)
11:	wi  ← wi + η × ∆L(wi, tn)
12:	i ← i + 1
∑M N    i
i=1
M N


  This model defines the conditional probability  of 
a derivation  t and the corresponding translation e 
given a source sentence f as:
exp ∑i λihi(t, e, f )


(Zhang et al., 2008). Some minimum  rules in the 
trees may be illegal according to the definition of


p(t, e|f ) =


(5)
Z (f )


Chiang (2007). We also add these rules to the rule


where the partition function is


table, so as to make sure every sentence is reachable 
given the rule table. A source sentence is reachable 
given a rule table if reference derivations exists. We 
refer these rules as added rules. However, this may



Z (f ) =


∑	∑

e   t∈△(e,f )



exp


∑ λihi(t, e, f ) 	(6)
i


introduce rules with more than two variables and in- 
crease the complexity  of bi-parsing.  To tackle this 
problem, we initialize the chart with minimum par- 
allel tree from the Zhang et al.  (2008) algorithm, 
ensuring that the bi-parsing has at least one path to 
create a reference derivation.  Then we only need to 
consider the traditional rules during bi-parsing.

5   Training


The partition function is approximated by our for-
est, which is labeled  as Z˜(f ), and the derivations 
that produce reference translation is approximated 
by reference derivations in Z˜(f ).
  We estimate the parameters in log-linear model 
using maximum  a posteriori  (MAP) estimator. It 
maximizes the likelihood of the bilingual corpus
S  = {f , e }n=1, penalized using a gaussian prior
(L2 norm) with the probability density function



We use the forest to train a log-linear model with a


p0(λi)  ∝  exp(−λ2/2σ2


).  We set σ2


to 1.0 in our



latent variable as describe in Blunsom et al.(2008).


experiments. This results in the following gradient:


The probability p(e|f ) is the sum over all possible
derivations:


∂L
∂λi



= Ep(t|e,f )


[hi] −



Ep(e|f )



[hi]


λi
− σ2



(7)


p(e|f ) =	∑
t∈△(e,f )


p(t, e|f ) 	(4)



  We use an online learning 
algorithm  to train the 
parameters.  We implement  
stochastic gradient de-


where △(e, f ) is the set of all possible derivations
that translate f into e and t is one such derivation.4


scent (SGD) recommended by Bottou.5	The dy-
namic learning rate we use is    N     , where N is the
0


4 Although the derivation is typically represent as d, we de-	 	


notes it by t since our paper use tree to represent derivation.


5 http://leon.bottou.org/projects/sgd



number of training example, i is the training itera- 
tion, and i0 is a constant number used to get a initial 
learning rate, which is determined by calibration.
  Algorithm 2 shows the entire process.  We first 
create an initial reference derivation for every train- 
ing examples using bi-parsing (lines 4-5), and then 
online learn the parameters using SGD (lines 6-12). 
We use the GENERATE function to calculate the gra- 
dient. In practice, instead of storing all the deriva- 
tions in a list, we traverse the tree twice. The first 
time is calculating the partition function, and the 
second time calculates the gradient normalized by 
partition function. During training, we also change 
the derivations (line 10). When training is finished 
after M  epochs, the algorithm  returns an averaged 
weight vector (Collins, 2002) to avoid overfitting 
(line 13). We use a development  set to select total 
epoch m, which  is set as M = 5 in our experiments.

6   Experiments

Our method is able to train a large number  of fea- 
tures on large data. We  use a set of word context 
features motivated  by word sense disambiguation 
(Chan et al., 2007) to test scalability. A word level 
context feature is a triple (f, e, f+1), which counts 
the number of time that f is aligned to e and f+1 oc- 
curs to the right of f . Triple (f, e, f−1) is similar ex- 
cept that f−1 locates to the left of f . We retain word 
alignment information in the extracted rules to ex- 
ploit such features.  To demonstrate the importance 
of scaling up the size of training  data and the effect 
of our method, we compare three types of training 
configurations which differ in the size of features 
and data.
  MERT. We use MERT (Och, 2003) to training 8 
features on a small data. The 8 features is the same 
as Chiang (2007) including 4 rule scores (direct and 
reverse translation  scores; direct and reverse lexi- 
cal translation scores); 1 target side language model 
score; 3 penalties for word counts, extracted rules 
and glue rule.  Actually, traditional pipeline often 
uses such configuration.
  Perceptron.  We also learn thousands of context 
word features together with the 8 traditional features 
on a small data using perceptron. Following (Chiang 
et al., 2009), we only use 100 most frequent words 
for word context feature. This setting use CKY de-




T
R
A
I
N
R
T
R
AI
N
D
EV
T
E
ST
#
S
e
n
t
.
   #
W
or
d 
A
v
g
. 
L
e
n
. 
L
o
n
. 
L
e
n
.
51
9,
35
9
8
.
6
M
1
6
.
5
9
9
1
8
6,
8
1
0
1
.
3
M
7
.
3
9
5
8
7
8
2
3
K
2
6.
4
7
7
3,
7
8
9
1
0
5
K
2
8
.
0
1
1
6

Table 1: Corpus statistics of Chinese side, where Sent., 
Avg., Lon., and Len.  are short for sentence, longest, 
average, and length  respectively. RTRAIN denotes the 
reachable (given rule table without  added rules) subset of 
TRAIN data.



coder to generate n-best lists for training. The com- 
plexity of CKY decoding limits the training  data into 
a small size. We fix the 8 traditional feature weights 
as MERT  to get a comparable  results as MERT.
  Our Method.  Finally,  we use our method to train 
millions of features on large data. The use of large 
data promises us to use full vocabulary of training 
data for the context word features, which results mil- 
lions of fully lexicalized  context features. During 
decoding, when a context feature does not exit, we 
simply ignore it.  The weights of 8 traditional fea- 
tures are fixed the same as MERT  also. We fix these 
weights because the translation  feature weights fluc- 
tuate intensely during online learning. The main rea- 
son may come from the degeneration solution men- 
tioned in Section 4.2, where rare rules with very high 
translation probability  are selected as the reference 
derivations. Another  reason could be the fact that 
translation  features are dense intensify  the fluctua- 
tion. We leave learning without fixing the 8 feature 
weights to future work.

6.1   Data
We focus on the Chinese-to-English translation task 
in this paper. The bilingual corpus we use con- 
tains 519, 359 sentence pairs, with an average length 
of 16.5 in source  side and 20.3 in target side, 
where 186, 810 sentence  pairs (36%) are  reach- 
able (without added  rules in Section  4.2).   The 
monolingual  data includes  the Xinhua portion of 
the GIGAWORD  corpus, which contains 238M En- 
glish words. We use the NIST evaluation  sets of
2002 (MT02) as our development  set, and sets of 
MT03/MT04/MT05 as test sets.  Table  2 shows the 
statistics of all bilingual corpus.
We use GIZA++ (Och and Ney, 2003) to perform



S
y
s
t
e
m
#
D
AT
A
#
F
EA
T
M
T0
3	MT04	MT05	ALL
M
E
R
T
8
7
8
8
33
.0
3	35.12	32.32	33.85
P
e
r
c
e
p
tr
o
n
8
7
8
2
.
4
K
32
.8
9	34.88	32.55	33.76

O
ur 
M
et
ho
d
1
8
7
K
2
.
0
M
33
.6
4	35.48	32.91*	34.41*

5
1
9
K
1
3.
9
M
34
.1
9*	35.72*	33.09*	34.69*
Improve
ment 
over 
MERT
+1
.1
6	+0.60	+0.77	+0.84

Table 2: Effect of our method comparing with MERT and perceptron in terms of BLEU. We also compare our fast 
generation method with different data (only reachable or full data). #Data is the size of data for training the feature 
weights. * means significantly  (Koehn, 2004) better than MERT (p < 0.01).





word alignment in both directions,  and grow-diag- 
final-and (Koehn et al., 2003) to generate symmet- 
ric word alignment.   We extract SCFG rules as de- 
scribed in Chiang (2007) and also added rules (Sec- 
tion 4.2). Our algorithm runs on the entire training 
data, which requires to load all the rules into the 
memory. To fit within memory, we cut off those 
composed rules which only happen once in the train- 
ing data. Here a composed rule is a rule that can be 
produced by any other extracted rules. A 4-grams 
language model is trained by the SRILM toolkit 
(Stolcke, 2002). Case-insensitive NIST BLEU4 (Pa- 
pineni et al., 2002) is used to measure translation 
performance.
  The training data comes  from a  subset  of the 
LDC data including LDC2002E18, LDC2003E07, 
LDC2003E14,  Hansards portion of LDC2004T07, 
LDC2004T08  and LDC2005T06.  Since the rule ta- 
ble of the entire data is too large to be loaded to 
the memory (even drop one-count rules), we remove 
many sentence pairs to create a much smaller  data 
yet having a comparable performance with the entire 
data. The intuition lies in that if most of the source 
words of a sentence  need  to be translated by the 
added rules, then the word alignment may be highly 
crossed and the sentence may be useless. We cre- 
ate minimum  rules from a sentence pair,  and count 
the number of source words in those minimum  rules 
that are added rules. For example, suppose the result 
minimum rules of a sentence contain r3 which is an 
added rule, then we count 1 time for the sentence. If 
the number of such source word is more than 10% 
of the total number, we will drop the sentence pair.
  We compare the performances of MERT setting 
on three bilingual  data: the entire data that contains
42.3M Chinese and 48.2M English words; 519K


data  that contains 8.6M Chinese and 10.6M En- 
glish words; FBIS (LDC2003E14)  parts that con- 
tains 6.9M Chinese and 9.1M English words. They 
produce 33.11/32.32/30.47  BLEU  tested on MT05 
respectively. The performance of 519K data is com- 
parable with that of entire data, and much higher 
than that of FBIS data.

6.2   Result

Table 3 shows the performance of the three different 
training configurations. The training of MERT and 
perceptron run on MT02. For our method, we com- 
pare two different training sets:  one is trained on 
all 519K sentence pairs, the other only uses 186K 
reachable sentences.
  Although the perceptron  system exploits  2.4K 
features,  it fails to produce  stable improvements 
over MERT.  The reason may come from overfitting, 
since the training  data for perceptron contains only
878 sentences. However,  when use our method to 
learn the word context feature on the 519K data, 
we significantly improve the performance by 0.84 
points on the entire  test sets (ALL). The improve- 
ments  range from 0.60 to 1.16 points on MT03-
05. Because we use the full vocabulary, the num- 
ber of features increased into 13.9 millions,  which is 
impractical to be trained on the small development 
set. These results confirm the necessity of exploiting 
more features and learning the parameters on large 
data. Meanwhile,  such results also demonstrate that 
we can benefits from the forest generated by our fast 
method instead of traditional CKY algorithm.
  Not surprisingly,  the improvements are smaller 
when only use 186K reachable sentences.  Some- 
times we even fail to gain significant improvement. 
This verifies our motivation to guarantee all sentence



180


small) 
rules.



150

120

90

60

30

0












0 	10 	20 
	30 	40 
	50 	60 
	70 	80 
	90
S
e
n
t
e
n
c
e
 
L
e
n
g
t
h



7
   
R
e
l
a
t
e
d
 
W
o
r
k

Minimum  
error rate 
training 
(Och, 
2003) is 
perhaps 
the most 
popular 
discrimin
ative  
training 
for SMT. 
However, 
it fails to 
scale to 
large 
number 
of 
features. 
Research
ers have 
propose 
many 
learning 
algorithm
s to train 
many 
features: 
perceptro
n (Shen et 
al., 2004; 
Liang et 
al., 2006), 
minimum 
risk 
(Smith 
and 
Eisner,
2006; Li 
et al., 
2009), 
MIRA 
(Watanab
e et al., 
2007;


Figure 3: Plot of training times (including forest genera- 
tion and SGD training)  versus sentence length. We ran- 
domly  select 1000 sentence from the 519K data for plot- 
ting.



are reachable, so as to use all training data.

6.3   Speed

How  about the speed of our framework? Our method 
learns in 32 mlliseconds/sentence. Figure 3 shows 
training times (including  forest generation and SGD 
training)  versus sentence length. The plot confirms 
that our training algorithm  scales linearly.  If we 
use n-best  lists which generated by CKY decoder 
as MERT,  it takes about 3105 milliseconds/sentence 
for producing 100-best lists. Our method accelerates 
the speed about  97 times (even though we search 
twice to calculate the gradient). This shows the effi- 
ciency of our method.
  The procedure of training includes two steps. (1) 
Bi-parsing to initialize a reference derivation  with 
max score. (2) Training procedure which generates 
a set of derivations to calculate the gradient and up- 
date parameters.  Step (1) only runs once. The av- 
erage time of processing  a sentence for each step 
is about 9.5 milliseconds and 30.2 milliseconds re- 
spectively.
  For simplicity we do not compress the generated 
derivations into forests, therefore the size of result- 
ing derivations is fairly small, which is about 265.8 
for each sentence on average, where 6.1 of them are 
reference derivations.  Furthermore, we use lexical- 
ize operator more often than generalize operator (the 
ration between them is 1.5 to 1). Lexicalize operator 
is used more frequently mainly dues to that the ref- 
erence derivations  are initialized with reusable (thus


Chiang et al., 2009), gradient descent (Blunsom  et 
al., 2008; Blunsom and Osborne, 2008). The com- 
plexity of n-best lists or packed forests generation 
hamper these algorithms  to scale to a large amount 
of data.
  For efficiency, we only use neighboring  deriva- 
tions for training. Such motivation is same as con- 
trastive estimation (Smith and Eisner, 2005; Poon et 
al., 2009). The difference lies in that the previous 
work actually care about their latent variables (pos 
tags, segmentation,  dependency  trees,  etc), while 
we are only interested in their marginal distribution. 
Furthermore, we focus on how to fast generate trans- 
lation forest for training.
  The local operators lexicalize/generalize  are use 
for greedy decoding. The idea is related to “peg- 
ging” algorithm (Brown et al., 1993) and greedy de- 
coding (Germann et al., 2001). Such types of local 
operators  are also used in Gibbs sampler for syn- 
chronous grammar induction (Blunsom et al., 2009; 
Cohn and Blunsom, 2009).

8   Conclusion and Future Work

We have presented a fast generation algorithm for 
translation  forest which contains  both reference 
derivations and neighboring  non-reference deriva- 
tions for large-scale SMT discriminative training. 
We have achieved significantly improvement of 0.84
BLEU by incorporate 13.9M feature trained on 519K
data in 0.03 second per sentence.
  In this paper, we define the forest based on com- 
peting derivations  which only differ in one rule. 
There may be better classes of forest that can pro- 
duce a better performance.  It’s interesting to modify 
the definition of forest, and use more local operators 
to increase the size of forest. Furthermore, since the 
generation of forests is quite general, it’s straight to


apply our forest on other learning algorithms. Fi- 
nally, we hope to exploit more features such as re- 
ordering  features and syntactic features so as to fur- 
ther improve the performance.

Acknowledgement

We would like to thank Yifan He, Xianhua Li, Daqi 
Zheng,  and the anonymous reviewers for their in- 
sightful comments. The authors were supported by 
National Natural Science Foundation of China Con- 
tracts 60736014, 60873167, and 60903138.


References

Phil Blunsom and Miles Osborne. 2008. Probabilistic 
inference for machine translation. In Proc. of EMNLP
2008.
Phil Blunsom, Trevor Cohn, and Miles Osborne.  2008.
A discriminative  latent variable model for statistical 
machine translation. In Proc. of ACL-08.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os- 
borne. 2009. A gibbs sampler for phrasal synchronous 
grammar induction. In Proc. of ACL 2009.
Peter F. Brown, Vincent J.Della Pietra, Stephen A. Della 
Pietra, and Robert. L. Mercer. 1993. The mathemat- 
ics of statistical machine translation. Computational 
Linguistics, 19:263–311.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
  Word sense disambiguation  improves  statistical  ma- 
chine translation. In Proc. of ACL 2007, pages 33–40. 
David Chiang, Kevin  Knight, and Wei Wang.	2009.
11,001 new features for statistical machine translation.
In Proc. of NAACL 2009.
David Chiang. 2007. Hierarchical phrase-based transla- 
tion. Computational Linguistics, 33(2):201–228.
Trevor Cohn and Phil Blunsom.  2009. A Bayesian model 
of syntax-directed tree to string grammar induction.  In 
Proc. of EMNLP 2009.
Michael Collins. 2002. Discriminative training methods 
for hidden markov models: Theory and experiments 
with perceptron algorithms. In Proc. of EMNLP 2002. 
John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006. Why generative phrase models underperform
surface heuristics.  In Proc. of the HLT-NAACL  2006
Workshop on SMT.
Chris Dyer.  2010. Two monolingual  parses are better 
than one (synchronous parse). In Proc. of NAACL
2010.
Ulrich Germann,  Michael Jahr, Kevin  Knight, Daniel 
Marcu, and Kenji Yamada.  2001. Fast decoding and 
optimal decoding for machine translation.  In Proc. of 
ACL 2001.


Philipp Koehn, Franz Josef  Och, and Daniel Marcu.
2003. Statistical  phrase-based translation. In Proc. 
of HLT-NAACL 2003.
Philipp Koehn. 2004. Statistical  significance  tests for 
machine translation evaluation.   In Proc. of EMNLP
2004.
Zhifei Li and Jason Eisner.  2009.  First- and second-order 
expectation semirings with applications to minimum- 
risk training on translation forests. In Proc. of EMNLP
2009.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.   2009.
Variational decoding for statistical  machine transla- 
tion. In Proc. of ACL 2009.
Percy Liang, Alexandre Bouchard-Coˆ te´, Dan Klein, and 
Ben Taskar. 2006. An end-to-end discriminative ap- 
proach to machine translation. In Proc. of ACL 2006.
Haitao Mi, Liang Huang, and Qun Liu.  2008. Forest- 
based translation.  In Proc. of ACL 2008.
Franz J. Och and Hermann Ney. 2002. Discriminative 
training and maximum entropy models for statistical 
machine translation. In Proc. of ACL 2002.
Franz Josef Och and Hermann Ney.  2003. A system- 
atic comparison of various statistical alignment mod- 
els. Computational Linguistics, 29(1):19–51.
Franz Josef Och. 2003. Minimum error rate training in 
statistical machine translation. In Proc. of ACL 2003.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei- 
Jing Zhu. 2002. Bleu: a method for automatic evalua- 
tion of machine translation. In Proc. of ACL 2002.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological  segmentation with 
log-linear models. In Proc. of NAACL 2009.
Libin Shen, Anoop  Sarkar, and Franz Josef Och. 2004.
Discriminative  reranking for machine translation. In
Proc. of NAACL 2004.
Noah A. Smith and Jason Eisner.  2005.  Contrastive  esti- 
mation: Training log-linear models on unlabeled data. 
In Proc. of ACL 2005.
David A. Smith and Jason Eisner. 2006. Minimum risk 
annealing for training log-linear models. In Proc. of 
COLING/ACL 2006.
Andreas Stolcke.  2002. Srilm – an extensible language 
modeling toolkit. In Proc. of ICSLP 2002.
Christoph Tillmann and Tong Zhang. 2006. A discrim- 
inative global training algorithm for statistical mt. In 
Proc. of ACL 2006.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki 
Isozaki. 2007. Online large-margin training for statis- 
tical machine translation. In Proc. of EMNLP-CoNLL
2007.
Hao Zhang, Daniel Gildea, and David Chiang. 2008. Ex- 
tracting synchronous grammar rules from word-level 
alignments in linear time. In Proc. of Coling 2008.

