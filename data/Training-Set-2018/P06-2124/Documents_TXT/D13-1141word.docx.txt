Bilingual Word Embeddings for Phrase-Based Machine Translation



Will Y. Zou†, Richard Socher, Daniel Cer, Christopher D. Manning
Department of Electrical Engineering† and Computer Science Department
Stanford University, Stanford, CA 94305, USA
{wzou,danielcer,manning}@stanford.edu, richard@socher.org









Abstract

We introduce bilingual word embeddings: se- 
mantic embeddings associated across two lan- 
guages in the context of neural language mod- 
els.  We propose a method to learn bilingual 
embeddings from a large unlabeled corpus, 
while utilizing MT word alignments to con- 
strain translational equivalence.  The new em- 
beddings  significantly  out-perform  baselines 
in word semantic similarity. A single semantic 
similarity feature induced with bilingual em- 
beddings adds near half a BLEU point to the 
results of NIST08 Chinese-English machine 
translation task.


1    Introduction

It is difficult to recognize and quantify semantic sim- 
ilarities across languages.   The Fr-En phrase-pair
{‘un cas de force majeure’, ‘case of absolute neces- 
sity’}, Zh-En phrase pair {‘依然故我’,‘persist in a
stubborn manner’} are similar in semantics.  If co- 
occurrences of exact word combinations are rare in 
the training parallel text, it can be difficult for classi- 
cal statistical MT methods to identify this similarity, 
or produce a reasonable translation given the source 
phrase.
  We  introduce  an  unsupervised   neural  model 
to learn bilingual semantic embedding for words 
across two languages.   As an extension to their 
monolingual   counter-part   (Turian   et  al.,   2010; 
Huang  et  al.,  2012;  Bengio  et  al.,  2003),  bilin- 
gual embeddings capture not only semantic infor- 
mation of monolingual words, but also semantic re- 
lationships across different languages.   This prop-


erty allows them to define semantic similarity met- 
rics across phrase-pairs, making them perfect fea- 
tures for machine translation.

  To learn bilingual embeddings, we use a new ob- 
jective function which embodies both monolingual 
semantics and bilingual translation equivalence. The 
latter utilizes word alignments,  a natural sub-task 
in the machine translation pipeline.  Through large- 
scale curriculum training (Bengio et al., 2009), we 
obtain  bilingual  distributed  representations  which 
lie in the same feature space.   Embeddings of di- 
rect translations overlap, and semantic relationships 
across bilingual embeddings were further improved 
through unsupervised learning on a large unlabeled 
corpus.

  Consequently, we produce for the research com- 
munity a first set of Mandarin Chinese word embed- 
dings with 100,000 words trained on the Chinese 
Gigaword  corpus.    We evaluate  these embedding 
on Chinese word semantic similarity from SemEval-
2012 (Jin and Wu, 2012).  The embeddings sig- 
nificantly out-perform prior work and pruned tf-idf 
base-lines.    In  addition,  the  learned  embeddings 
give rise to 0.11 F1 improvement in Named Entity 
Recognition on the OntoNotes dataset (Hovy et al.,
2006) with a neural network model.

  We apply the bilingual embeddings in an end-to- 
end phrase-based MT system by computing seman- 
tic similarities between phrase pairs.   On NIST08
Chinese-English translation task, we obtain an im- 
provement of 0.48 BLEU from a competitive base- 
line (30.01 BLEU to 30.49 BLEU) with the Stanford 
Phrasal MT system.




1393


Proceedings of the 2013 Conference on Empirical  Methods in Natural  Language Processing, pages 1393–1398, 
Seattle, Washington, USA, 18-21 October 2013. Qc 2013 Association for Computational Linguistics


2    Review of prior work

Distributed word representations are useful in NLP 
applications such as information retrieval (Pas¸ca et 
al.,  2006; Manning et al., 2008), search query ex- 
pansions (Jones et al., 2006), or representing se- 
mantics of words (Reisinger et al., 2010).  A num- 
ber of methods have been explored to train and ap- 
ply word embeddings using continuous models for 
language.      Collobert  et al. (2008)  learn  embed- 
dings in an unsupervised manner through a con- 
trastive estimation technique.    Mnih and Hinton (
2008), Morin and Bengio ( 2005) proposed efficient 
hierarchical continuous-space models.  To system- 
atically compare embeddings,  Turian et al. (2010) 
evaluated improvements they bring to state-of-the- 
art NLP benchmarks.    Huang et al. (2012) intro- 
duced global document context and multiple word 
prototypes.   Recently, morphology is explored to 
learn better word representations through Recursive 
Neural Networks (Luong et al.,  2013).
Bilingual  word  representations  have  been  ex-
plored   with   hand-designed   vector   space   mod-


Here f is a function defined by a neural network. 
wr  is a word chosen in a random subset VR of the 
vocabulary, and cwr   is the context window contain- 
ing word wr .   This unsupervised  objective  func- 
tion contrasts the score between when the correct 
word is placed in context with when a random word 
is placed in the same context.  We incorporate the 
global context information as in Huang et al. (2012), 
shown to improve performance of word embed- 
dings.

3.2    Bilingual initialization and training

In the joint semantic space of words across two lan- 
guages, the Chinese word ‘政府’ is expected to be
close to its English translation ‘government’. At the 
same time, when two words are not direct transla-
tions, e.g.  ‘lake’ and the Chinese word ‘潭’ (deep
pond), their semantic proximity could be correctly 
quantified.
  We describe in the next sub-sections the methods 
to intialize and train bilingual embeddings.  These 
methods  ensure  that  bilingual  embeddings  retain


els  (Peirsman  and  Pado´


,  2010;  Sumita,  2000),


their translational 
equivalence while their 
distribu-


and with unsupervised algorithms such as LDA and
LSA (Boyd-Graber and Resnik, 2010; Tam et al.,
2007; Zhao and Xing, 2006).  Only recently have 
continuous space models been applied to machine 
translation (Le et al.,   2012).  Despite growing in- 
terest in these models, little work has been done 
along the same lines to train bilingual distributioned 
word represenations to improve machine translation. 
In this paper, we learn bilingual word embeddings 
which achieve competitive performance on seman- 
tic word similarity,  and apply them in a practical


tional semantics are improved during online training
with a monolingual corpus.

3.2.1 	Initialization by MT alignments

  First, we use MT Alignment counts as weighting 
to initialize Chinese word embeddings.  In our ex- 
periments, we use MT word alignments extracted 
with the Berkeley Aligner (Liang et al.,   2006) 1 . 
Specifically, we use the following equation to com- 
pute starting word embeddings:


phrase-based MT system.

3 	Algorithm and methods


S
Wt-init  = )
s=1


Cts + 1
Ct + S



Ws 	(2)



3.1    Unsupervised training with global context

Our method starts with embedding learning formu- 
lations in Collobert et al. (2008).  Given a context 
window c in a document d, the optimization mini- 
mizes the following Context Objective for a word w 
in the vocabulary:
  

In this equation, S is the number of possible tar- 
get language words that are aligned with the source 
word. Cts denotes the number of times when word t 
in the target and word s in the source are aligned in 
the training parallel text; Ct  denotes the total num- 
ber of counts of word t that appeared in the target 
language.  Finally, Laplace smoothing is applied to 
this weighting function.


J (c,d)
wr ∈VR


max(0, 1 − f (cw 
, d) + f (cw  , d))

(1)



1 On NIST08 Zh-
En training data 
and data from 
GALE MT
evaluation in the past 
5 years


  Single-prototype English embeddings by Huang 
et al. (2012) are used to initialize Chinese em- 
beddings.  The initialization readily provides a set 
(Align-Init) of benchmark embeddings in experi- 
ments (Section 4), and ensures translation equiva- 
lence in the embeddings at start of training.

3.2.2 	Bilingual training
  Using the alignment counts, we form alignment 
matrices Aen→zh and Azh→en.   For Aen→zh,  each 
row corresponds to a Chinese word, and each col- 
umn an English word.  An element aij  is first as- 
signed the counts of when the ith Chinese word is 
aligned with the jth English word in parallel text. 
After assignments, each row is normalized such that 
it sums to one. The matrix Azh→en is defined sim- 
ilarly.  Denote the set of Chinese word embeddings 
as Vzh, with each row a word embedding, and the 
set of English word embeddings as Ven.  With the 
two alignment matrices, we define the Translation 
Equivalence Objective:


3.3    Curriculum training

We train 100k-vocabulary word embeddings using 
curriculum training (Turian et al., 2010) with Equa- 
tion 5.   For each curriculum, we sort the vocabu- 
lary by frequency and segment the vocabulary by a 
band-size taken from {5k, 10k, 25k, 50k}. Separate 
bands of the vocabulary are trained in parallel using 
minibatch L-BFGS on the Chinese Gigaword cor- 
pus 3 . We train 100,000 iterations for each curricu- 
lum, and the entire 100k vocabulary is trained for
500,000 iterations. The process takes approximately
19 days on a eight-core machine.  We show visual- 
ization of learned embeddings overlaid with English 
in Figure 1. The two-dimensional vectors for this vi- 
sualization is obtained with t-SNE (van der Maaten 
and Hinton,  2008). To make the figure comprehen- 
sible, subsets of Chinese words are provided with 
reference translations in boxes with green borders. 
Words across the two languages are positioned by 
the semantic relationships implied by their embed- 
dings.





JT EO-en    zh =  Vzh − Aen    zhVen  2

JT EO-zh     en =  Ven − Azh    enVzh  2


(3)


(4)



We optimize for a combined objective during train- 
ing. For the Chinese embeddings we optimize for:

           JC O-zh + λJT EO-en→zh 	(5) 
For the English embeddings we optimize for:

JC O-en + λJT EO-zh→en 	(6)



During bilingual training, we chose the value of λ 
such that convergence is achieved for both JC O and 
JT EO .  A small validation set of word similarities 
from (Jin and Wu, 2012) is used to ensure the em- 
beddings have reasonable semantics. 2
  In the next sections, ‘bilingual trained’ embed- 
dings refer to those initialized with MT alignments 
and  trained  with  the  objective  defined by  Equa- 
tion 5.  ‘Monolingual trained’ embeddings refer to 
those  intialized  by alignment  but trained  without 
JT EO-en→zh.

2 In our experiments, λ = 50.


Figure 1: Overlaid bilingual embeddings: English words 
are plotted in yellow boxes, and Chinese words in green; 
reference translations to English are provided in boxes 
with green borders directly below the original word.


4    Experiments

4.1    Semantic Similarity

We evaluate the Mandarin Chinese embeddings with 
the semantic similarity test-set provided by the or-

  3 Fifth Edition. LDC catelog number LDC2011T13. We only 
exclude cna cmn, the Traditional Chinese segment of the cor- 
pus.


Table 1: Results on Chinese Semantic Similarity
Method 	Sp. Corr. 	K. Tau
                        (×100)	(×100) 
Prior work (Jin and Wu, 2012) 		5.0
Tf-idf
Naive tf-idf 	41.5 	28.7
Pruned tf-idf 	46.7 	32.3
Word Embeddings
Align-Init 	52.9 	37.6
Mono-trained 	59.3 	42.1
Biling-trained 	60.8 	43.3


Table 2: Results on Named Entity Recognition
Embeddings 	Prec. 	Rec. 	F1 	Improve
Align-Init 	0.34 	0.52 	0.41
Mono-trained 	0.54 	0.62 	0.58 	0.17
Biling-trained 	0.48 	0.55 	0.52 	0.11


Table 3: Vector Matching Alignment AER (lower is 
bet- ter)
Embeddings 	Prec. 	Rec. 
	AER
Mono-trained 	0.27 	0.32 
	0.71
Biling-trained 	0.37 	0.45 
	0.59





ganizers of SemEval-2012 Task 4. This test-set con- 
tains 297 Chinese word pairs with similarity scores 
estimated by humans.
  The results for semantic similarity are shown in 
Table 1.  We show two evaluation metrics:  Spear- 
man Correlation and Kendall’s Tau. For both, bilin- 
gual embeddings trained with the combined objec- 
tive defined by Equation 5 perform best. For pruned 
tf-idf, we follow Reisinger et al. (2010; Huang et 
al. (2012) and count word co-occurrences in a 10- 
word  window.    We  use  the  best  results  from  a 
range of pruning and feature thresholds to compare 
against our method. The bilingual and monolingual 
trained embeddings4  out-perform pruned tf-idf  by
14.1 and 12.6 Spearman Correlation (×100), respec-
tively. Further, they out-perform embeddings initial- 
ized from alignment by 7.9 and 6.4. Both our tf-idf 
implementation and the word embeddings have sig- 
nificantly higher Kendall’s Tau value compared to 
Prior work (Jin and Wu, 2012). We verified Tau cal- 
culations with original submissions provided by the 
authors.

4.2    Named Entity Recognition

We perform NER experiments on OntoNotes (v4.0) 
(Hovy et al., 2006) to validate the quality of the 
Chinese word embeddings.   Our experimental set- 
up is the same as Wang et al. (2013).  With em- 
beddings, we build a naive feed-forward neural net- 
work (Collobert et al., 2008) with 2000 hidden neu- 
rons and a sliding window of five words. This naive 
setting, without sequence modeling or sophisticated



join optimization, is not competitive with state-of- 
the-art (Wang et al., 2013).  Table 2 shows that the 
bilingual embeddings obtains 0.11 F1 improvement, 
lagging monolingual, but significantly better than 
Align-Init (as in Section3.2.1) on the NER task.

4.3    Vector matching alignment

Translation equivalence of the bilingual embeddings 
is evaluated by naive word alignment to match word 
embeddings by cosine distance.5  The Alignment Er- 
ror Rates (AER) reported in Table 3 suggest that 
bilingual training using Equation 5 produces embed- 
dings with better translation equivalence compared 
to those produced by monolingual training.

4.4    Phrase-based machine translation

Our  experiments  are  performed  using  the  Stan- 
ford Phrasal phrase-based machine translation sys- 
tem (Cer et al., 2010). In addition to NIST08 train- 
ing  data,  we  perform  phrase  extraction,  filtering 
and phrase table learning with additional data from 
GALE MT evaluations in the past 5 years.  In turn, 
our baseline is established at 30.01 BLEU and rea- 
sonably competitive relative to NIST08 results. We 
use Minimum Error Rate Training (MERT) (Och,
2003) to tune the decoder.
  In the phrase-based MT system, we add one fea- 
ture to bilingual phrase-pairs.  For each phrase, the 
word embeddings are averaged to obtain a feature 
vector. If a word is not found in the vocabulary, we 
disregard and assume it is not in the phrase; if no 
word is found in a phrase, a zero vector is assigned


4 Due to variations caused by online minibatch L-BFGS, we	 	


take embeddings from five random points out of last 105 mini- 
batch iterations, and average their semantic similarity results.
  

5 This  is evaluated  on 10,000  randomly  selected  sentence 
pairs from the MT training set.



Table 4: NIST08 Chinese-English translation BLEU 
Method 	BLEU
Our baseline 	30.01
Embeddings
Random-Init Mono-trained 	30.09
Align-Init 	30.31
Mono-trained 	30.40
Biling-trained 	30.49



to it. We then compute the cosine distance between 
the feature vectors of a phrase pair to form a seman- 
tic similarity feature for the decoder.
  Results on NIST08 Chinese-English  translation 
task  are  reported  in  Table  46 . 	An  increase  of
0.48  BLEU  is  obtained  with  semantic  similarity 
with bilingual embeddings. The increase is modest, 
just surpassing a reference standard deviation 0.29
BLEU Cer et al. (2010)7 evaluated on a similar sys-
tem. We intend to publish further analysis on statis- 
tical significance of this result as an appendix. From 
these suggestive evidence in the MT results, random 
initialized monolingual trained embeddings add lit- 
tle gains to the baseline. Bilingual initialization and 
training seem to be offering relatively more consis- 
tent gains by introducing translational equivalence.


5    Conclusion


In this paper, we introduce bilingual word embed- 
dings through initialization and optimization con- 
straint using MT alignments The embeddings are 
learned through curriculum training on the Chinese 
Gigaword corpus.  We show good performance on 
Chinese semantic similarity with bilingual trained 
embeddings. When used to compute semantic simi- 
larity of phrase pairs, bilingual embeddings improve 
NIST08 end-to-end machine translation results by 
just below half a BLEU point. This implies that se- 
mantic embeddings are useful features for improv- 
ing MT systems.  Further, our results offer sugges- 
tive evidence that bilingual word embeddings act as 
high-quality semantic features and embody bilingual 
translation equivalence across languages.


6 We report case-insensitive BLEU
7 With 4-gram BLEU metric from Table 4


Acknowledgments

We gratefully acknowledge the support of the 
Defense Advanced Research Projects Agency 
(DARPA)  Broad  Operational  Language  Transla- 
tion (BOLT) program through IBM. Any opinions, 
findings, and conclusions or recommendations ex- 
pressed in this material are those of the author(s) and 
do not necessarily reflect the view of the DARPA, 
or the US government.  We thank John Bauer and 
Thang Luong for helpful discussions.


References

A. Klementiev, I. Titov and B. Bhattarai.  2012.  Induc- 
ing Crosslingual Distributed Representation of Words. 
COLING.
Y. Bengio,  J. Louradour,  R. Collobert  and J. Weston.
2009. Curriculum Learning. ICML.
Y. Bengio, R. Ducharme, P. Vincent and C. Jauvin. 2003.
A Neural Probabilistic Language Model.   Journal of
Machine Learning Research.
Y. Bengio and Y. LeCunn.  2007.  Scaling learning algo- 
rithms towards AI. Large-Scale Kernal Machines.
J. Boyd-Graber and P. Resnik.  2010.  Holistic sentiment 
analysis across languages: multilingual supervised la- 
tent dirichlet allocation. EMNLP.
D. Cer, M. Galley, D. Jurafsky and C. Manning.  2010.
Phrasal: A Toolkit for Statistical Machine Translation 
with Facilities for Extraction and Incorporation of Ar- 
bitrary Model Features.  In Proceedings of the North 
American Association of Computational Linguistics - 
Demo Session (NAACL-10).
D. Cer, C. Manning and D. Jurafsky.  2010.  The Best 
Lexical Metric for Phrase-Based Statistical MT Sys- 
tem Optimization. NAACL.
R. Collobert and J. Weston. 2008. A unified architecture 
for natural language processing: Deep neural networks 
with multitask learning. ICML.
G. Foster and R. Kuhn. 2009. Stabilizing minimum error 
rate training.  Proceedings of the Fourth Workshop on 
Statistical Machine Translation.
M. Galley, P. Chang, D. Cer, J. R. Finkel and C. D. Man- 
ning.   2008.   NIST Open Machine Translation 2008
Evaluation: Stanford University’s System Description. 
Unpublished working notes of the 2008 NIST Open 
Machine Translation Evaluation Workshop.
S. Green, S. Wang, D. Cer and C. Manning.  2013.  Fast 
and adaptive online training of feature-rich translation 
models. ACL.
G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N.
Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. Sainath


and B. Kingsbury.  2012.  Deep Neural Networks for 
Acoustic Modeling in Speech Recognition. IEEE Sig- 
nal Processing Magazine.
E. Hovy,  M. Marcus,  M. Palmer,  L. Ramshaw and R.
Weischedel.    2006.    OntoNotes:   the 90% solution.
NAACL-HLT.
E. H. Huang, R. Socher, C. D. Manning and A. Y. Ng.
2012.    Improving  Word Representations  via Global
Context and Multiple Word Prototypes. ACL.
P. Jin and Y. Wu.  2012.   SemEval-2012 Task 4:  Eval- 
uating Chinese Word Similarity.  Proceedings of the 
First Joint Conference on Lexical and Computational 
Semantics-Volume 1: Proceedings of the main confer- 
ence and the shared task, and Volume 2: Proceedings 
of the Sixth International Workshop on Semantic Eval- 
uation. Association for Computational Linguistics.
R. Jones.  2006.  Generating query substitutions.  In Pro- 
ceedings of the 15th international conference on World 
Wide Web.
P. Koehn,  F. J. Och and D. Marcu.   2003.   Statistical
Phrase-Based Translation. HLT.
H. Le, A. Allauzen and F. Yvon 2012. Continuous space 
translation models with neural networks. NAACL.
P. Liang, B. Taskar and D. Klein.  2006.  Alignment by 
agreement. NAACL.
M. Luong,  R. Socher and C. Manning.   2013.   Better 
word representations  with recursive neural networks 
for morphology. CONLL.
L. van der Maaten and G. Hinton. 2008. Visualizing data 
using t-SNE. Journal of Machine Learning Research.
A. Maas and R. E. Daly and P. T. Pham and D. Huang and 
A. Y. Ng and C. Potts.  2011.  Learning word vectors 
for sentiment analysis. ACL.
C. Manning and P. Raghavan and H. Schtze. 2008. Intro- 
duction to Information Retrieval.  Cambridge Univer- 
sity Press, New York, NY, USA.
T. Mikolov, M. Karafiat, L. Burget, J. Cernocky and S.
Khudanpur.  2010.  Recurrent neural network based 
language model. INTERSPEECH.
T. Mikolov, K. Chen, G. Corrado and J. Dean. 2013. Ef- 
ficient Estimation of Word Representations in Vector 
Space. arXiv:1301.3781v1.
A. Mnih and G. Hinton.  2008.  A scalable hierarchical 
distributed language model. NIPS.
F. Morin and Y. Bengio. 2005. Hierarchical probabilistic 
neural network language model. AISTATS.
F. Och.  2003.  Minimum error rate training in statistical 
machine translation. ACL.
M. Pas¸ca,  D. Lin, J. Bigham,  A. Lifchits and A. Jain.
2006. Names and similarities on the web: fact extrac- 
tion in the fast lane. ACL.
Y. Peirsman and S. Pado´ . 2010.  Cross-lingual induction 
of selectional preferences with bilingual vector spaces. 
ACL.


J. Reisinger and R. J. Mooney.   2010.   Multi-prototype 
vector-space models of word meaning. NAACL.
F. Sebastiani.  2002. Machine learning in automated text 
categorization. ACM Comput. Surv., 34:1-47, March.
R.  Socher,  J.  Pennington,  E.  Huang,  A.  Y.  Ng  and 
C. D. Manning.   2011.  Semi-Supervised Recursive 
Autoencoders for Predicting Sentiment Distributions. 
EMNLP.
R. Socher, E. H. Huang, J. Pennington,  A. Y. Ng, and 
C. D. Manning.  2011.  Dynamic Pooling and Unfold- 
ing Recursive Autoencoders for Paraphrase Detection. 
NIPS.
E. Sumita.  2000.  Lexical transfer using a vector-space 
model. ACL.
Y. Tam, I. Lane and T. Schultz.   2007.   Bilingual-LSA 
based LM adaptation for spoken language translation. 
ACL.
S. Tellex and B. Katz and J. Lin and A. Fernandes and 
G. Marton.  2003.  Quantitative evaluation of passage 
retrieval algorithms for question answering.  In Pro- 
ceedings of the 26th Annual International ACM SIGIR 
Conference on Search and Development in Informa- 
tion Retrieval, pages 41-47. ACM Press.
J. Turian and L. Ratinov and Y. Bengio. 2010. Word rep- 
resentations:  A simple and general method for semi- 
supervised learning. ACL.
M. Wang, W. Che and C. D. Manning. 2013. Joint Word 
Alignment and Bilingual Named Entity Recognition 
Using Dual Decomposition. ACL.
K. Yamada and K. Knight. 2001. A Syntax-based Statis- 
tical Translation Model. ACL.
B. Zhao and E. P. Xing  2006.  BiTAM: Bilingual topic
AdMixture Models for word alignment. ACL.

