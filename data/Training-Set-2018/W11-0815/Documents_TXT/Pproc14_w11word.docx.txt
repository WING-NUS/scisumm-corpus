Using Distributional Similarity of Multi-way Translations to Predict
Multiword Expression Compositionality

Bahar Salehi,•Paul Cookand Timothy Baldwin•
.. NICTA Victoria Research Laboratory
0 Department  of Computing and Information Systems
The University of Melbourne
Victoria 3010, Australia
bsalehi@student.unimelb.edu.au, paulcook@unimelb.edu.au, tb@ldwin.net




Abstract

We predict  the compositionality of multi­ 
word expressions using distributional sim­ 
ilarity  between  each component word and 
the overall expression, based on transla­ 
tions into multiple languages. We evaluate 
the method over English noun compounds, 
English  verb particle constructions and 
German  noun compounds.  We show  that 
the estimation of compositionality is im­ 
proved when using translations into multi­ 
ple languages, as compared to simply  us­ 
ing  distributional similarity in the source 
language. We further  find that string sim­ 
ilarity complements distributional similar­ 
ity.

1   Compositionality of MWEs

Multiword  expressions (hereafter MWEs)  are 
combinations of words which are lexically, syntac­ 
tically,  semantically or  statistically idiosyncratic 
(Sag et al., 2002; Baldwin  and Kim, 2009).  Much 
research has been carried out on the extraction and 
identification of MWEs1  in English  (Schone  and 
Jurafsky,  2001;  Pecina,  2008;  Fazly et al., 2009) 
and other languages (Dias, 2003; Evert and Krenn,
2005; Salehi et al., 2012).  However, considerably 
less work has addressed the task of predicting  the 
meaning of MWEs, especially in non-English lan­ 
guages.   As a step in this  direction, the focus  of 
this study is on predicting  the compositionality of 
MWEs.
  An MWE is fully compositional if its meaning 
is predictable from its component words, and it is 
non-compositional (or idiomatic) if not.   For ex­ 
ample,  stand up "rise  to one's feet"  is composi-

    1In this paper, we follow Baldwin and Kim (2009) in 
considering MWE "identification" to be a token-level disam­ 
biguation task, and MWE "extraction" to be a type-level lex· 
icon induction task.


tional, because its meaning is clear from the mean­ 
ing of the components stand and up. However, the 
meaning  of strike up "to  start  playing" is largely 
unpredictable from  the  component words  strike 
and up.
  In this study, following  McCarthy  et al. (2003) 
and Reddy et al. (2011), we consider composition­ 
ality  to be graded,  and aim to predict  the degree 
of compositionality.  For example,  in the dataset 
of Reddy  et al. (2011),  climate change is judged 
to be 99%  compositional, while  silver screen is
48%  compositional and  ivory tower is 9%  com­
positional.  Formally, we model compositionality 
prediction as a regression task.
  An explicit  handling of MWEs has been shown 
to be useful in NLP applications (Rarnisch, 2012). 
As an example, Carpuat and Diab (2010) proposed 
two strategies  for integrating MWEs  into statisti­ 
cal  machine  translation.  They  show  that  even  a 
large scale bilingual corpus cannot capture all the 
necessary information to translate MWEs, and that 
in adding  the facility  to model the compositional­ 
ity ofMWEs into their system, they could improve 
translation quality.   Acosta  et  al. (2011)  showed 
that  treating  non-compositional MWEs  as a sin­ 
gle unit in information retrieval improves retrieval 
effectiveness. For example, while searching for 
documents related  to ivory tower, we are almost 
certainly  not  interested in  documents relating  to 
elephant  tusks.
  Our approach  is to use a large-scale multi-way 
translation lexicon to source translations of MWEs 
and their component words, and then model the 
relative similarity between  each of the component 
words and the MWE, using distributional similar­ 
ity  based  on monolingual corpora  for  the source 
language and  each  of the  target  languages.  Our 
hypothesis is  that  using  distributional similarity 
in more  than one language will improve  the pre­ 
diction  of compositionality. hnportantly, in order
to make the method  as language-independent and




472

Proceedings of the 14th Conference of the European Chapter of the Association for Computational  Linguistics, pages 472--481, 
Gothenburg, Sweden, Apri126-30 2014. @2014 Association for Computational Linguistics


broadly-applicable as possible, we make no use of 
corpus preprocessing such as lemmatisation,  and 
rely only on the availability of a translation dictio­ 
nary and monolingual corpora.
  Our results confirm our hypothesis that distri­ 
butional similarity over the source language in ad­ 
dition to multiple target languages improves the 
quality  of compositionality  prediction.   We also 
show that our method can be complemented with 
string similarity (Salehi and Cook, 2013) to further 
improve compositionality prediction.  We achieve 
state-of-the-art results over two datasets.

2   Related Work

Most recent work on predicting the composi­ 
tionality of MWEs can be divided into two 
categories:  language/construction-specific and 
general-purpose.  This can be at either the token­ 
level (over token occurrences of an MWE in a cor­ 
pus) or type-level (over the MWE string, indepen­ 
dent of usage). The bulk of work on composition­ 
ality has been language/construction-specific  and 
operated at the token-level, using dedicated meth­ 
ods to identify instances of a given MWE, and 
specific properties of the MWE in that language 
to predict compositionality (Lin, 1999; Kim and 
Baldwin, 2007; Fazly et al., 2009).
  General-purpose  token-level  approaches  such 
as distributional similarity have been commonly 
applied to infer the semantics of a word/MWE 
(Schone and Jurafsky, 2001; Baldwin et al., 2003; 
Reddy et al., 2011).  These techniques are based 
on the assumption that the meaning of a word is 
predictable from its context of use, via the neigh­ 
bouring words of token-level occurrences of the 
MWE. In order to predict the compositionality of 
a given MWE using distributional similarity, the 
different contexts of the MWE are compared with 
the contexts of its components, and the MWE is 
considered to be compositional if the MWE and 
component words occur in similar contexts.
  Identifying token instances of MWEs is not al­ 
ways easy, especially when the component words 
do not occur sequentially.  For example consider 
put on in put your jacket on, and put your jacket 
on the chair.   In the first example put on is an 
MWE while in the second example,  put on is a 
simple verb with prepositional phrase and not an 
instance of an MWE. Moreover, if we adopt a con­ 
servative identification method, the number of to­ 
ken occurrences  will be limited and the distribu-


tional scores may not be reliable.   Additionally, 
for morphologically-rich languages, it can be dif­ 
ficult to predict the different word forms a given 
MWE type will occur across, posing a challenge 
for our requirement of no language-specific pre­ 
processing.
  Pichotta and DeNero (2013) proposed a token­ 
based  method  for  identifying   English  phrasal 
verbs based on parallel corpora for 50 languages. 
They show that they can identify phrasal verbs bet­ 
ter when they combine information from multiple 
languages, in addition to the information they get 
from a monolingual corpus.  This finding lends 
weight to our hypothesis that using translation data 
and distributional similarity from each of a range 
of target languages, can improve compositionality 
prediction.  Having said that, the general applica­ 
bility of the method is questionable -there are 
many parallel corpora involving English, but for 
other languages, this tends not to be the case.
  Salehi and Cook (2013) proposed a general­ 
purpose  type-based  approach  using  translation 
data from multiple languages, and string similar­ 
ity  between  the MWE  and each  of the compo­ 
nent words. They use training data to identify the 
best-10 languages for a given family ofMWEs, on 
which to base the string similarity, and once again 
find that translation data improves their results 
substantially.  Among the four string similarity 
measures they experimented with, longest com­ 
mon substring was found to perform best.  Their 
proposed method is general and applicable to dif­ 
ferent families of MWEs in different languages. In 
this paper, we reimplement the method of Salehi 
and Cook (2013) using longest common substring 
(LCS), and both benchmark  against this method 
and combine it with our distributional similarity­ 
based method.

3    Our Approach

To predict the compositionality of a given MWE, 
we first measure the semantic similarity between 
the MWE and each of its component words2 using 
distributional similarity based on a monolingual 
corpus in the source language. We then repeat the 
process for translations of the MWE and its com­ 
ponent words into each of a range of target lan­ 
guages, calculating distributional similarity using

  2Note that we will always assume that there are two 
component words, but the method is easily generalisable to 
MWEs with more than two components.



MWE	component1	component2


Score1foreachlanguage
	Score2foreachlangua
ge







Translate
(using Panlex)


Translate
(using Panlex)


Translate
(using Panlex)



csmethod 	csmethod




 


DS	DS
(using Wikiepdia) 	(using Wikiepdia)


j	j
score1	score2




Figure  1:  Outline  of our  approach  to computing 
the  distributional similarity   (DS)  of  translations 
of  an MWE  with  each  of  its  component words, 
for  a given  target  language.  score1  and  score2 
are the similarity for the first and second  compo­ 
nents,  respectively.  We obtain  translations from 
Panlex, and use Wikipedia  as our corpus  for each 
language.


a monolingual corpus in the target language (Fig­ 
ure 1). We additionally use supervised learning  to 
identify  which  target languages (or what weights 
for each language) optimise the prediction  of com­ 
positionality (Figure  2).   We hypothesise that  by 
using multiple  translations -rather than only in­ 
formation from the source language -we will be 
able to better predict compositionality.
  We optionally  combine  our proposed  approach 
with  string   similarity,   calculated   based  on  the 
method of Salehi and Cook (2013), using LCS.
  Below, we detail our method for calculating dis­ 
tributional  similarity in a given language,  the dif­ 
ferent  methods  for combining distributional simi­ 
larity scores into a single estimate  of composition­ 
ality, and finally the method for selecting  the target 
languages to use in calculating compositionality.

3.1   Calculating Distributional Similarity

In order to be consistent across  all languages and 
be as language-independent as possible,  we calcu-










 




Compositionality  score





Figure   2:  Outline   of  the  method   for  combin­ 
ing distributional similarity  scores  from  multiple 
languages, across  the components of  the MWE. 
CSmethod  refers  to one of the methods  described in 
Section  3.2 for calculating compositionality.


late distributional similarity  in the following man­
ner for a given language.
  Tokenisation  is based on whitespace delimiters 
and punctuation; no lemmatisation or case-folding is 
carried  out. Token instances of a given  MWE or 
component word are identified  by full-token  n­ gram 
matching over the token stream.  We assume that  all  
full  stops  and  equivalent   characters for other  
orthographies are sentence  boundaries, and chunk  
the corpora  into (pseudo-)sentences on the basis of 
them.  For each language, we identify  the
51st-1050th most  frequent   words,  and  consider 
them to be content-bearing words,  in the manner of 
Schtitze  (1997).  This is based on the assump­ tion 
that the top-50 most frequent words are stop words, 
and not a good choice of word for calculat­ ing 
distributional similarity  over. That is not to say that 
we can't calculate the distributional similarity for 
stop words, however (as we will for the verb particle 
construction dataset-see Section  4.3.2) they are 
simply  not used as the dimensions in our calculation 
of distributional similarity.
  We  form   a  vector  of  content-bearing  words 
across  all  token  occurrences of  the  target  word,


on the basis of these content-bearing words. Dis­ 
tributional similarity is calculated over these con­ 
text  vectors  using  cosine  similarity.     Accord­ 
ing  to  Weeds  (2003),   using  dependency  rela­ 
tions with the neighbouring words of the target 
word can better predict the meaning of the target 
word. However, in line with our assumption of no 
language-specific preprocessing, we just use word 
co-occurrence.

3.2    Calculating Compositionality
First, we need to calculate a combined composi­ 
tionality score from the individual distributional 
similarities between each component word and the 
MWE. Following Reddy et al. (2011), we combine 
the component scores using the weighted mean (as 
shown in Figure 2):
comp = as1 + (1- a)s2	(1)

where s1 and s2  are the scores for the first and 
the second component, respectively.  We use dif­ 
ferent a settings for each dataset, as detailed in 
Section 4.3.
We experiment with a range of methods for cal­
culating compositionality, as follows:

CS Lt : calculate  distributional   similarity  using 
only distributional similarity in the source 
language corpus (This is the approach used 
by Reddy et al. (2011), as discussed in Sec­ 
tion 2).

CS L2N: exclude the source language, and com­ 
pute the mean of the distributional similarity 
scores for the best-N  target languages.  The 
value of N is selected according to training 
data, as detailed in Section 3.3.

CS Lt +L2N: calculate    distributional    similarity 
over both the source language ( CS Lt)  and 
the mean of the best-N  languages ( CS L2N ), 
and combine via the arithmetic mean.3  This 
is to examine the hypothesis that using 
multiple target languages is better than just 
using the source language.

CSsvR(Lt+L2): train a support vector regressor 
(SVR: Smola and SchOlkopf (2004)) over the 
distributional similarities for all 52 languages 
(source and target languages).

  3We also experimented with taking the mean over all the 
languages -target and source -but found it best to com­ 
bine the scores for the target languages first, to give more 
weight to the source language.


CS string: calculate   string   similarity   using  the 
LCS-based method of Salehi and Cook 
(2013).4

CS string+Lt:  calculate  the  mean  of  the  string 
similarity (CS string) and distributional sim­ 
ilarity in the source language (Salehi and 
Cook, 2013).

CS all:   calculate the mean of the string similarity ( 
CS string) and distributional similarity scores ( 
CS L1 and CS L2N ).

3.3   Selecting Target Languages

We experiment with two approaches for combin­ 
ing the compositionality scores from multiple tar­ 
get languages.
  First, in CS L2N (and CS Lt +L2N and CS all that 
build off it), we use training data to rank the target 
languages according to Pearson's correlation be­ 
tween the predicted compositionality  scores and 
the gold-standard compositionality judgements. 
Based on this ranking, we take the best-N lan­ 
guages, and combine the individual composition­ 
ality scores by taking the arithmetic mean. We se­ 
lect N by determining the value that optimises the 
correlation over the training data. In other words, 
the selection of Nand accordingly the best-N lan­ 
guages are based on nested cross-validation over 
training data, independently of the test data for that 
iteration of cross-validation.
  Second in CSsvR(Lt+L2), we combine the 
compositionality scores from the source and all 51 
target languages into a feature vector, and train an 
SVR over the data using LIBSVM.5

4   Resources

In this section, we describe the resources required 
by our method, and also the datasets used to eval­ 
uate our method.

4.1    Monolingual Corpora for Different
Languages

We collected monolingual corpora for each of 52 
languages  (51 target languages  + 1 source lan­ 
guage) from XML dumps of Wikipedia.  These 
languages  are based on the 54  target languages

  4Due to differences in our random partitioning, our re­ 
ported results over the two English datasets differ slightly 
over the results of Salehi and Cook (2013) using the same 
method.
5http://www.csie.ntu.edu.tw/-cjlin/libsvm


used by Salehi  and Cook (2013), excluding Span­ 
ish because we happened not to have a dump of 
Spanish Wikipedia, and also Chinese and Japanese 
because  of the need for a language-specific word 
tokeniser.  The raw corpora  were preprocessed us­ 
ing the WP2TXT toolbox6 to eliminate XML tags, 
HTML  tags  and  hyperlinks, and  then  tokenisa­ 
tion based on whitespace and punctuation was per­ 
formed.    The  corpora  vary  in  size  from  roughly
150M  tokens for English,  to roughly  640K tokens 
for Marathi.


4.2   Multilingual Dictionary

To  translate the  MWEs   and  their  components, 
we follow  Salehi  and Cook  (2013)  in using  Pan­ 
lex  (Baldwin et  al.,  2010).    This  online  dictio­ 
nary is massively multilingual, covering more than
1353 languages. For each MWE dataset  (see Sec­
tion  4.3),  we translate the MWE  and component 
words  from  the source  language into each  of the
51 languages.

  In instances  where there is no direct translation 
in a given language for a term, we use a pivot lan­ 
guage to find translation(s) in the target language. 
For example,  the English  noun compound silver 
screen has direct translations in only 13languages 
in  Panlex,  including Vietnamese (m2m  bac) but 
not  French.    There  is,  however,  a translation of 
man bac into  French  (cinema), allowing  us  to 
infer an indirect  translation between  silver screen 
and cinema. In this way, if there are no direct 
translations into  a particular target  language, we 
search for a single-pivot translation via each of our 
other  target  languages, and combine  them  all to­ 
gether  as our set of translations for the target lan­ 
guage of interest.
  In the case that no translation (direct or indirect) 
can be found for a given source language term into 
a particular target language, the compositionality 
score for that target language is set to the average 
across all target languages for which scores can be 
calculated for the given term. If no translations are 
available  for any target language  (e.g. the term is 
not in Panlex) the compositionality score for each 
target language  is set to the average  score for that 
target  language across  all other  source  language 
terms.


6http://wp2txt.rubyforge.org/


4.3   Datasets

We  evaluate   our   proposed   method   over   three 
datasets  (two English,  one German), as described 
below.

4.3.1   English Noun Compounds (ENC)
Our first dataset  is made up of 90 binary  English 
noun  compounds, from  the work  of Reddy  et al. 
(2011).   Each  noun  compound was annotated by 
multiple  annotators using the integer scale 0 (fully 
non-compositional) to 5 (fully  compositional). A 
final  compositionality score  was  then  calculated 
as  the  mean  of  the  scores  from  the  annotators. 
If we simplistically consider 2.5 as the threshold 
for compositionality, the dataset  is relatively  well 
balanced, containing 48% compositional and 52% 
non-compositional noun compounds.  Following 
Reddy et al. (2011), in combining the component­ 
wise distributional similarities for this dataset,  we 
weight  the first component in Equation  1 higher 
than the second (a= 0.7).

4.3.2   English Verb Particle Constructions
(EVPC)
The second dataset contains 160 English  verb par­ 
ticle constructions (VPCs), from the work of Ban­ 
nard (2006).  In this dataset, a verb particle con­ 
struction consists of a verb (the head) and a prepo­ 
sitional particle (e.g. hand in, look up or battle on).
  For each  component word  (the verb and parti­ 
cle, respectively), multiple  annotators were asked 
whether  the VPC entails  the component word.  In 
order to translate the dataset into a regression task, 
we calculate the overall compositionality as the 
number  of annotations of entailment for the verb, 
divided by the total number of verb annotations for 
that VPC. That is, following  Bannard et al. (2003), 
we only consider  the compositionality of the verb
component in our experiments (and as such a = 1
in Equation  1).
  One area of particular interest  with this dataset 
will be the robustness of the method  to function 
words  (the  particles), both  under  translation and 
in terms of calculating distributional similarity, al­ 
though the findings of Baldwin  (2006) for English 
prepositions are  at  least  encouraging in  this  re­ 
spect.   Additionally, English  VPCs can occur in 
"split" form  (e.g. put  your jacket on,  from  our 
earlier example), which will complicate identifi­ 
cation,  and the verb component will often  be in­ 
flected and thus not match under our identification 
strategy (for both VPCs and the component verbs).


Dataset	Language 	Frequency	Family
Italian 	100 	Romance
French 	99	Romance


each N is selected over 100 folds on ENC, 
EVPC
and GNC datasets,  respectively.   From  the 
his­



ENC




EVPC




GNC



 German	   86 		Germanic Vietnamese	   83 	Viet-Muong Portuguese 	   62 		Romance Bulgarian 	100 		  Slavic
Breton 	100 	Celtic
 Occitan       
100      
Romance 
Indonesian      
100     
Indonesian 
Slovenian            
100               
Slavic
 Polish		100 		   Slavic Lithuanian 		 99		 Baltic Finnish	 74 		Uralic Bulgarian 		 72 		  Slavic Czech 	40 	Slavic


tograms, 
N = 6, N 
= 15 and 
N = 2 are 
the most
commonl
y selected 
settings 
for ENC, 
EVPC 
and GNC, 
respective
ly. That 
is, 
multiple 
languages 
are 
generally 
used, but 
more 
languages 
are used 
for 
English 
VPCs 
than 
either of 
the 
compoun
d noun 
datasets. 
The 5 
most-
selected 
languages 
for ENC, 
EVPC 
and GNC 
are 
shown in 
Table 1.   
As we 
can see, 
there are 
some 
languages 
which are 
al­ ways 
selected 
for a 
given 
dataset, 
but 
equally 
the 
commonl
y-selected 
languages 
vary 
considera
bly 
between 
datasets.
Further  
analysis  
reveals 
that  32  
(63%)  
target


Table 1: The 5 best languages for the ENC, EVPC 
and GNC datasets.  The language family is based 
on Voegelin and Voegelin (1977).


4.3.3   German  Noun Compounds (GNC)

Our final dataset is made up of 246 German noun 
compounds (von der Heide and Borgwaldt, 2009; 
Schulte im Walde et al., 2013).   Multiple anno­ 
tators were asked to rate the compositionality of 
each German noun compound on an integer scale 
of  1  (non-compositional)  to  7  (compositional). 
The overall compositionality score is then calcu­ 
lated as the mean across the annotators. Note that 
the component words are provided as part of the 
dataset, and that there is no need to perform de­ 
compounding. Following Schulte im Walde et al. 
(2013), we weight the first component higher in 
Equation 1 (a= 0.8) when calculating the overall 
compositionality score.
  This dataset is significant in being non-English, 
and also in that German has relatively rich mor­ 
phology, which we expect to impact on the iden­ 
tification of both the MWE and the component 
words.

5   Results

All experiments are carried out using 10 iterations 
of 10-fold cross validation, randomly partitioning 
the data independently on each of the 10 iterations, 
and averaging across all 100 test partitions in our 
presented results. In the case of CSL2N and other 
methods that make use of it (i.e. CSL1+L2N and 
CSau), the languages selected for a given training 
fold are then used to compute the compositionality 
scores for the instances in the test set. Figures 3a,
3b and 3c are histograms of the number of times


languages  for  ENC,  25  (49%)  target languages 
for EVPC, and only 5 (10%) target languages for 
GNC have a correlation of r  2: 0.1 with gold­ 
standard  compositionality  judgements.    On  the 
other hand, 8 (16%) target languages for ENC, 2 
(4%) target languages for EVPC, and no target lan­ 
guages for GNC have a correlation of r :::; -0.1.

5.1   ENC Results

English noun compounds are relatively easy to 
identify in a corpus,7  because the components oc­ 
cur sequentially, and the only morphological vari­ 
ation is in noun number (singular vs. plural).  In 
other words,  the precision for our token match­ 
ing method is very high, and the recall is also 
acceptably  high.   Partly  as a result  of the ease 
of  identification,  we  get  a  high  correlation  of r  
= 0.700 for CSL1  (using only source language 
data).  Using only target languages (CSL2N), the 
results drop tor  = 0.434, but when we combine 
the  two  (CSL1+L2N), the correlation  is  higher 
than using only source or target language data, at
r  = 0.725.  When we combine all languages us­
ing SVR, the results rise slightly higher again to r  
=  0.744,  which is slightly above the correla­ 
tion of the state-of-the-art method of Salehi and
Cook (2013), which combines their method with 
the method of Reddy et al. (2011) (CS string+LJ  ). 
These last two results support our hypothesis that 
using translation data can improve the prediction 
of compositionality. The results for string similar­ 
ity on its own (CSstring•  r  = 0.644) are slightly 
lower than those using only source language dis­ 
tributional  similarity,  but  when  combined  with

  7Although see Lapata and Lascarides (2003) for discus­ 
sion of the difficulty of reliably identifying low-frequency 
English noun compounds.



 

(a) ENC 	(b)EVPC




'"
10
8



bestN

(c) GNC

Figure 3: Histograms displaying how many times a given N is selected as the best number of languages 
over each dataset.  For example, according to the GNC chart, there is a peak for N  = 2, which shows 
that over 100 folds, the best-2languages achieved the highest correlation on 18 folds.

M
eth
od
Su
m
ma
ry 
of 
the 
M
eth
od
E
N
C
E
VP
C
G
N
C
CS
LJ
So
urc
e 
lan
gu
ag
e
0.7
00
0.
17
7
0.1
41
CS
L2
N
Be
st-
N  
tar
get 
lan
gu
ag
es
0.4
34
0.
39
8
0.1
13
CS
LJ
+
L2
N
So
urc
e 
+ 
be
st-
N 
tar
get 
lan
gu
ag
es
0.7
25
0.
31
2
0.1
78
C
S 
SV
R(
Ll 
+
L2
)
SV
R 
(S
ou
rc
e+ 
all 
51 
tar
get 
lan
gu
ag
es)
0.7
44
0.
38
9
0.0
85
CS
str
in
g
Str
ing 
Si
mi
lar
ity 
(S
ale
hi 
an
d 
Co
ok, 
20
13
)
0.6
44
0.
38
5
0.3
72
C
S 
str
ing
+L
l
C
S 
stri
ng 
+
C
S 
Ll  
(S
ale
hi 
an
d 
Co
ok
, 
20
13
)
0.7
39
0.
36
0
0.3
53
C
Sa
u
C
S 
Ll  
+ 
C
S 
L2
N 
+ 
C
S 
stri
ng
0.7
32
0.
41
7
0.3
64

Table 2: Pearson's correlation on the ENC, EVPC and GNC datasets




CS Ll +L2N  (i.e. CS au) there is a slight rise in cor­
relation (from r = 0.725 to r = 0.732).

5.2    EVPC Results

English VPCs are hard to identify.  As discussed 
in Section 2, VPC components may not occur se­ 
quentially, and even when they do occur sequen­ 
tially, they may not be a VPC. As such, our sim­ 
plistic identification method has low precision and 
recall (hand analysis of 927 identified VPC in­ 
stances would suggest a precision of around 74%). 
There is no question that this is a contributor to 
the low correlation for the source language method 
( CS LJ;   r   = 0.177).   When  we use target lan­ 
guages instead of the source language ( CS L2N ), 
the correlation jumps substantially to r = 0.398.
When we combine English and the target Ian-


guages ( CS Ll +L2N ), the results are actually lower 
than just using the target languages,  because of 
the high weight on the target language,  which is 
not desirable for VPCs, based on the source lan­ 
guage results.   Even for  CS SVR(LJ+£ 2 ), the re­ 
sults (r   = 0.389)  are slightly  below the target 
language-only results.  This suggests that when 
predicting the compositionality  of MWEs which 
are hard to identify in the source language, it may 
actually be better to use target languages only. The 
results for string similarity (CS string:  r = 0.385) 
are similar to those for CS L2N . However, as with 
the ENC dataset,  when we combine string simi­ 
larity and distributional similarity ( CS au), the re­ 
sults improve, and we achieve the state-of-the-art 
for the dataset.

In Table 3, we present classification-based eval-



M
eth
od
Pre
cis
ion
Re
cal
l
F-
sco
re 
({3 
= 
1)
Ac
cur
ac
y
Ba
nn
ard 
et 
al. 
(2
00
3)
6
0
.
8
6
6.
6
6
3
.
6
6
0
.
0
Sal
ehi 
an
d 
Co
ok 
(2
01
3)
8
6
.
2
7
1.
8
7
7
.
4
6
9
.
3


   CSau 	79.5


89.3 	82.0


74.5



Table 3: Results(%) for the binary compositionality prediction task on the EVPC dataset




uation over a subset of EVPC, binarising the com­ 
positionality judgements in the manner of Bannard 
et al. (2003). Our method achieves state-of-the-art 
results in terms of overall F-score and accuracy.

5.3    GNC Results

German is a morphologically-rich language, with 
marking of number  and case on nouns.    Given 
that we do not perform any lemmatization or other 
language-specific preprocessing,  we inevitably 
achieve low recall for the identification of noun 
compound tokens, although the precision should 
be nearly 100%.  Partly because of the resultant 
sparseness in the distributional similarity method, 
the results for  CS L1   are low (r   = 0.141),  al­ 
though they are lower again when using target lan­
guages (r = 0.113). However, when we combine
the source and target languages ( CS L1+L2N)  the 
results improve to r  = 0.178.   The results for 
CSSVR(L1+L2)•  on the other hand, are very low
(r =  0.085).   Ultimately, simple string similar­
ity achieves the best results for the dataset (r =
0.372), and this result actually drops slightly when 
combined with the distributional similarities.
  To better understand the reason for the lacklus­ 
tre results using SVR, we carried out error analysis 
and found that, unlike the other two datasets, about 
half of the target languages return scores which 
correlate negatively with the human judgements. 
When we filter these languages from the data, the 
score for SVR improves appreciably. For example, 
over the best-3 languages overall, we get a corre­
lation score of r = 0.179, which is slightly higher
than CS L1+L2N.
  We further investigated the reason for getting 
very low and sometimes negative correlations with 
many  of  our  target  languages.    We noted  that 
about  24%  of  the German  noun  compounds  in 
the dataset do not have entries in Panlex.  This 
contrasts with ENC where only one instance does 
not have an entry in Panlex, and EVPC where all 
VPCs have translations in at least one language in 
Panlex.  We experimented  with using string sim­ 
ilarity scores in the case of such missing transla-


tions, as opposed to the strategy described in Sec­ 
tion 4.2.   The results for  CS SVR(Ll +L2) rose to r 
= 0.269, although this is still below the correla­ 
tion for just using string similarity.
  Our results on the GNC dataset using string 
similarity are competitive with the state-of-the-art 
results (r = 0.45) using a window-based distribu­ 
tional similarity approach over monolingual Ger­ 
man data (Schulte im Walde et al., 2013).  Note, 
however, that their method used part-of-speech in­ 
formation and lemmatisation, where ours does not, 
in keeping with the language-independent philos­ 
ophy of this research.

6   Conclusion and Future Work

In this study, we proposed a method to predict the 
compositionality of MWEs based on monolingual 
distributional  similarity  between  the  MWE  and 
each  of its component  words,  under  translation 
into multiple target languages.  We showed that 
using translation and multiple target languages en­ 
hances compositionality modelling, and also that 
there is strong complementarity between our ap­ 
proach and an approach based on string similarity.
  In future work, we hope to address the ques­ 
tion of translation sparseness, as observed for the 
GNC dataset. We also plan to experiment with un­ 
supervised morphological analysis methods to im­ 
prove identification recall, and explore the impact 
of tokenization. Furthermore, we would like to in­ 
vestigate the optimal number of stop words and 
content-bearing words for each language, and to 
look into the development of general unsupervised 
methods for compositionality prediction.

Acknowledgements

We thank the anonymous reviewers for their 
insightful comments and valuable suggestions. 
NICTA is funded by the Australian government as 
represented by Department of Broadband, Com­ 
munication and Digital Economy, and the Aus­ 
tralian Research Council through the ICT Centre 
of Excellence programme.


References

Otavio Acosta, Aline Villavicencio, and Viviane Mor­ 
eira.   2011.   Identification  and treatment  of multi­ 
word expressions  applied  to information  retrieval. 
In Proceedings of the Workshop on Multiword  Ex­ 
pressions: from Parsing and Generation to the Real 
World, pages 101-109, Portland, USA.

Timothy Baldwin and Su Nam Kim. 2009. Multiword 
expressions.   In Nitin Indurkhya  and Fred J. Dam­ 
erau, editors, Handbook of Natural Language Pro­ 
cessing. CRC Press, Boca Raton, USA, 2nd edition.

Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and 
Dominic  Widdows.    2003.    An  empirical  model 
of multiword expression decomposability.  In Pro­ 
ceedings of the ACL-2003 Workshop on Multiword 
Expressions: Analysis, Acquisition and Treatment, 
pages 89-96, Sapporo, Japan.

Timothy Baldwin, Jonathan Pool, and Susan M Colow­ 
ick.    2010.    Panlex  and !extract:   Translating  all 
words of all languages  of the world.   In Proceed­ 
ings of the 23rd International Conference on Com­ 
putational  Linguistics:  Demonstrations,  pages 37-
40, Beijing, China.

Timothy Baldwin.  2006.  Distributional similarity and 
preposition semantics.   In Patrick Saint-Dizier, ed­ 
itor, Computational  Linguistics Dimensions of Syn­ 
tax and Semantics of Prepositions, pages 197-210. 
Springer, Dordrecht, Netherlands.

Colin  Bannard,   Timothy  Baldwin,   and  Alex  Las­ 
carides.  2003.  A statistical approach to the seman­ 
tics of verb-particles.    In Proceedings  of the ACL
2003 workshop on Multiword expressions: analysis, 
acquisition and treatment-Volume I8, pages 65-72, 
Sapporo, Japan.

Colin James Bannard. 2006. Acquiring Phrasal Lexi­ 
cons from Corpora.  Ph.D. thesis, University of Ed­ 
inburgh.

Marine Carpuat and Mona Diab.  2010.  Task-based 
evaluation of multiword  expressions:  a pilot study 
in statistical machine translation.   In Human Lan­ 
guage Technologies:  The 20IO Annual Conference 
of the North American  Chapter  of the Association 
for Computational  Linguistics, pages 242-245, Los 
Angeles, USA.

Gael Dias. 2003. Multiword unit hybrid extraction. In 
Proceedings of the ACL 2003 Workshop on Multi­ 
word Expressions:  Analysis, Acquisition and Treat­ 
ment, pages 41-48, Sapporo, Japan.

Stefan Evert and Brigitte Krenn.  2005.  Using small 
random samples for the manual evaluation of statis­ 
tical association measures.   Computer Speech and 
Language, Special Issue on Multiword Expressions,
19(4):450-466.

Msaneh  Fazly,  Paul  Cook,  and  Suzanne  Stevenson.
2009. Unsupervised type and token identification of



idiomatic  expressions.   Computational  Linguistics,
35(1):61-103.

Su Nam Kim and Timothy Baldwin.  2007.  Detecting 
compositionality of English verb-particle construc­ 
tions using semantic similarity.   In Proceedings of 
the 7th Meeting of the Pacific Association for Com­ 
putational Linguistics (PACUNG  2007), pages 40--
48, Melbourne, Australia.

Mirella Lapata  and Alex Lascarides.    2003.   Detect­ 
ing novel compounds:  The role of distributional ev­ 
idence.   In Proceedings of the 11th Conference  of 
the European Chapter for the Association of Compu­ 
tational Linguistics (EACL-2003), pages 235-242, 
Budapest, Hungary.

Dekang  Lin.      1999.     Automatic  identification  of 
non-compositional  phrases.   In Proceedings  of the
37th annual meeting of the Association for Compu­ 
tational Linguistics on Computational Linguistics, 
pages 317-324, College Park, USA.

Diana   McCarthy,   Bill   Keller,   and   John   Carroll.
2003.   Detecting  a continuum  of compositionality 
in phrasal verbs.  In Proceedings of the ACL 2003 
workshop on Multiword expressions:  analysis, ac­ 
quisition and treatment-Volume 18, pages 73-80, 
Sapporo, Japan.

Pavel Pecina.   2008.  Lexical Association Measures: 
Collocation Extraction.   Ph.D. thesis, Faculty of 
Mathematics and Physics, Charles University in 
Prague, Prague, Czech Republic.

Karl  Pichotta  and  John  DeNero.     2013.     Identify­ 
ing phrasal verbs using many bilingual corpora.  In 
Proceedings  of the 20I 3 Conference  on Empirical 
Methods in Natural Language Processing (EMNLP
2013), Seattle, USA.

Carlos Ramisch.  2012.  A generic framework for mul­ 
tiword expressions treatment:  from acquisition to 
applications.   In Proceedings of ACL 2012 Student 
Research Workshop, pages 61--66, Jeju Island, Ko­ 
rea.

Siva Reddy, Diana McCarthy, and Suresh Manandhar.
2011.   An empirical study on compositionality  in 
compound nouns. In Proceedings of IJCNLP, pages
210--218, Chiang Mai, Thailand.

Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes­ 
take, and Dan Flickinger.   2002.   Multiword ex­ 
pressions:  A pain in the neck for NLP.   In Pro­ 
ceedings of the 3rd International Conference on 
Intelligent Text Processing Computational  Linguis­ 
tics (CICLing-2002), pages 189-206, Mexico City, 
Mexico.

Bahar  Salehi  and  Paul  Cook.      2013.      Predicting 
the compositionality of multiword expressions using 
translations  in multiple languages.   In Proceedings 
of the Second Joint Conference on Lexical and Com­ 
putational Semantics, volume 1, pages 266-275, At­ 
lanta, USA.


Bahar  Salehi,  Narjes  Askarian,  and  Msaneh   Fazly.
2012. Automatic identification of Persian light verb 
constructions.     In  Proceedings of the 13th Inter­ 
national Conference on Intelligent Text Processing 
Computational Linguistics (ClCLing-2012), pages
201-210, New Delhi, India.

Patrick Schone and Dan Jurafsky. 2001. Is knowledge­ 
free induction of multiword unit dictionary head­ 
words a solved problem.  In Proceedings of the 6th 
Conference on Empirical Methods in Natural Lan­ 
guage Processing (EMNLP 2001), pages 100-108, 
Hong Kong, China.

Sabine Schulte im Walde, Stefan Miiller, and Stephen 
Roller.   2013.   Exploring vector space models to 
predict the compositionality of German noun-noun 
compounds.    In  Proceedings of the Second Joint 
Conference on Lexical and Computational Seman­ 
tics, Atlanta, USA.

Hinrich Schiitze.  1997.  Ambiguity Resolution in Lan­
guage Learning. CSLI Publications, Stanford, USA.

Alex J Smola and Bernhard Scholkopf.   2004.  A tu­ 
torial on support  vector regression.   Statistics and 
Computing, 14(3):199-222.

Charles Frederick Voegelin and Florence Marie 
Voegelin.    1977.    Classification and index of the 
world's languages, volume 4. New York: Elsevier.

Claudia von der Heide and Susanne Borgwaldt.  2009.
Assoziationen  zu  Unter,  Basis  und Oberbegriffen.
Eine explorative Studie.   In Proceedings of the 9th
Norddeutsches Linguistisches  Kolloquium,  pages
51-74.

Julie Elizabeth Weeds.  2003.  Measures and applica­ 
tions of lexical distributional similarity. Ph.D. the­ 
sis, University of Sussex.

