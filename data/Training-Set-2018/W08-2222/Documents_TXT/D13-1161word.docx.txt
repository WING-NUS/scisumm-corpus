Scaling Semantic Parsers with On-the-fly Ontology Matching



Tom Kwiatkowski 	Eunsol Choi	Yoav Artzi	Luke Zettlemoyer
Computer Science & Engineering 
University of Washington Seattle, 
WA 98195
{tomk,eunsol,yoav,lsz}@cs.washington.edu








Abstract

We consider the challenge of learning seman- 
tic parsers that scale to large, open-domain 
problems,  such as question  answering  with 
Freebase. In such settings, the sentences cover 
a  wide variety of topics and include many 
phrases  whose meaning  is difficult to rep- 
resent  in a  fixed target ontology.  For ex- 
ample,  even simple  phrases such as ‘daugh- 
ter’ and ‘number of people  living in’  can- 
not be directly  represented in Freebase, whose 
ontology instead  encodes  facts about gen- 
der, parenthood, and population. In this pa- 
per, we introduce a new semantic parsing ap- 
proach that learns to resolve  such ontologi- 
cal mismatches.   The parser is learned from 
question-answer  pairs, uses a probabilistic 
CCG to build linguistically motivated logical- 
form meaning  representations, and includes 
an ontology matching model that adapts the 
output logical forms for each target ontology. 
Experiments demonstrate state-of-the-art per- 
formance on two benchmark semantic parsing 
datasets, including  a nine point accuracy im- 
provement  on a recent Freebase QA corpus.


1   Introduction

Semantic parsers map sentences to formal represen- 
tations of their underlying meaning. Recently, al- 
gorithms have been developed to learn such parsers 
for many applications, including question answering 
(QA) (Kwiatkowski  et al., 2011; Liang et al., 2011), 
relation extraction  (Krishnamurthy  and Mitchell,
2012), robot control (Matuszek et al., 2012; Kr- 
ishnamurthy and Kollar, 2013), interpreting instruc-


tions (Chen and Mooney, 2011; Artzi and Zettle- 
moyer, 2013), and generating programs (Kushman 
and Barzilay, 2013).
  In each  case,  the parser  uses a  predefined   set 
of logical constants,  or an ontology, to construct 
meaning  representations.   In practice,  the choice 
of ontology significantly impacts learning.   For 
example, consider the following questions (Q) and 
candidate meaning representations (MR):

Q1:	What is the population of Seattle? 
Q2:	How many people live in Seattle?
MR1:	λx.population(Seattle, x)
MR2:	count(λx.person(x) ∧ live(x, Seattle))

A semantic parser might aim to construct MR1 for 
Q1 and MR2 for Q2; these pairings align constants 
(count, person, etc.)  directly to phrases (‘How 
many,’ ‘people,’ etc.). Unfortunately, few ontologies 
have sufficient  coverage to support both meaning 
representations,  for example  many QA databases 
would only include the population relation required 
for MR1.  Most existing  approaches would, given 
this deficiency, simply aim to produce MR1 for Q2, 
thereby introducing significant lexical ambiguity 
that complicates learning.  Such ontological  mis- 
matches become increasingly common as domain 
and language complexity  increases.
  In this paper, we introduce a semantic parsing ap- 
proach that supports scalable, open-domain ontolog- 
ical reasoning. The parser first constructs  a linguis- 
tically motivated domain-independent meaning rep- 
resentation. For example, possibly producing MR1 
for Q1 and MR2 for Q2 above. It then uses a learned 
ontology matching model to transform this represen-



1545


Proceedings of the 2013 Conference on Empirical  Methods in Natural  Language Processing, pages 1545–1556, 
Seattle, Washington, USA, 18-21 October 2013. Qc 2013 Association for Computational Linguistics


x : 	How many people visit the public library of New York annually
l0 : 	λx.eq(x, count(λy.people(y) ∧ ∃e.visit(y, ιz.public(z) ∧ library(z) ∧ of (z, new york), e) ∧ annually(e)))
y : 	λx.library.public library system.annual visits(x, new york public library)
a : 	13,554,002
x : 	What works did Mozart dedicate to Joseph Haydn
l0 : 	λx.works(x) ∧ ∃e.dedicate(mozart, x, e) ∧ to(haydn, e)))
y : 	λx.dedicated work(x) ∧ ∃e.dedicated by(mozart, e) ∧ dedication(x, e) ∧ dedicated to(haydn, e)))
a : 	{ String Quartet No. 19, Haydn Quartets, String Quartet No. 16, String Quartet No. 18, String Quartet No. 17 }
Figure 1: Examples of sentences x, domain-independent underspecified logical forms l0, fully specified 
logical forms y, and answers a drawn from  the Freebase domain.




tation for the target domain. In our example, pro- 
ducing either MR1, MR2 or another more appropri- 
ate option,  depending on the QA database schema. 
This two stage approach  enables parsing  without 
any domain-dependent lexicon that pairs words with 
logical constants.   Instead, word meaning is filled 
in on-the-fly through ontology matching, enabling 
the parser to infer the meaning of previously un- 
seen words and more easily transfer across domains. 
Figure 1 shows the desired outputs for two example 
Freebase sentences.
  The first parsing stage uses a probabilistic combi- 
natory categorial grammar (CCG) (Steedman, 2000; 
Clark and Curran, 2007) to  map sentences  to 
new, underspecified logical-form  meaning represen- 
tations containing generic logical constants that are 
not tied to any specific ontology.  This approach en- 
ables us to share grammar structure across domains, 
instead of repeatedly re-learning different grammars 
for each target  ontology.  The ontology-matching 
step considers  a large number of type-equivalent 
domain-specific meanings. It enables us to incorpo- 
rate a number of cues, including  the target ontology 
structure and lexical similarity between the names of 
the domain-independent  and dependent constants, to 
construct the final logical forms.
  During learning, we estimate a linear model over 
derivations that include all of the CCG parsing de- 
cisions and the choices for ontology matching. Fol- 
lowing a number of recent approaches (Clarke et al.,
2010; Liang et al., 2011), we treat all intermediate 
decisions as latent  and learn from data containing 
only easily gathered question answer pairs. This ap- 
proach aligns naturally with our two-stage parsing 
setup, where the final logical expression can be di- 
rectly used to provide answers.
We   report performance on  two  benchmark


datasets: GeoQuery (Zelle and Mooney, 1996) and 
Freebase QA (FQ) (Cai and Yates, 2013a). Geo- 
Query  includes  a geography database with a small 
ontology and questions  with relatively complex, 
compositional structure. FQ includes questions to 
Freebase, a large community-authored database that 
spans many sub-domains. Experiments demonstrate 
state-of-the-art performance in both cases, including 
a nine point improvement in recall for the FQ test.

2   Formal Overview

Task  Let an ontology O be a set of logical con- 
stants and a knowledge   base K be a collection  of
logical statements constructed with constants from 
O. For example, K could be facts in Freebase (Bol- 
lacker et al., 2008) and O would define the set
of entities and relation  types used to encode those 
facts. Also, let y be a logical expression that can
be executed  against  K to return an answer a  =
EXEC(y, K).  Figure 1 shows example queries and
answers for Freebase. Our goal is to build a function
y = PARSE(x, O) for mapping a natural language
sentence x to a domain-dependent logical form y.


Parsing  We  use a two-stage  approach  to define 
the space of possible parses GEN(x, O) (Section 5).
First, we use  a CCG and word-class information 
from Wiktionary1 to build domain-independent un- 
derspecified logical forms, which closely mirror the 
linguistic structure of the sentence but do not use
constants from O. For example, in Figure 1, l0 de-
notes the underspecified logical forms paired with
each sentence x. The parser then maps this interme- 
diate representation to a logical  form that uses con-
stants from O, such as the y seen in Figure 1.

1 www.wiktionary.com


question-answer pairs {(xi, ai)  : i = 1 . . . n} and 
a  corresponding   knowledge   base K.    The learn-
ing algorithm  (Section 7.1) estimates the parame- 
ters of a linear model for ranking the possible en-
tires in GEN(x, O).   Unlike  much previous work
(e.g., Zettlemoyer  and Collins (2005)), we do not

induce a CCG lexicon. The lexicon is open domain, 
using no symbols from the ontology O for K. This
allows  us to write a single  set of lexical templates 
that are reused in every domain (Section 5.1). The 
burden of learning word meaning is shifted to the 
second, ontology  matching,  stage of parsing (Sec- 
tion 5.2), and modeled with a number  of new fea- 
tures (Section 7.2) as part of the joint model.

Evaluation We  evaluate  on held out question- 
answer pairs in two benchmark  domains, Freebase 
and GeoQuery.   Following Cai and Yates (2013a), 
we also report a cross-domain evaluation where the 
Freebase data is divided by topics such as sports, 
film, and business.  This condition  ensures that the 
test data has a large percentage of previously unseen 
words, allowing us to measure the effectiveness of 
the real time ontology matching.

3   Related Work

Supervised approaches for learning semantic parsers 
have received significant  attention,  e.g. (Kate and 
Mooney, 2006; Wong and Mooney, 2007; Muresan,
2011; Kwiatkowski  et al., 2010, 2011, 2012; Jones 
et al., 2012).  However,  these techniques require 
training data with hand-labeled domain-specific log- 
ical expressions. Recently, alternative forms of su- 
pervision were introduced, including learning from 
question-answer pairs (Clarke et al., 2010; Liang 
et al., 2011), from conversational logs (Artzi and 
Zettlemoyer, 2011), with distant supervision  (Kr- 
ishnamurthy  and Mitchell, 2012; Cai and Yates,
2013b), and from sentences  paired with system 
behavior  (Goldwasser and Roth, 2011; Chen and 
Mooney, 2011; Artzi and Zettlemoyer,  2013). Our 
work adds to these efforts  by demonstrating  a new 
approach for learning with latent meaning represen- 
tations that scales to large databases like Freebase.
  Cai and Yates (2013a)  present the most closely 
related work. They applied schema matching tech- 
niques to expand  a CCG lexicon learned with the


proach was one of the first to scale to Freebase, but 
required labeled logical forms and did not jointly 
model semantic parsing and ontological  reasoning. 
This method  serves as the state of the art for our 
comparison in Section 9.
  We build on a number  of existing algorithmic 
ideas, including  using CCGs to build meaning rep- 
resentations (Zettlemoyer and Collins,  2005, 2007; 
Kwiatkowski et al., 2010, 2011), building deriva- 
tions to transform the output of the CCG parser 
based on context (Zettlemoyer  and Collins, 2009), 
and using weakly supervised margin-sensitive  pa- 
rameter updates  (Artzi  and Zettlemoyer, 2011,
2013). However, we introduce the idea of learning 
an open-domain CCG semantic parser; all previous 
methods suffered, to various degrees, from the onto- 
logical mismatch problem that motivates our work.
  The challenge of ontological  mismatch has been 
previously recognized  in many settings.  Hobbs 
(1985) describes the need for ontological promiscu- 
ity in general language understanding.   Many pre- 
vious hand-engineered natural language understand- 
ing systems (Grosz et al., 1987; Alshawi,  1992; Bos,
2008)  are designed to build general meaning rep- 
resentations that are adapted for different domains. 
Recent efforts to build natural language interfaces to 
large databases, for example DBpedia (Yahya et al.,
2012; Unger et al., 2012), have  also used hand- 
engineered ontology matching techniques. Fader 
et al. (2013) recently presented a scalable approach 
to learning an open domain QA system, where onto- 
logical mismatches are resolved with learned para- 
phrases. Finally, the databases research  commu- 
nity has a long history of developing  schema match- 
ing techniques (Doan et al., 2004; Euzenat et al.,
2007), which has inspired more recent work on dis- 
tant supervision for relation extraction with Free- 
base (Zhang et al., 2012).

4   Background

Semantic Modeling  We use the typed lambda cal- 
culus to build logical forms that represent the mean- 
ings of words, phrases and sentences. Logical forms 
contain constants, variables, lambda abstractions, 
and literals. In this paper, we use the term literal to 
refer to the application of a constant to a sequence of


N 	N \N/N P	N P
λx.library(x)   λyλf λx.f (x) ∧ loc(x, y) 	N Y C
>
N \N
λf.λx.f (x) ∧ loc(x, N Y C )
<
N
λx.library(x) ∧ loc(x, N Y C )

Figure 2: A sample CCG parse.


arguments. We include types for entities e, truth val- 
ues t, numbers i, events ev, and higher-order  func-
tions,  such as (e, t) and ((e, t), e).  We use David-
sonian event semantics (Davidson, 1967) to explic-
itly represent events using event-typed variables and 
conjunctive modifiers to capture thematic roles.

Combinatory Categorial Grammars (CCG) 
CCGs are  a  linguistically-motivated    formalism 
for modeling   a wide range of language phenom- 
ena (Steedman, 1996, 2000). A CCG is defined by 
a lexicon and a set  of combinators. The lexicon 
contains  entries  that pair words or phrases  with 
CCG categories. For example,  the lexical entry
library  f-  N  :  λx.library(x) in Figure 2 pairs
the word ‘library’ with the CCG category that has
syntactic category N and meaning λx.library(x). 
A CCG parse starts from assigning lexical  entries to 
words and phrases. These are then combined using 
the set of CCG combinators to build a logical  form 
that captures the meaning of the entire sentence. We 
use the application, composition, and coordination 
combinators. Figure 2 shows an example parse.


5   Parsing Sentences to Meanings
The function GEN(x, O) defines the set of possible 
derivations for an input sentence x. Each derivation
d = (Π, M ) builds a logical form y using constants 
from the ontology O.  Π is a CCG  parse tree that
maps x to an underspecified logical form l0. M is an 
ontological  match that maps l0 onto the fully spec- 
ified logical form y.  This section describes, with 
reference to the example in Figure 3, the operations 
used by Π and M .

5.1   Domain Independent Parsing

Domain-independent CCG parse trees Π are built 
using a predefined   set of 56 underspecified  lexi-


and the combinatory rules introduced in Section 4.
  An underspecified CCG lexical category  has a 
syntactic category and a logical  form containing no
constants from the domain ontology O. Instead, the
logical form includes underspecified constants that
are typed placeholders which will later be replaced 
during ontology matching. For example,   a noun 
might be assigned the lexical  category N : λx.p(x),
where p is an underspecified (e, t)-type constant.
During parsing, lexical categories are created dy-
namically.  We manually define a set of POS tags for 
each underspecified  lexical category,  and use Wik- 
tionary as a tag dictionary to define the possible POS 
tags for words and phrases. Each phrase is assigned 
every matching lexical category.  For example, the 
word ‘visit’ can be either a verb or a noun in Wik- 
tionary. We accordingly assign it all underspecified 
categories for the classes, including:

N : λx.p(x)  ,  S\NP/NP : λxλy∃ev.p(y, x, ev)

for nouns and transitive verbs respectively.
  We also define domain-independent lexical items 
for function words such as  ‘what,’  ‘when,’  and
‘how  many,’    ‘and,’    and  ‘is.’        These lexi- 
cal  items pair  a   word  with  a   lexical  cate- 
gory  containing only  domain-independent  con-
stants. For example,  how many  f-  S/(S\NP)/N  :
λf.λg.λx.eq(x, count(λy.f (y) ∧ g(y)))  contains
the function count and the predicate eq.
  Figure 3a shows the lexical categories and combi- 
nator applications used to construct the underspeci- 
fied logical form l0. Underspecified constants in this 
figure have been labeled with the words that they are 
associated with for readability.

5.2   Ontological Matching

The second, domain specific, step M maps the un- 
derspecified logical form l0 onto the fully specified 
logical form y.  The mapping from constants in l0 
to constants in y is not one-to-one. For example, in 
Figure 3, l0 contains 11 constants but y contains only
2. The ontological match is a sequence of matching 
operations M  = (o1 . . . , on) that can transform the
structure of the logical form or replace underspeci- 
fied constants with constants from O.


(a) Underspecified CCG parse Π: Map words onto underspecified lexical categories as described in Section 5.1. Use 
the CCG combinators to combine lexical categories to give the full underpecified logical form l0 .


S
λf.λg.λx.eq(x, count(  λx.P eople(x)    λx.λy.∃ev.	λf.ιx.f (x)  λf.λx.f (x)∧   λx.Library(x)  λy.λf.λx.Of    N ewY ork  λev.Annually(ev)
λy.f (y) ∧ g(y)))	V isit(y, x, ev)	P ublic(x)	(x, y) ∧ f (x)
>	>
<
>
>
>	<
>
S
l0  :     λx.eq(x, count(λy.P eople(y) ∧ ∃e.V isit(y, ιz.P ublic(z) ∧ Library(z) ∧ Of (z, N ewY ork)) ∧ Annually(e)))

(b) Structure  Matching  Steps in M : Use the operators described in Section 5.2.1 and Figure 4 to transform l0 . In 
each step one of the operators is applied to a subexpression  of the existing logical form to generate a modified  logical 
form with a new underspecified constant marked in bold.

l0 : 	λx.eq(x, count(λy.P eople(y) ∧ ∃e.V isit(y, ιz.P ublic(z) ∧ Library(z) ∧ Of (z, N ewY ork), e) ∧ Annually(e)))
l1 : 	λx.eq(x, count(λy.P eople(y) ∧ ∃e.V isit(y, PublicLibraryOfNewYork, e) ∧ Annually(e)))
l2 : 	λx.HowManyPeopleVisitAnnually(x, P ublicLibraryOf N ewY ork)))


(c) Constant Matching  Steps in M : Replace all underspecified constants in the transformed logical form with a 
similarly typed constant from O, as described  in Section 5.2.2. The underspecified constant to be replaced is marked 
in bold and constants from O are written in typeset.

λx.H owM anyP eopleV isitAnnually(x, PublicLibraryOfNewYork)
l3 :	1→ λx.H owM anyP eopleV isitAnnually(x, new york public library)
λx.HowManyPeopleVisitAnnually(x, new york public library)
y :	1→ λx.public library  system.annual visits(x, new york public library)

Figure 3: Example derivation for the query ‘how many people visit the public library of new york annu- 
ally.’ Underspecified constants are labelled with the words from the query that they are associated with for
readability.  Constants from O, written in typeset, are introduced in step (c).





1 	n



















Figure 4: Definition of the operations used to transform  the structure of the underspecified logical form l0 to 
match the ontology O. The function type(c) calculates a constant c’s type. The function freev(lf ) returns
the set of variables that are free in lf (not bound by a lambda term or quantifier). The function subexps(lf )
generates the set of all subexpressions of the lambda calculus expression lf .


5.2.1  Structure Matching
  Three structure matching operators, illustrated in 
Figure 4, are used to collapse or expand literals in 
l0. Collapses merge a subexpression from l0 to cre- 
ate a new underspecified  constant, generating a log- 
ical form with fewer constants. Expansions split a 
subexpression from l0 to generate a new logical form 
containing one extra constant.

Collapsing  Operators  The collapsing operator 
defined in  Figure 4a merges  all  constants  in  a 
literal to generate  a  single constant  of the same 
type.   This operator  is used  to map ιz.P ublic(z)∧ 
Library(z)∧Of (z,N ewY ork)  to P ublicLibraryOf N ewY ork 
in Figure 3b. Its operation is limited to entity typed 
expressions that do not contain free variables.
  The operator in Figure 4b, in contrast, can be used 
to collapse  the expression  eq(x,count(λy.P eople(y)∧
∃e.V isit(y,P ublicLibraryOf N ewY ork,e))∧Annually(e))), 
which contains free variable x onto a new expression 
C ountP eopleV isitAnnually(x,P ublicLibraryOf N ewY ork). 
This is only possible when the type of the newly
created constant is allowed in O and the variable x
is free in the output expression. Subsets of conjuncts
can be collapsed using the operator in Figure 4b by 
creating ad-hoc conjunctions  that encapsulate them. 
Disjunctions  are treated similarly.
  Performing collapses on the underspecified logi- 
cal form allows  non-contiguous  phrases to be rep- 
resented  in  the collapsed  form.    In  this exam- 
ple, the logical form representing the phrase ‘how 
many people visit’ has been merged with the logi- 
cal form representing the non-adjacent adverb ‘an- 
nually.’  This generates  a new underspecified con- 
stant that can be mapped onto the Freebase relation 
public library system annual visits that re- 
lates to both phrases.
  The collapsing operations preserve semantic type, 
ensuring  that all logical forms generated  by the 
derivation  sequence are well typed. The full set of 
allowed collapses of l0 is given by the transitive clo- 
sure of the collapsing operations.  The size of this 
set is limited by the number of constants in l0, since 
each collapse removes at least one constant. At each 
step, the number of possible collapses is polynomial 
in the number of constants in l0 and exponential in
the arity of the most complex type in O.  For do-
mains of interest this arity is unlikely to be high and


for triple  stores such as Freebase it is 2.

Expansion Operators  The fully specified logical 
form y can contain  constants relating to multiple 
words in x. It can also use multiple  constants to rep- 
resent the meaning of a single  word. For example, 
Freebase does not contain a relation  representing the 
concept ‘daughter’, instead using two relations rep- 
resenting ‘female’  and ‘child’. The expansion oper- 
ator in Figure 4c allows a single predicate to be split 
into a pair of conjoined predicates sharing an argu- 
ment variable. For example, in Figure 1, the constant 
for ‘dedicate’ is split in two to match its represen- 
tation in Freebase.  Underspecified  constants from 
l0 can be split once. For the experiments in Sec- 
tion 8, we constrain the expansion operator to work 
on event modifiers but the procedure generalizes to 
all predicates.

5.2.2  Constant Matching
  To build an executable logical form y, all under- 
specified constants must be replaced with constants
from O.  This is done through  a sequence of con-
stant replacement operations, each of which replaces

a single underspecified constant with a constant  of 
the same type from O. Two example replacements
are shown in Figure 3c. The output from the last re- 
placement operation is a fully specified logical form.

6   Building and Scoring Derivations

This section introduces a dynamic  program used to 
construct derivations and a linear scoring model.

6.1   Building Derivations

The space of derivations is too large to explicitly 
enumerate.  However,  each logical form (both final 
and interim) can be constructed with many differ- 
ent derivations, and we only need to find the highest 
scoring one. This allows us to develop a simple dy- 
namic program for our two-stage semantic parser.
  We use a CKY style chart parser to calculate the 
k-best logical forms output by parses of x. We then 
store each interim  logical form generated by an op- 
erator in M  once in a hyper-graph chart structure. 
The branching factor of this hypergraph is polyno- 
mial in the number of constants in l0 and linear in
the size of O.  Subsequently,  there are too many
possible logical forms to enumerate explicitly;  we



prune as follows. We allow the top N scoring on- 
tological  matches for each original  subexpression in 
l0 and remove matches that differ from score from 
the maximum scoring match by more than a thresh- 
old τ . When building derivations, we apply constant 
matching operators as soon as they are applicable to 
new underspecified  constants created by collapses 
and expansions.   This allows the scoring function 
used by the pruning strategy to take advantage of all 
features defined in Section 7.2.

6.2   Ranking Derivations

Given feature vector φ and weight vector θ, the score


Input: Q/A pairs {(xi , ai )  : i = 1 . . . n}; Knowledge  base
K; Ontology O; Function GEN(x, O) that computes deriva-
tions of x; Function YIELD(d)that returns logical form yield 
of derivation d; Function EXEC(y, K) that calculates execu- 
tion of y in K; Margin γ; Number of iterations T .
Output:  Linear model parameters θ.
Algorithm:
For t = 1 . . . T , i = 1 . . . n :
C = {d : d ∈ GEN(xi , O); EXEC(YIELD(d), K) = ai } 
W = {d : d ∈ GEN(xi , O); EXEC(YIELD(d), K) /= ai 
} C ∗ = arg maxd∈C (φ(d)θ)
W ∗ = {d : d ∈ W ; ∃c ∈ C ∗ s.t. φ(c)θ − φ(d)θ < γ)}
If |C ∗| > 0 ∧ |W ∗| > 0 :
θ = θ +	 	φ(c) 	 	φ(e)
|C ∗ |	c∈C ∗ 	|W ∗ |	e∈W ∗


1	1


of a derivation  d = (Π, M ) is a linear function that
decomposes over the parse tree Π and the individual
ontology-matching steps o.

SCORE(d) = φ(d)θ	(1)
= φ(Π)θ + )' φ(o)θ
o∈M
The function PARSE(x, O) introduced  as our goal in 
Section 2 returns the logical form associated with 
the highest scoring derivation of x:


Figure 5: Parameter estimation from Q/A pairs.


7.2   Features
The feature vector φ(d) introduced in Section 6.2 
decomposes over each of the derivation  steps in d.

CCG Parse Features    Each lexical item in Π has 
three indicator features. The first indicates the num- 
ber of times each underspecified category is used. 
For example, the parse in Figure 3a uses the under- 
specified category N : λx.p(x)  twice. The second 
feature indicates (word, category) pairings — e.g.


PARSE(x, O) = arg	max
d∈GEN(x,O)


(SCORE(d))


that N : λx.p(x)  is paired with 
‘library’ and ‘pub- lic’ once each 
in Figure 3a. The final lexical 
feature


The features and learning algorithm  used to estimate
θ are defined in the next section.

7   Learning

This section describes an online learning algorithm 
for question-answering data, along with the domain- 
independent feature set.

7.1   Learning Model Parameters

Our learning algorithm estimates the parameters θ
from a set {(xi, ai)  : i = 1 . . . n} of questions xi
paired with answers ai  from the knowledge   base
K.   Each derivation  d generated by the parser is
associated with a fully specified logical form y = 
YIELD(d) that can be executed in K. A derivation d 
of xi is correct if EXEC(YIELD(d), K) = ai. We use
a perceptron  to estimate a weight vector θ that sup- 
port a separation of γ between correct and incorrect 
answers. Figure 5 presents the learning algorithm.


indicates (part-of-speech, category) pairings for all 
parts of speech associated with the word.

Structural  Features    The structure matching op- 
erators (Section 5.2.1) in M  generate new under- 
specified constants that define the types of constants 
in the output logical form y.  These operators are 
scored using features that indicate the type of each 
complex-typed  constant present in y and the iden- 
tity of domain-independent functional  constants in 
y. The logical form y generated in Figure 3 contains
one complex typed constant with type (i, (e, t)) and
no domain-independent functional constants. Struc-

tural features allow the model to adapt to different 
knowledge bases K. They allow it to determine, for
example, whether a numeric quantity  such as ‘pop- 
ulation’ is likely to be explicitly listed in K or if it
should be computed with the count function.

Lexical Features    Each constant replacement op- 
erator (Section 5.2.2) in M  replaces an underspec-


ified constant cu with a constant  cO from O.  The
underspecified constant cu is associated with the se-
quence of words w  u  used in the CCG lexical en- 
tries that introduced it in Π.  We assume that each
of the constants cO in O is associated with a string
label w  O . This allows us to introduce five domain-
independent features that measure the similarity of
w  u and w  O .
  The feature φnp(cu, cO ) signals the replacement 
of an entity-typed constant cu with entity cO that has 
label w  u.  For the second example in Figure 1 this 
feature indicates the replacement of the underspeci- 
fied constant associated with the word ‘mozart’ with 
the Freebase entity mozart.  Stem and synonymy 
features φstem(cu, cO ) and φsyn(cu, cO ) signal the


indicates entities separated from a predicate  by one 
join in y, such as mozart and dedicated to in Fig-
ure 1, that exist in the same relationship  in K.
  If  two predicates  that share  a  variable in  y 
do not share  an argument  in that position in K 
then the execution of y against K will  fail.   The 
predicate-predicate φpp(y, K) feature indicates pairs
of predicates that share a variable  in y but can- 
not occur in  this relationship in  K. 	For ex-
ample, since the subject of  the Freebase  prop- 
erty date of birth does  not take  arguments  of
type location,   φpp(y, K)  will  fire  if  y  con-
tains the logical form λxλy.date of birth(x, y) ∧
location(x).
Both  the  predicate-argument   and  predicate-


existence of words wu   ∈


w u  and wu   ∈


w O  that


predicate features operate 
on subexpressions of y.


share a stem  or synonym respectively. Stems are
computed with the Porter stemmer and synonyms 
are extracted  from Wiktionary. A single  Freebase 
specific feature φf p:stem(cu, cO ) indicates  a word
stem match between wu  ∈ w u and the word filling
the most specific position in w u under Freebase’s hi-
erarchical naming schema.
  A final feature φgl (cu, cO ) calculates the overlap 
between Wiktionary definitions for w u and w O . Let 
gl(w) be the Wiktionary  definition for w. Then:


We also define the execution features: φemp(y, K) to 
signal an empty answer for y in K; φ0(y, K) to sig-
nal a zero-valued  answer created by counting over 
an empty set; and φ1(y, K) to signal  a one-valued
answer created by counting over a singleton set.
  As with the lexical  cues, we use knowledge  base 
features  as soft constraints since it is possible for 
natural language queries to refer to concepts that do
not exist in K.



φgl (cu, cO ) =	 


2·|gl(wO )∩gl(wc )|


8	Experimental 
Setup


wu ∈w-u ;wO ∈w-O  |


w-O |·|w-u


|·|


gl(wO )|+|gl(wc )|



Data	We evaluate 
performance on the 
benchmark


Domain-indepedent  lexical features  allow  the
model to reason about the meaning of unseen words. 
In small domains, however, the majority of word us- 
ages may be covered by training  data. We make use 
of this fact in the GeoQuery domain with features 
φm(cu, cO ) that indicate the pairing of w u with cO .

Knowledge Base Features    Guided by the obser- 
vation that we generally want to create queries y
which have answers in knowledge  base K, we de-
fine features to signal whether each operation could 
build a logical form y with an answer in K.
  If  a predicate-argument   relation in y does  not 
exist  in  K,  then the execution  of  y  against  K
will  not return an answer. Two features indicate 
whether predicate-argument relations in y exist in K. 
φdirect(y, K) indicates predicate-argument applica- 
tions in y that exists in K. For example, if the appli-
cation of dedicated by to mozart in Figure 1 ex- 
ists in Freebase, φdirect(y, K) will fire. φjoin(y, K)


GeoQuery dataset (Zelle and Mooney,  1996), and a 
newly  introduced  Freebase Query (FQ) dataset (Cai 
and Yates, 2013a). FQ contains 917 questions la- 
beled with logical form meaning representations for 
querying Freebase. We gathered question answer la- 
bels by executing the logical forms against Freebase, 
and manually correcting any inconsistencies.
  Freebase (Bollacker  et al., 2008) is a large,  col- 
laboratively authored database containing almost 40 
million entities and two billion facts, covering more 
than 100 domains.  We filter Freebase to cover the 
domains contained in the FQ dataset resulting  in a 
database containing  18 million entities, 2072 rela- 
tions, 635 types, 135 million facts and 81 domains, 
including for example film, sports, and business. We 
use this schema to define our target domain, allow- 
ing for a wider variety of queries than could be en- 
coded with the 635 collapsed relations previously 
used to label the FQ data.



  We report two different experiments on the FQ 
data: test results on the existing 642/275 train/test 
split and domain adaptation results where the data is 
split three ways, partitioning  the topics so that the 
logical  meaning expressions do not share any sym- 
bols across folds.  We report on the standard 600/280 
training/test split for GeoQuery.

Parameter Initialization  and Training  We ini- 
tialize weights for φnp and φdirect to 10, and weights 
for φstem and φjoin  to 5. This promotes the use of 
entities and relations named in sentences. We ini- 
tialize weights for φpp and φemp to -1 to favour log- 
ical forms that have an interpretation  in the knowl-
edge base K.  All other feature weights are initial-
ized to 0. We run the training algorithm for one it-
eration on the Freebase data, at which  point  perfor- 
mance on the development set had converged.  This 
fast convergence is due to the very small number of 
matching  parameters used (5 lexical features and 8
K features).  For GeoQuery, we include the larger
domain specific feature set introduced in Section 7.2
and train for 10 iterations.  We set the pruning pa- 
rameters from Section 6.1 as follows: k  = 5 for 
Freebase, k = 30 for GeoQuery, N = 50, τ = 10.

Comparison Systems   We compare performance 
to state-of-the-art  systems in both domains. On 
GeoQuery,  we report results from DCS (Liang 
et al., 2011) without special initialization (DCS) and 
with an small hand-engineered lexicon (DCS with 
L+).   We also include results for the FUBL algo- 
rithm (Kwiatkowski  et al., 2011), the CCG learning 
approach that is most closely related to our work. On 
FQ, we compare to Cai and Yates (2013a) (CY13).

Evaluation We  evaluate by comparing the pro- 
duced question answers to the labeled ones, with no 
partial credit. Because the parser can fail to pro- 
duce a complete query, we report recall, the percent 
of total questions answered correctly, and precision, 
the percentage of produced queries with correct an- 
swers. CY13 and FUBL report fully correct logical 
forms, which is a close proxy to our numbers.

9   Results

Quantitative Analysis For FQ, we report results 
on the test set and in the cross-domain setting, as de- 
fined in Section 8. Figure 6 shows both results. Our



Setting
System
R	P	F1
Test
Our Approach
CY13
68.0	76.7	72.1
59	67	63
Cross
Domain
Our Approach
CY13
67.9	73.5	71.5
60	69	65
Figure 6: Results on the FQ dataset.


R	P	F1
All Features
Without Wiktionary
Without K Features
68.6	72.0	70.3
67.2	70.7	68.9
61.8	62.5	62.1
Figure 7: Ablation  Results


approach outperforms  the previous state of the art, 
achieving  a nine point improvement in test recall, 
while not requiring  labeled logical forms in train- 
ing. We also see consistent improvements  on both 
scenarios, indicating that our approach is generaliz- 
ing well across topic domains. The learned ontology 
matching model is able to reason about previously 
unseen ontological  subdomains as well as if it was 
provided explicit, in-domain training data.
  We also performed feature ablations with 5-fold 
cross validation  on the training  set, as seen in Fig- 
ure 7. Both the Wiktionary features and knowledge 
base features were helpful. Without the Wiktionary 
features, the model must rely on word stem matches 
which, in combination with graph constraints, can 
still recover many of the correct queries. However, 
without the knowledge  base constraints, the model 
produces many queries that return empty answers, 
and significantly  impacts overall performance.
  For GeoQuery, we report test results in Figure 8. 
Our approach outperforms the most closely related 
CCG model (FUBL) and DCS without initialization, 
but falls short of DCS with a small hand-built initial 
lexicon. Given the small size of the test set, it is fair 
to say that all algorithms are performing at state-of- 
the-art levels.  This result demonstrates that our ap-



Recall
FUBL
DCS
DCS with L+
Our Approach
88.6
87.9
91.1
89.0
Figure 8: GeoQuery Results


Parse Failures (20%)

1.	Query
in what year did motorola have the most revenue
2	Query
on how many projects was james walker a design engineer
Structural Matching Failure (30%)
Query	how many children does jerry seinfeld have
3.	Labeled	λx.eq(x, count(λy.people.person.children(jerry seinfeld, y)))
Predicted	λx.eq(x, count(λy.people.person.children(y, jerry seinfeld)))
Incomplete Database (10%)

Query
4.	Labeled
Predicted
how many countries participated in the 2006 winter olympics
λy.olympics.olympic games.number of countries(2006 winter olympics, y)
λy.eq(y, count(λy.olympic participation country.olympics participated in(x, 2006 winter olympics)))
Query
5.	Labeled
Predicted
what programming  languages were used for aol instant messenger
λy.computer.software.languages used(aol  instant messenger, y)
λy.computer.software.languages used(aol  instant messenger, y) ∧ computer.programming language(y)
Lexical Ambiguity (35%)
Query	when was the frida kahlo exhibit at the philadelphia art museum
Labeled	λy.∃x.exhibition run.exhibition(x, frida  kahlo)∧
6.	exhibition venue.exhibitions at(philadelphia art museum, x) ∧ exhibition run.opened on(x, y)
Predicted	λy.∃x.exhibition run.exhibition(x, frida  kahlo)∧
exhibition venue.exhibitions at(philadelphia art museum, x) ∧ exhibition run.closed on(x, y)

Figure 9: Example  error cases, with associated frequencies, illustrating  system output and gold standard 
references. 5% of the cases were miscellaneous  or otherwise difficult to categorize.




proach can handle the high degree of lexical ambi- 
guity in the FQ data, without  sacrificing the ability 
to understanding the rich, compositional phenomena 
that are common in the GeoQuery data.




Qualitative  Analysis We  also did a  qualitative 
analysis of errors in the FQ domain. The model 
learns to correctly produce complex forms that join 
multiple  relations.  However,  there are a number of 
systematic  error cases, grouped into four categories 
as seen in Figure 9.

  The first and second examples  show  parse fail- 
ures, where the underspecified CCG grammar did 
not have  sufficient coverage. The third shows  a 
failed structural match, where all of the correct logi- 
cal constants are selected, but the argument order is 
reversed for one of the literals. The fourth and fifth 
examples demonstrate a failures due to database in- 
completeness.  In both cases, the predicted queries 
would  have returned the same answers as the gold- 
truth ones if Freebase contained all of the required 
facts. Developing  models that are robust to database 
incompleteness is a challenging  problem for future 
work. Finally, the last example demonstrates a lex- 
ical ambiguity,  where the system was unable to de- 
termine if the query should include the opening date 
or the closing date for the exhibit.


10   Conclusion

We  considered the problem of learning domain- 
independent semantic parsers, with application to 
QA against large knowledge bases. We introduced 
a new approach for learning  a two-stage semantic 
parser that enables scalable,  on-the-fly ontological 
matching. Experiments demonstrated state-of-the- 
art performance on benchmark  datasets, including 
effective generalization to previously  unseen words.
  We would like to investigate more nuanced no- 
tions of semantic correctness, for example to support 
many of the essentially equivalent meaning repre- 
sentations we found in the error analysis. Although 
we focused exclusively on QA applications, the gen- 
eral two-stage analysis approach should allow for 
the reuse of learned grammars  across a number  of 
different domains, including robotics or dialog ap- 
plications,  where data is more challenging  to gather.

11   Acknowledgements

This research was supported in part by DARPA un- 
der the DEFT program through the AFRL (FA8750-
13-2-0019) and the CSSG (N11AP20020),  the ARO 
(W911NF-12-1-0197), the NSF (IIS-1115966), and 
by a gift from Google.  The authors thank Anthony 
Fader, Nicholas FitzGerald, Adrienne Wang, Daniel 
Weld, and the anonymous reviewers for their helpful 
comments and feedback.


References

Alshawi, H. (1992). The core language engine. The
MIT Press.

Artzi, Y. and Zettlemoyer, L. (2011). Bootstrapping 
semantic parsers from conversations. In Proceed- 
ings of the Conference on Empirical Methods in 
Natural Language Processing.

Artzi, Y. and Zettlemoyer, L. (2013). Weakly super- 
vised learning of semantic parsers for mapping in- 
structions to actions. Transactions of the Associ- 
ation for Computational Linguistics, 1(1):49–62.

Bollacker, K., Evans, C., Paritosh, P., Sturge, T., and 
Taylor,  J. (2008).  Freebase: a collaboratively cre- 
ated graph database for structuring human knowl- 
edge. In Proceedings of the ACM SIGMOD Inter- 
national Conference on Management of Data.

Bos, J. (2008).  Wide-coverage  semantic analysis 
with boxer. In Proceedings of the Conference on 
Semantics in Text Processing.

Cai, Q. and Yates, A. (2013a). Large-scale semantic 
parsing via schema matching and lexicon  exten- 
sion. In Proceedings of the Annual Meeting of the 
Association for Computational Linguistics.

Cai, Q. and Yates, A. (2013b). Semantic parsing 
freebase: Towards open-domain semantic pars- 
ing.  In Proceedings of the Joint Conference on 
Lexical and Computational Semantics.

Chen, D. and Mooney, R. (2011). Learning to inter- 
pret natural language navigation instructions from 
observations. In Proceedings of the National Con- 
ference on Artificial Intelligence.

Clark,  S. and Curran, J. (2007). Wide-coverage ef- 
ficient statistical parsing with CCG and log-linear 
models. Computational Linguistics, 33(4):493–
552.

Clarke,  J., Goldwasser,  D., Chang, M., and Roth, 
D. (2010).  Driving semantic parsing from the 
world’s response.  In Proceedings of the Confer- 
ence on Computational Natural Language Learn- 
ing.

Davidson, D. (1967). The logical form of action sen- 
tences. Essays on actions and events, pages 105–
148.


Doan, A., Madhavan, J., Domingos,  P., and Halevy, 
A.  (2004).   Ontology matching: A  machine 
learning approach.  In Handbook on ontologies. 
Springer.

Euzenat, J., Euzenat, J., Shvaiko,  P., et al. (2007).
Ontology matching. Springer.

Fader, A., Zettlemoyer, L., and Etzioni,  O. (2013).
Paraphrase-driven learning for open question an- 
swering. In Proceedings of the Annual Meeting of 
the Association for Computational Linguistics.

Goldwasser, D. and Roth, D. (2011). Learning from 
natural instructions. In Proceedings of the In- 
ternational Joint Conference on Artificial Intelli- 
gence.

Grosz, B. J.,  Appelt, D. E., Martin, P.  A., and 
Pereira,  F. (1987).  TEAM: An experiment  in 
the design of transportable natural language inter- 
faces. Artificial Intelligence, 32(2):173–243.

Hobbs,  J. R. (1985). Ontological promiscuity. In 
Proceedings of the Annual Meeting on Associa- 
tion for Computational Linguistics.

Jones, B. K., Johnson, M., and Goldwater, S. (2012).
Semantic parsing with bayesian tree transducers. 
In Proceedings of the 50th Annual Meeting of the 
Association of Computational Linguistics.

Kate, R. and Mooney, R. (2006).  Using string- 
kernels for learning  semantic parsers. In Pro- 
ceedings of the Conference of the Association for 
Computational Linguistics.

Krishnamurthy,   J. and Kollar, T. (2013).  Jointly 
learning to parse and perceive:  Connecting  nat- 
ural language to the physical world. Transactions 
of the Association for Computational Linguistics,
1(2).

Krishnamurthy,  J. and Mitchell, T. (2012). Weakly 
supervised training of semantic parsers.  In Pro- 
ceedings of the Joint Conference on Empirical 
Methods  in Natural Language  Processing  and 
Computational Natural Language Learning.

Kushman, N. and Barzilay, R. (2013).  Using se- 
mantic unification to generate regular expressions 
from natural language. In Proceedings of the Con- 
ference of the North American Chapter of the As- 
sociation for Computational Linguistics.


Kwiatkowski, T., Goldwater,  S., Zettlemoyer,  L., 
and Steedman, M. (2012). A probabilistic model 
of syntactic and semantic acquisition from child- 
directed utterances and their meanings. Proceed- 
ings of the Conference of the European Chapter 
of the Association of Computational Linguistics.
Kwiatkowski, T., Zettlemoyer,  L., Goldwater,  S., 
and Steedman, M. (2010). Inducing probabilis- 
tic CCG grammars from logical form with higher- 
order unification. In Proceedings of the Confer- 
ence on Empirical Methods in Natural Language 
Processing.
Kwiatkowski, T., Zettlemoyer,  L., Goldwater,  S., 
and Steedman, M. (2011). Lexical generalization 
in CCG grammar induction for semantic parsing. 
In Proceedings of the Conference on Empirical 
Methods in Natural Language Processing.
Liang, P., Jordan, M., and Klein, D. (2011). Learn- 
ing dependency-based compositional  semantics. 
In Proceedings of the Conference of the Associ- 
ation for Computational Linguistics.
Matuszek, C., FitzGerald, N., Zettlemoyer, L., Bo, 
L., and Fox, D. (2012). A joint model of language 
and perception for grounded attribute learning. In 
Proceedings of the International  Conference on 
Machine Learning.
Muresan, S. (2011). Learning for deep language un- 
derstanding.  In Proceedings of the International 
Joint Conference on Artificial Intelligence.
Steedman, M. (1996). Surface Structure and Inter- 
pretation. The MIT Press.
Steedman, M. (2000). The Syntactic Process.  The
MIT Press.
Unger,   C.,    Bu¨ hmann,   L.,    Lehmann,   J., 
Ngonga Ngomo, A., Gerber, D., and Cimiano, P. 
(2012). Template-based question answering over 
RDF data. In Proceedings of the International 
Conference on World Wide Web.
Wong, Y. and Mooney,  R. (2007). Learning syn- 
chronous  grammars  for semantic  parsing with 
lambda calculus. In Proceedings of the Confer- 
ence of the Association for Computational Lin- 
guistics.
Yahya, M., Berberich, K., Elbassuoni, S., Ramanath, 
M., Tresp, V., and Weikum,  G. (2012). Natural


language questions for the web of data. In Pro- 
ceedings of the Conference on Empirical Methods 
in Natural Language Processing.
Zelle, J. and Mooney, R. (1996). Learning to parse 
database queries using inductive  logic program- 
ming. In Proceedings of the National Conference 
on Artificial Intelligence.
Zettlemoyer, L. and Collins, M. (2005). Learning 
to map sentences to logical  form: Structured clas- 
sification with probabilistic categorial grammars. 
In Proceedings of the Conference on Uncertainty 
in Artificial Intelligence.
Zettlemoyer,  L. and Collins, M. (2007).  Online 
learning of relaxed CCG grammars for parsing to 
logical form. In Proceedings of the Joint Confer- 
ence on Empirical Methods in Natural Language 
Processing and Computational Natural Language 
Learning.
Zettlemoyer,  L. and Collins, M. (2009).  Learn- 
ing context-dependent mappings from sentences 
to logical form. In Proceedings of the Joint Con- 
ference of the Association for Computational Lin- 
guistics and International Joint Conference  on 
Natural Language Processing.
Zhang, C., Hoffmann, R., and Weld, D. S. (2012).
Ontological smoothing for  relation extraction 
with minimal supervision. In Proceeds  of the 
Conference on Artificial Intelligence.


















