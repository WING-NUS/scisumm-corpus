Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 460–466,Sydney, July 2006. c©2006 Association for Computational Linguistics
Minority Vote: At-Least-N VotingImproves Recall for Extracting Relations
Nanda KambhatlaIBM T.J. Watson Research Center
1101 Kitchawan Road Rt 134Yorktown, NY 10598nanda@us.ibm.com
Abstract
Several NLP tasks are characterized byasymmetric data where one class labelNONE, signifying the absence of anystructure (named entity, coreference, re-lation, etc.) dominates all other classes.Classifiers built on such data typicallyhave a higher precision and a lower re-call and tend to overproduce the NONEclass. We present a novel scheme for vot-ing among a committee of classifiers thatcan significantly boost the recall in suchsituations. We demonstrate results show-ing up to a 16% relative improvement inACE value for the 2004 ACE relation ex-traction task for English, Arabic and Chi-nese.
1 Introduction
Statistical classifiers are widely used for diverseNLP applications such as part of speech tagging(Ratnaparkhi, 1999), chunking (Zhang et al., 2002),semantic parsing (Magerman, 1993), named entityextraction (Borthwick, 1999; Bikel et al., 1997; Flo-rian et al., 2004), coreference resolution (Soon et al.,2001), relation extraction (Kambhatla, 2004), etc. Anumber of these applications are characterized by adominance of a NONE class in the training exam-ples. For example, for coreference resolution, classi-fiers might classify whether a given pair of mentionsare references to the same entity or not. In this case,we typically have a lot more examples of mention
pairs that are not coreferential (i.e. the NONE class)than otherwise. Similarly, if a classifier is predictingthe presence/absence of a semantic relation betweentwo mentions, there are typically far more examplessignifying an absence of a relation.
Classifiers built with asymmetric data dominatedby one class (a NONE class donating absence of arelation or coreference or a named entity etc.) canovergenerate the NONE class. This often results in aunbalanced classifier where precision is higher thanrecall.
In this paper, we present a novel approach forimproving the recall of such classifiers by using anew voting scheme from a committee of classifiers.There are a plethora of algorithms for combiningclassifiers (e.g. see (Xu et al., 1992)). A widelyused approach is a majority voting scheme, whereeach classifier in the committee gets a vote and theclass with the largest number of votes ’wins’ (i.e. thecorresponding class is output as the prediction of thecommittee).
We are interested in improving overall recall andreduce the overproduction of the class NONE. Ourscheme predicts the class label C obtaining the sec-ond highest number of votes when NONE gets thehighest number of votes, provided C gets at leastN votes. Thus, we predict a label other than NONEwhen there is some evidence of the presense of thestructure we are looking for (relations, coreference,named entities, etc.) even in the absense of a clearmajority.
This paper is organized as follows. In section 2,we give an overview of the various schemes for com-bining classifiers. In section 3, we present our vot-
460
ing algorithm. In section 4, we describe the ACErelation extraction task. In section 5, we present em-pirical results for relation extraction and we discussour results and conclude in section 6.
2 Combining Classifiers
Numerous methods for combining classifiers havebeen proposed and utlized to improve the perfor-mance of different NLP tasks such as part of speechtagging (Brill and Wu, 1998), identifying base nounphrases (Tjong Kim Sang et al., 2000), named en-tity extraction (Florian et al., 2003), etc. Ho et al(1994) investigated different approaches for rerank-ing the outputs of a committee of classifiers and alsoexplored union and intersection methods for reduc-ing the set of predicted categories. Florian et al(2002) give a broad overview of methods for com-bining classifiers and present empirical results forword sense disambiguation.
Xu et al (1992) and Florian et al (2002) considerthree approaches for combining classifiers. In thefirst approach, individual classifiers output posteriorprobabilities that are merged (e.g. by taking an av-erage) to arrive at a composite posterior probabilityof each class. In the second scheme, each classifieroutputs a ranked list of classes instead of a proba-bility distribution and the different ranked lists aremerged to arrive at a final ranking. Methods us-ing the third approach, often called voting methods,treat each classifier as a black box that outputs onlythe top ranked class and combines these to arrive atthe final decision (class). The choice of approachand the specific method of combination may be con-strained by the specific classification algorithms inuse.
In this paper, we focus on voting methods, sincefor small data sets, it is hard to reliably estimateprobability distributions or even a complete order-ing of classes especially when the number of classesis large.
A widely used voting method for combining clas-sifiers is a Majority Vote scheme (e.g. (Brill andWu, 1998; Tjong Kim Sang et al., 2000)). Eachclassifier gets to vote for its top ranked class andthe class with the highest number of votes ’wins’.Henderson et al (1999) use a Majority Vote schemewhere different parsers vote on constituents’ mem-
bership in a hypothesized parse. Halteren et al(1998) compare a number of voting methods includ-ing a Majority Vote scheme with other combinationmethods for part of speech tagging.
In this paper, we induce multiple classifiers by us-ing bagging (Breiman, 1996). Following Breiman’sapproach, we obtain multiple classifiers by firstmaking bootstrap replicates of the training data andtraining different classifiers on each of the replicates.The bootstrap replicates are induced by repeatedlysampling with replacement training events from theoriginal training data to arrive at replicate data setsof the same size as the training data set. Breiman(1996) uses a Majority Vote scheme for combiningthe output of the classifiers. In the next section, wewill describe the different voting schemes we ex-plored in our work.
3 At-Least-N Voting
We are specifically interested in NLP tasks char-acterized by asymmetric data where, typically, wehave far more occurances of a NONE class that sig-inifies the absense of structure (e.g. a named en-tity, or a coreference relation or a semantic relation).Classifiers trained on such data sets can overgener-ate the NONE class, and thus have a higher preci-sion and lower recall in discovering the underlyingstructure (i.e. the named entities or coreference linksetc.). With such tasks, the benefits yielded by a Ma-jority Vote is limited, since, because of the asym-metry in the data, a majority of the classifiers mightpredict NONE most of the time.
We propose alternative voting schemes, dubbedAt-Least-N Voting, to deal with the overproductionof NONE. Given a committee of classifiers (obtainedby bagging or some other mechanism), the classi-fiers first cast their vote. If the majority vote is for aclass C other than NONE, we simply output C as theprediction. If the majority vote is for NONE, we out-put the class label obtaining the second highest num-ber of votes, provided it has at least N votes. Thus,we choose to defer to the minority vote of classifierswhich agree on finding some structure even whenthe majority of classifiers vote for NONE. We expectthis voting scheme to increase recall at the expenseof precision.
At-Least-N Voting induces a spectrum of combi-
461
nation methods ranging from a Majority Vote (whenN is more than half of the total number of classifiers)to a scheme, where the evidence of any structure byeven one classifier is believed (At-Least-1 Voting).The exact choice of N is an empirical one and de-pends on the amount of asymmetry in the data andthe imbalance between precision and recall in theclassifiers.
4 The ACE Relation Extraction TaskAutomatic Content Extraction (ACE) is an annualevaluation conducted by NIST (NIST, 2004) on in-formation extraction, focusing on extraction of en-tities, events, and relations. The Entity Detectionand Recognition task entails detection of mentionsof entities and grouping together the mentions thatare references to the same entity. In ACE terminol-ogy, mentions are references in text (or audio, chats,...) to real world entities. Similarly relation men-tions are references in text to semantic relations be-tween entity mentions and relations group togetherall relation mentions that identify the same semanticrelation between the same entities.
In the frament of text:
John’s son, Jim went for a walk. Jim likedhis father.
all the underlined words are mentions referring totwo entities, John, and Jim. Morover, John andJim have a family relation evidenced as two relationmentions ”John’s son” between the entity mentions”John” and ”son” and ”his father” between the entitymentions ”his” and ”father”.
In the relation extraction task, systems must pre-dict the presence of a predetermined set of binaryrelations among mentions of entities, label the rela-tion, and identify the two arguments. In the 2004ACE evaluation, systems were evaluated on their ef-ficacy in correctly identifying relations among bothsystem output entities and with ’true’ entities (i.e. asannotated by human annotators as opposed to sys-tem output). In this paper, we present results for ex-tracting relations between ’true’ entities.
Table 1 shows the set of relation types, subtypes,and their frequency counts in the training data for the2004 ACE evaluation. For training classifiers, thegreat paucity of positive training events (where rela-tions exist) compared to the negative events (where
Type Subtype CountART user-or-owner 140
(agent artifact) inventor/manufacturer 3other 6
EMP-ORG employ-executive 420employ-staff 416employ-undetermined 62member-of-group 126partner 11subsidiary 213other 37
GPE-AFF citizen-or-resident 173(GPE affiliation) based-in 225
other 63DISCOURSE -none- 122
PHYSICAL located 516near 81part-whole 333
PER-SOC business 119(personal/social) family 115
other 28OTHER-AFF ethnic 28
(PER/ORG ideology 26affiliation) other 27
Table 1: The set of types and subtypes of relationsused in the 2004 ACE evaluation.
relations do not exist) suggest that schemes for im-proving recall might benefit this task.
5 Experimental ResultsIn this section, we present results of experimentscomparing three different methods of combiningclassifiers for ACE relation extraction:
• At-Least-N for different values of N,
• Majority Voting, and• a simple algorithm, called summing, where we
add the posterior scores for each class from allthe classifiers and select the class with the max-imum summed score.
Since the official ACE evaluation set is not pub-licly available, to facilitate comparison with our re-sults and for internal testing of our algorithms, foreach language (English, Arabic, and Chinese), we
462
En Ar ChTraining Set (documents) 227 511 480Training Set (rel-mentions) 3290 4126 4347Test Set (documents) 114 178 166Test Set (rel-mentions) 1381 1894 1774
Table 2: The Division of LDC annotated data intotraining and development test sets.
divided the ACE 2004 training data provided byLDC in a roughly 75%:25% ratio into a training setand a test set. Table 2 summarizes the number ofdocuments and the number of relation mentions ineach data set. The test sets were deliberately chosento be the most recent 25% of documents in chrono-logical order, since entities and relations in newstend to repeat and random shuffles can greatly re-duce the out-of-vocabulary problem.
5.1 Maximum Entropy ClassifiersWe used bagging (Breiman, 1996) to create replicatetraining sets of the same size as the original trainingset by repeatedly sampling with replacement fromthe training set. We created 25 replicate training sets(bags) for each language (Arabic, Chinese, English)and trained separate maximum entropy classifiers oneach bag. We then applied At-Least-N (N = 1,2,5),Majority Vote, and Summing algorithms with thetrained classifiers and measured the resulting perfor-mance on our development set.
For each bag, we built maximum entropy modelsto predict the presence of relation mentions and thetype and subtype of relations, when their presenceis predicted. Our models operate on every pair ofmentions in a document that are not references tothe same entity, to extract relation mentions. Sincethere are 23 unique type-subtype pairs in Table 1,our classifiers have 47 classes: two classes for eachpair corresponding to the two argument orderings(e.g. ”John’s son” vs. ”his father”) and a NONEclass signifying no relation.
Similar to our earlier work (Kambhatla, 2004),we used a combination of lexical, syntactic, and se-mantic features including all the words in betweenthe two mentions, the entity types and subtypes ofthe two mentions, the number of words in betweenthe two mentions, features derived from the small-
est parse fragment connecting the two mentions, etc.These features were held constant throughout theseexperiments.
5.2 ResultsWe report the F-measure, precision and recall forextracting relation mentions for all three languages.We also report ACE value1, the official metric usedby NIST that assigns 0% value to a system that pro-duces no output and a 100% value to a system thatextracts all relations without generating any falsealarms. Note that the ACE value counts each rela-tion only once even if it is expressed in text manytimes as different relation mentions. The reader isreferred to the NIST web site (NIST, 2004) for moredetails on the ACE value computation.
Figures 1(a), 1(b), and 1(c) show the F-measure,precision, and recall respectively for the English testset obtained by different classifier combination tech-niques as we vary the number of bags. Figures 2(a),2(b), and 2(c) show similar curves for Chinese, andFigures 3(a), 3(b), and 3(c) show similar curves forArabic. All these figures show the performance of asingle classifier as a straight line.
From the plots, it is clear that our hope of increas-ing recall by combining classifiers is realized for allthree languages. As expected, the recall rises fastestfor At-Least-N when N is small, i.e when small mi-nority opinion or even a single dissenting opinion isbeing trusted. Of course, the rise in recall is at theexpense of a loss of precision. Overall, At-Least-Nfor intermediate ranges of N (N=5 for English andChinese and N=2 for Arabic) performs best wherethe moderate loss in precision is more than offset bya rise in recall.
Both the Majority Vote method and the Summingmethod succeed in avoiding a sharp loss of preci-sion. However, they fail to increase the recall signif-icantly either.
Table 3 summarizes the best results (F-measure)for each classifier combination method for all threelanguages compared with the result for a single clas-sifier. At their best operating points, all three combi-nation methods handily outperform the single clas-sifier. At-Least-N seems to have a slight edge overthe other two methods, but the difference is small.
1Here we use the ACE value metric used for the ACE 2004evaluation
463
 43
 44
 45
 46
 47
 48
 49
 50
 0  5  10  15  20  25
F
Number of Bags
At-Least-1At-Least-2At-Least-5
Majority VoteSumming
Single
(a) F-measure
 46 48 50 52 54 56 58 60 62 64 66 68
 0  5  10  15  20  25
Prec
isio
n
Number of Bags
At-Least-1At-Least-2At-Least-5
Majority VoteSumming
Single
(b) Precision
 34
 36
 38
 40
 42
 44
 46
 0  5  10  15  20  25
Rec
all
Number of Bags
At-Least-1At-Least-2At-Least-5
Majority VoteSumming
Single
(c) Recall
Figure 1: Comparing F-measure, precision, and recall of different voting schemes for English relationextraction.
 61
 62
 63
 64
 65
 66
 67
 0  5  10  15  20  25
F
Number of Bags
At-Least-1At-Least-2At-Least-5
Majority VoteSumming
Single
(a) F-measure
 56 58 60 62 64 66 68 70 72 74 76
 0  5  10  15  20  25
Prec
isio
n
Number of Bags
At-Least-1At-Least-2At-Least-5
Majority VoteSumming
Single
(b) Precision
 52
 54
 56
 58
 60
 62
 64
 66
 68
 70
 0  5  10  15  20  25
Rec
all
Number of Bags
At-Least-1At-Least-2At-Least-5
Majority VoteSumming
Single
(c) Recall
Figure 2: Comparing F-measure, precision, and recall of different voting schemes for Chinese relationextraction.
 25
 26
 27
 28
 29
 30
 31
 0  5  10  15  20  25
F
Number of Bags
At-Least-1At-Least-2At-Least-5
Majority VoteSumming
Single
(a) F-measure
 28
 30
 32
 34
 36
 38
 40
 42
 44
 0  5  10  15  20  25
Prec
isio
n
Number of Bags
At-Least-1At-Least-2At-Least-5
Majority VoteSumming
Single
(b) Precision
 18
 20
 22
 24
 26
 28
 30
 0  5  10  15  20  25
Rec
all
Number of Bags
At-Least-1At-Least-2At-Least-5
Majority VoteSumming
Single
(c) Recall
Figure 3: Comparing F-measure, precision, and recall of different voting schemes for Arabic relation ex-traction.
464
English Arabic ChineseSingle 46.87 27.47 63.75At-Least-N 49.52 30.41 66.79Majority Vote 49.24 29.02 66.21Summing 48.66 29.02 66.40
Table 3: Comparing the best F-measure obtained byAt-Least-N Voting with Majority Voting, Summingand the single best classifier.
English Arabic ChineseSingle 59.6 37.3 69.6At-Least-N 63.9 43.5 71.0
Table 4: Comparing the ACE Value obtained by At-Least-N Voting with the single best classifier for theoperating points used in Table 3.
Table 4 shows the ACE value obtained by ourbest performing classifier combination method (At-Least-N at the operating points in Table 3) comparedwith a single classifier. Note that while the improve-ment for Chinese is slight, for Arabic performanceimproves by over 16% relative and for English, theimprovement is over 7% relative over the single clas-sifier2. Since the ACE value collapses relation men-tions referring to the same relation, finding new re-lations (i.e. recall) is more important. This mightexplain the relatively larger difference in ACE valuebetween the single classifier performance and At-Least-N.
The rules of the ACE evaluation prohibit us frompresenting a detailed comparison of our relation ex-traction system with the other participants. How-ever, our relation extraction system (using the At-Least-N classifier combination scheme as describedhere) performed very competitively in 2004 ACEevaluation both in the system output relation ex-traction task (RDR) and the relation extraction taskwhere the ’true’ mentions and entities are given.
Due to time limitations, we did not try At-Least-Nwith N > 5. From the plots, there is a potential forgetting greater gains by experimenting with a larger
2Note that ACE value metric used in the ACE 2004 eval-uation weights entitites differently based on their type. Thus,relations with PERSON-NAME arguments end up contribut-ing a lot more the overall score than relations with FACILITY-PRONOUN arguments.
number of bags and with a larger N.
6 DiscussionSeveral NLP problems exhibit a dominance of aNONE class that typically signifies a lack of struc-ture like a named entity, coreference, etc. Especiallywhen coupled with small training sets, this results inclassifiers with unbalanced precision and recall. Wehave argued that a classifier voting scheme that is fo-cused on improving recall can help increase overallperformance in such situations.
We have presented a class of voting methods,dubbed At-Least-N that defer to the opinion of a mi-nority of classifiers (consisting of N members) evenwhen the majority predicts NONE. This can boostrecall at the expense of precision. However, by vary-ing N and the number of classifiers, we can pick anoperating point that improves the overall F-measure.
We have presented results for ACE relation ex-traction for three languages comparing At-Least-Nwith Majority Vote and Summing methods for com-bining classifiers. All three classifier combinationmethods significantly outperform a single classifier.Also, At-Least-N consistently gave us the best per-formance across different languages.
We used bagging to induce multiple classifiers forour task. Because of the random bootstrap sam-pling, different replicate training sets might tilt to-wards one class or another. Thus, if we have manyclassifiers trained on the replicate training sets, someof them are likely to be better at predicting certainclasses than others. In future, we plan to experi-ment with other methods for collecting a committeeof classifiers.
ReferencesD. M. Bikel, S. Miller, R. Schwartz, and R. Weischedel.
1997. Nymble: a high-performance learning name-finder. In Proceedings of ANLP-97, pages 194–201.
A. Borthwick. 1999. A Maximum Entropy Approach toNamed Entity Recognition. Ph.D. thesis, New YorkUniversity.
L. Breiman. 1996. Bagging predictors. In MachineLearning, volume 24, page 123.
E. Brill and J. Wu. 1998. Classifier combinationfor improved lexical disambiguation. Proceedings ofCOLING-ACL’98, pages 191–195, August.
465
Radu Florian and David Yarowsky. 2002. Modeling con-sensus: Classifier combination for word sense disam-biguation. In Proceedings of EMNLP’02, pages 25–32.
R. Florian, A. Ittycheriah, H. Jing, and T. Zhang. 2003.Named entity recognition through classifier combina-tion. In Proceedings of CoNNL’03, pages 168–171.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kamb-hatla, X. Luo, N Nicolov, and S Roukos. 2004. Astatistical model for multilingual entity detection andtracking. In Proceedings of the Human LanguageTechnology Conference of the North American Chap-ter of the Association for Computational Linguistics:HLT-NAACL 2004, pages 1–8.
J. Henderson and E. Brill. 1999. Exploiting diversity innatural language processing: Combining parsers. InProceedings on EMNLP99, pages 187–194.
T. K. Ho, J. J. Hull, and S. N. Srihari. 1994. Deci-sion combination in multiple classifier systems. IEEETransactions on Pattern Analysis and Machine Intelli-gence, 16(1):66–75, January.
Nanda Kambhatla. 2004. Combining lexical, syntactic,and semantic features with maximum entropy mod-els for information extraction. In The Proceedings of42st Annual Meeting of the Association for Computa-tional Linguistics, pages 178–181, Barcelona, Spain,July. Association for Computational Linguistics.
D. Magerman. 1993. Parsing as statistical pattern recog-nition.
NIST. 2004. The ACE evaluation plan.www.nist.gov/speech/tests/ace/index.htm.
Adwait Ratnaparkhi. 1999. Learning to parse naturallanguage with maximum entropy models. MachineLearning, 34:151–178.
W. M. Soon, H. T. Ng, and C. Y. Lim. 2001. A ma-chine learning approach to coreference resolution ofnoun phrases. Computational Linguistics, 27(4):521–544.
E. F. Tjong Kim Sang, W. Daelemans, H. Dejean,R. Koeling, Y. Krymolowsky, V. Punyakanok, andD. Roth. 2000. Applying system combination to basenoun phrase identification. In Proceedings of COL-ING 2000, pages 857–863.
H. van Halteren, J. Zavrel, and W. Daelemans. 1998. Im-proving data driven wordclass tagging by system com-bination. In Proceedings of COLING-ACL’98, pages491–497.
L. Xu, A. Krzyzak, and C. Suen. 1992. Methods ofcombining multiple classifiers and their applicationsto handwriting recognition. IEEE Trans. on Systems,Man. Cybernet, 22(3):418–435.
T. Zhang, F. Damerau, and D. E. Johnson. 2002. Textchunking based on a generalization of Winnow. Jour-nal of Machine Learning Research, 2:615–637.
466
