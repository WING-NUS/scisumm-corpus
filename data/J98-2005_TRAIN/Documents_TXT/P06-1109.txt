Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 865–872,Sydney, July 2006. c©2006 Association for Computational Linguistics
An All-Subtrees Approach to Unsupervised Parsing
Rens BodSchool of Computer ScienceUniversity of St AndrewsNorth Haugh, St Andrews KY16 9SX Scotland, UKrb@dcs.st-and.ac.uk
Abstract
We investigate generalizations of the all-subtrees "DOP" approach to unsupervisedparsing. Unsupervised DOP models assignall possible binary trees to a set of sentencesand next use (a large random subset of) allsubtrees from these binary trees to computethe most probable parse trees. We will testboth a relative frequency estimator forunsupervised DOP and a maximumlikelihood estimator which is known to bestatistically consistent. We report state-of-the-art results on English (WSJ), German(NEGRA) and Chinese (CTB) data. To thebest of our knowledge this is the first paperwhich tests a maximum likelihood estimatorfor DOP on the Wall Street Journal, leadingto the surprising result that an unsupervisedparsing model beats a widely usedsupervised model (a treebank PCFG).
1   Introduction
The problem of bootstrapping syntactic structurefrom unlabeled data has regained considerableinterest. While supervised parsers suffer fromshortage of hand-annotated data, unsupervisedparsers operate with unlabeled raw data of whichunlimited quantities are available. During the lastfew years there has been steady progress in the field.Where van Zaanen (2000) achieved 39.2%unlabeled f-score on ATIS word strings, Clark(2001) reports 42.0% on the same data, and Kleinand Manning (2002) obtain 51.2% f-score on ATISpart-of-speech strings using a constituent-context
model called CCM. On Penn Wall Street Journal p-o-s-strings = 10 (WSJ10), Klein and Manning(2002) report 71.1% unlabeled f-score with CCM.And the hybrid approach of Klein and Manning(2004), which combines constituency anddependency models, yields 77.6% f-score.
Bod (2006) shows that a further improve-ment on the WSJ10 can be achieved by an unsuper-vised generalization of the all-subtrees approachknown as Data-Oriented Parsing (DOP). Thisunsupervised DOP model, coined U-DOP, firstassigns all possible unlabeled binary trees to a set ofsentences and next uses all subtrees from (a largesubset of) these trees to compute the most probableparse trees. Bod (2006) reports that U-DOP notonly outperforms previous unsupervised parsers butthat its performance is as good as a binarized super-vised parser (i.e. a treebank PCFG) on the WSJ.
A possible drawback of U-DOP, however,is the statistical inconsistency of its estimator(Johnson 2002) which is inherited from the DOP1model (Bod 1998). That is, even with unlimitedtraining data, U-DOP's estimator is not guaranteedto converge to the correct weight distribution.Johnson (2002: 76) argues in favor of a maximumlikelihood estimator for DOP which is statisticallyconsistent. As it happens, in Bod (2000) we alreadydeveloped such a DOP model, termed ML-DOP,which reestimates the subtree probabilities by amaximum likelihood procedure based onExpectation-Maximization. Although cross-validation is needed to avoid overlearning, ML-DOPoutperforms DOP1 on the OVIS corpus (Bod2000).
This raises the question whether we cancreate an unsupervised  DOP model which is also
865
statistically consistent. In this paper we will showthat an unsupervised version of ML-DOP can beconstructed along the lines of U-DOP. We will startout by summarizing DOP, U-DOP and ML-DOP,and next create a new unsupervised model calledUML-DOP. We report that UML-DOP not onlyobtains higher parse accuracy than U-DOP on threedifferent domains, but that it also achieves this withfewer  subtrees than U-DOP. To the best of ourknowledge, this paper presents the firstunsupervised  parser that outperforms a widely usedsupervised  parser on the WSJ, i.e. a treebankPCFG. We will raise the question whether the endof supervised parsing is in sight.
2  DOP
The key idea of DOP is this: given an annotatedcorpus, use all subtrees, regardless of size, to parsenew sentences. The DOP1 model in Bod (1998)computes the probabilities of parse trees andsentences from the relative frequencies of thesubtrees. Although it is now known that DOP1'srelative frequency estimator is statisticallyinconsistent (Johnson 2002), the model yieldsexcellent empirical results and has been used instate-of-the-art systems. Let's illustrate DOP1 with asimple example. Assume a corpus consisting ofonly two trees, as given in figure 1.
NP VP
S
NP
Mary
V
likes
John
NP VP
S
NPVPeter
hates Susan
Figure 1. A corpus of two trees
New sentences may be derived by combiningfragments, i.e. subtrees, from this corpus, by meansof a node-substitution operation indicated as °.Node-substitution identifies the leftmostnonterminal frontier node of one subtree with theroot node of a second subtree (i.e., the secondsubtree is substituted  on the leftmost nonterminalfrontier node of the first subtree). Thus a newsentence such as Mary likes Susan  can be derived by
combining subtrees from this corpus, shown infigure 2.
NP VP
S
NPV
likes
NP
Mary
NP
Susan NP VP
S
NPMary V
likes Susan
=° °
Figure 2. A derivation for Mary likes Susan
Other derivations may yield the same tree, e.g.:
NP VP
S
NPV
NP
Mary NP VP
S
NPMary V
likes Susan
=
Susan
V
likes
° °
Figure 3. Another derivation yielding same tree
DOP1 computes the probability of a subtree t as theprobability of selecting t among all corpus subtreesthat can be substituted on the same node as t. Thisprobability is computed as the number ofoccurrences of t in the corpus, | t |, divided by thetotal number of occurrences of all subtrees t' withthe same root label as t .1 Let r(t) return the root labelof t. Then we may write:
P(t)  =   | t |S  t': r(t')= r(t)  | t' |
The probability of a derivation t1°...°tn is computedby the product of the probabilities of its subtrees t i:
P(t1°...°tn)  =  ?i P(ti)As we have seen, there may be several distinctderivations that generate the same parse tree. Theprobability of a parse tree T is the sum of the
1 This subtree probability is redressed by a simple
correction factor discussed in Goodman (2003: 136)and Bod (2003).
866
probabilities of its distinct derivations. Let tid be thei-th subtree in the derivation d that produces tree T ,then the probability of T is given by
P(T)  =  Sd?i P(tid)Thus DOP1 considers counts of subtrees of a widerange of sizes: everything from counts of single-level rules to entire trees is taken into account tocompute the most probable parse tree of a sentence.A disadvantage of the approach may be that anextremely large number of subtrees (andderivations) must be considered. Fortunately thereexists a compact isomorphic PCFG-reduction ofDOP1 whose size is linear rather than exponential inthe size of the training set (Goodman 2003).Moreover, Collins and Duffy (2002) show how atree kernel can be applied to DOP1's all-subtreesrepresentation. The currently most successfulversion of DOP1 uses a PCFG-reduction of themodel with an n-best parsing algorithm (Bod 2003).
3  U-DOP
U-DOP extends DOP1 to unsupervised parsing(Bod 2006). Its key idea is to assign all unlabeledbinary trees to a set of sentences and to next use (inprinciple) all subtrees from these binary trees toparse new sentences. U-DOP thus proposes one ofthe richest possible models in bootstrapping trees.Previous models like Klein and Manning's (2002,2005) CCM model limit the dependencies to"contiguous subsequences of a sentence". Thismeans that CCM neglects dependencies that arenon-contiguous such as between more  and than in"BA carried more people than cargo". Instead, U-DOP's all-subtrees  approach captures bothcontiguous and non-contiguous lexical dependen-cies.
As with most other unsupervised parsingmodels, U-DOP induces trees for p-o-s stringsrather than for word strings. The extension to wordstrings is straightforward as there exist highlyaccurate unsupervised part-of-speech taggers (e.g.Schütze 1995) which can be directly combined withunsupervised parsers.
To give an illustration of U-DOP, considerthe WSJ p-o-s string NNS VBD JJ NNS whichmay correspond for instance to the sentenceInvestors suffered heavy losses . U-DOP starts by
assigning all possible binary trees to this string,where each root node is labeled S and each internalnode is labeled X. Thus NNS VBD JJ NNS has atotal of five binary trees shown in figure 4 -- wherefor readability we add words as well.
NNS VBD JJ NNS
Investors suffered heavy losses
XX
S
  
NNS VBD JJ NNSInvestors suffered heavy losses
X
X
S
NNS VBD JJ NNSInvestors suffered heavy losses
XX
S
  
NNS VBD JJ NNSInvestors suffered heavy losses
X
X
S
NNS VBD JJ NNS
Investors suffered heavy losses
XX
S
Figure 4. All binary trees for NNS VBD JJ NNS(Investors suffered heavy losses)
While we can efficiently represent the set of allbinary trees of a string by means of a chart, we needto unpack the chart if we want to extract subtreesfrom this set of binary trees. And since the totalnumber of binary trees for the small WSJ10 isalmost 12 million, it is doubtful whether we canapply the unrestricted U-DOP model to such acorpus. U-DOP therefore randomly samples a largesubset from the total number of parse trees from thechart (see Bod 2006) and next converts the subtreesfrom these parse trees into a PCFG-reduction(Goodman 2003). Since the computation of themost probable parse tree is NP-complete (Sima'an1996), U-DOP estimates the most probable treefrom the 100 most probable derivations usingViterbi n-best parsing. We could also have used themore efficient k-best hypergraph parsing techniqueby Huang and Chiang (2005), but we have not yetincorporated this into our implementation.
To give an example of the dependencies thatU-DOP can take into account, consider thefollowing subtrees in figure 5 from the trees in
867
figure 4 (where we again add words for readability).These subtrees show that U-DOP takes into accountboth contiguous and non-contiguous substrings.
NNS VBD
Investors suffered
X
X
S
VBD
suffered
X
X
NNS NNS
Investors losses
X
X
S
JJ NNS
heavy losses
XX
S
JJ NNS
heavy losses
X
NNS VBD
Investors suffered
X
VBD JJ
suffered heavy
X
Figure 5. Some subtrees from trees in figure 4
Of course, if we only had the sentence Investorssuffered heavy losses  in our corpus, there would beno difference in probability between the five parsetrees in figure 4. However, if we also have adifferent sentence where JJ NNS ( heavy losses)appears in a different context, e.g. in Heavy losseswere reported , its covering subtree gets a relativelyhigher frequency and the parse tree where heavylosses  occurs as a constituent gets a higher totalprobability.
4  ML-DOP
ML-DOP (Bod 2000) extends DOP with amaximum likelihood reestimation technique basedon the expectation-maximization (EM) algorithm(Dempster et al. 1977) which is known to bestatistically consistent (Shao 1999). ML-DOPreestimates DOP's subtree probabilities in aniterative way until the changes become negligible.The following exposition of ML-DOP is heavilybased on previous work by Bod (2000) andMagerman (1993).
It is important to realize that there is animplicit assumption in DOP that all possiblederivations of a parse tree contribute equally to the
total probability of the parse tree. This is equivalentto saying that there is a hidden component to themodel, and that DOP can be trained using an EMalgorithm to determine the maximum likelihoodestimate for the training data. The EM algorithm forthis ML-DOP model is related to the Inside-Outsidealgorithm for context-free grammars, but thereestimation formula is complicated by the presenceof subtrees of depth greater than 1. To derive thereestimation formula, it is useful to consider thestate space of all possible derivations of a tree.
The derivations of a parse tree T  can beviewed as a state trellis, where each state contains apartially constructed tree in the course of a leftmostderivation of T. st denotes a state containing the treet which is a subtree of T . The state trellis is definedas follows.
The initial state, s0, is a tree with depth zero,consisting of simply a root node labeled with S. Thefinal state, sT, is the given parse tree T.
A state st is connected forward to all statesstf such that tf  = t ° t', for some t' . Here theappropriate t'  is defined to be tf - t.
A state s t is connected backward to all statesstb such that t = tb ° t', for some t' . Again, t'  isdefined to be t - tb.
The construction of the state lattice andassignment of transition probabilities according tothe ML-DOP model is called the forward pass. Theprobability of a given state, P (s), is referred to asa(s). The forward probability of a state s t iscomputed recursively
a(st) = S a(st  ) P(t - tb).bs tb
The backward probability of a state, referred to asß(s), is calculated according to the followingrecursive formula:
ß(st) = S ß(st  ) P(tf - t)ffst
where the backward probability of the goal state isset equal to the forward probability of the goal state,ß(sT) = a(sT).
The update formula for the count of asubtree t is (where r(t) is the root label of t):
868
ct(t) =      S      ß(st  )a(st  )P(t | r(t))f b
a(sgoal)st  :?st ,tb°t=tfb f
The updated probability distribution, P'(t  | r(t)), isdefined to be
P' (t | r(t)) =  ct(t)ct(r(t))
where ct(r(t)) is defined as
ct(r(t)) = S ct(t')t ': r( t')=r( t)
In practice, ML-DOP starts out by assigning thesame relative frequencies to the subtrees as DOP1,which are next reestimated by the formulas above.We may in principle start out with any initialparameters, including random initializations, butsince ML estimation is known to be very sensitiveto the initialization of the parameters, it is convenientto start with parameters that are known to performwell.
To avoid overtraining, ML-DOP uses thesubtrees from one half of the training set to betrained on the other half, and vice versa. This cross-training is important since otherwise UML-DOPwould assign the training set trees their empiricalfrequencies and assign zero weight to all othersubtrees (cf. Prescher et al. 2004). The updatedprobabilities are iteratively reestimated until thedecrease in cross-entropy becomes negligible.Unfortunately, no compact PCFG-reduction of ML-DOP is known. As a consequence, parsing withML-DOP is very costly and the model has hithertonever been tested on corpora larger than OVIS(Bonnema et al. 1997). Yet, we will show that byclever pruning we can extend our experiments notonly to the WSJ, but also to the German NEGRAand the Chinese CTB. (Zollmann and Sima'an 2005propose a different consistent estimator for DOP,which we cannot go into here).
5  UML-DOP
Analogous to U-DOP, UML-DOP is anunsupervised generalization of ML-DOP: it firstassigns all unlabeled binary trees to a set of
sentences and next extracts a large (random) set ofsubtrees from this tree set. It then reestimates theinitial probabilities of these subtrees by ML-DOP onthe sentences from a held-out part of the tree set.The training is carried out by dividing the tree setinto two equal parts, and reestimating the subtreesfrom one part on the other. As initial probabilitieswe use the subtrees' relative frequencies as describedin section 2 (smoothed by Good-Turing -- see Bod1998), though it would also be interesting to seehow the model works with other initial parameters,in particular with the usage frequencies proposed byZuidema (2006).
As with U-DOP, the total number ofsubtrees that can be extracted from the binary treeset is too large to be fully taken into account.Together with the high computational cost ofreestimation we propose even more drastic pruningthan we did in Bod (2006) for U-DOP. That is,while for sentences = 7 words we use all binarytrees, for each sentence = 8 words we randomlysample a fixed number of 128 trees (whicheffectively favors more frequent trees). The resultingset of trees is referred to as the binary tree set. Next,we randomly extract for each subtree-depth a fixednumber of subtrees, where the depth of subtree isthe longest path from root to any leaf. This hasroughly the same effect as the correction factor usedin Bod (2003, 2006). That is, for each particulardepth we sample subtrees by first randomlyselecting a node in a random tree from the binarytree set after which we select random expansionsfrom that node until a subtree of the particular depthis obtained. For our experiments in section 6, werepeated this procedure 200,000 times for eachdepth. The resulting subtrees are then given to ML-DOP's reestimation procedure. Finally, thereestimated subtrees are used to compute the mostprobable parse trees for all sentences using Viterbin-best, as described in section 3, where the mostprobable parse is estimated from the 100 mostprobable derivations.
A potential criticism of (U)ML-DOP is thatsince we use DOP1's relative frequencies as initialparameters, ML-DOP may only find a localmaximum nearest to DOP1's estimator. But this isof course a criticism against any iterative MLapproach: it is not guaranteed that the globalmaximum is found (cf. Manning and Schütze 1999:401). Nevertheless we will see that our reestimation
869
procedure leads to significantly better accuracycompared to U-DOP (the latter would be equal toUML-DOP under 0 iterations). Moreover, incontrast to U-DOP, UML-DOP can  be theoreticallymotivated: it maximizes the likelihood of the datausing the statistically consistent EM algorithm.
6  Experiments: Can we beat supervised byunsupervised parsing?
To compare UML-DOP to U-DOP, we started outwith the WSJ10 corpus, which contains 7422sentences = 10 words after removing emptyelements and punctuation. We used the sameevaluation metrics for unlabeled precision (UP) andunlabeled recall (UR) as defined in Klein (2005: 21-22). Klein's definitions differ slightly from thestandard PARSEVAL metrics: multiplicity ofbrackets is ignored, brackets of span one are ignoredand the bracket labels are ignored. The two metricsof UP and UR are combined by the unlabeled f -score F1 which is defined as the harmonic mean ofUP and UR: F1 = 2*UP*UR/(UP+UR).
For the WSJ10, we obtained a binary treeset of 5.68 * 105 trees, by extracting the binary treesas described in section 5. From this binary tree setwe sampled 200,000 subtrees for each subtree-depth. This resulted in a total set of roughly 1.7 *1 0 6  subtrees that were reestimated by ourmaximum-likelihood procedure. The decrease incross-entropy became negligible after 14 iterations(for both halfs of WSJ10). After computing themost probable parse trees, UML-DOP achieved anf-score of 82.9% which is a 20.5% error reductioncompared to U-DOP's f-score of 78.5% on thesame data (Bod 2006).
We next tested UML-DOP on twoadditional domains which were also used in Kleinand Manning (2004) and Bod (2006): the GermanNEGRA10 (Skut et al. 1997) and the ChineseCTB10 (Xue et al. 2002) both containing 2200+sentences = 10 words after removing punctuation.Table 1 shows the results of UML-DOP comparedto U-DOP, the CCM model by Klein and Manning(2002), the DMV dependency learning model byKlein and Manning (2004) as well as theircombined model DMV+CCM.
Table 1 shows that UML-DOP scores betterthan U-DOP and Klein and Manning's models in allcases. It thus pays off to not only use subtrees rather
than substrings (as in CCM) but to also reestimatethe subtrees' probabilities by a maximum-likelihoodprocedure rather than using their (smoothed) relativefrequencies (as in U-DOP). Note that UML-DOPachieves these improved results with fewer  subtreesthan U-DOP, due to UML-DOP's more drasticpruning of subtrees. It is also noteworthy that UML-DOP, like U-DOP, does not employ a separate classfor non-constituents, so-called distituents, whileCCM and CCM+DMV do. (Interestingly, the top10 most frequently learned constituents by UML-DOP were exactly the same as by U-DOP -- see therelevant table in Bod 2006).
Model English German Chinese(WSJ10) (NEGRA10) (CTB10)CCM 71.9 61.6 45.0DMV 52.1 49.5 46.7DMV+CCM 77.6 63.9 43.3U-DOP 78.5 65.4 46.6UML-DOP 82.9 67.0 47.2
Table 1. F-scores of UML-DOP compared toprevious models on the same data
We were also interested in testing UML-DOP onlonger sentences. We therefore included all WSJsentences up to 40 words after removing emptyelements and punctuation (WSJ40) and againsampled 200,000 subtrees for each depth, using thesame method as before. Furthermore, we comparedUML-DOP against a supervised binarized PCFG,i.e. a treebank PCFG whose simple relativefrequency estimator corresponds to maximumlikelihood (Chi and Geman 1998), and which weshall refer to as "ML-PCFG". To this end, we useda random 90%/10% division of WSJ40 into atraining set and a test set. The ML-PCFG had thusaccess to the Penn WSJ trees in the training set,while UML-DOP had to bootstrap all structure fromthe flat strings from the training set to next parse the10% test set -- clearly a much more challengingtask. Table 2 gives the results in terms of f-scores.
The table shows that UML-DOP scoresbetter than U-DOP, also for WSJ40. Our results onWSJ10 are somewhat lower than in table 1 due tothe use of a smaller training set of 90% of the data.But the most surprising result is that UML-DOP's f-
870
score is higher than the supervised  binarized tree-bank PCFG (ML-PCFG) for both WSJ10 andWSJ40. In order to check whether this difference isstatistically significant, we additionally tested on 10different 90/10 divisions of the WSJ40 (which werethe same splits as in Bod 2006). For these splits,UML-DOP achieved an average f-score of 66.9%,while ML-PCFG obtained an average f-score of64.7%. The difference in accuracy between UML-DOP and ML-PCFG was statistically significantaccording to paired t-testing (p=0.05). To the best ofour knowledge this means that we have shown forthe first time that an unsupervised parsing model(UML-DOP) outperforms a widely used supervisedparsing model (a treebank PCFG) on the WSJ40 .
Model       WSJ10  WSJ40
U-DOP 78.1 63.9UML-DOP 82.5 66.4ML-PCFG 81.5 64.6
Table 2. F-scores of U-DOP, UML-DOP and asupervised treebank PCFG (ML-PCFG) for a
random 90/10 split of WSJ10 and WSJ40.
We should keep in mind that (1) a treebank PCFGis not state-of-the-art: its performance is mediocrecompared to e.g. Bod (2003) or McClosky et al.(2006), and (2) that our treebank PCFG is binarizedas in Klein and Manning (2005) to make resultscomparable. To be sure, the unbinarized version ofthe treebank PCFG obtains 89.0% average f-scoreon WSJ10 and 72.3% average f-score on WSJ40.Remember that the Penn Treebank annotations areoften exceedingly flat, and many branches have aritylarger than two. It would be interesting to see howUML-DOP performs if we also accept ternary (andwider) branches -- though the total number ofpossible trees that can be assigned to strings wouldthen further explode.
UML-DOP's performance still remainsbehind that of supervised  (binarized) DOP parsers,such as DOP1, which achieved 81.9% average f-score on the 10 WSJ40 splits, and ML-DOP, whichperformed slightly better with 82.1% average f-score. And if DOP1 and ML-DOP are notbinarized, their average f-scores are respectively90.1% and 90.5% on WSJ40. However, DOP1 and
ML-DOP heavily depend on annotated data whereasUML-DOP only needs unannotated data. It wouldthus be interesting to see how close UML-DOP canget to ML-DOP's performance if we enlarge theamount of training data.
7  Conclusion: Is the end of supervisedparsing in sight?
Now that we have outperformed a well-knownsupervised parser by an unsupervised one, we mayraise the question as to whether the end ofsupervised NLP comes in sight. All supervisedparsers are reaching an asymptote and furtherimprovement does not seem to come from morehand-annotated data but by adding unsupervised orsemi-unsupervised techniques (cf. McClosky et al.2006). Thus if we modify our question as: does theexclusively  supervised approach to parsing come toan end, we believe that the answer is certainly yes.
Yet we should neither rule out thepossibility that entirely unsupervised methods willin fact surpass semi-supervised methods. The mainproblem is how to quantitatively compare thesedifferent parsers, as any evaluation on hand-annotated data (like the Penn treebank) willunreasonably favor semi-supervised parsers. Thereis thus is a quest for designing an annotation-independent evaluation scheme. Since parsers arebecoming increasingly important in applications likesyntax-based machine translation and structurallanguage models for speech recognition, one way togo would be to compare these different parsingmethods by isolating their contribution in improvinga concrete NLP system, rather than by testing themagainst gold standard annotations which areinherently theory-dependent.
The initially disappointing results ofinducing trees entirely from raw text was not somuch due to the difficulty of the bootstrappingproblem per se , but to (1) the poverty of the initialmodels and (2) the difficulty of finding theory-independent evaluation criteria. The time has cometo fully reappraise unsupervised parsing modelswhich should be trained on massive amounts ofdata, and be evaluated in a concrete application.
There is a final question as to how far theDOP approach to unsupervised parsing can bestretched. In principle we can assign all possiblesyntactic categories, semantic roles, argument
871
structures etc. to a set of given sentences and let thestatistics decide which assignments are most usefulin parsing new sentences. Whether such a massivelymaximalist approach is feasible can only beanswered by empirical investigation in due time.
Acknowledgements
Thanks to Willem Zuidema, David Tugwell andespecially to three anonymous reviewers whoseunanymous suggestions on DOP and EMconsiderably improved the original paper. Asubstantial part of this research was carried out inthe context of the NWO Exact project"Unsupervised Stochastic Grammar Induction fromUnlabeled Data", project number 612.066.405.
ReferencesBod, R. 1998. Beyond Grammar: An Experience-Based
Theory of Language, CSLI Publications, distributedby Cambridge University Press.
Bod, R. 2000. Combining semantic and syntacticstructure for language modeling. Proceedings ICSLP2000, Beijing.
Bod, R. 2003. An efficient implementation of a newDOP model. Proceedings EACL 2003, Budapest.
Bod, R. 2006. Unsupervised Parsing with U-DOP.Proceedings CONLL 2006, New York.
Bonnema, R., R. Bod and R. Scha, 1997. A DOPmodel for semantic interpretation, ProceedingsACL/EACL 1997, Madrid.
Chi, Z. and S. Geman 1998. Estimation ofProbab i l i s t i c  Con tex t -F ree  Grammars .Computational Linguistics 24, 299-305.
Clark, A. 2001. Unsupervised induction of stochasticcontext-free grammars using distributionalclustering. Proceedings CONLL 2001.
Collins, M. and N. Duffy 2002. New rankingalgorithms for parsing and tagging: kernels overdiscrete structures, and the voted perceptron.Proceedings ACL 2002, Philadelphia.
Dempster, A., N. Laird and D. Rubin, 1977. MaximumLikelihood from Incomplete Data via the EMAlgorithm, Journal of the Royal Statistical Society39, 1-38.
Goodman, J. 2003. Efficient algorithms for the DOPmodel. In R. Bod, R. Scha and K. Sima'an (eds.).Data-Oriented Parsing, University of Chicago Press.
Huang, L. and D. Chiang 2005. Better k-best parsing.Proceedings IWPT 2005, Vancouver.
Johnson, M. 2002. The DOP estimation method isbiased and inconsistent. Computational Linguistics28, 71-76.
Klein, D. 2005. The Unsupervised Learning of NaturalLanguage Structure . PhD thesis, StanfordUniversity.
Klein, D. and C. Manning 2002. A generalconstituent-context model for improved grammarinduction. Proceedings ACL 2002, Philadelphia.
Klein, D. and C. Manning 2004. Corpus-basedinduction of syntactic structure: models ofdependency and constituency. Proceedings ACL2004, Barcelona.
Klein, D. and C. Manning 2005. Natural languagegrammar induction with a generative constituent-context model. Pattern Recognition 38, 1407-1419.
Magerman, D. 1993. Expectation-Maximization forData-Oriented Parsing, IBM Technical Report,Yorktown Heights, NY.
McClosky, D., E. Charniak and M. Johnson 2006.Effective self-training for parsing. Proceedings HLT-NAACL 2006, New York.
Manning, C. and H. Schütze 1999. Foundations ofStatistical Natural Language Processing. The MITPress.
Prescher, D., R. Scha, K. Sima'an and A. Zollmann2004. On the statistical consistency of DOPestimators. Proceedings CLIN 2004, Leiden.
Schütze, H. 1995. Distributional part-of-speechtagging. Proceedings ACL 1995, Dublin.
Shao, J. 1999. Mathematical Statistics. SpringerVerlag, New York.
Sima'an, K. 1996. Computational complexity ofprobabilistic disambiguation by means of treegrammars. Proceedings COLING 1996, Copenhagen.
Skut, W., B. Krenn, T. Brants and H. Uszkoreit 1997.An annotation scheme for free word orderlanguages. Proceedings ANLP 1997.
Xue, N., F. Chiou and M. Palmer 2002. Building alarge-scale annotated Chinese corpus. ProceedingsCOLING 2002, Taipei.
van Zaanen, M. 2000. ABL: Alignment-BasedLearning. Proceedings COLING 2000, Saarbrücken.
Zollmann, A. and K. Sima'an 2005. A consistent andefficient estimator for data-oriented parsing. Journalof Automata, Languages and Combinatorics, in press.
Zuidema, W. 2006. What are the productive units ofnatural language grammar? A DOP approach to theautomatic identification of constructions.Proceedings CONLL 2006, New York.
872
