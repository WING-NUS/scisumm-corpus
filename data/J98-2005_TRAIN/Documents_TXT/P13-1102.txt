The effect of non-tightness on Bayesian estimation of PCFGs
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1033–1041,Sofia, Bulgaria, August 4-9 2013. c©2013 Association for Computational Linguistics
The effect of non-tightness on Bayesian estimation of PCFGs
Shay B. CohenDepartment of Computer Science
Columbia Universityscohen@cs.columbia.edu
Mark JohnsonDepartment of Computing
Macquarie Universitymark.johnson@mq.edu.au
Abstract
Probabilistic context-free grammars havethe unusual property of not always defin-ing tight distributions (i.e., the sum of the“probabilities” of the trees the grammargenerates can be less than one). This paperreviews how this non-tightness can ariseand discusses its impact on Bayesian es-timation of PCFGs. We begin by present-ing the notion of “almost everywhere tightgrammars” and show that linear CFGs fol-low it. We then propose three differentways of reinterpreting non-tight PCFGs tomake them tight, show that the Bayesianestimators in Johnson et al. (2007) arecorrect under one of them, and provideMCMC samplers for the other two. Weconclude with a discussion of the impactof tightness empirically.
1 Introduction
Probabilistic Context-Free Grammars (PCFGs)play a special role in computational linguistics be-cause they are perhaps the simplest probabilisticmodels of hierarchical structures. Their simplicityenables us to mathematically analyze their prop-erties to a detail that would be difficult with lin-guistically more accurate models. Such analysisis useful because it is reasonable to expect morecomplex models to exhibit similar properties aswell.
The problem of inferring PCFG rule probabil-ities from training data consisting of yields orstrings alone is interesting from both cognitive andengineering perspectives. Cognitively it is implau-sible that children can perceive the parse trees ofthe language they are learning, but it is more rea-sonable to assume that they can obtain the terminalstrings or yield of these trees. Unsupervised meth-ods for learning a grammar from terminal stringsalone is also interesting from an engineering per-spective because such training data is cheap and
plentiful, while the manually parsed data requiredby supervised methods are expensive to produceand relatively rare.
Cohen and Smith (2012) show that inferringPCFG rule probabilities from strings alone is com-putationally intractable, so we should not expectto find an efficient, general-purpose algorithm forthe unsupervised problem. Instead, approxima-tion algorithms are standardly used. For exam-ple, the Inside-Outside (IO) algorithm efficientlyimplements the Expectation-Maximization (EM)procedure for approximating a Maximum Likeli-hood estimator (Lari and Young, 1990). Bayesianestimators for PCFG rule probabilities have alsobeen attracting attention because they provide atheoretically-principled way of incorporating priorinformation. Kurihara and Sato (2006) proposeda Variational Bayes estimator based on a mean-field approximation, and Johnson et al. (2007) pro-posed MCMC samplers for the posterior distribu-tion over rule probabilities and the parse trees ofthe training data strings.
PCFGs have the interesting property (which weexpect most linguistically more realistic models toalso possess) that the distributions they define arenot always properly normalized or “tight”. In anon-tight PCFG the partition function (i.e., sumof the “probabilities” of all the trees generated bythe PCFG) is less than one. (Booth and Thomp-son, 1973, called such non-tight PCFGs “incon-sistent”, but we follow Chi and Geman (1998)in calling them “non-tight” to avoid confusionwith the consistency of statistical estimators). Chi(1999) showed that renormalized non-tight PCFGs(which he called “Gibbs CFGs”) define the sameclass of distributions over trees as do tight PCFGswith the same rules, and provided an algorithm formapping any PCFG to a tight PCFG with the samerules that defines the same distribution over trees.
An obvious question is then: how does tightnessaffect the inference of PCFGs? Chi and Geman(1998) studied the question for Maximum Likeli-hood (ML) estimation, and showed that ML es-
1033
timates are always tight for both the supervisedcase (where the input consists of parse trees) andthe unsupervised case (where the input consists ofyields or terminal strings). This means that MLestimators can simply ignore issues of tightness,and rest assured that the PCFGs they estimate arein fact tight.
The situation is more subtle with Bayesian es-timators. We show that for the special case oflinear PCFGs (which include HMMs) with non-degenerate priors the posterior puts zero mass onnon-tight PCFGs, so tightness is not an issue withBayesian estimation of such grammars. However,because all of the commonly used priors (such asthe Dirichlet or the logistic normal) assign non-zero probability across the whole probability sim-plex, in general the posterior may assign non-zeroprobability to non-tight PCFGs. We discuss threedifferent possible approaches to this in this paper:
1. the only-tight approach, where we modify theprior so it only assigns non-zero probabilityto tight PCFGs,
2. the renormalization approach, where werenormalize non-tight PCFGs so they definea probability distribution over trees, and
3. the sink-element approach, where we reinter-pret non-tight PCFGs as assigning non-zeroprobability to a “sink element”, so both tightand non-tight PCFGs are properly normal-ized.
We show how to modify the Gibbs sampler de-scribed by Johnson et al. (2007) so it producessamples from the posterior distributions definedby the only-tight and renormalization approaches.Perhaps surprisingly, we show that Gibbs sampleras defined by Johnson et al. actually producessamples from the posterior distributions defined bythe sink-element approach.
We conclude by studying the effect of requir-ing tightness on the estimation of some simplePCFGs. Because the Bayesian posterior convergesaround the (tight) ML estimate as the size ofthe data grows, requiring tightness only seems tomake a difference with highly biased priors or withvery small training corpora.
2 PCFGs and tightness
LetG = (T,N, S,R) be a Context-Free Grammarin Chomsky normal form with no useless produc-tions, where T is a finite set of terminal symbols,
N is a finite set of nonterminal symbols (disjointfrom T ), S ? N is a distinguished nonterminalcalled the start symbol, andR is a finite set of pro-ductions of the form A ? BC or A ? w, whereA,B,C ? N and w ? T . In what follows we useß as a variable ranging over (N ×N) ? T .
A Probabilistic Context-Free Grammar (G,T)is a pair consisting of a context-free grammar Gand a real-valued vector T of length |R| indexedby productions, where ?A?ß is the productionprobability associated with the production A ?ß ? R. We require that ?A?ß = 0 and that forall nonterminals A ? N , ?A?ß?RA ?A?ß = 1,where RA is the subset of rules R expanding thenonterminal A.
A PCFG (G,T) defines a measure µT overtrees t as follows:
µT(t) =?r?R
?fr(t)r
where fr(t) is the number of times the productionr = A? ß ? R is used in the derivation of t.
The partition function Z or measure of all pos-sible trees is:
Z(T) =?t'?T
?r?R
?fr(t')
r
where T is the set of all (finite) trees generatedby G. A PCFG is tight iff the partition functionZ(T) = 1. In this paper we use T? to denote theset of rule probability vectors T for which G isnon-tight. Nederhof and Satta (2008) survey sev-eral algorithms for computing Z(T), and hencefor determining whether a PCFG is tight.1
Non-tightness can arise in very simple PCFGs,such as the “Catalan” PCFG S ? S S | a. Thisgrammar produces binary trees where all internalnodes are labeled as S and the yield of these treesis a sequence of as. If the probability of the ruleS ? S S is greater than 0.5 then this PCFG isnon-tight.
Perhaps the most straight-forward way to under-stand this non-tightness is to view this grammar asdefining a branching process where an S can either“reproduce” with probability ?S?S S or “die out”
1We found out that finding whether a PCFG is tight bydirectly inspecting the partition function value is less stablethan using the method in Wetherell (1980). For this reason,we used Wetherell’s approach, which is based on finding theprincipal eigenvalue of the matrix M .
1034
with probability ?S?a. When ?S?S S > ?S?a theS nodes reproduce at a faster rate than they dieout, so the derivation has a non-zero probability ofendlessly rewriting (Atherya and Ney, 1972).
3 Bayesian inference for PCFGs
The goal of Bayesian inference for PCFGs is to in-fer a posterior distribution over the rule probabil-ity vectors T given observed data D. This poste-rior distribution is obtained by combining the like-lihood P(D | T) with a prior distribution P(T)over T using Bayes Rule.
P(T | D) ? P(D | T) P(T)
We now formally define the three approaches tohandling non-tightness mentioned earlier:
the only-tight approach: we only permit priorswhere P(T?) = 0, i.e., we insist that theprior assign zero mass to non-tight rule prob-ability vectors, so Z = 1. This means we candefine:
P(t | T) = µT(t)
the renormalization approach: we renormalizenon-tight PCFGs by dividing by the partitionfunction:
P(t | T) = 1Z(T)
µT(t) (1)
the sink-element approach: we redefine ourprobability distribution so its domain is a setT ' = T ? {?}, where T is the set of (finite)trees generated by G and ? 6? T is a newelement that serves as a “sink state” to whichthe “missing mass” 1 - Z(T) is assigned.Then we define:2
P(t | T) ={µT(t) if t ? T1- Z(T) if t = ?
2This definition of a distribution over trees can be inducedby a tight PCFG with a special ? symbol in its vocabulary.Given G, the first step is to create a tight grammar G0 usingthe renormalization approach. Then, a new start symbol isadded to G0, S0, and also rules S0 ? S (where S is theold start symbol in G0) and S0 ? ?. The first rule is givenprobability Z(T) and the second rule is given probability 1-Z(T). It can be then readily shown that the new tight PCFGG0 induces a distribution over trees just like in Eq. 3, onlywith additional S0 on top of all trees.
With this in hand, we can now define the likeli-hood term. We consider two types of data D here.In the supervised setting the data D consists of acorpus of parse trees D = (t1, . . . , tn) where eachtree ti is generated by the PCFG G, so
P(D | T) =n?i=1
P(ti | T)
In the unsupervised setting the data D consistsof a corpus of strings D = (w1, . . . , wn) whereeach string wi is the yield of one or more treesgenerated by G. In this setting
P(D | T) =n?i=1
P(wi | T),where:
P(w | T) =?
t?T :yield(t)=wP(t | T)
4 The special case of linear PCFGs
One way to handle the issue of tightness is to iden-tify a family of CFGs for which practically any pa-rameter setting will yield a tight PCFG. This is thefocus of this section, in which we identify a sub-set of CFGs, which are “almost everywhere” tight.This family of CFGs includes many of the CFGsused in NLP applications.
We cannot expect that a CFG will yield a tightPCFG for any assignment to the rule probabilities(i.e. that T? = Ø). Even in simple cases, such asthe grammar S ? S|a, the assignment of proba-bility 1 to S ? S and 0 to the other rule rendersthe S nonterminal useless, and places all of theprobability mass on infinite structures of the formS ? S ? S ? . . ..
However, we can weaken our requirement sothat the cases in which parameter assignmentyields a non-tight PCFG are rare, or have measurezero. To put it more formally, we say that a priorP(T) is “tight almost everywhere for G” if
P(T?) =?
T?T?P(T) dT = 0.
We now provide a sufficient condition (linear-ity) for CFGs under which they are tight almosteverywhere with any continuous prior.
For a nonterminal A ? N and ß ? (N ? T )*,we use A?k ß to denote that A can be re-writtenusing a sequence of rules from R to the sententialform ß in k derivation steps. We use A ?+ ß todenote that there exists a k > 0 such thatA?k ß.
1035
Definition 1 A context-free grammarG is linear ifthere are no A ? N such that
A?+ . . . A . . . A . . . .Definition 2 A nonterminal A ? N in a proba-bilistic context-free grammar G with parametersT is nonterminating if
PG(A?+ . . . A . . . |T) = 1.Here P(A?+ . . . A . . . |T) is defined as:?
ß:ß=...A...
PG(A?+ ß|T).
Lemma 1 A linear PCFG G with parameters Twhich does not have any nonterminating nonter-minals is tight.
Proof: Our proof relies on the properties of a cer-tain |N | × |N | matrix M where:
MAB =?
A?ß?RAn(ß,B) ?A?ß
where n(ß,B) is the number of appearances of thenonterminal B in the sequence ß. MAB is the ex-pected number of B nonterminals generated froman A nonterminal in one single derivational step,so [Mk]AB is the expected number ofB nontermi-nals generated from an A nonterminal in a k-stepderivation (Wetherell, 1980).
Since M is a non-negative matrix, under someregularity conditions, the Frobenius-Perron theo-rem states that the largest eigenvalue of this ma-trix (in absolute value) is a real number. Let thiseigenvalue be denoted by ?.
A PCFG is called “subcritical” if ? < 1 andsupercritical if ? > 1. Then, in turn, a PCFG istight if it is subcritical. It is not tight if it is su-percritical. The case of ? = 1 is a borderline casethat does not give sufficient information to knowwhether the PCFG is tight or not. In the Bayesiancase, for a continuous prior such as the Dirichletprior, this borderline case will have measure zerounder the prior.
Now let A ? N . Since the grammar is lin-ear, there is no derivation A ?+ . . . A . . . A . . ..Therefore, any derivation of the form A ?+. . . A . . . includes A on the right hand-side exactlyonce. Because the grammar has no useless non-terminals, the probability of such a derivation isstrictly smaller than 1.
For each A ? N , define:
pA =?
ß=...A...
P(A?|N | ß|T).
Since A is not useless, then pA < 1. Thereforeq = maxA pA < 1. Since any derivation of lengthk of the formA? . . . A . . . can be decomposed toat least
k
2|N | cycles that start at a terminal B ? Nand end in the same nonterminal B ? N , it holdsthat:
[Mk]AA = qk
2|N| k?8? 0.
This means that trace(Mk) k?8? 0. This meansthat the eigenvalue of M is strictly smaller than 1(linear algebra), and therefore the PCFG is tight.?Proposition 1 Any continuous prior P(T) on alinear grammar G is tight almost everywhere forG.
Proof: Let G be a linear grammar. With a contin-uous prior, the probability ofG getting parametersfrom the prior which yield a useless non-terminalis 0 – it would require setting at least one rule inthe grammar with rule probability which is exactly1. Therefore, with probability 1, the parameterstaken from the prior yield a PCFG which is linearand does not have nonterminating nonterminals.According to Lemma 1, this means the PCFG istight. ?
Deciding whether a grammar G is linear canbe done in polynomial time using the constructionfrom Bar-Hillel et al. (1964). We can first elimi-nate the differences between nonterminals and ter-minal symbols by adding a rule A ? cA for eachnonterminal A ? N , after extending the set ofterminal symbols A with {cA|A ? N}. Let GAbe the grammar G with the start symbol being re-placed with A. We can then intersect the grammarGA with the regular language T *cAT *cAT * (foreach nonterminal A ? N ). If for any nontermi-nal A the intersection is not the empty set (withrespect to the language that the intersection gen-erates), then the grammar is not linear. Checkingwhether the intersection is the empty set or not canbe done in polynomial time.
We conclude this section by remarking thatmany of the models used in computational lin-guistics are in fact equivalent to linear PCFGs, socontinuous Bayesian priors are almost everywheretight. For example, HMMs and many kinds of“stacked” finite-state machines are equivalent to
1036
linear PCFGs, as are the example PCFGs given inJohnson et al. (2007) to motivate the MCMC esti-mation procedures.
5 Dirichlet priors
The first step in Bayesian inference is to specify aprior on T. In the rest of this paper we take P(T)to be a product of Dirichlet distributions, with onedistribution for each non-terminal A ? N , as thisturns out to simplify the computations consider-ably. The prior is parameterized by a positive realvalued vector a indexed by productionsR, so eachproduction probability ?A?ß has a correspondingDirichlet parameter aA?ß . As before, let RA bethe set of productions in R with left-hand side A,and let ?A and aA refer to the component subvec-tors of ? and a respectively indexed by produc-tions in RA. The Dirichlet prior P(T | a) is:
P(T | a) =?A?N
PD(TA | aA),
where
PD(TA | aA) = 1C(aA)
?r?RA
?ar-1r and
C(aA) =
?r?RA G(ar)
G(?
r?RA ar)
where G is the generalized factorial function andC(a) is a normalization constant that does not de-pend on TA.
Dirichlet priors are useful because they are con-jugate to the multinomial distribution, which isthe building block of PCFGs. Ignoring issues oftightness for the moment and setting P(t | T) =µT(t), this means that in the supervised setting theposterior distribution P(T | t, a) given a set ofparse trees t = (t1, . . . , tn) is also a product ofDirichlets distribution.
P(T | t, a) ? P(t | T) P(T | a)
?(?r?R
?fr(t)r
)(?r?R
?ar-1r
)=?r?R
?fr(t)+ar-1r
which is a product of Dirichlet distributions withparameters f(t) + a, where f(t) is the vector ofrule counts in t indexed by r ? R. We can thuswrite:
P(T | t, a) = P(T | f(t) + a)
Input: Grammar G, vector of trees t, vector ofhyperparameters a, previous parameters T0.
Result: A vector of parameters Trepeat
draw ? from products of Dirichlet withhyperparameters a+ f(t)
until T is tight for G;return T
Algorithm 1: An algorithm for generating sam-ples from P(T | t, a) for the only-tight ap-proach.
Input: Grammar G, vector of trees t, vector ofhyperparameters a, previous rule parametersT0.
Result: A vector of parameters Tdraw a proposal T* from a product of Dirichlets withparameters a+ f(t).draw a uniform number u from [0, 1].
if u < min{1,(Z(T(i-1))/Z(T*)
)n} return T*.
return T0.
Algorithm 2: One step of Metropolis-Hastingsalgorithm for generating samples from P(T |t, a) for the renormalization approach.
which makes it clear that the rule counts are di-rectly added to the parameters of the prior to pro-duce the parameters of the posterior.
6 Inference in the supervised setting
We first discuss Bayesian inference in the super-vised setting, as inference in the unsupervised set-ting is based on inference for the supervised set-ting. For each of the three approaches to non-tightness we provide an algorithm that character-izes the posterior P(T | t), where t = (t1, . . . , tn)is a sequence of trees, by generating samples fromthat posterior. Our MCMC algorithms for the un-supervised setting build on these samplers for thesupervised setting.
6.1 The only-tight approachThe “only-tight” approach requires that the priorassign zero mass to non-tight rule probability vec-tors T?. One way to define such a distribution isto restrict the domain of an existing prior distribu-tion with the set of tight T and renormalize. Inmore detail, if P(T) is a prior over rule probabili-ties, then its renormalization is the prior P' definedas:
P'(T) =P(T)I(T /? T?)
Z(T?). (2)
where Z(T?) =?
T P(T)I(T /? T?)dT.
1037
Input: Grammar G, vector of trees t, vector ofhyperparameters a, previous parameters T0.
Result: A vector of parameters Tdraw T from products of Dirichlet withhyperparameters a+ f(t)return T
Algorithm 3: An algorithm for generating sam-ples from P(T | t, a) for the sink-state approach.
Perhaps surprisingly, it turns out that if P(T)belongs to a family of conjugate priors, then P'(T)also belongs to a (different) family of conjugatepriors as well.
Proposition 2 Let P(T|a) be a prior with hyper-parameters a over the parameters of G such thatP is conjugate to the grammar likelihood. ThenP', defined in Eq. 2, is conjugate to the grammarlikelihood as well.
Proof: Assume that trees t are observed, and theprior over the grammar parameters is the prior de-fined in Eq. 2. Therefore, the posterior is:
P(T|t, a) ? P'(T|a)p(t|T)
=P(T|a)p(t|T)I(T /? T?)
Z(T?)
? P(T|t, a)I(T /? T?)
Z(T?).
Since P(T|a) is a conjugate prior to the PCFGlikelihood, then there exists a' = a'(t) such thatP(T|t, a) = P'(T|a'). Therefore:
P(T|t, a) ? P(T|a')I(T /? T?)Z(T?)
.
which exactly equals P'(T|a'). ?Sampling from the posterior over the parame-
ters given a set of trees t is therefore quite sim-ple when assuming the base prior being renormal-ized is a product of Dirichlets. Algorithm 1 sam-ples from a product of Dirichlets distribution withhyperparameters a + f(t) repeatedly, each timechecking and rejecting the sample until we obtaina tight PCFG.
The more mass the Dirichlet distribution withhyperparameters a + f(t) puts on non-tightPCFGs, the more rejections will happen. In gen-eral, if the probability mass on non-tight PCFGs isq?, then it would require, on average 1/(1 - q?)samples from this distribution in order to obtain atight PCFG.
6.2 The renormalization approachThe renormalization approach modifies the likeli-hood function instead of the prior. Here we use aproduct of Dirichlets prior P(T | a) on rule prob-ability vectors T, but the presence of the partitionfunctionZ(T) in Eq. 1 means that the likelihood isno longer conjugate to the prior. Instead we have:
P(T | t) =n?i=1
µT(ti)
Z(T)P(T | a)
? 1Z(T)n
P(T | a+ f(t)). (3)
Note that the factor Z(T) depends on T, andtherefore cannot be absorbed into the constant. Al-gorithm 2 describes a Metropolis-Hastings sam-pler for sampling from the posterior in Eq. 3that uses a product of Dirichlets with parametersa+ f(t) as a proposal distribution.
In our experiments, we use the algorithm fromNederhof and Satta (2008) to compute the parti-tion function which is needed in Algorithm 2.
6.3 The “sink element” approachThe “sink element” approach does not affect thelikelihood (since the probability of a tree t is justthe product of the probabilities of the rules usedto generate it), nor does it require a change to theprior. (The sink element ? is not a member of theset of trees T , so it cannot appear in the data t).
This means that the conjugacy argument givenat the bottom of section 5 holds in this approach,so the posterior P(T | t, a) is a product of Dirich-lets with parameters f(t) + a. Algorithm 3 givesa sampler for P(T | t, a) for the sink element ap-proach.
7 Inference in the unsupervised setting
Johnson et al. (2007) provide two Markov chainMonte Carlo algorithms for Bayesian inference forPCFG rule probabilities in the unsupervised set-ting (i.e., where the data consists of a corpus ofstrings w = (w1, . . . , wn) alone). The algorithmswe give here are based on their Gibbs sampler,which in each iteration first samples parse treest = (t1, . . . , tn), where each ti is a parse forwi, from P(t | w,T), and then samples T fromP(T | t, a).
Notice that the conditional distribution P(t |w,T) is unaffected in each of our three ap-proaches (the partition functions cancel in the
1038
Input: Grammar G, vector of hyperparameters a,vector of strings w = (w1, . . . , wn), previousrule parameters T0.
Result: A vector of parameters Tfor i? 1 to n do
draw ti from P(ti|wi,T0)enduse Algorithm 2 to sample T given G, t, a and T0return T
Algorithm 4: One step of the Metropolis-within-Gibbs sampler for the renormalization approach.
renormalization approach), so the algorithm forsampling from P(t | w,T) given by Johnson etal. applies in each of our three approaches as well.
Johnson et al. ignored tightness and assumedthat P(T | t, a) is a product of Dirichlets withparameters f(t) + a. As we noted in section 6.3,this assumption holds for the sink-state approachto non-tightness, so their sampler is in fact correctfor the sink-state approach.
In fact, we obtain samplers for the unsupervisedsetting for each of our approaches by “pluggingin” the corresponding sampling algorithm (Eq. 1–3) for P(T | t, a) into the generic Gibbs samplerframework of Johnson et al.
The one complication is that because we use aMetropolis-Hastings procedure to generate sam-ples from P(T | t, a) in the renormalization ap-proach, we use the Metropolis-within-Gibbs pro-cedure given in Algorithm 4 (Robert and Casella,2004).
8 The expressive power of the threeapproaches
Probably the most important question to ask withrespect to the three different approaches to non-tightness is whether they differ in terms of expres-sive power. Clearly the three approaches differ interms of the grammars they admit (the only-tightapproach requires the prior to only assign non-zeroprobability to tight PCFGs, while the other two ap-proaches permit the prior to assign non-zero prob-ability to non-tight PCFGs as well). However, ifwe regard a grammar as merely a device for defin-ing a distribution over trees and a prior as defininga distribution over distributions over trees, it is rea-sonable to ask whether the class of distributionsover distributions of trees that each of these ap-proaches define are the same or differ. We believe,but have not proved, that all three approaches de-fine the same class of distributions over distribu-
tions of trees in the following sense: any prior usedwith one of the approaches can be transformedinto a different prior that can be used with one ofthe other approaches, and yield the same posteriorover trees conditioned on a string, marginalizingout the parameters.
This does not mean that the three approachesare equivalent, however. In this section we pro-vide a grammar such that with a uniform prior overrule probabilities, the conditional distribution overtrees given a fixed string varies under each of thethree different approaches.
The grammar we consider has three rules S ?S S S|S S|a with probabilities ?1, ?2 and 1- ?1-?2, respectively. The T parameters are required tosatisfy ?1 + ?2 = 1 and ?i = 0 for i = 1, 2.
We compute the posterior distribution overparse trees for the string w = a a a. The gram-mar generates three parse trees for w1, namely:
t1 = S
S
a
S
a
S
a
t2 = S
S
a
S
S
a
S
a
t3 = S
S
S
a
S
a
S
a
The partition function Z for this grammar is thesmallest positive root of the cubic equation:
Z = ?1Z3 + ?2Z
2 + (1- ?1 - ?2)We used Mathematica to find an analytic solutionfor Z in this equation, obtaining not only an ex-pression for the partition function Z(T) but alsoidentifying the non-tight region T?.
In order to compute P(t1|w), we used Mathe-matica to first compute the following quantities:
qsinkElement(ti) =
?TµT(ti) dT
qtightOnly(ti) =
?TµT(ti) I(T /? T?) dT
qrenormalization(ti) =
?TµT(ti)/Z(T) dT
where i ? {1, 2, 3}. We used Mathematica to ana-lytically compute q(ti) for each approach and eachi ? {1, 2, 3}. Then it’s easy to show that:
P(ti | w) = q(ti)?3i'=1 q(ti')
where the q used is based on the approach totightness desired. For the sink-element approach,
1039
0
10
20
30
0.35 0.40 0.45 0.50 0.55Average f-score
Den
sity
Inferenceonly-tight
sink-state
renormalise
Figure 1: The density of the F1-scores with thethree approaches. The prior used is a symmetricDirichlet with a = 0.1.
P(t1|w) = 711 ˜ 0.636364. For the only-tightapproach P(t1|w) = 1117917221 ˜ 0.649149. Forthe renormalization approach the analytic ex-pression is too complex to include in this paper,but it approximately equals 0.619893. A logof our Mathematica calculations is availableat http://www.cs.columbia.edu/˜scohen/acl13tightness-mathematica.pdf, and weconfirmed these results to three decimal places us-ing the samplers described above (which required107 samples per approach).
While the differences between these conditionalprobabilities are not great, the conditional prob-abilities are clearly different, so the three ap-proaches do in fact define different distributionsover trees under a uniform prior on rule probabili-ties.
9 Empirical effects of the threeapproaches in unsupervised grammarinduction
In this section we present experiments using thethree samplers just described in an unsupervisedgrammar induction problem. Our goal here isnot to improve the state-of-the-art in unsupervisedgrammar induction, but to try to measure empir-ical differences in the estimates produced by thethree different approaches to tightness just de-scribed. The bottom line of our experiments is thatwe could not detect any significant difference inthe estimates produced by samplers for these threedifferent approaches.
In our experiments we used the English Penntreebank (Marcus et al., 1993). We use the part-
of-speech tag sequences of sentences shorter than11 words in sections 2–21. The grammar we use isthe PCFG version of the dependency model withvalence (Klein and Manning, 2004), as it appearsin Smith (2006).
We used a symmetric Dirichlet prior with hy-perparameter a = 0.1. For each of the three ap-proaches for handling tightness, we ran 100 timesthe samplers in §7, each for 1,000 iterations. Wediscarded the first 900 sweeps of each run, and cal-culated the F1-scores of the sampled trees every10th sweep from the last 100 sweeps. For eachrun we calculated the average F1-score over the10 sweeps we evaluated. We thus have 100 aver-age F1-scores for each of the samplers.
Figure 1 plots the density of F1 scores (com-pared to the gold standard) resulting from theGibbs sampler, using all three approaches. Themean value for each of the approaches is 0.41with standard deviation 0.06 (only-tight), 0.41with standard deviation 0.05 (renormalization)and 0.42 with standard deviation 0.06 (sink ele-ment). In addition, the only-tight approach resultsin an average of 437 (s.d., 142) rejected propos-als in 1,000 samples, while the renormalizationapproach results in an average of 232 (s.d., 114)rejected proposals in 1,000 samples. (It’s not sur-prising that the only-tight approach results in morerejections as it keeps proposing new T until a tightproposal is found, while the renormalization ap-proach simply uses the old T).
We performed two-sample Kolmogorov-Smirnov tests (which are non-parametric testsdesigned to determine if two distributions aredifferent; see DeGroot, 1991) on each of the threepairs of 100 F1-scores. None of the tests wereclose to significant; the p-values were all above0.5. Thus our experiments provided no evidencethat the samplers produced different distributionsover trees, although it’s reasonable to expect thatthese distributions do indeed differ.
In terms of running time, our implementationof the renormalization approach was several timesslower than our implementations of the other twoapproaches because we used the naive fixed-pointalgorithm to compute the partition function: per-haps this could be improved using one of themore sophisticated partition function algorithmsdescribed in Nederhof and Satta (2008).
1040
10 Conclusion
In this paper we characterized the notion of an al-most everywhere tight grammar in the Bayesiansetting and showed it holds for linear CFGs. Fornon-linear CFGs, we described three different ap-proaches to handle non-tightness. The “only-tight” approach restricts attention to tight PCFGs,and perhaps surprisingly, we showed that conju-gacy still obtains when the domain of a productof Dirichlets prior is restricted to the subset oftight grammars. The renormalization approach in-volves renormalizing the PCFG measure µ overtrees when the grammar is non-tight, which de-stroys conjugacy with a product of Dirichlets prior.Perhaps most surprisingly of all, the sink-elementapproach, which assigns the missing mass in non-tight PCFG to a sink element ?, turns out to beequivalent to existing practice where tightness isignored.
We studied the posterior distributions over treesinduced by the three approaches under a uniformprior for a simple grammar and showed that theydiffer. We leave for future work the importantquestion of whether the classes of distributionsover distributions over trees that the three ap-proaches define are the same or different.
We described samplers for the supervisedand unsupervised settings for each of these ap-proaches, and applied them to an unsupervisedgrammar induction problem. (The code for theunsupervised samplers is available from http://web.science.mq.edu.au/˜mjohnson).
We could not detect any difference in the pos-terior distributions over trees produced by thesesamplers, despite devoting considerable computa-tional resources to the problem. This suggests thatfor these kinds of problems at least, tightness isnot of practical concern for Bayesian inference ofPCFGs.
Acknowledgements
We thank the anonymous reviewers and Gior-gio Satta for their valuable comments. ShayCohen was supported by the National ScienceFoundation under Grant #1136996 to the Com-puting Research Association for the CIFellowsProject, and Mark Johnson was supported by theAustralian Research Council’s Discovery Projectsfunding scheme (project numbers DP110102506and DP110102593).
ReferencesK. B. Atherya and P. E. Ney. 1972. Branching Pro-
cesses. Dover Publications.
Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. Onformal properties of simple phrase structure gram-mars. Language and Information: Selected Essayson Their Theory and Application, pages 116–150.
T. L. Booth and R. A. Thompson. 1973. Applyingprobability measures to abstract languages. IEEETransactions on Computers, C-22:442–450.
Z. Chi and S. Geman. 1998. Estimation of probabilis-tic context-free grammars. Computational Linguis-tics, 24(2):299–305.
Z. Chi. 1999. Statistical properties of probabilisticcontext-free grammars. Computational Linguistics,25(1):131–160.
S. B. Cohen and N. A. Smith. 2012. Empirical riskminimization for probabilistic grammars: Samplecomplexity and hardness of learning. Computa-tional Linguistics, 38(3):479–526.
M. H. DeGroot. 1991. Probability and Statistics (3rdedition). Addison-Wesley.
M. Johnson, T. L. Griffiths, and S. Goldwater. 2007.Bayesian inference for PCFGs via Markov chainMonte Carlo. In Proceedings of NAACL.
D. Klein and C. D. Manning. 2004. Corpus-basedinduction of syntactic structure: Models of depen-dency and constituency. In Proceedings of ACL.
K. Kurihara and T. Sato. 2006. Variational Bayesiangrammar induction for natural language. In 8th In-ternational Colloquium on Grammatical Inference.
K. Lari and S.J. Young. 1990. The estimation ofStochastic Context-Free Grammars using the Inside-Outside algorithm. Computer Speech and Lan-guage, 4(35-56).
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.1993. Building a large annotated corpus of En-glish: The Penn treebank. Computational Linguis-tics, 19:313–330.
M.-J. Nederhof and G. Satta. 2008. Computing par-tition functions of PCFGs. Research on Languageand Computation, 6(2):139–162.
C. P. Robert and G. Casella. 2004. Monte Carlo Sta-tistical Methods. Springer-Verlag New York.
N. A. Smith. 2006. Novel Estimation Methods for Un-supervised Discovery of Latent Structure in NaturalLanguage Text. Ph.D. thesis, Johns Hopkins Univer-sity.
C. S. Wetherell. 1980. Probabilistic languages: A re-view and some open questions. Computing Surveys,12:361–379.
1041
