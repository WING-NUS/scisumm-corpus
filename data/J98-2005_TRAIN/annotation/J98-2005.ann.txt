Citance Number: 1 | Reference Article:  J98-2005.txt | Citing Article:  P06-1109.txt | Citation Marker Offset:  24466-24484 | Citation Marker:  Chi and Geman 1998 | Citation Offset:  24373-24464 | Citation Text:  a treebank PCFG whose simple relative frequency estimator corresponds to maximum likelihood (Chi and Geman 1998) | Reference Offset:  ['2810-3150'] | Reference Text:  If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG's assign some mass to infinite trees? We will show that in both cases the estimated probability is tight | Discourse Facet:  Aim_Citation | Annotator:  Ankur Khanna, NUS |


Citance Number: 2 | Reference Article:  J98-2005.txt | Citing Article:  N06-1043.txt | Citation Marker Offset:  13003-13075 | Citation Marker:  Chi and Geman, 1998 | Citation Offset:  12922-12980 | Citation Text:  The line of our argument below follows a proof provided in (Chi and Geman, 1998) for the maximum likelihood estimator based on finite tree distributions | Reference Offset:  ['3084-3150'] | Reference Text:  We will show that in both cases the estimated probability is tight | Discourse Facet:  Aim_Citation | Annotator:  Ankur Khanna, NUS |


Citance Number: 3 | Reference Article:  J98-2005.txt | Citing Article:  N06-1043.txt | Citation Marker Offset:  21080-21138 | Citation Marker:  Chi and Geman, 1998 | Citation Offset:  12982-13001 | Citation Text:  Such maximization provides the estimator (see for instance (Chi and Geman, 1998)) | Reference Offset:  ['8002-8092'] | Reference Text:  The maximum-likelihood estimator is the natural, "relative frequency," estimator | Discourse Facet:  Method_Citation | Annotator:  Ankur Khanna, NUS |


Citance Number: 4 | Reference Article:  J98-2005.txt | Citing Article:  N06-1043.txt | Citation Marker Offset:  22271-22308 | Citation Marker:  Chi and Geman, 1998 | Citation Offset:  21140-21159 | Citation Text:  This result has been firstly shown in (Chi and Geman, 1998) | Reference Offset:  ['2810-3150'] | Reference Text:  If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG's assign some mass to infinite trees? We will show that in both cases the estimated probability is tight | Discourse Facet:  Aim_Citation | Annotator:  Ankur Khanna, NUS |


Citance Number: 5 | Reference Article:  J98-2005.txt | Citing Article:  J01-2004.txt | Citation Marker Offset:  14806-14832 | Citation Marker:  1998 | Citation Offset:  14833-14944 | Citation Text:  Chi and Geman (1998) proved that any PCFG estimated from a treebank with the relative frequency estimator is tight | Reference Offset:  ['3084-3150'] | Reference Text:  We will show that in both cases the estimated probability is tight | Discourse Facet:  Aim_Citation | Annotator:  Ankur Khanna, NUS |


Citance Number: 6 | Reference Article:  J98-2005.txt | Citing Article:  N03-1027.txt | Citation Marker Offset:  10861-10881 | Citation Marker:  1998 | Citation Offset:  10882-11007 | Citation Text:  Chi and Geman (1998) proved that this condition is met if the rule probabilities are estimated using relative frequency estimation from a corpus | Reference Offset:  ['8002-8092'] | Reference Text:  The maximum-likelihood estimator is the natural, "relative frequency" estimator | Discourse Facet:  Method_Citation | Annotator:  Ankur Khanna, NUS |


Citance Number: 7 | Reference Article:  J98-2005.txt | Citing Article:  J98-4004.txt | Citation Marker Offset:  8672-8690 | Citation Marker:  Chi and Geman 1998 | Citation Offset:  8518-8670 | Citation Text: and a consistency or tightness constraint not discussed here, that PCFGs estimated from tree banks using the relative frequency estimator always satisfy (Chi and Geman, 1998) | Reference Offset:  ['3084-3150'] | Reference Text:  We will show that in both cases the estimated probability is tight | Discourse Facet:  Aim_Citation | Annotator:  Ankur Khanna, NUS |


Citance Number: 8 | Reference Article:  J98-2005.txt | Citing Article:  P01-1017.txt | Citation Marker Offset:  15028-15029 | Citation Marker:  5 | Citation Offset:  14846-15026 | Citation Text:  When a PCFG probability distribution is estimated from training data (in our case the Penn tree-bank) PCFGs dene a tight (summing to one) probability distribution over strings [5] | Reference Offset:  ['3084-3150'] | Reference Text:  We will show that in both cases the estimated probability is tight | Discourse Facet:  Aim_Citation | Annotator:  Ankur Khanna, NUS |


Citance Number: 9 | Reference Article:  J98-2005.txt | Citing Article:  N06-1044.txt | Citation Marker Offset:  2963-2982 | Citation Marker:  Chi and Geman, 1998 | Citation Offset:  2985-3052 | Citation Text:  In later work by ... (Chi and Geman, 1998) the result was independently extended to expectation maximization | Reference Offset:  ['3084-3150'] | Reference Text:  We will show that in both cases the estimated probability is tight | Discourse Facet:  Aim_Citation | Annotator:  Ankur Khanna, NUS |


Citance Number: 10 | Reference Article:  J98-2005.txt | Citing Article:  N06-1044.txt | Citation Marker Offset:  3368-3407 | Citation Marker:  Chi and Geman, 1998 | Citation Offset:  3331-3344 | Citation Text:  the proof in  (Chi and Geman, 1998) is based on a simpler counting argument | Reference Offset:  ['8002-8092'] | Reference Text:  The maximum-likelihood estimator is the natural, "relative frequency," estimator | Discourse Facet:  Method_Citation | Annotator:  Ankur Khanna, NUS |


Citance Number: 11 | Reference Article:  J98-2005.txt | Citing Article:  N06-1044.txt | Citation Marker Offset:  3508-3511 | Citation Marker:  Chi and Geman, 1998 | Citation Offset:  3345-3366 | Citation Text:  in  (Chi and Geman, 1998) empty rules and unary rules are not allowed, thus excluding innite ambiguity, that is, the possibility that some string in the input sample has an innite number of derivations in the grammar | Reference Offset:  ['215-264'] | Reference Text:  Estimation of Probabilistic Context-Free Grammars | Discourse Facet:  Aim_Citation | Annotator:  Ankur Khanna, NUS |


Citance Number: 12 | Reference Article:  J98-2005.txt | Citing Article:  N06-1044.txt | Citation Marker Offset:  3512-3531 | Citation Marker:  Chi and Geman, 1998 | Citation Offset:  3533-3727 | Citation Text:  The maximization problem above has a unique solution, provided by the estimator ... (Chi and Geman, 1998) | Reference Offset:  ['10363-10469'] | Reference Text:  It is, then, perhaps best to simplify the treatment by assuming that there are no null or unit productions | Discourse Facet:  Method_Citation | Annotator:  Ankur Khanna, NUS |


Citance Number: 13 | Reference Article:  J98-2005.txt | Citing Article:  N06-1044.txt | Citation Marker Offset:  9737-9756 | Citation Marker:  Chi and Geman, 1998 | Citation Offset:  9636-9717 | Citation Text:  The above maximization problem provides a system of R nonlinear equations ... (Chi and Geman, 1998) | Reference Offset:  ['5850-5934'] | Reference Text:  the EM algorithm can be used to iteratively "climb" the likelihood surface | Discourse Facet:  Method_Citation | Annotator:  Ankur Khanna, NUS |


Citance Number: 14 | Reference Article:  J98-2005.txt | Citing Article:  N06-1044.txt | Citation Marker Offset:  10707-10726 | Citation Marker:  Chi and Geman, 1998 | Citation Offset:  10623-10700 | Citation Text:  This solves a problem that was left open in the literature (Chi and Geman, 1998) | Reference Offset:  ['2810-3150'] | Reference Text:  If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG's assign some mass to infinite trees? We will show that in both cases the estimated probability is tight | Discourse Facet:  Aim_Citation | Annotator:  Ankur Khanna, NUS |


Citance Number: 15 | Reference Article:  J98-2005.txt | Citing Article:  J99-1004.txt | Citation Marker Offset:  4640-4876 | Citation Marker:  1998 | Citation Offset:  4553-4618 | Citation Text:  A sufficient condition for proper assignment is established by  Chi and Geman (1998), who prove that production probabilities estimated by the maximum-likelihood (ML) estimation procedure (or relative frequency estimation procedure, as it is called in computational linguistics) always impose proper PCFG distributions | Reference Offset:  ['3084-3150'] | Reference Text:  We will show that in both cases the estimated probability is tight | Discourse Facet:  Aim_Citation | Annotator:  Ankur Khanna, NUS |


Citance Number: 16 | Reference Article:  J98-2005.txt | Citing Article:  J99-1004.txt | Citation Marker Offset:  13682-13773 | Citation Marker:  1998 | Citation Offset:  4618-4638 | Citation Text:  Chi and Geman (1998) proved the properness of PCFG distributions imposed by estimated production probabilities | Reference Offset:  ['2810-3150'] | Reference Text:  If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG's assign some mass to infinite trees? We will show that in both cases the estimated probability is tight | Discourse Facet:  Aim_Citation | Annotator:  Ankur Khanna, NUS |


Citance Number: 17 | Reference Article:  J98-2005.txt | Citing Article:  J99-1004.txt | Citation Marker Offset:  25299-25338 | Citation Marker:  1998 | Citation Offset:  13661-13681 | Citation Text:  This simple estimator, as shown by Chi and Geman (1998), assigns proper production probabilities for PCFGs | Reference Offset:  ['8553-8747'] | Reference Text:  We will show that if f~ is the set of all (finite) parse trees generated by G, and if f~(ca) is the probability of ca ff fl under the maximum-likelihood production probabilities, then fi(f~) = 1 | Discourse Facet:  Aim_Citation | Annotator:  Ankur Khanna, NUS |


Citance Number: 18 | Reference Article:  J98-2005.txt | Citing Article:  J99-1004.txt | Citation Marker Offset:  255113-255117 | Citation Marker:  1998 | Citation Offset:  28354-28424 | Citation Text:  The proof is almost identical to the one given by Chi and Geman (1998) | Reference Offset:  ['196-264'] | Reference Text:  Squibs and Discussions Estimation of Probabilistic Context-Free Grammars | Discourse Facet:  Aim_Citation | Annotator:  Ankur Khanna, NUS |


Citance Number: 19 | Reference Article:  J98-2005.txt | Citing Article:  J99-1004.txt | Citation Marker Offset:  26356-26374 | Citation Marker:  Chi and Geman, 1998 | Citation Offset:  26309-26375 | Citation Text:  impose proper probability distributions on f~ (Chi and Geman, 1998) | Reference Offset:  ['866-952'] | Reference Text:  We show here that estimated production probabilities always yield proper distributions | Discourse Facet:  Results_Citation | Annotator:  Ankur Khanna, NUS |

Citance Number: 20 | Reference Article:  J98-2005.txt | Citing Article:  P13-1102.txt | Citation Marker Offset:  4202-4295 | Citation Marker:  1998 | Citation Offset:  4171-4180 | Citation Text:  we follow Chi and Geman (1998) in calling them non-tight to avoid confusion with the consistency of statistical estimators | Reference Offset:  ['215-264'] | Reference Text:  Estimation of Probabilistic Context-Free Grammars | Discourse Facet:  Aim_Citation | Annotator:  Ankur Khanna, NUS |


Citance Number: 21 | Reference Article:  J98-2005.txt | Citing Article:  P13-1102.txt | Citation Marker Offset:  4698-4783 | Citation Marker:  1998 | Citation Offset:  4181-4201 | Citation Text:  Chi and Geman (1998) studied the question for Maximum Likelihood (ML) estimation, and showed that ML estimates are always tight for both the supervised case (where the input consists of parse trees) and the unsupervised case (where the input consists of yields or terminal strings) | Reference Offset:  ['2810-3150'] | Reference Text:  If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG's assign some mass to infinite trees? We will show that in both cases the estimated probability is tight | Discourse Facet:  Aim_Citation | Annotator:  Ankur Khanna, NUS |


