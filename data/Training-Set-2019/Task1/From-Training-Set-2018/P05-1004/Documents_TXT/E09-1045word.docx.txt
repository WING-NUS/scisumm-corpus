An Empirical Study on Class-based Word Sense Disambiguation∗



Rube´n Izquierdo & Armando Sua´ rez 
Deparment of Software and Computing  Systems 
University of Alicante.  Spain
{ruben,armando}@dlsi.ua.es


German Rigau IXA NLP 
Group. EHU. Donostia, Spain
german.rigau@ehu.es






Abstract

As empirically demonstrated by the last 
SensEval exercises, assigning  the appro- 
priate meaning to words in context has re- 
sisted all attempts to be successfully ad- 
dressed. One possible reason could be the 
use of inappropriate  set of meanings.  In 
fact, WordNet  has been used as a de-facto 
standard repository  of meanings. How- 
ever, to our knowledge, the meanings rep- 
resented by WordNet have been only used 
for WSD at a very fine-grained sense level 
or at a very coarse-grained class level. We 
suspect that selecting the appropriate level 
of abstraction could be on between both 
levels. We use a very simple method for 
deriving  a small set of appropriate mean- 
ings using basic structural  properties of 
WordNet.  We also empirically demon- 
strate that this automatically  derived set of 
meanings groups senses into an adequate 
level of abstraction in order to perform 
class-based Word Sense Disambiguation, 
allowing accuracy figures over 80%.

1   Introduction

Word Sense Disambiguation  (WSD) is an inter- 
mediate Natural Language Processing (NLP) task 
which consists in assigning the correct semantic 
interpretation to ambiguous words in context. One 
of the most successful approaches in the last years 
is the supervised learning from examples, in which 
statistical or Machine Learning classification mod- 
els are induced from semantically  annotated cor- 
pora (Ma`rquez et al., 2006).  Generally,  super- 
vised systems have obtained  better  results  than 
the unsupervised  ones, as shown by experimental 
work and international  evaluation  exercises such
   ∗This paper has been supported  by the European Union 
under the projects QALL-ME (FP6 IST-033860) and KY- 
OTO (FP7 ICT-211423),  and the Spanish Government under 
the project Text-Mess (TIN2006-15265-C06-01) and KNOW 
(TIN2006-15049-C03-01)


as Senseval1.   These annotated  corpora  are usu- 
ally manually tagged by lexicographers with word 
senses taken from a particular lexical semantic re- 
source –most commonly  WordNet2 (WN) (Fell- 
baum, 1998).
  WN has been widely criticized for being a sense 
repository  that often provides  too fine–grained 
sense distinctions   for higher level applications 
like Machine Translation or Question & Answer- 
ing.   In fact, WSD at this level of granularity 
has resisted all attempts of inferring robust broad- 
coverage models. It seems that many word–sense 
distinctions are too subtle to be captured by auto- 
matic systems with the current small volumes of 
word–sense annotated examples.  Possibly, build- 
ing class-based classifiers  would allow to avoid 
the data sparseness problem  of the word-based ap- 
proach. Recently, using WN as a sense reposi- 
tory, the organizers of the English all-words task 
at SensEval-3 reported an inter-annotation  agree- 
ment of 72.5% (Snyder and Palmer, 2004). In- 
terestingly, this result is difficult to outperform by 
state-of-the-art sense-based WSD  systems.
  Thus, some research has been focused on deriv- 
ing different word-sense groupings  to overcome 
the fine–grained distinctions of WN (Hearst and 
Schu¨ tze, 1993), (Peters et al., 1998), (Mihalcea 
and Moldovan, 2001), (Agirre and LopezDeLa- 
Calle, 2003), (Navigli, 2006) and (Snow et al.,
2007). That is, they provide methods for grouping 
senses of the same word, thus producing  coarser 
word  sense groupings for better disambiguation.
  Wikipedia3 has been also recently used to over- 
come some problems of automatic learning meth- 
ods: excessively fine–grained definition  of mean- 
ings, lack of annotated data and strong domain de- 
pendence of existing  annotated corpora. In this 
way, Wikipedia  provides a new very large source 
of annotated data, constantly expanded (Mihalcea,
2007).

1 http://www.senseval.org
2 http://wordnet.princeton.edu
3 http://www.wikipedia.org



  Proceedings of the 12th Conference of the European Chapter of the ACL, pages 389–397, 
Athens, Greece, 30 March – 3 April 2009. Qc 2009 Association for Computational Linguistics


  In contrast, some research have been focused on 
using predefined sets of sense-groupings for learn- 
ing class-based classifiers for WSD (Segond et al.,
1997), (Ciaramita  and Johnson, 2003), (Villarejo 
et al., 2005), (Curran, 2005) and (Ciaramita  and 
Altun, 2006). That is, grouping senses of different 
words into the same explicit and comprehensive 
semantic class.
  Most of the later approaches  used the origi- 
nal Lexicographical Files of WN (more recently 
called SuperSenses) as very coarse–grained sense 
distinctions.  However, not so much attention has 
been paid on learning class-based classifiers from 
other available  sense–groupings such as WordNet 
Domains (Magnini and Cavaglia`,  2000), SUMO 
labels (Niles and Pease,  2001), EuroWordNet 
Base Concepts (Vossen  et al., 1998), Top Con- 
cept Ontology labels (Alvez  et al., 2008) or Ba- 
sic Level Concepts (Izquierdo et al., 2007). Obvi- 
ously,  these resources relate senses at some level 
of abstraction using different semantic criteria and 
properties that could be of interest for WSD. Pos- 
sibly, their combination could improve the overall 
results since they offer different semantic perspec- 
tives of the data. Furthermore, to our knowledge, 
to date no comparative  evaluation  has been per- 
formed on SensEval data exploring different levels 
of abstraction. In fact, (Villarejo et al., 2005) stud- 
ied the performance of class–based WSD com- 
paring only SuperSenses and SUMO by 10–fold 
cross–validation on SemCor, but they did not pro- 
vide results for SensEval2 nor SensEval3.
  This paper empirically  explores on the super- 
vised WSD task the performance  of  different 
levels of abstraction provided  by WordNet Do- 
mains (Magnini and Cavaglia`, 2000), SUMO la- 
bels (Niles  and Pease, 2001) and Basic Level  Con- 
cepts (Izquierdo et al., 2007). We refer to this ap- 
proach  as class–based WSD since the classifiers 
are created at a class  level instead of at a sense 
level. Class-based WSD  clusters senses of differ- 
ent words into the same explicit and comprehen- 
sive grouping.  Only those cases belonging  to the 
same semantic class are grouped to train the clas- 
sifier. For example, the coarser word grouping ob- 
tained in (Snow et al., 2007) only has one remain- 
ing sense for “church”. Using  a set of Base Level 
Concepts (Izquierdo et al., 2007), the three senses 
of “church” are still represented by faith.n#3, 
building.n#1 and religious ceremony.n#1.
The contribution of this work is threefold.  We


empirically  demonstrate that a) Basic Level Con- 
cepts group  senses into an adequate level of ab- 
straction in order to perform supervised  class– 
based WSD, b) that these semantic  classes can 
be successfully  used as semantic features to boost 
the performance of these classifiers  and c) that 
the class-based approach to WSD reduces dramat- 
ically the required amount of training examples to 
obtain competitive classifiers.
  After this introduction,  section 2 presents the 
sense-groupings used in this study. In section 3 the 
approach followed  to build the class–based system 
is explained.  Experiments  and results are shown in 
section 4. Finally some conclusions are drawn in 
section 5.

2   Semantic Classes

WordNet  (Fellbaum, 1998) synsets are organized 
in forty five  Lexicographer  Files, more recetly 
called SuperSenses, based on open syntactic cat- 
egories (nouns, verbs, adjectives and adverbs) and 
logical groupings,  such as person, phenomenon, 
feeling, location, etc.  There  are 26 basic cate- 
gories for nouns, 15 for verbs, 3 for adjectives and
1 for adverbs.
WordNet Domains4   (Magnini and Cavaglia`,
2000) is a hierarchy  of 165 Domain Labels which 
have been used to label all WN synsets. Informa- 
tion brought by Domain Labels is complementary 
to what is already in WN. First of all a Domain La- 
bels may include synsets of different syntactic cat- 
egories: for instance MEDICINE groups together 
senses from nouns, such as doctor and hospital, 
and from Verbs such as to operate.  Second, a Do- 
main  Label  may also contain  senses from differ- 
ent WordNet subhierarchies. For example, SPORT 
contains  senses such as athlete, deriving from life 
form, game equipment, from physical object, sport 
from act, and playing field, from location.
SUMO5 (Niles  and Pease, 2001)  was created as
part of the IEEE Standard Upper Ontology Work- 
ing Group. The goal of this Working Group is 
to develop a standard upper ontology  to promote 
data interoperability,   information  search and re- 
trieval, automated inference, and natural language 
processing.  SUMO   consists of a set of concepts, 
relations, and axioms that formalize  an upper on- 
tology. For these experiments,  we used the com- 
plete WN1.6 mapping with 1,019 SUMO  labels.

4 http://wndomains.itc.it/
5 http://www.ontologyportal.org/


Basic Level Concepts6 (BLC) (Izquierdo et al.,
2007) are small sets of meanings representing the 
whole nominal and verbal part of WN. BLC can 
be obtained by a very simple  method that uses ba- 
sic structural WN properties. In fact, the algorithm 
only considers the relative number of relations of 
each synset along the hypernymy  chain. The pro- 
cess follows  a bottom-up approach using the chain 
of hypernymy relations.  For each synset in WN, 
the process selects as its BLC the first local maxi- 
mum according to the relative number of relations. 
The local maximum  is the synset in the hypernymy 
chain having more relations than its immediate 
hyponym and immediate hypernym. For synsets 
having multiple hypernyms, the path having the 
local maximum with higher number of relations 
is selected. Usually,  this process finishes having 
a number  of preliminary BLC. Obviously, while 
ascending through  this chain, more synsets  are 
subsumed by each concept.  The process finishes 
checking if the number of concepts subsumed by 
the preliminary list of BLC is higher  than a cer- 
tain threshold. For those BLC not representing 
enough concepts according  to the threshold, the 
process selects the next local maximum  following 
the hypernymy hierarchy. Thus, depending on the 
type of relations considered to be counted and the 
threshold established, different sets of BLC can be 
easily obtained for each WN version.
  In this paper, we empirically  explore the perfor- 
mance of the different levels of abstraction pro- 
vided by Basic Level Concepts (BLC) (Izquierdo 
et al., 2007).
  Table 1 presents the total number of BLC and 
its average depth for WN1.6, varying the threshold 
and the type of relations considered (all relations 
or only hyponymy).


Thr
es.
Rel.
PoS
#B
LC
Av. 
dep
th.

0
all
Nou
n
Ver
b
3,0
94
1,2
56
7
.
0
9
3
.
3
2

hyp
o
Nou
n
Ver
b
2,4
90
1,0
41
7
.
0
9
3
.
3
1

2
0
all
Nou
n
Ver
b
5
5
8
6
7
3
5
.
8
1
1
.
2
5

hyp
o
Nou
n
Ver
b
5
5
8
6
7
2
5
.
8
0
1
.
2
1

5
0
all
Nou
n
Ver
b
2
5
3
6
3
3
5
.
2
1
1
.
1
3

hyp
o
Nou
n
Ver
b
2
4
8
6
3
3
5
.
2
1
1
.
1
0

Table 1: BLC for WN1.6 using all or hyponym relations



6 http://adimen.si.ehu.es/web/BLC



C
l
a
s
s
i
f
i
e
r
Ex
a
m
pl
es
#
 
o
f
 
e
x
a
m
p
l
e
s
ch
ur
ch
.n
#
2  
(s
e
ns
e 
a
p
pr
o
a
c
h)
ch
urc
h.n
#2
5
8


buil
ding
, 
edifi
ce 
(cla
ss 
app
roa
ch)
ch
ur
ch
.n
#2 
bu
ildi
ng
.n
#1 
ho
tel
.n
#1 
ho
spi
tal
.n
#1 
ba
rn.
n#
1
.
.
.
.
.
.
.
5
8
4
8
3
9
2
0
1
7
.
.
.
.
.
.
TO
TA
L= 
371 
exa
mp
les

Table 2:  Examples and number of them in Semcor,  
for sense approach and for class approach


3   Class-based 
WSD

We followed a supervised machine  learning  
ap- proach to develop  a set of class-based 
WSD tag- gers. Our systems use an 
implementation of a Sup- port Vector 
Machine algorithm to train the clas- sifiers 
(one per class) on semantic annotated cor- 
pora for acquiring positive and negative 
examples of each class and on the definition  of 
a set of fea- tures for representing these 
examples. The system decides and selects 
among the possible semantic classes defined 
for a word. In the sense approach, one 
classifier is generated for each word sense, and 
the classifiers choose between the possible 
senses for the word. The examples to train a 
single clas- sifier for a concrete  word are all 
the examples of this word sense.  In the 
semantic–class approach, one classifier is 
generated for each semantic class. So, when 
we want to label  a word, our program obtains 
the set of possible  semantic  classes for this 
word, and then launch each of the semantic 
classifiers related with these semantic 
categories. The most likely category is 
selected for the word. In this approach, 
contrary to the word sense ap- proach, to train 
a classifier  we can use all examples of all words 
belonging to the class represented by the 
classifier. In table 2 an example for a sense 
of “church” is shown. We think that this 
approach has several advantages. First,  
semantic classes re- duce the average polysemy 
degree of words (some word  senses are 
grouped  together  within the same class). 
Moreover,  the well known problem of ac- 
quisition bottleneck in supervised machine 
learn- ing algorithms is attenuated, because 
the number of examples for each classifier is 
increased.

3.1   The learning algorithm: 
SVM
Support Vector Machines (SVM)  have   
been proven to be robust and very competitive 
in many NLP tasks, and in WSD in particular 
(Ma`rquez et al., 2006).  For these experiments, 
we used SVM- Light (Joachims, 1998). 
SVM are used to learn an hyperplane  that 
separates the positive  from the


negative examples with the maximum margin. It 
means that the hyperplane is located in an interme- 
diate position  between positive  and negative ex- 
amples, trying to keep the maximum distance to 
the closest positive  example,  and to the closest 
negative example.  In some cases, it is not possi- 
ble to get a hyperplane that divides the space lin- 
early, or it is better to allow some errors to obtain a 
more efficient hyperplane. This is known as “soft- 
margin SVM”, and requires the estimation of a pa- 
rameter (C), that represent the trade-off allowed 
between training  errors and the margin. We have 
set this value to 0.01, which  has been proved as a 
good value for SVM in WSD tasks.
  When classifying an example,  we obtain the 
value of the output function for each SVM clas- 
sifier corresponding to each semantic class for the 
word example.  Our system simply  selects the class 
with the greater value.


3.2   Corpora

Three semantic annotated corpora have been used 
for training and testing. SemCor  has been used 
for training while the corpora from the English 
all-words tasks of  SensEval-2  and SensEval-3 
has  been used  for  testing.   We  also consid- 
ered SemEval-2007  coarse–grained  task corpus 
for testing, but this dataset was discarded because 
this corpus is also annotated with clusters of word 
senses.
  SemCor (Miller et al., 1993) is a subset of the 
Brown Corpus plus the novel The Red Badge of 
Courage,  and it has been developed  by the same 
group that created WordNet.  It contains 253 texts 
and around 700,000 running words, and more than
200,000 are also lemmatized  and sense-tagged ac- 
cording to Princeton WordNet 1.6.
  SensEval-27   English all-words corpus  (here- 
inafter SE2) (Palmer et al., 2001) consists on 5,000 
words of text from three WSJ articles represent- 
ing different domains from the Penn TreeBank II. 
The sense inventory  used for tagging is WordNet
1.7. Finally, SensEval-38 English all-words cor-
pus (hereinafter SE3) (Snyder and Palmer, 2004), 
is made up of 5,000 words, extracted from two 
WSJ articles and one excerpt from the Brown Cor- 
pus. Sense repository  of WordNet  1.7.1 was used 
to tag 2,041 words with their proper senses.

7 http://www.sle.sharp.co.uk/senseval2
8 http://www.senseval.org/senseval3


3.3   Feature types
We have defined a set of features to represent the 
examples according  to previous works in WSD 
and the nature of  class-based  WSD. Features 
widely used  in the literature as  in (Yarowsky,
1994) have  been  selected. These  features  are 
pieces of information that occur in the context of 
the target word, and can be organized as:
  Local features:    bigrams and trigrams that 
contain the target word, including part-of-speech 
(PoS), lemmas or word-forms.
  Topical features:  word–forms  or lemmas ap- 
pearing in windows around the target word.
  In particular, our systems use the following ba- 
sic features:
  Word–forms  and lemmas in a window of 10 
words around the target word
  PoS:    the   concatenation of   the   preced- 
ing/following  three/five PoS
  Bigrams and trigrams formed by lemmas and 
word-forms and obtained in a window  of 5 words. 
We use of all tokens regardless their PoS to build 
bi/trigrams.  The target  word is replaced by X 
in these features to increase the generalization  of 
them for the semantic classifiers
  Moreover, we also defined  a set of Semantic 
Features to explode different  semantic resources 
in order to enrich the set of basic features:
  Most frequent semantic class calculated  over 
SemCor, the most frequent semantic class for the 
target word.
  Monosemous   semantic classes semantic 
classes  of the monosemous  words arround  the 
target word in a window  of size 5. Several types 
of semantic classes have been considered to create 
these features. In particular,  two different sets 
of  BLC  (BLC20 and BLC509), SuperSenses, 
WordNet Domains (WND) and SUMO.
  In order to increase the generalization capabil- 
ities of the classifiers we filter out irrelevant fea- 
tures. We measure the relevance of a feature10.   f 
for a class c in terms of the frequency of f. For each 
class c, and for each feature f of that class, we cal- 
culate the frequency of the feature within the class 
(the number of times that it occurs in examples

  9 We have selected these set since they represent different 
levels of abstraction.  Remember that 20 and 50 refer to the 
threshold of minimum number of synsets that a possible BLC 
must subsume to be considered  as a proper BLC. These BLC 
sets were built using all kind of relations.
    10 That is, the value of the feature, for example  a feature 
type can be word-form,   and a feature  of that type can be 
“houses”


of the class), and also obtain the total frequency 
of the feature, for all the classes. We divide both 
values (classFreq / totalFreq)  and if the result is 
not greater than a certain threshold t, the feature 
is removed from the feature list of the class c11. 
In this way, we ensure that the features selected 
for a class are more  frequently  related  with that 
class than with others.  We set this threshold t to
0.25, obtained empirically  with very preliminary 
versions of the classifiers on SensEval3 test.

4   Experiments and Results

To analyze the influence of each feature type in the 
class-based WSD,  we designed a large set of ex- 
periments. An experiment is defined by two sets of 
semantic classes. First, the semantic class type for 
selecting the examples used to build the classifiers 
(determining the abstraction level of the system). 
In this case, we tested: sense12, BLC20,  BLC50, 
WordNet Domains (WND), SUMO and Super- 
Sense (SS). Second, the semantic class type used 
for building the semantic features. In this case, we 
tested: BLC20, BLC50, SuperSense, WND and 
SUMO.  Combining  them, we generated the set of 
experiments described later.






Table 3: Average polysemy on SE2 and SE3

  Table 3 presents the average polysemy  on SE2 
and SE3 of the different semantic classes.

4.1   Baselines

The most frequent  classes (MFC) of each word 
calculated over SemCor are considered to be the 
baselines of our systems. Ties between classes on 
a specific word are solved obtaining  the global fre- 
quency in SemCor of each of these tied classes, 
and selecting the more frequent  class over the 
whole training corpus. When there are no occur- 
rences of a word of the test corpus in SemCor (we 
are not able to calculate the most frequent class of 
the word), we obtain again the global frequency 
for each of its possible semantic classes (obtained

  11 Depending on the experiment, around 30% of the origi- 
nal features are removed by this filter.
12 We included this evaluation for comparison  purposes


from WN) over SemCor, and we select the most 
frequent.

4.2   Results

Tables 4 and 5 present the F1 measures (harmonic 
mean of recall and precision) for nouns and verbs 
respectively when training our systems on Sem- 
Cor and testing on SE2 and SE3. Those results 
showing  a statistically  significant13   positive  dif- 
ference when compared with the baseline are in 
marked bold. Column labeled as “Class”  refers to 
the target set of semantic classes for the classifiers, 
that is, the desired semantic level for each exam- 
ple. Column  labeled as “Sem. Feat.”  indicates 
the class of the semantic features used to train the 
classifiers.  For example, class BLC20 combined 
with Semantic Feature BLC20  means that this set 
of classes were used both to label the test exam- 
ples and to define the semantic features. In order 
to compare their contribution we also performed 
a “basicFeat” test without including  semantic fea- 
tures.
  As expected  according  to most literature in 
WSD, the performances of the MFC baselines are 
very high.  In particular,  those corresponding to 
nouns (ranging from 70% to 80%). While nom- 
inal baselines seem to perform similarly in both 
SE2 and SE3, verbal baselines appear to be con- 
sistently much lower for SE2 than for SE3. In 
SE2, verbal baselines range from 44% to 68% 
while in SE3 verbal baselines range from 52% to
79%. An exception is the results for verbs con- 
sidering WND: the results are very high due to 
the low polysemy for verbs according to WND. 
As expected, when increasing the level of abstrac- 
tion (from senses to SuperSenses) the results also 
increase.  Finally, it also seems that SE2 task is 
more difficult than SE3 since the MFC baselines 
are lower.
  As expected, the results of the systems increase 
while augmenting the level of abstraction (from 
senses to SuperSenses), and almost in every case, 
the baseline results are reached or outperformed. 
This is very relevant since the baseline results are 
very high.
  Regarding  nouns, a very different behaviour is 
observed for SE2 and SE3. While for SE3 none 
of the system presents a significant  improvement 
over the baselines, for SE2 a significant  improve- 
ment is obtained by using several types of seman-


since the current system have been designed for class-based	 	


evaluation only.


13 Using the 
McNemar’s test.



tic features. In particular,  when using WordNet 
Domains but also BLC20. In general, BLC20  se- 
mantic features seem to be better than BLC50 and 
SuperSenses.
  Regarding verbs, the system obtains significant 
improvements over the baselines using different 
types of semantic features both in SE2 and SE3. 
In particular, when using again WordNet Domains 
as semantic features.
  In general, the results obtained by BLC20 are 
not so  much different to the results of BLC50 
(in a  few cases,  this difference is greater than
2 points).  For instance,  for nouns,  if  we con- 
sider the number of classes within BLC20 (558 
classes), BLC50  (253 classes) and SuperSense (24 
classes), BLC classifiers obtain high performance 
rates while maintaining much higher expressive 
power than SuperSenses.   In fact, using Super- 
Senses (40 classes for nouns and verbs) we can 
obtain  a very accurate semantic tagger with per- 
formances close to 80%. Even better, we can use 
BLC20 for tagging nouns (558 semantic  classes 
and F1 over 75%) and SuperSenses for verbs (14 
semantic classes and F1 around 75%).
  Obviously, the classifiers using WordNet  Do- 
mains  as target  grouping  obtain very high per- 
formances  due to its reduced average polysemy. 
However,  when used as semantic features it seems 
to improve the results in most of the cases.
  In addition, we obtain very competitive classi- 
fiers at a sense level.

4.3   Learning curves

We also performed  a set of experiments for mea- 
suring the behaviour of the class-based WSD  sys- 
tem when gradually increasing  the number of 
training  examples.  These experiments have been 
carried for nouns and verbs, but only noun results 
are shown since in both cases, the trend is very 
similar but more clear for nouns.
  The training  corpus has been divided  in portions 
of 5% of the total number of files. That is, com- 
plete files are added to the training corpus of each 
incremental test. The files were randomly selected 
to generate portions of 5%, 10%, 15%, etc. of the 
SemCor corpus14.  Then, we train the system on 
each of the training portions and we test the sys- 
tem on SE2 and SE3. Finally,  we also compare the

  14 Each portion  contains also the same files than the previ- 
ous portion. For example, all files in the 25% portion are also 
contained in the 30% portion.



Cl
as
s
Sem
. 
Fea
t.
SensE
val2
SensE
val3


Pol
y
A
l
l
Pol
y
A
l
l


Sen
se
b
a
s
e
li
n
e
 
b
a
s
i
c
F
e
a
t 
B
L
C
2
0
B
L
C
5
0
S
S
 
W
N
D
 
S
U
M
O
59.
66
61.
13
61.
93
61.
79
61.
00
61.
13
61.
66
70.
02
71.
20
71.
79
71.
69
71.
10
71.
20
71.
59
64.
45
65.
45
65.
45
65.
30
64.
86
65.
45
65.
45
72.
30
73.
15
73.
15
73.
04
72.
70
73.
15
73.
15


BL
C20
b
a
s
e
li
n
e
 
b
a
s
i
c
F
e
a
t 
B
L
C
2
0
B
L
C
5
0
S
S
 
W
N
D
 
S
U
M
O
65.
92
65.
65
68.
70
68.
83
65.
12
68.
97
68.
57
75.
71
75.
52
77.
69
77.
79
75.
14
77.
88
77.
60
67.
98
64.
64
68.
29
67.
22
64.
64
65.
25
64.
49
76.
29
73.
82
76.
52
75.
73
73.
82
74.
24
73.
71


BL
C50
b
a
s
e
li
n
e
 
b
a
s
i
c
F
e
a
t 
B
L
C
2
0
B
L
C
5
0
S
S
 
W
N
D
 
S
U
M
O
67.
20
64.
28
69.
72
67.
20
65.
60
70.
39
71.
31
76.
65
74.
57
78.
45
76.
65
75.
52
78.
92
79.
58
68.
01
66.
77
68.
16
68.
01
65.
07
65.
38
66.
31
76.
74
75.
84
76.
85
76.
74
74.
61
74.
83
75.
51


W
ND
b
a
s
e
li
n
e
 
b
a
s
i
c
F
e
a
t 
B
L
C
2
0
B
L
C
5
0
S
S
 
W
N
D
 
S
U
M
O
78.
97
70.
96
72.
53
73.
25
74.
39
78.
83
75.
11
86.
11
80.
81
81.
85
82.
33
83.
08
86.
01
83.
55
76.
74
67.
85
72.
37
71.
41
68.
82
76.
58
73.
02
83.
8
77.
64
80.
79
80.
11
78.
31
83.
71
81.
24


SU
MO
b
a
s
e
li
n
e
 
b
a
s
i
c
F
e
a
t 
B
L
C
2
0
B
L
C
5
0
S
S
 
W
N
D
 
S
U
M
O
66.
40
68.
53
65.
60
65.
60
68.
39
68.
92
68.
92
76.
09
77.
60
75.
52
75.
52
77.
50
77.
88
77.
88
71.
96
68.
10
68.
10
68.
72
68.
41
69.
03
70.
88
79.
55
76.
74
76.
74
77.
19
76.
97
77.
42
78.
76


S
S
b
a
s
e
li
n
e
 
b
a
s
i
c
F
e
a
t 
B
L
C
2
0
B
L
C
5
0
S
S
 
W
N
D
 
S
U
M
O
70.
48
69.
77
71.
47
70.
20
70.
34
73.
59
70.
62
80.
41
79.
94
81.
07
80.
22
80.
32
82.
47
80.
51
72.
59
69.
60
72.
43
72.
92
65.
12
70.
10
71.
93
81.
50
79.
48
81.
39
81.
73
76.
46
79.
82
81.
05

Table 4: Results for nouns


resulting system with the baseline computed over 
the same training  portion.
  Figures 1 and 2 present the learning curves over 
SE2 and SE3, respectively, of a class-based WSD 
system based on BLC20 using the basic features 
and the semantic features built with WordNet Do- 
mains.
  Surprisingly, in SE2 the system only improves 
the F1 measure around 2% while increasing the 
training corpus from 25% to 100% of SemCor. 
In SE3, the system again only improves the F1 
measure around 3% while increasing the training 
corpus from 30% to 100% of SemCor. That is, 
most of the knowledge required for the class-based 
WSD system  seems to be already  present on a 
small part of SemCor.
  Figures 3 and 4 present the learning curves over 
SE2 and SE3, respectively, of a class-based WSD 
system based on SuperSenses using the basic fea- 
tures and the semantic features built with WordNet 
Domains.
Again, in SE2 the system only improves the F1



80
System SV2
MFC SV2
78

76

74

72
F1
70

68

66

64

62
5     10    15    20    25    30    35    40    45    50    55    60    65    70    75    80    85    90    95   100
% corpus

Figure 1: Learning curve of BLC20 on SE2

78
System SV3
MFC SV3   	
76

74

72

F1    70

68

66

64






Table 5: Results for verbs


measure around 2% while increasing the training 
corpus from 25% to 100% of SemCor. In SE3, 
the system again only improves the F1 measure 
around 2% while increasing the training corpus 
from 30% to 100% of SemCor. That is, with only
25% of the whole corpus, the class-based WSD 
system reaches a F1 close to the performance us- 
ing all corpus.  This evaluation  seems to indicate 
that the class-based approach to WSD reduces dra- 
matically the required amount of training exam- 
ples.
  In both cases,  when using BLC20 or Super- 
Senses as semantic   classes for tagging, the be- 
haviour of the system is similar to MFC baseline. 
This is very interesting since the MFC obtains high 
results due to the way it is defined, since the MFC 
over the total corpus is assigned if there are no oc- 
currences of the word in the training corpus. With- 
out this definition,  there would be a large number 
of words in the test set with no occurrences when 
using small training portions. In these cases, the 
recall of the baselines (and in turn F1) would be


62
5     10    15    20    25    30    35    40    45    50    55    60    65    70    75    80    85    90    95   
100
% 
corp
us

Figure 2: Learning curve of BLC20 on SE3


much 
lower.

5   Conclusions and 
discussion

We explored on the WSD task the 
performance of different levels of abstraction  
and sense group- ings.   We  empirically 
demonstrated  that Base Level Concepts are 
able to group word  senses into an adequate 
medium  level of abstraction to per- form 
supervised class–based disambiguation.   We 
also demonstrated that the semantic  classes 
pro- vide a rich information  about 
polysemous words and can be successfully   
used  as semantic  fea- tures. Finally we 
confirm the fact that the class– based approach 
reduces dramatically the required amount of 
training examples, opening the way to solve 
the well known acquisition bottleneck prob- 
lem for supervised machine learning 
algorithms.
  In general, the results obtained by BLC20 
are not very different to the results of BLC50. 
Thus, we can select a medium level of 
abstraction, with- out having a significant   
decrease of the perfor-



84
System SV2


 		features, we can generate robust and 
competitive



82

80

78

F1    76

74

72

70

68


MFC SV2   	


c
l
a
s
s
-
b
a
s
e
d
 
c
l
a
s
s
i
f
i
e
r
s
.
  T
o 
our 
kno
wle
dge, 
the 
best 
resul
ts  
for 
clas
s– 
bas
ed 
WS
D  
are 
thos
e 
repo
rted 
by 
(Cia
rami
ta 
and 
Altu
n, 
200
6). 
Thi
s 
syst
em  
perf
orm
s  a 
sequ
ence 
tagg
ing  
usin
g a 
perc
eptr
on–
train
ed  
HM
M, 
usin
g 
Sup
erSe
nses
, 
train
ing  
on 
Sem
Cor 
and 
testi
ng 
on 
Sen
sEv
al3.  
The 
syst
em 
achi
eves 
an 
F1–
scor
e of
70.5
4, 
obta
inin
g  a 
signi
fica
nt  
impr
ove
men
t  
fro
m a 
base
line  
syst
em  
whi
ch 
scor
es 
only 
64.
09. 
In 
this 
case
, the 
first 
sens
e 
bas
eline 
is 
the 
Sup
erSe
nse


5     10    15    20    25    30    35    40    45    50    55    60    65    70    75    80    85    90    95   100   of the most frequent synset for a word, according
% corpus



Figure 3: Learning curve of SuperSense on SE2


82
System SV3


to the WN sense ranking. Although this 
result is
achieved for the all words SensEval3 
task, includ- ing adjectives, we can 
compare both results since
 		in SE2 and SE3 adjectives obtain very 
high per-



80


78


F1    76


74


72


70


MFC SV3   	


form
anc
e 
figu
res. 
Usin
g  
Sup
erSe
nses
, 
adje
ctive
s 
only 
have 
thre
e 
clas
ses 
(W
N 
Lexi
cogr
aphi
c 
File
s
00, 
01 
and 
44) 
and 
mor
e 
than 
80% 
of 
the
m 
belo
ng 
to 
clas
s 
00. 
This 
yiel
ds 
to 
reall
y 
very 
high 
perf
or- 
man
ces 
for 
adje
ctive
s 
whic
h 
usu
ally 
are 
over 
90
%.
  A
s we 
have 
see
n, 
sup
ervis
ed 
WS
D 
syst
ems 
are 
ver
y 
dep
end
ent 
of 
the 
corp
ora  
use
d to 
trai
n 
and 
test 
the 
syst
em. 
We 
plan 
to 
exte
nd 
our 
syst
em 
by 
sele
ctin
g 
new 
corp
ora 
to 
train 
or 
test. 
For 
inst
anc
e, 
by 
usin
g 
the 
sens
e 
ann
otat
ed 
glos
ses 
fro
m 
Wor
d-


5     10    15    20    25    30    35    40    45    50    55    60    65    70    75    80    85    90    95   100   Net.
% corpus



Figure 4: Learning curve of SuperSense on SE3


mance. Considering the number of classes, BLC 
classifiers  obtain high performance  rates while 
maintaining much higher expressive power than 
SuperSenses.  However,  using SuperSenses (46 
classes) we can obtain  a very accurate semantic 
tagger with performances around 80%. Even bet- 
ter, we can use BLC20  for tagging nouns (558 se- 
mantic  classes and F1 over 75%) and SuperSenses 
for verbs (14 semantic  classes and F1 around
75%).
  As BLC are defined by a simple  and fully au- 
tomatic  method, they can provide  a user–defined 
level of abstraction that can be more suitable for 
certain NLP tasks.
  Moreover, the traditional set of features used for 
sense-based classifiers  do not seem to be the most 
adequate or representative for the class-based ap- 
proach. We have enriched the usual  set of fea- 
tures, by adding semantic information  from the 
monosemous words of the context and the MFC 
of the target word. With this new enriched set of



References

E. Agirre and O. LopezDeLaCalle.  2003. Clustering 
wordnet word senses. In Proceedings of RANLP’03, 
Borovets, Bulgaria.

J. Alvez,  J. Atserias, J. Carrera, S. Climent, E. Laparra, 
A. Oliver, and G. Rigau G.  2008. Complete and 
consistent annotation of wordnet using the top con- 
cept ontology. In 6th International  Conference on 
Language  Resources and Evaluation  LREC, Mar- 
rakesh, Morroco.

M. Ciaramita and Y. Altun.  2006.  Broad-coverage 
sense disambiguation   and information extraction 
with a supersense sequence tagger.  In Proceedings 
of the Conference on Empirical  Methods in Natural 
Language Processing (EMNLP’06), pages 594–602, 
Sydney, Australia. ACL.

M. Ciaramita and M. Johnson.  2003. Supersense tag- 
ging of unknown nouns in wordnet. In Proceedings 
of the Conference on Empirical  methods in natural 
language processing (EMNLP’03),  pages 168–175. 
ACL.

J.  Curran.  2005.  Supersense tagging of unknown 
nouns using semantic similarity. In Proceedings of 
the 43rd Annual Meeting on Association for Compu- 
tational Linguistics (ACL’05), pages 26–33. ACL.


C. Fellbaum, editor.  1998. WordNet. An Electronic
Lexical Database. The MIT Press.

M. Hearst and H. Schu¨ tze. 1993. Customizing  a lexi- 
con to better suit a computational  task. In Proceed- 
ingns of the ACL SIGLEX Workshop on Lexical Ac- 
quisition, Stuttgart, Germany.

R. Izquierdo, A. Suarez, and G. Rigau. 2007. Explor- 
ing the automatic selection of basic level concepts. 
In Galia Angelova et al., editor, International Con- 
ference Recent Advances in Natural Language Pro- 
cessing, pages 298–302, Borovets,  Bulgaria.

T. Joachims.  1998. Text categorization with support 
vector machines: learning with many relevant fea- 
tures. In Claire Ne´dellec and Ce´line Rouveirol, edi- 
tors, Proceedings of ECML-98, 10th European Con- 
ference on Machine Learning,  number 1398, pages
137–142, Chemnitz, DE. Springer Verlag, Heidel-
berg, DE.

B. Magnini and G. Cavaglia`. 2000. Integrating subject 
field codes into wordnet. In Proceedings of LREC, 
Athens. Greece.

Ll. Ma`rquez, G. Escudero, D. Mart´ınez, and G. Rigau.
2006. Supervised corpus-based methods for wsd. In 
E. Agirre and P. Edmonds  (Eds.)  Word Sense Disam- 
biguation: Algorithms and applications., volume 33 
of Text, Speech and Language Technology.  Springer.

R. Mihalcea and D. Moldovan. 2001. Automatic gen- 
eration of coarse grained wordnet. In Proceding of 
the NAACL workshop on WordNet and Other Lex- 
ical Resources: Applications,  Extensions and Cus- 
tomizations, Pittsburg, USA.

R. Mihalcea. 2007.  Using wikipedia for automatic 
word sense disambiguation.  In Proceedings of 
NAACL HLT 2007.

G. Miller, C. Leacock, R. Tengi, and R. Bunker. 1993.
A Semantic Concordance. In Proceedings of the
ARPA Workshop on Human Language Technology.

R. Navigli.   2006.  Meaningful clustering of senses 
helps boost word sense  disambiguation   perfor- 
mance. In ACL-44: Proceedings of the 21st Inter- 
national Conference on Computational Linguistics 
and the 44th annual meeting of the Association for 
Computational Linguistics, pages 105–112, Morris- 
town, NJ, USA. Association for Computational Lin- 
guistics.

I. Niles and A. Pease. 2001.  Towards  a standard upper 
ontology. In Proceedings of the 2nd International 
Conference on Formal Ontology in Information  Sys- 
tems (FOIS-2001),   pages 17–19. Chris Welty and 
Barry Smith, eds.

M. Palmer,  C. Fellbaum, S. Cotton, L. Delfs, and 
H.  Trang Dang.    2001.    English tasks:  All- 
words and verb lexical sample.  In Proceedings 
of the SENSEVAL-2 Workshop. In conjunction with 
ACL’2001/EACL’2001,  Toulouse, France.


W. Peters, I. Peters, and P. Vossen.  1998. Automatic 
sense clustering  in eurowordnet. In First Interna- 
tional  Conference on Language Resources and Eval- 
uation (LREC’98),  Granada, Spain.

F. Segond, A. Schiller, G. Greffenstette, and J. Chanod.
1997. An experiment in semantic tagging using hid- 
den markov model tagging. In ACL Workshop on 
Automatic Information Extraction and Building of 
Lexical  Semantic Resources for NLP Applications, 
pages 78–81. ACL, New Brunswick, New Jersey.

R. Snow, Prakash S., Jurafsky D., and Ng A.  2007.
Learning to merge word senses.  In Proceedings of 
Joint Conference on Empirical Methods in Natural 
Language  Processing and Computational Natural 
Language Learning (EMNLP-CoNLL), pages 1005–
1014.

B. Snyder and M. Palmer. 2004. The english all-words 
task.  In Rada Mihalcea  and Phil Edmonds, edi- 
tors, Senseval-3:  Third International  Workshop on 
the Evaluation of Systems for the Semantic Analysis 
of Text, pages 41–43, Barcelona, Spain, July. Asso- 
ciation for Computational Linguistics.

L. Villarejo, L. Ma`rquez,  and G. Rigau. 2005. Ex- 
ploring the construction of semantic class classi- 
fiers for wsd.  In Proceedings of the 21th Annual 
Meeting of Sociedad Espaola para el Procesamiento 
del Lenguaje Natural SEPLN’05,  pages 195–202, 
Granada, Spain, September. ISSN 1136-5948.

P.  Vossen,  L. Bloksma, H. Rodriguez,  S. Climent, 
N. Calzolari, A. Roventini, F. Bertagna, A. Alonge, 
and W. Peters. 1998. The eurowordnet  base con- 
cepts  and top ontology.  Technical report, Paris, 
France, France.

D. Yarowsky.  1994. Decision lists for lexical ambigu- 
ity resolution: Application to accent restoration in 
spanish and french.  In Proceedings of the 32nd An- 
nual Meeting of the Association for Computational 
Linguistics (ACL’94).

