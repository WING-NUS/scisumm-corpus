<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">The notion of fertility in word alignment (the number of words emitted by a sin­ gle state) is useful but difficult to model.</S>
		<S sid ="2" ssid = "2">Initial attempts at modeling fertility used heuristic search methods.</S>
		<S sid ="3" ssid = "3">Recent ap­ proaches instead use more principled ap­ proximate inference techniques such as Gibbs sampling for parameter estimation.</S>
		<S sid ="4" ssid = "4">Yet in practice we also need the single best alignment, which is difficult to find us­ ing Gibbs.</S>
		<S sid ="5" ssid = "5">Building on recent advances in dual decomposition, this paper introduces an exact algorithm for finding the sin­ gle best alignment with a fertility HMM.</S>
		<S sid ="6" ssid = "6">Finding the best alignment appears impor­ tant, as this model leads to a substantial improvement in alignment quality.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="7" ssid = "7">Word-based translation models intended to model the translation process have found new uses iden­ tifying word correspondences in sentence pairs.</S>
			<S sid ="8" ssid = "8">These word alignments are a crucial training com­ ponent in most machine translation systems.</S>
			<S sid ="9" ssid = "9">Fur­ thermore, they are useful in other NLP applica­ tions, such as entailment identification.</S>
			<S sid ="10" ssid = "10">The simplest models may use lexical infor­ mation alone.</S>
			<S sid ="11" ssid = "11">The seminal Model 1 (Brown et al., 1993) has proved very powerful, per­ forming nearly as well as more complicated models in some phrasal systems (Koehn et al., 2003).</S>
			<S sid ="12" ssid = "12">With minor improvements to initializa­ tion (Moore, 2004) (which may be important (Toutanova and Galley, 2011)), it can be quite competitive.</S>
			<S sid ="13" ssid = "13">Subsequent IBM models include more detailed information about context.</S>
			<S sid ="14" ssid = "14">Models 2 and 3 incorporate a positional model based on the absolute position of the word; Models 4 and 5 use a relative position model instead (an English word tends to align to a French word that is nearby the French word aligned to the previous English word).</S>
			<S sid ="15" ssid = "15">Models 3, 4, and 5 all incorporate a no­ tion of&quot;fertility&quot;: the number of French words that align to any English word.</S>
			<S sid ="16" ssid = "16">Although these latter models covered a broad range of phenomena, estimation techniques and MAP inference were challenging.</S>
			<S sid ="17" ssid = "17">The au­ thors originally recommended heuristic proce­ dures based on local search for both.</S>
			<S sid ="18" ssid = "18">Such meth­ ods work reasonably well, but can be computation­ ally inefficient and have few guarantees.</S>
			<S sid ="19" ssid = "19">Thus, many researchers have switched to the HMM model (Vogel et al., 1996) and variants with more parameters (He, 2007).</S>
			<S sid ="20" ssid = "20">This captures the posi­ tional information in the IBM models in a frame­ work that admits exact parameter estimation infer­ ence, though the objective function is not concave: local maxima are a concern.</S>
			<S sid ="21" ssid = "21">Modeling fertility is challenging in the HMM framework as it violates the Markov assump­ tion.</S>
			<S sid ="22" ssid = "22">Where the HMM jump model considers only the prior state, fertility requires looking across the whole state space.</S>
			<S sid ="23" ssid = "23">Therefore, the standard forward-backward and Viterbi algorithms do not apply.</S>
			<S sid ="24" ssid = "24">Recent work (Zhao and Gildea, 2010) de­ scribed an extension to the HMM with a fertility model, using MCMC techniques for parameter es­ timation.</S>
			<S sid ="25" ssid = "25">However, they do not have a efficient means of MAP inference, which is necessary in many applications such as machine translation.</S>
			<S sid ="26" ssid = "26">This paper introduces a method for exact MAP inference with the fertility HMM using dual de­ composition.</S>
			<S sid ="27" ssid = "27">The resulting model leads to sub­ stantial improvements in alignment quality.</S>
			<S sid ="28" ssid = "28">7 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 711, Sofia, Bulgaria, August 49 2013.</S>
			<S sid ="29" ssid = "29">@2013 Association for Computational Linguistics</S>
	</SECTION>
	<SECTION title="IIMM alignment. " number = "2">
			<S sid ="30" ssid = "1">Let us briefly review the HMM translation model as a starting point.</S>
			<S sid ="31" ssid = "2">We are given a sequence of English words e = e, 1.</S>
			<S sid ="32" ssid = "3">This model pro­ duces distributions over French word sequences f = !I, ...</S>
			<S sid ="33" ssid = "4">, !J and word alignment vectors a = at, ...</S>
			<S sid ="34" ssid = "5">, aJ, where aj E [O..J] indicates the En­ glish word generating the jth French word, 0 rep­ resenting a special NULL state to handle systemat­ ically unaligned words.</S>
			<S sid ="35" ssid = "6">J Pr(f, ale) = p(JII) ITp(ajlaj-t) p(Jilea;) j=l The generative story begins by predicting the num­ ber of words in the French sentence (hence the number of elements in the alignment vector).</S>
			<S sid ="36" ssid = "7">Then for each French word position, first the alignment variable (English word index used to generate the current French word) is selected based on only the prior alignment variable.</S>
			<S sid ="37" ssid = "8">Next the French word is predicted based on its aligned English word.</S>
			<S sid ="38" ssid = "9">Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility dis­ tribution.</S>
			<S sid ="39" ssid = "10">I Pr(f, ale) =p(JII) ITP(cPilei) estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sam­ pling (Zhao and Gildea, 2010).</S>
			<S sid ="40" ssid = "11">In this case, we make some initial estimate of the a vector, potentially randomly.</S>
			<S sid ="41" ssid = "12">We then repeatedly re­ sample each element of that vector conditioned on all other positions according to the distribu­ tion Pr(aj la-j, e, f).</S>
			<S sid ="42" ssid = "13">Given a complete assign­ ment of the alignment for all words except the cur­ rent, computing the complete probability includ­ ing transition, emission, and jump, is straightfor­ ward.</S>
			<S sid ="43" ssid = "14">This estimate comes with a computational cost: we must cycle through all positions of the vector repeatedly to gather a good estimate.</S>
			<S sid ="44" ssid = "15">In practice, a small number of samples will suffice.</S>
			<S sid ="45" ssid = "16">2.2 MAP inference with dual decomposition.</S>
			<S sid ="46" ssid = "17">Dual decomposition, also known as Lagrangian relaxation, is a method for solving complex combinatorial optimization problems (Rush and Collins, 2012).</S>
			<S sid ="47" ssid = "18">These complex problems are sepa­ rated into distinct components with tractable MAP inference procedures.</S>
			<S sid ="48" ssid = "19">The subproblems are re­ peatedly solved with some communication over consistency until a consistent and globally optimal solution is found.</S>
			<S sid ="49" ssid = "20">Here we are interested in the problem of find­ i=l J (1) ing the most likely alignment of a sentence pair ITp(ajlaj-t) p(/j leai) j=l where cPi = L,f=l 8(i, aj) indicates the number of times that state j is visited.</S>
			<S sid ="50" ssid = "21">This deficient model wastes some probability mass on inconsistent con­ figurations where the number of times that a state iis visited does not match its fertility cPi· Follow­ ing in the footsteps of older, richer, and wiser col­ leagues (Brown et al., 1993),we forge ahead un­ concerned by this complication.</S>
			<S sid ="51" ssid = "22">2.1 Parameter estimation.</S>
			<S sid ="52" ssid = "23">Of greater concern is the exponential complex­ ity of inference in this model.</S>
			<S sid ="53" ssid = "24">For the standard HMM, there is a dynamic programming algorithm to compute the posterior probability over word alignments Pr(ale, f).</S>
			<S sid ="54" ssid = "25">These are the sufficient statistics gathered in the E step of EM.</S>
			<S sid ="55" ssid = "26">The structure of the fertility model violates the Markov assumptions used in this dynamic pro­ gramming method.</S>
			<S sid ="56" ssid = "27">However, we may empirically e, f. Thus, we need to solve the combinatorial op­ timization problem arg maxa Pr(f, ale).</S>
			<S sid ="57" ssid = "28">Let us rewrite the objective function as follows: h(a) t,(logp(&lt;P.Ie;) +;;logp ;le;)) +t,(logp(a;l•;-d + logp(;;je.,)) Because f is fixed, the p( J I I) term is constant and may be omitted.</S>
			<S sid ="58" ssid = "29">Note how we&apos;ve split the opti­ mization into two portions.</S>
			<S sid ="59" ssid = "30">The first captures fer­ tility as well as some component of the translation distribution, and the second captures the jump dis­ tribution and the remainder of the translation dis­ tribution.</S>
			<S sid ="60" ssid = "31">Our dual decomposition method follows this segmentation.</S>
			<S sid ="61" ssid = "32">Define Ya as Ya (i, j) = 1 if aj = i, and 0 otherwise.</S>
			<S sid ="62" ssid = "33">Let z E {0, 1}/xJ be a binary u(o)(i,j) := 0 &apos;ViE l..J,j E l..J fork=1toK a(k) := argmaxa f(a)+I;.,,J. u(k-l)(i,j)ya(i,j)) z(k) := argmaxz g(z)- I;.,,J. u(k-l)(i,j)z(i, j)) if Ya = Z return a(k) end if u(k)(i,j) := u(k)(i,j) + lik (Ya&lt;•l (i,j)- z(k)(i,j)) end for return a(K) Figure 1: The dual decomposition algorithm for the fertility HMM, where 6k is the step size at the kth iteration for 1kK, and K is the max number of iterations.</S>
			<S sid ="63" ssid = "34">matrix.</S>
			<S sid ="64" ssid = "35">Define the functions f and g as /(a) t,(logp(a;la;-1) + logp(/; !e.;)) g(z) L ( logp(¢ (zi)lei) + i=l z(i,j) ) L.J 2logp(filei) j=l Then we want to find argmaxf(a) + g(z) a,z subject to the constraints Ya(i,j) = z(i,j)Vi,j. Note how this recovers the original objective func­ tion when matching variables are found.</S>
			<S sid ="65" ssid = "36">We use the dual decomposition algorithm from Rush and Collins (2012), reproduced here in Figure 1.</S>
			<S sid ="66" ssid = "37">Note how the langrangian adds one additional term word, scaled by a value indicating whether that word is aligned in the current position.</S>
			<S sid ="67" ssid = "38">Because it is only added for those words that are aligned, we can merge this with the log p(fi I eai) terms in both f and g. Therefore, we can solve argmax8 (!(a)+ i,j u&lt;k-l)(i,j)ya(i,j)) us­ ing the standard Viterbi algorithm.</S>
			<S sid ="68" ssid = "39">The g function, on the other hand, does not have a commonly used decomposition structure.</S>
			<S sid ="69" ssid = "40">Luck­ ily we can factor this maximization into pieces that allow for efficient computation.</S>
			<S sid ="70" ssid = "41">Note that g sums over arbitrary binary matrices.</S>
			<S sid ="71" ssid = "42">Unlike the HMM, where each French word must have exactly one English generator, this maximization allows each z(i,j) := 0 \:l(i,j) E [l..J] X [l..J] v :=0 fori=1toJ forj=1toJ x(j) := (logpCJile;) ,j) end for sort x in descending order by first component max:= logp(¢&gt; =Ole;), arg := 0, sum:= 0 forf=ltoJ sum:= sum+ x[f, 1] if sum+ logp(¢&gt; =fie;) &gt;max max:= sum+ logp(¢&gt; =fie;) arg := f end if end for v := v+max for f = 1 to arg z(i, x[f, 2]) := 1 end for end for retumz,v Figure 2: Algorithm for finding the arg max and max of g, the fertility-related component of the dual decomposition objective.</S>
			<S sid ="72" ssid = "43">French word to have zero or many generators.</S>
			<S sid ="73" ssid = "44">Be­ cause assignments that are in accordance between this model and the HMM will meet the HMM&apos;s constraints, the overall dual decomposition algo­ rithm will return valid assignments, even though individual selections for this model may fail to meet the requirements.</S>
			<S sid ="74" ssid = "45">As the scoring function g can be decomposed into a sum of scores for each row9i (i.e., there are no interactions between distinct rows of the matrix) we can maximize each row independently: I I m:X L9i(zi) = Lm:Xgi(zi) i=l i=l Within each row, we seek the best of all 2J pos­ sible configurations.</S>
			<S sid ="75" ssid = "46">These configurations may be grouped into equivalence classes based on the number of nonzero entries.</S>
			<S sid ="76" ssid = "47">In each class, the max assignment is the one using words with the highest log probabilities; the total score of this as­ signment is the sum those log probabilities and the log probability of that fertility.</S>
			<S sid ="77" ssid = "48">Sorting the scores of each cell in the row in descending or­ der by log probability allows for linear time com­ putation of the max for each row.</S>
			<S sid ="78" ssid = "49">The algorithm described in Figure 2 finds this maximal assign­ ment in O(J Jlog J) time, generally faster than the 0(!2 J) time used by Viterbi.</S>
			<S sid ="79" ssid = "50">We note in passing that this maximizer is pick­ing from an unconstrained set of binary matri ces.</S>
			<S sid ="80" ssid = "51">Since each English word may generate as many French words as it likes, regardless of all other words in the sentence, the underlying ma­ trix have many more or many fewer nonzero en­ tries than there are French words.</S>
			<S sid ="81" ssid = "52">A straightfor­ ward extension to the algorithm of Figure 2 returns only z matrices with exactly J nonzero entries.</S>
			<S sid ="82" ssid = "53">Rather than maximizing each row totally indepen­ dently, we keep track of the best configurations for each number of words generated in each row, and then pick the best combination that sums to J: another straightforward exercise in dynamic pro­ gramming.</S>
			<S sid ="83" ssid = "54">This refinement does not change the correctness of the dual decomposition algorithm; rather it speeds the convergence.</S>
	</SECTION>
	<SECTION title="Fertility distribution parameters. " number = "3">
			<S sid ="84" ssid = "1">Original IBM models used a categorical distribu­ tion of fertility, one such distribution for each En­ glish word.</S>
			<S sid ="85" ssid = "2">This gives EM a great amount of free­ dom in parameter estimation, with no smoothing or parameter tying of even rare words.</S>
			<S sid ="86" ssid = "3">Prior work addressed this by using the single parameter Pois­ son distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).</S>
			<S sid ="87" ssid = "4">We explore instead a feature-rich approach to address this issue.</S>
			<S sid ="88" ssid = "5">Prior work has explored feature-rich approaches to modeling the transla­ tion distribution (Berg-Kirkpatrick et al., 2010); we use the same technique, but only for the fertil­ ity model.</S>
			<S sid ="89" ssid = "6">The fertility distribution is modeled as a log-linear distribution ofF, a binary feature set: p(¢1e) ex exp (0 · F(e, ¢)).</S>
			<S sid ="90" ssid = "7">We include a simple set of features: • A binary indicator for each fertility ¢.</S>
			<S sid ="91" ssid = "8">This feature is present for all words, acting as smoothing.</S>
			<S sid ="92" ssid = "9">• A binary indicator for each word id and fer­ tility, if the word occurs more than 10 times.</S>
			<S sid ="93" ssid = "10">• A binary indicator for each word length (in letters) and fertility.</S>
			<S sid ="94" ssid = "11">• A binary indicator for each four letter word prefix and fertility.</S>
			<S sid ="95" ssid = "12">Together these produce a distribution that can learn a reasonable distribution not only for com­ mon words, but also for rare words.</S>
			<S sid ="96" ssid = "13">Including word length information aids in for languages with Algorithm AE R(G +E ) AE R(E +G ) H M M FH M M Vit erbi FH M M Dual dec 2 4 . 0 1 9 . 7 1 8 . 0 2 1 . 8 1 9 . 6 1 7 . 4 Table 1: Experimental results over the 120 evalu­ ation sentences.</S>
			<S sid ="97" ssid = "14">Alignment error rates in both di­ rections are provided here.</S>
	</SECTION>
	<SECTION title="Evaluation. " number = "4">
			<S sid ="98" ssid = "1">We explore the impact of this improved MAP in­ ference procedure on a task in GermanEnglish word alignment.</S>
			<S sid ="99" ssid = "2">For training data we use the news commentary data from the WMT 2012 translation task.1 120 of the training sentences were manually annotated with word alignments.</S>
			<S sid ="100" ssid = "3">The results in Table 1 compare several differ­ ent algorithms on this same data.</S>
			<S sid ="101" ssid = "4">The first line is a baseline HMM using exact posterior computa­ tion and inference with the standard dynamic pro­ gramming algorithms.</S>
			<S sid ="102" ssid = "5">The next line shows the fer­ tility HMM with approximate posterior computa­ tion from Gibbs sampling but with final alignment selected by the Viterbi algorithm.</S>
			<S sid ="103" ssid = "6">Clearly fertil­ ity modeling is improving alignment quality.</S>
			<S sid ="104" ssid = "7">The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).</S>
			<S sid ="105" ssid = "8">Here, however, the difference be­ tween a dual decomposition and Viterbi is signifi­ cant: their results were likely due to search error.</S>
	</SECTION>
	<SECTION title="Conclusions and future work. " number = "5">
			<S sid ="106" ssid = "1">We have introduced a dual decomposition ap­ proach to alignment inference that substantially reduces alignment error.</S>
			<S sid ="107" ssid = "2">Unfortunately the algo­ rithm is rather slow to converge: after 40 iterations of the dual decomposition, still only 55 percent of the test sentences have converged.</S>
			<S sid ="108" ssid = "3">We are ex­ ploring improvements to the simple sub-gradient method applied here in hopes of finding faster con­ vergence, fast enough to make this algorithm prac­ tical.</S>
			<S sid ="109" ssid = "4">Alternate parameter estimation techniques appear promising given the improvements of dual decomposition over sampling.</S>
			<S sid ="110" ssid = "5">Once the perfor­ mance issues of this algorithm are improved, ex­ ploring hard EM or some variant thereof might lead to more substantial improvements.</S>
			<S sid ="111" ssid = "6">compounding: long words in one language may correspond to multiple words in the other.</S>
			<S sid ="112" ssid = "7">1www.statmt.org/wmt12/trans1ation-task.html</S>
	</SECTION>
</PAPER>
