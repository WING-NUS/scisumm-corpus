Citance Number: 1 | Reference Article: D10-1058 | Citing Article: C16-1060 | Citation Marker Offset: ['48'] | Citation Marker: 2010.0 | Citation Offset: ['48'] | Citation Text: <S sid ="48" ssid = "27">Zhao and Gildea (2010) explored a model with a word order and fertility model as described above, but based their work on the EM algorithm, using Gibbs sampling only for approximating the expectations.</S> | Reference Offset: ['3','4','6'] | Reference Text: <S sid ="3" ssid = "3">We built a fertility hidden Markov model by adding fertility to the hidden Markov model.</S><S sid ="4" ssid = "4">This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster.</S><S sid ="6" ssid = "6">We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.</S> | Discourse Facet: Method_Citation | 
Citance Number: 2 | Reference Article: D10-1058 | Citing Article: P13-2002 | Citation Marker Offset: ['24'] | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: ['24'] | Citation Text: <S sid ="24" ssid = "24">Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter estimation.</S> | Reference Offset: ['18','26','33','34'] | Reference Text: <S sid ="18" ssid = "18">Most other researchers take either the HMM alignments (Liang et al., 2006) or IBM Model 4 alignments (Cherry and Lin, 2003) as input and perform post-processing, whereas our model is a potential replacement for the HMM and IBM Model 4.</S><S sid ="26" ssid = "26">Our model is a coherent generative model that combines the HMM and IBM Model 4.</S><S sid ="33" ssid = "33">Our model is much faster than IBM Model 4.</S><S sid ="34" ssid = "34">In fact, we will show that it is also faster than the HMM, and has lower alignment error rate than the HMM.</S> | Discourse Facet: Method_Citation | 
Citance Number: 3 | Reference Article: D10-1058 | Citing Article: P13-2002 | Citation Marker Offset: ['33'] | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: ['33'] | Citation Text: <S sid ="33" ssid = "4">, fJ and word alignment vectors a = estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sampling (Zhao and Gildea, 2010).</S> | Reference Offset: ['6','46'] | Reference Text: <S sid ="6" ssid = "6">We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.</S><S sid ="46" ssid = "46">We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, i = j=1 (aj , i) which has nice probabilistic guarantees.</S> | Discourse Facet: Method_Citation | 
Citance Number: 4 | Reference Article: D10-1058 | Citing Article: P13-2002 | Citation Marker Offset: ['43'] | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: ['43'] | Citation Text: <S sid ="43" ssid = "14">Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility distribution.</S> | Reference Offset: ['86'] | Reference Text: <S sid ="86" ssid = "1">Our fertility IBM Model 1 and fertility HMM are both generative models and start by defining the probability of fertilities (for each nonempty target word and all empty words), alignments, and the source sentence given the target sentence: P (I , , aJ , f J PIPEe2I +1); 1 1 1 1 are further away from the mean have low probability.</S> | Discourse Facet: Method_Citation | 
Citance Number: 5 | Reference Article: D10-1058 | Citing Article: P13-2002 | Citation Marker Offset: ['82'] | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: ['82'] | Citation Text: <S sid ="82" ssid = "3">Prior work addressed this by using the single parameter Poisson distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).</S> | Reference Offset: ['88','90'] | Reference Text: <S sid ="88" ssid = "3">Our model has only one parameter for each target word, which can be learned more reliably.</S><S sid ="90" ssid = "5">i=1 i! 1 1 1 , ,a1 1 1 1 1 The fertility for a nonempty word ei is a random variable i, and we assume i follows a Poisson distribution Poisson(i; (ei)).</S> | Discourse Facet: Method_Citation | 
Citance Number: 6 | Reference Article: D10-1058 | Citing Article: P13-2002 | Citation Marker Offset: ['99'] | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: ['99'] | Citation Text: <S sid ="99" ssid = "7">The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).</S> | Reference Offset: ['110'] | Reference Text: <S sid ="110" ssid = "4">1 , e2I +1 J 2I +1 For each target sentence e2I +1 and source sentence f J , we initialize the alignment aj for each source word fj using the Viterbi alignments from IBM Model 1.</S> | Discourse Facet: Method_Citation | 
Citance Number: 7 | Reference Article: D10-1058 | Citing Article: P59105ca | Citation Marker Offset: ['45'] | Citation Marker: 15.0 | Citation Offset: ['45'] | Citation Text: <S sid ="45" ssid = "45">Zhao and Gildea [15] use sampling in their proposed fertility extensions to IBM Model 1 and HMM, but they do not place any prior on the parameters.</S> | Reference Offset: ['113','114','115'] | Reference Text: <S sid ="113" ssid = "7">This Gibbs sampling method updates parameters constantly, so it is an online learning algorithm.</S><S sid ="114" ssid = "8">However, this sampling method needs a large amount of communication between machines in order to keep the parameters up to date if we compute the expected counts in parallel.</S><S sid ="115" ssid = "9">Instead, we do batch learning: we fix the parameters, scan through the entire corpus and compute expected counts in parallel (E-step); then combine all the counts together and update the parameters (M- step).</S> | Discourse Facet: Method_Citation | 
Citance Number: 8 | Reference Article: D10-1058 | Citing Article: P87-94 | Citation Marker Offset: ['79'] | Citation Marker: Zhao | Citation Offset: ['79'] | Citation Text: <S sid ="79" ssid = "2">Zhao proposes a brief fertility based HMM model,8 which also decreases the complexity of Model A Fully Bayesian Inference for Word Alignment 93 Table 2.</S> | Reference Offset: ['14','26','27'] | Reference Text: <S sid ="14" ssid = "14">Our goal is to build a model that includes lexicality, locality, and fertility; and, at the same time, to make it easy to understand.</S><S sid ="26" ssid = "26">Our model is a coherent generative model that combines the HMM and IBM Model 4.</S><S sid ="27" ssid = "27">It is easier to understand than IBM Model 4 (see Section 3).</S> | Discourse Facet: Method_Citation | 
Citance Number: 9 | Reference Article: D10-1058 | Citing Article: Pbulletin | Citation Marker Offset: ['104'] | Citation Marker: 2010.0 | Citation Offset: '103','104' | Citation Text: <S sid ="103" ssid = "24">For models with fertility computing the expectations instead becomes intractable, and previous authors have solved this by using approximative 2 The approximation consists of ignoring the dependence between the two draws from the word order jump distribution (second and third factors).</S><S sid ="104" ssid = "25">134 R. stling, J. Tiedemann Ecient Word Alignment with MCMC (125146) greedy optimization techniques (Brown et al., 1993) or local Gibbs sampling (Zhao and Gildea, 2010).</S> | Reference Offset: ['29','30'] | Reference Text: <S sid ="29" ssid = "29">We use Gibbs sampling instead of a heuristic-based neighborhood method for parameter 596 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 596605, MIT, Massachusetts, USA, 911 October 2010.</S><S sid ="30" ssid = "30">Qc 2010 Association for Computational Linguistics estimation.</S> | Discourse Facet: Method_Citation | 
Citance Number: 10 | Reference Article: D10-1058 | Citing Article: Pbulletin | Citation Marker Offset: ['131'] | Citation Marker: 2010.0 | Citation Offset: ['131'] | Citation Text: <S sid ="131" ssid = "13">Zhao and Gildea (2010) instead chose to use Gibbs sampling to approximate these expectations, which allowed them to perform efficient inference with EM for a HMM model with fertility.</S> | Reference Offset: ['29','30','107','108'] | Reference Text: <S sid ="29" ssid = "29">We use Gibbs sampling instead of a heuristic-based neighborhood method for parameter 596 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 596605, MIT, Massachusetts, USA, 911 October 2010.</S><S sid ="30" ssid = "30">Qc 2010 Association for Computational Linguistics estimation.</S><S sid ="107" ssid = "1">Although we can estimate the parameters by using 1 1 (EM) algorithm (Dempster et al., 1977).</S><S sid ="108" ssid = "2">The the EM algorithm, in order to compute the expected counts, we have to sum over all possible alignments1 , which is, unfortunately, exponential.</S> | Discourse Facet: Method_Citation | 
Citance Number: 11 | Reference Article: D10-1058 | Citing Article: Pcoling_D10 | Citation Marker Offset: ['232'] | Citation Marker: 2010.0 | Citation Offset: ['232'] | Citation Text: <S sid ="232" ssid = "9">Another interesting extension of the HMM alignment is presented in Zhao and Gildea (2010) who added a fertility distribution to the HMM.</S> | Reference Offset: ['3'] | Reference Text: <S sid ="3" ssid = "3">We built a fertility hidden Markov model by adding fertility to the hidden Markov model.</S> | Discourse Facet: Method_Citation | 
Citance Number: 12 | Reference Article: D10-1058 | Citing Article: Pproc_D10 | Citation Marker Offset: ['24'] | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: ['24'] | Citation Text: <S sid ="24" ssid = "24">Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter es timation.</S> | Reference Offset: ['3','46'] | Reference Text: <S sid ="3" ssid = "3">We built a fertility hidden Markov model by adding fertility to the hidden Markov model.</S><S sid ="46" ssid = "46">We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, i = j=1 (aj , i) which has nice probabilistic guarantees.</S> | Discourse Facet: Method_Citation | 
Citance Number: 13 | Reference Article: D10-1058 | Citing Article: Pproc_D10 | Citation Marker Offset: ['38'] | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: ['38'] | Citation Text: <S sid ="38" ssid = "9">Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility dis tribution.</S> | Reference Offset: ['86'] | Reference Text: <S sid ="86" ssid = "1">Our fertility IBM Model 1 and fertility HMM are both generative models and start by defining the probability of fertilities (for each nonempty target word and all empty words), alignments, and the source sentence given the target sentence: P (I , , aJ , f J PIPEe2I +1); 1 1 1 1 are further away from the mean have low probability.</S> | Discourse Facet: Method_Citation | 
Citance Number: 14 | Reference Article: D10-1058 | Citing Article: Pproc_D10 | Citation Marker Offset: ['39'] | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: ['39'] | Citation Text: <S sid ="39" ssid = "10">I Pr(f, ale) =p(JII) ITP(cPilei) estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sam pling (Zhao and Gildea, 2010).</S> | Reference Offset: ['6','46'] | Reference Text: <S sid ="6" ssid = "6">We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.</S><S sid ="46" ssid = "46">We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, i = j=1 (aj , i) which has nice probabilistic guarantees.</S> | Discourse Facet: Method_Citation | 
Citance Number: 15 | Reference Article: D10-1058 | Citing Article: Pproc_D10 | Citation Marker Offset: ['86'] | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: ['86'] | Citation Text: <S sid ="86" ssid = "3">Prior work addressed this by using the single parameter Pois son distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).</S> | Reference Offset: ['88','90'] | Reference Text: <S sid ="88" ssid = "3">Our model has only one parameter for each target word, which can be learned more reliably.</S><S sid ="90" ssid = "5">i=1 i! 1 1 1 , ,a1 1 1 1 1 The fertility for a nonempty word ei is a random variable i, and we assume i follows a Poisson distribution Poisson(i; (ei)).</S> | Discourse Facet: Method_Citation | 
Citance Number: 16 | Reference Article: D10-1058 | Citing Article: Pproc_D10 | Citation Marker Offset: ['104'] | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: ['104'] | Citation Text: <S sid ="104" ssid = "7">The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).</S> | Reference Offset: ['110'] | Reference Text: <S sid ="110" ssid = "4">1 , e2I +1 J 2I +1 For each target sentence e2I +1 and source sentence f J , we initialize the alignment aj for each source word fj using the Viterbi alignments from IBM Model 1.</S> | Discourse Facet: Method_Citation | 
Citance Number: 17 | Reference Article: D10-1058 | Citing Article: Q13-1024 | Citation Marker Offset: ['60'] | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: ['60'] | Citation Text: <S sid ="60" ssid = "3">The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011).</S> | Reference Offset: ['3','4','5','6'] | Reference Text: <S sid ="3" ssid = "3">We built a fertility hidden Markov model by adding fertility to the hidden Markov model.</S><S sid ="4" ssid = "4">This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster.</S><S sid ="5" ssid = "5">It is similar in some ways to IBM Model 4, but is much easier to understand.</S><S sid ="6" ssid = "6">We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.</S> | Discourse Facet: Method_Citation | 
Citance Number: 18 | Reference Article: D10-1058 | Citing Article: Q13-1024 | Citation Marker Offset: ['130'] | Citation Marker: Zhao and Gildea, 2010 | Citation Offset: ['130'] | Citation Text: <S sid ="130" ssid = "22">Our Gibbs sampler is similar to the MCMC algorithm in Zhao and Gildea (2010), but we assume Dirichlet priors when sampling model parameters and take a different sampling approach based on the source side dependency tree.</S> | Reference Offset: ['46'] | Reference Text: <S sid ="46" ssid = "46">We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, i = j=1 (aj , i) which has nice probabilistic guarantees.</S> | Discourse Facet: Method_Citation | 
