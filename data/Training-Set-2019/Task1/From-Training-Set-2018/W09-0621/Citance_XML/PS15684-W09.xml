<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Paraphrase Extraction involves the discovery of equivalent text segments from large corpora and ﬁnds application in tasks such as multi-document summarization and document clustering.</S>
		<S sid ="2" ssid = "2">Semantic similarity identiﬁcation is a challenging problem which is further compounded by the large size of the corpus.</S>
		<S sid ="3" ssid = "3">In this paper a two- stage approach which involves clustering followed by Paraphrase Recognition has been proposed for extraction of sentence-level paraphrases from text collections.</S>
		<S sid ="4" ssid = "4">In order to handle the ambiguity and inherent variability of natural language a fuzzy hierarchical clustering approach which combines agglomeration based on verbs and division on nouns has been used.</S>
		<S sid ="5" ssid = "5">Sentences within each resultant cluster are then processed by a machine-learning based Paraphrase Recognizer to discover the paraphrases.</S>
		<S sid ="6" ssid = "6">The two-stage approach has been applied on the Microsoft Research Paraphrase Corpus and a subset of the Microsoft Research Video Description Corpus.</S>
		<S sid ="7" ssid = "7">The performance has been evaluated against an existing k-means clustering approach as well as cosine-similarity technique and Fuzzy C-Means clustering and the two- stage system has consistently demonstrated better performance.</S>
		<S sid ="8" ssid = "8">©2015 Elsevier B.V. All rights reserved.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="9" ssid = "9">Vast amounts of natural language text are available on the web as well as in large-scale repositories; much of which is redundant.</S>
			<S sid ="10" ssid = "10">The task of Paraphrase Extraction focuses on identiﬁcation of text units which convey the same meaning.</S>
			<S sid ="11" ssid = "11">The detection of similar text is complicated due to the rich variability of natural languages.</S>
			<S sid ="12" ssid = "12">The large scale of the corpora is another factor which poses hurdles in Paraphrase Extraction.</S>
			<S sid ="13" ssid = "13">Performing one-on-one matching of sentences is practically ruled out, even if it is assumed that a suitable Paraphrase Recognition system is available.</S>
			<S sid ="14" ssid = "14">Therefore, efﬁcient techniques are required to identify possibly similar candidates from large scale corpora and then subject them to further processing to detect exact matches.</S>
			<S sid ="15" ssid = "15">An effective Paraphrase Extraction system will beneﬁt various Natural Language Processing applications such as multi-document summarization, plagiarism detection, question answering and document clustering.</S>
			<S sid ="16" ssid = "16">The signiﬁcant aspect of this work is that a novel two-level fuzzy clustering technique has been proposed for sentence-level Paraphrase Extraction.</S>
			<S sid ="17" ssid = "17">As similar sentences tend to describe the same or similar actions, Fuzzy Agglomerative Clustering based on verbs ∗ Corresponding author.</S>
			<S sid ="18" ssid = "18">Tel.: +91 9443760000.</S>
			<S sid ="19" ssid = "19">Email addresses: ctr.psg@gmail.co (A. Chitra), anupriya rajkumar@yahoo.co.in (A. Rajkumar).</S>
			<S sid ="20" ssid = "20">1 Tel.: +91 9843222273..</S>
			<S sid ="21" ssid = "21">is performed initially.</S>
			<S sid ="22" ssid = "22">Divisive clustering is then applied to identify subgroups of sentences which center on the same nouns.</S>
			<S sid ="23" ssid = "23">A Support Vector Machine (SVM) based Paraphrase Recognizer is then used to identify the paraphrases within each cluster.</S>
			<S sid ="24" ssid = "24">The performance of the Paraphrase Extraction system has been evaluated using the Microsoft Research Paraphrase Corpus (MSRPC) and a subset of the Microsoft Research Video Description Corpus (MSRVDC).</S>
			<S sid ="25" ssid = "25">The outline of the paper is as follows: Section 2 contains an overview of previous work related to Paraphrase Extraction and Hierarchical Clustering.</S>
			<S sid ="26" ssid = "26">Section 3 describes the methodology adopted for extracting paraphrases using a fuzzy hierarchical clustering approach and machine learning based Paraphrase Recognizer.</S>
			<S sid ="27" ssid = "27">Section 4 presents the results of experiments conducted using two different corpora.</S>
			<S sid ="28" ssid = "28">Possible directions for future work and applications are discussed in Section 5, which concludes the paper.</S>
	</SECTION>
	<SECTION title="Related work. " number = "2">
			<S sid ="29" ssid = "1">The task of Paraphrase Extraction or acquisition aims at extracting paraphrases from a given corpus.</S>
			<S sid ="30" ssid = "2">Several approaches exploit Harris’s distributional hypothesis which states that words in similar contexts tend to have same meanings [1].</S>
			<S sid ="31" ssid = "3">Bhagat and Ravichandran [2] have used this approach to extract phrase level entities, by constructing a feature vector for each phrase based on its context and computing the cosine similarity between the feature vectors of phrases.</S>
			<S sid ="32" ssid = "4">Metzler and Hovy [3] have deployed the http://dx.doi.org/10.1016/j.asoc.2015.05.017 15684946/© 2015 Elsevier B.V. All rights reserved.</S>
			<S sid ="33" ssid = "5">distributional hypothesis in a Hadoop-based framework to operate on large-scale corpora.</S>
			<S sid ="34" ssid = "6">Bootstrapping methods rely on seed patterns for acquiring paraphrases.</S>
			<S sid ="35" ssid = "7">Szpektor et al. have used terms from a domain-speciﬁc lexicon and coupled these with frequently co-occurring noun phrases to form seed slots and have then extracted templates [4].</S>
			<S sid ="36" ssid = "8">Regneri et al. have extracted phrase level paraphrases from similar sentence pairs by assigning semantic role labels and then locating equivalent arguments or anchors.</S>
			<S sid ="37" ssid = "9">Dependency parse of the sentences has then been used to group anchors with their corresponding predicates, yielding matched predicate argument structures which were then extended to extract equivalent phrases [5].</S>
			<S sid ="38" ssid = "10">Barzilay and Lee (2003) have applied hierarchical complete-link clustering to cluster the sentences describing the same type of event [6].</S>
			<S sid ="39" ssid = "11">Sentence level paraphrases were discovered by applying multiple sequence alignment on pairs of sentences from each cluster.</S>
			<S sid ="40" ssid = "12">A similar sequence alignment technique has been employed by Regneri and Wang [7] to ﬁrst extract sentence-level paraphrases and then phrase-level paraphrases.</S>
			<S sid ="41" ssid = "13">Yan et al. [8] have extracted multilingual phrasal paraphrases by aligning deﬁnition sentences extracted from Wikipedia articles.</S>
			<S sid ="42" ssid = "14">A minimally supervised approach was then used to extract paraphrases by computing global and local similarity measures between phrasal pairs from the aligned sentences.</S>
			<S sid ="43" ssid = "15">Wubben et al. have compared Clustering against pairwise matching for extracting paraphrases from news corpora [9].</S>
			<S sid ="44" ssid = "16">A k-means clustering approach was used to subdivide already existing clusters of headlines.</S>
			<S sid ="45" ssid = "17">Sentence-level paraphrases were then extracted by matching all possible sentence pairs within each cluster.</S>
			<S sid ="46" ssid = "18">Traditional clustering algorithms create a hard partitioning of data in which each object is assigned to only one cluster.</S>
			<S sid ="47" ssid = "19">Fuzzy clustering is an alternate, wherein a soft partition is constructed with each object belonging to multiple clusters with different degrees of membership.</S>
			<S sid ="48" ssid = "20">Two popular variants of fuzzy clustering are Fuzzy C-Means and fuzzy hierarchical approach.</S>
			<S sid ="49" ssid = "21">The Fuzzy C-Means approach is a fuzzy variant of the original k-means partitioning approach and is limited by the necessity of specifying ‘c’; though techniques for automatically detecting ‘c’ exist, they become infeasible in large corpora.</S>
			<S sid ="50" ssid = "22">Hierarchical clustering methods operate on the inter-cluster similarity matrix.</S>
			<S sid ="51" ssid = "23">Bank and Schwenker [10] have proposed a fuzzi- ﬁed version of the agglomerative algorithm where, on merging Ci and Cj to create Cij , only the similarity value Sij is set to zero and the remaining values corresponding to the clusters Ci and Cj are retained.</S>
			<S sid ="52" ssid = "24">This permits the clusters Ci and Cj to take part in subsequent mergers.</S>
			<S sid ="53" ssid = "25">When a cluster is a part of several parent clusters, normalized membership values are assigned.</S>
			<S sid ="54" ssid = "26">Rodrigues et al. [11] have proposed an Online Divisive Agglomerative Clustering where a semi-fuzzy approach has been used for assigning objects to clusters by computing the distance between the object and the candidate clusters during division.</S>
			<S sid ="55" ssid = "27">If the computed distance is greater than the Hoeffding bound, the object is assigned to the corresponding cluster otherwise it is assigned to multiple clusters.</S>
			<S sid ="56" ssid = "28">DiazValenzuela et al. [12] have developed a fuzzy hierarchical clustering technique which works in a semi-supervised fashion for classifying scientiﬁc publications.</S>
			<S sid ="57" ssid = "29">The method outperforms the unsupervised approach but requires instance-level constraints to be speciﬁed to determine the optimal ex-cut of the dendrograms produced by hierarchical clustering.</S>
			<S sid ="58" ssid = "30">Clustering of sentences is a common task which ﬁnds several applications.</S>
			<S sid ="59" ssid = "31">Seno and Nunes [13] have used an incremental approach for clustering sentences from multiple documents belonging to Portuguese language.</S>
			<S sid ="60" ssid = "32">The ﬁrst cluster was created with the ﬁrst sentence, and each subsequent sentence was either added to an existing cluster based on cosine similarity and word overlap measures or assigned to a new cluster.</S>
			<S sid ="61" ssid = "33">Khoury has proposed a sentence clustering approach based on Parts-Of-Speech (POS) information [14].</S>
			<S sid ="62" ssid = "34">A POS hierarchy has been used to compute the distance between two words or POS based on which existing clusters are split.</S>
			<S sid ="63" ssid = "35">Fuzzy approaches have been previously applied to both document clustering as well as sentence clustering.</S>
			<S sid ="64" ssid = "36">Rodrigues and Sacks [15] have proposed Hierarchical Hyper-spherical Fuzzy C- Means (H2FCM) approach for clustering documents which uses cosine similarity for forming hyper-spherical clusters that are then merged.</S>
			<S sid ="65" ssid = "37">Skabar and Abdalgader [16] have proposed a fuzzy cluster ing algorithm for relational data termed as FRECCA, where sentence similarity values are used rather than the vector space representation of the sentences and expectation maximization has been used to determine cluster memberships.</S>
			<S sid ="66" ssid = "38">Wazarkar and Manjrekar have proposed an extension of the FRECCA approach which forms hierarchical clusters by using a divisive clustering approach [17].</S>
			<S sid ="67" ssid = "39">From a study of related literature it has been concluded that, of the approaches applied for Paraphrase Extraction, the alignment approach is better suited for the extraction of sentence level paraphrases.</S>
			<S sid ="68" ssid = "40">Applying alignment approach directly on a large corpus is infeasible.</S>
			<S sid ="69" ssid = "41">Hence it is better to ﬁrst apply clustering and then match sentences within the cluster.</S>
			<S sid ="70" ssid = "42">Fuzzy clustering approach is more applicable to natural language input than crisp approaches.</S>
			<S sid ="71" ssid = "43">Likewise hierarchical clustering approach which automatically establishes the number of clusters using thresholds on the similarity metric and also supports incremental clustering is better than partitioning approaches.</S>
	</SECTION>
	<SECTION title="Fuzzy  clustering for Paraphrase Extraction. " number = "3">
			<S sid ="72" ssid = "1">In this work, Paraphrase Extraction is performed in two stages: fuzzy hierarchical clustering stage which focuses on grouping similar sentences followed by Paraphrase Recognition applied on each pair of sentences within a cluster.</S>
			<S sid ="73" ssid = "2">Since a sentence may describe multiple events and involve several entities and can therefore be placed within multiple clusters, fuzzy clustering approach has been preferred.</S>
			<S sid ="74" ssid = "3">Further, since the number of clusters is not known the hierarchical clustering technique has been used.</S>
			<S sid ="75" ssid = "4">The process of fuzzy hierarchical clustering shown in Fig.</S>
			<S sid ="76" ssid = "5">1 consists of the following steps: • Sentences from the corpus are subject to pre-processing where Parts-Of Speech tags are assigned.</S>
			<S sid ="77" ssid = "6">• All sentences which contain the same verb are clustered together.</S>
			<S sid ="78" ssid = "7">These clusters are then merged based on the similarity between the verbs.• Divisive clustering is then used to split the clusters into sub clusters, such that each sub-cluster relates to the same/similar set of nouns.</S>
			<S sid ="79" ssid = "8">In the second stage of the Paraphrase Extraction process, a Paraphrase Recognizer is used to identify the paraphrases within each cluster as shown in Fig.</S>
			<S sid ="80" ssid = "9">2.</S>
			<S sid ="81" ssid = "10">Various lexical, syntactic and semantic features are extracted from pairs of sentences and an SVM classi- ﬁer uses these features to classify each pair as positive or negative cases of Paraphrasing.</S>
			<S sid ="82" ssid = "11">The positive pairs are grouped together due to the transitive nature of paraphrases, to produce the output which consists of a collection of sentence-level paraphrases.</S>
			<S sid ="83" ssid = "12">3.1.</S>
			<S sid ="84" ssid = "13">Preprocessing.</S>
			<S sid ="85" ssid = "14">During preprocessing, each sentence is assigned a unique identi- ﬁer.</S>
			<S sid ="86" ssid = "15">Since the clustering process is centered on the verbs and nouns within each sentence, the words of each sentence are assigned POS tags using the Tree Tagger developed by Helmut Schmidt [18].</S>
			<S sid ="87" ssid = "16">The Corpus Pre-processing Sentences Fuzzy Agglomerative Clustering on Verbs Cluster 1 (C1) Cluster 2 (C2) Cluster n (Cn) Fuzzy Divisive Clustering on Nouns Fuzzy Divisive Clustering on Nouns Fuzzy Divisive Clustering on Nouns C1 -Sub Clusters C2 -Sub Clusters Cn -Sub Clusters Fig.</S>
			<S sid ="88" ssid = "17">1.</S>
			<S sid ="89" ssid = "18">Fuzzy hierarchical clustering for Paraphrase Extraction.</S>
			<S sid ="90" ssid = "19">Sub-Cluster Pair of Feature Extraction SVM Classifier Features Paraphrases Group Formation Collection of Paraphrases Sentences Fig.</S>
			<S sid ="91" ssid = "20">2.</S>
			<S sid ="92" ssid = "21">Collecting paraphrases using a Paraphrase Recognizer.</S>
			<S sid ="93" ssid = "22">Tree Tagger uses Probabilistic tagging to assign POS tags based on the Penn Treebank tagset [19] and also identiﬁes the root form or lemma for each word of the sentence.</S>
			<S sid ="94" ssid = "23">3.2.</S>
			<S sid ="95" ssid = "24">Fuzzy agglomerative clustering based on verbs.</S>
			<S sid ="96" ssid = "25">In order to cluster the sentences, a two-stage approach has been used.</S>
			<S sid ="97" ssid = "26">In the ﬁrst stage fuzzy agglomerative clustering based on verbs has been carried out due to the following reasons: • Due to the ambiguous nature of natural language input, fuzzy clustering is better suited than crisp techniques.</S>
			<S sid ="98" ssid = "27">• Partitioning based approaches require the number of clusters to be prespeciﬁed; therefore a hierarchical clustering approach has been preferred.</S>
			<S sid ="99" ssid = "28">• Typically in a large-scale corpus, several sentences focus on same or similar actions.</S>
			<S sid ="100" ssid = "29">Hence sentences involving same verbs are ﬁrst placed in the same cluster, these clusters are then merged depending on the similarity between the verbs.</S>
			<S sid ="101" ssid = "30">• Ability to handle incremental data; new sentences can be added to the most similar cluster during any stage of the clustering process.</S>
			<S sid ="102" ssid = "31">The process of fuzzy agglomerative clustering on verbs starts with initial cluster formation, followed by grouping and merging of clusters.</S>
			<S sid ="103" ssid = "32">The grouping and merging steps are repeatedly carried out, until no further merging is possible.</S>
			<S sid ="104" ssid = "33">The algorithm has been given in Fig.</S>
			<S sid ="105" ssid = "34">3.</S>
			<S sid ="106" ssid = "35">3.2.1.</S>
			<S sid ="107" ssid = "36">Initial cluster formation A fuzzy clustering of the sentences is initially established by placing each sentence in as many clusters as there are verbs.</S>
			<S sid ="108" ssid = "37">To prevent the formation of generic clusters, the process targets only main verbs and ignores auxiliary verbs.</S>
			<S sid ="109" ssid = "38">Sentences which do not contain a main verb are placed in the ﬁrst cluster with a membership value of 1.</S>
			<S sid ="110" ssid = "39">In case multiple verbs are present, sentence i has to be placed in the corresponding clusters – j, with a membership value as follows: 1 J-ij = number of main verbs in sentence i (1) J-ik = 0 for all other clusters k (2) In order to create ﬁner clusters, Word Sense Disambiguation (WSD) can be used.</S>
			<S sid ="111" ssid = "40">The speciﬁc sense of the candidate verb is determined based on the context of the verb by using the Lesk disambiguation algorithm.</S>
			<S sid ="112" ssid = "41">The simpliﬁed version of the Lesk algorithm is used to determine the correct sense of a word, given its context [20,21].</S>
			<S sid ="113" ssid = "42">A target word may have multiple senses, each of which has a corresponding gloss or textual deﬁnition.</S>
			<S sid ="114" ssid = "43">The sense whose gloss has the highest degree of word overlap with the context is chosen as the best sense of the word.</S>
			<S sid ="115" ssid = "44">Here, context refers to the sentence containing the target word.</S>
			<S sid ="116" ssid = "45">In the absence of WSD, the default or most common sense is chosen for each word.</S>
			<S sid ="117" ssid = "46">In either case, the selected sense number is concatenated to the target verb to generate the cluster label.</S>
			<S sid ="118" ssid = "47">In case the cluster label matches any of the existing clusters, the sentence is placed in the matching cluster; otherwise a new cluster is created with the generated label and the sentence is added to it.</S>
			<S sid ="119" ssid = "48">The pseudo-code for the simpliﬁed Lesk algorithm [21] is given in Fig.</S>
			<S sid ="120" ssid = "49">4.</S>
			<S sid ="121" ssid = "50">3.2.2.</S>
			<S sid ="122" ssid = "51">Grouping of clusters Once the clusters are formed, similar clusters will have to be combined.</S>
			<S sid ="123" ssid = "52">The fuzzy agglomerative clustering approach used here differs from previous work with respect to the formation of overlapping fuzzy partitions or groups.</S>
			<S sid ="124" ssid = "53">Each cluster is labeled with a representative verb and similarity between clusters is computed between their labels.</S>
			<S sid ="125" ssid = "54">The Jiang–Conrath measure [22] which assesses the similarity between two words in terms of information content of the given words and their lowest common subsumer in the WordNet hierarchy as given in Eq.</S>
			<S sid ="126" ssid = "55">(3) has been used to compute the similarity.</S>
			<S sid ="127" ssid = "56">sim(w1 , w2 ) = 1 IC (w1 ) + IC (w2 ) − 2 ∗ IC (LCS) (3) In Eq.</S>
			<S sid ="128" ssid = "57">(3) LCS represents the Least Common Subsumer or lowest common ancestor for the two words w1 , w2 in the Wordnet hierarchy and IC represents the information content assessed using Eq.</S>
			<S sid ="129" ssid = "58">(4).</S>
			<S sid ="130" ssid = "59">IC (w) = − log P(w) (4) where P(w) is the probability of encountering an instance of word w in a large corpus.The traditional Centroid Clustering technique proceeds by iden tifying the pair of clusters which have greatest similarity and merges them; this, results in dendrograms with several levels as shown in Fig.</S>
			<S sid ="131" ssid = "60">5a.</S>
			<S sid ="132" ssid = "61">In this work, a grouping strategy is utilized to Fig.</S>
			<S sid ="133" ssid = "62">3.</S>
			<S sid ="134" ssid = "63">Algorithm for Fuzzy Agglomerative Clustering based on verbs.</S>
			<S sid ="135" ssid = "64">Fig.</S>
			<S sid ="136" ssid = "65">4.</S>
			<S sid ="137" ssid = "66">Simpliﬁed Lesk algorithm [21].</S>
			<S sid ="138" ssid = "67">ﬁrst identify all clusters which have a similarity greater than or equal to a prespeciﬁed threshold.</S>
			<S sid ="139" ssid = "68">Initially each cluster Ci is placed in a group of its own.</S>
			<S sid ="140" ssid = "69">Each group is then expanded, by including all other clusters whose similarity with Ci exceeds the threshold.</S>
			<S sid ="141" ssid = "70">Before merging, all subgroups, duplicate and singleton groups are removed from further consideration.</S>
			<S sid ="142" ssid = "71">The merging of multiple clusters in each step results in ﬂatter dendrograms as shown in Fig.</S>
			<S sid ="143" ssid = "72">5b.</S>
			<S sid ="144" ssid = "73">The similarity threshold imposed during merging has been chosen based on a study conducted using the benchmark verb dataset consisting of 130 verb pairs developed by Yang and Powers [23].</S>
			<S sid ="145" ssid = "74">The Jiang–Conrath scores for word pairs in the categories ‘inseparably related’ and ‘strongly related’ were examined and it was concluded that 0.15 and 0.2 are suitable similarity threshold values.</S>
			<S sid ="146" ssid = "75">(a) P3 P2 P1 C4 C3 C1 C2 (b) P C1 C2 C3 C4 Fig.</S>
			<S sid ="147" ssid = "76">5.</S>
			<S sid ="148" ssid = "77">(a) Binary merging of clusters.</S>
			<S sid ="149" ssid = "78">(b) Merging of multiple clusters.</S>
			<S sid ="150" ssid = "79">Fig.</S>
			<S sid ="151" ssid = "80">6.</S>
			<S sid ="152" ssid = "81">Algorithm for fuzzy divisive clustering based on nouns.</S>
			<S sid ="153" ssid = "82">3.2.3.</S>
			<S sid ="154" ssid = "83">Merging of clusters Once the clusters are grouped, the candidate clusters within each group have to be merged together by ﬁrst determining the parent verb for the representative verbs from each cluster.</S>
			<S sid ="155" ssid = "84">The parent of two words in the WordNet hierarchy is the lowest common ancestor of the words in the WordNet hierarchy.</S>
			<S sid ="156" ssid = "85">This concept is extended to multiple candidates using the following steps: Step 1: Identify the Parent – P, or lowest common ancestor of the ﬁrst two candidate cluster labels.</S>
			<S sid ="157" ssid = "86">Step 2: For each subsequent candidate cluster label Li , determine the parent – Pi of P and Li . If no parent exists, which happens when the words may not be directly related, the process terminates.</S>
			<S sid ="158" ssid = "87">Step 3: Set P = Pi and repeat Step 2.</S>
			<S sid ="159" ssid = "88">If no parent verb can be identiﬁed the candidates within the group are not merged.</S>
			<S sid ="160" ssid = "89">If a parent verb exists, and in case of WSD, the correct sense of the parent verb is determined by using the original Lesk algorithm [20].</S>
			<S sid ="161" ssid = "90">Here instead of matching the target word and context, overlap is determined between each of the n senses of the parent and the best sense of each candidate verb.</S>
			<S sid ="162" ssid = "91">In the absence of WSD, the default sense is used.</S>
			<S sid ="163" ssid = "92">If the parent cluster happens to be any of the existing clusters, the candidate clusters are all attached as child clusters of the parent; if not a new parent cluster is created.</S>
			<S sid ="164" ssid = "93">When a child cluster Ci is added to a parent cluster Pj its membership is assigned according to Eq.</S>
			<S sid ="165" ssid = "94">(5), using the method proposed by Bank and Schwenker [10]: sij J-Ci Pj = L p (5) where p represents all the parent clusters for the child cluster i, sij and sip are the Jiang–Conrath scores calculated between representative verbs of the child cluster and the parent cluster.</S>
			<S sid ="166" ssid = "95">The membership value for a sentence k of the child cluster i in parent cluster j is computed using Eq.</S>
			<S sid ="167" ssid = "96">(6).</S>
			<S sid ="168" ssid = "97">J-kj = J-ki J-Ci Pj (6) After all groups have been processed, a new similarity matrix is constructed between the current set of cluster labels and the pro cess of grouping and merging clusters is repeated until no further groups are formed.</S>
			<S sid ="169" ssid = "98">This results when all similarity values are lesser than the speciﬁed threshold.</S>
			<S sid ="170" ssid = "99">3.3.</S>
			<S sid ="171" ssid = "100">Fuzzy divisive clustering.</S>
			<S sid ="172" ssid = "101">The clusters formed by grouping sentences with the same or similar verbs are then divided such that all sentences within the same sub-cluster deal with the same or related nouns.</S>
			<S sid ="173" ssid = "102">Fuzzy division is applied since a sentence may contain multiple nouns and C S5 S2 S6 S8 S1 S9 S4 S3 S7 S10 S11 Fig.</S>
			<S sid ="174" ssid = "103">7.</S>
			<S sid ="175" ssid = "104">Example of fuzzy divisive clustering.</S>
			<S sid ="176" ssid = "105">may therefore belong to more than one cluster.</S>
			<S sid ="177" ssid = "106">The algorithm used is given in Fig.</S>
			<S sid ="178" ssid = "107">6.</S>
			<S sid ="179" ssid = "108">The normalized noun similarity nsim, between every pair of sentences x and y, is computed as in Eq.</S>
			<S sid ="180" ssid = "109">(7).</S>
			<S sid ="181" ssid = "110">nx and ny represent the number of nouns in the sentences x and y, respectively.</S>
			<S sid ="182" ssid = "111">The Jiang–Conrath measure is used to compute the noun similarities.</S>
			<S sid ="183" ssid = "112">nx max ny 1 {similarity(noun xi , noun xj )} nsimxy = ) j= x i=1 (7)These similarity values are used to create a partition of the sen tences within the cluster.</S>
			<S sid ="184" ssid = "113">A threshold value is used for controlling the division of clusters.</S>
			<S sid ="185" ssid = "114">The similarity scores for the Miller–Charles dataset [24] were used to identify suitable threshold values of 0.15 and 0.2.</S>
			<S sid ="186" ssid = "115">In the next step the sentences are partitioned based on the threshold.</S>
			<S sid ="187" ssid = "116">A group is formed for each sentence, comprising of itself as well as other sentences which have a similarity greater than the threshold.</S>
			<S sid ="188" ssid = "117">Duplicate groups and subgroups are eliminated as before.</S>
			<S sid ="189" ssid = "118">In practice, when the number of sentences is high, different groups which have a high degree of overlap are formed.</S>
			<S sid ="190" ssid = "119">This, results in the presence of the same sentence in several groups, which ultimately increases processing time and reduces the advantage gained due to clustering of sentences.</S>
			<S sid ="191" ssid = "120">This problem is overcome by permitting a sentence to be a part of only two groups to which it is most similar to.</S>
			<S sid ="192" ssid = "121">In Fig.</S>
			<S sid ="193" ssid = "122">7, the 11 sentences in Cluster C are split into four partitions: {S1, S2, S3, S4} {S5} {S2, S4, S6, S7, S8, S9} and {S8, S9, S10, S11}.</S>
			<S sid ="194" ssid = "123">Sentences S2, S4, S8 and S9 are placed within two groups.</S>
			<S sid ="195" ssid = "124">Once groups are identiﬁed, the original cluster is split by creating child clusters and distributing the sentences according to the partitioning scheme.</S>
			<S sid ="196" ssid = "125">The membership value J-ij , of a sentence i in child cluster j, is computed using Eq.</S>
			<S sid ="197" ssid = "126">(8).</S>
			<S sid ="198" ssid = "127">J- J-iC (8) ij = number of groups containing i where J-iC is the membership of the sentence in the original cluster C. Splitting of clusters can be continued recursively until all sentences within a cluster have a similarity not less than the given threshold.</S>
			<S sid ="199" ssid = "128">This continued splitting results in highly fragmented clusters and may produce several clusters with a single sentence.</S>
			<S sid ="200" ssid = "129">Hence, in this work recursive splitting has not been applied.</S>
			<S sid ="201" ssid = "130">After divisive clustering, each cluster contains sentences with the same/similar nouns and verbs.</S>
			<S sid ="202" ssid = "131">3.4.</S>
			<S sid ="203" ssid = "132">Paraphrase Recognition.</S>
			<S sid ="204" ssid = "133">In the second phase of Paraphrase Extraction, the sentences within each cluster must be processed to identify paraphrases.</S>
			<S sid ="205" ssid = "134">Sentences which share the same verbs and nouns need not necessarily be paraphrases.</S>
			<S sid ="206" ssid = "135">Therefore an SVM based Paraphrase Recognizer [25] has been used to identify the paraphrases within each cluster.</S>
			<S sid ="207" ssid = "136">Of the existing approaches for Paraphrase Recognition, machine learning techniques, notably SVMs have achieved considerable success [1] by extracting various features from the input sentences.</S>
			<S sid ="208" ssid = "137">Lexical, syntactic and semantic features have been previously used both individually and in various combinations for detecting paraphrases [1,25].</S>
			<S sid ="209" ssid = "138">In order to extract only sentences which are very similar, the membership value of a sentence can be considered.</S>
			<S sid ="210" ssid = "139">A simple strategy would be to choose all sentences with membership greater than a threshold.</S>
			<S sid ="211" ssid = "140">A better tactic is to rank sentences by membership and choose only the top 50% as it avoids the usage of a threshold.</S>
			<S sid ="212" ssid = "141">3.4.1.</S>
			<S sid ="213" ssid = "142">Lexical features The adapted BiLingual Evaluation Understudy (BLEU) precision and recall metrics [26] have been calculated by considering the extent of unigram match with respect to each of the input sentences.</S>
			<S sid ="214" ssid = "143">Skipgrams of a sentence can be formed by considering both contiguous and non-contiguous n-grams [27].</S>
			<S sid ="215" ssid = "144">Skipgram precision and recall have been computed by dividing the number of common skipgrams by total possible number of Skipgrams constructed from each of the input sentences.</S>
			<S sid ="216" ssid = "145">The Longest Common Subsequence feature was evaluated by dividing the length of the longest common in-sequence portion by the length of the shorter sentence.</S>
			<S sid ="217" ssid = "146">3.4.2.</S>
			<S sid ="218" ssid = "147">Syntactic features Stanford Parser developed by Klein and Manning [28] was used to construct dependency trees which are syntactic representations of a sentence as shown in Fig.</S>
			<S sid ="219" ssid = "148">8.</S>
			<S sid ="220" ssid = "149">Tree edit distance has been computed as the minimum number of operations required to transform one dependency tree into another [29].</S>
			<S sid ="221" ssid = "150">For every edge in a dependency tree a triple can be formed in terms of parent/headword, child/dependent and relationship between them.</S>
			<S sid ="222" ssid = "151">The number of shared triples between the sentences was divided by the number of triples in the ﬁrst and second sentence to obtain a pair of triple similarity measures [30].</S>
			<S sid ="223" ssid = "152">A total of 96 POSPER (Parts-Of-Speech Position independent word Error Rate) features [31] have been generated by taking into account the degree of matches and non-matches for each tag.</S>
			<S sid ="224" ssid = "153">3.4.3.</S>
			<S sid ="225" ssid = "154">Semantic features Four word similarity features have been evaluated by computing the Jiang–Conrath score between nouns, verbs, adverbs and adjectives.</S>
			<S sid ="226" ssid = "155">As in [32], features which assess the extent of cardinal number and proper noun match have been used.</S>
			<S sid ="227" ssid = "156">Additional negation features which consider the number of antonym occurrences as well as the presence of explicit negation terms have also been used.</S>
			<S sid ="228" ssid = "157">From earlier work carried out using a SVM classiﬁer on the MSRPC, it was observed that the choice of features has a significant effect on the performance of the Paraphrase Recognizer as shown in Table 1 [25].</S>
			<S sid ="229" ssid = "158">For discovering the best subset of features, Wrapper method of feature selection was employed by combining genetic algorithms with SVM classiﬁers.</S>
			<S sid ="230" ssid = "159">The best performance was obtained using 57 features out of the original set of 114 features.</S>
			<S sid ="231" ssid = "160">The selected features include: • All ﬁve lexical features.</S>
			<S sid ="232" ssid = "161">• Verb, adverb similarity and two of the negation features from the Semantic category.</S>
			<S sid ="233" ssid = "162">• Dependency tree edit distance, triple similarity function from the syntactic category in addition to POSPER features corresponding to simple and comparative adjectives, singular and plural nouns, particles, modals, ‘be’ forms of the verb.</S>
			<S sid ="234" ssid = "163">After feature extraction, for classifying the input sentences as paraphrases, a SVM classiﬁer has been used.</S>
			<S sid ="235" ssid = "164">The LibSVM tool [33] has been used for performing SVM classiﬁcation.</S>
			<S sid ="236" ssid = "165">A nu-classiﬁcation scheme with a Radial Basis Kernel function was employed.</S>
			<S sid ="237" ssid = "166">The classiﬁer model which yielded a high performance of 76.97% [25] on the test set of the MSRPC has been used for the current work.</S>
			<S sid ="238" ssid = "167">Candidate sentences extracted from each cluster are fed to the Paraphrase Recognizer and classiﬁed.</S>
			<S sid ="239" ssid = "168">With respect to the MSRVDC there are no annotations available on whether all the sentences describing the same video are paraphrases or not.</S>
			<S sid ="240" ssid = "169">As the MSRPC is a standard corpora suitable for Paraphrase Recognition evaluation, in this work the classiﬁer model constructed from the MSRPC training set has been used for the MSRVDC also.</S>
			<S sid ="241" ssid = "170">3.5.</S>
			<S sid ="242" ssid = "171">Grouping of paraphrases.</S>
			<S sid ="243" ssid = "172">Since the objective of this work is to extract all the equivalent sentences within a cluster, once the Paraphrase Recognizer categorizes the sentences they must be collected together.</S>
			<S sid ="244" ssid = "173">Paraphrasing is transitive in nature, that is if A ⇔ B and B ⇔ C, then A ⇔ C. Hence a chaining model is used to group the paraphrases within a cluster.</S>
	</SECTION>
	<SECTION title="Results. " number = "4">
			<S sid ="245" ssid = "1">Some of the factors which inﬂuence the evaluation of Paraphrase Extraction systems are: • One-on-one comparison of Paraphrase Extraction systems may not be possible as the systems may work on different types of corpora and extract units of different sizes.</S>
			<S sid ="246" ssid = "2">• The systems may be evaluated either in a stand-alone manner or in the context of speciﬁc tasks such as information extraction, query expansion, etc. • The efﬁciency of the underlying Paraphrase Recognition system.</S>
			<S sid ="247" ssid = "3">The best Paraphrase Recognition systems today have an accuracy of only 77% [1] which will deﬁnitely impact the task of Paraphrase Extraction.</S>
			<S sid ="248" ssid = "4">• Availability of benchmark corpora is yet another challenge.</S>
			<S sid ="249" ssid = "5">Though there are a few large-scale sentence-level paraphrase collections such as the MSRVDC, annotating the entire corpus is difﬁcult.</S>
			<S sid ="250" ssid = "6">• Choosing suitable evaluation metrics is another hurdle because of the difﬁculty in identifying set of all Positives and Negatives in large scale corpora.</S>
			<S sid ="251" ssid = "7">The fuzzy hierarchical clustering approach has been implemented in Java and the WordNet electronic dictionary has been used as an additional resource.</S>
			<S sid ="252" ssid = "8">The performance of the proposed approach has been evaluated on two different corpora: the MSRPC and the MSRVDC.</S>
			<S sid ="253" ssid = "9">The obtained results have been compared with that of the Paraphrase Acquisition system developed by Wubben et al. [9].</S>
			<S sid ="254" ssid = "10">4.1.</S>
			<S sid ="255" ssid = "11">Evaluation metrics.</S>
			<S sid ="256" ssid = "12">In addition to two traditional Paraphrase Extraction evaluation measures – precision and relative-recall [2], clustering measures Burke said Mr landing nn(Burke3, Said-1) nn(Burke3, Mr2) nsubj(landing 8, it-4) cop(landing-8, be-5) det(landing-8, a-6) it be a textbook consider circumstance the nn(landing-8, textbook-7) dep(Burke3, landing-8) partmod(landing-8, consider-9) det(circumstance 11, the-10) dobj(consider-9, circumstance-11) Fig.</S>
			<S sid ="257" ssid = "13">8.</S>
			<S sid ="258" ssid = "14">Dependency parse and triples for the sentence – “Mr Burke said it was a textbook landing considering the circumstances”.</S>
			<S sid ="259" ssid = "15">Table 1 Performance of Paraphrase Recognition system on MSRPC [25].</S>
			<S sid ="260" ssid = "16">Features Number of features Accuracy % Precision % Recall % F-Measure % Lexical 573.6 873.4 094.7 782.7 2 Syntactic 10 0 73 .3 3 74 .3 8 91 .3 7 82 .0 0 Se ma ntic 9 68 .4 1 69 .0 3 95 .2 0 80 .0 3 Lexi cal, POS PER, NV Sim, pro per nou n 10 4 75 .3 6 76 .2 91 .5 4 83 .1 7 All lexi cal, synt acti c and sem anti c feat ures exc ept card inal nu mb er mat ch 11 3 75 .5 8 79 .8 3 86 .5 9 83 .0 7 Fea ture s sele cted usin g GA SV M met hod 5 7 76 .9 7 80 .4 7 88 .0 9 84 .1 1 which are used to evaluate the goodness of clusters have also been used here.</S>
			<S sid ="261" ssid = "17">Precision is deﬁned as the number of true paraphrases out of the total number of sentences declared as paraphrases.</S>
			<S sid ="262" ssid = "18">It is calculated using Eq.</S>
			<S sid ="263" ssid = "19">(9).</S>
			<S sid ="264" ssid = "20">Entropy of a cluster Ei is a measure of disorder and measures how the classes are distributed within a cluster [16].</S>
			<S sid ="265" ssid = "21">The overall entropy E is the weighted average of the individual cluster entropies.</S>
			<S sid ="266" ssid = "22">1 |K | C K C K TP Precision = TP ( 9) FP Ei = − log |K | ) | i ∩ Kj j=1 j | log | i ∩ j | Kj (13) where TP refers to True Positives and FP stands for False Positives.</S>
			<S sid ="267" ssid = "23">Due to the non-availability of decisions (paraphrase/non- paraphrase) for all the possible sentence pairs from the MSRPC, TP and FP are assessed only with respect to the annotated sentence 1 E = N |C | ) i=1 (|Ci | · Ei ) (14) pairs.</S>
			<S sid ="268" ssid = "24">The other metric Relative Recall (RR) has been adapted from the Information Retrieval version [34] and assesses the recall of the current system S with respect to other systems as given in Eq.</S>
			<S sid ="269" ssid = "25">(10).</S>
			<S sid ="270" ssid = "26">This measure has been used due to limited information on all the possible relevant paraphrase pairs.</S>
			<S sid ="271" ssid = "27">Number of relevant pairs extracted by Sv-Measure focuses on both homogeneity as well as complete ness, which is the extent to which all objects from a single class are assigned to a single cluster.</S>
			<S sid ="272" ssid = "28">It is deﬁned as the harmonic mean of homogeneity h and completeness c which are calculated in terms of entropies H(K), H(C) and conditional entropies H(K|C) and H(C|K).</S>
			<S sid ="273" ssid = "29">By varying ˇ, the preference given to h and c can be controlled; if both are equally important, ˇ is set to 1 [35].</S>
			<S sid ="274" ssid = "30">(ˇ + 1)hc relative recallS = Cumulative set of relevant pairs extracted by all systems (10) V = (ˇh + c) H(K |C ) (15) Purity, entropy and v-measure are the popular measures used to assess the quality of clustering.</S>
			<S sid ="275" ssid = "31">Purity of a cluster is the fraction of one class, to the number of objects in the cluster [16].</S>
			<S sid ="276" ssid = "32">Given that C represents the set of clusters and K the set of reference classes h = 1 − c = 1 − H(K ) H(C |K ) H(C ) (16) (17) purity of a cluster Ci is: 1 Besides the above measures, the Partition Coefﬁcient (PC) has been used speciﬁcally with reference to fuzzy clustering.</S>
			<S sid ="277" ssid = "33">The parti Pi = |Ci | maxj (|Ci ∩ Kj |) (11) tion coefﬁcient measure given in Eq.</S>
			<S sid ="278" ssid = "34">(18) quantiﬁes the fuzziness of a partition with higher values which indicate least fuzzy clustering, being preferable.</S>
			<S sid ="279" ssid = "35">The overall purity (P) for a set of clusters is the weighted sum of the purity of the individual clusters, where N is the total number of sentences.</S>
			<S sid ="280" ssid = "36">|C | N PC 1 )) = N i=1 j=1 J-2 (18) 1 P = N |C | ) i=1 (|Ci | · Pi ) (12) In Eq.</S>
			<S sid ="281" ssid = "37">(18), N represents the total number of objects and J-ij is the membership value of object j in cluster i. News headlines have ﬁrst been re-clustered into ﬁner sub-clusters similarity was between thresholds, the context in which the headline occurred was used to decide.</S>
			<S sid ="282" ssid = "38">The cosine similarity approach has been used by Wubben as a baseline.</S>
			<S sid ="283" ssid = "39">For the current performance evaluation, both systems proposed by Wubben have been used with modiﬁcations.</S>
			<S sid ="284" ssid = "40">In k-means clustering approach, PK3 cluster stopping criteria has been used in place of PK1, as PK3 has been reported to be more efﬁcient than PK1 [38].</S>
			<S sid ="285" ssid = "41">Since no clusters exist initially, k-means clustering has straightaway been applied on the entire corpus.</S>
			<S sid ="286" ssid = "42">In the pairwise cosine similarity computation, due to lack of context when dealing with stand-alone sentences, a single threshold value has been used.</S>
			<S sid ="287" ssid = "43">If the similarity exceeds the threshold then the sentence pair is accepted as equivalent and rejected otherwise.</S>
			<S sid ="288" ssid = "44">Besides Wubben et al.’s systems, a Fuzzy C-Means (FCM) clustering approach has also been adopted.</S>
			<S sid ="289" ssid = "45">Wubben et al. have attempted k-means clustering, while the proposed system uses fuzzy clustering.</S>
			<S sid ="290" ssid = "46">In order to obtain a comprehensive performance evaluation the FCM approach has also been implemented using the R package.</S>
			<S sid ="291" ssid = "47">The optimal number of clusters has been identiﬁed by choosing the partition which yields highest partition coefﬁcient and lowest classiﬁcation entropy.</S>
			<S sid ="292" ssid = "48">4.3.</S>
			<S sid ="293" ssid = "49">Experiments on MSRPC.</S>
			<S sid ="294" ssid = "50">The MSRPC consists of 5801 pairs of sentences and is best suited for evaluating the performance of Paraphrase Recognition systems.</S>
			<S sid ="295" ssid = "51">It consists of 3900 positive cases of paraphrases and 1901 negative cases [39].</S>
			<S sid ="296" ssid = "52">In order to use it for Paraphrase Extraction, the corpus has been viewed as a collection of individual sentences.</S>
			<S sid ="297" ssid = "53">The 10,948 unique sentences out of 11,602 sentences were taken up for further processing after assigning unique IDs.</S>
			<S sid ="298" ssid = "54">Since MSRPC contains pairs of paraphrases rather than groups, only precision and relative recall have been calculated.</S>
			<S sid ="299" ssid = "55">For testing the performance of the proposed system, the sentences were subject to Fuzzy Agglomerative Clustering followed by divisive clustering, both with and without WSD.</S>
			<S sid ="300" ssid = "56">As described in Sections 3.2 and 3.3, the thresholds for merging and splitting of clusters were chosen as 0.15 and 0.2.</S>
			<S sid ="301" ssid = "57">Candidate sentences were chosen from each cluster based on the two schemes: all sentences and 50% of sentences from each cluster based on the membership values.</S>
			<S sid ="302" ssid = "58">Using these choices, eight variants of the proposed system are framed, namely: With and Without WSD, threshold of 0.15 and 0.2, using all sentences in a cluster and only the top 50%.</S>
			<S sid ="303" ssid = "59">The best set of features (57 features) identiﬁed in the feature selection process (Section 3.4) were extracted from each pair of sentences and passed on to the Paraphrase Recognizer to classify the sentences.</S>
			<S sid ="304" ssid = "60">Since the MSRPC contains only paraphrase pairs, the chaining of paraphrases described in Section 3.5 has been omitted.</S>
			<S sid ="305" ssid = "61">Eq.</S>
			<S sid ="306" ssid = "62">(9) has been used to assess the precision by ﬁrst picking the set of sentence pairs with a decision from the complete set of retrieved pairs.</S>
			<S sid ="307" ssid = "63">The positive cases retrieved are counted as True Positives whereas the non-paraphrased pairs retrieved are considered as False Positives.</S>
			<S sid ="308" ssid = "64">Table 2 presents the performance of the existing system as well as the proposed system variants in terms of pairs with a known decision, True Positives and False Positives.</S>
			<S sid ="309" ssid = "65">The system which involves direct computation of cosine similarity was tested with ﬁve different thresholds starting from 0.3 and an increment of 0.1.</S>
			<S sid ="310" ssid = "66">The value of T represents the cutoff threshold used in the cosine similarity system and the semantic similarity threshold for the proposed system variants.</S>
			<S sid ="311" ssid = "67">The cosine similarity based system with low thresholds is found to retrieve a larger number of known pairs followed by the k-means approach.</S>
			<S sid ="312" ssid = "68">Though it has reported a very large number of possible pairs, the FCM approach was found to retrieve the least number of known pairs.</S>
			<S sid ="313" ssid = "69">The WSD based variants of the proposed system retrieve more candidates due to the tighter clusters based on speciﬁc word senses.</S>
			<S sid ="314" ssid = "70">The precision of the existing approaches: cosine similarity, k-means, FCM Clustering as well as all the proposed system variants have been presented in Table 3.</S>
			<S sid ="315" ssid = "71">The proposed system outscores the existing system variants in terms of precision by combining the fuzzy clustering approach with a Paraphrase Recognizer which exploits lexical, syntactic and semantic features.</S>
			<S sid ="316" ssid = "72">The best precision is registered by the most rigorous system: With WSD, threshold = 0.2 and extracting only top 50% sentences.</S>
			<S sid ="317" ssid = "73">Since these options result in ﬁner clusters, precision is better.</S>
			<S sid ="318" ssid = "74">From Table 3 the following inferences can be drawn: • Using WSD option yields better precision across all cases.</S>
			<S sid ="319" ssid = "75">This can be attributed to the fact that, by using WSD a distinction can be made among the sentences having the same verb.</S>
			<S sid ="320" ssid = "76">This results in ﬁner clusters and hence during Paraphrase Extraction the number of true positives is higher.</S>
			<S sid ="321" ssid = "77">• With respect to similarity thresholds, since both the thresholds yield comparable performance, any one of the thresholds can be used in the proposed system.</S>
			<S sid ="322" ssid = "78">This observation is in line with the study conducted on the Miller–Charles dataset and Yang–Powers dataset for ﬁxing the thresholds.• In terms of the membership values of sentences within a clus ter, the strategy of using only the top ranking 50% yields slightly better precision than using all the sentences.</S>
			<S sid ="323" ssid = "79">This shows that the precision is affected only by the overall clustering and not by the speciﬁc membership values.</S>
			<S sid ="324" ssid = "80">Table 3 Precision of existing and proposed approaches.</S>
			<S sid ="325" ssid = "81">Proposed system variants No WSD Threshold 0.15 Threshold 0.2 WSD Threshold 0.15 Threshold 0.2 The results of the four systems which are considered for further evaluation (Fig.</S>
			<S sid ="326" ssid = "82">9) have been given in bold.</S>
			<S sid ="327" ssid = "83">Table 4 Relative recall evaluation.</S>
			<S sid ="328" ssid = "84">System Relative recall k-Means clustering 0.78 Proposed system variant – WSD, threshold = 0.2, top 50% 0.73 Cosine similarity – threshold = 0.7 0.64 FCM clustering 0.11 The relative recall has been assessed with respect to four systems namely: • Best proposed system variant using WSD option, threshold of 0.2 and top 50% of sentences.</S>
			<S sid ="329" ssid = "85">• k-Means approach [9].</S>
			<S sid ="330" ssid = "86">• Cosine similarity variant [9] using threshold = 0.7.</S>
			<S sid ="331" ssid = "87">• FCM Clustering approach.</S>
			<S sid ="332" ssid = "88">The relative recall computation has also been carried out according to Eq.</S>
			<S sid ="333" ssid = "89">(10) with respect to known relevant pairs or True Positives (Table 2) retrieved by the systems.</S>
			<S sid ="334" ssid = "90">The results are presented in Table 4 and the k-means approach exhibits the highest relative recall.</S>
			<S sid ="335" ssid = "91">This is followed by the proposed system variant whereas the FCM Clustering approach has least relative recall.</S>
			<S sid ="336" ssid = "92">The system which retrieves most pairs is found to have higher relative recall as there is a greater scope for containing the relevant pairs.</S>
			<S sid ="337" ssid = "93">From the results it is obvious that the proposed system has yielded the best precision and also possesses reasonable relative recall when compared to the other variants.</S>
			<S sid ="338" ssid = "94">Since the objective of the proposed system is to extract paraphrases from large corpora, an unsupervised clustering approach has been used to ﬁrst cluster the sentences.</S>
			<S sid ="339" ssid = "95">In the second stage a supervised approach has been used to ﬁlter the paraphrases from within each cluster.</S>
			<S sid ="340" ssid = "96">Prior work using MSRPC have either relied on a purely supervised approach or in the case of unsupervised learning, have computed scores between each speciﬁc sentence pair only.</S>
			<S sid ="341" ssid = "97">In contrast this work considers all the sentences within the MSRPC corpus for determining paraphrases.</S>
			<S sid ="342" ssid = "98">Further, when the unsupervised approach is used a large number of sentence pairs are reported as paraphrases but the output decision is available only for 5801 pairs.</S>
			<S sid ="343" ssid = "99">Hence the evaluation results presented in Tables 3 and 4 have been computed using only the known pairs.</S>
			<S sid ="344" ssid = "100">Due to the above reasons a direct comparison of the results with that of previous work on MSRPC would deﬁnitely be biased with the unsupervised approaches which use speciﬁc pairwise comparison approaches having an advantage.</S>
			<S sid ="345" ssid = "101">However to identify the performance level of the proposed system, a brief comparison with prior work using unsupervised approaches on MSRPC has been presented in Table 5.</S>
			<S sid ="346" ssid = "102">It can be seen that the proposed system gives good results only with respect to precision.</S>
			<S sid ="347" ssid = "103">The results can be attributed to the fact that the previous unsupervised approaches – Fernando et al. [40] and Mihalcea et al. [41] judge only the given sentence pair and focus on Paraphrase Recognition only.</S>
			<S sid ="348" ssid = "104">However the proposed system undertakes the responsibility of ﬁrst identifying possible candidates through clustering and then judging them, which would be the requirement of a Paraphrase Extraction system.</S>
			<S sid ="349" ssid = "105">However, the proposed system performs better than Wubben et al.’s approaches as well as FCM Clustering for Paraphrase Extraction.</S>
			<S sid ="350" ssid = "106">Therefore, it can be concluded that the proposed system is suitable for the task of Paraphrase Extraction.</S>
			<S sid ="351" ssid = "107">4.4.</S>
			<S sid ="352" ssid = "108">Experiments on MSRVDC.</S>
			<S sid ="353" ssid = "109">The MSRVDC was constructed by Chen and Dolan [42] and consists of 120,000 sentences collected from multilingual descriptions of short video snippets supplied by workers on Mechanical Turk.</S>
			<S sid ="354" ssid = "110">Out of the 85,000 English sentences, 33,855 were from Tier-2 workers who had a higher rating.</S>
			<S sid ="355" ssid = "111">For evaluating the performance of the proposed system, two different datasets were constructed from MSRVDC.</S>
			<S sid ="356" ssid = "112">The ﬁrst dataset consists of 2007 sentences extracted from the 35 K sentences contributed by Tier-2 workers.</S>
			<S sid ="357" ssid = "113">Descriptions of the same video were treated as belonging to a single cluster.</S>
			<S sid ="358" ssid = "114">Duplicate sentences were eliminated from within each cluster and the major or repeated verbs in each cluster were identiﬁed.</S>
			<S sid ="359" ssid = "115">It was observed that many of the clusters described the same actions and involved the same entities.</S>
			<S sid ="360" ssid = "116">Hence in order to test the performance of the system in a controlled environment consisting of clusters with little or no overlap, it was decided to handpick a set of clusters and to limit the size of the dataset to around 2000 sentences.</S>
			<S sid ="361" ssid = "117">143 clusters involving distinct verbs were chosen and each cluster was inspected by two judges to eliminate sentences which did not agree with the overall theme of the cluster; disagreement between judges was resolved by a third judge.</S>
			<S sid ="362" ssid = "118">The proposed system, the existing systems (Wubben et al.) and FCM Clustering approach were tested on this dataset.</S>
			<S sid ="363" ssid = "119">For implementing the proposed system, after fuzzy clustering, all the candidate sentences within a cluster were classiﬁed by the Paraphrase Recognizer and the positive pairs were chained together as described in Section 3.5 to generate clusters of paraphrases.</S>
			<S sid ="364" ssid = "120">As in the case of MSRPC, various implementations of the proposed system with respect to similarity threshold, sentence membership and WSD were investigated.</S>
			<S sid ="365" ssid = "121">Since the focus is on extracting groups of paraphrases, the second set of performance evaluation measures namely entropy, purity and v-measure were computed as shown in Table 6.</S>
			<S sid ="366" ssid = "122">The 143 clusters identiﬁed were ﬁxed as reference classes against which the created clusters have been judged.</S>
			<S sid ="367" ssid = "123">In terms of entropy, the best performance was registered by the rigorous system, with WSD, threshold 0.2 and using only the Table 5 Comparative performance on MSRPC.</S>
			<S sid ="368" ssid = "124">Approach Accuracy % Precision % Recall % F-Measure % Fernando et al.</S>
			<S sid ="369" ssid = "125">(2008) [40] 74.1 75.2 91.3 82.4 Mihalcea et al.</S>
			<S sid ="370" ssid = "126">(2006) [41] 70.3 69.6 97.7 81.3 Proposed system variant WSD, threshold = 0.2, top 50% 66.4 81.0 65.2 72.3 Cosine similarity, threshold = 0.7 (Wubben et al. [9]) 62.5 80.5 56.6 66.5 k-Means clustering (Wubben et al. [9]) 60.6 70.6 71.0 70.8 FCM clustering 36.2 68.1 9.5 16.7 Table 6 Performance of proposed system on MSRVDC Dataset 1.</S>
			<S sid ="371" ssid = "127">Without WSD With WSD Threshold = 0.15 Threshold = 0.2 Threshold = 0.15 Threshold = 0.2 Top 50% All Top 50% All Top 50% All Top 50% All Entropy % 5.68 10.98 5.55 10.9 5.06 6.88 4.7 56.86 Purity % 64.82 79.42 65.62 78.77 63.08 70.8 61.34 70.05 v-Measure % 83.87 83.05 84.04 82.97 83.98 84.</S>
			<S sid ="372" ssid = "128">14 84.25 84.3 The highest value for each evaluation measure has been given in bold.</S>
			<S sid ="373" ssid = "129">top 50% in terms of membership values.</S>
			<S sid ="374" ssid = "130">Using WSD forms several smaller clusters and therefore the entropy is lesser.</S>
			<S sid ="375" ssid = "131">Likewise using a threshold of 0.2 and only top 50% of sentences is more restrictive, hence the entropy is lower.</S>
			<S sid ="376" ssid = "132">On the other hand, the most lenient system that is the one without WSD, threshold = 0.15 and considering all sentences within a cluster, has the highest purity.</S>
			<S sid ="377" ssid = "133">This can be attributed to the fact that this variant, results in the formation of lesser number of large clusters which tend to have a greater degree of overlap with reference classes.</S>
			<S sid ="378" ssid = "134">With respect to the v-measure, the variant with WSD and threshold of 0.2 but considering all sentences has registered the best performance of 84.3%.</S>
			<S sid ="379" ssid = "135">Using a higher threshold with WSD improves the homogeneity, whereas including all sentences within a cluster increases the completeness.</S>
			<S sid ="380" ssid = "136">Since the rigorous system has very close v-measure, it is chosen as the best out of the eight variants as it has moderate performance with respect to purity and low entropy and high v-measure.</S>
			<S sid ="381" ssid = "137">Table 7 records the performance of the existing systems on MSRVDC Dataset1.</S>
			<S sid ="382" ssid = "138">It can be observed that similar to the results obtained on MSRPC, the cosine similarity method with threshold = 0.7, achieves better performance than k-means and FCM clustering techniques.</S>
			<S sid ="383" ssid = "139">Comparing proposed and existing systems, it can be seen that almost all the eight variants of the proposed system perform better with respect to all three parameters.</S>
			<S sid ="384" ssid = "140">Additionally the rigorous variant of the proposed fuzzy hierarchical clustering approach and the FCM approach were compared in terms of the partition coefﬁcient yielding values of 0.44 and 0.16, respectively.</S>
			<S sid ="385" ssid = "141">This indicates that the quality of fuzzy clustering is better in the proposed system.</S>
			<S sid ="386" ssid = "142">Hence it can be concluded that the proposed system is better at identifying groups of paraphrases due to reﬁnement of fuzzy clusters by using the Paraphrase Recognizer.</S>
			<S sid ="387" ssid = "143">The second dataset was also extracted from MSRVDC and consists of 27,291 unique sentences from 33,855 Tier-II English sentences.</S>
			<S sid ="388" ssid = "144">All sentences describing the same video were grouped into a cluster and a total of 1931 clusters were formed.</S>
			<S sid ="389" ssid = "145">Due to the large size of the corpus, only the lenient and rigorous variants of the proposed system have been evaluated against Wubben’s Clustering approach, cosine similarity system with threshold = 0.7 and FCM approach as shown in Table 8.</S>
			<S sid ="390" ssid = "146">The rigorous variant of the Proposed system, performs much better than the other systems in terms of entropy as well as v- measure.</S>
			<S sid ="391" ssid = "147">In terms of purity, the lenient variant performs well as in the case of Dataset 1, but the improvement in purity is less when compared to the increase in entropy.</S>
			<S sid ="392" ssid = "148">In terms of partition coef- ﬁcient also the rigorous variant performs better than the lenient variant as well as FCM Clustering.</S>
			<S sid ="393" ssid = "149">Therefore, the rigorous variant is chosen as the better of the two proposed system variants.</S>
			<S sid ="394" ssid = "150">Sample clusters produced by the various systems have been shown in Table 9.</S>
			<S sid ="395" ssid = "151">The clusters produced by the proposed system and cosine similarity approach tend to be more homogeneous.</S>
			<S sid ="396" ssid = "152">It can be observed that in the cluster produced by the proposed system all sentences involve the same verb – ‘reading’ and related nouns – teacher or woman.</S>
			<S sid ="397" ssid = "153">Though the cosine similarity clustering is better than the k-means approach, it has also considered only word matching and not the concepts.</S>
			<S sid ="398" ssid = "154">FCM Clustering has resulted in large-sized clusters of low purity and high entropy and has therefore not been included in Table 9.</S>
	</SECTION>
	<SECTION title="Discussion. " number = "5">
			<S sid ="399" ssid = "1">The proposed system has consistently exhibited better performance with respect to all the three datasets as shown in Fig.</S>
			<S sid ="400" ssid = "2">9.</S>
			<S sid ="401" ssid = "3">The rigorous variant has been chosen for comparison against the existing systems.</S>
			<S sid ="402" ssid = "4">Of the four systems, the two stage fuzzy clustering followed by Paraphrase Recognition approach performs best.</S>
			<S sid ="403" ssid = "5">The second best performance is demonstrated by Wubben’s cosine similarity approach.</S>
			<S sid ="404" ssid = "6">A signiﬁcant aspect is that, the improvement in performance of the proposed system is more with respect to MSRVDC Dataset 2, which is the biggest of the three datasets.</S>
			<S sid ="405" ssid = "7">With respect to k-means clustering, the determination of ideal k value for clustering is tedious for large corpora.</S>
			<S sid ="406" ssid = "8">Cosine similarity computation has a complexity of O (N2 ) where N is the number of sentences in the corpus.</S>
			<S sid ="407" ssid = "9">In fuzzy hierarchical clustering approach the number of clusters is determined automatically.</S>
			<S sid ="408" ssid = "10">Also, once the sentences are initially assigned to clusters in O (N) time all further operations are either within the clusters or between the cluster representatives.</S>
			<S sid ="409" ssid = "11">Therefore the efﬁciency of the fuzzy hierarchical clustering approach is better.</S>
			<S sid ="410" ssid = "12">The proposed approach also supports incremental clustering as a new sentence can be added to the clusters labeled with the most similar verbs.</S>
			<S sid ="411" ssid = "13">The major contribution of this work is the design of a novel fuzzy hierarchical clustering approach and its application for the task of Paraphrase Extraction.</S>
			<S sid ="412" ssid = "14">Though several sentence clustering approaches exist previously in information extraction and multi-document summarization applications, the uniqueness of this approach is its usage of a fuzzy hierarchical technique based on Sentence similarity.</S>
			<S sid ="413" ssid = "15">The approach has been tested on two different corpora and the following inferences can be drawn from the performance evaluation.</S>
			<S sid ="414" ssid = "16">k-Means clustering [9] FCM clustering Cosine similarity approach [9] with varying thresholds (T) T = 0.3 T = 0.4 T = 0.5 T = 0.6 T = 0.7 Entr opy % 1 9 . 6 8 3 2 . 6 9 9 . 5 6 9 9 . 5 8 9 3 . 4 8 6 3 . 8 6 1 0 . 2 7 Puri ty % 3 9 . 0 1 4 7 . 7 3 1 . 6 9 3 . 3 4 1 1 . 2 6 3 3 . 5 8 5 4 . 9 5v Me asur e % 8 0 . 0 3 6 7 . 3 3 0 . 1 7 0 . 1 5 1 1 . 3 6 3 3 . 8 9 8 4 . 0 3 The highest value for each evaluation measure has been given in bold.</S>
			<S sid ="415" ssid = "17">Table 8 Performance evaluation on MSRVDC Dataset 2.</S>
			<S sid ="416" ssid = "18">W i t h o u t W S D T h r e s h o l d = 0 . 1 5 , a l l W i t h W S D t h r e s h o l d = 0 . 2 , t o p 5 0 % k M e a n s c l u s t e r i n g C o s i n e s i m i l a r i t y T h r e s h o l d = 0 . 7 F C M c l u s t e r i n g Entr opy % 3 4 . 8 8 1 9 . 4 1 6 8 . 0 3 6 6 . 9 1 8 6 . 7 8 Puri ty % 4 4 . 0 7 4 1 . 3 0 2 . 4 2 6 . 9 7 0 . 7 8v Me asur e % 6 8 . 9 5 7 7 . 0 9 4 2 . 6 7 4 3 . 2 4 1 9 . 5 6 Part itio n coe fﬁci ent 0 . 7 8 0 . 9 8 – – 0 . 3 3 The highest value for each evaluation measure has been given in bold.</S>
			<S sid ="417" ssid = "19">Table 9 Sample clusters on MSRVDC Dataset 2.</S>
			<S sid ="418" ssid = "20">Proposed system clustering (sample clusterpurity = 0.82, entropy = 0.08) • A woman is reading something, while another woman measures the ﬁrst woman’s ankle with a tape measure • A female teacher reads to the class • A female teacher is reading out loud from a piece of paper • A teacher read a paper to the class • A teacher reads aloud from a piece of paper • A teacher reads to her class • A woman is holding a sheet of paper and reading the text • A woman is reading a piece of paper • The teacher is reading the paper • The teacher read from a paper to the class k-Means clustering (sample cluster purity = 0.11 (highest), entropy = 0.37 (lowest)) • A woman is applying a lotion to her hair • Workers are tending to the ﬁeld • A woman is dancing by the water • A woman is baking a ﬁsh in the pan • People sing and dance in a surreal scene • A woman trims a ﬂower plant • A cat is jumping into a cardboard box • A woman is straightening her hair • The boxers fought in the ring • A whale rises out of the water • A whale surfaces in the water • Two dogs are wrestling • The man did acrobatic jumps in his routine • Two women eat hamburgers in a cafe and chat • Two men are standing in a kitchen and one is talking on a cell phone • A boy and woman are playing catch with a pumpkin • Someone is putting sauce into a pan • A little kid dances Cosine sample cluster purity = 0.78, entropy = 0.07) • A group of cars are driving down a road • Several different kinds of racing cars are driving down a road • A group of deer are crossing a road • A group of deer cross the road in a forest • A group of deers are crossing road • A herd of caribou are crossing a road • A herd of deer are crossing a road • A herd of deer are crossing the street • A herd of deer cross a road Fig.</S>
			<S sid ="419" ssid = "21">9.</S>
			<S sid ="420" ssid = "22">Performance evaluation of proposed and existing systems.</S>
			<S sid ="421" ssid = "23">• The approach results in meaningful and cohesive clusters when compared to other clustering approaches such k-means and FCM.</S>
			<S sid ="422" ssid = "24">• It is computationally efﬁcient when compared to the cosine similarity approach and at the same time considers semantic similarity also.• The approach is incremental and can also be parallelized as divi sive clustering can be carried out on each cluster independently.</S>
			<S sid ="423" ssid = "25">• Therefore, the fuzzy hierarchical clustering approach is a viable alternative to existing distributional and bootstrapping approaches for Paraphrase Extraction and can be employed for sentence-level Paraphrase Extraction.</S>
	</SECTION>
	<SECTION title="Conclusions. " number = "6">
			<S sid ="424" ssid = "1">A two-stage approach centered on fuzzy clustering followed by Paraphrase Recognition has been proposed for Paraphrase Extraction.</S>
			<S sid ="425" ssid = "2">The signiﬁcant aspects of the approach are: usage of a soft hierarchical clustering which has scope for parallelism and the ability to perform incremental clustering.</S>
			<S sid ="426" ssid = "3">Further a novel fuzzy grouping strategy has been utilized for merging clusters which ensures faster clustering and ﬂatter hierarchies.</S>
			<S sid ="427" ssid = "4">The system has been evaluated on two different paraphrase corpora and has exhibited good performance in comparison to a cosine similarity technique as well as k-means and FCM Clustering approaches.</S>
			<S sid ="428" ssid = "5">The effect of applying WSD has also been investigated and is found to improve the performance.</S>
			<S sid ="429" ssid = "6">In future, the performance of the two- stage approach on larger corpora can be probed.</S>
			<S sid ="430" ssid = "7">The supervised classiﬁer used in the second stage for Paraphrase Recognition can be replaced by suitable paraphrase evaluation metrics enabling the entire process to work in an unsupervised fashion.</S>
			<S sid ="431" ssid = "8">The approach can also be adapted for tasks such as tweet clustering, plagiarism detection and multi-document summarization.</S>
	</SECTION>
</PAPER>
