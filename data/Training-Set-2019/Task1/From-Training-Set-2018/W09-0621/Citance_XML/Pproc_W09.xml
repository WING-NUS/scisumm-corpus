<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Previous work on paraphrase extraction and application has relied on either parallel datasets, or on distributional similarity met­ tics over large text corpora.</S>
		<S sid ="2" ssid = "2">Our approach combines these two orthogonal sources of in­ formation and directly integrates them into our paraphrasing system&apos;s log-linear model.</S>
		<S sid ="3" ssid = "3">We compare different distributional similar­ ity feature-sets and show significant improve­ ments in grarnmaticality and meaning reten­ tion on the example text-to-text generation task of sentence compression, achieving state­ of-the-art quality.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="4" ssid = "4">A wide variety of applications in natural language processing can be cast in terms of text-to-text gen­ eration.</S>
			<S sid ="5" ssid = "5">Given input in the form of natural lan­ guage, a text-to-text generation system produces natural language output that is subject to a set of constraints.</S>
			<S sid ="6" ssid = "6">Compression systems, for instance, pro­ duce shorter sentences.</S>
			<S sid ="7" ssid = "7">Paraphrases, i.e. differ­ ing textual realizations of the same meaning, are a crucial components of text-to-text generation sys­ tems, and have been successfully applied to tasks such as multi-document summarization (Barzilay et a!., 1999; Barzilay, 2003), query expansion (Arr­ ick and Tipirneni, 1999; Riezler eta!., 2007), ques­ tion answering (McKeown, 1979; Ravichandran and Hovy, 2002), sentence compression (Cohn and La­ pata, 2008; Zhao et a!., 2009), and simplification (Wubben et a!., 2012).</S>
			<S sid ="8" ssid = "8">Paraphrase collections for text-to-text generation have been extracted from a variety of different cor­pora.</S>
			<S sid ="9" ssid = "9">Several approaches rely on bilingual para!</S>
			<S sid ="10" ssid = "10">lei data (Bannard and CallisonBurch, 2005; Zhao et a!., 2008; CallisonBurch, 2008; Ganitkevitch et a!., 2011), while others leverage distributional meth­ ods on monolingual text corpora (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008).</S>
			<S sid ="11" ssid = "11">So far, how­ ever, ouly preliminary studies have been undertaken to combine the information from these two sources (Chan eta!., 2011).</S>
			<S sid ="12" ssid = "12">In this paper, we describe an extension of Gan­ itkevitch et a!.</S>
			<S sid ="13" ssid = "13">(2011)&apos;s bilingual data-based ap­ proach.</S>
			<S sid ="14" ssid = "14">We augment the bilingually-sourced para­ phrases using features based on monolingual distri­ butional similarity.</S>
			<S sid ="15" ssid = "15">More specifically: • We show that using monolingual distributional similarity features improves paraphrase quality beyond what we can achieve with features esti­ mated from bilingual data.</S>
			<S sid ="16" ssid = "16">• We define distributional similarity for para­ phrase patterns that contain constituent-level gaps, e.g. sim(one JJ instance of NP, a JJ case of NP).</S>
			<S sid ="17" ssid = "17">This generalizes over distributional similarity for contiguous phrases.</S>
			<S sid ="18" ssid = "18">• We compare different types of monolingual distributional information and show that they can be used to achieve siguificant improve­ ments in grammaticality.</S>
			<S sid ="19" ssid = "19">• Finally, we compare our method to several strong baselines on the text-to-text generation task of sentence compression.</S>
			<S sid ="20" ssid = "20">Our method shows state-of-the-art results, beating a purely bilingually sourced paraphrasing system.</S>
			<S sid ="21" ssid = "21">256 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 256264, Montreal, Canada, June 78, 2012.</S>
			<S sid ="22" ssid = "22">@2012 Association for Computational Linguistics ...their IJII!nsithe term would...</S>
			<S sid ="23" ssid = "23">\ t&apos; ......, ...ihre langfrisligen P*le wOrden...</S>
			<S sid ="24" ssid = "24">...&apos;*!ne sei,ne lang en e aufzuQeben...</S>
			<S sid ="25" ssid = "25">....----- : / ·-.·.</S>
			<S sid ="26" ssid = "26">-&quot;&apos;&apos; ...without giving up his long-lenn plans Figure 1:Pivot-based paraphrase extraction for con­ tiguous phrases.Two phrases translating to the same phrase in the foreign language are assumed to be paraphrases of one another.</S>
	</SECTION>
	<SECTION title="Background. " number = "2">
			<S sid ="27" ssid = "1">Approaches to paraphrase extraction differ based on their underlying data source.</S>
			<S sid ="28" ssid = "2">In Section 2.1we out­ line pivot-based paraphrase extraction from bilin­ gual data, while the contextual features used to de­ termine closeness in meaning in monolingual ap­ proaches is described in Section 2.2.</S>
			<S sid ="29" ssid = "3">2.1 Paraphrase Extraction via Pivoting.</S>
			<S sid ="30" ssid = "4">Following Ganitkevitch et al.</S>
			<S sid ="31" ssid = "5">(2011), we formulate our paraphrases as a syntactically annotated syn­ chronous context-free grammar (SCFG) (Abo and Ullman, 1972; Chiang, 2005).</S>
			<S sid ="32" ssid = "6">An SCFG rule has thefonn: Figure 2: Extraction of syntactic paraphrases via the pivoting approach: We aggregate over different sur­ face realizations, matching the lexicalized portions of the rule and generalizing over the nonterminals.</S>
			<S sid ="33" ssid = "7">To extract paraphrases we follow the intuition that two English strings e1 and e2 that translate to the same foreign string f can be assumed to have the same meaning,as illustrated in Figure 1.1 First, we use standard machine translation meth­ ods to extract a foreign-to-English translation gram­ mar from a bilingual parallel corpus (Koehn.</S>
			<S sid ="34" ssid = "8">2010).</S>
			<S sid ="35" ssid = "9">Then, for each pair of translation rules where the left-hand side C and foreign string f match: r1 = C -{!,e1, &quot;&apos;1,&apos;P1) r2 = C- {!,e2,&quot;&apos;2, 2), wepivot over f to create a paraphrase rule rp: r = C -{!, e,&quot;&apos;,&lt;jJ), where the left-hand side of the rule, C.is a nonter­ rp = C- (e1, e2,&quot;&apos;p 1c,Op), minal and the right-hand sides f and e are strings of terminal and nonterminal symbols.</S>
			<S sid ="36" ssid = "10">There is a one-to-one correspondency between the nontermi­ nals in f and e: each nonterminal symbol in f has to also appear in e.The function&quot;&apos; captures this bi­ jective mapping between the nonterminals.</S>
			<S sid ="37" ssid = "11">Drawing on machine translation terminology, we refer to f as the source and e as the target side of the rule.</S>
			<S sid ="38" ssid = "12">Each ruleis annotated with a feature vector of fea­ ture functions rp = {&apos;J&apos;l..·&apos;PN} that, using a corre­ sponding weight vector X, are combined in a log­ linear model to compute the cost of applying r: cost(r) = -L,\i log&apos;Pi· (1) i=l A wide variety of feature functions can be formu­ lated.</S>
			<S sid ="39" ssid = "13">We detail the feature-set used in our experi­ ments inSection 4.</S>
			<S sid ="40" ssid = "14">with a combined nonterminal correspondency func­ tion &quot;&apos;p· Not.e that the common source side f im­ pliesthat e1 and e2 share the same set of nontenninal symbols.</S>
			<S sid ="41" ssid = "15">The paraphrase feature vector q}p is computed from the translation feature vectors q}1 and2 by following the pivoting idea.</S>
			<S sid ="42" ssid = "16">For instance, we esti­ mate the conditional paraphrase probabilityp(e2le1) by margjnaJizing over all shared foreign-language translations f: p(e2let) = Lp{e2, /let) (2) I Lp(e2l/, et)P(flet) (3) I R1 LP(e2if)p(flet).</S>
			<S sid ="43" ssid = "17">(4) I 1See Yao et al.(2012) for an analysis of this assumption.</S>
			<S sid ="44" ssid = "18">Figure 3: An example of a synchronous paraphras­ tic derivation, here a sentence compression.</S>
			<S sid ="45" ssid = "19">Shaded words are deleted in the indicated rule applications.</S>
			<S sid ="46" ssid = "20">Figure 2 illustrates syntax-constrained pivoting and feature aggregation over multiple foreign language translations for a paraphrase pattern.</S>
			<S sid ="47" ssid = "21">After the SCFG has been extracted, it can be used within standard machine translation machinery, such as the Joshua decoder (Ganitkevitch et a!., 2012).</S>
			<S sid ="48" ssid = "22">Figure 3 shows an example for a synchronous para­ phrastic derivation produced as a result of applying our paraphrase grammar in the decoding process.</S>
			<S sid ="49" ssid = "23">The approach outlined relies on aligned bilingual texts to identify phrases and patterns that are equiva­ lent in meaning.</S>
			<S sid ="50" ssid = "24">When extracting paraphrases from monolingual text, we have to rely on an entirely dif­ ferent set of semantic cues and features.</S>
			<S sid ="51" ssid = "25">2.2 Monolingual Distributional Similarity.</S>
			<S sid ="52" ssid = "26">Methods based on monolingual text corpora mea­ sure the similarity of phrases based on contextual features.</S>
			<S sid ="53" ssid = "27">To describe a phrase e, we define a set of features that capture the context of an occurrence of e in our corpus.</S>
			<S sid ="54" ssid = "28">Writing the context vector for the i-th occurrence of e as se,i. we can aggregate over all occurrences of e, resulting in a distributional sig­ nature for e, S. = L; S.,i· Following the intuition that phrases with similar meanings occur in similar contexts, we can then quantify the goodness of e&apos; as a paraphrase of e by computing the cosine similarity between their distributional signatures: .</S>
			<S sid ="55" ssid = "29">( &apos;) Be ·Be&apos; linguistically informed feature-sets that rely on de­ pendency and constituency parses, part-of-speech tags, or lemmatization have been proposed in widely known work such as by Church and Hanks (1991) and Lin and Pantel (2001).</S>
			<S sid ="56" ssid = "30">For instance, a phrase is described by the various syntactic relations it has with lexical items in its context, such as: &quot;for what verbs do we see with the phrase as the subject?&quot;, or &quot;what adjectives modify the phrase?&quot;.</S>
			<S sid ="57" ssid = "31">However, when moving to vast text collections or collapsed representations of large text corpora, lin­ guistic annotations can become impractically expen­ sive to produce.</S>
			<S sid ="58" ssid = "32">A straightforward and widely used solution is to fall back onto lexical n-gram features, e.g. &quot;what words or bigrams have we seen to the left of this phrase?&quot;</S>
			<S sid ="59" ssid = "33">A substantial body of work has fo­ cussed on using this type of feature-set for a variety of purposes in NLP (Lapata and Keller, 2005; Bha­ gat and Ravichandran, 2008; Lin et a!., 2010; Van Durme and Lall, 2010).</S>
			<S sid ="60" ssid = "34">2.3 Other Related Work.</S>
			<S sid ="61" ssid = "35">Recently, Chan eta!.</S>
			<S sid ="62" ssid = "36">(2011) presented an initial in­ vestigation into combining phrasal paraphrases ob­ tained through bilingual pivoting with monolingual distributional information.</S>
			<S sid ="63" ssid = "37">Their work investigated a reranking approach and evaluated their method via a substitution task, showing that the two sources of information are complementary and can yield im­ provements in paraphrase quality when combined.</S>
	</SECTION>
	<SECTION title="Incorporating Distributional Similarity. " number = "3">
			<S sid ="64" ssid = "1">In order to incorporate distributional similarity in­ formation into the paraphrasing system, we need to calculate similarity scores for the paraphrastic SCFG rules in our grammar.</S>
			<S sid ="65" ssid = "2">For rules with purely lexical right-hand sides e1 and e2 this is a simple task, and the similarity score sim(e1, e2) can be di­ rectly included in the rule&apos;s feature vector &apos;P.</S>
			<S sid ="66" ssid = "3">How­ ever, if e1 and e2 are long, their occurrences become sparse and their similarity can no longer be reliably estimated.</S>
			<S sid ="67" ssid = "4">In our case, the right-hand sides of our rules often contain gaps and computing a similarity szm e,e = . Se Se&apos; 1 score is less straightforward.</S>
			<S sid ="68" ssid = "5">Figure 4 shows an example of such a discontin­ A wide variety of features have been used to de­ scribe the distributional context of a phrase.</S>
			<S sid ="69" ssid = "6">Rich, uous rule and illustrates our solution: we decom­ pose the discontinuous patterns that make up the NP---·i&gt; .</S>
			<S sid ="70" ssid = "7">( ) _ 1( .</S>
			<S sid ="71" ssid = "8">( the long-term ) .</S>
			<S sid ="72" ssid = "9">(&apos;s)) Slm r - Slm in the long term nm of Figure 4: Scoring a rule by extracting and scoring contiguous phrases consistent with the alignment.</S>
			<S sid ="73" ssid = "10">The overall score of the rule is detennined by av­ eraging across all pairs of contiguous subphrases.</S>
			<S sid ="74" ssid = "11">right-hand sides of a role r into pairs of contiguous phrases 1&apos;(r) = {(e, e&apos;)}.</S>
			<S sid ="75" ssid = "12">for which we can look up distributional signatures and compute similarity scores.</S>
			<S sid ="76" ssid = "13">This decomposition into phrases is non­ trivial, since our sentential paraphrase roles often involve significant reordering or structural changes.</S>
			<S sid ="77" ssid = "14">To avoid comparing unrelated phrase pairs, were­ quire 1&apos;(r) to be consistent with a token alignment a. The alignment is defined analogously to word alignments in machine translation.</S>
			<S sid ="78" ssid = "15">and computed by treating the source and target sides of our paraphrase roles as a parallel corpus.</S>
			<S sid ="79" ssid = "16">We define the overall similarity score of the role to be the average of the similarity scores of all ex­ tracted phrase pairs: 1 sim(r,a)= IP(a)l sim(e,e&apos;).</S>
			<S sid ="80" ssid = "17">towards comparing the substitutability and context similarity of a pair of paraphrases.</S>
			<S sid ="81" ssid = "18">Our two similarity scores are incorporated into the paraphraser as additional rule features in cp, simngram and simllJifl• respectively.</S>
			<S sid ="82" ssid = "19">We estimate the corresponding weights along withthe otheras de­ tailed in Section 4.</S>
	</SECTION>
	<SECTION title="Experimental Setup. " number = "4">
			<S sid ="83" ssid = "1">4.1 &apos;IBsk:Sentence Compression To evaluate our method on a real text-to-text appli­ cation, we use the sentence compression task.</S>
			<S sid ="84" ssid = "2">To tune the parameters of our paraphrase system for sentence compression, we need an appropriate cor­ pus of reference compressions.</S>
			<S sid ="85" ssid = "3">Since our model is designed to compress by paraphrasing rather than deletion, the commonly used deletion-based com­ pression data sets like the ZiffDavis corpus are not suitable.</S>
			<S sid ="86" ssid = "4">We thus use the dataset introduced in our previous work (Ganitkevitch et al., 2011).</S>
			<S sid ="87" ssid = "5">Beginning with 9570 tuples of parallel Engli• English sentences obtained from multiple reference translations for machine translation evaluation, we construct a parallel compression corpus by select­ ing the longest reference in each tuple as the source sentence and the shortest reference as the target sen­ tence.</S>
			<S sid ="88" ssid = "6">We further retain only those sentence pairs where the compression ratio cr falls in the range 0.5 &lt; cr 0.8.</S>
			<S sid ="89" ssid = "7">From these, we select 936 sen­ tences for the development set, as well as 560 sen­ tences for a test set that we use to gauge the perfor­ (e,e&apos;)E&apos;P(a) Since the distributional signatures for long, rare phrases may be computed from only a handful of occurrences, we additionally query for the shorter sub-phrases that are more likely to have been ob­ served often enough to have reliable signatures and thus similarity estimates.</S>
			<S sid ="90" ssid = "8">Our definition of the similarity of two discon­ tinuous phrases substantially differs from others in the literature.</S>
			<S sid ="91" ssid = "9">This difference is due to a differ­ ence in motivation.</S>
			<S sid ="92" ssid = "10">Un and Pantel (2001), for in­ stance, seek to find new paraphrasepairs by compar­ ing their arguments.</S>
			<S sid ="93" ssid = "11">In this work.</S>
			<S sid ="94" ssid = "12">however, we try to add orthogonal information to existing paraphrase pairs.</S>
			<S sid ="95" ssid = "13">Both our definition of pattern similarity and our feature-set (see Section 4.3) are therefore geared mance of our system.</S>
			<S sid ="96" ssid = "14">We contrast our distributional similarity-informed paraphrase system with a pivoting-only baseline, as well as an implementation of Clarke and Lapata (2008)&apos;s state--of-the-art compression model which uses a series of constraints in an integer linear pro­ gramming (ILP) solver.</S>
			<S sid ="97" ssid = "15">4.2 Baseline Paraphrase Grammar.</S>
			<S sid ="98" ssid = "16">We extract our paraphrase grammar from the French-English portion of the Europarl corpus (ver­ sion 5) (Koehn, 2005).</S>
			<S sid ="99" ssid = "17">The Berkeley aligner (Liang et al., 2006) and the Berkeley parser (Petrov and Klein, 2007) are used to align the bitext and parse the English side, respectively.The paraphrase gram­ mar is produced using the Hadoop-based Thrax Left 26 ,.....-;-;---l-----:---.1 Right goals 23.....................-.-.: : - -SR 11&apos;.:••..,.</S>
			<S sid ="100" ssid = "18">..... •••• ••.amocf·· . holdlrQ on tO the long.:term Investment VBG IN TO DT J J NN Figure 5: An example of the n-gra.m feature extrac­ tion on ann-gram corpus.</S>
			<S sid ="101" ssid = "19">Here, &apos;&apos;the long-term&quot; is seen preceded by &apos;&apos;revise&quot; (43 times) and followed by &apos;&apos;plans., (97 times).</S>
			<S sid ="102" ssid = "20">The corresponding left- and right-side features are added to the phrase signature with the counts of the n-gra.ms that gave rise to them.</S>
			<S sid ="103" ssid = "21">grammar extractor&apos;s paraphrase mode (Ganitkevitch et al., 2012).</S>
			<S sid ="104" ssid = "22">The syntactic nonterminallabels we allowed in the grammar were limited to constituent labels and CCG-style slashed categories.Paraphrase grammars extracted via pivoting tend to grow very large.</S>
			<S sid ="105" ssid = "23">To keep the grammar size manageable, we pruned away all paraphrase rules whose phrasal paraphrase probabilities p(e1le2) or p(e2le1) were smaller than 0.001.</S>
			<S sid ="106" ssid = "24">We extend the feature-set used in Ganitkevitch et al.</S>
			<S sid ="107" ssid = "25">(2011) with a number of features that aim to bet­ ter describe a rule&apos;s compressive power: on top of the word count features wcountsrc and wcounttgt and the word count difference feature wcountdiJJ, we addcharacter based count and difference features ccount.</S>
			<S sid ="108" ssid = "26">rc.ccounttgt• and ccountdiff• as well as log­ compression ratio features word cr = log =::!!!!</S>
			<S sid ="109" ssid = "27">and the analogously defined charcr = log ::;:!;:.</S>
			<S sid ="110" ssid = "28">For model tuning and decoding we used the Joshua machine translation system (Ganitkevitch et al., 2012).The model weights are estimated using an implementation of the PRO tuning algorithm (Hop­ kins and May, 2011), with PRECIS as our objective function (Ganitkevitch et al., 2011).</S>
			<S sid ="111" ssid = "29">The language model used in our paraphraser and the Clarke and Lapata (2008) baseline system is a KneserNey dis­ counted 5-gram model estimated on the Gigaword corpus using the SRILM toolkit (Stolcke, 2002).</S>
			<S sid ="112" ssid = "30">Figure 6: An example of the syntactic feature­ set The phrase &apos;&apos;the long-term&quot; is annotated with position-aware lexical and part-of-speech n-gra.m features (e.g. &quot;on to&quot; on the left, and &apos;&apos;investment&quot; and &quot;NN&quot; to its right), labeled dependency links (e.g. amod- investment) and features derived from the phrase&apos;s CCG label NP /NN.</S>
			<S sid ="113" ssid = "31">4.3 Distributional Similarlty Model.</S>
			<S sid ="114" ssid = "32">To investigate the impact of the feature-set used to construct distributional signatures, we contrast two approaches: a high-coverage collection of distribu­ tional signatures with a relatively simple feature-set, and a much smaller set of signatures with a rich, syn­ tactically informed feature-set.</S>
			<S sid ="115" ssid = "33">4.3.1 n-gram Model The high-coverage model (from here on: n-gram model) is drawn from a web-scale n-gram corpus (Brants and Franz, 2006; Lin et al., 2010).</S>
			<S sid ="116" ssid = "34">We ex­ tract signatures for phrases up to a length of 4.</S>
			<S sid ="117" ssid = "35">For each phrase p we look at n-grams of the form wp and pv, where w and v are single words.</S>
			<S sid ="118" ssid = "36">We then extract the corresponding features Wteft and Vnght· The feature count is set to the count of the n-gram, reflecting the frequency with which p was preceded or followed, respectively, by w and v in the data the n-gra.m corpus is based on.</S>
			<S sid ="119" ssid = "37">Figure 5 illustrates this feature extraction approach.The resulting collection comprises distributional signatures for the 200 mil­ lion most frequent 1-to-4-grams in the n-gram cor­ pus.</S>
			<S sid ="120" ssid = "38">4.3.2 Syntactic Model For the syntactically informed signature model (from here on: syntax model), we use the constituency and dependency parses provided in the Annotated Gigaword corpus (Napoles et al., 2012).</S>
			<S sid ="121" ssid = "39">We limit ourselves to the Los Angeles Times/Washington Post portion of the corpus and extract phrases up to a length of 4.</S>
			<S sid ="122" ssid = "40">The following feature set is used to compute distributional signa­ tures for the extracted phrases: • Position-aware lexical and part-of-speech uni­ gram and bigram features, drawn from a three­ word window to the right and left of the phrase.</S>
			<S sid ="123" ssid = "41">• Features based on dependencies for both links into and out of the phrase, labeled with the cor­ responding lexical item and POS.</S>
			<S sid ="124" ssid = "42">If the phrase corresponds to a complete subtree in the con­ stituency parse we additionally include lexical and POS features for its head word.</S>
			<S sid ="125" ssid = "43">• Syntactic features for any constituents govern­ ing the phrase, as well as for CCG-style slashed constituent labels for the phrase.</S>
			<S sid ="126" ssid = "44">The latter are split in governing constituent and missing con­ stituent (with directionality).</S>
			<S sid ="127" ssid = "45">Fignre 6 illustrates the syntax model&apos;s feature ex­ traction for an example phrase occurrence.</S>
			<S sid ="128" ssid = "46">Using this method we extract distributional signatures for over 12 million 1-to-4-gram phrases.</S>
			<S sid ="129" ssid = "47">4.3.3 Locality Sensitive Hashing Collecting distributional signatures for a large number of phrases quickly leads to unmanageably large datasets.</S>
			<S sid ="130" ssid = "48">Storing the syntax model&apos;s 12 mil­ lion signatures in a compressed readable format, for instance, requires over 20GB of disk space.</S>
			<S sid ="131" ssid = "49">Uke Ravichandran et al.</S>
			<S sid ="132" ssid = "50">(2005) and Bhagat and Ravichandran (2008), we rely on locality sensitive hashing (LSH) to make the use of these large collec­ tions practical.</S>
			<S sid ="133" ssid = "51">In order to avoid explicitly computing the fea­ ture vectors, which can be memory intensive for fre­ quent phrases, we chose the online LSH variant de­ scribed by Van Dunne and Lall (2010), as imple­ mented in the Jerboa toolkit (Van Dunne, 2012).</S>
			<S sid ="134" ssid = "52">This method, based on the earlier work of Indyk and Motwaui (1998) and Charikar (2002), approximates the cosine similarity between two feature vectors based on the Harurning distance in a dimensionality­ reduced bitwise representation.</S>
			<S sid ="135" ssid = "53">Two feature vec­ tors u, v each of dimension d are first projected through a d x b random matrix populated with draws from N(O,1).</S>
			<S sid ="136" ssid = "54">We then convert the resulting b­ dimensional vectors into bit-vectors by setting each bit of the signature conditioned on whether the cor­ responding projected value is less than 0.</S>
			<S sid ="137" ssid = "55">Now, given the bit signatures h(il) and h(V), we can ap­ proximate the cosine similarity of u and v as: where d(-,·) is the Harurning distance.</S>
			<S sid ="138" ssid = "56">In our ex­ periments we use 256-bit signatures.</S>
			<S sid ="139" ssid = "57">This reduces the memory requirements for the syntax model to around 600MB.</S>
	</SECTION>
	<SECTION title="Evaluation Results. " number = "5">
			<S sid ="140" ssid = "1">To rate the quality of our output, we solicit human judgments of the compressions along two five-point scales: grammaticality and meaning preservation.</S>
			<S sid ="141" ssid = "2">Judges are instructed to decide how much the mean­ ing from a reference translation is retained in the compressed sentence, with a score of 5 indicating that all of the important information is present, and 1 being that the compression does not retain any of the original meaning.</S>
			<S sid ="142" ssid = "3">Similarly, a grammar score of 5 indicates perfect grammaticality, while a score of 1 is assigned to sentences that are entirely un­ grammatical.</S>
			<S sid ="143" ssid = "4">We ran our evaluation on Mechaui­ cal Turk, where a total of 126 judges provided 3 re­ dundant judgments for each system output.</S>
			<S sid ="144" ssid = "5">To pro­ vide additional quality control, our illTs were aug­ mented with both positive and negative control com­ pressions.</S>
			<S sid ="145" ssid = "6">For the positive control we used the refer­ ence compressions from our test set.</S>
			<S sid ="146" ssid = "7">Negative con­ trol was provided by adding a compression model based on random word deletions to the mix.</S>
			<S sid ="147" ssid = "8">In Table 1 we compare our distributional similarity-augmented systems to the plain pivoting­ based baseline and the lLP approach.</S>
			<S sid ="148" ssid = "9">The compres­ sion ratios of the paraphrasing systems are tuned to match the average compression ratio seen on the de­ velopment and test set.</S>
			<S sid ="149" ssid = "10">The lLP system is config uredto loosely match this ratio,as to not overly con­ strain its search space.</S>
			<S sid ="150" ssid = "11">Our results indicate that the paraphrase approach significantly outperforms ll..P on meaning retention.</S>
			<S sid ="151" ssid = "12">However, the baseline sys­ tem shows notable weaknesses in grammaticality.</S>
			<S sid ="152" ssid = "13">300 2SO 200 150 100 so ..</S>
			<S sid ="153" ssid = "14">pp Adding the n-gram distributional similarity model to the paraphraser recovers some of the difference in grammaticality while simultaneously yielding some gain in the compressions&apos; meaning retention.</S>
			<S sid ="154" ssid = "15">Mov­ ing to distributional similarity estimated on the syn­ tactic feature-set yields additional improvement, de­ spite the model&apos;s lower coverage.</S>
			<S sid ="155" ssid = "16">It is known that human evaluation scores correlate linearly with the compression ratio produced by a sentence compression system (Napoles et al., 2011).</S>
			<S sid ="156" ssid = "17">Thus, to ensure fairness in our comparisons, we pro­ duce a pairwise comparison breakdown that only takes into account compressions of almost identical length.2 Figure 7 shows the results of this analysis, detailing the number of wins and ties in the human judgements.</S>
			<S sid ="157" ssid = "18">We note that the gains in meaning retention over both the baseline and the ll..P system are still present in the pairwise breakdown.</S>
			<S sid ="158" ssid = "19">The gains over the paraphrasing baseline, as well as the improvement in meaning over ll..P are statistically significant at p &lt; 0.05 (using the sign test).</S>
			<S sid ="159" ssid = "20">We can observe that there is substantial overlap between the baseline paraphraser and the n-gram model, while the syntax model appears to yield no­ ticeably different output far more often.</S>
			<S sid ="160" ssid = "21">Table 2 shows two example sentences drawn from our test set and the compressions produced by the different systems.</S>
			<S sid ="161" ssid = "22">It can be seen that both the paraphrase-based and n.P systems produce good quality results, with the paraphrase system retaining the meaning of the source sentence more accurately.</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "6">
			<S sid ="162" ssid = "1">We presented a method to incorporate monolingual distributional similarity into linguistically informed paraphrases extracted from bilingual parallel data.</S>
			<S sid ="163" ssid = "2">Having extended the notion of similarity to dis­ contiguous pattern with multi-word gaps, we inves­ tigated the effect of using feature-sets of varying 2We require the compressions to be within ±10% length of one another.</S>
			<S sid ="164" ssid = "3">§3 F======l ==== F=====H 2SO 200 150 100 50O L- ---J&apos;-- -- -- - Figure 7: A pairwise breakdown of the human judg­ ments comparing the systems.</S>
			<S sid ="165" ssid = "4">Dark grey regions show the number of times the two systems were tied, and light grey shows how many times one system was judged to be better than the other.</S>
			<S sid ="166" ssid = "5">I RandomDeletions I 0.78 I 2.91 2.53 Thble 1: Results of the human evaluation on longer compressions: pairwise compression rates (CR), meaning and grammaticality scores.</S>
			<S sid ="167" ssid = "6">Bold indicates a statistically significance difference at p &lt; 0.05.</S>
			<S sid ="168" ssid = "7">complexity to compute distributional similarity for our parapluase collection.</S>
			<S sid ="169" ssid = "8">We conclude that, com­ pared to a simple large-scale model, a rich, syntax­ based feature-set, even with significantly lower cov­ erage, noticeably improves output quality in a text­ to-text generation task.</S>
			<S sid ="170" ssid = "9">Our syntactic method sig­ nificantly improves grammaticality and meaning re­ tention over a strong paraphrastic baseline, and of­ fers substantial gains in meaning retention over a deletion-based state-&lt;Jf-the-art system.</S>
			<S sid ="171" ssid = "10">Acknowledgements This research was supported in part by the NSF under grant llS0713448 and in part by the EuroMatrixPlus project funded by the European Commission (7th Framework Pro­ gramme).</S>
			<S sid ="172" ssid = "11">Opinions, interpretations, and conclu­ sions are the authors&apos; alone.</S>
			<S sid ="173" ssid = "12">S o u r c e sh ou ld the se po liti cal de vel op me nts ha ve an im pa ct on sp ort s ? Re fer en ce sh ou ld the se po liti cal ev ent s aff ect sp ort s ? S y n t a x sh ou ld the se ev ent s ha ve an im pa ct on sp ort s ? n g r a m the se pol itic al de vel op me nts im pa ct on sp ort s ? p p sh ou ld the se ev ent s im pa ct on sp ort s ? l l . . P pol itic al de vel op me nts ha ve an im pa ct S o u r c e n o w w e h a v e t o t h i n k a n d m a k e a d e c i s i o n a b o n t o u r d i r e c t i o n a n d c h o o s e o u l y o n e w a y . t h a n k s . Re fer en ce we sh oul d po nd er it an d de cid e ou r pat h an d fol lo w it , tha nk s . S y n t a x no w we thi nk an d de cid e on ou r wa y an d ch oo se on e wa y . tha nk s . n g r a m no w we ha ve an d de cid e on ou r wa y an d ch oo se on e wa y . tha nk s . p p no w we ha ve an d de cid e on ou r wa y an d ch oo se on e wa y . tha nk s . l l . . P we ha ve to thi nk an d ma ke a de cis ion an d ch oo se wa y tha nk s Table 2: Example compressions produced by our systems and the baselines Table I for three input sentences from our test data.</S>
	</SECTION>
</PAPER>
