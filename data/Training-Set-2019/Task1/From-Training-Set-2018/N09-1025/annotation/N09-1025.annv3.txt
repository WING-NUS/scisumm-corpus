Citance Number: 1 | Reference Article: N09-1025 | Citing Article: C10-2052 | Citation Marker Offset: ['22'] | Citation Marker: Chiang et al., 2009 | Citation Offset: ['22'] | Citation Text: <S sid ="22" ssid = "22">We explore the different options for context and feature se 1 See (Chan et al., 2007) for the relevance of word sense disambiguation and (Chiang et al., 2009) for the role of prepositions in MT. 454 Coling 2010: Poster Volume, pages 454462, Beijing, August 2010 lection, the influence of different preprocessing methods, and different levels of sense granularity.</S> | Reference Offset: ['23'] | Reference Text: <S sid ="23" ssid = "9">Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.</S> | Discourse Facet: Method_Citation | 
Citance Number: 2 | Reference Article: N09-1025 | Citing Article: D11-1081 | Citation Marker Offset: ['6'] | Citation Marker: Chiang et al., 2009 | Citation Offset: ['6'] | Citation Text: <S sid ="6" ssid = "6">Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al., 2006; Tillmann and Zhang, 2006; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009).</S> | Reference Offset: ['9'] | Reference Text: <S sid ="9" ssid = "9">We add more than 250 features to improve a syntax- based MT systemalready the highest-scoring single system in the NIST 2008 ChineseEnglish common-data trackby +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.</S> | Discourse Facet: Method_Citation | 
Citance Number: 3 | Reference Article: N09-1025 | Citing Article: D11-1081 | Citation Marker Offset: ['9'] | Citation Marker: Chiang et al., 2009 | Citation Offset: ['9'] | Citation Text: <S sid ="9" ssid = "9">Overfitting problem often comes when training many features on a small data (Watanabe et al., 880 2007; Chiang et al., 2009).</S> | Reference Offset: ['23'] | Reference Text: <S sid ="23" ssid = "9">Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.</S> | Discourse Facet: Method_Citation | 
Citance Number: 4 | Reference Article: N09-1025 | Citing Article: D11-1081 | Citation Marker Offset: ['25'] | Citation Marker: Chiang et al., 2009 | Citation Offset: ['25'] | Citation Text: <S sid ="25" ssid = "25">(3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3 . Generally, this is done by deleting a node X0,1 . ing to constructing the oracle reference (Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2009), which is nontrivial for SMT and needs to be determined experimentally.</S> | Reference Offset: ['51'] | Reference Text: <S sid ="51" ssid = "24">We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).</S> | Discourse Facet: Method_Citation | 
Citance Number: 5 | Reference Article: N09-1025 | Citing Article: D11-1081 | Citation Marker Offset: ['159'] | Citation Marker: Chiang et al., 2009 | Citation Offset: ['159'] | Citation Text: <S sid ="159" ssid = "11">Following (Chiang et al., 2009), we only use 100 most frequent words for word context feature.</S> | Reference Offset: ['32','33'] | Reference Text: <S sid ="32" ssid = "5">Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models.</S> <S sid ="33" ssid = "6">This grammar can be parsed efficiently using cube pruning (Chiang, 2007).</S> | Discourse Facet: Method_Citation | 
Citance Number: 6 | Reference Article: N09-1025 | Citing Article: D11-1081 | Citation Marker Offset: ['239'] | Citation Marker: Chiang et al., 2009 | Citation Offset: ['239'] | Citation Text: <S sid ="239" ssid = "91">Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008).</S> | Reference Offset: ['29'] | Reference Text: <S sid ="29" ssid = "2">Hiero (Chiang, 2005) is a hierarchical, string-to- string translation system.</S> | Discourse Facet: Method_Citation | 
Citance Number: 7 | Reference Article: N09-1025 | Citing Article: D11-1125 | Citation Marker Offset: ['13'] | Citation Marker: Chiang et al., 2009 | Citation Offset: ['13'] | Citation Text: <S sid ="13" ssid = "13">The MIRA technique of Chiang et al. has been shown to perform well on large-scale tasks with hundreds or thousands of features (2009).</S> | Reference Offset: ['51'] | Reference Text: <S sid ="51" ssid = "24">We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).</S> | Discourse Facet: Method_Citation | 
Citance Number: 8 | Reference Article: N09-1025 | Citing Article: D11-1125 | Citation Marker Offset: ['78', '79'] | Citation Marker: Chiang et al., 2009 | Citation Offset: ['77', '78', '79'] | Citation Text: <S sid ="77" ssid = "2">The most prominent example of a tuning method that performs well on high-dimensionality candidate spaces is the MIRA-based approach used by Watanabe et al.</S>   <S sid ="78" ssid = "3">(2007) and Chiang et al.</S>   <S sid ="79" ssid = "4">(2008b; 2009).</S> | Reference Offset: ['14'] | Reference Text: <S sid ="14" ssid = "14">Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.</S> | Discourse Facet: Method_Citation | 
Citance Number: 9 | Reference Article: N09-1025 | Citing Article: D11-1125 | Citation Marker Offset: ['208'] | Citation Marker: Chiang et al., 2009 | Citation Offset: ['207', '208', '209'] | Citation Text: <S sid ="207" ssid = "55">We used the following feature classes in SBMT and PBMT extended scenarios: • Discount features for rule frequency bins (cf.</S> <S sid ="208" ssid = "56">Chiang et al.</S> <S sid ="209" ssid = "57">(2009), Section 4.1) • Target word insertion features9 We used the following feature classes in SBMT extended scenarios only (cf.</S> | Reference Offset: ['9'] | Reference Text: <S sid ="9" ssid = "9">We add more than 250 features to improve a syntax- based MT systemalready the highest-scoring single system in the NIST 2008 ChineseEnglish common-data trackby +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.</S> | Discourse Facet: Method_Citation | 
Citance Number: 10 | Reference Article: N09-1025 | Citing Article: D11-1125 | Citation Marker Offset: ['210'] | Citation Marker: Chiang et al., 2009 | Citation Offset: ['210', '211'] | Citation Text: <S sid ="210" ssid = "58">Chiang et al.</S> <S sid ="211" ssid = "59">(2009), Section 4.1):10 • Rule overlap features • Node count features 9 For ChineseEnglish and UrduEnglish SBMT these features only fired when the inserted target word was unaligned to any source word.</S> | Reference Offset: ['9'] | Reference Text: <S sid ="9" ssid = "9">We add more than 250 features to improve a syntax- based MT systemalready the highest-scoring single system in the NIST 2008 ChineseEnglish common-data trackby +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.</S> | Discourse Facet: Method_Citation | 
Citance Number: 11 | Reference Article: N09-1025 | Citing Article: D11-1125 | Citation Marker Offset: ['227'] | Citation Marker: Chiang et al., 2009 | Citation Offset: ['227', '228', '229'] | Citation Text: <S sid ="227" ssid = "16">5.4.1 MERT We used David Chiang’s CMERT implementation We for the most part follow the MIRA algorithm for machine translation as described by Chiang et al.</S> <S sid ="228" ssid = "17">(2009)12 but instead of using the 10-best of each of the best hw , hw +g, and hw -g, we use the 30-best according to hw .13 We use the same sentence-level BLEU calculated in the context of previous 1-best translations as Chiang et al.</S> <S sid ="229" ssid = "18">(2008b; 2009).</S> | Reference Offset: ['14'] | Reference Text: <S sid ="14" ssid = "14">Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.</S> | Discourse Facet: Method_Citation | 
Citance Number: 12 | Reference Article: N09-1025 | Citing Article: N12-1006 | Citation Marker Offset: ['102'] | Citation Marker: Chiang et al., 2009 | Citation Offset: ['102', '103'] | Citation Text: <S sid ="102" ssid = "7">Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al. </S> <S sid ="103" ssid = "8">(2009).</S> | Reference Offset: ['51'] | Reference Text: <S sid ="51" ssid = "24">We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).</S> | Discourse Facet: Method_Citation | 
Citance Number: 13 | Reference Article: N09-1025 | Citing Article: P12-1001 | Citation Marker Offset: ['147'] | Citation Marker: Chiang et al., 2009 | Citation Offset: ['147'] | Citation Text: <S sid ="147" ssid = "64">Alternatively, by using the large- margin optimizer in (Chiang et al., 2009) and moving it into the for-each loop (lines 49), one can get an online algorithm such PMOMIRA.</S> | Reference Offset: ['14'] | Reference Text: <S sid ="14" ssid = "14">Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.</S> | Discourse Facet: Method_Citation | 
Citance Number: 14 | Reference Article: N09-1025 | Citing Article: P13-1110 | Citation Marker Offset: ['36'] | Citation Marker: Chiang et al., 2009 | Citation Offset: ['36'] | Citation Text: <S sid ="36" ssid = "16">Online large-margin algorithms, such as MIRA, have also gained prominence in SMT, thanks to their ability to learn models in high-dimensional feature spaces (Watanabe et al., 2007; Chiang et al., 2009).</S> | Reference Offset: ['51'] | Reference Text: <S sid ="51" ssid = "24">We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).</S> | Discourse Facet: Method_Citation | 
Citance Number: 15 | Reference Article: N09-1025 | Citing Article: P13-1110 | Citation Marker Offset: ['102'] | Citation Marker: Chiang et al., 2009 | Citation Offset: ['102'] | Citation Text: <S sid ="102" ssid = "56">task Corpus Sentences Tokens En Zh/Ar MT05 1082 35k 33k training 1M 23.7M 22.8M tune (MT06) 1797 55k 49k baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and RAMPION (Gimpel and Smith, 2012).</S> | Reference Offset: ['14'] | Reference Text: <S sid ="14" ssid = "14">Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.</S> | Discourse Facet: Method_Citation | 
Citance Number: 16 | Reference Article: N09-1025 | Citing Article: P13-1110 | Citation Marker Offset: ['115'] | Citation Marker: Chiang et al., 2009 | Citation Offset: ['115'] | Citation Text: <S sid ="115" ssid = "9">The bound constraint B was set to 1.4 The approximate sentence-level BLEU cost i is computed in a manner similar to (Chiang et al., 2009), namely, in the context of previous 1-best translations of the tuning set.</S> | Reference Offset: ['65'] | Reference Text: <S sid ="65" ssid = "38">Following Chiang et al. (2008), we calculate the sentence B scores in (1), (2), and (3) in the context of some previous 1-best translations.</S> | Discourse Facet: Method_Citation | 
Citance Number: 17 | Reference Article: N09-1025 | Citing Article: P13-1110 | Citation Marker Offset: ['120'] | Citation Marker: Chiang et al., 2009 | Citation Offset: ['120'] | Citation Text: <S sid ="120" ssid = "14">For experiments with a larger feature set, we introduced additional lexical and non-lexical sparse Boolean features of the form commonly found in the literature (Chiang et al., 2009; Watan 4 We also conducted an investigation into the setting of the B parameter.</S> | Reference Offset: ['23'] | Reference Text: <S sid ="23" ssid = "9">Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.</S> | Discourse Facet: Method_Citation | 
Citance Number: 18 | Reference Article: N09-1025 | Citing Article: P28_n09 | Citation Marker Offset: ['51'] | Citation Marker: Chiang et al., 2009 | Citation Offset: ['51'] | Citation Text: <S sid ="51" ssid = "23">Recently, a simple method was presented in (Chiang eta!., 2009), which keeps partial English and Urdu words in the training data for alignment training.</S> | Reference Offset: ['29'] | Reference Text: <S sid ="29" ssid = "2">Hiero (Chiang, 2005) is a hierarchical, string-to- string translation system.</S> | Discourse Facet: Method_Citation | 
Citance Number: 19 | Reference Article: N09-1025 | Citing Article: P134_n09 | Citation Marker Offset: ['27'] | Citation Marker: Chiang et al., 2009 | Citation Offset: ['27', '28'] | Citation Text: <S sid ="27" ssid = "27">First, we used features proposed by Chiang et al.</S>	<S sid ="28" ssid = "28">(2009): • phrase pair count bin features (bins 1, 2, 3, 45, 69, 10+) • target word insertion features • source word deletion features • word translation features • phrase length feature (source, target, both) Table 4:Sparse domain features When combining the domain features and the other sparse features, we see roughly additive gains (Table 5).</S> | Reference Offset: ['9'] | Reference Text: <S sid ="9" ssid = "9">We add more than 250 features to improve a syntax- based MT systemalready the highest-scoring single system in the NIST 2008 ChineseEnglish common-data trackby +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.</S> | Discourse Facet: Method_Citation | 
Citance Number: 20 | Reference Article: N09-1025 | Citing Article: Pmert_n09 | Citation Marker Offset: ['15'] | Citation Marker: Chiang et al., 2009 | Citation Offset: ['15'] | Citation Text: <S sid ="15" ssid = "15">These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization.</S> | Reference Offset: ['9'] | Reference Text: <S sid ="9" ssid = "9">We add more than 250 features to improve a syntax- based MT systemalready the highest-scoring single system in the NIST 2008 ChineseEnglish common-data trackby +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.</S> | Discourse Facet: Method_Citation | 
Citance Number: 21 | Reference Article: N09-1025 | Citing Article: Pmert_n09 | Citation Marker Offset: ['16'] | Citation Marker: Chiang et al., 2009 | Citation Offset: ['16'] | Citation Text: <S sid ="16" ssid = "16">In particular, recent work (Chiang et al., 2009) has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation.</S> | Reference Offset: ['9'] | Reference Text: <S sid ="9" ssid = "9">We add more than 250 features to improve a syntax- based MT systemalready the highest-scoring single system in the NIST 2008 ChineseEnglish common-data trackby +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.</S> | Discourse Facet: Method_Citation | 
Citance Number: 22 | Reference Article: N09-1025 | Citing Article: PMTS_n09 | Citation Marker Offset: ['218'] | Citation Marker: Chiang et al., 2009 | Citation Offset: ['218'] | Citation Text: <S sid ="218" ssid = "109">This observation conrms previous ndings (Chiang et al., 2009) regarding the inability of the MERT algorithm to converge on an optimal set of weights for a reasonably large number of parameters.</S> | Reference Offset: ['51'] | Reference Text: <S sid ="51" ssid = "24">We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).</S> | Discourse Facet: Method_Citation | 
Citance Number: 23 | Reference Article: N09-1025 | Citing Article: PMTS_n09 | Citation Marker Offset: ['245'] | Citation Marker: Chiang et al., 2009 | Citation Offset: ['245'] | Citation Text: <S sid ="245" ssid = "136">The MERT algorithm is known to be unable to learn optimal weights for large parameter settings (Chiang et al., 2009).</S> | Reference Offset: ['65'] | Reference Text: <S sid ="65" ssid = "38">Following Chiang et al. (2008), we calculate the sentence B scores in (1), (2), and (3) in the context of some previous 1-best translations.</S> | Discourse Facet: Method_Citation | 
Citance Number: 24 | Reference Article: N09-1025 | Citing Article: PMTS_n09 | Citation Marker Offset: ['252'] | Citation Marker: Chiang et al., 2009 | Citation Offset: ['252'] | Citation Text: <S sid ="252" ssid = "6">We would also like to look into alternative tuning techniques, especially ones based on the MIRA algorithm to improve the quality of log-linear mixture adaptation in large parameter settings (Chiang et al., 2009).</S> | Reference Offset: ['51'] | Reference Text: <S sid ="51" ssid = "24">We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).</S> | Discourse Facet: Method_Citation | 
Citance Number: 25 | Reference Article: N09-1025 | Citing Article: PSMPT_n09 | Citation Marker Offset: ['73'] | Citation Marker: Chiang et al., 2009 | Citation Offset: ['73', '74'] | Citation Text: <S sid ="73" ssid = "6">Chiang et w(X → (α, γ, ∼ )) = λiφi (2) i al.</S> <S sid ="74" ssid = "7">(2009) define new translational features using neighbouring word contexts of the source phrase, which are directly integrated into the translation model of Hiero system.</S> | Reference Offset: ['29'] | Reference Text: <S sid ="29" ssid = "2">Hiero (Chiang, 2005) is a hierarchical, string-to- string translation system.</S> | Discourse Facet: Method_Citation | 
Citance Number: 26 | Reference Article: N09-1025 | Citing Article: PTASL_n09 | Citation Marker Offset: ['79'] | Citation Marker: Chiang et al., 2009 | Citation Offset: ['79'] | Citation Text: <S sid ="79" ssid = "79">For example, Chiang et al. [3] designed many target-side syntax features to improve the string-to-tree translation.</S> | Reference Offset: ['51'] | Reference Text: <S sid ="51" ssid = "24">We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).</S> | Discourse Facet: Method_Citation | 
Citance Number: 27 | Reference Article: N09-1025 | Citing Article: PTASL_n09 | Citation Marker Offset: ['86'] | Citation Marker: Chiang et al., 2009 | Citation Offset: ['86'] | Citation Text: <S sid ="86" ssid = "86">Moreover, to establish a strong baseline, we also include the discount feature used by Chiang et al. [3] in our baseline string-to-tree model.</S> | Reference Offset: ['29'] | Reference Text: <S sid ="29" ssid = "2">Hiero (Chiang, 2005) is a hierarchical, string-to- string translation system.</S> | Discourse Facet: Method_Citation | 
Citance Number: 28 | Reference Article: N09-1025 | Citing Article: W10-1757 | Citation Marker Offset: ['48'] | Citation Marker: Chiang et al., 2009 | Citation Offset: ['48'] | Citation Text: <S sid ="48" ssid = "25">When the number of features is too large, even popular reranking algorithms such as SVM (Shen et al., 2004) and MIRA (Watanabe et al., 2007; Chiang et al., 2009) may fail.</S> | Reference Offset: ['23'] | Reference Text: <S sid ="23" ssid = "9">Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.</S> | Discourse Facet: Method_Citation | 
Citance Number: 29 | Reference Article: N09-1025 | Citing Article: W10-1757 | Citation Marker Offset: ['209'] | Citation Marker: Chiang et al., 2009 | Citation Offset: ['209'] | Citation Text: <S sid ="209" ssid = "25">Recent work by (Chiang et al., 2009) describes new features for hierarchical phrase-based MT, while (Collins and Koo, 2005) describes features for parsing.</S> | Reference Offset: ['32', '33'] | Reference Text: <S sid ="32" ssid = "5">Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models.</S> <S sid ="33" ssid = "6">This grammar can be parsed efficiently using cube pruning (Chiang, 2007).</S> | Discourse Facet: Method_Citation | 
Citance Number: 30 | Reference Article: N09-1025 | Citing Article: W10-1761 | Citation Marker Offset: ['53'] | Citation Marker: Chiang et al., 2009 | Citation Offset: ['53', '54'] | Citation Text: <S sid ="53" ssid = "30">Chiang et al.</S> <S sid ="54" ssid = "31">(2009) added thousands of linguistically-motivated features to hierarchical and syntax systems, however, the source syntax features are derived from the research above.</S> | Reference Offset: ['9'] | Reference Text: <S sid ="9" ssid = "9">We add more than 250 features to improve a syntax- based MT systemalready the highest-scoring single system in the NIST 2008 ChineseEnglish common-data trackby +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.</S> | Discourse Facet: Method_Citation | 
Citance Number: 31 | Reference Article: N09-1025 | Citing Article: W10-1761 | Citation Marker Offset: ['92'] | Citation Marker: Chiang et al., 2009 | Citation Offset: ['92'] | Citation Text: <S sid ="92" ssid = "25">A non-exhaustive sample is given below: [X  Ls, i, i] Terminal Symbol (X  Ls)  G X  ADJ A1 Akt # X1 Act N P  N E1 X2 # X1 X2 T OP  N E1 letzter X2 # X1 Last X2 [X    Fj,k Ls, i, j] [X  Fj,k  Ls, i, j + 1] Non-Terminal Symbol [X    Fj,k Ls, i, j] [X, j, Rj,k ] [X  Fj,k  Ls, i, Rj,k ] [X    Ls, i, Ri,j ] [Fi,j = Ls] the log function (hm = log hm) [X  Ls, i, Ri,j ] log p(tPIPEs) = m hm(t, s) (3) m Goal [X  Ls, 0, PIPEV PIPE  1] An advanta ge of our model over (Marto n and Resnik, 2008; Chiang et al., 2008, 2009) is the number of feature function s remains the same,This model allows translation rules to take ad vantage of both syntactic label and word context.</S> | Reference Offset: ['23'] | Reference Text: <S sid ="23" ssid = "9">Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.</S> | Discourse Facet: Method_Citation | 
Citance Number: 32 | Reference Article: N09-1025 | Citing Article: W10-1761 | Citation Marker Offset: ['97'] | Citation Marker: Chiang et al., 2009 | Citation Offset: ['97'] | Citation Text: <S sid ="97" ssid = "30">The model also differs from (Marton and Resnik, 2008; Chiang et al., 2008, 2009) by adding informative labels to rule non-terminals and requiring them to match the source span label.</S> | Reference Offset: ['32','33'] | Reference Text: <S sid ="32" ssid = "5">Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models.</S> <S sid ="33" ssid = "6">This grammar can be parsed efficiently using cube pruning (Chiang, 2007).</S> | Discourse Facet: Method_Citation | 
