Clustering Polysemic Subcategorization Frame Distributions Semantically




Anna Korhonen∗
Computer Laboratory
University of Cambridge
15 JJ Thomson Avenue 
Cambridge CB3 0FD, UK 
alk23@cl.cam.ac.uk


Yuval Krymolowski 
Division of Informatics 
University of Edinburgh
2 Buccleuch Place 
Edinburgh EH8 9LW 
Scotland, UK
ykrymolo@inf.ed.ac.uk


Zvika Marx 
Interdisciplinary Center 
for Neural Computation, 
The Hebrew University 
Jerusalem, Israel
zvim@cs.huji.ac.il








Abstract

Previous research has demonstrated the 
utility of clustering in inducing semantic 
verb classes from undisambiguated cor- 
pus data.  We describe a new approach 
which involves clustering subcategoriza- 
tion frame (S C F) distributions using the 
Information Bottleneck and nearest neigh- 
bour methods.   In contrast to previous 
work,  we particularly focus on cluster- 
ing polysemic verbs.  A novel evaluation 
scheme is proposed which accounts for 
the effect of polysemy on the clusters, of- 
fering us a good insight into the potential 
and limitations of semantically classifying 
undisambiguated S C F data.


1   Introduction

Classifications which aim to capture the close rela- 
tion between the syntax and semantics of verbs have 
attracted a considerable research interest in both lin- 
guistics and computational linguistics (e.g. (Jack- 
endoff, 1990; Levin, 1993; Pinker, 1989; Dang et 
al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)). 
While such classifications may not provide a means 
for full semantic inferencing, they can capture gen- 
eralizations over a range of linguistic properties, and 
can therefore be used as a means of reducing redun- 
dancy in the lexicon and for filling gaps in lexical 
knowledge.
∗This work was partly supported by UK EPSRC project
GR/N36462/93: ‘Robust Accurate Statistical Parsing (RASP)’.
  

Verb classifications have, in fact, been used to 
support many natural language processing (N L P) 
tasks, such as language generation, machine transla- 
tion (Dorr, 1997), document classification (Klavans 
and Kan, 1998), word sense disambiguation (Dorr 
and Jones, 1996) and subcategorization acquisition 
(Korhonen, 2002).
  One attractive property of these classifications is 
that they make it possible, to a certain extent, to in- 
fer the semantics of a verb on the basis of its syn- 
tactic behaviour.  In recent years several attempts 
have been made to automatically induce semantic 
verb classes from (mainly) syntactic information 
in corpus data (Joanis, 2002; Merlo et al., 2002; 
Schulte im Walde and Brew, 2002).
  In this paper,  we focus on the particular task 
of classifying subcategorization frame (S C F) distri- 
butions in a semantically motivated manner.  Pre- 
vious  research  has  demonstrated  that  clustering 
can be useful in inferring Levin-style semantic 
classes (Levin, 1993) from both English and Ger- 
man verb subcategorization information (Brew and 
Schulte im Walde, 2002; Schulte im Walde, 2000; 
Schulte im Walde and Brew, 2002).
  We propose a novel approach, which involves: (i) 
obtaining S C F  frequency information from a lexi- 
con extracted automatically using the comprehen- 
sive system of Briscoe and Carroll (1997) and (ii) 
applying a clustering mechanism to this informa- 
tion.  We use clustering methods that process raw 
distributional data directly, avoiding complex pre- 
processing steps required by many advanced meth- 
ods (e.g. Brew and Schulte im Walde (2002)).
In contrast to earlier work, we give special empha-


sis to polysemy.  Earlier work has largely ignored 
this issue by assuming a single gold standard class 
for each verb (whether polysemic or not). The rel- 
atively good clustering results obtained suggest that 
many polysemic verbs do have some predominating 
sense in corpus data. However, this sense can vary 
across corpora (Roland et al., 2000), and assuming a 
single sense is inadequate for an important group of 
medium and high frequency verbs whose distribu- 
tion of senses in balanced corpus data is flat rather 
than zipfian (Preiss and Korhonen, 2002).
  To allow for sense variation, we introduce a new 
evaluation scheme against a polysemic gold stan- 
dard.  This helps to explain the results and offers 
a better insight into the potential and limitations of 
clustering undisambiguated S C F data semantically.
  We discuss our gold standards and the choice of 
test verbs in section 2.   Section 3 describes the 
method for subcategorization acquisition and sec- 
tion 4 presents the approach to clustering.  Details 
of the experimental evaluation are supplied in sec- 
tion 5. Section 6 concludes with directions for future 
work.

2   Semantic Verb Classes and Test Verbs

Levin’s taxonomy of verbs and their classes (Levin,
1993) is the largest syntactic-semantic verb classifi- 
cation in English, employed widely in evaluation of 
automatic classifications. It provides a classification 
of 3,024 verbs (4,186 senses) into 48 broad / 192 
fine grained classes. Although it is quite extensive, 
it is not exhaustive. As it primarily concentrates on 
verbs taking N P and P P complements and does not 
provide a comprehensive set of senses for verbs, it 
is not suitable for evaluation of polysemic classifi- 
cations.
  We  employed  as  a  gold  standard  a  substan- 
tially extended version of Levin’s classification 
constructed  by  Korhonen (2003).    This  incorpo- 
rates  Levin’s  classes,  26  additional  classes  by 
Dorr (1997)1, and 57 new classes for verb types not 
covered comprehensively by Levin or Dorr.
  110 test verbs were chosen from this gold stan- 
dard, 78 polysemic and 32 monosemous ones. Some 
low frequency verbs were included to investigate the

  1 These  classes  are  incorporated  in  the  ’L C S   database’ 
(http://www.umiacs.umd.edu/∼bonnie/verbs-English.lcs).


effect of sparse data on clustering performance. To 
ensure that our gold standard covers all (or most) 
senses of these verbs, we looked into WordNet 
(Miller, 1990) and assigned all the WordNet senses 
of the verbs to gold standard classes.2
  Two versions of the gold standard were created: 
monosemous and polysemic. The monosemous one 
lists only a single sense for each test verb, that cor- 
responding to its predominant (most frequent) sense 
in WordNet. The polysemic one provides a compre- 
hensive list of senses for each verb.  The test verbs 
and their classes are shown in table 1.  The classes 
are indicated by number codes from the classifica- 
tions of Levin, Dorr (the classes starting with 0) and 
Korhonen (the classes starting with A).3  The pre- 
dominant sense is indicated by bold font.

3   Subcategorization Information

We obtain our S C F data using the subcategorization 
acquisition  system  of  Briscoe and Carroll (1997). 
We  expect  the  use  of  this  system to  be  benefi- 
cial: it employs a robust statistical parser (Briscoe 
and Carroll, 2002) which yields complete though 
shallow parses, and a comprehensive S C F classifier, 
which incorporates 163 S C F  distinctions, a super- 
set of those found in the A N LT  (Boguraev et al.,
1987) and C O M L E X (Grishman et al., 1994) dictio- 
naries.   The S C Fs abstract over specific lexically- 
governed particles and prepositions and specific 
predicate selectional preferences but include some 
derived semi-predictable bounded dependency con- 
structions, such as particle and dative movement.
  78 of these ‘coarse-grained’ S C Fs appeared in our 
data.  In addition, a set of 160 fine grained frames 
were employed. These were obtained by parameter- 
izing two high frequency S C Fs for prepositions: the 
simple P P and N P + P P frames.  The scope was re- 
stricted to these two frames to prevent sparse data 
problems in clustering.
  A S C F  lexicon was acquired using this system 
from  the  British  National Corpus  (Leech,  1992, 
B N C) so that the maximum of 7000 citations were

  2 As WordNet incorporates particularly fine grained sense 
distinctions, some senses were found which did not appear in 
our gold standard. As many of them appeared marginal and/or 
low in frequency, we did not consider these additional senses in 
our experiment.
3 The gold standard assumes Levin’s broad classes (e.g. class
10) instead of possible fine-grained ones (e.g. class 10.1).



TE
ST
VE
RB
G
O
L
D
 
S
T
A
N
D
A
R
D
C
L
A
S
S
E
S
T
E
S
T
V
E
R
B
GO
LD 
ST
AN
DA
RD
C
L
A
S
S
E
S
T
E
S
T
V
E
R
B
GO
LD 
ST
AN
DA
RD
C
L
A
S
S
E
S
T
ES
T
V
ER
B
GO
LD 
ST
AN
DA
RD
C
L
A
S
S
E
S
plac
e
9
dye
24, 
21, 
41
focu
s
31, 
45
stare
30
lay
9
buil
d
26, 
45
forc
e
002, 
11
glow
43
drop
9, 
45, 
004, 
47,
51
, 
A
54
, 
A
30
bake
26, 
45
pers
uade
002
spar
kle
43
pour
9, 
43, 
26, 
57, 
13, 
31
inve
nt
26, 
27
urge
002, 
37
dry
45
load
9
publ
ish
26, 
25
want
002, 
005, 
29, 
32
shut
45
settle
9, 
46, 
A16, 
36, 
55
caus
e
27, 
002
need
002, 
005, 
29, 
32
han
g
47, 
9, 
42, 
40
fill
9, 
45, 
47
gene
rate
27, 
13, 
26
gras
p
30, 
15
sit
47, 9
remo
ve
10, 
11, 
42
indu
ce
27, 
002, 
26
unde
rsta
nd
30
disa
ppea
r
48
with
draw
10, 
A30
ackn
owle
dge
29, 
A25, 
A35
conc
eive
30, 
29, 
A56
vani
sh
48
wipe
10, 9
proc
laim
29, 
37, 
A25
cons
ider
30, 
29
mar
ch
51
brus
h
10, 
9, 
41, 
18
reme
mbe
r
29, 
30
perc
eive
30
walk
51
filter
10
ima
gine
29, 
30
anal
yse
34, 
35
trav
el
51
send
11, 
A55
spec
ify
29
eval
uate
34, 
35
hurr
y
53, 
51
ship
11, 
A58
esta
blish
29, 
A56
expl
ore
35, 
34
rush
53, 
51
trans
port
11, 
31
supp
ose
29, 
37
inve
stiga
te
35, 
34
begi
n
55
carr
y
11, 
54
assu
me
29, 
A35, 
A57
agre
e
36, 
22, 
A42
cont
inue
55, 
47, 
51
drag
11, 
35, 
51, 
002
thin
k
29, 
005
com
mun
icate
36, 
11
sno
w
57, 
002
push
11, 
12, 
23, 
9, 
002
conf
irm
29
shou
t
37
rain
57
pull
11, 
12, 
13, 
23, 
40, 
016
belie
ve
29, 
31, 
33
whis
per
37
sin
003
give
13
adm
it
29, 
024, 
045, 
37
talk
37
rebe
l
003
lend
13
allo
w
29, 
024, 
13, 
002
spea
k
37
risk
008, 
A7
study
14, 
30, 
34, 
35
act
29
say
37, 
002
gam
ble
008, 
009
hit
18, 
17, 
47, 
A56, 
31, 
42
beha
ve
29
ment
ion
37
beg
015, 
32
bang
18, 
43, 
9, 
47, 
36
feel
30, 
31, 
35, 
29
eat
39
pray
015, 
32
carv
e
21, 
25, 
26
see
30, 
29
drin
k
39
see
m
020
add
22, 
37, 
A56
hear
30, 
A32
laug
h
40, 
37
appe
ar
020, 
48, 
29
mix
22, 
26, 
36
noti
ce
30, 
A32
smil
e
40, 
37

colo
ur
24, 
31, 
45
conc
entr
ate
31, 
45
look
30, 
35


Table 1: Test verbs and their monosemous/polysemic gold standard senses




used per test verb. The lexicon was evaluated against 
manually analysed corpus data after an empirically 
defined threshold of 0.025 was set on relative fre- 
quencies of S C Fs to remove noisy S C Fs. The method 
yielded 71.8% precision and 34.5% recall. When we 
removed the filtering threshold, and evaluated the 
noisy distribution, F-measure4 dropped from 44.9 to
38.51.5

4   Clustering Method

Data clustering is a process which aims to partition a 
given set into subsets (clusters) of elements that are 
similar to one another, while ensuring that elements 
that are not similar are assigned to different clusters. 
We use clustering for partitioning a set of verbs. Our 
hypothesis is that information about S C Fs and their 
associated frequencies is relevant for identifying se- 
mantically related verbs. Hence, we use S C Fs as rel- 
evance features to guide the clustering process.6

4	2· precision· 
recall 
precision+recall
5 These figures are not particularly impressive because our
evaluation is exceptionally hard.  We use 1) highly polysemic 
test verbs, 2) a high number of S C Fs and 3) evaluate against 
manually analysed data rather than dictionaries (the latter have 
high precision but low recall).
    6 The relevance of the features to the task is evident when 
comparing the probability of a randomly chosen pair of verbs
  

We chose two clustering methods which do not in- 
volve task-oriented tuning (such as pre-fixed thresh- 
olds or restricted cluster sizes) and which approach 
data straightforwardly, in its distributional form: (i) 
a simple hard method that collects the nearest neigh- 
bours (N N) of each verb (figure 1), and (ii) the In- 
formation Bottleneck (I B), an iterative soft method 
(Tishby et al., 1999) based on information-theoretic 
grounds.
  The N N  method is very simple, but it has some 
disadvantages. It outputs only one clustering config- 
uration, and therefore does not allow examination 
of different cluster granularities.  It is also highly 
sensitive to noise.  Few exceptional neighbourhood 
relations contradicting the typical trends in the data 
are enough to cause the formation of a single cluster 
which encompasses all elements.
  Therefore we employed the more sophisticated 
I B  method as well.    The  I B  quantifies the rele- 
vance information of a  S C F  distribution with re- 
spect to output clusters, through their mutual infor- 
mation I (C lusters; S C Fs). The relevance informa- 
tion is maximized, while the compression informa- 
tion I (C lusters; V erbs)  is minimized.   This en- 
sures optimal compression of data through clusters. 
The tradeoff between the two constraints is realized


verbi  and verbj  to share the same predominant sense (4.5%)	 	


with the probability obtained when verbj  is the JS-divergence


nearest neighbour of verbi  (36%).


NN Clustering:
1. For each verb v:
2.  Calculate the J S divergence between the S C F
distributions of v and all other verbs:


IB Clustering (fixed β):
Perform till convergence, for each time step
t = 1, 2, . . . :
βD[p(S|V )1pt−1 (S|K )]



J S(p, q) =  1


rD p  p+q   + D  q   
p+q  l


1. zt(K, V ) = 
pt−1(K ) e−
(When t = 1, 
initialize zt(K, V ) 
arbitrarily)


2	2	2


3.  Connect v with the most similar verb;
4. Find all the connected components

Figure 1:  Connected components nearest neighbour (N N)
clustering. D is the Kullback-Leibler distance.


through minimizing the cost term:

L = I (C lusters; V erbs) − βI (C lusters; S C Fs) ,

where β is a parameter that balances the constraints.
  The I B iterative algorithm finds a local minimum 
of the above cost term. It takes three inputs: (i) S C F- 
verb distributions, (ii) the desired number of clusters 
K, and (iii) the value of β.
  Starting from a random configuration, the algo- 
rithm repeatedly calculates, for each cluster K , verb 
V and S C F  S, the following probabilities:  (i) the 
marginal proportion of the cluster p(K );  (ii) the 
probability p(S|K ) for a S C F  to occur with mem- 
bers of the cluster; and (iii) the probability p(K |V ) 
for a verb to be assigned to the cluster. These prob- 
abilities are used, each in its turn, for calculating the 
other probabilities (figure 2).  The collection of all 
p(S|K )’s for a fixed cluster K can be regarded as a 
probabilistic center (centroid) of that cluster in the 
S C F space.
  The   I B    method  gives  an  indication  of  the 
most informative values of K.7     Intensifying the 
weight  β  attached  to  the  relevance  information 
I (C lusters; S C Fs) allows us to increase the num-


2.  p (K V ) =	zt (K,V ) 	
K 1  zt (K 1,V )
3. pt(K ) = LV  p(V )pt(K |V )
4. pt(S|K ) = LV  p(S|V )pt(V |K )

Figure 2: Information Bottleneck (I B) iterative clustering. D
is the Kullback-Leibler distance.


  When the weight of relevance grows, the assign- 
ment to clusters is more constrained and p(K |V ) be- 
comes more similar to hard clustering. Let

K (V ) = argmax p(K |V )
K

denote  the  most  probable  cluster  of  a  verb  V . 
For K  ≥ 30, more than 85% of the verbs have 
p(K (V )|V ) > 90% which makes the output cluster- 
ing approximately hard. For this reason, we decided 
to use only K (V ) as output and defer a further ex- 
ploration of the soft output to future work.

5   Experimental Evaluation

5.1   Data

The input data to clustering was obtained from the 
automatically acquired S C F lexicon for our 110 test 
verbs (section 2).  The counts were extracted from 
unfiltered (noisy) S C F distributions in this lexicon.8
The N N algorithm produced 24 clusters on this in- 
put.  From the I B  algorithm, we requested K  = 2 
to 60 clusters.  The upper limit was chosen so as
to slightly exceed the case when the average clus-


ber K of distinct clusters being produced (while too
small β would cause some of the output clusters to



ter size 110/K



= 2.  We chose for evaluation the


be identical to one another). Hence, the relevance in- 
formation grows with K.  Accordingly, we consider 
as the most informative output configurations those 
for which the relevance information increases more 
sharply between K − 1 and K clusters than between 
K and K + 1.


I B  results for K = 25, 35 and 42.  For these val-
ues, the S C F  relevance satisfies our criterion for a 
notable improvement in cluster quality (section 4). 
The value K = 35 is very close to the actual number 
(34) of predominant senses in the gold standard. In 
this way, the I B yields structural information beyond 
clustering.


7 Most works on clustering ignore this issue and refer to an	 	


arbitrarily chosen number of clusters, or to the number of gold 
standard classes, which cannot be assumed in realistic applica- 
tions.
  

8 This yielded better results, which might indicate that the 
unfiltered “noisy” S C Fs contain information which is valuable 
for the task.



5.2   Method

A number of different strategies have been proposed 
for evaluation of clustering. We concentrate here on 
those which deliver a numerical value which is easy 
to interpret, and do not introduce biases towards spe- 
cific numbers of classes or class sizes.  As we cur- 
rently assign a single sense to each polysemic verb 
(sec. 5.4) the measures we use are also applicable 
for evaluation against a polysemous gold standard.
  Our first measure, the adjusted pairwise preci- 
sion (A P P), evaluates clusters in terms of verb pairs 
(Schulte im Walde and Brew, 2002) 9:



Al
g.
K
+P
P
–
PP
+P
P
–
PP


A P P:
mP U R:
N
 
N
(24
)
21
%
19
%
48
%
45
%

I
 
B
25
35
42
12
%
14
%
15
%
9
%
9
%
9
%
39
%
48
%
50
%
32
%
38
%
39
%
R A 
N D
25
3
%
1
5
%

Table 2: Clustering performance on the predominant senses, 
with and without prepositions. The last entry presents the per-
formance of random clustering with K = 25, which yielded the 
best results among the three values K=25, 35 and 42.



better on the task than our random clustering base- 
line. Both methods show clearly better performance


A P P = 1  LK


num. of correct 
pairs in ki


|ki |−1 .


with fine-
grained S C Fs 
(with 
prepositions, 
+P P) than


K i=1


num. of pairs in 
ki	· |ki |+1



with coarse-
grained ones (-
P P).



A P P  is the average proportion of all within-cluster 
pairs that are correctly co-assigned. It is multiplied 
by a factor that increases with cluster size. This fac- 
tor compensates for a bias towards small clusters.
  Our second measure is derived from purity, a 
global measure which evaluates the mean precision 
of the clusters, weighted according to the cluster size 
(Stevenson and Joanis, 2003).   We associate with 
each cluster its most prevalent semantic class, and 
denote the number of verbs in a cluster K that take 
its prevalent class by nprevalent(K ).  Verbs that do 
not take this class are considered as errors.  Given 
our task, we are only interested in classes which con- 
tain two or more verbs. We therefore disregard those 
clusters where nprevalent(K ) = 1. This leads us to 
define modified purity:



  Surprisingly,  the  simple  N N   method  performs 
very similarly to the more sophisticated I B.  Being 
based on pairwise similarities, it shows better per- 
formance than I B on the pairwise measure. The I B 
is, however, slightly better according to the global 
measure (2% with K = 42). The fact that the N N 
method performs better than the I B with similar K 
values (N N K = 24 vs. I B K = 25) seems to suggest 
that the J S  divergence provides a better model for 
the predominant class than the compression model 
of the I B.  However, it is likely that the I B  perfor- 
mance suffered due to our choice of test data. As the 
method is global, it performs better when the target 
classes are represented by a high number of verbs. 
In our experiment, many semantic classes were rep- 
resented by two verbs only (section 2).

Nevertheless, the I B method has the clear advan-


L
nprevalent (ki )≥2



nprevalent (ki )


tage that it allows for more 
clusters to be produced. At 
best it classified half of the 
verbs correctly ac-


mP U R =


number of verbs	.


cording to their 
predominant sense 
(m


P U R


= 50%).


The modification we introduce to purity removes the 
bias towards the trivial configuration comprised of 
only singletons.

5.3   Evaluation Against the Predominant Sense

We first evaluated the clusters against the predom- 
inant sense, i.e. using the monosemous gold stan- 
dard.  The results, shown in Table 2, demonstrate 
that both clustering methods perform significantly

9 Our  definition  differs  by  a  factor  of  2  from  that  of
Schulte im Walde and Brew (2002).


Although this leaves room for improvement, the re- 
sult compares favourably to previously published re- 
sults10. We argue, however, that evaluation against a 
monosemous gold standard reveals only part of the 
picture.



  10 Due  to  differences in  task  definition and  experimental 
setup, a direct comparison with earlier results is impossible. 
For example, Stevenson and Joanis (2003) report an accuracy
of 29% (which implies mP U R ≤ 29%), but their task involves
classifying 841 verbs to 14 classes based on differences in the
predicate-argument structure.



Dif
fer
ent
S
en
se
s
Pai
rs
Fr
act
io
n
in 
clu
ste
r
0
1
2
3
4
3
9
8
5
6
2
5
12
84
14
37
5
1
%
1
0
%
7
%
3
%
3
%





Table 3: Evaluation against the monosemous (Pred.) and pol- 
ysemous (Multiple) gold standards. The figures in parentheses 
are results of evaluation on randomly polysemous data + sig- 
nificance of the actual figure. Results were obtained with fine- 
grained S C Fs (including prepositions).



5.4   Evaluation Against Multiple Senses

In evaluation against the polysemic gold standard, 
we assume that a verb which is polysemous in our 
corpus data may appear in a cluster with verbs that 
share any of its senses. In order to evaluate the clus- 
ters against polysemous data, we assigned each pol- 
ysemic verb V a single sense: the one it shares with 
the highest number of verbs in the cluster K (V ).
  Table 3 shows the results against polysemic and 
monosemous gold standards. The former are notice- 
ably better than the latter (e.g. I B with K = 42 is 9% 
better). Clearly, allowing for multiple gold standard 
classes makes it easier to obtain better results with 
evaluation.
  In order to show that polysemy makes a non- 
trivial contribution in shaping the clusters, we mea- 
sured the improvement that can be due to pure 
chance by creating randomly polysemous gold stan- 
dards. We constructed 100 sets of random gold stan- 
dards. In each iteration, the verbs kept their original 
predominant senses, but the set of additional senses 
was taken entirely from another verb - chosen at ran- 
dom. By doing so, we preserved the dominant sense 
of each verb, the total frequency of all senses and the 
correlations between the additional senses.
The  results included in  table  3  indicate,  with
99.5% confidence (3σ and above), that the improve- 
ment obtained with the polysemous gold standard is 
not artificial (except in two cases with 95% confi- 
dence).

5.5   Qualitative Analysis of Polysemy

We performed qualitative analysis to further inves- 
tigate the effect of polysemy on clustering perfor-


Table 4:  The fraction of verb pairs clustered together, as a 
function of the number of different senses between pair mem- 
bers (results of the N N algorithm)


Co
m
mo
n
S
e
ns
es
one 
irregu
lar
no 
irreg
ular

Pai
rs
in 
clu
ste
r
Pai
rs
in 
clu
ste
r
0
1
2
21
80
3
8
8
4
4
3
%
9
%
2
0
%
30
18
3
3
1
3
1
3
%
1
2
%
3
5
%

Table 5:  The fraction of verb pairs clustered together, as a 
function of the number of shared senses (results of the N N algo- 
rithm)


mance.  The results in table 4 demonstrate that the 
more two verbs differ in their senses, the lower their 
chance of ending up in the same cluster.  From the 
figures in table 5 we see that the probability of two 
verbs to appear in the same cluster increases with 
the number of senses they share. Interestingly, it is 
not only the degree of polysemy which influences 
the results, but also the type. For verb pairs where at 
least one of the members displays ‘irregular’ poly- 
semy (i.e. it does not share its full set of senses with 
any other verb), the probability of co-occurrence in 
the same cluster is far lower than for verbs which are 
polysemic in a ‘regular’ manner (Table 5).
  Manual cluster analysis against the polysemic 
gold standard revealed a yet more comprehensive 
picture. Consider the following clusters (the I B out- 
put with K = 42):

A1: talk (37), speak (37)
A2: look (30, 35), stare (30)
A3: focus (31, 45), concentrate (31, 45)
A4: add (22, 37, A56)

  We identified a close relation between the clus- 
tering performance and the following patterns of se- 
mantic behaviour:
  1) Monosemy:  We had 32 monosemous test 
verbs. 10 gold standard classes included 2 or more 
or  these.    7  classes were correctly acquired us- 
ing clustering (e.g. A1), indicating that clustering 
monosemous verbs is fairly ‘easy’.


  2) Predominant sense:  10 clusters were exam- 
ined by hand whose members got correctly classi- 
fied together, despite one of them being polysemous 
(e.g. A2). In 8 cases there was a clear indication in 
the data (when examining S C Fs and the selectional 
preferences on argument heads) that the polysemous 
verb indeed had its predominant sense in the rele- 
vant class and that the co-occurrence was not due to 
noise.
  3) Regular Polysemy: Several clusters were pro- 
duced which represent linguistically plausible inter- 
sective classes (e.g. A3) (Dang et al., 1998) rather 
than single classes.
  4) Irregular Polysemy: Verbs with irregular pol- 
ysemy11 were frequently assigned to singleton clus- 
ters.  For example, add (A4) has a ‘combining and 
attaching’ sense in class 22 which involves N P and 
P P  S C Fs and another ‘communication’ sense in 37 
which takes sentential S C Fs. Irregular polysemy was 
not a marginal phenomenon: it explains 5 of the 10 
singletons in our data.
  These observations confirm that evaluation 
against a polysemic gold standard is necessary in 
order to fully explain the results from clustering.

5.6   Qualitative Analysis of Errors
Finally, to provide feedback for further development 
of our verb classification approach, we performed a 
qualitative analysis of errors not resulting from poly- 
semy. Consider the following clusters (the I B output 
for K = 42):

B1: place (9), build (26, 45),
publish (26, 25), carve (21, 25, 26)
B2: sin (003), rain (57), snow (57, 002)
B3: agree (36, 22, A42), appear (020, 48, 29),
begin (55), continue (55, 47, 51)
B4: beg (015, 32)

Three main error types were identified:
  1) Syntactic idiosyncracy: This was the most fre- 
quent error type, exemplified in B1, where place is 
incorrectly clustered with build, publish and carve 
merely because it takes similar prepositions to these 
verbs (e.g. in, on, into).
2) Sparse data: Many of the low frequency verbs
(we had 12 with frequency less than 300) performed

11 Recall our definition of irregular polysemy, section 5.4.


poorly.  In B2, sin (which had 53 occurrences) is 
classified with rain and snow because it does not 
occur in our data with the preposition against - 
the ‘hallmark’ of its gold standard class (’Conspire 
Verbs’).
  3) Problems in S C F acquisition: These were not 
numerous but occurred e.g. when the system could 
not distinguish between different control (e.g. sub- 
ject/object equi/raising) constructions (B3).

6   Discussion and Conclusions

This paper has presented a novel approach to auto- 
matic semantic classification of verbs. This involved 
applying the N N and I B methods to cluster polysemic 
S C F distributions extracted from corpus data using 
Briscoe and Carroll’s (1997) system.  A principled 
evaluation scheme was introduced which enabled us 
to investigate the effect of polysemy on the resulting 
classification.
  Our investigation revealed that polysemy has a 
considerable impact on the clusters formed:  pol- 
ysemic verbs with a clear predominant sense and 
those with similar regular polysemy are frequently 
classified together. Homonymic verbs or verbs with 
strong irregular polysemy tend to resist any classifi- 
cation.
  While it is clear that evaluation should account 
for these cases rather than ignore them, the issue of 
polysemy is related to another, bigger issue: the po- 
tential and limitations of clustering in inducing se- 
mantic information from polysemic S C F data.  Our 
results show that it is unrealistic to expect that the
‘important’ (high frequency) verbs in language fall 
into classes corresponding to single senses.  How- 
ever, they also suggest that clustering can be used 
for novel, previously unexplored purposes:  to de- 
tect from corpus data general patterns of seman- 
tic behaviour (monosemy, predominant sense, reg- 
ular/irregular polysemy).
  In the future, we plan to investigate the use of soft 
clustering (without hardening the output) and de- 
velop methods for evaluating the soft output against 
polysemous gold standards.  We also plan to work 
on improving the accuracy of subcategorization ac- 
quisition, investigating the role of noise (irregular / 
regular) in clustering, examining whether different 
syntactic/semantic verb types require different ap-


proaches in clustering, developing our gold standard 
classification further, and extending our experiments 
to a larger number of verbs and verb classes.


References

B. Boguraev, E. J. Briscoe, J. Carroll, D. Carter, and 
C. Grover. 1987.  The derivation of a grammatically- 
indexed lexicon from the longman dictionary of con- 
temporary english.  In Proc. of the 25th  ACL, pages
193–200, Stanford, CA.

C. Brew and S. Schulte im Walde. 2002. Spectral clus- 
tering for german verbs.  In Conference on Empirical 
Methods in Natural Language Processing, Philadel- 
phia, USA.

E. J. Briscoe and J. Carroll. 1997. Automatic extraction 
of subcategorization from corpora. In 5th ACL Confer- 
ence on Applied Natural Language Processing, pages
356–363, Washington DC.

E. J. Briscoe and J. Carroll.  2002.  Robust accurate sta- 
tistical annotation of general text. In 3rd International 
Conference on Language Resources and Evaluation, 
pages 1499–1504, Las Palmas, Gran Canaria.

H. T. Dang, K. Kipper, M. Palmer, and J. Rosenzweig.
1998. Investigating regular sense extensions based on 
intersective Levin classes. In Proc. of COLING/ACL, 
pages 293–299, Montreal, Canada.

B. Dorr and D. Jones. 1996. Role of word sense disam- 
biguation in lexical acquisition: predicting semantics 
from syntactic cues. In 16th  International Conference 
on Computational Linguistics, pages 322–333, Copen- 
hagen, Denmark.

B. Dorr.    1997.    Large-scale dictionary construction 
for foreign language tutoring and interlingual machine 
translation. Machine Translation, 12(4):271–325.

R. Grishman, C. Macleod, and A. Meyers. 1994. Com- 
lex syntax: building a computational lexicon.  In In- 
ternational Conference on Computational Linguistics, 
pages 268–272, Kyoto, Japan.

R. Jackendoff.  1990.  Semantic Structures.  MIT Press, 
Cambridge, Massachusetts.

E. Joanis.  2002.  Automatic verb classification using a 
general feature space.  Master’s thesis, University of 
Toronto.

J. L. Klavans and M. Kan. 1998. Role of verbs in docu- 
ment analysis. In Proc. of COLING/ACL, pages 680–
686, Montreal, Canada.

A. Korhonen.    2002.    Subcategorization Acquisition.
Ph.D. thesis, University of Cambridge, UK.



A. Korhonen.   2003.   Extending Levin’s Classification 
with New Verb Classes. Unpublished manuscript, Uni- 
versity of Cambridge Computer Laboratory.

G. Leech. 1992. 100 million words of english: the british 
national corpus. Language Research, 28(1):1–13.

B. Levin. 1993. English Verb Classes and Alternations.
Chicago University Press, Chicago.

P. Merlo and S. Stevenson. 2001.  Automatic verb clas- 
sification based on statistical distributions of argument 
structure. Computational Linguistics, 27(3):373–408.

P. Merlo, S. Stevenson, V. Tsang, and G. Allaria.  2002.
A multilingual paradigm for automatic verb classifica- 
tion. In Proc. of the 40th  ACL, Pennsylvania, USA.

G.  A.  Miller.     1990.     WordNet:   An  on-line  lexi- 
cal database.  International Journal of Lexicography,
3(4):235–312.

S. Pinker. 1989. Learnability and Cognition: The Acqui- 
sition of Argument Structure. MIT Press, Cambridge, 
Massachusetts.

J. Preiss and A. Korhonen.  2002.  Improving subcate- 
gorization acquisition with WSD.  In ACL Workshop 
on Word Sense Disambiguation: Recent Successes and 
Future Directions, Philadelphia, USA.

D. Roland, D. Jurafsky, L. Menn, S. Gahl, E. Elder, and 
C. Riddoch. 2000.  Verb subcatecorization frequency 
differences between business-news and balanced cor- 
pora. In ACL Workshop on Comparing Corpora, pages
28–34.

S. Schulte im Walde and C. Brew.  2002.  Inducing ger- 
man semantic verb classes from purely syntactic sub- 
categorisation information. In Proc. of the 40th  ACL, 
Philadephia, USA.

S. Schulte im Walde.   2000.   Clustering verbs seman- 
tically according to their alternation behaviour.   In 
Proc. of COLING-2000, pages 747–753, Saarbru¨ cken, 
Germany.

S. Stevenson and E. Joanis.   2003.   Semi-supervised 
verb-class discovery using noisy features. In Proc. of 
CoNLL-2003, Edmonton, Canada.

N. Tishby, F. C. Pereira, and W. Bialek. 1999. The infor- 
mation bottleneck method. In Proc. of the 37th Annual 
Allerton Conference on Communication, Control and 
Computing, pages 368–377.
