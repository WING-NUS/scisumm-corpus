UGroningen:  Negation detection with Discourse Representation Structures



Valerio Basile and Johan Bos and Kilian Evang and Noortje Venhuizen
{v.basile,johan.bos,k.evang,n.j.venhuizen}@rug.nl
Center for Language and Cognition Groningen (CLCG)
University of Groningen, The Netherlands







Abstract


We use the NLP toolchain that is used to con- 
struct the Groningen Meaning Bank to address 
the task of detecting  negation cue and scope, 
as defined  in the shared task “Resolving  the 
Scope and Focus of Negation”. This toolchain 
applies the C&C tools for parsing, using the 
formalism of Combinatory Categorial Gram- 
mar, and applies Boxer to produce  seman- 
tic representations in the form of Discourse 
Representation Structures (DRSs). For nega- 
tion cue detection,  the DRSs are converted 
to flat, non-recursive  structures,  called Dis- 
course Representation Graphs (DRGs). DRGs 
simplify cue detection  by means of edge la- 
bels representing relations. Scope detection 
is done by gathering  the tokens  that occur 
within the scope of a negated  DRS. The re- 
sult is a system  that is fairly reliable for cue 
detection and scope detection. Furthermore, it 
provides  a fairly robust algorithm for detect- 
ing the negated event or property within the 
scope.


1   Introduction

Nothing is more home to semantics than the phe- 
nomenon of negation. In classical theories of mean- 
ing all states of affairs are divided in two truth val- 
ues, and negation  plays a central role to determine 
which truth value is at stake for a given sentence. 
Negation lies at the heart of deductive inference, of 
which consistency checking (searching for contra- 
dictions in texts) is a prime example in natural lan- 
guage understanding.
  It  shouldn’t therefore  come as  a  surprise   that 
detecting  negation and adequately representing its


scope is of utmost importance in computational  se- 
mantics. In this paper we present and evaluate a sys- 
tem that transforms texts into logical formulas – us- 
ing the C&C tools and Boxer (Bos, 2008) – in the 
context of the shared task on recognising negation 
in English texts (Morante and Blanco, 2012).
  We will first sketch the background and the basics 
of the formalism that we employ in our analysis of 
negation (Section 2). In Section 3 we explain how 
we detect negation cues and scope. Finally, in Sec- 
tion 4 we present the results obtained in the shared 
task, and we discuss them in Section 5.

2   Background

The semantic representations that are used in this 
shared task on detecting negation in texts are con- 
structed by means of a pipeline  of natural language 
processing components,  of which the backbone is 
provided by the C&C tools and Boxer (Curran et 
al., 2007). This tool chain is currently in use semi- 
automatically for constructing a large semantically 
annotated  corpus, the Groningen  Meaning Bank 
(Basile et al., 2012).
  The C&C tools are applied  for tagging the data 
with part-of-speech and super tags and for syntactic 
parsing, using the formalism  of Combinatory Cate- 
gorial Grammar, CCG (Steedman, 2001). The out- 
put of the parser,  CCG derivations,  form the in- 
put of Boxer, producing formal semantic representa- 
tions in the form of Discourse Representation Struc- 
tures (DRSs), the basic meaning-carrying  structures 
in the framework of Discourse Representation The- 
ory (Kamp and Reyle, 1993). DRT is a widely ac- 
cepted formal theory of natural language meaning 
that has been used to study a wide range of linguistic



301


First Joint Conference on Lexical and Computational  Semantics (*SEM),  pages 301–309, 
Montre´al,  Canada, June 7-8, 2012. Qc 2012 Association for Computational Linguistics


I
PRP


saw
VBD


nothing
DT


suspici
ous 	.
JJ	.


NP
λv0. (   x1 
person(x1)



α (v0 @ x1) )


(S[dcl]\NP)/NP
λv0. λv1. λv2. (v1 @ 
λv3. (v0 @ λv4. (   e5
s
e
e
(
e
5
)
 
a
g
e
n
t
(
e
5
,
 
v
3
)
 
p
a
t
i
e
n
t
(
e
5
,
 
v
4
)



; (v2 @ e5) ) ) )


NP
λv0.




 (   x1 thing(x1)




; (v0 @ x1) )


S[adj]\NP
λv0. λv1. (v0 @ λv2. (


suspicious
NP\NP




suspicious(v2)



; (v1 @ v2) ) )

*


S[dcl]\S[dcl]
λv0.v0





nothing suspicious
NP
λv0.


λv0. λv1. (v0 @ λv2. (

) )



suspicious(v2)


; (v1 @ v2)


<





>
saw nothing suspicious
S[dcl]\NP
λv0. λv1. (v0 @ λv2. 	)




<
I saw nothing suspicious
S[dcl]
λv0. (   x1 	α 	)
person(x1)



<
I saw nothing suspicious .
S[dcl]
λv0. (   x1 	α 	)
person(x1)




Figure 1: CCG derivation and unresolved semantics for the sentence “I saw nothing suspicious”




phenomena,  such as anaphoric pronouns, temporal 
relations (Kamp and Reyle, 1993), presuppositions 
(Van der Sandt, 1992), abstract anaphora and rhetor- 
ical relations (Asher, 1993; Asher and Lascarides,
2003).
  A DRS contains two parts:  a set of of discourse 
referents, and a set of conditions. Negation is repre- 
sented in a condition by a unary operator in DRT. As 
an example, Figure 1 shows the derivation  for one 
sentence  as produced  by the pipeline, illustrating 
how lexical semantic  entries  are used to construct 
a DRS for a whole sentence, guided by the syntac- 
tic parse tree. Here, machinery of the λ-calculus is 
employed to deal with variable renaming when re- 
quired.
  DRSs are recursive structures by nature. They can 
be produced in several formats (in Prolog or XML) 
and translated into first-order formulas. The rep- 
resentations can also be generated  as a set of tu- 
ples, forming together a directed  graph equivalent


to the original DRS, where discourse referents and 
symbols  are nodes and predicates and relations  are 
viewed  as labelled  edges.   These “flat” Discourse 
Representation Graphs, DRGs, are often more suit- 
able for certain processing tasks. The tuples also 
hold additional information,  mapping DRS condi- 
tions to surface tokens.  This mapping is important 
in tasks where surface realisation  plays a role. We 
also use it in this shared task to get back from com- 
plex structures to a flat, token-based annotation of 
scope.

3   Method

The shared task aims at detecting negation in text — 
systems are supposed to label tokens that are in the 
scope of negation,  and also identify the token that 
triggered the negation. The basic idea of our method 
was to run the existing Boxer system for semantic 
analysis, then traverse the produced DRSs, and, on 
encountering an embbeded negated DRS, output the


tokens associated with this negation,  as well as the 
token triggering it.
  As this isn’t what Boxer is usually  asked to do, 
it required some bookkeeping adjustments. Boxer’s 
anaphora resolution feature was turned off because 
it  is not necessary  for the task and would lead 
to unwanted inclusion of antecedents into negation 
scopes.  Also, its features for representing tense in- 
formation and rhetorical relations were not used.
  The rest of this section pays a closer look at how 
negation  cues are detected  and how scope  is as- 
signed to tokens.   We address the issues of trans- 
lating a formal  representation such as DRS into the 
format required by the shared task — a represen- 
tation more oriented at the surface form. We sub- 
mitted two runs of our system, which both used the 
C&C tools and Boxer. For the second run, we added 
some postprocessing  steps that tune the result to- 
wards a higher performance, especially on scope de- 
tection. While these postprocessing  steps improve 
performance, many of them may be specific to the 
genre and style of the texts used in the shared task.

3.1   Cue detection

Since Boxer has  been  developed   as a  system  to 
generate full semantic representations,  its lexicon 
implicitly contains  a list of negation cues: those 
words giving rise to semantic representations of the
form B,  where B  is the DRS representing the
meaning of the scope of the negation. Key examples
here are determiners and noun phrases (no, none, no- 
one), and verb phrase negation (not).
  However,  negation detection is not the primary 
function of Boxer,  as it is part of the larger aim of 
providing  interpretable semantic representation for 
English texts, and doing so robustly.  So for the cur- 
rent task, after investigating the development  data 
made available  by the organisers, Boxer’s lexicon 
was revised at a few points to account for particu- 
lar negation  cues that Boxer originally did not de- 
tect. This included the detection of never as negation 
cue, as well as words with a negative prefix or suffix 
(e.g. inadequate, motionless). These affix negations 
were detected using an automatically  generated list 
of negatively affixed nouns, adjectives and adverbs 
from WordNet (Fellbaum, 1998). The list was cre- 
ated by means of an algorithm that returns all nouns, 
adjectives  and adverbs in WordNet that start with


one of a, an, dis, in, il, im, ir, non, non-, un, or end 
with one of less, lessness, lessly,  and have a direct 
antonym such that the lemma form equals the stem 
of the affixed negation (i.e., without the affix).
  On the other hand, not everything that introduces 
a negated DRS  in Boxer is a typical negation cue. 
A case in point is the quantifier all, which up un- 
til the shared task received  a semantics  similar to
λP λQ.∃x(P (x) ∧ Q(x)) in Boxer’s lexicon. As
a consequence,  Boxer predicted all to be a nega-
tion cue trigger,  in contrast to the shared task gold 
standard data. Such instances were replaced by log- 
ically equivalent representations (in the case of all:
λP λQ.∀x(P (x) → Q(x))).
In order to obtain the tokens that triggered the
negated DRS, Boxer’s DRG output was used. Oc- 
currences of predicates, relations and connectives in 
the DRG output carry explicit associations with the 
tokens in the input whose lexical entries they come 
from. For basic cue detection, the system annotates 
as a negation cue those tokens (or affixes) associated
with the connective  (represented in the DRG  as the
relation subordinates:neg). Example (1) shows a
part of the DRG’s tuple format that represents the 
negation cue “no”.  Argument structure tuples (la- 
beled concept and instance) are also shown, cor- 
responding to a noun  in the negation  scope,  as in 
“no problem”. The first and third columns represent 
nodes of the DRG graph (both discourse units in this 
example), the second column represents the label of 
the edge between the nodes, and the fourth column 
shows the token associated with the relation (if any).



(1)



In this case, the token “no” is detected  as negation 
cue because it is associated with the relation subor- 
dinates:neg.
  In the case of affix negation, ideally only the af- 
fix should be associated with the negation tuple, and 
the stem with a corresponding  instance tuple. How- 
ever, since the last column contains tokens, this does 
not easily fit into the format. We therefore associate 
the whole affix-negated token with the negation tu- 
ple and use separate tuples for affix and stem in order 
to preserve the information  which part of the word


is the cue and which part is in the scope of the nega- 
tion. The resulting three tuples from a sentence con- 
taining the word “injustice” are shown in the follow- 
ing example:



(2)



The target nodes of the two argument structure tu- 
ples (labeled concept because “injustice” is a noun) 
are labeled with the relevant part of the affix-negated 
word, and a special  ID to indicate the presence of 
a prefix or suffix. This information is used by the 
script producing the token-based result format. Al- 
though multi-word cues, such as neither...nor and on 
the contrary, were not correctly predicted as such by 
Boxer, no effort was made to include them. Due to 
the token-based detection approach, the cue detec- 
tion algorithm would have to be severly complicated 
to include  these cases as one negation cue. Because 
of the relatively small frequency of multi-word  cues, 
we decided not to include  special processing steps to 
account for them.
  The second  run includes some postprocessing 
steps implemented  on top of the basic output. Since 
Boxer is not designed to deal with dialogue, inter- 
jections were originally ignored  as negation  cues. 
Therefore, the postprocessing script added the word 
“no” as a negation  cue (with empty scope) when it 
occurred as an interjection (tagged “UH”). It also ex- 
cluded negations with the cue “no” when occurring 
as part of the expression “no doubt” and not imme- 
diately preceded by a verb with the lemma “have” 
or “be” as in “I have no doubt that...”, which is to be 
annotated  as a negation.   High precision and recall 
for cue detection on the training  data suggested that 
no further  processing steps were worth the effort.

3.2   Scope detection

The tokens in the scope of a negation  are deter- 
mined on the basis of the detected negation cue. It 
is associated with the negation connective of some
negated DRS B, so the system annotates as scope
all the tokens (and stems in the case of affix nega-
tion) directly or indirectly associated with predicates 
and relations inside B.   This includes  tokens di- 
rectly associated with predicates that appear within


the negated DRS, as well as those predicates outside 
of the negated DRS whose discourse referent occurs 
within the negation scope as the second argument of 
a thematic role relation.

x2 
person(x2)
  x3 e4

see(e4) 
thing(x3) 
suspicious(x3) 
Agent(e4, x2) 
Theme(e4, x3)


Figure 2: DRS for the sentence “I saw nothing  suspi- 
cious”


  An example is given in Figure 2, where e.g. the 
tokens  see and suspicious  are associated,  respec- 
tively, with see(e4) and suspicious(x3). Although 
the predicate person(x2) associated with the pro- 
noun I occurs outside of the negated DRS, its refer- 
ent occurs as an argument  within the negated DRS 
in Agent(e4, x2) and therefore it is taken to be part 
of the scope of the negation.  The desired scope is 
thus detected, containing the tokens I, saw and sus- 
picious.
  Again, in the second  run some postprocessing 
steps were implemented to improve  performance. 
We observed that the scopes in the manually anno- 
tated data were usually continuous, except for nega- 
tion cues within them. However, the scopes pro- 
duced by the DRS algorithm contained many “gaps” 
between the tokens of the detected scope, due to an 
intrinsic feature of the DRS representation. Conven- 
tionally, DRSs only explicitly contain content words 
(i.e. nouns, verbs, adjectives, adverbs), while func- 
tion words, such as determiners,  modals and auxil- 
iary verbs, are represented e.g. as structural  proper- 
ties or temporal features, or not at all, as in the case 
of the infinitival to. Thus, when retrieving  the sur- 
face representation of the negated scopes from the 
DRSs, not all structural properties can be directly  as- 
sociated with a surface token and thus not all tokens 
required for the scope are retrieved.  Because in the 
gold standard annotation these function  words were 
considered part of the negation scope, we designed 
an ad hoc mechanism to include them, namely filling 
all the gaps that occur in the negation scope (leaving


out the negation cue). For the same reason, deter- 
miners immediately preceding the detected scopes 
were added  in postprocessing.    Finally, conjunc- 
tions were removed from the beginning of negation 
scopes, because they were sometimes wrongly  rec- 
ognized by our pipeline  as adverbs.



3.3   Negated event/property detection


Although not among our main goals, we also ad- 
dressed the issue of detecting the “negated event or 
property” in negation  scopes within factual state- 
ments. This is done using a heuristic algorithm  that


4   Results

Here we discuss our results on the Shared Task as 
compared to the gold standard annotations provided 
by (Morante and Daelemans, 2012). The output of 
our two runs will be discussed with respect to Task 1. 
The first run includes the results of our system with- 
out postprocessing  steps and in the second run the 
system is augmented with the postprocessing steps, 
as discussed in Section 3.
  During  the process of evaluating the results of the 
training  data, an issue with the method of evaluation 
was discovered. In the first version of the evaluation 
script precision  was calculated using the standard 
formula:     tp    .  However, partial matches are ex-


uses the detected  scope, as well as the syntax tree


cluded


tp+fp
from this


calculation  (they are only counted


provided  as part of the data.

  Since the scope is provided  as a set of tokens, the 
first step is to identify what we call the scope con- 
stituent, i.e. a constituent  in the syntax tree that cor- 
responds to the scope. This is done by going through 
the tokens in the scope from left to right and de- 
termining for each token the largest constituent that 
starts with this token. The first constituent found in 
this way the category of whose root is one of SBAR, 
S and VP is taken to be the scope constituent.


as false negatives),  which means that in the case 
of scopes(cue match), precision is calculated  as the 
number of exact scope matches (true positives)  di- 
vided by the number of exact scope matches plus 
the number of completely wrong instances with no 
overlap (false positives).  As precision is a measure 
for calculating  correctly  detected instances among 
all detected instances, it seems that partial  matches 
should also be taken into account  as detected  in- 
stance. Therefore, we proposed  a new evaluation 
method (B):     tp     , where system includes all 
de-


In the second step, the scope VP is determined


tected neg


system


the current  system (including



as the first VP encountered when doing a pre-order, 
left-to-right  traversal of the scope constituent.  The 
first verb directly dominated by this VP node deter- 
mines how the process continues:  (i) For non-factual 
modals (e.g. may, must, should), no event/property 
is annotated.  (ii) For futurity modals (e.g. would, 
will, shall), the negated event/property is determined 
recursively by taking the first embedded VP as the 
new scope VP. (iii) For forms of the verb be, the 
algorithm first looks for the head of an embedded 
ADJP or NP. If one is found, this is annotated  as a 
negated property. Otherwise, the verb is assumed to 
be a passive auxiliary and the negated event/property 
is again determined recursively on the basis of the 
first embedded VP. (iv) In all other cases, the verb 
itself is annotated as the negated event.

  To  limit  the undesired  detection of  negated 
events/properties outside of factual statements, the


ations of
partial matches). However, this measure may be too 
strict as it penalizes a system harder for outputting  a 
partially  correct scope than for outputting no scope 
at all.1  This choice between two evils seems to in- 
dicate that precision is too simple a measure for tar- 
gets where partial  matches are possible. Therefore, 
in our evaluation of scope detection, we will focus 
on the scope tokens measure where there are no par- 
tial matches. For cue and negated event/property de- 
tection, we use the stricter,  but more meaningful B 
version. The difference here is almost negligible be- 
cause these targets typically  have just one token.

4.1   Run 1 (without postprocessing)

Table 1 shows the results of the basic system with- 
out postprocessing, with the most important results 
for our system highlighted.  As we can see,  the 
basic system performs well on cue detection (F1=


algorithm is not applied to any sentence that con-	 	


tains a question mark.


1 This was pointed out by an 
anonymous reviewer.


Table 1: Results of the first run (without postprocessing)

Ta
sk
go
ld
sy
ste
m
t
p
f
p
f
n
pre
cisi
on 
(%
)
rec
all 
(%
)
F1 
(%
)
Cu
es:
S
c
o
p
e
s
(
c
u
e
 
m
a
t
c
h
)
:
 
S
c
o
p
e
s
(
n
o
 
c
u
e
 
m
a
t
c
h
)
:
S
co
pe 
to
ke
ns
(n
o 
cu
e 
m
at
ch
): 
N
eg
at
ed
(n
o 
cu
e 
m
at
c
h)
:
Ful
l 
ne
gat
ion
:
2
6
4
2
4
9
2
4
9
18
05
1
7
3
2
6
4
2
6
1
2
6
1
2
6
1
1
8
2
1
1
6
9
2
6
1
2
1
9
3
2
3
2
12
69
8
9
2
0
3
3
3
7
3
7
55
2
7
6
3
3
4
5
21
7
21
7
53
6
8
2
24
4
8
6
.
9
0
4
6
.
3
8
4
6
.
3
8
6
9
.
6
9
5
3
.
9
4
3
7
.
7
4
8
2
.
9
5
1
2
.
8
5
1
2
.
8
5
7
0
.
3
0
5
2
.
0
5
7
.
5
8
8
4.
8
8
2
0.
1
2
2
0.
1
2
6
9.
9
9
5
2.
9
8
1
2.
6
2
Cu
es 
B:
S
c
o
p
e
s
 
B
 
(
c
u
e
 
m
a
t
c
h
)
:
 
S
c
o
p
e
s
 
B
 
(
n
o
 
c
u
e
 
m
a
t
c
h
)
:
 
N
e
g
a
t
e
d
 
B
 
(
n
o
 
c
u
e
 
m
a
t
c
h
)
:
 
F
u
l
l
 
n
e
g
a
t
i
o
n
 
B
:
2
6
4
2
4
9
2
4
9
1
7
3
2
6
4
2
6
1
2
6
1
2
6
1
1
6
9
2
6
1
2
1
9
3
2
3
2
8
9
2
0
3
3
3
7
3
7
7
6
3
3
4
5
21
7
21
7
8
2
24
4
8
3
.
9
1
1
2
.
2
6
1
2
.
2
6
5
2
.
6
6
7
.
6
6
8
2
.
9
5
1
2
.
8
5
1
2
.
8
5
5
2
.
0
5
7
.
5
8
8
3.
4
3
1
2.
5
5
1
2.
5
5
5
2.
3
5
7
.
6
2


Table 2: Results of the second run (with postprocessing)

Ta
sk
go
ld
sy
ste
m
t
p
f
p
f
n
pre
cisi
on 
(%
)
rec
all 
(%
)
F1 
(%
)
Cu
es:
S
c
o
p
e
s
(
c
u
e
 
m
a
t
c
h
)
:
 
S
c
o
p
e
s
(
n
o
 
c
u
e
 
m
a
t
c
h
)
:
S
co
pe 
to
ke
ns
(n
o 
cu
e 
m
at
ch
): 
N
eg
at
ed
(n
o 
cu
e 
m
at
c
h)
:
Ful
l 
ne
gat
ion
:
2
6
4
2
4
9
2
4
9
18
05
1
7
3
2
6
4
2
6
1
2
5
6
2
5
6
2
1
4
6
2
0
1
2
6
1
2
2
4
1
0
2
1
0
2
14
85
1
1
1
7
2
2
8
3
2
3
2
66
1
8
5
2
8
4
0
14
7
14
7
32
0
5
9
19
2
8
8
.
8
9
7
6
.
1
2
7
6
.
1
2
6
9
.
2
0
5
6
.
6
3
7
2
.
0
0
8
4
.
8
5
4
0
.
9
6
4
0
.
9
6
8
2
.
2
7
6
5
.
2
9
2
7
.
2
7
8
6.
8
2
5
3.
2
6
5
3.
2
6
7
5.
1
7
6
0.
6
5
3
9.
5
6
Cu
es 
B:
S
c
o
p
e
s
 
B
 
(
c
u
e
 
m
a
t
c
h
)
:
 
S
c
o
p
e
s
 
B
 
(
n
o
 
c
u
e
 
m
a
t
c
h
)
:
 
N
e
g
a
t
e
d
 
B
 
(
n
o
 
c
u
e
 
m
a
t
c
h
)
:
 
F
u
l
l
 
n
e
g
a
t
i
o
n
 
B
:
2
6
4
2
4
9
2
4
9
1
7
3
2
6
4
2
6
1
2
5
6
2
5
6
2
0
1
2
6
1
2
2
4
1
0
2
1
0
2
1
1
1
7
2
2
8
3
2
3
2
8
5
2
8
4
0
14
7
14
7
5
9
19
2
8
5
.
8
2
3
9
.
8
4
3
9
.
8
4
5
5
.
2
2
2
7
.
5
9
8
4
.
8
5
4
0
.
9
6
4
0
.
9
6
6
5
.
2
9
2
7
.
2
7
8
5.
3
3
4
0.
3
9
4
0.
3
9
5
9.
8
3
2
7.
4
3




83.43%),  and reasonably well on the detection of 
scope tokens (F1= 69.99%).
  Note that the results for Scopes(cue match) and 
Scopes(no cue match) are the same for our system. 
Since we make use of token-based cue detection, the 
only cases of partial  cue detection are instances of 
multi-word cues, which, as discussed  above,  were 
not accounted for in our system. In these cases, the 
part of the cue that is not detected has a large chance 
of becoming part of the scope of the cue that is de- 
tected due to collocation. So, we hypothesize that 
Scopes(cue match) and Scopes(no cue match) are the 
same because in all cases of partial cue detection, the 
scope incorrectly  contains part of the gold-standard 
cue, which  affects both measures negatively.
  There is a large discrepancy  between the detec- 
tion of scope  tokens  and the detection  of com- 
plete  scopes, as the latter is low on both precision 
(12.26%) and recall (12.85%). The relatively high 
precision and recall for scope tokens (69.69%  and
70.30%,  respectively)  suggests that there are many 
cases of partial scope detection, i.e. cases where the 
scope is either under- or overdetected with respect


to the gold standard scope. Since the postprocessing 
steps for scope detection were developed to reduce 
exactly this under- and overdetection, we expect that 
the results for the second run are significantly better. 
The same holds for negated/event property detection 
(F1=  52.98%) since it uses the results from scope 
detection.

4.2   Run 2 (with postprocessing)

Table 2 reports the results of the extended system, 
which extends the basic system with postprocessing 
steps for cue detection and especially for scope de- 
tection. The postprocessing steps indeed result  in 
higher precision and recall for all tasks, except for 
Scope tokens, which  shows a negligible  decrease in 
precision (from 69.69% to 69.20%).  This suggests 
that there  are more cases  of overdetected  scopes 
than underdetected  scopes, because the number of 
wrongly  detected tokens (false positives) increased 
while the number of undetected scope tokens (false 
negatives)  decreased.  This is probably  due to the 
gap-filling mechanism that was implemented  as a 
postprocessing  step for scope detection, generaliz-


ing that all scopes should be continuous.  We will 
elaborate more on this point in the discussion in Sec- 
tion 5.
  As expected,  the detection of complete  scopes 
shows the highest increase in F1 score (from 12.55% 
to 40.39%). This indicates that the postprocessing 
steps effectively  targeted the weak points of the ba- 
sic system.
  While  there are  no postprocessing   steps  for 
negated event or property detection, the F1 score for 
this task also increases (from 52.35% to 59.83%), 
as expected, due to the improvement in scope detec- 
tion.

5   Discussion

Overall, we can say that both of our systems perform 
well on cue detection, with a small increase when in- 
cluding the postprocessing steps. This was expected 
since the postprocessing for cue detection targeted 
only two specific types of cues, namely, interjections 
and occurrences of “no doubt”.  The scope detection 
benefits consideraby from adding the postprocessing 
steps, as was their main goal. In the final results of 
the shared task, run 2 of our system ended second 
out of five in the open track, while run 1 was ranked 
last. We will here discuss some points that deserve 
special attention.

5.1   Affix Negation

As discussed above, affix negations received a spe- 
cial treatment because they were not originally de- 
tected as negation  cues in Boxer. In the DRS, the 
token containing the affixed negation cue is associ- 
ated with two predicates, representing the negative 
affix and the negated stem. The algorithm  secures 
that only the affix is annotated as the negation  cue 
and that the negated stem is annotated as part of the 
scope. An example of a sentence containing  affix 
negation is shown in (3) (cardboard 31).2

(3)
a.
[Y
o
u 
do 
yo
ur
se
lf 
an
] 
in
[ju
sti
ce
].
go
ld

b.
Y
o
u 
do 
yo
ur
se
lf 
an 
in
[ju
sti
ce
].
ru
n1

c.
Y
o
u 
do 
yo
ur
se
lf 
[a
n] 
in
[ju
sti
ce
].
ru
n2

  2 In the following, boldfaced  tokens represent the negation 
cues, brackets embed the scope and underlining  signifies  the 
negated event or property (subscripts added in case of multiple 
negation cues).



Table 3: Results of negated event/property detection on 
gold standard cue and scope annotation

Ta
sk
pr
ec.
(%
)
rec
.(
%)
F1
(%
)
Ne
gat
ed 
(no 
cue 
ma
tch
):
6
4
.
0
6
7
6
.
8
8
69
.8
9
Ne
gat
ed 
B 
(no 
cue 
ma
tch
):
5
9
.
7
1
7
6
.
8
8
67
.2
2


Note that in neither of the runs the complete scope 
from the gold standard is detected, although post- 
processing  increases the recall of scope tokens by 
adding the determiner “an” to the scope of the nega- 
tion. However, examples like this are not unambigu- 
ous with respect to their negation scope. For ex- 
ample, the sentence in (3) can be interpreted in two 
ways: “It is not the case that you do yourself (some- 
thing that is) justice” and “It is the case that you do 
yourself (something that is) not justice”. While the 
gold standard annotation  assumes the former,  wide- 
scope reading, our system predicts the narrow scope 
reading for the negation. The narrow  scope read- 
ing can be motivated by means of Grice’s Maxim of 
Manner (Grice, 1975); the choice of an affix nega- 
tion instead of a verbal  negation  signals  a narrow 
scope, because in case a wide  scope negation  is in- 
tended, a verbal  negation would be more perspicu- 
ous. Thus, the difference in the output of our sys- 
tem and the gold standard annotation is in this case 
caused by a different choice in disambiguating nega- 
tion scope, rather than by a shortcoming  of the de- 
tection algorithm.

5.2   Negated event/property detection

Although correct detection of the negated event or 
property was  not our prime concern,  the results 
obtained with our algorithm were quite promising. 
Among the systems participating  in the closed track 
of task 1, our extended system is ranked third out 
of seven for negated event/property  detection even 
though the performance on scope detection is lower 
than all of the other systems in this track.  Since 
negated event/property  detection depends on the de- 
tected scope, it seems that our heuristic algorithm 
for detecting the negated event/property  is very ro- 
bust against noisy input. The performance of the de- 
tection algorithm on the gold-standard annotation of 
scopes is shown in Table 3.  Although we cannot 
compare these results to the performance of other 
systems on the gold standard data, it should be noted


that the results shown  here are unmatched by the 
test results of any other system.  It would therefore 
be worthwile for future work to refine the negated 
event/property detection algorithm outlined here.

5.3   Postprocessing

The results for the two versions  of our system 
showed that the postprocessing steps implemented 
in the extended system improved the results consid- 
erably, especially for scope detection.  Example (4) 
(cardboard 62) shows the effect of postprocessing on 
the detection of scopes for negative interjections.

(4)
a.
“
N
o1
, [I 
sa
w]
2 
no
thi
ng
2.”
g
o
l
d

b.
c.
“[
N
o], 
[I 
sa
w] 
no
thi
ng
.”
“[
N
o1
, I 
sa
w]
2 
no
thi
ng
2.”
r
u
n
1
r
u
n
2
In Run 1, the system correctly  detects the cue “noth- 
ing” and the event  “saw”, although the detected 
scope is too wide due to an error in the output of 
the parser we used. In Run 2, postprocessing also 
correctly recognizes the interjection  “no” as a nega- 
tion cue. Gap filling in this case makes  the scope 
overdetection  worse by also adding the comma to 
the scope. A similar case of the overdetection of 
scope is shown in (5) (cardboard 85).

(5)	a.     [The box] is a half-pound box of honey- 
dew tobacco and [does] not [help us in 
any way].                                 gold
b.    [The box] is a half-pound  box of hon- 
eydew tobacco and does not [help us in 
any way].                                 run1
c.	[The box is a half-pound  box of honey- 
dew tobacco and does] not [help us in 
any way].                                 run2

Note that in Run 1 the first part of the coordinated 
structure is correctly excluded from the scope of 
the negation, but the auxiliary “does” is incorrectly 
not counted  as scope. The gap-filling mechanism 
then adds the intermediary  part to the scope of the 
negation, resulting in an increase in recall for scope 
tokens detection (since “does” is now part of the 
scope) but a lower  precision  because of the overgen- 
eration of the coordinated part.
  Nevertheless, the increased precision  and recall 
for scope detection  can mainly be ascribed to the 
gap-filling  mechanism implemented in the postpro-


cessing steps for scope detection. As discussed 
above, the presence of gaps in the original output 
is due to the non-sequential nature of the DRT rep- 
resentation and the fact that function words are not 
directly  associated with any element in the represen- 
tations. This suggests that future work  on surface re- 
alisation from DRSs should focus on translating the 
structural properties of DRSs into function words.

5.4   Differences between texts

We noted that there was a difference  between the 
performance on text 1 (The Adventure of the Red 
Circle) and text 2 (The Adventure of the Cardboard 
Box). The results for text 2 were overall higher than 
the results for text 1 (except for a 1% decline in re- 
call for Full negation). There was a higher  scope 
precision for text 2 and after the postprocessing steps 
an even larger difference  was found for scope de- 
tection (15% versus 44% increase in F1 score for 
Scopes). We hypothesize that this difference may be 
due to a higher number of multiword expressions in 
text 1 (7 vs. 2) and to the fact that text 1 seems to 
have more scopes containing  gaps.  This latter ob- 
servation is supported by the fact that gap filling re- 
sults in more overgeneration (more false positives), 
which is reflected in the ratios of false positives in 
text 1 (38%) and text 2 (27%). Thus, while the post- 
processing steps improve  performance,  they seem to 
be genre and style dependent. This motivates further 
development of the “clean”, theoretically motivated 
version of our system in order to secure domain- 
independent broad coverage of texts, which is the 
goal of the Groningen Meaning Bank project.

6   Conclusion

Participating in this shared task on negation detec- 
tion gave us a couple of interesting insights into our 
natural language processing pipeline that we are de- 
veloping in the context of the Groningen Meaning 
Bank. It also showed that it is not easy to trans- 
fer the information  about negation from a formal, 
logical representation of scope to a theory-neutral 
surface-oriented approach. The results were in line 
with what we expected beforehand, with the highest 
loss appearing in the awkward translation from one 
formalism to another.


References

Nicholas Asher and Alex Lascarides.  2003. Logics of 
conversation.  Studies in natural language processing. 
Cambridge University  Press.
Nicholas Asher. 1993. Reference to Abstract Objects in
Discourse. Kluwer Academic Publishers.
Valerio Basile, Johan Bos, Kilian Evang,  and Noortje 
Venhuizen.  2012. Developing  a large semantically  an- 
notated corpus. In Proceedings of the Eighth Interna- 
tional Conference on Language Resources and Evalu- 
ation (LREC’12). To appear.
Johan Bos.  2008.  Wide-Coverage  Semantic Analysis 
with Boxer. In J. Bos and R. Delmonte,  editors, Se- 
mantics in Text Processing.  STEP 2008 Conference 
Proceedings, volume 1 of Research in Computational 
Semantics, pages 277–286. College Publications.
James Curran, Stephen Clark,  and Johan Bos.  2007.  Lin- 
guistically Motivated Large-Scale NLP with C&C and 
Boxer. In Proceedings of the 45th Annual Meeting of 
the Association for Computational Linguistics Com- 
panion Volume Proceedings of the Demo and Poster 
Sessions, pages 33–36, Prague, Czech Republic.
Christiane Fellbaum, editor. 1998. WordNet. An Elec- 
tronic Lexical Database. The MIT Press.
H. P. Grice. 1975. Logic and conversation.   In P. Cole 
and J. L. Morgan, editors, Syntax and Semantics: Vol.
3:  Speech Acts, pages 41–58.  Academic  Press, San
Diego, CA.
Hans Kamp and Uwe Reyle. 1993. From Discourse to 
Logic; An Introduction to Modeltheoretic Semantics of 
Natural Language, Formal Logic and DRT. Kluwer, 
Dordrecht.
Roser Morante and Eduardo Blanco. 2012. *SEM 2012
Shared Task: Resolving Scope and Focus of Negation. 
In Proceedings of the First Joint Conference on Lexi- 
cal and Computational  Semantics (*SEM 2012), Mon- 
treal, Canada. To appear.
Roser Morante  and  Walter  Daelemans.	2012.
ConanDoyle-neg: Annotation of negation in Conan 
Doyle stories. In Proceedings of the Eighth Interna- 
tional Conference on Language Resources and Evalu- 
ation (LREC’12). To appear.
Mark Steedman. 2001. The Syntactic Process. The MIT 
Press.
Rob Van der Sandt.  1992. Presupposition  Projection  as
Anaphora Resolution. Journal of Semantics, 9:333–
377.

