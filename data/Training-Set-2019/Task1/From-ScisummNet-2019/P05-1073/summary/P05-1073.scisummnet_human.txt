Joint Learning Improves Semantic Role Labeling
Despite much recent progress on accurate semantic role labeling, previous work has largely used independent classifiers, possibly combined with separate label sequence models via Viterbi decoding.
This stands in stark contrast to the linguistic observation that a core argument frame is a joint structure, with strong dependencies between arguments.
We show how to build a joint model of argument frames, incorporating novel features that model these interactions into discriminative log-linear models.
This system achieves an error reduction of 22% on all arguments and 32% on core arguments over a state-of-the art independent classifier for gold-standard parse trees on PropBank.
We introduce a joint approach for SRL and demonstrate that a model that scores the full predicate argument structure of a parse tree could lead to significant error reduction over independent classifiers for each predicate-argument relation.
We employ decomposition for efficiency in training: that is, the decomposition allows us to train the classification models on a subset of training examples consisting only of those phrases that have a case marker.
