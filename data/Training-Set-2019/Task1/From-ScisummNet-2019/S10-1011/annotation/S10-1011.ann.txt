Citance Number: 1 | Reference Article:  S10-1011.txt | Citing Article:  D10-1073.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Finally in terms of evaluation, our future work also focuses on evaluating HRGs using a fine grained sense inventory, extending the evaluation on the SemEval-2010 WSI task dataset (Manandhar et al., 2010) as well as applying HRGs to other related tasks such as taxonomy learning.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The evaluation framework of SemEval-2010 WSI task considered two types of evaluation.</S><S sid = NA ssid = NA>Our future work will focus on the assessment of sense induction on a task-oriented basis as well as on clustering evaluation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  S10-1011.txt | Citing Article:  P12-1092.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Similar to Manandhar et al (2010), we use WordNet to first randomly select one synset of the first word, we then construct a set of words in various relations to the first word's chosen synset, including hypernyms, hyponyms, holonyms, meronyms and attributes.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The relations considered were WordNetâ€™s hypernyms, hyponyms, synonyms, meronyms and holonyms.</S><S sid = NA ssid = NA>The target word dataset consisted of 100 words, i.e.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  S10-1011.txt | Citing Article:  W11-1104.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Last, we include all the nouns and verbs used in the SemEval 2010 WSI Task (Manandhar et al, 2010), which are used in our evaluation.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Neither of these measures were used in the SemEval2007 WSI task.</S><S sid = NA ssid = NA>The evaluation framework of SemEval-2010 WSI task considered two types of evaluation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  S10-1011.txt | Citing Article:  E12-1060.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our primary WSI evaluation is based on the dataset provided by the SemEval-2010 WSI shared task (Manandhar et al 2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The evaluation framework of SemEval-2010 WSI task considered two types of evaluation.</S><S sid = NA ssid = NA>The primary aim of SemEval-2010 WSI task is to allow comparison of unsupervised word sense induction and disambiguation systems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  S10-1011.txt | Citing Article:  E12-1060.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The original task also made use of V-measure and Paired F-score to evaluate the induced word sense clusters, but have degenerate behaviour in correlating strongly with the number of senses induced by the method (Manandhar et al 2010), and are hence omitted from this paper.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For that reason, we introduced the second unsupervised evaluation measure (paired F-Score) that penalises systems when they produce: (1) a higher number of clusters (low recall) or (2) a lower number of clusters (low precision), than the GS number of senses.</S><S sid = NA ssid = NA>This section presents the measures of unsupervised evaluation, i.e V-Measure (Rosenberg and Hirschberg, 2007) and (2) paired F-Score (Artiles et al., 2009).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  S10-1011.txt | Citing Article:  W11-2214.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>As a second experiment, we analyze incorrect sense assignments on SemEval-2 Task 14 (Manandhar et al., 2010) to measure whether sense-relatedness biases which sense was incorrectly selected.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>SemEval-2010 Task 14: Word Sense Induction &#x26;Disambiguation</S><S sid = NA ssid = NA>The main difference of the SemEval-2010 as compared to the SemEval-2007 sense induction task is that the training and testing data are treated separately, i.e the testing data are only used for sense tagging, while the training data are only used for sense induction.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  S10-1011.txt | Citing Article:  P11-1148.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our word sense induction and disambiguation model is trained and tested on the dataset of the SEMEVAL-2010 WSI/WSD task (Manandhar et al, 2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>SemEval-2010 Task 14: Word Sense Induction &#x26;Disambiguation</S><S sid = NA ssid = NA>The primary aim of SemEval-2010 WSI task is to allow comparison of unsupervised word sense induction and disambiguation systems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  S10-1011.txt | Citing Article:  P11-1148.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In the paired F-Score (Artiles et al, 2009) evaluation, the clustering problem is transformed into a classification problem (Manandhar et al, 2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this evaluation, the clustering problem is transformed into a classification problem.</S><S sid = NA ssid = NA>This section presents the measures of unsupervised evaluation, i.e V-Measure (Rosenberg and Hirschberg, 2007) and (2) paired F-Score (Artiles et al., 2009).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  S10-1011.txt | Citing Article:  W12-3305.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>However, this induction step has proven to be greatly challenging, in the most recent shared tasks, induction systems either appear to perform poorly or fail to outperform the simple Most Frequent Sense baseline (Agirre and Soroa, 2007a; Manandhar et al, 2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Despite that, none of the systems outperform the MFS baseline.</S><S sid = NA ssid = NA>All systems outperform this baseline, apart from one, whose V-Measure is equal to 0.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  S10-1011.txt | Citing Article:  W12-3305.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We first propose evaluating ensemble configurations of Word Sense Induction models using the standard shared tasks from SemEval-1 (Agirre and Soroa, 2007a) and SemEval-2 (Manandhar et al, 2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>SemEval-2010 Task 14: Word Sense Induction &#x26;Disambiguation</S><S sid = NA ssid = NA>The primary aim of SemEval-2010 WSI task is to allow comparison of unsupervised word sense induction and disambiguation systems.</S> | Discourse Facet:  NA | Annotator: Automatic


