[
  {
    "citance_No": 1, 
    "citing_paper_id": "W02-2008", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "James R., Curran | Miles, Osborne", 
    "raw_text": "Smoothing typically adds considerable computational complexity to the system since multiple models need to be estimated and applied together, and it is often considered a black art (Chen and Goodman, 1996)", 
    "clean_text": "Smoothing typically adds considerable computational complexity to the system since multiple models need to be estimated and applied together, and it is often considered a black art (Chen and Goodman, 1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P14-1075", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Hayato, Kobayashi", 
    "raw_text": "Constant restoring is similar to the additive smoothing defined by p? (w)? p? (w)+?, which isused to solve the zero-frequency problem of language models (Chen and Goodman, 1996)", 
    "clean_text": "Constant restoring is similar to the additive smoothing, which isused to solve the zero-frequency problem of language models (Chen and Goodman, 1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "D11-1064", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Denis, Filimonov | Mary P., Harper", 
    "raw_text": "2We refer the reader to (Chen and Goodman, 1996) for a survey of the discounting methods for n-gram models", 
    "clean_text": "We refer the reader to (Chen and Goodman, 1996) for a survey of the discounting methods for n-gram models.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "W07-2044", 
    "citing_paper_authority": 14, 
    "citing_paper_authors": "Deniz, Yuret", 
    "raw_text": "I used a smoothing method loosely based on the one-count method given in (Chen and Goodman,1996)", 
    "clean_text": "I used a smoothing method loosely based on the one-count method given in (Chen and Goodman, 1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P09-2058", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Yonggang, Deng | Bowen, Zhou", 
    "raw_text": "The language model is a statistical 4-gram model estimated with Modified Kneser-Ney smoothing (Chen and Goodman, 1996) using only English sentences in the parallel training data", 
    "clean_text": "The language model is a statistical 4-gram model estimated with Modified Kneser-Ney smoothing (Chen and Goodman, 1996) using only English sentences in the parallel training data.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "W08-0326", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "John, Tinsley | Yanjun, Ma | Sylwia, Ozdowska | Andy, Way", 
    "raw_text": "This was done using the SRI Language Modelling toolkit (Stolcke, 2002) employing linear interpolation and modified Kneser Ney discounting (Chen and Goodman, 1996)", 
    "clean_text": "This was done using the SRI Language Modelling toolkit (Stolcke, 2002) employing linear interpolation and modified Kneser Ney discounting (Chen and Goodman, 1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "W09-1007", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Herman, Stehouwer | Menno, van Zaanen", 
    "raw_text": "Alternatively, smoothing techniques (Chen and Goodman, 1996) redistribute the probabilities, taking into account previously unseen word sequences", 
    "clean_text": "Alternatively, smoothing techniques (Chen and Goodman, 1996) redistribute the probabilities, taking into account previously unseen word sequences.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "W09-1007", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Herman, Stehouwer | Menno, van Zaanen", 
    "raw_text": "The n-gram language models described here are relatively simple, but more complex language models could improve performance. In particular, instead of back-off, smoothing techniques could be investigated to reduce the impact of zero probability problems (Chen and Goodman, 1996)", 
    "clean_text": "In particular, instead of back-off, smoothing techniques could be investigated to reduce the impact of zero probability problems (Chen and Goodman, 1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "N12-1005", 
    "citing_paper_authority": 27, 
    "citing_paper_authors": "Fran&ccedil;ois, Yvon | Hai-Son, Le | Alexandre, Allauzen", 
    "raw_text": "Another consequence is that phrase-based models usually consider a very restricted context1. This is a general issue in statistical Natural Language Processing (NLP) and many possible remedies have been proposed in the literature, such as, for instance, using smoothing techniques (Chen and Goodman, 1996), or working with linguistically enriched, or more abstract, representations", 
    "clean_text": "This is a general issue in statistical Natural Language Processing (NLP) and many possible remedies have been proposed in the literature, such as, for instance, using smoothing techniques (Chen and Goodman, 1996), or working with linguistically enriched, or more abstract, representations.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "N12-1005", 
    "citing_paper_authority": 27, 
    "citing_paper_authors": "Fran&ccedil;ois, Yvon | Hai-Son, Le | Alexandre, Allauzen", 
    "raw_text": "Smoothing techniques, such as Kneser-Ney and Witten-Bell back off schemes (see (Chen and Goodman, 1996) for an empirical overview, and (Teh, 2006) for a Bayesianinterpretation), perform back-off to lower order distributions, thus providing an estimate for the probability of these unseen events", 
    "clean_text": "Smoothing techniques, such as Kneser-Ney and Witten-Bell back off schemes (see (Chen and Goodman, 1996) for an empirical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off to lower order distributions, thus providing an estimate for the probability of these unseen events.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "D10-1076", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Hai-Son, Le | Alexandre, Allauzen | Guillaume, Wisniewski | Fran&ccedil;ois, Yvon", 
    "raw_text": "Conventional smoothing techniques, such as Kneser Ney and Witten-Bell back-off schemes (see (Chen and Goodman, 1996) for an empirical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off on lower order distributions to provide an estimate for the probability of these unseen events", 
    "clean_text": "Conventional smoothing techniques, such as Kneser Ney and Witten-Bell back-off schemes (see (Chen and Goodman, 1996) for an empirical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off on lower order distributions to provide an estimate for the probability of these unseen events.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "P09-1056", 
    "citing_paper_authority": 22, 
    "citing_paper_authors": "Fei, Huang | Alexander, Yates", 
    "raw_text": "Sophisticated smoothing techniques like modified Kneser-Ney and Katz smoothing (Chen and Goodman, 1996) smooth together the predictions of unigram ,bigram, trigram, and potentially higher n-gram sequences to obtain accurate probability estimates in the face of data sparsity", 
    "clean_text": "Sophisticated smoothing techniques like modified Kneser-Ney and Katz smoothing (Chen and Goodman, 1996) smooth together the predictions of unigram, bigram, trigram, and potentially higher n-gram sequences to obtain accurate probability estimates in the face of data sparsity.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "W11-2139", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Chris, Dyer | Kevin, Gimpel | Jonathan H., Clark | Noah A., Smith", 
    "raw_text": "A 4-gram modified Kneser-Ney language model (Chen and Goodman, 1996) was constructed using the SRI language modeling toolkit (Stolcke, 2002) from the English side of the parallel text, the monolingual English data, and the English version 4 Giga word corpus (Parker et al, 2009)", 
    "clean_text": "A 4-gram modified Kneser-Ney language model (Chen and Goodman, 1996) was constructed using the SRI language modeling toolkit (Stolcke, 2002) from the English side of the parallel text, the monolingual English data, and the English version 4 Giga word corpus (Parker et al, 2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "D12-1075", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Dan, Garrette | Jason, Baldridge", 
    "raw_text": "We use one-count smoothing (Chen and Goodman, 1996), where? (ti) is based on the number of words that occur with ti once:? (ti)= |wi: C (ti ,wi)= 1| Since open-class tags occur more frequently with words that appear once, they will reserve more mass for unknown words than closed-class tags will", 
    "clean_text": "We use one-count smoothing (Chen and Goodman, 1996), where (ti) is based on the number of words that occur with ti once: (ti)= |wi: C (ti ,wi)= 1|.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "W12-3131", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Michael, Denkowski | Greg, Hanneman | Alon, Lavie", 
    "raw_text": "Modified Knesser-Ney smoothed (Chen and Goodman, 1996) n-gram language models are built from the monolingual English data using the SRIlanguage modeling toolkit (Stolke, 2002)", 
    "clean_text": "Modified Knesser-Ney smoothed (Chen and Goodman, 1996) n-gram language models are built from the monolingual English data using the SRI language modeling toolkit (Stolke, 2002).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "W09-0417", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Alexandre, Allauzen | Josep M., Crego | Aur&eacute;lien, Max | Fran&ccedil;ois, Yvon", 
    "raw_text": "A conventional ngram language model of the target language provides the third component of the system. In all our experiments, we used 4-gram reordering models and bilingual tuple models built using Kneser-Ney back off (Chen and Goodman, 1996)", 
    "clean_text": "A conventional ngram language model of the target language provides the third component of the system. In all our experiments, we used 4-gram reordering models and bilingual tuple models built using Kneser-Ney back off (Chen and Goodman, 1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "D09-1116", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "Denis, Filimonov | Mary P., Harper", 
    "raw_text": "This type of model is called an ngram model: p (w i |w i? 1 1)? p (w i |w i? 1 i ?n+1) Even with limited context, the parameter space canbe quite sparse and requires sophisticated techniques for reliable probability estimation (Chen and Goodman, 1996)", 
    "clean_text": "Even with limited context, the parameter space can be quite sparse and requires sophisticated techniques for reliable probability estimation (Chen and Goodman, 1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "D09-1116", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "Denis, Filimonov | Mary P., Harper", 
    "raw_text": "3We used one-count smoothing (Chen and Goodman, 1996)", 
    "clean_text": "We used one-count smoothing (Chen and Goodman, 1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "D10-1018", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Wei, Lu | Hwee Tou, Ng", 
    "raw_text": "When training all the language models, modified Kneser-Ney smoothing (Chen and Goodman, 1996) for n-grams is used. To assess the performance of the punctuation prediction task, we compute precision (prec.), recall (rec.), and F1-measure (F1), as defined by the fol lowing equations :prec", 
    "clean_text": "When training all the language models, modified Kneser-Ney smoothing (Chen and Goodman, 1996) for n-grams is used.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "W12-3160", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Vladimir, Eidelman", 
    "raw_text": "We constructed a 5-gram language model from the provided English News monolingual training data as well as the English side of the parallel corpus using the SRIlanguage modeling toolkit with modified Kneser Ney smoothing (Chen and Goodman, 1996)", 
    "clean_text": "We constructed a 5-gram language model from the provided English News monolingual training data as well as the English side of the parallel corpus using the SRI language modeling toolkit with modified Kneser Ney smoothing (Chen and Goodman, 1996).", 
    "keep_for_gold": 0
  }
]