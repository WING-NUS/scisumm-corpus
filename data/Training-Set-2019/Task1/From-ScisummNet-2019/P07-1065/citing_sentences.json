[
  {
    "citance_No": 1, 
    "citing_paper_id": "D07-1049", 
    "citing_paper_authority": 19, 
    "citing_paper_authors": "David, Talbot | Miles, Osborne", 
    "raw_text": "Our framework makes use of the log-frequency Bloom filter presented in (Talbot and Osborne, 2007), and described briefly below, to compute smoothed conditional n-gram probabilities on the fly", 
    "clean_text": "Our framework makes use of the log-frequency Bloom filter presented in (Talbot and Osborne, 2007), and described briefly below, to compute smoothed conditional n-gram probabilities on the fly.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "D07-1049", 
    "citing_paper_authority": 19, 
    "citing_paper_authors": "David, Talbot | Miles, Osborne", 
    "raw_text": "Recent work (Talbot and Osborne, 2007) presented a scheme for associating static frequency information with a set of n-grams in a BFefficiently.1 3.1 Log-frequency Bloom filter", 
    "clean_text": "Recent work (Talbot and Osborne, 2007) presented a scheme for associating static frequency information with a set of n-grams in a BF efficiently.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "D07-1049", 
    "citing_paper_authority": 19, 
    "citing_paper_authors": "David, Talbot | Miles, Osborne", 
    "raw_text": "As noted in Talbot and Osborne (2007), errors for this log-frequency BF scheme are one-sided: frequencies will never be underestimated", 
    "clean_text": "As noted in Talbot and Osborne (2007), errors for this log-frequency BF scheme are one-sided: frequencies will never be underestimated.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "D07-1049", 
    "citing_paper_authority": 19, 
    "citing_paper_authors": "David, Talbot | Miles, Osborne", 
    "raw_text": "3.2.1 Proxy items There is a potential risk of redundancy if we rep resent related statistics using the log-frequency BF scheme presented in Talbot and Osborne (2007)", 
    "clean_text": "There is a potential risk of redundancy if we represent related statistics using the log-frequency BF scheme presented in Talbot and Osborne (2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "D07-1049", 
    "citing_paper_authority": 19, 
    "citing_paper_authors": "David, Talbot | Miles, Osborne", 
    "raw_text": "We refer to (Talbot and Osborne, 2007) for empirical results establishing the performance of the log frequency BF-LM: overestimation errors occur with 474 0.01 0.025 0.05 0.1 0.25 0.5 0.03 0.02 0.01 0.005 0.002 5 0.001 Mean squared error of log probabilites Memory in G B MSE bet we en WB 3-gra m SR ILM and BF -LMs Base 3 Base 1.5 Base 1.1 Figure 5: MSE between SRILM and BF-LMs 22 23 24 25 26 27 28 29 30 0.0 1 0.1 1 BLEU Score Meansqua red error WB-smooth ed BF -LM 3 -gram mode l BF-LM base 1.1 BF-LM base 1.5 BF-LM base 3 Figure 6: MSE vs. BLEU for WB 3-gram BF-LMs a probability that decays exponentially in the size of the overestimation error", 
    "clean_text": "We refer to (Talbot and Osborne, 2007) for empirical results establishing the performance of the log frequency BF-LM: overestimation errors occur with a probability that decays exponentially in the size of the overestimation error.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "D07-1049", 
    "citing_paper_authority": 19, 
    "citing_paper_authors": "David, Talbot | Miles, Osborne", 
    "raw_text": "We hope the present work will, together with Talbot and Osborne (2007), establish the Bloom filter as a practical alternative to conventional associative data structures used in computational linguistics", 
    "clean_text": "We hope the present work will, together with Talbot and Osborne (2007), establish the Bloom filter as a practical alternative to conventional associative data structures used in computational linguistics.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "P14-2112", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Miles, Osborne | Ashwin, Lall | Benjamin, van Durme", 
    "raw_text": "Work by Talbot and Osborne (2007), Van Durmeand Lall (2009) and Goyal et al (2009) cons id ered the problem of building very large language models via the use of randomized data structures known as sketches", 
    "clean_text": "Work by Talbot and Osborne (2007), Van Durme and Lall (2009) and Goyal et al (2009) considered the problem of building very large language models via the use of randomized data structures known as sketches.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "P08-1058", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "David, Talbot | Thorsten, Brants", 
    "raw_text": "Recent work (Talbot and Osborne,2007b) has demonstrated that randomized encodings can be used to represent n-gram counts for LMs with signficant space-savings, circumventing information-theoretic constraints on lossless data structures by allowing errors with some small prob ability", 
    "clean_text": "Recent work (Talbot and Osborne, 2007b) has demonstrated that randomized encodings can be used to represent n-gram counts for LMs with signficant space-savings, circumventing information-theoretic constraints on lossless data structures by allowing errors with some small probability.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "P08-1058", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "David, Talbot | Thorsten, Brants", 
    "raw_text": "Hence as the training corpus and vocabulary grow, a model will require more space per parameter. However, if we are willing to accept that occasionally our model will be unable to distinguish be tween distinct n-grams, then it is possible to store each parameter in constant space independent of both n and the vocabulary size (Carter et al, 1978), (Talbot and Osborne, 2007a)", 
    "clean_text": "However, if we are willing to accept that occasionally our model will be unable to distinguish between distinct n-grams, then it is possible to store each parameter in constant space independent of both n and the vocabulary size (Carter et al, 1978), (Talbot and Osborne, 2007a).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "P08-1058", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "David, Talbot | Thorsten, Brants", 
    "raw_text": "Recent work (Talbot and Osborne, 2007b) has used lossy encodings based on Bloom filters (Bloom,1970) to represent logarithmically quantized corpus statistics for language modeling", 
    "clean_text": "Recent work (Talbot and Osborne, 2007b) has used lossy encodings based on Bloom filters (Bloom, 1970) to represent logarithmically quantized corpus statistics for language modeling.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "P08-1058", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "David, Talbot | Thorsten, Brants", 
    "raw_text": "Note that unlike the constructions in (Talbot and Osborne, 2007b) and (Church et al, 2007) no errors are possible for n grams stored in the model", 
    "clean_text": "Note that unlike the constructions in (Talbot and Osborne, 2007b) and (Church et al, 2007) no errors are possible for n grams stored in the model.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "P08-1058", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "David, Talbot | Thorsten, Brants", 
    "raw_text": "Fol lowing (Talbot and Osborne, 2007a) we can avoid unnecessary false positives by not querying for the longer n-gram in such cases", 
    "clean_text": "Following (Talbot and Osborne, 2007a) we can avoid unnecessary false positives by not querying for the longer n-gram in such cases.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "W09-0424", 
    "citing_paper_authority": 47, 
    "citing_paper_authors": "Zhifei, Li | Chris, Callison-Burch | Chris, Dyer | Sanjeev P., Khudanpur | Lane, Schwartz | Wren N. G., Thornton | Jonathan, Weese | Omar F., Zaidan", 
    "raw_text": "We have also implemented a Bloom Filter LM in Joshua, following Talbot and Osborne (2007)", 
    "clean_text": "We have also implemented a Bloom Filter LM in Joshua, following Talbot and Osborne (2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "W10-1728", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "J&ouml;rg, Tiedemann", 
    "raw_text": "All of them were estimated using the SRILM toolkit except the English News LM for which we applied RandLM (Talbot and Osborne, 2007) to cope with the large amount of training data", 
    "clean_text": "All of them were estimated using the SRILM toolkit except the English News LM for which we applied RandLM (Talbot and Osborne, 2007) to cope with the large amount of training data.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "W09-0420", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "Alexander, Fraser", 
    "raw_text": "RANDLM (Talbot and Osborne, 2007) performs well and scaled to the full data with improvement (resulting in our best overall system)", 
    "clean_text": "RANDLM (Talbot and Osborne, 2007) performs well and scaled to the full data with improvement (resulting in our best overall system).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "D12-1052", 
    "citing_paper_authority": 13, 
    "citing_paper_authors": "Daniel, Dahlmeier | Hwee Tou, Ng", 
    "raw_text": "For language modeling, we use RandLM (Talbot and Osborne, 2007)", 
    "clean_text": "For language modeling, we use RandLM (Talbot and Osborne, 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "W10-1724", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Markus, Saers | Joakim, Nivre | Dekai, Wu", 
    "raw_text": "system, but with RandLM (Talbot and Osborne, 2007) as language model rather than SRILM", 
    "clean_text": "The system we submitted corresponds to the GIZA++ and SBLITG (only news) system, but with RandLM (Talbot and Osborne, 2007) as language model rather than SRILM.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "W11-2123", 
    "citing_paper_authority": 83, 
    "citing_paper_authors": "Kenneth, Heafield", 
    "raw_text": "RandLM 0.2 (Talbot and Osborne, 2007) stores large-scale models in less memory using randomized data structures", 
    "clean_text": "RandLM 0.2 (Talbot and Osborne, 2007) stores large-scale models in less memory using randomized data structures.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "W11-2123", 
    "citing_paper_authority": 83, 
    "citing_paper_authors": "Kenneth, Heafield", 
    "raw_text": "Lossy compressed models RandLM (Talbot and Osborne, 2007) and Sheffield (Guthrie and Hepple,2010) offer better memory consumption at the expense of CPU and accuracy", 
    "clean_text": "Lossy compressed models RandLM (Talbot and Osborne, 2007) and Sheffield (Guthrie and Hepple,2010) offer better memory consumption at the expense of CPU and accuracy.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "N09-1058", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Amit, Goyal | Hal, Daum&eacute; III | Suresh, Venkatasubramanian", 
    "raw_text": "If we take a model of size comparable to count cutoff of 100 ,i.e., with?= 5e-7, we see both count-based pruning as well as entropy pruning per forms the same. There also have been prior work on maintaining approximate counts for higher-order language models (LMs) ((Talbot and Osborne, 2007a; Tal bot and Osborne, 2007b; Talbot and Brants, 2008)) operates under the model that the goal is to store a compressed representation of a disk-resident table of counts and use this compressed representation to answer count queries approximately", 
    "clean_text": "There also have been prior work on maintaining approximate counts for higher-order language models (LMs) ((Talbot and Osborne, 2007a) operates under the model that the goal is to store a compressed representation of a disk-resident table of counts and use this compressed representation to answer count queries approximately.", 
    "keep_for_gold": 0
  }
]