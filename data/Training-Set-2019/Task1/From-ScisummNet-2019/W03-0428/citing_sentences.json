[
  {
    "citance_No": 1, 
    "citing_paper_id": "W03-0419", 
    "citing_paper_authority": 127, 
    "citing_paper_authors": "Erik F., Tjong Kim Sang | Fien, De Meulder", 
    "raw_text": "Klein et al (2003) also applied the related Conditional Markov Models for combining clas sifiers. Learning methods that were based on connection ist approaches were applied by four systems", 
    "clean_text": "Klein et al (2003) also applied the related Conditional Markov Models for combining classifiers.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "W03-0419", 
    "citing_paper_authority": 127, 
    "citing_paper_authors": "Erik F., Tjong Kim Sang | Fien, De Meulder", 
    "raw_text": "Klein et al (2003) employed a stacked learning system which contains Hidden Markov Models, Maximum Entropy Models and Conditional Markov Models", 
    "clean_text": "Klein et al (2003) employed a stacked learning system which contains Hidden Markov Models, Maximum Entropy Models and Conditional Markov Models.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "W03-0419", 
    "citing_paper_authority": 127, 
    "citing_paper_authors": "Erik F., Tjong Kim Sang | Fien, De Meulder", 
    "raw_text": "Here there is no significant difference between them and the systems of Klein et al (2003) and Zhang and Johnson (2003) .We have combined the results of the sixteen system in order to see if there was room for improvement", 
    "clean_text": "Here there is no significant difference between them and the systems of Klein et al (2003) and Zhang and Johnson (2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "W03-0419", 
    "citing_paper_authority": 127, 
    "citing_paper_authors": "Erik F., Tjong Kim Sang | Fien, De Meulder", 
    "raw_text": "The performance of the system of Chieu et al (2003) was not significantly different from the best performance for English and the method of Klein et al (2003) and the approach of Zhang and Johnson (2003) were not significantly worse than the best result for German", 
    "clean_text": "The performance of the system of Chieu et al (2003) was not significantly different from the best performance for English and the method of Klein et al (2003) and the approach of Zhang and Johnson (2003) were not significantly worse than the best result for German.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P05-1001", 
    "citing_paper_authority": 36, 
    "citing_paper_authors": "Rie Kubota, Ando | Tong, Zhang", 
    "raw_text": "Previous best results: FIJZ03 (Florian et al, 2003), CN03 (Chieu and Ng, 2003), KSNM03 (Klein et al, 2003)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "E06-3004", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Zornitsa, Kozareva", 
    "raw_text": "(Klein et al, 2003), (Mayfield et al, 2003), (Wu et al, 2003), (Kozareva et al, 2005c) among others, combined several classifiers to obtain better named entity coverage rate", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "I08-5010", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Praneeth M., Shishtla | Prasad, Pingali | Vasudeva, Varma", 
    "raw_text": "Character n-gram based approach (Klein et al, 2003) using generative mod els, was experimented on English language and it proved to be useful over the word based models", 
    "clean_text": "Character n-gram based approach (Klein et al, 2003) using generative models, was experimented on English language and it proved to be useful over the word based models.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "I08-5015", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Praneeth M., Shishtla | Karthik, Gali | Prasad, Pingali | Vasudeva, Varma", 
    "raw_text": "We plan to experiment with the character n-gram approach (Klein et al, 2003) and include gazetteer information", 
    "clean_text": "We plan to experiment with the character n-gram approach (Klein et al, 2003) and include gazetteer information.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "P05-2023", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Beatrice, Alex", 
    "raw_text": "We experimented with a conditional Markov model tagger that performed well on language-independent NER (Klein et al, 2003) and the identification of gene and protein names (Finkel et al, 2005)", 
    "clean_text": "We experimented with a conditional Markov model tagger that performed well on language-independent NER (Klein et al, 2003) and the identification of gene and protein names (Finkel et al, 2005).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "W10-3102", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Hercules, Dalianis | Maria, Skeppstedt", 
    "raw_text": "Where the best results for English with Stanford NER CRF gave a precision of 86.1 percent, a recall of 86.5 percent and F-score of 86.3 percent, for German the best results hada precision of 80.4 percent, a recall of 65.0 per cent and an F-score of 71.9 percent, (Klein et al, 2003)", 
    "clean_text": "Where the best results for English with Stanford NER CRF gave a precision of 86.1 percent, a recall of 86.5 percent and F-score of 86.3 percent, for German the best results had a precision of 80.4 percent, a recall of 65.0 percent and an F-score of 71.9 percent, (Klein et al, 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "P11-3019", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Reyyan, Yeniterzi", 
    "raw_text": "Another approach we will also focus is dividing words into characters and applying character-level models (Klein et al, 2003)", 
    "clean_text": "Another approach we will also focus is dividing words into characters and applying character-level models (Klein et al, 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "I08-1071", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Wisam, Dakka | Silviu, Cucerzan", 
    "raw_text": "Since the introduction of this task in MUC-6 (Grishman and Sundheim, 1996), numerous systems using various ways of exploiting entity-specific and local context features were proposed, from relatively simple character based models such as Cucerzan and Yarowsky (2002) and Klein et al (2003) to complex models making use of various lexical, syntactic ,morpho logical, and orthographical information, such as Wacholder et al (1997), Fleischman and Hovy (2002), and Florian et al (2003)", 
    "clean_text": "Since the introduction of this task in MUC-6 (Grishman and Sundheim, 1996), numerous systems using various ways of exploiting entity-specific and local context features were proposed, from relatively simple character based models such as Cucerzan and Yarowsky (2002) and Klein et al (2003) to complex models making use of various lexical, syntactic, morphological, and orthographical information, such as Wacholder et al (1997), Fleischman and Hovy (2002), and Florian et al (2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "W04-1217", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "Jenny Rose, Finkel | Shipra, Dingare | Huy, Nguyen | Malvina, Nissim | Christopher D., Manning | Gail, Sinclair", 
    "raw_text": "Our system is a Maximum Entropy Markov Model, which further develops a system earlier used for the CoNLL 2003 shared task (Klein et al, 2003) and the 2004 BioCreative critical assessment of information", 
    "clean_text": "Our system is a Maximum Entropy Markov Model, which further develops a system earlier used for the CoNLL 2003 shared task (Klein et al, 2003) and the 2004 BioCreative critical assessment of information.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "W07-1712", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Sophia, Katrenko | Pieter, Adriaans", 
    "raw_text": "2Sometimes, these types of features are referred to as word-external and word-internal (Klein et al, 2003) The feature set of some NER methods (Wu, 2002) also includes part-of-speech information and/orword prefixes and suffixes", 
    "clean_text": "Sometimes, these types of features are referred to as word-external and word-internal (Klein et al, 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "D11-1144", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Duangmanee, Putthividhya | Jun, Hu", 
    "raw_text": "In addition, because of data sparsity (out-of-vocabulary) problem due to the long-tailed distribution of words in natural language ,sophisticated unknown word models are generally needed for good performance (Klein et al 2003)", 
    "clean_text": "In addition, because of data sparsity (out-of-vocabulary) problem due to the long-tailed distribution of words in natural language, sophisticated unknown word models are generally needed for good performance (Klein et al 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "D11-1144", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Duangmanee, Putthividhya | Jun, Hu", 
    "raw_text": "For our supervised NER system, we use the following features, as described in detail in Table 1, as input to the discriminative classifiers. The use of char N -gram (N -gram substring )features was inspired by the work of (Klein et al 2003), where the introduction of such features has been shown to improve the overall F1 score by over 20% .In (Kanaris et al 2006), char N -gram features consistently outperform word features in learning effective spam classifiers", 
    "clean_text": "The use of char N-gram (N-gram substring) features was inspired by the work of (Klein et al 2003), where the introduction of such features has been shown to improve the overall F1 score by over 20%.", 
    "keep_for_gold": 1
  }
]