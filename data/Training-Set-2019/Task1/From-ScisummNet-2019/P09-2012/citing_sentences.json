[
  {
    "citance_No": 1, 
    "citing_paper_id": "P10-1112", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Mohit, Bansal | Dan, Klein", 
    "raw_text": "For example, our best result on the English WSJ task is an F1 of over 88%, where recent TSGparsers2 achieve 82-84% and top refinement-based parsers3 achieve 88-90% (e.g., Table 5) .Rather than select fragments, we use a simplification of the PCFG-reduction of DOP (Goodman, 2Zuidema (2007), Cohn et al (2009), Post and Gildea (2009)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "D11-1008", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Federico, Sangati | Willem, Zuidema", 
    "raw_text": "78.5 17.2 77.4 16.0 FRAGMENT-BASED PARSERSZuidema (2007)* 83.8 26.9 Cohn et al (2010) MRS 85.4 27.2 84.7 25.8Post and Gildea (2009) 82.6- Bansal and Klein (2010) MCP 88.5 33.0 87.6 30.8 Bansal and Klein (2010) MCP 88.7 33.8 88.1 31.7+ Additional Refinement THIS PAPER Double-DOP 87.7 33.1 86.8 31.0 Double-DOP Lex smooth", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "D11-1067", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Spence, Green | Marie-Catherine, de Marneffe | John, Bauer | Christopher D., Manning", 
    "raw_text": "where c? V are non-terminals; 4Similar models were developed independently by O? Donnell et al (2009) and Post and Gildea (2009)", 
    "clean_text": "Similar models were developed independently by O'Donnell et al. (2009) and Post and Gildea (2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "P12-2038", 
    "citing_paper_authority": 13, 
    "citing_paper_authors": "Benjamin, Swanson | Eugene, Charniak", 
    "raw_text": "A more principled technique is to use a sparse nonparametric prior, as was recently presented by Cohn et al (2009) and Post and Gildea (2009)", 
    "clean_text": "A more principled technique is to use a sparse nonparametric prior, as was recently presented by Cohn et al (2009) and Post and Gildea (2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P12-1046", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Hiroyuki, Shindo | Yusuke, Miyao | Akinori, Fujino | Masaaki, Nagata", 
    "raw_text": "40) F1 (all) TSG (no symbol refinement) Post and Gildea (2009) 82.6 Cohn et al (2010) 85.4 84.7 TSG with Symbol Refinement Zuidema (2007) *83.8 Bansal et al (2010) 88.7 88.1 SR-TSG (single) 91.6 91.1 SR-TSG (multiple) 92.9 92.4 CFG with Symbol Refinement Collins (1999) 88.6 88.2 Petrov and Klein (2007) 90.6 90.1 Petrov (2010) 91.8 Discriminative Carreras et al (2008) 91.1 Charniak and Johnson (2005) 92.0 91.4 Huang (2008) 92.3 91.7 Table 3: Our parsing performance for the testing set compared with those of other parsers", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "P11-2036", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Hiroyuki, Shindo | Akinori, Fujino | Masaaki, Nagata", 
    "raw_text": "trees) F1 CFG 35374 () 71.0 BTSG 80026 (0) 85.0 BTSG+ insertion 65099 (25) 85.3 (Post and Gildea, 2009) 82.62 (Cohn and Blunsom, 2010) 85.3 Table 3: Full Penn Treebank dataset experiments words using lexical features", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "P11-2038", 
    "citing_paper_authority": 11, 
    "citing_paper_authors": "Matt, Post", 
    "raw_text": "Of these approaches, work in Bayesian learning of TSGs produces intuitive grammars in a principled way, and has demonstrated potential in language modeling tasks (Post and Gildea, 2009b; Post, 2010)", 
    "clean_text": "Of these approaches, work in Bayesian learning of TSGs produces intuitive grammars in a principled way, and has demonstrated potential in language modeling tasks (Post and Gildea, 2009b; Post, 2010).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "P11-2038", 
    "citing_paper_authority": 11, 
    "citing_paper_authors": "Matt, Post", 
    "raw_text": "A Bayesian-learned tree substitution grammar (Post and Gildea, 2009a) 2 2The sampler was run with the default settings for 1,000iterations, and a grammar of 192,667 fragments was then extracted from counts taken from every 10th iteration between iterations 500 and 1,000, inclusive", 
    "clean_text": "A Bayesian-learned tree substitution grammar (Post and Gildea, 2009a).", 
    "keep_for_gold": 0
  }
]