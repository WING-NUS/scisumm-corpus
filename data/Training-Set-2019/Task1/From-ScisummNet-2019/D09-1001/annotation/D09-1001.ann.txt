Citance Number: 1 | Reference Article:  D09-1001.txt | Citing Article:  E12-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Take the following example from Poon and Domingos (2009), in which the same semantic relation can be expressed by a transitive verb or an attributive prepositional phrase: (1) Utah borders Idaho.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For example, the aforementioned sentence can be rephrased as “Utah is next to Idaho,”“Utah shares a border with Idaho,” etc.</S><S sid = NA ssid = NA>For example, to express the meaning that Utah borders Idaho, we can use any form in the cluster representing the next-to relation (e.g., “borders”, “shares a border with”), any form in the cluster representing the state of Utah (e.g., “the beehive state”), and any form in the cluster representing the state of Idaho (e.g., “Idaho”).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  D09-1001.txt | Citing Article:  E12-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>To do so, we semi-automatically restrict the question-answer pairs by using the output of an unsupervised clustering semantic parser (Poon and Domingos, 2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We present the first unsupervised approach to the problem of learning a semantic parser, using Markov logic.</S><S sid = NA ssid = NA>Unsupervised Semantic Parsing</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  D09-1001.txt | Citing Article:  E12-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We used the question-answer pairs extracted by the Poon and Domingos (2009) semantic parser from the GENIA biomedical corpus that have been manually checked to be correct (295 pairs).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The second one (KW-SYN) is informed by syntax: the answer is extracted from the subject or object of the verb, depending on the question.</S><S sid = NA ssid = NA>It obtained the highest accuracy at 88%, and the number of correct answers it extracted is three times that of the second highest system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  D09-1001.txt | Citing Article:  P13-1042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>More recent examples of similar techniques include the Resolver system (Yates and Etzioni, 2009) and Poon and Domingos's USP system (Poon and Domingos, 2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>RESOLVER: RESOLVER (Yates and Etzioni, 2009) inputs TextRunner triples and collectively resolves coreferent relation and argument strings.</S><S sid = NA ssid = NA>Even an efficient sampler like MC-SAT (Poon and Domingos, 2006), as used in Poon & Domingos (2008), would have a hard time generating accurate estimates within a reasonable amount of time.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  D09-1001.txt | Citing Article:  P11-1060.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This paper introduces the first unsupervised approach to learning semantic parsers.</S><S sid = NA ssid = NA>Unsupervised Semantic Parsing</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  D09-1001.txt | Citing Article:  P11-1145.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Following Poon and Domingos (2009), we consider a semantic parsing setting where the goal is to (1) decompose the syntactic dependency tree of a sentence into fragments, (2) assign each of these fragments to a cluster of semantically equivalent syntactic structures, and (3) predict predicate-argument relations between the fragments.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The semantic parse of a sentence is derived by starting with logical forms in the lexical entries and recursively composing the meaning of larger fragments from their parts.</S><S sid = NA ssid = NA>In particular, lexical entries are no longer limited to be adjacent words as in Zettlemoyer and Collins (2005), but can be arbitrary fragments in a dependency tree.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  D09-1001.txt | Citing Article:  P11-1145.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our non-parametric model automatically discovers granularity of clustering appropriate for the dataset, unlike the parametric method of (Poon and Domingos, 2009) which have to perform model selection and use heuristics to penalize more complex models of semantics.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>To model distributions over lambda forms, we introduce the predicates Form(p, f!) and ArgForm(p, i, f!</S><S sid = NA ssid = NA>The first formula models the mixture of core forms given the cluster, and the others model the mixtures of argument forms, argument types, and argument numbers, respectively, given the argument type.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  D09-1001.txt | Citing Article:  P11-1145.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In our case, the state space size equals the total number of distinct semantic clusters, and, thus, is expected to be exceedingly large even for moderate datasets: for example, the MLN model induces 18,543 distinct clusters from 18,471 sentences of the GENIA corpus (Poon and Domingos, 2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Each lambda-form cluster may contain some number of argument types, which cluster distinct forms of the same argument in a relation.</S><S sid = NA ssid = NA>On the GENIA data, using the default parameters, RESOLVER produces only a few trivial relation clusters and no argument clusters.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  D09-1001.txt | Citing Article:  P11-1145.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In both cases, we follow (Poon and Domingos, 2009) in using the corpus of biomedical abstracts.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We evaluate our approach by using it to extract a knowledge base from biomedical abstracts and answer questions.</S><S sid = NA ssid = NA>In this paper, we applied USP to extracting knowledge from biomedical abstracts and evaluated its performance in answering a set of questions that simulate the information needs of biomedical researchers.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  D09-1001.txt | Citing Article:  P11-1145.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Second, lambda calculus is a considerably more powerful formalism than the predicate-argument structure used in frame semantics, normally supporting quantification and logical connectors (for example, negation and disjunction), neither of which is modeled by our model or in (Poon and Domingos, 2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Compared to phrase-structure syntax, dependency trees are the more appropriate starting point for semantic processing, as they already exhibit much of the relation-argument structure at the lexical level.</S><S sid = NA ssid = NA>The first formula models the mixture of core forms given the cluster, and the others model the mixtures of argument forms, argument types, and argument numbers, respectively, given the argument type.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  D09-1001.txt | Citing Article:  P11-1145.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Semantic classes correspond to lambda-form clusters in (Poon and Domingos, 2009) terminology.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Lambda-form clusters abstract away syntactic variations of the same meaning.</S><S sid = NA ssid = NA>The MAP semantic parse of a sentence is obtained by recursively assigning its parts to lambda-form clusters and composing them.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  D09-1001.txt | Citing Article:  P11-1145.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The work of (Poon and Domingos, 2009) models joint probability of the dependency tree and its latent semantic representation using Markov Logic Networks (MLNs) (Richardson and Domingos, 2006), selecting parameters (weights of first-order clauses) to maximize the probability of the observed dependency structures.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this paper we develop the first unsupervised approach to semantic parsing, using Markov logic (Richardson and Domingos, 2006).</S><S sid = NA ssid = NA>One of the most powerful representations for this is Markov logic, which is a probabilistic extension of first-order logic (Richardson and Domingos, 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  D09-1001.txt | Citing Article:  P11-1145.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In order to overcome this problem, (Poon and Domingos, 2009) group parameters and impose local normalization constraints within each group.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>To address this problem, we impose local normalization constraints on specific groups of formulas that are mutually exclusive and exhaustive, i.e., in each group, we require that Eki=1 ewi = 1, where wi are the weights of formulas in the group.</S><S sid = NA ssid = NA>Specifically, for the rule p E +c n Form(p,+f), all instances given a fixed c form a group; for each of the remaining three rules, all instances given a fixed a form a group.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  D09-1001.txt | Citing Article:  P11-1145.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We now turn to the QA task and compare our model (USP-BAYES) with the results of baselines considered in (Poon and Domingos, 2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>USP substantially outperforms TextRunner, DIRT and an informed baseline on both precision and recall on this task.</S><S sid = NA ssid = NA>Finally, we present our experiments and results.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  D09-1001.txt | Citing Article:  D12-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Other approaches are completely unsupervised, but do not tie the language to an existing meaning representation (Poon and Domingos, 2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Existing approaches differ in the meaning representation languages they use and the amount of annotation required.</S><S sid = NA ssid = NA>The standard language for formal meaning representation is first-order logic.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  D09-1001.txt | Citing Article:  W10-0907.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>USP (Poon and Domingos, 2009) is based on Markov Logic Networks and attempts to create a full semantic parse in an unsupervised fashion.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this paper we develop the first unsupervised approach to semantic parsing, using Markov logic (Richardson and Domingos, 2006).</S><S sid = NA ssid = NA>We present the first unsupervised approach to the problem of learning a semantic parser, using Markov logic.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  D09-1001.txt | Citing Article:  W11-0112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Markov Logic has been used previously in other NLP application (e.g. Poon and Domingos (2009)).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Finally, we present the Markov logic network (MLN) used by USP.</S><S sid = NA ssid = NA>We then describe our Markov logic network for unsupervised semantic parsing, and the learning and inference algorithms we used.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  D09-1001.txt | Citing Article:  D12-1111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Despite the existence of a large amount of related work in the literature, distinguishing synonyms and antonyms is still considered as a difficult open problem in general (Poon and Domingos, 2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In general, this is a difficult open problem that only recently has started to receive some attention (Mohammad et al., 2008).</S><S sid = NA ssid = NA>Resolving this is not the focus of this paper, but we describe a general heuristic for fixing this problem.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  D09-1001.txt | Citing Article:  C10-1007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Dependency information is useful for a wealth of natural language processing tasks, including question answering (Wang et al, 2007), semantic parsing (Poon and Domingos, 2009), and machine translation (Galley and Manning, 2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Unsupervised approaches have been applied to shallow semantic tasks (e.g., paraphrasing (Lin and Pantel, 2001), information extraction (Banko et al., 2007)), but not to semantic parsing.</S><S sid = NA ssid = NA>In the past, unsupervised approaches have been applied to some semantic tasks, but not to semantic parsing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  D09-1001.txt | Citing Article:  P10-1098.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Such annotated resources are scarce and expensive to create, motivating the need for unsupervised or semi-supervised techniques (Poon and Domingos, 2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For larger k and m, this is very expensive.</S><S sid = NA ssid = NA>To simulate the real information need, we sample the relations from the 100 most frequently used verbs (excluding the auxiliary verbs be, have, and do), and sample the entities from those annotated in GENIA, both according to their numbers of occurrences.</S> | Discourse Facet:  NA | Annotator: Automatic


