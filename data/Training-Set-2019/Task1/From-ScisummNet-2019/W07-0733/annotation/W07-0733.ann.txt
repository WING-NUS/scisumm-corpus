Citance Number: 1 | Reference Article:  W07-0733.txt | Citing Article:  D07-1091.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Factored translation models have also been used for the integration of CCG supertags (Birch et al, 2007), domain adaptation (Koehn and Schroeder, 2007) and for the improvement of English-Czech translation (Bojar, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This alternate decoding path model was developed by Birch et al. (2007).</S><S sid = NA ssid = NA>The open source Moses (Koehn et al., 2007) MT system was originally developed at the University of Edinburgh and received a major boost through a 2007 Johns Hopkins workshop.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W07-0733.txt | Citing Article:  P13-2058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The novel aspect of task alternation introduced in this paper can be applied to all approaches incorporating SMT for sentence retrieval from comparable data. For our baseline system we use in-domain language models (Bertoldi and Federico, 2009) and meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We trained two language models, one for each the out-of-domain and the in-domain training data.</S><S sid = NA ssid = NA>All perform better than the three baseline approaches.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W07-0733.txt | Citing Article:  W08-0327.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Work on domain adaptation for statistical machine translation (Koehn and Schroeder, 2007) tries to bring solutions to this issue.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Experiments in Domain Adaptation for Statistical Machine Translation</S><S sid = NA ssid = NA>Since training data for statistical machine translation is typically collected opportunistically from wherever it is available, the application domain for a machine translation system may be very different from the domain of the system’s training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W07-0733.txt | Citing Article:  W09-0421.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Based on Koehn and Schroeder (2007) we adapted our system from last year, which was focused on Europarl, to perform well on test data.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For the Europarl test sets, we did not use any domain adaptation techniques, but simply used either just the Europarl training data or the combined data — whatever gave the higher score on the development test set, although scores differed by only about 0.1–0.2 %BLEU.</S><S sid = NA ssid = NA>All perform better than the three baseline approaches.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W07-0733.txt | Citing Article:  W09-0405.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Although the Moses decoder is able to work with two phrase tables at once (Koehn and Schroeder, 2007), it is difficult to use this method when there is more than one additional model.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Here, we take advantage of a feature of the Moses decoder’s factored translation model framework.</S><S sid = NA ssid = NA>We decided to use the interpolated language model method described in Section 2.5.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W07-0733.txt | Citing Article:  W08-0321.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Genre adaptation is one of the major challenges in statistical machine translation since translation models suffer from data sparseness (Koehn and Schroeder, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Experiments in Domain Adaptation for Statistical Machine Translation</S><S sid = NA ssid = NA>Since training data for statistical machine translation is typically collected opportunistically from wherever it is available, the application domain for a machine translation system may be very different from the domain of the system’s training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W07-0733.txt | Citing Article:  W11-2211.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Combining multiple translation models has been investigated for domain adaptation by Foster and Kuhn (2007) and Koehn and Schroeder (2007) before.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Experiments in Domain Adaptation for Statistical Machine Translation</S><S sid = NA ssid = NA>We trained two language models, one for each the out-of-domain and the in-domain training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W07-0733.txt | Citing Article:  E12-1083.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The pooled model for pairing data from abstracts and claims is trained on data composed of 250,000 sentences from each text section. Another approach to exploit commonalities between tasks is to train separate language and translation models on the sentences from each task and combine the models in the global log-linear model of the SMT framework, following Foster and Kuhn (2007) and Koehn and Schroeder (2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We trained two language models, one for each the out-of-domain and the in-domain training data.</S><S sid = NA ssid = NA>The log-linear modeling approach of statistical machine translation enables a straight-forward combination of the in-domain and out-of-domain language models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W07-0733.txt | Citing Article:  N10-1062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>See, for example, Koehn and Schroeder (2007) or Bertoldi and Federico (2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The open source Moses (Koehn et al., 2007) MT system was originally developed at the University of Edinburgh and received a major boost through a 2007 Johns Hopkins workshop.</S><S sid = NA ssid = NA>For the WMT 2007 shared task, the challenge was to use a large amount of out-of-domain training data Proceedings of the Second Workshop on Statistical Machine Translation, pages 224–227, Prague, June 2007. c�2007 Association for Computational Linguistics (about 40 million words) combined with a much smaller amount of in-domain training data (about 1 million words) to optimize translation performance on that particular domain.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W07-0733.txt | Citing Article:  W11-2130.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Separate 5-gram language models were built from the target side of the two data set sand then they were interpolated using weights chosen to minimise the perplexity on the tuning set (Koehn and Schroeder, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Since we want to obtain a language model that gives us the best performance on the target domain, we set this weight so that the perplexity of the development set from that target domain is optimized.</S><S sid = NA ssid = NA>We included them as two separate features, whose weights are set with minimum error rate training.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W07-0733.txt | Citing Article:  W09-0414.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We implemented a TM interpolation strategy following the ideas proposed in (Schwenk and Estve, 2008), where the authors present a promising technique of target LMs linear interpolation; in (Koehn and Schroeder, 2007) where a log-linear combination of TMs is performed; and specifically in (Foster and Kuhn, 2007) where the authors present various ways of TM combination and analyze in detail the TM domain adaptation.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The log-linear modeling approach of statistical machine translation enables a straight-forward combination of the in-domain and out-of-domain language models.</S><S sid = NA ssid = NA>Language modeling software such as the SRILM toolkit we used (Stolke, 2002) allows the interpolation of these language models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W07-0733.txt | Citing Article:  C10-2124.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In (Koehn and Schroeder, 2007), different ways to combine available data belonging to two different sources was explored; in (Bertoldi and Federico, 2009) similar experiments were performed, but considering only additional source data.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Since training data for statistical machine translation is typically collected opportunistically from wherever it is available, the application domain for a machine translation system may be very different from the domain of the system’s training data.</S><S sid = NA ssid = NA>The open source Moses (Koehn et al., 2007) MT system was originally developed at the University of Edinburgh and received a major boost through a 2007 Johns Hopkins workshop.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W07-0733.txt | Citing Article:  W10-1759.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>(Koehn and Schroeder, 2007) used two language models and two translation models: one in-domain and other out-of-domain to adapt the system.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We trained two language models, one for each the out-of-domain and the in-domain training data.</S><S sid = NA ssid = NA>In our next setup, we used only in-domain data for training the language model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W07-0733.txt | Citing Article:  P13-1126.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>(Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The log-linear modeling approach of statistical machine translation enables a straight-forward combination of the in-domain and out-of-domain language models.</S><S sid = NA ssid = NA>The relative weight for each model is set directly by optimizing translation performance.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W07-0733.txt | Citing Article:  P13-1126.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>One is the log-linear combination of TMs trained on each subcorpus (Koehn and Schroeder, 2007), with weights of each model tuned under minimal error rate training using MIRA.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Again, respective weights are set with minimum error rate training.</S><S sid = NA ssid = NA>We included them as two separate features, whose weights are set with minimum error rate training.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W07-0733.txt | Citing Article:  W11-2133.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Researchers such as Foster and Kuhn (2007) and Koehn and Schroeder (2007) have investigated mixture model approaches to adaptation.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>All perform better than the three baseline approaches.</S><S sid = NA ssid = NA>The open source Moses (Koehn et al., 2007) MT system was originally developed at the University of Edinburgh and received a major boost through a 2007 Johns Hopkins workshop.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W07-0733.txt | Citing Article:  W11-2133.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Koehn and Schroeder (2007) learn mixture weights for language models trained with in-domain and out of-domain data respectively by minimizing the perplexity of a tuning (development) set and interpolating the models.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We trained two language models, one for each the out-of-domain and the in-domain training data.</S><S sid = NA ssid = NA>When interpolating, we give the out-of-domain language model a weight in respect to the in-domain language model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W07-0733.txt | Citing Article:  W12-3153.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Further approaches to domain adaptation for SMT include adaptation using in-domain language models (Bertoldi and Federico, 2009), meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007), or translation model adaptation using self-translations of in-domain source language texts (Ueffing et al, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We trained two language models, one for each the out-of-domain and the in-domain training data.</S><S sid = NA ssid = NA>Experiments in Domain Adaptation for Statistical Machine Translation</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W07-0733.txt | Citing Article:  W12-3153.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>There were three different adaptation measures: First, the turker-generated development set was used for optimizing the weights of the decoding meta parameters, as introduced by Koehn and Schroeder (2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The relative weight for each model is set directly by optimizing translation performance.</S><S sid = NA ssid = NA>4.3 Training and decoding parameters We tried to improve performance by increasing some of the limits imposed on the training and decoding setup.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W07-0733.txt | Citing Article:  W12-3153.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>HR0011-06-C-0022 and in part under the EuroMatrix project funded by the European Commission (6th Framework Programme).</S><S sid = NA ssid = NA>The best-scoring translation is searched for by the decoding algorithm and outputted by the system as the best translation.</S> | Discourse Facet:  NA | Annotator: Automatic


