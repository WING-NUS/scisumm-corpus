Citance Number: 1 | Reference Article:  P98-2182.txt | Citing Article:  W99-0609.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We chose three clusters produced by a program similar to Roark and Charniak (1998) except that it is based on a generative probability model and tries to classify all nouns rather than just those in pre-selected clusters.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Whereas the R&S algorithm produced just 11 terms not already present in Wordnet for the two categories combined, our algorithm produced 106, or over 3 for every 5 valid terms produced.</S><S sid = NA ssid = NA>For the final ranking, we chose the log likelihood statistic outlined in Dunning (1993), which is based upon the co-occurrence counts of all nouns (see Dunning for details).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P98-2182.txt | Citing Article:  N03-1036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This problem is addressed by Riloff and Shepherd (1997), Roark and Charniak (1998) and more recently by Widdows and Dorow (2002).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In Riloff and Shepherd (1997), noun co-occurrence statistics were used to indicate nominal category membership, for the purpose of aiding in the construction of semantic lexicons.</S><S sid = NA ssid = NA>The solution to this problem is to make the initial seed word selection from among the most frequent head nouns in the corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P98-2182.txt | Citing Article:  P09-1031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Inspired by the conjunction and appositive structures, Riloff and Shepherd (1997), Roark and Charniak (1998) used co occurrence statistics in local context to discover sibling relations.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In Riloff and Shepherd (1997), noun co-occurrence statistics were used to indicate nominal category membership, for the purpose of aiding in the construction of semantic lexicons.</S><S sid = NA ssid = NA>In our algorithm, co-occurrence is only counted within a noun phrase, between head nouns that are separated by a comma or conjunction.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P98-2182.txt | Citing Article:  W02-1028.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Rilo and Shepherd (Rilo and Shepherd, 1997) developed a bootstrap ping algorithm that exploits lexical co-occurrence statistics, and Roark and Charniak (Roark and Charniak, 1998) re ned this algorithm to focus more explicitly on certain syntactic structures.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In Riloff and Shepherd (1997), noun co-occurrence statistics were used to indicate nominal category membership, for the purpose of aiding in the construction of semantic lexicons.</S><S sid = NA ssid = NA>It is for this reason that we are billing our algorithm as something that could enhance existing broadcoverage resources with domain-specific lexical information.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P98-2182.txt | Citing Article:  C02-1114.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The main results to date in the field of automatic lexical acquisition are concerned with extracting lists of words reckoned to belong together in a particular category, such as vehicles or weapons (Riloff and Shepherd, 1997) (Roarkand Charniak, 1998).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>If one of those weapons occurs frequently enough, the scores for the words that it co-occurs with may exceed those of any vehicles, and this effect may be strong enough that no vehicles are selected in any future iteration.</S><S sid = NA ssid = NA>In Riloff and Shepherd (1997), noun co-occurrence statistics were used to indicate nominal category membership, for the purpose of aiding in the construction of semantic lexicons.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P98-2182.txt | Citing Article:  C02-1114.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Roark and Charniak describe a "generic algorithm" for extracting such lists of similar words using the notion of semantic similarity, as follows (Roark and Charniak, 1998).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The algorithm then proceeds as follows:</S><S sid = NA ssid = NA>To identify conjunctions, lists, and appositives, we first parsed the corpus, using an efficient statistical parser (Charniak et al., 1998), trained on the Penn Wall Street Journal Treebank (Marcus et al., 1993).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P98-2182.txt | Citing Article:  C02-1114.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Algorithms of this type were used by Riloff and Shepherd (1997) and Roark and Charniak (1998), reporting accuracies of 17% and 35% respectively.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In Riloff and Shepherd (1997), noun co-occurrence statistics were used to indicate nominal category membership, for the purpose of aiding in the construction of semantic lexicons.</S><S sid = NA ssid = NA>R&S reported a ratio of .17 valid to total entries for both the vehicle and weapon categories (see table 2).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P98-2182.txt | Citing Article:  C02-1114.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Since lists are usually comprised of objects which are similar in some way, these relationships have been used to extract lists of nouns with similar properties (Riloff and Shepherd, 1997) (Roark and Charniak, 1998).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This is similar to the figure of merit used in R&S, and also tends to promote low frequency nouns.</S><S sid = NA ssid = NA>In Riloff and Shepherd (1997), noun co-occurrence statistics were used to indicate nominal category membership, for the purpose of aiding in the construction of semantic lexicons.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P98-2182.txt | Citing Article:  C02-1114.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our results are an order of magnitude better than those reported by Riloff and Shepherd (1997) and Roark and Charniak (1998), who report average accuracies of 17% and 35% respectively.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>R&S reported a ratio of .17 valid to total entries for both the vehicle and weapon categories (see table 2).</S><S sid = NA ssid = NA>In Riloff and Shepherd (1997), noun co-occurrence statistics were used to indicate nominal category membership, for the purpose of aiding in the construction of semantic lexicons.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P98-2182.txt | Citing Article:  C02-1114.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The experiments in (Riloff and Shepherd, 1997) were performed on the 500,000 word MUC-4 corpus, and those of (Roark and Charniak, 1998) were performed using MUC-4 and the Wall Street Journal corpus (some 30 million words).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We ran our algorithm against both the MUC-4 corpus and the Wall Street Journal (WSJ) corpus for a variety of categories, beginning with the categories of vehicle and weapon, both included in the five categories that Rk,S investigated in their paper.</S><S sid = NA ssid = NA>Even more valid terms were generated for appropriate categories using the Wall Street Journal.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P98-2182.txt | Citing Article:  C02-1114.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The high accuracy achieved thus questions the conclusion drawn by Roark and Charniak (1998) that parsing is invaluable.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>To this end, parsing is invaluable.</S><S sid = NA ssid = NA>In addition, because it promotes high frequency terms, such a statistic tends to have the same effect as a minimum occurrence cutoff, i.e. few if any low frequency words get added.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P98-2182.txt | Citing Article:  P03-2011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Roark and Charniak (1998) used the co-occurrence of words as features to classify nouns.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>R&S used the same figure of merit both for selecting new seed words and for ranking words in the final output.</S><S sid = NA ssid = NA>Co-Occurrence bigrams are collected for head nouns according to the notion of co-occurrence outlined above.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P98-2182.txt | Citing Article:  W03-0415.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The goal of extracting semantic information from text is well-established, and has encouraged work on lexical acquisition (Roark and Charniak, 1998), information extraction (Cardie, 1997), and ontology engineering (Hahn and Schnattinger, 1998).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>It is for this reason that we are billing our algorithm as something that could enhance existing broadcoverage resources with domain-specific lexical information.</S><S sid = NA ssid = NA>Extracting semantic information from word co-occurrence statistics has been effective, particularly for sense disambiguation (Schiitze, 1992; Gale et al., 1992; Yarowsky, 1995).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P98-2182.txt | Citing Article:  P07-3013.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This may be explained by the fact that words appearing in conjunctions are often taxonomically similar (Roark and Charniak, 1998) and that taxonomic information is particularly useful for compound interpretation, as evidenced by the success of WordNet-based methods (see Section 5).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A simple probability does not capture this fact.</S><S sid = NA ssid = NA>All compound nouns in the former constructions are represented by the head of the compound.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P98-2182.txt | Citing Article:  P10-1029.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>To select seed words, we used the procedure proposed by Roark and Charniak (1998), ranking all of the head nouns in the training corpus by frequency and manually selecting the first 10 nouns that unambiguously belong to each category.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The simple ratio used to select new seed words will tend not to select higher frequency words in the category.</S><S sid = NA ssid = NA>R&S used the same figure of merit both for selecting new seed words and for ranking words in the final output.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P98-2182.txt | Citing Article:  C10-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Thanks to Mark Johnson for insightful discussion and to Julie Sedivy for helpful comments.</S><S sid = NA ssid = NA>If the sentence had read: &quot;A cargo aircraft, fighter plane, or combat helicopter ...&quot;, then aircraft, plane, and helicopter would all have counted as co-occuring with each other in our algorithm.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P98-2182.txt | Citing Article:  W02-1017.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For example, Hearst (Hearst, 1992) learned hyponymy relationships by collecting words in lexico-syntactic expressions, such as NP, NP, and other NPs, and Roarkand Charniak (Roark and Charniak, 1998) generated semantically related words by applying statistical measures to syntactic contexts involving appositives, lists, and conjunctions.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>To identify conjunctions, lists, and appositives, we first parsed the corpus, using an efficient statistical parser (Charniak et al., 1998), trained on the Penn Wall Street Journal Treebank (Marcus et al., 1993).</S><S sid = NA ssid = NA>Related words (e.g. crash for the category vehicle) did not count.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P98-2182.txt | Citing Article:  W02-1017.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In previous research on semantic lexicon induction, Roark and Charniak (Roark and Charniak, 1998) showed that 3 of every 5 words learned by their system were not present in WordNet.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Noun-Phrase Co-occurrence Statistics for Semi-Automatic Semantic Lexicon Construction</S><S sid = NA ssid = NA>We have outlined an algorithm in this paper that, as it stands, could significantly speed up the task of building a semantic lexicon.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P98-2182.txt | Citing Article:  W02-1017.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Roark and Charniak (Roark and Charniak, 1998) followed up on this work by using a parser to explicitly capture these structures.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>To identify conjunctions, lists, and appositives, we first parsed the corpus, using an efficient statistical parser (Charniak et al., 1998), trained on the Penn Wall Street Journal Treebank (Marcus et al., 1993).</S><S sid = NA ssid = NA>A simple probability does not capture this fact.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P98-2182.txt | Citing Article:  P08-3001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Roark and Charniak (1998) applied this idea to extraction of words which belong to the same categories, utilizing syntactic relations such as conjunctions and appositives.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>To identify conjunctions, lists, and appositives, we first parsed the corpus, using an efficient statistical parser (Charniak et al., 1998), trained on the Penn Wall Street Journal Treebank (Marcus et al., 1993).</S><S sid = NA ssid = NA>Table 1 shows the seed words that were used for some of the categories tested.</S> | Discourse Facet:  NA | Annotator: Automatic


