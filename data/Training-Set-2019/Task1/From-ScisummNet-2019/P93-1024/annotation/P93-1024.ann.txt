Citance Number: 1 | Reference Article:  P93-1024.txt | Citing Article:  E95-1008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Class based methods (Pereira et al 1993) approximate the likelihood of unobserved words based on similar words.</S> | Reference Offset:  ['7','135'] | Reference Text:  <S sid = 7 ssid = >Methods for automatically classifying words according to their contexts of use have both scientific and practical interest.</S><S sid = 135 ssid = >verb distribution for n from the cluster centroids close to n. The data for this test was built from the training data for the previous one in the following way, based on a suggestion by Dagan et al. (1993).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P93-1024.txt | Citing Article:  P06-1102.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['54','148'] | Reference Text:  <S sid = 54 ssid = >While clusters based on distributional similarity are interesting on their own, they can also be profitably seen as a means of summarizing a joint distribution.</S><S sid = 148 ssid = >We would like to thank Don Hindle for making available the 1988 Associated Press verb-object data set, the Fidditch parser and a verb-object structure filter, Mats Rooth for selecting the objects of &quot;fire&quot; data set and many discussions, David Yarowsky for help with his stemming and concordancing tools, and Ido Dagan for suggesting ways of testing cluster models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P93-1024.txt | Citing Article:  W06-1664.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Distributional clustering of words was first proposed by Pereira Tishby and Leein (Pereira et al, 1993): They cluster nouns according to their conditional verb distributions.</S> | Reference Offset:  ['0','32'] | Reference Text:  <S sid = 0 ssid = >Distributional Clustering Of English Words</S><S sid = 32 ssid = >To cluster nouns n according to their conditional verb distributions prz, we need a measure of similarity between distributions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P93-1024.txt | Citing Article:  D07-1039.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The cluster output can then be used as classes for selectional preferences (Pereira et al, 1993), or one can directly use frequency information from distributionally similar words for smoothing (Grishman and Sterling, 1994).</S> | Reference Offset:  ['16','18'] | Reference Text:  <S sid = 16 ssid = >His notion of similarity seems to agree with our intuitions in many cases, but it is not clear how it can be used directly to construct word classes and corresponding models of association.</S><S sid = 18 ssid = >While it may be worth basing such a model on preexisting sense classes (Resnik, 1992), in the work described here we look at how to derive the classes directly from distributional data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P93-1024.txt | Citing Article:  W04-3221.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This indicates that techniques for differentiating between different senses are needed e.g., using a soft clustering technique as in (Pereira et al 1993) instead of a hard clustering technique.</S> | Reference Offset:  ['19','44'] | Reference Text:  <S sid = 19 ssid = >More specifically, we model senses as probabilistic concepts or clusters c with corresponding cluster membership probabilities p(clw) for each word w. Most other class-based modeling techniques for natural language rely instead on &quot;hard&quot; Boolean classes (Brown et al., 1990).</S><S sid = 44 ssid = >It turns out that the problem is avoided by our clustering technique, since it does not need to compute the KL distance between individual word distributions, but only between a word distribution and average distributions, the current cluster centroids, which are guaranteed to be nonzero whenever the word distributions are.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P93-1024.txt | Citing Article:  C00-2104.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Hatzivassiloglou and MeKeown (1993) clustered adjectives into semantic classes, and Pereira et al (1993) clustered nouns on their appearance in verb-object pairs.</S> | Reference Offset:  ['121','131'] | Reference Text:  <S sid = 121 ssid = >This collection process yielded 1112041 verb-object pairs.</S><S sid = 131 ssid = >As the figure shows, the cluster model provides over one bit of information about the selectional properties of the new nouns, but the overtraining effect is even sharper than for the held-out data involving the 1000 clustered nouns.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P93-1024.txt | Citing Article:  W02-0705.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Of course, this is one of many different possible similarity measures which could have been used (Pereira et al (1993)), including ones that do not depend on additional labels.</S> | Reference Offset:  ['14','135'] | Reference Text:  <S sid = 14 ssid = >This requires a reasonable definition of verb similarity and a similarity estimation method.</S><S sid = 135 ssid = >verb distribution for n from the cluster centroids close to n. The data for this test was built from the training data for the previous one in the following way, based on a suggestion by Dagan et al. (1993).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P93-1024.txt | Citing Article:  W01-1602.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >So far we have used a weighted string edit distance matcher and experimented with di erent substitution weights including ones based on measures of statistical similarity between words such as the one described by Pereira et al (1993).</S> | Reference Offset:  ['120','135'] | Reference Text:  <S sid = 120 ssid = >The evaluation described below was performed on the largest data set we have worked with so far, extracted from 44 million words of 1988 Associated Press newswire with the pattern matching techniques mentioned earlier.</S><S sid = 135 ssid = >verb distribution for n from the cluster centroids close to n. The data for this test was built from the training data for the previous one in the following way, based on a suggestion by Dagan et al. (1993).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P93-1024.txt | Citing Article:  N09-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Distributional clustering (Dcluster) (Pereira et al., 1993) measures similarity among words in terms of the similarity among their local contexts.</S> | Reference Offset:  ['0','2'] | Reference Text:  <S sid = 0 ssid = >Distributional Clustering Of English Words</S><S sid = 2 ssid = >Words are represented by the relative frequency distributions of contexts in which they appear, and relative entropy between those distributions is used as the similarity measure for clustering.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P93-1024.txt | Citing Article:  D07-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In particular, a cluster learning algorithm that permits clusters to split and/or merge, as in Petrov et al (2006) or in Pereira et al (1993), may be appropriate.</S> | Reference Offset:  ['100','135'] | Reference Text:  <S sid = 100 ssid = >We say then that the original cluster has split into the two new clusters.</S><S sid = 135 ssid = >verb distribution for n from the cluster centroids close to n. The data for this test was built from the training data for the previous one in the following way, based on a suggestion by Dagan et al. (1993).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P93-1024.txt | Citing Article:  H05-1123.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >While previous cognitively-motivated computational frameworks required structured input (e.g. Falkenhainer et al, 1989), the CP method adapts distributional clustering (Pereira et al, 1993), a standard approach applicable to unstructured data.</S> | Reference Offset:  ['63','135'] | Reference Text:  <S sid = 63 ssid = >The combined entropy maximization entropy and distortion minimization is carried out by a two-stage iterative process similar to the EM method (Dempster et al., 1977).</S><S sid = 135 ssid = >verb distribution for n from the cluster centroids close to n. The data for this test was built from the training data for the previous one in the following way, based on a suggestion by Dagan et al. (1993).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P93-1024.txt | Citing Article:  D07-1018.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >There are a few exceptions to this tradition, such as Pereira et al (1993), Rooth et al (1999), Korhonen et al (2003), who used soft clustering methods for multiple assignment to verb semantic classes.</S> | Reference Offset:  ['63','135'] | Reference Text:  <S sid = 63 ssid = >The combined entropy maximization entropy and distortion minimization is carried out by a two-stage iterative process similar to the EM method (Dempster et al., 1977).</S><S sid = 135 ssid = >verb distribution for n from the cluster centroids close to n. The data for this test was built from the training data for the previous one in the following way, based on a suggestion by Dagan et al. (1993).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P93-1024.txt | Citing Article:  W08-2006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Lexical relatedness between terms could be derived either from a thesaurus like WordNet or from raw monolingual corpora via distributional similarity (Pereira et al, 1993).</S> | Reference Offset:  ['8','96'] | Reference Text:  <S sid = 8 ssid = >The scientific questions arise in connection to distributional views of linguistic (particularly lexical) structure and also in relation to the question of lexical acquisition both from psychological and computational learning perspectives.</S><S sid = 96 ssid = >Distributional similarity plays here the role of distortion.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P93-1024.txt | Citing Article:  H05-1064.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our approach differs from standard word clustering in that the clustering criteria is directly linked to the re ranking objective, whereas previous word clustering approaches (e.g. Brown et al (1992) or Pereira et al (1993)) have typically leveraged distributional similarity.</S> | Reference Offset:  ['0','44'] | Reference Text:  <S sid = 0 ssid = >Distributional Clustering Of English Words</S><S sid = 44 ssid = >It turns out that the problem is avoided by our clustering technique, since it does not need to compute the KL distance between individual word distributions, but only between a word distribution and average distributions, the current cluster centroids, which are guaranteed to be nonzero whenever the word distributions are.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P93-1024.txt | Citing Article:  N10-1025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The importance of distributional features is well known for named entity recognition and part of speech tagging (Pereira et al, 1993).</S> | Reference Offset:  ['0','135'] | Reference Text:  <S sid = 0 ssid = >Distributional Clustering Of English Words</S><S sid = 135 ssid = >verb distribution for n from the cluster centroids close to n. The data for this test was built from the training data for the previous one in the following way, based on a suggestion by Dagan et al. (1993).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P93-1024.txt | Citing Article:  P10-1040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, Pereira et al (1993) begin with a co occurrence matrix and transform this matrix into a clustering.</S> | Reference Offset:  ['23','135'] | Reference Text:  <S sid = 23 ssid = >Our raw knowledge about the relation consists of the frequencies fâ€ž of occurrence of particular pairs (v, n) in the required configuration in a training corpus.</S><S sid = 135 ssid = >verb distribution for n from the cluster centroids close to n. The data for this test was built from the training data for the previous one in the following way, based on a suggestion by Dagan et al. (1993).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P93-1024.txt | Citing Article:  P08-1057.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In a similar vein, our model is both similar and distinct in comparison to the soft clustering approaches by Pereira et al (1993) and Korhonenetal.</S> | Reference Offset:  ['56','63'] | Reference Text:  <S sid = 56 ssid = >We will take (1) as our basic clustering model.</S><S sid = 63 ssid = >The combined entropy maximization entropy and distortion minimization is carried out by a two-stage iterative process similar to the EM method (Dempster et al., 1977).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P93-1024.txt | Citing Article:  P08-1057.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Pereira et al (1993) suggested deterministic annealing to cluster verb-argument pairs into classes of verbs and nouns.</S> | Reference Offset:  ['5','94'] | Reference Text:  <S sid = 5 ssid = >Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical &quot;soft&quot; clustering of the data.</S><S sid = 94 ssid = >The analogy with statistical mechanics suggests a deterministic annealing procedure for clustering (Rose et al., 1990), in which the number of clusters is determined through a sequence of phase transitions by continuously increasing the parameter 0 following an annealing schedule.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P93-1024.txt | Citing Article:  W04-1216.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['54','148'] | Reference Text:  <S sid = 54 ssid = >While clusters based on distributional similarity are interesting on their own, they can also be profitably seen as a means of summarizing a joint distribution.</S><S sid = 148 ssid = >We would like to thank Don Hindle for making available the 1988 Associated Press verb-object data set, the Fidditch parser and a verb-object structure filter, Mats Rooth for selecting the objects of &quot;fire&quot; data set and many discussions, David Yarowsky for help with his stemming and concordancing tools, and Ido Dagan for suggesting ways of testing cluster models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P93-1024.txt | Citing Article:  P99-1016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Pereira et al (1993) used clustering to build an unlabeled hierarchy of nouns.</S> | Reference Offset:  ['135','137'] | Reference Text:  <S sid = 135 ssid = >verb distribution for n from the cluster centroids close to n. The data for this test was built from the training data for the previous one in the following way, based on a suggestion by Dagan et al. (1993).</S><S sid = 137 ssid = >The resulting training set was used to build a sequence of cluster models as before.</S> | Discourse Facet:  NA | Annotator: Automatic


