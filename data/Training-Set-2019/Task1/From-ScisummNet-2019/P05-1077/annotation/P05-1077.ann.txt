Citance Number: 1 | Reference Article:  P05-1077.txt | Citing Article:  P14-2081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>NNS is essential in dealing with many search related tasks, and also fundamental to a broad range of Natural Language Processing (NLP) downstream problems including person name spelling correction (Udupa and Kumar, 2010), document translation pair acquisition (Krstovski and Smith, 2011), large-scale similar noun list generation (Ravichandran et al, 2005), lexical variants mining (Gouws et al, 2011), and large-scale first story detection (Petrovic et al, 2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In the last decade, the field of Natural Language Processing (NLP), has seen a surge in the use of corpus motivated techniques.</S><S sid = NA ssid = NA>However, most language analysis tools are too infeasible to run on the scale of the web.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P05-1077.txt | Citing Article:  W11-2504.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We wish to thank USC Center for High Performance Computing and Communications (HPCC) for helping us use their cluster computers.</S><S sid = NA ssid = NA>This algorithm was further improved by Charikar (2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P05-1077.txt | Citing Article:  P08-1077.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Ravichandran et al (2005) have shown that by using the LSH nearest neighbors calculation can be done in O (nd) time.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Using fast search algorithm to find nearest neighbors.</S><S sid = NA ssid = NA>B of its closest neighbors in the sorted list.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P05-1077.txt | Citing Article:  P12-1041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>III and Marcu (2005), who use word class features derived from a Web-scale corpus via a process described in Ravichandran et al (2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We do not use grammatical features in the web corpus since parsing is generally not easily web scalable.</S><S sid = NA ssid = NA>We use the context words as features of the noun vector.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P05-1077.txt | Citing Article:  P06-1101.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our classifier for (m, n)-cousins is derived from the algorithm and corpus given in (Ravichandran et al, 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The algorithm given in Charikar (2002) is described to find the nearest neighbor for a given vector.</S><S sid = NA ssid = NA>We eliminate duplicate and near duplicate documents by using the algorithm described by Kolcz et al. (2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P05-1077.txt | Citing Article:  P07-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We wish to thank USC Center for High Performance Computing and Communications (HPCC) for helping us use their cluster computers.</S><S sid = NA ssid = NA>This algorithm was further improved by Charikar (2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P05-1077.txt | Citing Article:  P06-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Ravichandran et al (2005) used this cosine variant and showed it to produce over 70% accuracy in extracting synonyms when compared against Pantel and Lin (2002).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We use the same 6GB corpus that was used for training by Pantel and Lin (2002) so that the results are comparable.</S><S sid = NA ssid = NA>We then compare this output to the one provided by the system of Pantel and Lin (2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P05-1077.txt | Citing Article:  P06-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>It was used by Ravichandran et al (2005) to improve the efficiency of distributional similarity calculations.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>But with the rapidly growing amount of raw text available on the web, one could improve clustering performance by carefully harnessing its power.</S><S sid = NA ssid = NA>Interestingly, cosine similarity is widely used in NLP for various applications such as clustering.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P05-1077.txt | Citing Article:  P06-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The frequency statistics were weighted using mutual information, as in Ravichandran et al (2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We first construct a frequency count vector C(e) = (ce1, ce2, ..., cek), where k is the total number of features and cef is the frequency count of feature f occurring in word e. Here, cef is the number of times word e occurred in context f. We then construct a mutual information vector MI(e) = (mie1, mie2, ..., miek) for each word e, where mief is the pointwise mutual information between word e and feature f, which is defined as: where n is the number of words and N = En Em j=1 cij is the total frequency count of all i=1 features of all words.</S><S sid = NA ssid = NA>Having collected all nouns and their features, we now proceed to construct feature vectors (and values) for nouns from both corpora using mutual information (Church and Hanks, 1989).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P05-1077.txt | Citing Article:  P06-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>When the cut-off was increased to 100, as used by Ravichandran et al (2005), the results improved significantly.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This algorithm was further improved by Charikar (2002).</S><S sid = NA ssid = NA>These similarity lists are cut off at a threshold of 0.15.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P05-1077.txt | Citing Article:  D11-1122.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We used randomized algorithms (Ravichandran et al, 2005) to build the semantic space efficiently.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>LSH functions are generally based on randomized algorithms and are probabilistic.</S><S sid = NA ssid = NA>In doing so, we are going to explore the literature and techniques of randomized algorithms.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P05-1077.txt | Citing Article:  N10-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This scheme was used, e.g., for creating similarity lists of nouns collected from a web corpus in Ravichandran et al (2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this paper, we perform high speed similarity list creation for nouns collected from a huge web corpus.</S><S sid = NA ssid = NA>The web corpus is used to show that our framework is webscalable, while the newspaper corpus is used to compare the output of our system with the similarity lists output by an existing system, which are calculated using the traditional formula as given in equation 1.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P05-1077.txt | Citing Article:  C10-1096.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This baseline system follows the design of previous work (Ravichandran et al, 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In the previous section, we introduced the theory for calculation of fast cosine similarity.</S><S sid = NA ssid = NA>Why does the fast hamming distance algorithm work?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P05-1077.txt | Citing Article:  C10-1096.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We followed the notation of the original paper (Ravichandran et al, 2005) here.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This is a huge saving from the original O(n2k) algorithm.</S><S sid = NA ssid = NA>In this paper, we investigate the first two avenues.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P05-1077.txt | Citing Article:  C10-1096.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Ravichandran et al (2005) applied LSH to the task of noun clustering.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>However, for noun clustering, we generally have the number of nouns, n, smaller than the number of features, k.</S><S sid = NA ssid = NA>For each noun we take the grammatical context of the noun as identified by Minipar5.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P05-1077.txt | Citing Article:  D12-1098.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>However, Ravichandran et al (2005) approach stored an enormous matrix of all unique words and their contexts in main memory, which is infeasible for very large data sets.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>With large amounts of data, say n in the order of millions or even billions, having an n2k algorithm would be very infeasible.</S><S sid = NA ssid = NA>The experiment was infeasible.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P05-1077.txt | Citing Article:  D12-1098.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In practice p is generally large, Ravichandran et al (2005) used p= 1000 in their work.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>However, to unleash the real power of clustering one has to work with large amounts of text.</S><S sid = NA ssid = NA>Why does the fast hamming distance algorithm work?</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P05-1077.txt | Citing Article:  D12-1098.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Data sets: We use two data sets: Gigaword (Graff, 2003) and a copy of news web (Ravichandran et al., 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>All clustering algorithms make use of some distance similarity (e.g., cosine similarity) to measure pair wise distance between sets of vectors.</S><S sid = NA ssid = NA>For evaluating the quality of our final similarity lists, we use the system developed by Pantel and Lin (2002) as gold standard on a much smaller data set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P05-1077.txt | Citing Article:  D12-1098.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We set the number of projections k= 3000 for all three methods and for PLEB and FAST-PLEB, we set number of permutations p= 1000 as used in large-scale noun clustering work (Ravichandran et al 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>However, for noun clustering, we generally have the number of nouns, n, smaller than the number of features, k.</S><S sid = NA ssid = NA>Number of bit index random permutations functions q; 2.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P05-1077.txt | Citing Article:  N09-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Language modeling (Chen and Goodman, 1996), noun-clustering (Ravichandran et al, 2005), constructing syntactic rules for SMT (Galley et al, 2004), and finding analogies (Turney, 2008) are examples of some of the problems where we need to compute relative frequencies.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We eliminate duplicate and near duplicate documents by using the algorithm described by Kolcz et al. (2004).</S><S sid = NA ssid = NA>Thus, the above theorem, converts the problem of finding cosine distance between two vectors to the problem of finding hamming distance between their bit streams (as given by equation 4).</S> | Discourse Facet:  NA | Annotator: Automatic


