We present experiments with a dependency parsing model defined on rich factors.
Our model represents dependency trees with factors that include three types of relations between the tokens of a dependency and their children.
We extend the projective parsing algorithm of Eisner (1996) for our case, and train models using the averaged perceptron.
Our experiments show that considering higher-order information yields significant improvements in parsing accuracy, but comes at a high cost in terms of both time and memory consumption.
In the multilingual exercise of the CoNLL-2007 shared task (Nivre et al., 2007), our system obtains the best accuracy for English, and the second best accuracies for Basque and Czech.
