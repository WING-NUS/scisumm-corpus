Citance Number: 1 | Reference Article:  D08-1089.txt | Citing Article:  W09-0436.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>All differences between DE-Annotated and BASELINE are significant at the level of 0.05 with the approximate randomization test in (Riezler and Maxwell, 2005) conduct additional experiments with a hierarchical phrase reordering model introduced by Galley and Manning (2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Statistical significance is computed using the approximate randomization test (Noreen, 1989), whose application to MT evaluation (Riezler and Maxwell, 2005) was shown to be less sensitive totype-I errors (i.e., incorrectly concluding that im provement is significant) than the perhaps more widely used bootstrap resampling method (Koehn, 2004b).</S><S sid = NA ssid = NA>The baseline is Moses with no lexicalized reordering model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  D08-1089.txt | Citing Article:  P14-2023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We use a phrase-based system similar to Moses (Koehn et al, 2007) based on a set of common features including maximum likelihood estimates p ML (e|f) and p ML (f |e), lexically weighted estimates p LW (e|f) and p LW (f |e), word and phrase-penalties, a hierarchical reordering model (Galley and Manning, 2008), a linear distortion feature, and a modified KneserNey language model trained on the target-side of the parallel data.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>exactly as in Moses: four translation features(phrase-based translation probabilities and lexically weighted probabilities), word penalty, phrase penalty, linear distortion, and language model score.We experiment with two language pairs: Chinese to-English (C-E) and Arabic-to-English (A-E).</S><S sid = NA ssid = NA>In our experiments, we use a re-implementationof the Moses decoder (Koehn et al, 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  D08-1089.txt | Citing Article:  W11-2150.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In (Galley and Manning, 2008) the authors present an extension of the famous MSD model (Tillman, 2004) able to handle long distance word-block permutations.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Current phrase reordering models can properly handle swaps between adjacent phrases, but they typically lack theability to perform the kind of long-distance re orderings possible with syntax-based systems.</S><S sid = NA ssid = NA>Computing reordering scores during decoding with word-based3 and phrase-based models (Tillman, 2004) is trivial, since they only make use of localinformation to determine the orientation of a new in coming block bi.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  D08-1089.txt | Citing Article:  N10-2003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>From the word-to-word alignments, the system extracts a phrase table (Koehn et al, 2003) and hierarchical reordering model (Galley and Manning, 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Recent efforts (Tillman, 2004; Och et al, 2004; Koehn et al, 2007) have directly addressed this issue by introducing lexicalized reordering models into phrase-based systems, which condition reordering probabilities on the words of each phrase pair.</S><S sid = NA ssid = NA>Statistical phrase-based systems (Och and Ney,2004; Koehn et al, 2003) have consistently delivered state-of-the-art performance in recent machine translation evaluations, yet these systems remain weak at handling word order changes.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  D08-1089.txt | Citing Article:  N10-2003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Instead of just looking at the reordering relationship between individual phrases, the new feature examines the reordering of blocks of adjacent phrases (Galley and Manning, 2008) and improves translation quality when the material being reordered cannot be captured by single phrase.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this example, our reordering modeleffectively treats the adjacent phrases the develop ment and and progress as one single phrase, and the displacement of of the region with respect to thisphrase can be treated as a swap.</S><S sid = NA ssid = NA>2(b)), and is set to D otherwise.Hierarchical orientation model: This model analyzes alignments beyond adjacent phrases.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  D08-1089.txt | Citing Article:  C10-2033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Galley and Manning (2008) present a hierarchical phrase reordering model aimed at improving non-local reorderings.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this paper, we present a novel hierarchical phrase reordering model aimed at improvingnon-local reorderings, which seamlessly in tegrates with a standard phrase-based system with little loss of computational efficiency.</S><S sid = NA ssid = NA>A Simple and Effective Hierarchical Phrase Reordering Model</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  D08-1089.txt | Citing Article:  P13-1124.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our MOS concept is also closely related to hierarchical reordering model (Galley and Manning, 2008) in phrase-based decoding, which computes o of b with respect to a multi-block unit that may go beyond b?.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Despite the fact that Moses estimates a word-based orientation model during training (i.e., it analyzes the orientation of a given phrase with respect to adjacent wordalignments), this model is then treated as a phrase-based orien tation model during testing (i.e., as a model that orients phrases with respect to other phrases).</S><S sid = NA ssid = NA>Another benefit of the hierarchical model is thatits representation of phrases remains the same dur ing both training and decoding, which is not the casefor word-based and phrase-based reordering mod els.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  D08-1089.txt | Citing Article:  P10-2003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Given no constraint on maximum phrase length, the hierarchical phrase reordering model (Galley and Manning, 2008) also analyzes the adjacent bilingual phrases for bp and identifies its orientation as S.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>2(b)), and is set to D otherwise.Hierarchical orientation model: This model analyzes alignments beyond adjacent phrases.</S><S sid = NA ssid = NA>Specifically, orientation is set to oi = M if the phrase extract algorithm is able to extract a phrase pair at (s?1,u?1) given no constraint on maximum phrase length.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  D08-1089.txt | Citing Article:  P10-2003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We plan to apply our method to the complex lexicalized reordering models, for example, the hierarchical reordering model (Galley and Manning, 2008) and the MEBTG reordering model (Xiong et al, 2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We also plan to experiment with discriminative approaches to estimating reordering probabil ities (Zens and Ney, 2006; Xiong et al, 2006), whichcould also be applied to our work.</S><S sid = NA ssid = NA>The baseline is Moses with no lexicalized reordering model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  D08-1089.txt | Citing Article:  W11-1007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In (Galley and Manning, 2008) a hierarchical orientation model is introduced that captures some non-local phrase reordering by a shift reduce algorithm.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>include shift (S), reduce (R), and accept (A).</S><S sid = NA ssid = NA>The employed method is an instance of thewell-known shift-reduce parsing algorithm, and re lies on a stack (S) of foreign substrings that have already been translated.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  D08-1089.txt | Citing Article:  D10-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This method reduces the complexity to O (nbdmax) but fails to capture long distance reorderings (Galley and Manning, 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>While previouswork reasonably models phrase reordering in simple ex amples (a), it fails to capture more complex reorderings, such as the swapping of ?of the region?</S><S sid = NA ssid = NA>While these lexicalized re-ordering models have shown substantial improvements over unlexicalized phrase-based systems, these models only have a 848limited ability to capture sensible long distance re orderings, as can be seen in Fig.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  D08-1089.txt | Citing Article:  D10-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For example Galley and Manning (2008) propose a shift-reduce style method to allow hieararchical non-local reorderings in a phrase-based decoder.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In Section 4, we describe how to combine shift-reduce parsing with left-to-right beamsearch phrase-based decoding with the same asymptotic running time as the original phrase-based decoder.</S><S sid = NA ssid = NA>include shift (S), reduce (R), and accept (A).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  D08-1089.txt | Citing Article:  N12-1007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In addition to the baseline Phrasal feature set, we used the lexicalized re-ordering model of Galley and Manning (2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The baseline is Moses with no lexicalized reordering model.</S><S sid = NA ssid = NA>3 as single feature function, we follow the approach of Moses, which is to assign three distinct parameters (?m,?s,?d) for the three feature functions: ? fm = ?ni=1 log p(oi = M| . . .)</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  D08-1089.txt | Citing Article:  W12-3125.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Permutation parsers have been used to implement hierarchical re-ordering models (Galley and Manning, 2008) and to enforce inversion transduction grammar (ITG) constraints (Fengetal., 2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>to the other side, a constraint thatis difficult to enforce with the other two reorder ing models.</S><S sid = NA ssid = NA>For both orientation sets, we observe in A-E that the hierarchical model significantly outperforms thelocal ordering models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  D08-1089.txt | Citing Article:  W12-3125.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Thus far, they have been used to enable a hierarchical re-ordering model, or HRM (Galley and Manning, 2008), as well as an ITG constraint (Feng et al, 2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The re ordering models used in the original phrase-basedsystems penalize phrase displacements proportionally to the amount of nonmonotonicity, with no con sideration of the fact that some words are far more M M D S D !" #$ %& '( )* +, -.</S><S sid = NA ssid = NA>Crucially, our work distinguishes itself from previous hierarchical models in that it does not rely on any cubic-timeparsing algorithms such as CKY (used in, e.g., (Chiang, 2005)) or the Earley algorithm (used in (Watan abe et al, 2006)).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  D08-1089.txt | Citing Article:  W12-3125.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The HRM (Galley and Manning, 2008) maintains similar re-ordering statistics, but determines orientation differently.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For both orientation sets, we observe in A-E that the hierarchical model significantly outperforms thelocal ordering models.</S><S sid = NA ssid = NA>In all other cases, it is set to oi = D. This procedure is exactly the same as the one implemented in Moses.2 Phrase-based orientation model: The modelpresented in (Tillman, 2004) is similar to the word based orientation model presented above, except that it analyzes adjacent phrases rather than specificword alignments to determine orientations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  D08-1089.txt | Citing Article:  W12-3125.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Galley and Manning (2008) introduce a deterministic shift-reduce parser into decoding, so that the decoder always has access to the largest possible previous block, given the current translation history.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The incorporation of the shift reduce parser into such a decoder does not worsenoverall time complexity: whenever the decoder expands a given partial translation into a longer hy pothesis, it simply copies its stack into the newlycreated hypothesis (similarly to copying the cover age vector, this is an O(n) operation).</S><S sid = NA ssid = NA>In practice, we observe based on a set of experiments for Chinese-English and Arabic-English translation that our phrase-based decoder is on average only 1.35 times slower when it is running using hierarchical reordering features and the shift-reduce parser.We finally note that the decoding algorithm presented in this section can only be applied left-to right if the decoder itself is operating left-to-right.In order to predict orientations relative to the rightto-left hierarchical reordering model, we must resort to approximations at decoding time.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  D08-1089.txt | Citing Article:  W12-3125.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Galley and Manning (2008) propose an algorithm that begins by running standard phrase extraction (Och and Ney, 2004) without a phrase-length limit, noting the corners of each phrase found.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>heuristic of (Och and Ney, 2004).</S><S sid = NA ssid = NA>Specifically, orientation is set to oi = M if the phrase extract algorithm is able to extract a phrase pair at (s?1,u?1) given no constraint on maximum phrase length.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  D08-1089.txt | Citing Article:  P13-1156.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In all experiments we use phrase-orientation lexicalized reordering (Galley and Manning, 2008) which models monotone, swap, discontinuous orientations from both reordering with previous phrase pair and with the next phrase pair.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The phrase of the region should swap with the rest of the noun phrase, yet these previous approaches are unable to model this movement, and assume the orientation of this phrase is discontinuous (D).</S><S sid = NA ssid = NA>For instance, in Fig ure 4, the phrase-based reordering model categorizes the block in the near future as discontinuous, though if the sentence pair had been a training example,this block would count as a swap because of the ex tracted phrase on this issue.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  D08-1089.txt | Citing Article:  P13-1156.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Galley and Manning (2008) introduce three orientation models for lexicalized reordering: word-based, phrase-based and hierarchical orientation model.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Despite the fact that Moses estimates a word-based orientation model during training (i.e., it analyzes the orientation of a given phrase with respect to adjacent wordalignments), this model is then treated as a phrase-based orien tation model during testing (i.e., as a model that orients phrases with respect to other phrases).</S><S sid = NA ssid = NA>In all other cases, it is set to oi = D. This procedure is exactly the same as the one implemented in Moses.2 Phrase-based orientation model: The modelpresented in (Tillman, 2004) is similar to the word based orientation model presented above, except that it analyzes adjacent phrases rather than specificword alignments to determine orientations.</S> | Discourse Facet:  NA | Annotator: Automatic


