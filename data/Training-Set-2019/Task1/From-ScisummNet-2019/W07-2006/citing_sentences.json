[
  {
    "citance_No": 1, 
    "citing_paper_id": "D08-1105", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Zhi, Zhong | Hwee Tou, Ng | Yee Seng, Chan", 
    "raw_text": "To address this issue, a coarse-grained English all-words task (Navigli et al, 2007) was conducted during SemEval-2007", 
    "clean_text": "To address this issue, a coarse-grained English all-words task (Navigli et al, 2007) was conducted during SemEval-2007.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "I08-1073", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Ryu, Iida | Diana, McCarthy | Rob, Koeling", 
    "raw_text": "For example, in the English coarse grained all words task (Navigli et al,2007) at the recent SemEval Workshop the base line of choosing the most frequent sense using the first WordNet sense attained precision and recall of 78.9% which is only a few percent lower than the top scoring system which obtained 82.5%", 
    "clean_text": "For example, in the English coarse grained all words task (Navigli et al,2007) at the recent SemEval Workshop the base line of choosing the most frequent sense using the first WordNet sense attained precision and recall of 78.9% which is only a few percent lower than the top scoring system which obtained 82.5%.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "W11-1413", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Jack, Mostow | Weisi, Duan", 
    "raw_text": "The problem of how to cluster fine-grained senses into coarse senses is hard, especially if consensus is required (Navigli et al 2007)", 
    "clean_text": "The problem of how to cluster fine-grained senses into coarse senses is hard, especially if consensus is required (Navigli et al 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "P09-1002", 
    "citing_paper_authority": 14, 
    "citing_paper_authors": "Katrin, Erk | Diana, McCarthy | Nicholas, Gaylord", 
    "raw_text": "Similarly, the performance of WSD systems clearly indicates that WSD is not easy unless one adopts a coarse-grained approach, and then systems tagging all words at best perform a few percentage points above the most frequent sense heuristic (Navigli et al, 2007)", 
    "clean_text": "Similarly, the performance of WSD systems clearly indicates that WSD is not easy unless one adopts a coarse-grained approach, and then systems tagging all words at best perform a few percentage points above the most frequent sense heuristic (Navigli et al, 2007).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "W09-2405", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Hansen A., Schwartz | Fernando, Gomez", 
    "raw_text": "Finally, results are presented from the SemEval-2007 coarse grained all-words task (Navigli et al, 2007), and we explore the in flu ence of various types of selectors on the algorithm in order to draw insight for future improvement of Web-based methods", 
    "clean_text": "Finally, results are presented from the SemEval-2007 coarse grained all-words task (Navigli et al, 2007), and we explore the influence of various types of selectors on the algorithm in order to draw insight for future improvement of Web-based methods.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "W09-2405", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Hansen A., Schwartz | Fernando, Gomez", 
    "raw_text": "The sense inventory was created by mapping senses in WordNet 2.1 to the Oxford Dictionary of English (Naviglietal., 2007)", 
    "clean_text": "The sense inventory was created by mapping senses in WordNet 2.1 to the Oxford Dictionary of English (Navigli et al., 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "W09-2405", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Hansen A., Schwartz | Fernando, Gomez", 
    "raw_text": "P? RP+R. For SemEval2007, all systems performed better than the random base line of 53.43%, but only 4 of 13 systems achieved an F1 score higher than the MFS baseline of 78.89% (Navigli et al, 2007) .Table 2 lists the results of applying the generalized Web selector algorithm described in this paper in a straight-forward manner, such that all scale (T) are set to 1", 
    "clean_text": "For SemEval 2007, all systems performed better than the random base line of 53.43%, but only 4 of 13 systems achieved an F1 score higher than the MFS baseline of 78.89% (Navigli et al, 2007). Table 2 lists the results of applying the generalized Web selector algorithm described in this paper in a straight-forward manner, such that all scale (T) are set to 1.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "W10-2304", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Diego, de Cao | Roberto, Basili | Matteo, Luciani | Francesco, Mesiano | Riccardo, Rossi", 
    "raw_text": "This will allow to asses its applicability to realistic tasks, such as query processing or document indexing. Experimental Set-up In order to measure ac curacy, the Senseval 2007 coarse WSDdataset2 (Navigli et al, 2007) has been employed", 
    "clean_text": "This will allow to asses its applicability to realistic tasks, such as query processing or document indexing. Experimental Set-up In order to measure accuracy, the Senseval 2007 coarse WSD dataset (Navigli et al, 2007) has been employed.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "P10-1154", 
    "citing_paper_authority": 17, 
    "citing_paper_authors": "Simone Paolo, Ponzetto | Roberto, Navigli", 
    "raw_text": "We report our results in terms of precision, recall and F1-measure on the Semeval-2007 coarse-grained all-words dataset (Navigli et al, 2007)", 
    "clean_text": "We report our results in terms of precision, recall and F1-measure on the Semeval-2007 coarse-grained all-words dataset (Navigli et al, 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "W08-2114", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Hansen A., Schwartz | Fernando, Gomez", 
    "raw_text": "(Navigli et al, 2007) Out of 2269 noun, verb, adjective, or adverb in stances we are concerned with disambiguating the 1108 noun instances from the 245 sentences in the corpus. These noun instances represent 593 different words", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "W08-2114", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Hansen A., Schwartz | Fernando, Gomez", 
    "raw_text": "For the SemEval workshop, only 6 of 15 systems performed better than this baseline on the nouns (Navigli et al, 2007), all of which used MFS as a back off strategy and an external sense tagged data set", 
    "clean_text": "For the SemEval workshop, only 6 of 15 systems performed better than this baseline on the nouns (Navigli et al, 2007), all of which used MFS as a back off strategy and an external sense tagged data set.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "W08-2114", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Hansen A., Schwartz | Fernando, Gomez", 
    "raw_text": "All systems performing better than the MFS used the heuristic as a back off strategy when unable to output asense (Navigli et al, 2007)", 
    "clean_text": "All systems performing better than the MFS used the heuristic as a back off strategy when unable to output a sense (Navigli et al, 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "N09-1004", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Ping, Chen | Wei, Ding | Chris, Bowes | David, Brown", 
    "raw_text": "Currently supervised methods achieve the best disambiguation quality (about 80% precision and recall for coarse-grained WSD in the most recent WSD evaluation conference SemEval 2007 (Navigli et al, 2007))", 
    "clean_text": "Currently supervised methods achieve the best disambiguation quality (about 80% precision and recall for coarse-grained WSD in the most recent WSD evaluation conference SemEval 2007 (Navigli et al, 2007)).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "N09-1004", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Ping, Chen | Wei, Ding | Chris, Bowes | David, Brown", 
    "raw_text": "In the most recent SemEval 2007 (Navigli et al, 2007), the best unsupervised systems only achieved about 70% precision and 50% recall", 
    "clean_text": "In the most recent SemEval 2007 (Navigli et al, 2007), the best unsupervised systems only achieved about 70% precision and 50% recall.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "N09-1004", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Ping, Chen | Wei, Ding | Chris, Bowes | David, Brown", 
    "raw_text": "We have evaluated our method using SemEval-2007 Task 07 (Coarse-grained English All-words Task) test set (Navigli et al, 2007)", 
    "clean_text": "We have evaluated our method using SemEval-2007 Task 07 (Coarse-grained English All-words Task) test set (Navigli et al, 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "N09-1004", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Ping, Chen | Wei, Ding | Chris, Bowes | David, Brown", 
    "raw_text": "Two authors of (Navigli et al, 2007) independently and manually annotated part of the test set (710 word instances), and the pairwise agreement was 93.80%", 
    "clean_text": "Two authors of (Navigli et al, 2007) independently and manually annotated part of the test set (710 word instances), and the pairwise agreement was 93.80%.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "P12-3012", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Roberto, Navigli | Simone Paolo, Ponzetto", 
    "raw_text": "We benchmark our API by performing knowledge based WSD with BabelNet on standard SemEval datasets, namely the SemEval-2007 coarse-grained all-words (Navigli et al, 2007, Coarse-WSD, hence forth) and the SemEval-2010 cross-lingual (Lefever and Hoste, 2010, CL-WSD) WSD tasks", 
    "clean_text": "We benchmark our API by performing knowledge based WSD with BabelNet on standard SemEval datasets, namely the SemEval-2007 coarse-grained all-words (Navigli et al, 2007, Coarse-WSD, hence forth) and the SemEval-2010 cross-lingual (Lefever and Hoste, 2010, CL-WSD) WSD tasks.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "P12-3012", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Roberto, Navigli | Simone Paolo, Ponzetto", 
    "raw_text": "Since the selected Babelsynset can contain multiple translations in a target language for the given English word, we use for this task an 70 Algorithm Nouns only All words NUS-PT 82.3 82.5 SUSSX-FR 81.1 77.0 Degree 84.7 82.3 MFS BL 77.4 78.9 Random BL 63.5 62.7 Table 1: Performance on SemEval-2007 coarse-grained all-words WSD (Navigli et al, 2007)", 
    "clean_text": "Table 1: Performance on SemEval-2007 coarse-grained all-words WSD (Navigli et al, 2007).", 
    "keep_for_gold": 0
  }
]