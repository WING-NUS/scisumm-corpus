[
  {
    "citance_No": 1, 
    "citing_paper_id": "W01-1408", 
    "citing_paper_authority": 18, 
    "citing_paper_authors": "Franz Josef, Och | Nicola, Ueffing | Hermann, Ney", 
    "raw_text": "This would be more difficult in the HMM alignment model (Vogel et al., 1996)", 
    "clean_text": "This would be more difficult in the HMM alignment model (Vogel et al., 1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P14-1138", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Akihiro, Tamura | Taro, Watanabe | Eiichiro, Sumita", 
    "raw_text": "The most classical approaches are the probabilistic IBM models 15 (Brown et al, 1993) and the HMM model (Vogel et al, 1996)", 
    "clean_text": "The most classical approaches are the probabilistic IBM models (Brown et al, 1993) and the HMM model (Vogel et al, 1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P14-1138", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Akihiro, Tamura | Taro, Watanabe | Eiichiro, Sumita", 
    "raw_text": "These models are roughly clustered into two groups: generative models, such as those pro posed by Brown et al (1993), Vogel et al (1996), and Och and Ney (2003), and discriminative mod els, such as those proposed by Taskar et al (2005), Moore (2005), and Blunsom and Cohn (2006)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "N12-1095", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Mohit, Bansal | John, DeNero | Dekang, Lin", 
    "raw_text": "Bilingual features were computed from 0.78 (S? E )and1.04 (J? E) billion tokens of parallel text, primarily extracted from the Web using automated parallel document identification (Uszkoreit et al, 2010) .Word alignments were induced from the HMMbased alignment model (Vogel et al, 1996), initialized with the bi lexical parameters of IBM Model 1 (Brown et al, 1993)", 
    "clean_text": "Word alignments were induced from the HMM based alignment model (Vogel et al, 1996), initialized with the bi lexical parameters of IBM Model 1 (Brown et al, 1993).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "D07-1006", 
    "citing_paper_authority": 18, 
    "citing_paper_authors": "Alexander, Fraser | Daniel, Marcu", 
    "raw_text": "Muchof the additional work on generative modeling of 1 to-N word alignments is based on the HMM model (Vogel et al, 1996)", 
    "clean_text": "Much of the additional work on generative modeling of 1 to N word alignments is based on the HMM model (Vogel et al, 1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "W12-3124", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Antti-Veikko, Rosti | Xiaodong, He | Damianos, Karakos | Gregor, Leusch | Yuan, Cao | Markus, Freitag | Spyros, Matsoukas | Hermann, Ney | Jason, Smith | Bing, Zhang", 
    "raw_text": "Extensions of the last two are included in this study together with alignments based on hidden Markov model (HMM) (Vogel et al, 1996) and inversion transduction grammars (ITG) (Wu, 1997) .System combinations produced via confusion net work decoding using different hypothesis alignment algorithms have been entered into open evaluations, most recently in 2011 Workshop on Statistical Machine Translation (Callison-Burch et al, 2011)", 
    "clean_text": "Extensions of the last two are included in this study together with alignments based on hidden Markov model (HMM) (Vogel et al, 1996) and inversion transduction grammars (ITG) (Wu, 1997).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "W10-0704", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Qin, Gao | Stephan, Vogel", 
    "raw_text": "GIZA++ (Och and Ney, 2003) is the most widely used implementation of IBM models and HMM (Vogel et al, 1996) where EM algorithm is employed to estimate the model parameters", 
    "clean_text": "GIZA++ (Och and Ney, 2003) is the most widely used implementation of IBM models and HMM (Vogel et al, 1996) where EM algorithm is employed to estimate the model parameters.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "P06-1067", 
    "citing_paper_authority": 44, 
    "citing_paper_authors": "Yaser, Al-Onaizan | Kishore, Papineni", 
    "raw_text": "Any aligner such as (Al-Onaizan et al, 1999) or (Vogel et al, 1996) can be used to obtain word alignments", 
    "clean_text": "Any aligner such as (Al-Onaizan et al, 1999) or (Vogel et al, 1996) can be used to obtain word alignments.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "P11-2074", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Bing, Xiang | Abraham, Ittycheriah", 
    "raw_text": "For each sentence in the training, three types of word alignments are created: maximum entropy alignment (Ittycheriah and Roukos, 2005), GIZA++ alignment (Och and Ney, 2000), and HMM alignment (Vogel et al, 1996)", 
    "clean_text": "For each sentence in the training, three types of word alignments are created: maximum entropy alignment (Ittycheriah and Roukos, 2005), GIZA++ alignment (Och and Ney, 2000), and HMM alignment (Vogel et al, 1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "P06-2014", 
    "citing_paper_authority": 23, 
    "citing_paper_authors": "Colin, Cherry | Dekang, Lin", 
    "raw_text": "They model operations that are meaningful at a syntax level, like re-ordering children, but ignore features that have proven useful in IBM models, such as the preference to align words with similar positions, and the HMM preference for links to appear near one another (Vogel et al, 1996)", 
    "clean_text": "They model operations that are meaningful at a syntax level, like re-ordering children, but ignore features that have proven useful in IBM models, such as the preference to align words with similar positions, and the HMM preference for links to appear near one another (Vogel et al, 1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "W10-1747", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Gregor, Leusch | Hermann, Ney", 
    "raw_text": "We use the IBM Model 1 (Brown et al, 1993) and the Hid den Markov Model (HMM, (Vogel et al, 1996)) 315alignmentGIZA++Network generation Weighting& amp; RescoringReordering 200-bestlist Hyp 1 Hyp k..", 
    "clean_text": "We use the IBM Model 1 (Brown et al, 1993) and the Hidden Markov Model (HMM, (Vogel et al, 1996)) to estimate the alignment model.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "D10-1052", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Hendra, Setiawan | Chris, Dyer | Philip, Resnik", 
    "raw_text": "Over the years, there have been many proposals to improve these reordering models, most notably Vogel et al (1996), which adds a first-order dependency", 
    "clean_text": "Over the years, there have been many proposals to improve these reordering models, most notably Vogel et al (1996), which adds a first-order dependency.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "W05-0814", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Alexander, Fraser | Daniel, Marcu", 
    "raw_text": "The starting point is the final alignment generated using GIZA++? s implementation of IBM Model 1 and the Aachen HMM model (Vogel et al, 1996)", 
    "clean_text": "The starting point is the final alignment generated using GIZA++'s implementation of IBM Model 1 and the Aachen HMM model (Vogel et al, 1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "W10-3813", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Yanjun, Ma | Andy, Way", 
    "raw_text": "Generative word alignment models including IBM models (Brown et al, 1993) and HMM word alignment models (Vogel et al, 1996) have been widely used in various types of Statisti cal Machine Translation (SMT) systems", 
    "clean_text": "Generative word alignment models including IBM models (Brown et al, 1993) and HMM word alignment models (Vogel et al, 1996) have been widely used in various types of Statistical Machine Translation (SMT) systems.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "D09-1136", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Ding, Liu | Daniel, Gildea", 
    "raw_text": "Because such approaches directly learn a generative model over phrase pairs, they are theoretically preferable to the standard heuristics for extracting the phrase pairs from the many-to-one word-level alignments produced by the IBM series models (Brown et al, 1993) or the Hidden Markov Model (HMM) (Vogel et al,1996)", 
    "clean_text": "Because such approaches directly learn a generative model over phrase pairs, they are theoretically preferable to the standard heuristics for extracting the phrase pairs from the many-to-one word-level alignments produced by the IBM series models (Brown et al, 1993) or the Hidden Markov Model (HMM) (Vogel et al,1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "P12-1098", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Boxing, Chen | Roland, Kuhn | Samuel, Larkin", 
    "raw_text": "in this example: Ref: Recently, I visited Paris Hyp: I visited Paris recently Inspired by HMM word alignment (Vogel et al, 1996), our second distance measure is based on jump width", 
    "clean_text": "Inspired by HMM word alignment (Vogel et al, 1996), our second distance measure is based on jump width.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "P06-1002", 
    "citing_paper_authority": 23, 
    "citing_paper_authors": "Necip Fazil, Ayan | Bonnie Jean, Dorr", 
    "raw_text": "Starting with the IBM models (Brown et al,1993), researchers have developed various statistical word alignment systems based on different models, such as hidden Markov models (HMM) (Vogel et al, 1996), log-linear models (OchandNey, 2003), and similarity-based heuristic methods (Melamed, 2000)", 
    "clean_text": "Starting with the IBM models (Brown et al,1993), researchers have developed various statistical word alignment systems based on different models, such as hidden Markov models (HMM) (Vogel et al, 1996), log-linear models (OchandNey, 2003), and similarity-based heuristic methods (Melamed, 2000).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "P11-1042", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Chris, Dyer | Jonathan H., Clark | Alon, Lavie | Noah A., Smith", 
    "raw_text": "through the source sentence (Vogel et al., 1996)", 
    "clean_text": "One class of particularly useful features assesses the goodness of the alignment path through the source sentence (Vogel et al., 1996).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "P11-2030", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Hui, Lin | Jeff A., Bilmes", 
    "raw_text": "While classical approaches for word alignment are based on generative models (e.g., IBM models (Brown et al, 1993) and HMM (Vogel et al, 1996)), word alignment can also be viewed as a matching problem, where each word pair is associated with a score reflecting the desirability of aligning that pair, and the alignment is then the highest scored matching under some constraints", 
    "clean_text": "While classical approaches for word alignment are based on generative models (e.g., IBM models (Brown et al, 1993) and HMM (Vogel et al, 1996)), word alignment can also be viewed as a matching problem, where each word pair is associated with a score reflecting the desirability of aligning that pair, and the alignment is then the highest scored matching under some constraints.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "N10-1063", 
    "citing_paper_authority": 27, 
    "citing_paper_authors": "Jason R., Smith | Chris, Quirk | Kristina, Toutanova", 
    "raw_text": "This data is used to train a word alignment model, such as IBM Model 1 (Brown et al, 1993) or HMM-based word alignment (Vogel et al, 1996)", 
    "clean_text": "This data is used to train a word alignment model, such as IBM Model 1 (Brown et al, 1993) or HMM-based word alignment (Vogel et al, 1996).", 
    "keep_for_gold": 0
  }
]