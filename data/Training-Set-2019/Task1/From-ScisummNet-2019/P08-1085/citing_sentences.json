[
  {
    "citance_No": 1, 
    "citing_paper_id": "C08-1026", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Markus, Dickinson", 
    "raw_text": "Thus, an orthogonal line of research can involve inducing classes for words which are more general than single categories ,i.e., something akin to ambiguity classes (see ,e.g., the discussion of ambiguity class guessers in Goldberg et al, 2008)", 
    "clean_text": "Thus, an orthogonal line of research can involve inducing classes for words which are more general than single categories, i.e., something akin to ambiguity classes (see, e.g., the discussion of ambiguity class guessers in Goldberg et al, 2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P14-2132", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Hui, Zhang | John, DeNero", 
    "raw_text": "This efficient and data-driven approach gives the best reported tagging accuracy for type-supervised sequence models, outperforming the minimized model of Ravi and Knight (2009), the Bayesian LDA-based model of Toutanova and Johnson (2008), and an HMM trained with language-specific initialization described by Goldberg et al (2008)", 
    "clean_text": "This efficient and data-driven approach gives the best reported tagging accuracy for type-supervised sequence models, outperforming the minimized model of Ravi and Knight (2009), the Bayesian LDA-based model of Toutanova and Johnson (2008), and an HMM trained with language-specific initialization described by Goldberg et al (2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P14-2132", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Hui, Zhang | John, DeNero", 
    "raw_text": "93.4? Linguistic initialization (Goldberg et al, 2008) 91.4? 93.8? Minimal models (Ravi and Knight, 2009)? 92.3? 96.8 Table 2: Tagging accuracy of different approaches on English Penn Treebank", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "P14-2132", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Hui, Zhang | John, DeNero", 
    "raw_text": "However, it is an open question whether the technique is as effective for tag dictionaries derived from more natural sources than the labels of an existing treebank. All of the methods to which we compare except Goldberg et al (2008) focus on learning and modeling techniques, while our method only addresses initialization", 
    "clean_text": "All of the methods to which we compare except Goldberg et al (2008) focus on learning and modeling techniques, while our method only addresses initialization.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "E09-1042", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Kazi Saidul, Hasan | Vincent, Ng", 
    "raw_text": "See Goldberg et al (2008) for details", 
    "clean_text": "See Goldberg et al (2008) for details.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "P10-2039", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Ashish, Vaswani | Adam, Pauls | David, Chiang", 
    "raw_text": "Goldberg et al (2008) provide a linguistically-informed starting point for EM to achieve 91.4% accuracy", 
    "clean_text": "Goldberg et al (2008) provide a linguistically-informed starting point for EM to achieve 91.4% accuracy.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "P10-1132", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Omri, Abend | Roi, Reichart | Ari, Rappoport", 
    "raw_text": "Goldberg et al (2008) use linguistic con side rations for choosing a good starting point forthe EM algorithm", 
    "clean_text": "Goldberg et al (2008) use linguistic considerations for choosing a good starting point for the EM algorithm.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "D10-1071", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Rushin, Shah | Paramveer S., Dhillon | Mark Y., Liberman | Dean, Foster | Mohamed, Maamouri | Lyle H., Ungar", 
    "raw_text": "(Goldberg et al, 2008) extend the work of (Adler and Elhadad, 2006) by using an EM algorithm, and achieve an accuracy of 88% for full morphological analysis, but again, this does not include lemma IDs", 
    "clean_text": "(Goldberg et al, 2008) extend the work of (Adler and Elhadad, 2006) by using an EM algorithm, and achieve an accuracy of 88% for full morphological analysis, but again, this does not include lemma IDs.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "C10-2159", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Mehmet Ali, Yatbaz | Deniz, Yuret", 
    "raw_text": "dictionary (Goldberg et al, 2008) 91.8 Minimized models for EM-HMM with 100 random restarts (Ravi and Knight, 2009).", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "C10-2159", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Mehmet Ali, Yatbaz | Deniz, Yuret", 
    "raw_text": "Bayesian methods by achieving 88.6% accuracy on the same 24K corpus. Despite the fact that HMM-EM has a poor reputation in POS literature (Goldberg et al, 2008) has shown that with good initialization together with some language specific features and language dependent constraints HMM-EM achieves 91.4% accuracy", 
    "clean_text": "Despite the fact that HMM-EM has a poor reputation in POS literature (Goldberg et al, 2008) has shown that with good initialization together with some language specific features and language dependent constraints HMM-EM achieves 91.4% accuracy.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "E09-1038", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Yoav, Goldberg | Reut, Tsarfaty | Meni, Adler | Michael, Elhadad", 
    "raw_text": "Traditionally, such unsupervised EM-trained HMM taggers are thought to be inaccurate, but (Goldberg et al, 2008) showed that by feeding thee M process with sufficiently good initial probabilities, accurate taggers (& gt; 91% accuracy) can be learned for both English and Hebrew, based on a (possibly incomplete) lexicon and large amount of raw text", 
    "clean_text": "Traditionally, such unsupervised EM-trained HMM taggers are thought to be inaccurate, but (Goldberg et al, 2008) showed that by feeding thee M process with sufficiently good initial probabilities, accurate taggers (> 91% accuracy) can be learned for both English and Hebrew, based on a (possibly incomplete) lexicon and large amount of raw text.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "P11-2124", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Yoav, Goldberg | Michael, Elhadad", 
    "raw_text": "Our baseline lattice parsing experiment (without the lexicon) results in an F-score of around 76% .4 Segmentation? Parsing pipeline As another baseline, we experimented with a pipeline system in which the input text is automatically segmented and tagged using a state-of-the-art HMMpos-tagger (Goldberg et al, 2008)", 
    "clean_text": "As another baseline, we experimented with a pipeline system in which the input text is automatically segmented and tagged using a state-of-the-art HMMpos-tagger (Goldberg et al, 2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "D12-1075", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Dan, Garrette | Jason, Baldridge", 
    "raw_text": "Our own next steps are to move in a similar direction to explore the possibilities for encoding the intuitions we developed for initialization and minimization as a single generative model. Goldberg et al2008) note that fixing noisy dictionaries by hand is actually quite feasible, and suggest that effort should focus on exploiting human knowledge rather than just algorithmic improvements", 
    "clean_text": "Goldberg et al 2008 note that fixing noisy dictionaries by hand is actually quite feasible, and suggest that effort should focus on exploiting human knowledge rather than just algorithmic improvements.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "S12-1009", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Jonathan, Schler | Chaya, Liebeskind | Ido, Dagan", 
    "raw_text": "The probability of a lemma was defined as the sum of probabilities for all morphological analyses containing the lemma, using a morpho-lexical context-independent probabilities approximation (Goldberg et al, 2008)", 
    "clean_text": "The probability of a lemma was defined as the sum of probabilities for all morphological analyses containing the lemma, using a morpho-lexical context-independent probabilities approximation (Goldberg et al, 2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "S12-1009", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Jonathan, Schler | Chaya, Liebeskind | Ido, Dagan", 
    "raw_text": "The latter had two important characteristics: The first is flexibility This tagger allows adapting the estimates of the prior (context-independent) probability of each morphological analysis in an unsupervised manner, from an unlabeled corpus of the target domain (Goldberg et al, 2008)", 
    "clean_text": "The latter had two important characteristics: The first is flexibility This tagger allows adapting the estimates of the prior (context-independent) probability of each morphological analysis in an unsupervised manner, from an unlabeled corpus of the target domain (Goldberg et al, 2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "W12-1905", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Dirk, Hovy | Eduard, Hovy", 
    "raw_text": "The importance is underscored succinctly by Goldberg et al (2008)", 
    "clean_text": "The importance is underscored succinctly by Goldberg et al (2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "P09-1057", 
    "citing_paper_authority": 35, 
    "citing_paper_authors": "Sujith, Ravi | Kevin, Knight", 
    "raw_text": "6. EM-HMM tagger provided with good initial conditions (Goldberg et al, 2008) 91.4*", 
    "clean_text": "EM-HMM tagger provided with good initial conditions (Goldberg et al, 2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "P09-1057", 
    "citing_paper_authority": 35, 
    "citing_paper_authors": "Sujith, Ravi | Kevin, Knight", 
    "raw_text": "Goldberg et al (2008) depart from the Bayesian framework and show how EM can be used to learn good POS taggers for Hebrew and English, when provided with good initial conditions", 
    "clean_text": "Goldberg et al (2008) depart from the Bayesian framework and show how EM can be used to learn good POS taggers for Hebrew and English, when provided with good initial conditions.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "P09-1057", 
    "citing_paper_authority": 35, 
    "citing_paper_authors": "Sujith, Ravi | Kevin, Knight", 
    "raw_text": "The system achieves a better accuracy than the 88.6% from Smith and Eisner (2005), and even surpasses the 91.4% achieved by Goldberg et al (2008) without using any additional linguistic constraints or manual cleaning of the dictionary", 
    "clean_text": "The system achieves a better accuracy than the 88.6% from Smith and Eisner (2005), and even surpasses the 91.4% achieved by Goldberg et al (2008) without using any additional linguistic constraints or manual cleaning of the dictionary.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "P09-1057", 
    "citing_paper_authority": 35, 
    "citing_paper_authors": "Sujith, Ravi | Kevin, Knight", 
    "raw_text": "Their models are trained on the entire Penn Treebank data (in stead of using only the 24,115-token test data), and so are the tagging models used by Goldberg et al (2008)", 
    "clean_text": "Their models are trained on the entire Penn Treebank data (instead of using only the 24,115-token test data), and so are the tagging models used by Goldberg et al (2008).", 
    "keep_for_gold": 0
  }
]