Reinforcement Learning for Mapping Instructions to Actions
In this paper, we present a reinforcement learning approach for mapping natural language instructions to sequences of executable actions.
We assume access to a reward function that defines the quality of the executed actions.
During training, the learner repeatedly constructs action sequences for a set of documents, executes those actions, and observes the resulting reward.
We use a policy gradient algorithm to estimate the parameters of a log-linear model for action selection.
We apply our method to interpret instructions in two domains -- Windows troubleshooting guides and game tutorials.
Our results demonstrate that this method can rival supervised learning techniques while requiring few or no annotated training examples.
We show that performing policy-gradient with this function is equivalent to training a fully supervised, stochastic gradient algorithm that optimizes conditional likelihood.
