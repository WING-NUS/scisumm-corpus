Citance Number: 1 | Reference Article:  P09-1010.txt | Citing Article:  P10-1129.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our approach is most closely related to the reinforcement learning algorithm for mapping text instructions to commands developed by Branavan et al (2009) (see Section 4 for more detail).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Reinforcement Learning for Mapping Instructions to Actions</S><S sid = NA ssid = NA>In this paper, we presented a reinforcement learning approach for inducing a mapping between instructions and actions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P09-1010.txt | Citing Article:  P10-1129.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Previous work (Branavan et al, 2009) is only able to handle low-level instructions.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We aim to map this text to the corresponding low-level commands and parameters.</S><S sid = NA ssid = NA>Reinforcement Learning for Language Processing Reinforcement learning has been previously applied to the problem of dialogue management (Scheffler and Young, 2002; Roy et al., 2000; Litman et al., 2000; Singh et al., 1999).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P09-1010.txt | Citing Article:  P10-1129.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This environment reward function is a simplification of the one described in Branavan et al (2009), and it performs comparably in our experiments.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Reward Function Environment feedback can be used as a reward function in this domain.</S><S sid = NA ssid = NA>Reinforcement Learning for Language Processing Reinforcement learning has been previously applied to the problem of dialogue management (Scheffler and Young, 2002; Roy et al., 2000; Litman et al., 2000; Singh et al., 1999).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P09-1010.txt | Citing Article:  P10-1129.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In addition to the look-ahead features described in Section 5.2, the policy also includes the set of features used by Branavan et al (2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In total, there are 8,094 features.</S><S sid = NA ssid = NA>In total, there are 4,438 features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P09-1010.txt | Citing Article:  P10-1129.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Baselines As a baseline, we compare our method against the results reported by Branavan et al (2009), denoted here as BCZB09.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Reinforcement Learning for Language Processing Reinforcement learning has been previously applied to the problem of dialogue management (Scheffler and Young, 2002; Roy et al., 2000; Litman et al., 2000; Singh et al., 1999).</S><S sid = NA ssid = NA>Our method seamlessly combines these two kinds of rewards. sider two naive baselines.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P09-1010.txt | Citing Article:  P10-1129.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Performing policy-gradient with this function is equivalent to training a fully supervised, stochastic gradient algorithm that optimizes conditional likelihood (Branavan et al, 2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Thus, with this reward policy gradient is equivalent to stochastic gradient ascent with a maximum likelihood objective.</S><S sid = NA ssid = NA>As shown there, policy gradient with this reward is equivalent to stochastic gradient ascent with a maximum likelihood objective. when only a subset of training documents is annotated, and environment reward is used for the remainder.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P09-1010.txt | Citing Article:  W12-2802.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>(JAIR), 15:31–90.</S><S sid = NA ssid = NA>The Environment The environment state £ specifies the set of objects available for interaction, and their properties.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P09-1010.txt | Citing Article:  W12-2802.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>To learn from an unaligned corpus, we derive a new training algorithm that combines the Generalized Grounding Graph (G3) framework introduced by Tellex et al [2011] with the policy gradient method described by Branavan et al [2009].</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Reinforcement Learning for Language Processing Reinforcement learning has been previously applied to the problem of dialogue management (Scheffler and Young, 2002; Roy et al., 2000; Litman et al., 2000; Singh et al., 1999).</S><S sid = NA ssid = NA>Although there is no closed form solution, policy gradient algorithms (Sutton et al., 2000) estimate the parameters θ by performing stochastic gradient ascent.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P09-1010.txt | Citing Article:  W12-2802.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>(JAIR), 15:31–90.</S><S sid = NA ssid = NA>The Environment The environment state £ specifies the set of objects available for interaction, and their properties.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P09-1010.txt | Citing Article:  P10-1083.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We also compare against the policy gradient learning algorithm of Branavan et al (2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Algorithm 1 details the complete policy gradient algorithm.</S><S sid = NA ssid = NA>Reinforcement Learning for Language Processing Reinforcement learning has been previously applied to the problem of dialogue management (Scheffler and Young, 2002; Roy et al., 2000; Litman et al., 2000; Singh et al., 1999).</S> | Discourse Facet:  NA | Annotator: Automatic


