[
  {
    "citance_No": 1, 
    "citing_paper_id": "N03-1031", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Mark, Steedman | Rebecca, Hwa | Stephen, Clark | Miles, Osborne | Anoop, Sarkar | Julia, Hockenmaier | Paul, Ruhlen | Steven, Baker | Jeremiah, Crim", 
    "raw_text": "We have explored using different settings for the seed set size (Steedman et al, 2003)", 
    "clean_text": "We have explored using different settings for the seed set size (Steedman et al, 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "D12-1131", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Alexander M., Rush | Michael John, Collins | Roi, Reichart | Amir, Globerson", 
    "raw_text": "The complete literature on domain adaptation is beyond the scope of this paper, but we refer the reader to Blitzer and Daume (2010) for a recent survey. Specifically for parsing and POS tagging, self training (Reichart and Rappoport, 2007), co-training (Steedman et al 2003) and active learning (Hwa,2004) have been shown useful in the lightly supervised setup", 
    "clean_text": "Specifically for parsing and POS tagging, self training (Reichart and Rappoport, 2007), co-training (Steedman et al 2003) and active learning (Hwa,2004) have been shown useful in the lightly supervised setup.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P07-1078", 
    "citing_paper_authority": 19, 
    "citing_paper_authors": "Roi, Reichart | Ari, Rappoport", 
    "raw_text": "Steedman et al (2003b) followed a similar co-training protocol except that the selection function (three functions were explored) considered the differences between the confidence scores of the two parsers", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "P07-1078", 
    "citing_paper_authority": 19, 
    "citing_paper_authors": "Roi, Reichart | Ari, Rappoport", 
    "raw_text": "Steedman et al (2003b )andHwa et al (2003) also used several versions of corrected co-training which are not comparable to our sand other suggested methods because their evaluation requires different measures (e.g. reviewed and corrected constituents are separately counted)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P07-1078", 
    "citing_paper_authority": 19, 
    "citing_paper_authors": "Roi, Reichart | Ari, Rappoport", 
    "raw_text": "This protocol and that of Steedman et al (2003a) were applied to the problem, with the same seed, self-training and test sets", 
    "clean_text": "This protocol and that of Steedman et al (2003a) were applied to the problem, with the same seed, self-training and test sets.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "P07-1078", 
    "citing_paper_authority": 19, 
    "citing_paper_authors": "Roi, Reichart | Ari, Rappoport", 
    "raw_text": "3 (Steedman et al, 2003a) used the first 500 sentences of WSJ training section as seed data", 
    "clean_text": "(Steedman et al, 2003a) used the first 500 sentences of WSJ training section as seed data.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "P07-1078", 
    "citing_paper_authority": 19, 
    "citing_paper_authors": "Roi, Reichart | Ari, Rappoport", 
    "raw_text": "The self-training protocol of (Steedman et al, 2003a) does not actually improve over the baseline of using only the seed data", 
    "clean_text": "The self-training protocol of (Steedman et al, 2003a) does not actually improve over the baseline of using only the seed data.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "P07-1078", 
    "citing_paper_authority": 19, 
    "citing_paper_authors": "Roi, Reichart | Ari, Rappoport", 
    "raw_text": "The only previous work that adapts a parser trained on a small dataset between domains is that of (Steedman et al, 2003a), which used co-training (no self-training results were reported there or elsewhere)", 
    "clean_text": "The only previous work that adapts a parser trained on a small dataset between domains is that of (Steedman et al, 2003a), which used co-training (no self-training results were reported there or elsewhere).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "P07-1078", 
    "citing_paper_authority": 19, 
    "citing_paper_authors": "Roi, Reichart | Ari, Rappoport", 
    "raw_text": "In deed, in the II scenario, (Steedman et al, 2003a; McClosky et al, 2006a; Charniak, 1997) reported no improvement of the base parser for small (500sentences, in the first paper) and large (40K sentences, in the last two papers) seed datasets respectively", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "P06-1043", 
    "citing_paper_authority": 52, 
    "citing_paper_authors": "David, McClosky | Eugene, Charniak | Mark, Johnson", 
    "raw_text": "Steedman et al (2003) apply co training to parser adaptation and find that co training can work across domains", 
    "clean_text": "Steedman et al (2003) apply co-training to parser adaptation and find that co-training can work across domains.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "W08-1122", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Deirdre, Hogan | Jennifer, Foster | Joachim, Wagner | Josef, van Genabith", 
    "raw_text": "These parser output trees can by produced by a second parser in a co-training scenario (Steedman et al, 2003), or by the same parser with a re ranking component in a type of self training scenario (McClosky et al, 2006)", 
    "clean_text": "These parser output trees can by produced by a second parser in a co-training scenario (Steedman et al, 2003), or by the same parser with a re ranking component in a type of self training scenario (McClosky et al, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "W07-2204", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Jennifer, Foster | Joachim, Wagner | Djam&eacute;, Seddah | Josef, van Genabith", 
    "raw_text": "Self training is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield enc our aging results (Steedman et al, 2003)", 
    "clean_text": "Self training is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results (Steedman et al, 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "W09-2205", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Barbara, Plank", 
    "raw_text": "Steedman et al (2003) as well as Reichart and Rappoport (2007) examine self-training for PCFG parsing in the small seed case (&lt; 1k labeled data), with different results", 
    "clean_text": "Steedman et al (2003) as well as Reichart and Rappoport (2007) examine self-training for PCFG parsing in the small seed case (< 1k labeled data), with different results.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "W09-2205", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Barbara, Plank", 
    "raw_text": "In the iterative setting, we fol low Steedman et al (2003) and parse 30 sentences from which 20 are selected in every iteration", 
    "clean_text": "In the iterative setting, we follow Steedman et al (2003) and parse 30 sentences from which 20 are selected in every iteration.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "N06-1020", 
    "citing_paper_authority": 88, 
    "citing_paper_authors": "David, McClosky | Eugene, Charniak | Mark, Johnson", 
    "raw_text": "It is not surprising that self-training is not normally effective: Charniak (1997) and Steedman et al (2003) report either mi nor improvements or significant damage from using self-training for parsing", 
    "clean_text": "It is not surprising that self-training is not normally effective: Charniak (1997) and Steedman et al (2003) report either minor improvements or significant damage from using self-training for parsing.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "N06-1020", 
    "citing_paper_authority": 88, 
    "citing_paper_authors": "David, McClosky | Eugene, Charniak | Mark, Johnson", 
    "raw_text": "If this is the case, we can be more confident that the data was labelled correctly than if only one learner had labelled it. Sarkar (2001) and Steedman et al (2003 )investigated using co-training for parsing", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "P06-1072", 
    "citing_paper_authority": 22, 
    "citing_paper_authors": "Noah A., Smith | Jason M., Eisner", 
    "raw_text": "Bootstrapping was applied to syntax learning by Steedman et al (2003)", 
    "clean_text": "Bootstrapping was applied to syntax learning by Steedman et al (2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "D09-1087", 
    "citing_paper_authority": 21, 
    "citing_paper_authors": "Zhongqiang, Huang | Mary P., Harper", 
    "raw_text": "Steedman et al (2003) re ported some degradation using a lexicalized tree adjoining grammar parser and minor improvement using Collinslexicalized PCFG parser; how ever, this gain was obtained only when the parser 832 was trained on a small labeled set", 
    "clean_text": "Steedman et al (2003) reported some degradation using a lexicalized tree adjoining grammar parser and minor improvement using Collins lexicalized PCFG parser; however, this gain was obtained only when the parser was trained on a small labeled set.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "W03-1015", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Vincent, Ng | Claire, Cardie", 
    "raw_text": "Goldman and Zhou (2000) and Steedman et al (2003b) use two different learning algorithms in lieu of the multiple views required by standard co-training.1 The intuition is that the two learning algorithms can potentially substitute for the two views: different learners have different representation and search biases and can complement each other by inducing different hypotheses from the data", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "W03-1015", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Vincent, Ng | Claire, Cardie", 
    "raw_text": "On the other hand, Steedman et al use two learning algorithms that correspond to coarsely different features, thus retaining in spirit the advantages1Steedman et al (2003b) bootstrap two parsers that use different statistical models via co-training", 
    "clean_text": "Steedman et al (2003b) bootstrap two parsers that use different statistical models via co-training.", 
    "keep_for_gold": 0
  }
]