Citance Number: 1 | Reference Article:  A00-1031.txt | Citing Article:  W00-1326.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The sentences in the DSO collection were tagged with parts of speech using TnT (Brants, 2000) trained on the Brown Corpus itself.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>TnT - A Statistical Part-Of-Speech Tagger</S><S sid = NA ssid = NA>Transition and output probabilities are estimated from a tagged corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  A00-1031.txt | Citing Article:  P14-2043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>And, last but not least, I would like to thank the users of TnT who provided me with bug reports and valuable suggestions for improvements.</S><S sid = NA ssid = NA>It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  A00-1031.txt | Citing Article:  W06-3120.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The English POS-tagging has been carried out using freely available TNT tagger (Brants, 2000).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>TnT is freely available to universities and related organizations for research purposes (see http://www.coli.uni-sb.derthorstenAnt).</S><S sid = NA ssid = NA>This tagger, TnT, not only yielded the highest accuracy, it also was the fastest both in training and tagging.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  A00-1031.txt | Citing Article:  I08-3012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This proposition is quite viable as statistical POS taggers like TnT (Brants, 2000) are available.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>TnT - A Statistical Part-Of-Speech Tagger</S><S sid = NA ssid = NA>Trigrams'n'Tags (TnT) is an efficient statistical part-of-speech tagger.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  A00-1031.txt | Citing Article:  W11-1502.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We use TnT (Brants, 2000), a second order Markov Model tagger.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>TnT uses second order Markov models for part-ofspeech tagging.</S><S sid = NA ssid = NA>For example, the Markov model tagger used in the comparison of (van Halteren et al., 1998) yielded worse results than all other taggers.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  A00-1031.txt | Citing Article:  W10-3007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For PoS tagging and lemmatization, we combine GENIA (with its built-in, occasionally deviant to kenizer) and TnT (Brants, 2000), which operates on pre-tokenized inputs but in its default models trained on financial news from the Penn Tree bank.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>TnT uses second order Markov models for part-ofspeech tagging.</S><S sid = NA ssid = NA>Tagging accuracies for the Penn Treebank are shown in table 5.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  A00-1031.txt | Citing Article:  W08-0325.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Tag the tokens with PTB-style POS tags using a tagger (Brants, 2000).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The latter are interesting, since usually unknown tokens are much more difficult to process than known tokens, for which a list of valid tags can be found in the lexicon.</S><S sid = NA ssid = NA>The accuracy for known tokens is significantly higher than for unknown tokens.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  A00-1031.txt | Citing Article:  P13-2112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For example, Petrov et al (2012) build supervised POS taggers for 22 languages using the TNT tagger (Brants, 2000), with an average accuracy of 95.2%.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For example, the Markov model tagger used in the comparison of (van Halteren et al., 1998) yielded worse results than all other taggers.</S><S sid = NA ssid = NA>Additionally, we present results of the tagger on the NEGRA corpus (Brants et al., 1999) and the Penn Treebank (Marcus et al., 1993).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  A00-1031.txt | Citing Article:  P13-2112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Forun aligned words, we simply assign a random POS and very low probability, which does not substantially affect transition probability estimates. In Step 6 we build a tagger by feeding the es ti mated emission and transition probabilities into the TNT tagger (Brants, 2000), an implementation of a trigram HMM tagger.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Therefore, we estimate a trigram probability as follows: P are maximum likelihood estimates of the probabilities, and A1 + A2 ± A3 = 1, SO P again represent probability distributions.</S><S sid = NA ssid = NA>Transition and output probabilities are estimated from a tagged corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  A00-1031.txt | Citing Article:  W05-0807.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>based on tree-structures of various complexity in the tree-adjoining grammar model. Using such tags, Brants (2000) has achieved the automated tagging of a syntactic-structure-based set of grammatical function tags including phrase-chunk and syntactic-role modifiers trained in supervised mode from a tree bank of German.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Contrary to claims found elsewhere in the literature, we argue that a tagger based on Markov models performs at least as well as other current approaches, including the Maximum Entropy framework.</S><S sid = NA ssid = NA>It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m — 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  A00-1031.txt | Citing Article:  W04-1217.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We also incorporated part-of speech tagging, using the TnT tagger (Brants, 2000) retrained on the GENIA corpus gold standard part of-speech tagging.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>TnT - A Statistical Part-Of-Speech Tagger</S><S sid = NA ssid = NA>Trigrams'n'Tags (TnT) is an efficient statistical part-of-speech tagger.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  A00-1031.txt | Citing Article:  W08-1708.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>POS Majority lexical type noun count-noun-le c-n-f verb trans-nerg-str-verb-le haben-auxf adj adj-non-prd-le adv intersect-adv-le Table 5: POS tags to lexical types mapping Again for comparison, we have built another simple baseline model using the TnT POS tagger (Brants, 2000).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The reader will be surprised how simple the underlying model is.</S><S sid = NA ssid = NA>As a second step, contextual frequencies are smoothed and lexical frequences are completed by handling words that are not in the lexicon (see below).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  A00-1031.txt | Citing Article:  W02-1040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The texts were POS-tagged using TnT (Brants,2000).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Part of it was tagged at the IMS Stuttgart.</S><S sid = NA ssid = NA>Transition and output probabilities are estimated from a tagged corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  A00-1031.txt | Citing Article:  C10-1030.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The freely-available POS lexicon from Sharoff et al (2008), specifically the file for the POS tagger TnT (Brants, 2000), contains full words (239,889 unique forms), with frequency information.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Additionally, we present results of the tagger on the NEGRA corpus (Brants et al., 1999) and the Penn Treebank (Marcus et al., 1993).</S><S sid = NA ssid = NA>TnT is freely available to universities and related organizations for research purposes (see http://www.coli.uni-sb.derthorstenAnt).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  A00-1031.txt | Citing Article:  C10-1030.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We use a corpus of 5 million words automatically tagged by TnT (Brants, 2000) and freely available online (Sharoff et al, 2008). Because we want to make linguistically-informed corruptions, we corrupt only the words we have information for, identifying the words in the corpus which are found in the lexicon with the appropriate POS tag. We also select only words which have inflectional morphology: nouns, verbs, adjectives, pronouns, and numerals.7 4.2.1 Determining word properties (step 1) We use the POS tag to restrict the properties of a word, regardless of how exactly we corrupt it.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Should we use all words, or are some of them better suited than others?</S><S sid = NA ssid = NA>Accepting that unknown words are most probably infrequent, one can argue that using suffixes of infrequent words in the lexicon is a better approximation for unknown words than using suffixes of frequent words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  A00-1031.txt | Citing Article:  C10-1030.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>To POS tag, we use the HMM tagger TnT (Brants, 2000) with the model from http: //corpus.leeds.ac.uk/mocky/.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>TnT - A Statistical Part-Of-Speech Tagger</S><S sid = NA ssid = NA>The tagger is allowed to assign exactly one tag to each token.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  A00-1031.txt | Citing Article:  C02-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>After finishing the corrections, we experimented with training and testing the TnT tagger (Brants,2000) on the& quot; old& quot; and on the& quot; corrected& quot; version of NEGRA?.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The result of the tagger comparison seems to support the maxime &quot;the simplest is the best&quot;.</S><S sid = NA ssid = NA>They are only surpassed by combinations of different systems, forming a &quot;voting tagger&quot;.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  A00-1031.txt | Citing Article:  W12-1911.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>To make them useful, the necessary preprocessing steps must have been done. The texts were first automatically segmented and tokenized and then they were part-of-speech tagged by TnT tagger (Brants, 2000), which was trained on the respective WILS training data.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>TnT - A Statistical Part-Of-Speech Tagger</S><S sid = NA ssid = NA>Trigrams'n'Tags (TnT) is an efficient statistical part-of-speech tagger.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  A00-1031.txt | Citing Article:  W11-0415.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>POS tags, on the other, represent more of a challenge with only 91.6% NORM LEMMA POS Agreed tokens (out of 57,845) 56,052 55,217 52,959 Accuracy (%) 96.9% 95.5% 91.6% Table 3: Inter-annotator agreement agreement between two annotators, which is cons id erably lower than the agreement level reported for annotating a corpus of modern German using STTS, at 98.6% (Brants, 2000a).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>As few as 1000 tokens are sufficient to achieve 95%-96% accuracy for them.</S><S sid = NA ssid = NA>The states of the model represent tags, outputs represent the words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  A00-1031.txt | Citing Article:  W11-0415.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We further plan to retrain state-of the-art POS taggers such as the TreeTagger and TnT Tagger (Brants, 2000b) on our data. Finally, we plan to investigate how linguistic annotations can be automatically integrated in the TEI annotated version of the corpus to produce TEI con formant output.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We have shown that a tagger based on Markov models yields state-of-the-art results, despite contrary claims found in the literature.</S><S sid = NA ssid = NA>Transition and output probabilities are estimated from a tagged corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


