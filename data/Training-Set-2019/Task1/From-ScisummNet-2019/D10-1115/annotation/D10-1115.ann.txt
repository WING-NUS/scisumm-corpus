Citance Number: 1 | Reference Article:  D10-1115.txt | Citing Article:  D11-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Compositionality of adjective-noun phrases and how it can be adequately modeled in VSMs is the main concern in Baroni and Zamparelli (2010) and Guevara (2010), who are in search of the best composition operator for combining adjective with noun meanings.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The main innovation of Guevara (2010), who focuses on adjective-noun combinations (AN), is to use the cooccurrence vectors of observed ANs to train a supervised composition model (we became aware of Guevara’s approach after we had developed our own model, that also exploits observed ANs for training).</S><S sid = NA ssid = NA>Following the insight of FS, we treat attributive adjectives as functions over noun meanings; however, noun meanings are vectors, not sets, and the functions are learnt from corpus-based noun-AN vector pairs.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  D10-1115.txt | Citing Article:  W11-1301.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We evaluate four different compositionality models shown to have various levels of success in representing the meaning of AN pairs: the simple additive and multiplicative models of Mitchell and Lapata (2008), and the linear-map-based models of Guevara (2010) and Baroni and Zamparelli (2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Guevara compares his model to the simplified additive and multiplicative models of Mitchell and Lapata.</S><S sid = NA ssid = NA>Given two vectors u and v, they identify two general classes of composition models, (linear) additive models: where A and B are weight matrices, and multiplicative models: where C is a weight tensor projecting the uv tensor product onto the space of p. Mitchell and Lapata derive two simplified models from these general forms.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  D10-1115.txt | Citing Article:  W11-1301.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The final approach we re-implement is the one proposed by Baroni and Zamparelli (2010), who treat attributive adjectives as functions from noun meanings to noun meanings.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Following the insight of FS, we treat attributive adjectives as functions over noun meanings; however, noun meanings are vectors, not sets, and the functions are learnt from corpus-based noun-AN vector pairs.</S><S sid = NA ssid = NA>As discussed in the introduction, we will take adjectives in attributive position to be functions from one noun meaning to another.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  D10-1115.txt | Citing Article:  W11-1301.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Baroni and Zamparelli (2010) show that their model significantly outperforms other vector composition methods, including addition, multiplication and Guevara's approach, in the task of approximating the correct vectors for previously unseen (but corpus-attested) ANs.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In Section 6, we show that our model outperforms other approaches at the task of approximating such vectors for unseen ANs.</S><S sid = NA ssid = NA>Our model significantly outperforms the rivals on the task of reconstructing AN vectors not seen in training.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  D10-1115.txt | Citing Article:  W11-1301.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Note that they follow very closely the procedure of Baroni and Zamparelli (2010), including choices of source corpus and parameter values, so that we expect their results on the quality of the various models in predicting ANs to also hold for our re-implementations.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Moreover, coherently with this view, our evaluation below will be based on how closely the models approximate the observed vectors of unseen ANs.</S><S sid = NA ssid = NA>We conjecture that this is because the SVD dimensions can have negative values, leading to counter-intuitive results with component-wise multiplication (multiplying large opposite-sign values results in large negative values).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  D10-1115.txt | Citing Article:  W11-1301.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Confirming the results of Baroni and Zamparelli (2010), non-normalized versions of add and mult were also tested, but did not produce significant results (in the case of multiplication, normalization amounts to multiplying the composite vector by a scalar, so it only affects the length-dependent vector length measure).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Multiplicative vectors (mult method) were obtained by componentwise multiplication of the adjective and noun vectors (normalization does not matter here since it amounts to multiplying the composite vector by a scalar, and the cosine similarity measure we use is scale-invariant).</S><S sid = NA ssid = NA>Thus, in Section 6 we report mult results from the full co-occurrence matrix; reduced space results for all other methods.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  D10-1115.txt | Citing Article:  W11-1301.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>It is important to note that, as reported in Baroni and Zamparelli (2010), the mult method can be expected to perform better in the original, non reduced semantic space because the SVD dimensions can have negative values, leading to counter intuitive results with component-wise multiplication (multiplying large opposite-sign values results in large negative values instead of being cancelled out).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We conjecture that this is because the SVD dimensions can have negative values, leading to counter-intuitive results with component-wise multiplication (multiplying large opposite-sign values results in large negative values).</S><S sid = NA ssid = NA>Thus, in Section 6 we report mult results from the full co-occurrence matrix; reduced space results for all other methods.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  D10-1115.txt | Citing Article:  W11-1301.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Following Guevara, we estimate the coefficients of the equation using (multivariate) partial least squares regression (PLSR) as implemented in the Rpls package (MevikandWehrens, 2007), with the latent dimension parameter of PLSR set to 50, the same value used by Baroni and Zamparelli (2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We estimate the coefficients using (multivariate) partial least squares regression (PLSR) as implemented in the R pls package (Mevik and Wehrens, 2007).</S><S sid = NA ssid = NA>The number of latent variables to be used in the core regression are a free parameter of PLSR.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  D10-1115.txt | Citing Article:  W11-1301.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Finally, in the adjective-specific linear map (alm) method of Baroni and Zamparelli (2010), an AN is generated by multiplying an adjective weight matrix with a noun vector.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In the proposed adjective-specific linear map (alm) method, an AN is generated by multiplying an adjective weight matrix with a noun (column) vector.</S><S sid = NA ssid = NA>In our approach, the weight matrix B is specific to a single adjective – as we will see in Section 7 below, it is our representation of the meaning of the adjective.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  D10-1115.txt | Citing Article:  W11-1301.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In Baroni and Zamparelli (2010), the alm model performed far better than add and mult in approximating the correct vectors for unseen ANs, while on this (in a sense, more meta linguistic) task add and mult work better, while alm is successful only in the more sophisticated measure of neighbor density.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In Section 6, we show that our model outperforms other approaches at the task of approximating such vectors for unseen ANs.</S><S sid = NA ssid = NA>Although, in relative terms and considering the difficulty of the task, alm performs well, it is still far from perfect – for 27% alm-predicted ANs, the observed vector is not even in the top 1K neighbor set!</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  D10-1115.txt | Citing Article:  W11-1301.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Although, somewhat disappointingly, the model that has been shown in a previous study (Baroni and Zamparelli, 2010) to be the best at capturing the semantics of well-formed ANs turns out to be worse than simple addition and multiplication.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>5 Study 1: ANs in semantic space The actual distribution of ANs in the corpus, as recorded by their co-occurrence vectors, is fundamental to what we are doing.</S><S sid = NA ssid = NA>The best results on the task of paraphrasing noun-verb combinations with ambiguous verbs (sales slump is more like declining than slouching) are obtained using the multiplicative approach, and by weighted combination of addition and multiplication (we do not test model combinations in our current experiments).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  D10-1115.txt | Citing Article:  D11-1016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In particular, Baroni and Zamparelli (2010) tackle adjective-noun compositions using a vector representation for nouns and learning a matrix representation for each adjective.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In our approach, the weight matrix B is specific to a single adjective – as we will see in Section 7 below, it is our representation of the meaning of the adjective.</S><S sid = NA ssid = NA>Original contribution We propose and evaluate a new method to derive distributional representations for ANs, where an adjective is a linear function from a vector (the noun representation) to another vector (the AN representation).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  D10-1115.txt | Citing Article:  D11-1016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Work by Baroni and Zamparelli (2010) models nouns as vectors in some semantic space and adjectives as matrices.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Nouns are Vectors Adjectives are Matrices: Representing Adjective-Noun Constructions in Semantic Space</S><S sid = NA ssid = NA>Given two vectors u and v, they identify two general classes of composition models, (linear) additive models: where A and B are weight matrices, and multiplicative models: where C is a weight tensor projecting the uv tensor product onto the space of p. Mitchell and Lapata derive two simplified models from these general forms.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  D10-1115.txt | Citing Article:  E12-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Baroni and Zamparelli (2010) and Guevara (2010) look at corpus-harvested phrase vectors to learn composition functions that should derive such composite vectors automatically.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Unlike Guevara, (i) we train separate models for each adjective (we learn adjective-specific functions, whereas Guevara learns a generic “AN-slot” function) and, consequently, (ii) corpus-harvested adjective vectors play no role for us (their values would be constant across the training input vectors).</S><S sid = NA ssid = NA>Since the observed vectors look like plausible representations of composite meaning, we expect that the closer the modelgenerated vectors are to the observed ones, the better they should also perform in any task that requires access to the composite meaning, and thus that the results of the current evaluation should correlate with applied performance.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  D10-1115.txt | Citing Article:  W11-2506.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Baroni and Zamparelli (2010) and Guevara (2010) focus on how best to represent compositionality in adjective-noun phrases considering different types of composition operators.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The main innovation of Guevara (2010), who focuses on adjective-noun combinations (AN), is to use the cooccurrence vectors of observed ANs to train a supervised composition model (we became aware of Guevara’s approach after we had developed our own model, that also exploits observed ANs for training).</S><S sid = NA ssid = NA>Since our current focus is on alternative composition methods evaluated on a shared semantic space, exploring parameters pertaining to the construction of the semantic space is not one of our priorities, although we cannot of course exclude that the nature of the underlying semantic space affects different composition methods differently.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  D10-1115.txt | Citing Article:  D12-1110.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Baroni and Zamparelli (2010) computed the parent vector of adjective-noun pairs by p= Ab, where A is an adjective matrix and b is a vector for a noun.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In the framework of Mitchell and Lapata, our approach derives from the additive form in Equation (1) with the matrix multiplying the adjective vector (say, A) set to 0: p=Bv where p is the observed AN vector, B the weight matrix representing the adjective at hand, and v a noun vector.</S><S sid = NA ssid = NA>In the proposed adjective-specific linear map (alm) method, an AN is generated by multiplying an adjective weight matrix with a noun (column) vector.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  D10-1115.txt | Citing Article:  S12-1011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Relevant papers include O Seaghdha (2010), who evaluates several topic models adapted to learning selectional preference using co-occurence and Baroni and Zamparelli (2010), who represent nouns as vectors and adjectives as matrices, thus treating them as functions over noun meaning.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Nouns are Vectors Adjectives are Matrices: Representing Adjective-Noun Constructions in Semantic Space</S><S sid = NA ssid = NA>As discussed in the introduction, we will take adjectives in attributive position to be functions from one noun meaning to another.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  D10-1115.txt | Citing Article:  D12-1112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The model by Baroni and Zamparelli (2010) emerges as a suitable model of adjectival composition, while multiplication and addition shed mixed results.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In Section 7, we discuss how adjectival meaning can be represented in our model and evaluate this representation in an adjective clustering task.</S><S sid = NA ssid = NA>Our proposed method, alm, emerges as the best approach.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  D10-1115.txt | Citing Article:  D12-1112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Interestingly, recent approaches to the semantic composition of adjectives with nouns such as Baroni and Zamparelli (2010) and Guevara (2010) draw on the classical analysis of adjectives within the Montagovian tradition of formal semantic theory (Montague, 1974), on which they are treated as higher order predicates, and model adjectives as matrices of weights that are applied to noun vectors.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Nouns are Vectors Adjectives are Matrices: Representing Adjective-Noun Constructions in Semantic Space</S><S sid = NA ssid = NA>However, to make the analysis more challenging and interesting, we populate the semantic space where we will look at the behaviour of the ANs with a large number of adjectives and nouns, as well as further ANs not in the test set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  D10-1115.txt | Citing Article:  D12-1112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In the adjective-specific linear map (alm) model, proposed by Baroni and Zamparelli (2010), a different matrix B is learnt for each adjective.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In the proposed adjective-specific linear map (alm) method, an AN is generated by multiplying an adjective weight matrix with a noun (column) vector.</S><S sid = NA ssid = NA>The linear map for a specific adjective is learnt, using linear regression, from pairs of noun and AN vectors extracted from a corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


