Citance Number: 1 | Reference Article:  J99-4005.txt | Citing Article:  P01-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The bias introduced by TMEMs is a practical alternative to finding optimal translations, which is NP-complete (Knight, 1999).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In that case, finding a legal word ordering is like finding a complete circuit in a graph.</S><S sid = NA ssid = NA>Next we give separate polynomial-time reductions from two NP-complete problems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  J99-4005.txt | Citing Article:  P13-1001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>However, as phrase-based decoding usually casts translation as a string concatenation problem and permits arbitrary permutation, it proves to be NP-complete (Knight, 1999).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We note that Brew (1992) discusses the NPcompleteness of a related problem, that of finding some permutation of a string that is acceptable to a given context-free grammar.</S><S sid = NA ssid = NA>We show that for the simplest form of statistical models, this problem is NP-complete, i.e., probably exponential in the length of the observed sentence.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  J99-4005.txt | Citing Article:  E06-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In (Knight,1999) it was proved that the Exact Decoding problem is NP-Hard when the language model is a bigram model.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>As with tagging, we can assume an alphabet of v source tokens, a bigram source model, a substitution channel model, and an m-token coded text.</S><S sid = NA ssid = NA>If we assume that the channel model offers deterministic, word-for-word translations, then the bigram source model takes responsibility for ordering them.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  J99-4005.txt | Citing Article:  E06-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Note that our results for decoding are sharper than that of (Knight, 1999).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Both of these results deal with decision problems.</S><S sid = NA ssid = NA>We note that Brew (1992) discusses the NPcompleteness of a related problem, that of finding some permutation of a string that is acceptable to a given context-free grammar.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  J99-4005.txt | Citing Article:  H05-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>However allowing reordering in translation is computationally expensive and in some cases even provably NP-complete (Knight, 1999).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Finally, expensive decoding also suggests expensive training from unannotated (monolingual) texts, which presents a challenging bottleneck for extending statistical machine translation to language pairs and domains where large bilingual corpora do not exist.</S><S sid = NA ssid = NA>Next we give separate polynomial-time reductions from two NP-complete problems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  J99-4005.txt | Citing Article:  D10-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In theory, this process can be reduced to the Traveling Salesman Problem and thus requires an exponential time algorithm (Knight, 1999).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Salesman Problem.</S><S sid = NA ssid = NA>To the extent that word ordering is like solving the Traveling Salesman Problem, it is encouraging substantial progress continues to be made on Traveling Salesman algorithms.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  J99-4005.txt | Citing Article:  P07-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Part of the complexity arises from the expressive power of the translation model: for example, a phrase or word-based model with full reordering has exponential complexity (Knight, 1999).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The former is more closely tied to the source model, and the latter to the channel model, though the complexity arises from the interaction of the two.</S><S sid = NA ssid = NA>Decoding Complexity In Word-Replacement Translation Models</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  J99-4005.txt | Citing Article:  P03-2041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This approach offers four features absent from IBM-style models: (1) a recursive phrase-based translation, (2) a syntax-based language model, (3) the ability to condition a word's translation on the translation of syntactically related words, and (4) polynomial-time optimal alignment and decoding (Knight, 1999).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We should note that Model 1 is an intentionally simple translation model, one whose primary purpose in machine translation has been to allow bootstrapping into more complex translation models (e.g., IBM Models 2-5).</S><S sid = NA ssid = NA>Decoding Complexity In Word-Replacement Translation Models</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  J99-4005.txt | Citing Article:  D09-1108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Both of the two steps are very time-consuming due to the exponential number of translation rules and the complex nature of machine translation as an NP-hard search problem (Knight, 1999).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We should note that Model 1 is an intentionally simple translation model, one whose primary purpose in machine translation has been to allow bootstrapping into more complex translation models (e.g., IBM Models 2-5).</S><S sid = NA ssid = NA>We review it here for purposes of comparison with machine translation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  J99-4005.txt | Citing Article:  P05-1068.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>If arbitrary re-orderings are allowed, the search problem is NP-complete (Knight, 1999).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We show that for the simplest form of statistical models, this problem is NP-complete, i.e., probably exponential in the length of the observed sentence.</S><S sid = NA ssid = NA>We show that for the simplest form of statistical models, this problem is NP-complete, i.e., probably exponential in the length of the observed sentence.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  J99-4005.txt | Citing Article:  W06-3602.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Under certain restrictions, both algorithms handle MT-related problems efficiently that are generally NP complete (Knight, 1999).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>It is also possible to devise approximation algorithms like those devised for other NP-complete problems.</S><S sid = NA ssid = NA>Next we give separate polynomial-time reductions from two NP-complete problems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  J99-4005.txt | Citing Article:  W06-3602.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In the general case, no efficient search algorithm exists to search all word or phrase reorderings (Knight, 1999).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>So no Hamilton Circuit exists.</S><S sid = NA ssid = NA>So far, statistical translation research has either opted for heuristic beam-search algorithms or different channel models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  J99-4005.txt | Citing Article:  W06-3602.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>(Knight, 1999) shows that the decoding problem for SMT as well as some bilingual tiling problems are NP-complete, so no efficient algorithm exists in the general case.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Next we give separate polynomial-time reductions from two NP-complete problems.</S><S sid = NA ssid = NA>So no Hamilton Circuit exists.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  J99-4005.txt | Citing Article:  H05-1101.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Investigation of the computational complexity of translation models has started in (Knight, 1999) for word-to-word models.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Decoding Complexity In Word-Replacement Translation Models</S><S sid = NA ssid = NA>We should note that Model 1 is an intentionally simple translation model, one whose primary purpose in machine translation has been to allow bootstrapping into more complex translation models (e.g., IBM Models 2-5).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  J99-4005.txt | Citing Article:  D08-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Knight (1999) has shown that even for a simple form of statistical MT models, the decoding problem is NP-complete.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We show that for the simplest form of statistical models, this problem is NP-complete, i.e., probably exponential in the length of the observed sentence.</S><S sid = NA ssid = NA>We show that for the simplest form of statistical models, this problem is NP-complete, i.e., probably exponential in the length of the observed sentence.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  J99-4005.txt | Citing Article:  P09-1038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>in particular (Knight, 1999) has shown that any TSP instance can be mapped to a sub-case of a word-based SMT model, demonstrating NP-hardness of the decoding task.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We transform any Hamilton Circuit instance into an M1-DECIDE instance as follows.</S><S sid = NA ssid = NA>The proofs we presented are based on a worst-case analysis.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  J99-4005.txt | Citing Article:  P09-1038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>As already mentioned, the similarity between SMT decoding and TSP was recognized in (Knight, 1999), who focussed on showing that any TSP can be reformulated as a sub-class of the SMT decoding problem, proving that SMT decoding is NP-hard.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Decoding Complexity In Word-Replacement Translation Models</S><S sid = NA ssid = NA>This paper looks at decoding complexity.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  J99-4005.txt | Citing Article:  N03-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Knight (1999) has shown the problem to be NP-complete.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We show that for the simplest form of statistical models, this problem is NP-complete, i.e., probably exponential in the length of the observed sentence.</S><S sid = NA ssid = NA>We show that for the simplest form of statistical models, this problem is NP-complete, i.e., probably exponential in the length of the observed sentence.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  J99-4005.txt | Citing Article:  N03-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Finally, expensive decoding also suggests expensive training from unannotated (monolingual) texts, which presents a challenging bottleneck for extending statistical machine translation to language pairs and domains where large bilingual corpora do not exist.</S><S sid = NA ssid = NA>In the main channel model of Brown et al. (1993), each English word token e, in a source sentence is assigned a &quot;fertility&quot; 0â€ž which dictates how many French words it will produce.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  J99-4005.txt | Citing Article:  N10-1016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>It has been known that phrase-based decoding should be constrained to some extent not only for transferring the NP-hard problem (Knight,1999) into a tractable one in practice but also for improving translation quality.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Decoding Complexity In Word-Replacement Translation Models</S><S sid = NA ssid = NA>If word pairs have probabilities attached to them, then word ordering resembles the finding the least-cost circuit, also known as the Traveling Salesman Problem.</S> | Discourse Facet:  NA | Annotator: Automatic


