Citance Number: 1 | Reference Article:  J99-4005.txt | Citing Article:  P01-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The bias introduced by TMEMs is a practical alternative to finding optimal translations, which is NP-complete (Knight, 1999).</S> | Reference Offset:  ['81','109'] | Reference Text:  <S sid = 81 ssid = >Next we give separate polynomial-time reductions from two NP-complete problems.</S><S sid = 109 ssid = >In that case, finding a legal word ordering is like finding a complete circuit in a graph.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  J99-4005.txt | Citing Article:  P13-1001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, as phrase-based decoding usually casts translation as a string concatenation problem and permits arbitrary permutation, it proves to be NP-complete (Knight, 1999).</S> | Reference Offset:  ['5','104'] | Reference Text:  <S sid = 5 ssid = >We show that for the simplest form of statistical models, this problem is NP-complete, i.e., probably exponential in the length of the observed sentence.</S><S sid = 104 ssid = >We note that Brew (1992) discusses the NPcompleteness of a related problem, that of finding some permutation of a string that is acceptable to a given context-free grammar.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  J99-4005.txt | Citing Article:  E06-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In (Knight,1999) it was proved that the Exact Decoding problem is NP-Hard when the language model is a bigram model.</S> | Reference Offset:  ['38','107'] | Reference Text:  <S sid = 38 ssid = >As with tagging, we can assume an alphabet of v source tokens, a bigram source model, a substitution channel model, and an m-token coded text.</S><S sid = 107 ssid = >If we assume that the channel model offers deterministic, word-for-word translations, then the bigram source model takes responsibility for ordering them.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  J99-4005.txt | Citing Article:  E06-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Note that our results for decoding are sharper than that of (Knight, 1999).</S> | Reference Offset:  ['104','105'] | Reference Text:  <S sid = 104 ssid = >We note that Brew (1992) discusses the NPcompleteness of a related problem, that of finding some permutation of a string that is acceptable to a given context-free grammar.</S><S sid = 105 ssid = >Both of these results deal with decision problems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  J99-4005.txt | Citing Article:  H05-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However allowing reordering in translation is computationally expensive and in some cases even provably NP-complete (Knight, 1999).</S> | Reference Offset:  ['81','148'] | Reference Text:  <S sid = 81 ssid = >Next we give separate polynomial-time reductions from two NP-complete problems.</S><S sid = 148 ssid = >Finally, expensive decoding also suggests expensive training from unannotated (monolingual) texts, which presents a challenging bottleneck for extending statistical machine translation to language pairs and domains where large bilingual corpora do not exist.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  J99-4005.txt | Citing Article:  D10-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In theory, this process can be reduced to the Traveling Salesman Problem and thus requires an exponential time algorithm (Knight, 1999).</S> | Reference Offset:  ['112','143'] | Reference Text:  <S sid = 112 ssid = >Salesman Problem.</S><S sid = 143 ssid = >To the extent that word ordering is like solving the Traveling Salesman Problem, it is encouraging substantial progress continues to be made on Traveling Salesman algorithms.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  J99-4005.txt | Citing Article:  P07-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Part of the complexity arises from the expressive power of the translation model: for example, a phrase or word-based model with full reordering has exponential complexity (Knight, 1999).</S> | Reference Offset:  ['0','136'] | Reference Text:  <S sid = 0 ssid = >Decoding Complexity In Word-Replacement Translation Models</S><S sid = 136 ssid = >The former is more closely tied to the source model, and the latter to the channel model, though the complexity arises from the interaction of the two.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  J99-4005.txt | Citing Article:  P03-2041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This approach offers four features absent from IBM-style models: (1) a recursive phrase-based translation, (2) a syntax-based language model, (3) the ability to condition a word's translation on the translation of syntactically related words, and (4) polynomial-time optimal alignment and decoding (Knight, 1999).</S> | Reference Offset:  ['0','137'] | Reference Text:  <S sid = 0 ssid = >Decoding Complexity In Word-Replacement Translation Models</S><S sid = 137 ssid = >We should note that Model 1 is an intentionally simple translation model, one whose primary purpose in machine translation has been to allow bootstrapping into more complex translation models (e.g., IBM Models 2-5).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  J99-4005.txt | Citing Article:  D09-1108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Both of the two steps are very time-consuming due to the exponential number of translation rules and the complex nature of machine translation as an NP-hard search problem (Knight, 1999).</S> | Reference Offset:  ['23','137'] | Reference Text:  <S sid = 23 ssid = >We review it here for purposes of comparison with machine translation.</S><S sid = 137 ssid = >We should note that Model 1 is an intentionally simple translation model, one whose primary purpose in machine translation has been to allow bootstrapping into more complex translation models (e.g., IBM Models 2-5).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  J99-4005.txt | Citing Article:  P05-1068.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >If arbitrary re-orderings are allowed, the search problem is NP-complete (Knight, 1999).</S> | Reference Offset:  ['5','11'] | Reference Text:  <S sid = 5 ssid = >We show that for the simplest form of statistical models, this problem is NP-complete, i.e., probably exponential in the length of the observed sentence.</S><S sid = 11 ssid = >We show that for the simplest form of statistical models, this problem is NP-complete, i.e., probably exponential in the length of the observed sentence.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  J99-4005.txt | Citing Article:  W06-3602.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Under certain restrictions, both algorithms handle MT-related problems efficiently that are generally NP complete (Knight, 1999).</S> | Reference Offset:  ['81','142'] | Reference Text:  <S sid = 81 ssid = >Next we give separate polynomial-time reductions from two NP-complete problems.</S><S sid = 142 ssid = >It is also possible to devise approximation algorithms like those devised for other NP-complete problems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  J99-4005.txt | Citing Article:  W06-3602.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the general case, no efficient search algorithm exists to search all word or phrase reorderings (Knight, 1999).</S> | Reference Offset:  ['102','146'] | Reference Text:  <S sid = 102 ssid = >So no Hamilton Circuit exists.</S><S sid = 146 ssid = >So far, statistical translation research has either opted for heuristic beam-search algorithms or different channel models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  J99-4005.txt | Citing Article:  W06-3602.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Knight, 1999) shows that the decoding problem for SMT as well as some bilingual tiling problems are NP-complete, so no efficient algorithm exists in the general case.</S> | Reference Offset:  ['81','102'] | Reference Text:  <S sid = 81 ssid = >Next we give separate polynomial-time reductions from two NP-complete problems.</S><S sid = 102 ssid = >So no Hamilton Circuit exists.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  J99-4005.txt | Citing Article:  H05-1101.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Investigation of the computational complexity of translation models has started in (Knight, 1999) for word-to-word models.</S> | Reference Offset:  ['0','137'] | Reference Text:  <S sid = 0 ssid = >Decoding Complexity In Word-Replacement Translation Models</S><S sid = 137 ssid = >We should note that Model 1 is an intentionally simple translation model, one whose primary purpose in machine translation has been to allow bootstrapping into more complex translation models (e.g., IBM Models 2-5).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  J99-4005.txt | Citing Article:  D08-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Knight (1999) has shown that even for a simple form of statistical MT models, the decoding problem is NP-complete.</S> | Reference Offset:  ['5','11'] | Reference Text:  <S sid = 5 ssid = >We show that for the simplest form of statistical models, this problem is NP-complete, i.e., probably exponential in the length of the observed sentence.</S><S sid = 11 ssid = >We show that for the simplest form of statistical models, this problem is NP-complete, i.e., probably exponential in the length of the observed sentence.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  J99-4005.txt | Citing Article:  P09-1038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >in particular (Knight, 1999) has shown that any TSP instance can be mapped to a sub-case of a word-based SMT model, demonstrating NP-hardness of the decoding task.</S> | Reference Offset:  ['84','140'] | Reference Text:  <S sid = 84 ssid = >We transform any Hamilton Circuit instance into an M1-DECIDE instance as follows.</S><S sid = 140 ssid = >The proofs we presented are based on a worst-case analysis.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  J99-4005.txt | Citing Article:  P09-1038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As already mentioned, the similarity between SMT decoding and TSP was recognized in (Knight, 1999), who focussed on showing that any TSP can be reformulated as a sub-class of the SMT decoding problem, proving that SMT decoding is NP-hard.</S> | Reference Offset:  ['0','21'] | Reference Text:  <S sid = 0 ssid = >Decoding Complexity In Word-Replacement Translation Models</S><S sid = 21 ssid = >This paper looks at decoding complexity.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  J99-4005.txt | Citing Article:  N03-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Knight (1999) has shown the problem to be NP-complete.</S> | Reference Offset:  ['5','11'] | Reference Text:  <S sid = 5 ssid = >We show that for the simplest form of statistical models, this problem is NP-complete, i.e., probably exponential in the length of the observed sentence.</S><S sid = 11 ssid = >We show that for the simplest form of statistical models, this problem is NP-complete, i.e., probably exponential in the length of the observed sentence.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  J99-4005.txt | Citing Article:  N03-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['54','148'] | Reference Text:  <S sid = 54 ssid = >In the main channel model of Brown et al. (1993), each English word token e, in a source sentence is assigned a &quot;fertility&quot; 0â€ž which dictates how many French words it will produce.</S><S sid = 148 ssid = >Finally, expensive decoding also suggests expensive training from unannotated (monolingual) texts, which presents a challenging bottleneck for extending statistical machine translation to language pairs and domains where large bilingual corpora do not exist.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  J99-4005.txt | Citing Article:  N10-1016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It has been known that phrase-based decoding should be constrained to some extent not only for transferring the NP-hard problem (Knight,1999) into a tractable one in practice but also for improving translation quality.</S> | Reference Offset:  ['0','111'] | Reference Text:  <S sid = 0 ssid = >Decoding Complexity In Word-Replacement Translation Models</S><S sid = 111 ssid = >If word pairs have probabilities attached to them, then word ordering resembles the finding the least-cost circuit, also known as the Traveling Salesman Problem.</S> | Discourse Facet:  NA | Annotator: Automatic


