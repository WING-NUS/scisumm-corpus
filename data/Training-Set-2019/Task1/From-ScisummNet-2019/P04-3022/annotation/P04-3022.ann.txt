Citance Number: 1 | Reference Article:  P04-3022.txt | Citing Article:  P07-2040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>They use two kinds of features: syntactic ones and word based ones, for example, the path of the given pair of NEs in the parse tree and the word n-gram between NEs (Kambhatla, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Here is an example.</S><S sid = NA ssid = NA>Dependency (word on which is depedent), (POS of word on which is dependent), (chunk label of word on which is dependent), Parse Tree PERSON-NP-PP-ORGANIZATION, PERSON-NP-PP:of-ORGANIZATION (both derived from the path shown in bold in Figure 1).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P04-3022.txt | Citing Article:  P07-2040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Supervised learning method using syntactic and word-based features, the path of the pairs of NEs in the parse tree and the word n gram between pairs of NEs (Kambhatla, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Dependency (word on which is depedent), (POS of word on which is dependent), (chunk label of word on which is dependent), Parse Tree PERSON-NP-PP-ORGANIZATION, PERSON-NP-PP:of-ORGANIZATION (both derived from the path shown in bold in Figure 1).</S><S sid = NA ssid = NA>All the syntactic features are derived from the syntactic parse tree and the dependency tree that we compute using a statistical parser trained on the PennTree Bank using the Maximum Entropy framework (Ratnaparkhi, 1999).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P04-3022.txt | Citing Article:  W06-1659.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The approaches proposed to the ACE RDC task such as kernel methods (Zelenko et al, 2002) and Maximum Entropy methods (Kambhatla, 2004) required the availability of large set of human annotated corpora which are tagged with relation instances.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>More recently, (Zelenko et al., 2003) have proposed extracting relations by computing kernel functions between parse trees and (Culotta and Sorensen, 2004) have extended this work to estimate kernel functions between augmented dependency trees.</S><S sid = NA ssid = NA>The reader is referred to (Florian et al., 2004; Ittycheriah et al., 2003; Luo et al., 2004) for more details of our mention detection and mention chaining modules.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P04-3022.txt | Citing Article:  W06-1659.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We compare our results to a state-of-the-art supervised system similar to the system described in (Kambhatla, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We built several models to compare the relative utility of the feature streams described in the previous section.</S><S sid = NA ssid = NA>We trained Maximum Entropy models using features derived from the feature streams described above.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P04-3022.txt | Citing Article:  I08-2119.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Kambhatla (2004) took a similar approach but used multivariate logistic regression (Kambhatla, 2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Here we present our general approach and describe our ACE results.</S><S sid = NA ssid = NA>Our approach can easily scale to include more features from a multitude of sources–e.g.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P04-3022.txt | Citing Article:  P06-1104.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We thank Salim Roukos for several invaluable suggestions and the entire ACE team at IBM for help with various components, feature suggestions and guidance.</S><S sid = NA ssid = NA>Here is an example: The American Medical Association voted yesterday to install the heir apparent as its president-elect, rejecting a strong, upstart challenge by a District doctor who argued that the nation’s largest physicians’ group needs stronger ethics and new leadership.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P04-3022.txt | Citing Article:  W10-0913.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Kambhatla (2004) developed a method for extracting relations by applying Maximum Entropy models to combine lexical, syntactic and semantic features and report that they obtain improvement in results when they combine variety of features.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We build Maximum Entropy models for extracting relations that combine diverse lexical, syntactic and semantic features.</S><S sid = NA ssid = NA>We employ Maximum Entropy models to combine diverse lexical, syntactic and semantic features derived from the text.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P04-3022.txt | Citing Article:  P06-1017.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We thank Salim Roukos for several invaluable suggestions and the entire ACE team at IBM for help with various components, feature suggestions and guidance.</S><S sid = NA ssid = NA>Here is an example: The American Medical Association voted yesterday to install the heir apparent as its president-elect, rejecting a strong, upstart challenge by a District doctor who argued that the nation’s largest physicians’ group needs stronger ethics and new leadership.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P04-3022.txt | Citing Article:  P06-2060.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Similar to our earlier work (Kambhatla, 2004), we used a combination of lexical, syntactic, and semantic features including all the words in between the two mentions, the entity types and subtypes of the two mentions, the number of words in between the two mentions, features derived from the smallest parse fragment connecting the two mentions, etc.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The feature streams are: Words The words of both the mentions and all the words in between.</S><S sid = NA ssid = NA>Words , , , .</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P04-3022.txt | Citing Article:  D07-1076.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For the feature-based methods, Kambhatla (2004) employed Maximum Entropy models to combine diverse lexical, syntactic and semantic features in relation extraction, and achieved the F-measure of 52.8 on the 24 relation subtypes in the ACE RDC 2003 corpus.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We build Maximum Entropy models for extracting relations that combine diverse lexical, syntactic and semantic features.</S><S sid = NA ssid = NA>We employ Maximum Entropy models to combine diverse lexical, syntactic and semantic features derived from the text.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P04-3022.txt | Citing Article:  D07-1076.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Another problem is that, although they can explore some structured information in the parse tree (e.g. Kambhatla (2004) used the non-terminal path connecting the given two entities in a parse tree while Zhou et al (2005) introduced additional chunking features to enhance the performance), it is found difficult to well preserve structured information in the parse trees using the feature-based methods.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Parse Tree The path of non-terminals (removing duplicates) connecting the two mentions in the parse tree, and the path annotated with head words.</S><S sid = NA ssid = NA>For the Template Relations task of MUC-7, BBN researchers (Miller et al., 2000) augmented syntactic parse trees with semantic information corresponding to entities and relations and built generative models for the augmented trees.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P04-3022.txt | Citing Article:  D07-1076.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We thank Salim Roukos for several invaluable suggestions and the entire ACE team at IBM for help with various components, feature suggestions and guidance.</S><S sid = NA ssid = NA>Here is an example: The American Medical Association voted yesterday to install the heir apparent as its president-elect, rejecting a strong, upstart challenge by a District doctor who argued that the nation’s largest physicians’ group needs stronger ethics and new leadership.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P04-3022.txt | Citing Article:  P05-1053.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Dependency The words and part-of-speech and chunk labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic parse tree.</S><S sid = NA ssid = NA>All the syntactic features are derived from the syntactic parse tree and the dependency tree that we compute using a statistical parser trained on the PennTree Bank using the Maximum Entropy framework (Ratnaparkhi, 1999).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P04-3022.txt | Citing Article:  P05-1053.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Overlap The number of words (if any) separating the two mentions, the number of other mentions in between, flags indicating whether the two mentions are in the same noun phrase, verb phrase or prepositional phrase.</S><S sid = NA ssid = NA>Combining Lexical Syntactic And Semantic Features With Maximum Entropy Models For Information Extraction</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P04-3022.txt | Citing Article:  P05-1053.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Evaluation on the ACE corpus shows that our system outperforms Kambhatla (2004) by about 3 F-measure on extracting 24 ACE relation subtypes.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We report both the F-measure' and the ACE value of relation extraction.</S><S sid = NA ssid = NA>Automatic Content Extraction (ACE, 2004) is an evaluation conducted by NIST to measure Entity Detection and Tracking (EDT) and relation detection and characterization (RDC).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P04-3022.txt | Citing Article:  I08-1057.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>(Culotta and Sorensen, 2004) extended this work to estimate kernel functions between augmented dependency trees, while (Kambhatla, 2004) combined lexical features, syntactic features, and semantic features in a maximum entropy model.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>More recently, (Zelenko et al., 2003) have proposed extracting relations by computing kernel functions between parse trees and (Culotta and Sorensen, 2004) have extended this work to estimate kernel functions between augmented dependency trees.</S><S sid = NA ssid = NA>Combining Lexical Syntactic And Semantic Features With Maximum Entropy Models For Information Extraction</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P04-3022.txt | Citing Article:  I08-1057.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>However, the semantic features discussed in (Kambhatla, 2004) still focus on the word level instead of the conceptual level.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Mention Level The mention level (one of NAME, NOMINAL, PRONOUN) of both the mentions.</S><S sid = NA ssid = NA>Combining Lexical Syntactic And Semantic Features With Maximum Entropy Models For Information Extraction</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P04-3022.txt | Citing Article:  N06-1037.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Kambhatla (2004) employs Maximum Entropy models to combine diverse lexical, syntactic and semantic features derived from the text for relation extraction.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We employ Maximum Entropy models to combine diverse lexical, syntactic and semantic features derived from the text.</S><S sid = NA ssid = NA>We build Maximum Entropy models for extracting relations that combine diverse lexical, syntactic and semantic features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P04-3022.txt | Citing Article:  N06-1037.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We thank Salim Roukos for several invaluable suggestions and the entire ACE team at IBM for help with various components, feature suggestions and guidance.</S><S sid = NA ssid = NA>Here is an example: The American Medical Association voted yesterday to install the heir apparent as its president-elect, rejecting a strong, upstart challenge by a District doctor who argued that the nation’s largest physicians’ group needs stronger ethics and new leadership.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P04-3022.txt | Citing Article:  N06-1037.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Kambhatla (2004) use the path of non-terminals connecting two mentions in a parse tree as the parse tree features.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Parse Tree The path of non-terminals (removing duplicates) connecting the two mentions in the parse tree, and the path annotated with head words.</S><S sid = NA ssid = NA>Adding the parse tree and dependency tree based features gives us our best result by exploiting the consistent syntactic patterns exhibited between mentions for some relations.</S> | Discourse Facet:  NA | Annotator: Automatic


