Unsupervised grammar induction models tend to employ relatively simple models of syntax when compared to their supervised counterparts.
Traditionally, the unsupervised models have been kept simple due to tractability and data sparsity concerns.
In this paper, we introduce basic valence frames and lexical information into an unsupervised dependency grammar inducer and show how this additional information can be leveraged via smoothing.
Our model produces state-of-the-art results on the task of unsupervised grammar induction, improving over the best previous work by almost 10 percentage points.
