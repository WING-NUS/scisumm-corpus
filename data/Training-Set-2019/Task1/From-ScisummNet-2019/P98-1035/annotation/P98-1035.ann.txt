Citance Number: 1 | Reference Article:  P98-1035.txt | Citing Article:  P00-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>But (Chelba and Jelinek, 1998) chooses the lexical heads of the two previous constituents as determined by a shift-reduce parser, and works better than a trigram model.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The probability associated with each model action is determined as described in section 3.1, based on counts C(m)(y(m), x(m)), one set for each model component.</S><S sid = NA ssid = NA>Our solution is inspired by an HMM re-estimation technique that works on pruned — N-best — trellises(Byrne et al., 1998).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P98-1035.txt | Citing Article:  I05-5012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This marraige of models has been tested in other fields such as speech recognition (Chelba and Jelinek, 1998) with success.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The probabilistic model, its parameterization and a few experiments that are meant to evaluate its potential for speech recognition are presented.</S><S sid = NA ssid = NA>The model assigns probability to every joint sequence of words-binary-parse-structure with headword annotation and operates in a left-to-right manner — therefore usable for automatic speech recognition.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P98-1035.txt | Citing Article:  N01-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>(Chelba and Jelinek, 1998) combine unlabeled and labeled data for parsing with a view towards language modeling applications.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Exploiting Syntactic Structure for Language Modeling</S><S sid = NA ssid = NA>This is the main difference between our approach and other approaches to statistical natural language parsing.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P98-1035.txt | Citing Article:  N01-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Aware that the n-gram obscures many linguistically-signi cant distinctions (Chomsky, 1956, section 2.3), many speech researchers (Jelinek and Laerty, 1991) sought to incorporate hierarchical phrase structure into language modeling (see (Stolcke, 1997)) although it was not until the late 1990s that such models were able to signi cantly improve on 3-grams (Chelba and Jelinek, 1998).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Exploiting Syntactic Structure for Language Modeling</S><S sid = NA ssid = NA>The headword of a phrase is the word that best represents the phrase, all the other words in the phrase being modifiers of the headword.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P98-1035.txt | Citing Article:  W00-1301.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The language model described by Chelba and Jelinek (1998) similarly conditions on linguistically relevant words by assigning partial phrase structure to the history and percolating headwords.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The linguistically correct partial parse of the word history when predicting after is shown in Figure 1.</S><S sid = NA ssid = NA>The headword of a phrase is the word that best represents the phrase, all the other words in the phrase being modifiers of the headword.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P98-1035.txt | Citing Article:  C02-1157.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In this situation, a structured language model (SLM) was proposed by Chelba and Jelinek (1998).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The model we present is closely related to the one investigated in (Chelba et al., 1997), however different in a few important aspects: • our model operates in a left-to-right manner, allowing the decoding of word lattices, as opposed to the one referred to previously, where only whole sentences could be processed, thus reducing its applicability to n-best list re-scoring; the syntactic structure is developed as a model component; • our model is a factored version of the one in (Chelba et al., 1997), thus enabling the calculation of the joint probability of words and parse structure; this was not possible in the previous case due to the huge computational complexity of the model.</S><S sid = NA ssid = NA>Exploiting Syntactic Structure for Language Modeling</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P98-1035.txt | Citing Article:  N06-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Also thanks to Eric Brill, Sanjeev Khudanpur, David Yarowsky, Radu Florian, Lidia Mangu and Jun Wu for useful input during the meetings of the people working on our STIMULATE grant.</S><S sid = NA ssid = NA>The workings of the parser module are similar to those of Spatter (Jelinek et al., 1994).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P98-1035.txt | Citing Article:  P11-1063.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Chelba and Jelinek (1998) proposed that syntactic structure could be used as an alternative technique in language modeling.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Exploiting Syntactic Structure for Language Modeling</S><S sid = NA ssid = NA>The main goal of the present work is to develop a language model that uses syntactic structure to model long-distance dependencies.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P98-1035.txt | Citing Article:  W07-0735.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>As an alternative option to our verb-modifier experiments, structured language models (Chelba and Jelinek, 1998) might be considered to improve clause coherence, until full-featured syntax-based MT models (Yamada and Knight (2002), Eisner (2003), Chiang (2005) among many others) aretested when translating to morphologically rich languages.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We believe that the above experiments show the potential of our approach for improved language models.</S><S sid = NA ssid = NA>The results we obtained are presented in the experiments section.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P98-1035.txt | Citing Article:  C04-1167.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In the first type, probability of a word is decided based on a parse-tree information like grammatical headwords in a sentence (Charniak, 2001) (Chelba and Jelinek, 1998), or based on part of-speech (POS) tag information (Galescu and Ringger, 1999).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Let an elementary event in the derivation(W,T) = (word-to-tag, ho.tag , h_i.tag).</S><S sid = NA ssid = NA>The probability associated with each model action is determined as described in section 3.1, based on counts C(m)(y(m), x(m)), one set for each model component.</S> | Discourse Facet:  NA | Annotator: Automatic


