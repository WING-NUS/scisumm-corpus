Citance Number: 1 | Reference Article:  W09-1119.txt | Citing Article:  N12-2011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The system uses the Illinois Entity Tagger (Ratinov and Roth, 2009) and Orthomatcher from the GATE framework for within a document co-reference resolution.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We have presented a simple model for NER that uses expressive features to achieve new state of the art performance on the Named Entity recognition task.</S><S sid = NA ssid = NA>Our baseline NER system uses a regularized averaged perceptron (Freund and Schapire, 1999).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W09-1119.txt | Citing Article:  W11-0208.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We use the IOBES notation (Ratinov and Roth, 2009) to represent NE mentions with label sequences, thereby NER is formalized as a multi class classification problem in which a given token is classified into IOBES labels.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>These include: what model to use for sequential inference, how to represent text chunks and what inference (decoding) algorithm to use.</S><S sid = NA ssid = NA>When making the prediction for token instance xi, we record the label assignment distribution for all token instances for the same token type in the previous 1000 words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W09-1119.txt | Citing Article:  P13-2029.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Finally both the training and test data were sentence-segmented and word-tokenized by NLTK (Bird and Loper, 2004), dependency parsed by the Stanford Parser (Klein and Manning, 2003), and NER-tagged by the Illinois Named Entity Tagger (Ratinov and Roth, 2009) with an 18-label type set.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The named entities mentioned in the test dataset are considerably different from those that appear in the training or the development set.</S><S sid = NA ssid = NA>Next, we have compared the performance of our system to that of the Stanford NER tagger, across the datasets discussed above.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W09-1119.txt | Citing Article:  C10-2145.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Cited authors of each paper are extracted from the reference section and automatically identified by a named entity recognizer tuned for citation extraction (Ratinov and Roth, 2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>That is, if a named entity token was identified as such, we counted it as a correct prediction ignoring the named entity type.</S><S sid = NA ssid = NA>In this paper we investigate one such application– Named Entity Recognition (NER).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W09-1119.txt | Citing Article:  W11-0902.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Ratinov and Roth (2009) also investigate design challenges for named entity recognition, and showed that other design choices, such as the representation of output labels and using features built on external knowledge, are more important than the learning model itself.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Design Challenges and Misconceptions in Named Entity Recognition</S><S sid = NA ssid = NA>We explored four fundamental design decisions: text chunks representation, inference algorithm, using non-local features and external knowledge.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W09-1119.txt | Citing Article:  C10-1018.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In this work, we use the Brown clustering algorithm (Brown et al, 1992), which has been shown to improve performance in various NLP applications such as dependency parsing (Koo et al., 2008), named entity recognition (Ratinov and Roth, 2009), and relation extraction (Boschee et al., 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this work, we analyze a simple technique of using word clusters generated from unlabeled text, which has been shown to improve performance of dependency parsing (Koo et al., 2008), Chinese word segmentation (Liang, 2005) and NER (Miller et al., 2004).</S><S sid = NA ssid = NA>The technique is based on word class models, pioneered by (Brown et al., 1992), which hierarchically The approach is related, but not identical, to distributional similarity (for details, see (Brown et al., 1992) and (Liang, 2005)).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W09-1119.txt | Citing Article:  C10-1127.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Ratinov and Roth (2009) have shown for the CoNLL-2003 shared task that Greedy decoding (i.e., beam search of width 1) is competitive to the widely used Viterbi algorithm while being over 100 times faster at the same time.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>First, due to the second-order of the model, the greedy decoding is over 100 times faster than Viterbi.</S><S sid = NA ssid = NA>Table 1 compares between the greedy decoding, beamsearch with varying beam size, and Viterbi, both for the system with baseline features and for the end system (to be presented later).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W09-1119.txt | Citing Article:  P12-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Such mention type information as shown on the left of Figure 1 can be obtained from various sources such as dictionaries, gazetteers, rule-based systems (Stro ?tgen and Gertz, 2010), statistically trained classifiers (Ratinov and Roth, 2009), or some web resources such as Wikipedia (Ratinov et al, 2011). However, in practice, outputs from existing mention identification and typing systems can be far from ideal.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Systems based on perceptron have been shown to be competitive in NER and text chunking (Kazama and Torisawa, 2007b; Punyakanok and Roth, 2001; Carreras et al., 2003) We specify the model and the features with the LBJ (Rizzolo and Roth, 2007) modeling language.</S><S sid = NA ssid = NA>Most NER systems use additional features, such as POS tags, shallow parsing information and gazetteers.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W09-1119.txt | Citing Article:  P11-2112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Extracts 20,176 titles and 15,182 redirects.</S><S sid = NA ssid = NA>Systems based on perceptron have been shown to be competitive in NER and text chunking (Kazama and Torisawa, 2007b; Punyakanok and Roth, 2001; Carreras et al., 2003) We specify the model and the features with the LBJ (Rizzolo and Roth, 2007) modeling language.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W09-1119.txt | Citing Article:  P11-1056.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>NE tags We automatically annotate the sentences with named entity (NE) tags using the named entity tagger of (Ratinov and Roth, 2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>That is, if a named entity token was identified as such, we counted it as a correct prediction ignoring the named entity type.</S><S sid = NA ssid = NA>Design Challenges and Misconceptions in Named Entity Recognition</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W09-1119.txt | Citing Article:  P11-1056.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>From a sentence, we gather the following as candidate mentions: all nouns and possessive pronouns, all named entities annotated by the NE tagger (Ratinov and Roth, 2009), all base noun phrase (NP) chunks, all chunks satisfying the pattern: NP (PP NP)+, all NP constituents in the syntactic parse tree, and from each of these constituents, all substrings consisting of two or more words, provided the sub strings do not start nor end on punctuation marks. These mention candidates are then fed to our mention entity typing (MET) classifier for type prediction (more details in Section 6.3).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This is due to the fact that when a named entity is introduced for the first time in text, a canonical name is used, while in the following discussion abbreviated mentions, pronouns, and other references are used.</S><S sid = NA ssid = NA>That is, if a named entity token was identified as such, we counted it as a correct prediction ignoring the named entity type.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W09-1119.txt | Citing Article:  D12-1082.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For NER we used the Illinois Named Entity Tagger (Ratinov and Roth, 2009) on the highest setting (that achieved 90.5 F1 score on the CoNLL03 shared task).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Combining recent advances, we develop a publicly available NER system that achieves 90.8 F1 score on the CoNLL-2003 NER shared task, the best reported result for this dataset.</S><S sid = NA ssid = NA>Indeed, the baseline for the CoNLL03 shared task was essentially a dictionary lookup of the entities which appeared in the training data, and it achieves 71.91 F1 score on the test set (Tjong and De Meulder, 2003).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W09-1119.txt | Citing Article:  D11-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>used the state-of-the-art named entity tagger of Ratinov and Roth (2009) to label the text.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We have presented a simple model for NER that uses expressive features to achieve new state of the art performance on the Named Entity recognition task.</S><S sid = NA ssid = NA>Our system significantly outperforms the current state of the art and is available to download under a research license.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W09-1119.txt | Citing Article:  D11-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We also used the CBC word clusters of Pantel and Lin (2002) as additional gazetteers and Brown cluster features as used by Ratinov and Roth (2009) and Koo et al.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this work, we analyze a simple technique of using word clusters generated from unlabeled text, which has been shown to improve performance of dependency parsing (Koo et al., 2008), Chinese word segmentation (Liang, 2005) and NER (Miller et al., 2004).</S><S sid = NA ssid = NA>When Brown clusters are used as features in the following sections, it implies that all features in the system which contain a word form will be duplicated and a new set of features containing the paths of varying length will be introduced.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W09-1119.txt | Citing Article:  W11-0810.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The LBJ Tagger is based on a regularized average perceptron (Ratinov and Roth, 2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Systems based on perceptron have been shown to be competitive in NER and text chunking (Kazama and Torisawa, 2007b; Punyakanok and Roth, 2001; Carreras et al., 2003) We specify the model and the features with the LBJ (Rizzolo and Roth, 2007) modeling language.</S><S sid = NA ssid = NA>Our baseline NER system uses a regularized averaged perceptron (Freund and Schapire, 1999).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W09-1119.txt | Citing Article:  P12-3003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For example, the average F1 of the Stanford NER (Finkel et al, 2005) drops from 90.8% (Ratinov and Roth, 2009) to 45.8% on tweets, while Liuetal.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The results we obtained on the CoNLL03 test set were consistent with what was reported in (Finkel et al., 2005).</S><S sid = NA ssid = NA>In this work, we analyze a simple technique of using word clusters generated from unlabeled text, which has been shown to improve performance of dependency parsing (Koo et al., 2008), Chinese word segmentation (Liang, 2005) and NER (Miller et al., 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W09-1119.txt | Citing Article:  D10-1075.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our gazetteers comes from (Ratinov and Roth, 2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In the rest of this section, we discuss the construction of gazetteers from Wikipedia.</S><S sid = NA ssid = NA>In this section, we discuss two important knowledge resources– gazetteers and unlabeled text.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W09-1119.txt | Citing Article:  D10-1075.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>As an unlabeled adaptation method to address feature sparsity, we add cluster-like features based on the gazetteers and word clustering resources used in (Ratinov and Roth, 2009) to bridge the source and target domain.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this section, we discuss two important knowledge resources– gazetteers and unlabeled text.</S><S sid = NA ssid = NA>When Brown clusters are used as features in the following sections, it implies that all features in the system which contain a word form will be duplicated and a new set of features containing the paths of varying length will be introduced.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W09-1119.txt | Citing Article:  D10-1075.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For example, (Ratinov and Roth, 2009) only use the cluster-like features to address the feature sparsity problem, and (Finkel and Manning, 2009) only use target labeled data without using gazetteers and word-cluster information.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Most NER systems use additional features, such as POS tags, shallow parsing information and gazetteers.</S><S sid = NA ssid = NA>Since word class models use large amounts of unlabeled data, they are essentially a semi-supervised technique, which we use to considerably improve the performance of our system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W09-1119.txt | Citing Article:  D10-1075.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The work (Ratinov and Roth, 2009) also combines their system with several document-level features.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this work, we call this type of features context aggregation features.</S><S sid = NA ssid = NA>Therefore they apply a baseline NER system, and use the resulting predictions as features in a second level of inference.</S> | Discourse Facet:  NA | Annotator: Automatic


