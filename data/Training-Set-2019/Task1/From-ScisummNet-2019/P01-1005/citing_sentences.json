[
  {
    "citance_No": 1, 
    "citing_paper_id": "P02-1030", 
    "citing_paper_authority": 17, 
    "citing_paper_authors": "James R., Curran | Marc, Moens", 
    "raw_text": "A crucial part of training these systems lies in extracting from the data high-quality contextual information, in the sense of defining contexts that are both accurate and correlated with the information (the POS tags, the word senses, the chunks) the system is trying to extract. The quality of contextual information is often determined by the size of the training corpus: with less data available, extracting context information for any given phenomenon becomes less reliable. However, corpus size is no longer a limiting factor: whereas up to now people have typically worked with corpora of around one million words, it has become feasible to build much larger document collections; for example, Banko and Brill (2001) report on experiments with a one billion word corpus", 
    "clean_text": "However, corpus size is no longer a limiting factor: whereas up to now people have typically worked with corpora of around one million words, it has become feasible to build much larger document collections; for example, Banko and Brill (2001) report on experiments with a one billion word corpus.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P02-1030", 
    "citing_paper_authority": 17, 
    "citing_paper_authors": "James R., Curran | Marc, Moens", 
    "raw_text": "Banko and Brill, (2001) also found this trend for the task of confusion set disambiguation on corpora of up to one billion words", 
    "clean_text": "Banko and Brill, (2001) also found this trend for the task of confusion set disambiguation on corpora of up to one billion words.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "N10-1007", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Taniya, Mishra | Srinivas, Bangalore", 
    "raw_text": "We also included spatial indexicals such as here, and other sub strings such as cost of and how much is. A question is considered static if it does not contain any such words/phrases. Self-training with bagging: The general self training with bagging algorithm (Banko and Brill,2001) is presented in Table 6 and illustrated in Figure 7 (a)", 
    "clean_text": "Self-training with bagging: The general self training with bagging algorithm (Banko and Brill, 2001) is presented in Table 6 and illustrated in Figure 7 (a).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "N10-1007", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Taniya, Mishra | Srinivas, Bangalore", 
    "raw_text": "Figure 7 (b) illustrates the algorithm and Figure 8describes the algorithm, also known as committee based active-learning (Banko and Brill, 2001)", 
    "clean_text": "Figure 7 (b) illustrates the algorithm and Figure 8 describes the algorithm, also known as committee based active-learning (Banko and Brill, 2001).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "W02-0814", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "V&eacute;ronique, Hoste | Walter, Daelemans | Iris, Hendrickx | Antal, van den Bosch", 
    "raw_text": "So the creation of more annotated data is necesssary and will certainly cause major improvements of current WSD systems and NLP systems in general (see also (Banko and Brill, 2001))", 
    "clean_text": "So the creation of more annotated data is necesssary and will certainly cause major improvements of current WSD systems and NLP systems in general (see also (Banko and Brill, 2001)).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "W02-2008", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "James R., Curran | Miles, Osborne", 
    "raw_text": "Banko and Brill (2001) suggested that the development of very large training corpora may be more effective for progress in empirical Natural LanguageProcessing than improving methods that use existing smaller training corpora", 
    "clean_text": "Banko and Brill (2001) suggested that the development of very large training corpora may be more effective for progress in empirical Natural Language Processing than improving methods that use existing smaller training corpora.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "W02-2008", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "James R., Curran | Miles, Osborne", 
    "raw_text": "Banko and Brill (2001) report on confusion set disambiguation experiments where they apply relatively simple learning methods to a one billion word training corpus", 
    "clean_text": "Banko and Brill (2001) report on confusion set disambiguation experiments where they apply relatively simple learning methods to a one billion word training corpus.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "W02-2008", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "James R., Curran | Miles, Osborne", 
    "raw_text": "Recent work has replicated the Banko and Brill (2001) results on the much more complex task of automatic thesaurus extraction, showing that con textual statistics, collected over a very large corpus, significantly improve system performance (Curran and Moens, 2002)", 
    "clean_text": "Recent work has replicated the Banko and Brill (2001) results on the much more complex task of automatic thesaurus extraction, showing that contextual statistics, collected over a very large corpus, significantly improve system performance (Curran and Moens, 2002).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "W03-1015", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Vincent, Ng | Claire, Cardie", 
    "raw_text": "Like anaphora resolution, noun phrase co reference resolution is a problem for which a natural feature split is not readily available. In related work (Ng and Cardie, 2003), we compare the performance of the Blum and Mitchell co training algorithm with that of two existing single view bootstrapping algorithms? self-training with bagging (Banko and Brill, 2001) and EM (Nigam et al., 2000)? on co reference resolution, and show that single-view weakly supervised learners are a viable alternative to co-training for the task", 
    "clean_text": "In related work (Ng and Cardie, 2003), we compare the performance of the Blum and Mitchell co training algorithm with that of two existing single view bootstrapping algorithms - self-training with bagging (Banko and Brill, 2001) and EM (Nigam et al., 2000) - on coreference resolution, and show that single-view weakly supervised learners are a viable alternative to co-training for the task.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "C10-1151", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Muhua, Zhu | Jingbo, Zhu | Tong, Xiao", 
    "raw_text": "Typically, increase in the scale of training data boosts the performance of machine learning methods, which in turn enhances the quality of learning-based NLP systems (Banko and Brill, 2001)", 
    "clean_text": "Typically, increase in the scale of training data boosts the performance of machine learning methods, which in turn enhances the quality of learning-based NLP systems (Banko and Brill, 2001).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "W10-1005", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Michael, Gamon | Claudia, Leacock", 
    "raw_text": "It has been well-established that statistical models improve as the size of the training data increases (Banko and Brill 2001a, 2001b)", 
    "clean_text": "It has been well-established that statistical models improve as the size of the training data increases (Banko and Brill 2001a, 2001b).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "N04-1016", 
    "citing_paper_authority": 24, 
    "citing_paper_authors": "Mirella, Lapata | Frank, Keller", 
    "raw_text": "We start by using web counts for two generation tasks for which theuse of large data sets has shown promising results: (a) tar get language candidate selection for machine translation (Grefenstette, 1998) and (b) context sensitive spelling correction (Banko and Brill, 2001a, b)", 
    "clean_text": "We start by using web counts for two generation tasks for which the use of large data sets has shown promising results: (a) target language candidate selection for machine translation (Grefenstette, 1998) and (b) context sensitive spelling correction (Banko and Brill, 2001a, b).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "P10-4011", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Taniya, Mishra | Srinivas, Bangalore", 
    "raw_text": "We also included spatial indexicals such as here, and other substrings such as cost of and how much is. A question is considered static if it does not contain any such words/phrases. Self-training with bagging: The general self training with bagging algorithm (Banko and Brill, 2001)", 
    "clean_text": "Self-training with bagging: The general self training with bagging algorithm (Banko and Brill, 2001).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "W03-0301", 
    "citing_paper_authority": 49, 
    "citing_paper_authors": "Rada, Mihalcea | Ted, Pedersen", 
    "raw_text": "As expected, word alignment, like many other NLP tasks (Banko and Brill, 2001), highly benefits from large amounts of training data", 
    "clean_text": "As expected, word alignment, like many other NLP tasks (Banko and Brill, 2001), highly benefits from large amounts of training data.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "W10-0712", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Nolan, Lawson | Kevin, Eustice | Mike, Perkowitz | Meliha, Yetisgen-Yildiz", 
    "raw_text": "relevant as the size of the corpus increases (Banko and Brill 2001)", 
    "clean_text": "Additionally, for certain applications in natural language processing (NLP), it has been noted that the particular algorithms or feature sets used tend to become irrelevant as the size of the corpus increases (Banko and Brill 2001).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "H05-1105", 
    "citing_paper_authority": 19, 
    "citing_paper_authors": "Preslav, Nakov | Marti A., Hearst", 
    "raw_text": "Past approaches have dealt with the data sparseness problem by attempting to generalize from semantic classes, either manually built or automatically derived. More recently, Banko and Brill (2001) have advocated for the creative use of very large text collections as an alternative to sophisticated algorithms and hand-built resources", 
    "clean_text": "More recently, Banko and Brill (2001) have advocated for the creative use of very large text collections as an alternative to sophisticated algorithms and hand-built resources.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "H05-1105", 
    "citing_paper_authority": 19, 
    "citing_paper_authors": "Preslav, Nakov | Marti A., Hearst", 
    "raw_text": "Banko and Brill (2001) show that even using a very simple algorithm, the results continue to improve log-linearly with more training data, even out to a billion words", 
    "clean_text": "Banko and Brill (2001) show that even using a very simple algorithm, the results continue to improve log-linearly with more training data, even out to a billion words.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "H05-1105", 
    "citing_paper_authority": 19, 
    "citing_paper_authors": "Preslav, Nakov | Marti A., Hearst", 
    "raw_text": "The trick is to figure out how to use information that is latent in the web as a corpus, and web search engines as query interfaces to that corpus. In this paper we describe two techniques ?sur face features and paraphrases? that push the ideas of Banko and Brill (2001) and Lapata and Keller (2004) farther, enabling the use of statistics gathered from very large corpora in an unsupervised manner", 
    "clean_text": "In this paper we describe two techniques - surface features and paraphrases - that push the ideas of Banko and Brill (2001) and Lapata and Keller (2004) farther, enabling the use of statistics gathered from very large corpora in an unsupervised manner.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "C10-1100", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Emily, Pitler | Shane, Bergsma | Dekang, Lin | Kenneth Ward, Church", 
    "raw_text": "Google V2, which comes from the same snapshot of the web as Google V1, but has only unique sentences, does not show this drop. We regard the results in Figure 2 as a companion to Banko and Brill (2001)? s work on exponentially increasing the amount of labeled training data", 
    "clean_text": "We regard the results in Figure 2 as a companion to Banko and Brill (2001)'s work on exponentially increasing the amount of labeled training data.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "W10-0404", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Raphael, Mudge", 
    "raw_text": "As others have pointed out (Banko and Brill, 2001), with enough data the complex algorithms with their tricks cease to have an advantage over the simpler methods", 
    "clean_text": "As others have pointed out (Banko and Brill, 2001), with enough data the complex algorithms with their tricks cease to have an advantage over the simpler methods.", 
    "keep_for_gold": 0
  }
]