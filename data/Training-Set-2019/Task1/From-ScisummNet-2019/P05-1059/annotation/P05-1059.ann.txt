Citance Number: 1 | Reference Article:  P05-1059.txt | Citing Article:  I05-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Zhang and Gildea (2005) [6] show that lexicalized ITGs can further improve alignment accuracy.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We trained both the unlexicalized and the lexicalized ITGs on a parallel corpus of Chinese-English newswire text.</S><S sid = NA ssid = NA>Alignment results improve over unlexicalized ITG on short sentences for which full EM is feasible, but pruning seems to have a negative impact on longer sentences.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P05-1059.txt | Citing Article:  P12-1018.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In order to take into account competing hypotheses, we can use for our queue discipline not only the inside probability I (ak), but also the outside probability O (ak), the probability of generating all spans other than ak, as in A* search for CFGs (Klein and Manning, 2003), and tic-tac-toe pruning for word based ITGs (Zhang and Gildea, 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In including an estimate of the outside probability, our technique is related to A* methods for monolingual parsing (Klein and Manning, 2003), although our estimate is not guaranteed to be lower than complete outside probabity assigned by ITG.</S><S sid = NA ssid = NA>In the Model 1 estimate of the outside probability, source and target words can align using any combination of points from the four outside corners of the tic-tac-toe pattern.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P05-1059.txt | Citing Article:  W07-0711.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The classical approaches to word alignment are based on IBM models 1-5 (Brown et al, 1994) and the HMM based alignment model (Vogel et al, 1996) (Och and Ney, 2000a, 2000b), while recently discriminative approaches (Moore, 2006) and syntax-based approaches (Zhang and Gildea, 2005) for word alignment are also studied.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Zens and Ney (2003) found that the constraints of ITG were a better match to the decoding task than the heuristics used in the IBM decoder of Berger et al. (1996).</S><S sid = NA ssid = NA>λL1,L2 is the probability of any word alignment template for a pair of L1word source string and L2-word target string, which we model as a uniform distribution of word-forword alignment patterns after a Poisson distribution of target string’s possible lengths, following Brown et al. (1993).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P05-1059.txt | Citing Article:  P06-2122.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This work was partially supported by NSF ITR IIS-09325646 and NSF ITR IIS-0428020.</S><S sid = NA ssid = NA>Since we are only lexicalizing rather than bilexicalizing the rules, the non-head constituents need to be lexicalized using head generation rules so that the top-down generation process can proceed in all branches.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P05-1059.txt | Citing Article:  P06-2122.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Zhang and Gildea (2005) described a model in which the nonterminals are lexicalized by Englishand foreign language word pairs so that the inversions are dependent on lexical information on the left hand side of synchronous rules.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Most work on ITG has focused on the 2-normal form, which consists of unary production rules that are responsible for generating word pairs: and binary production rules in two forms that are responsible for generating syntactic subtree pairs: The rules with square brackets enclosing the right hand side expand the left hand side symbol into the two symbols on the right hand side in the same order in the two languages, whereas the rules with pointed brackets expand the left hand side symbol into the two right hand side symbols in reverse order in the two languages.</S><S sid = NA ssid = NA>The probabilities of the binary rules, which are conditioned on lexicalized nonterminals, however, need to be backed off to the probabilities of generalized rules in the following forms: where * stands for any lexical pair.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P05-1059.txt | Citing Article:  P06-2122.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We apply one of the pruning techniques used in Zhang and Gildea (2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this experiment, we didn’t apply the pruning techniques for the lexicalized ITG.</S><S sid = NA ssid = NA>In the second experiment, we enabled the pruning techniques for the LITG with the beam ratio for the tic-tac-toe pruning as 10−5 and the number k for the top-k pruning as 25.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P05-1059.txt | Citing Article:  P06-2122.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In the first comparison, we measured the performance of five word aligners, including IBM models, ITG, the lexical ITG (LITG) of Zhang and Gildea (2005), and our bi lexical ITG (BLITG), on a hand-aligned bilingual corpus.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The interaction between lexical information and word order also explains the higher performance of IBM model 4 over IBM model 3 for alignment.</S><S sid = NA ssid = NA>Zhang and Gildea (2004) found ITG to outperform the tree-to-string model for word-level alignment, as measured against human gold-standard alignments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P05-1059.txt | Citing Article:  W07-0403.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Past approaches have pruned spans using IBM Model 1 probability estimates (Zhang and Gildea, 2005) or using agreement with an existing parse tree (Cherry and Lin, 2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This can be also be thought of as squeezing together the four outside corners, creating a new cell whose probability is estimated using IBM Model 1.</S><S sid = NA ssid = NA>The performance of the full model of unlexicalized ITG is compared with the pruned model of lexicalized ITG using more training data and evaluation data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P05-1059.txt | Citing Article:  W06-1627.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Fortunately, exploiting the recursive nature of the cells, we can compute values for the inside and outside components of each cell using dynamic programming in O (n4) time (Zhang and Gildea, 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Fortunately, we can exploit the recursive nature of the cells.</S><S sid = NA ssid = NA>Figure 3(a) displays the tic-tac-toe pattern for the inside and outside components of a particular cell.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P05-1059.txt | Citing Article:  P10-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For example, Haghighi et al. (2009) do pruning based on the probabilities of links from a simpler alignment model (viz. HMM); Zhang and Gildea (2005) propose Tic-tac-toe pruning, which is based on the Model 1 probabilities of word pairs inside and outside a pair of spans.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We use IBM Model 1 as our estimate of both the inside and outside probabilities.</S><S sid = NA ssid = NA>In the second experiment, we enabled the pruning techniques for the LITG with the beam ratio for the tic-tac-toe pruning as 10−5 and the number k for the top-k pruning as 25.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P05-1059.txt | Citing Article:  C10-2084.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Zhang and Gildea (2005) show that Model 1 (Brown et al, 1993) probabilities of the word pairs inside and outside a span pair are useful.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We use IBM Model 1 as our estimate of both the inside and outside probabilities.</S><S sid = NA ssid = NA>λL1,L2 is the probability of any word alignment template for a pair of L1word source string and L2-word target string, which we model as a uniform distribution of word-forword alignment patterns after a Poisson distribution of target string’s possible lengths, following Brown et al. (1993).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P05-1059.txt | Citing Article:  C10-2084.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Tic-tac-toe pruning algorithm (Zhang and Gildea, 2005) uses dynamic programming to compute inside and outside scores for a span pair in O (n4).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Figure 3(a) displays the tic-tac-toe pattern for the inside and outside components of a particular cell.</S><S sid = NA ssid = NA>In the second experiment, we enabled the pruning techniques for the LITG with the beam ratio for the tic-tac-toe pruning as 10−5 and the number k for the top-k pruning as 25.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P05-1059.txt | Citing Article:  P08-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Like Zhang and Gildea (2005), it is used to prune bi text cells rather than score phrases.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We prune cells whose probability is lower than a fixed ratio below the best cell for the same source substring.</S><S sid = NA ssid = NA>We trained both the unlexicalized and the lexicalized ITGs on a parallel corpus of Chinese-English newswire text.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P05-1059.txt | Citing Article:  P08-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our pruning differs from Zhang and Gildea (2005) in two major ways.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Once the cells have been scored, there can be many ways of pruning.</S><S sid = NA ssid = NA>It differs in that the head words are chosen through EM rather than deterministic rules.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P05-1059.txt | Citing Article:  P08-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The tic-tac-toe pruning algorithm (ZhangandGildea, 2005) uses dynamic programming to compute the product of inside and outside scores for all cells in O (n4) time.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Figure 3(a) displays the tic-tac-toe pattern for the inside and outside components of a particular cell.</S><S sid = NA ssid = NA>Hence, the algorithm takes just O(n4) steps to compute the figure of merit for all cells in the chart.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P05-1059.txt | Citing Article:  P08-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Figure 2 compares the speed of the fast tic-tac-toe algorithm against the algorithm in Zhang and Gildea (2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Figure 3(a) displays the tic-tac-toe pattern for the inside and outside components of a particular cell.</S><S sid = NA ssid = NA>Hence, the algorithm takes just O(n4) steps to compute the figure of merit for all cells in the chart.</S> | Discourse Facet:  NA | Annotator: Automatic


