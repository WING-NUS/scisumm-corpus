Citance Number: 1 | Reference Article:  N10-1013.txt | Citing Article:  D10-1113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>More recently, Reisinger and Mooney (2010) present a method that uses clustering to produce multiple sense-specific vectors for each word.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This paper presents a method that uses clustering to produce multiple “sense-specific” vectors for each word.</S><S sid = NA ssid = NA>The multi-prototype approach uses word sense discovery to partition a word’s contexts and construct “sense specific” prototypes for each cluster.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N10-1013.txt | Citing Article:  D10-1113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Following Reisinger and Mooney (2010), we also evaluated mixture models that combine the output of models with varying parameter settings.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Cluster similarity metrics: Besides AvgSim and MaxSim, there are many similarity metrics over mixture models, e.g.</S><S sid = NA ssid = NA>Multi-Prototype Vector-Space Models of Word Meaning</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N10-1013.txt | Citing Article:  D10-1113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>It is difficult to relate our results to Reisinger and Mooney (2010), due to differences in the training data and the vector representations it gives rise to.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Figure 1 gives an overview of this process.</S><S sid = NA ssid = NA>The results demonstrate the superiority of a clustered approach over both traditional prototype and exemplar-based vector-space models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N10-1013.txt | Citing Article:  D10-1113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>As a comparison, a baseline configuration with tf-idf weighting and the cosine similarity measure yields a correlation of 0.38 with our data and 0.49 in Reisinger and Mooney (2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>All results reported in this paper use cosine similarity, 1 We compare across two different feature functions tf-idf weighting and X2 weighting, chosen due to their ubiquity in the literature (Agirre et al., 2009; Curran, 2004).</S><S sid = NA ssid = NA>The exemplar approach yields significantly higher correlation than the single prototype approach in all cases except Gigaword with tf-idf features (p < 0.05).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N10-1013.txt | Citing Article:  P14-2035.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In general, our approach is quite close to the multi prototype models of Reisinger and Mooney (2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Multi-Prototype Vector-Space Models of Word Meaning</S><S sid = NA ssid = NA>Also, once again, the performance of the multi-prototype approach is better for homonyms than polysemes.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N10-1013.txt | Citing Article:  P12-1092.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Reisinger and Mooney (2010b) introduced a multi-prototype VSM where word sense discrimination is first applied by clustering contexts, and then prototypes are built using the contexts of the sense-labeled words.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The multi-prototype approach uses word sense discovery to partition a word’s contexts and construct “sense specific” prototypes for each cluster.</S><S sid = NA ssid = NA>The set of vectors for a word is determined by unsupervised word sense discovery (WSD) (Sch¨utze, 1998), which clusters the contexts in which a word appears.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N10-1013.txt | Citing Article:  P12-1092.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Instead of using only one representation per word, Reisinger and Mooney (2010b) proposed the multi prototype approach for vector-space models, which uses multiple representations to capture different senses and usages of a word.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Multi-Prototype Vector-Space Models of Word Meaning</S><S sid = NA ssid = NA>This notion is evaluated empirically by computing the correlation between the predicted similarity using the contextual multi-prototype method and human similarity judgements for different usages of the same word.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N10-1013.txt | Citing Article:  P12-1092.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Pruned tf-idf (Reisinger and Mooney, 2010b) and ESA (Gabrilovich and Markovitch, 2007) are also included.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Figure 2 plots Spearman’s p on WordSim-353 against the number of clusters (K) for Wikipedia and Gigaword corpora, using pruned tf-idf and k2 features.2 In general pruned tf-idf features yield higher correlation than k2 features.</S><S sid = NA ssid = NA>However we have not yet evaluated its performance when using more powerful feature representations such those based on Latent or Explicit Semantic Analysis (Deerwester et al., 1990; Gabrilovich and Markovitch, 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N10-1013.txt | Citing Article:  P12-1092.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Reisinger and Mooney (2010b) found pruning the low-value tf-idf features helps performance.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Furthermore, it performs significantly worse 2(Feature pruning) We find that results using tf-idf features are extremely sensitive to feature pruning while x2 features are more robust.</S><S sid = NA ssid = NA>Squares indicate performance when combining across clusterings. than combined multi-prototype for tf-idf features, and does not differ significantly for x2 features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N10-1013.txt | Citing Article:  P12-1092.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We first tried movMF as in Reisinger and Mooney (2010b), but were unable to get decent results (only 31.5).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Based on preliminary experiments comparing various clustering methods, we found movMF gave the best results.</S><S sid = NA ssid = NA>However, given the right number of clusters, it also produces better results for polysemous words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N10-1013.txt | Citing Article:  P12-1092.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Reisinger and Mooney (2010b) combined the two approaches and applied them to vector-space models, which was further improved in Reisinger and Mooney (2010a).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This paper shows how they can be combined to create an improved vector-space model of lexical semantics.</S><S sid = NA ssid = NA>Multi-Prototype Vector-Space Models of Word Meaning</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N10-1013.txt | Citing Article:  P13-1055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The multi-prototype (Reisinger and Mooney, 2010) or examplar-based models (Erk and Pado, 2010), the Deep Learning approach of (Huang et al, 2012) or the redefinition of the distributional approach in a Bayesian framework (Kazama et al, 2010) can be classified into this second category.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Such models have been widely studied in the Psychology literature (Griffiths et al., 2007; Love et al., 2004; Rosseel, 2002).</S><S sid = NA ssid = NA>The Usage Similarity (USim) data set collected in Erk et al. (2009) provides such similarity scores from human raters.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N10-1013.txt | Citing Article:  P13-1055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We plan to extend this work by taking into account the notion of word sense as it is done in (Reisinger and Mooney, 2010) or (Huang et al, 2012): since we rely on occurrences of words in texts, this extension should be quite straightforward by turning our word-in-context classifiers into true word sense classifiers.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The multi-prototype approach uses word sense discovery to partition a word’s contexts and construct “sense specific” prototypes for each cluster.</S><S sid = NA ssid = NA>In previous work, vector-space lexical similarity and word sense discovery have been treated as two separate tasks.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N10-1013.txt | Citing Article:  N12-1076.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>A different approach has been taken by Erk and Padó (2010), Reisinger and Mooney (2010) and Reddy et al. (2011), who make use of token vectors for individual occurrences of a word, rather than using the already mixed type vectors.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our approach is similar to standard vector-space models of word meaning, with the addition of a perword-type clustering step: Occurrences for a specific word type are collected from the corpus and clustered using any appropriate method (§3.1).</S><S sid = NA ssid = NA>Occurrences are clustered and cluster centroids are used as prototype vectors.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N10-1013.txt | Citing Article:  E12-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The top-down multi-prototype approach determines a number of senses for each word, and then clusters the occurrences of the word (Reisinger and Mooney, 2010) into these senses.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This approach is commonly employed in unsupervised word sense discovery; however, we do not assume that clusters correspond to traditional word senses.</S><S sid = NA ssid = NA>They are grouped into homonyms (words with very distinct senses) and polysemes (words with related senses).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  N10-1013.txt | Citing Article:  P13-1171.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Contextual term-vectors created using the Wikipedia corpus have shown to perform well on measuring word similarity (Reisinger and Mooney, 2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This notion is evaluated empirically by computing the correlation between the predicted similarity using the contextual multi-prototype method and human similarity judgements for different usages of the same word.</S><S sid = NA ssid = NA>We next evaluated the multi-prototype approach on its ability to determine the most closely related words for a given target word (using the Wikipedia corpus with tf-idf features).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  N10-1013.txt | Citing Article:  W11-1310.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Related work to ours is (Reisinger and Mooney, 2010) where exemplars of a word are first clustered and then prototype vectors are built.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Occurrences are clustered and cluster centroids are used as prototype vectors.</S><S sid = NA ssid = NA>First, a word’s contexts are clustered to produce groups of similar context vectors.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  N10-1013.txt | Citing Article:  N12-1077.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Experiments were run on the Mastodon Cluster, provided by NSF Grant EIA-0303609.</S><S sid = NA ssid = NA>However, movMF introduces an additional per-cluster concentration parameter controlling its semantic breadth, allowing it to more accurately model non-uniformities in the distribution of cluster sizes.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  N10-1013.txt | Citing Article:  N12-1077.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Although in previous work, researchers try to capture word senses using different vectors (Reisinger and Mooney, 2010) from the same text corpus, this is in fact difficult in practice.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In previous work, vector-space lexical similarity and word sense discovery have been treated as two separate tasks.</S><S sid = NA ssid = NA>Clustering more accurately identifies homonyms’ clearly distinct senses and produces prototypes that better capture the different uses of these words.</S> | Discourse Facet:  NA | Annotator: Automatic


