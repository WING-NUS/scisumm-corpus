Citance Number: 1 | Reference Article:  D11-1141.txt | Citing Article:  E12-2014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We employ the method of Ritter et al (2011) to tokenise messages, and use token unigrams as features, including any hash tags, but ignoring twitter mentions, URLs and purely numeric tokens.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For instance, tweets often start with a verb (where the subject ‘I’ is implied), as in: “watchng american dad.” To overcome these differences in style and vocabulary, we manually annotated a set of 800 tweets (16K tokens) with tags from the Penn TreeBank tag set for use as in-domain training data for our POS tagging system, T-POS.4 We add new tags for the Twitter specific phenomena: retweets, @usernames, #hashtags, and urls.</S><S sid = NA ssid = NA>LabeledLDA) for classifying developed in parallel, Gimpell et al. (2011) build a named entities has a similar effect, in that informaPOS tagger for tweets using 20 coarse-grained tags. tion about an entity’s distribution of possible types Benson et. al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  D11-1141.txt | Citing Article:  E12-2014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Us1532</S><S sid = NA ssid = NA>On a 4-fold cross validation over 800 tweets, T-POS outperforms the Stanford tagger, obtaining a 26% reduction in error.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  D11-1141.txt | Citing Article:  P14-1118.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our second component - chunker - is taken from (Ritter et al, 2011), which also comes with a model trained on Twitter data and shown to perform better on noisy data such as user comments.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The performance of off-the-shelf news-trained POS taggers also suffers on Twitter data.</S><S sid = NA ssid = NA>We now discuss our approach to named entity recognition on Twitter data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  D11-1141.txt | Citing Article:  P14-1118.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The chunker from (Ritter et al., 2011) relies on its own POS tagger, however, in our structural representations we favor the POS tags from the CMU Twitter tagger and take only the chunk tags from the chunker.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>T-POS and T-CHUNK in segmenting Named Entities.</S><S sid = NA ssid = NA>LabeledLDA) for classifying developed in parallel, Gimpell et al. (2011) build a named entities has a similar effect, in that informaPOS tagger for tweets using 20 coarse-grained tags. tion about an entity’s distribution of possible types Benson et. al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  D11-1141.txt | Citing Article:  P12-1055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For example, Ritter et al (2011) develop a system that exploits a CRF model to segment named entities and then uses a distantly supervised approach based on LabeledLDA to classify named entities.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For example, we are able to use discriminative methods for named entity segmentation and distantly supervised approaches for classification.</S><S sid = NA ssid = NA>To address these issues we propose a distantly supervised approach which applies LabeledLDA (Ramage et al., 2009) to leverage large amounts of unlabeled data in addition to large dictionaries of entities gathered from Freebase, and combines information about an entity’s context across its mentions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  D11-1141.txt | Citing Article:  P14-2114.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We have also used a named entity tagger trained specifically on the Twitter data (Ritter et al, 2011) to directly extract named entities from tweets.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>All tools in §2 are used as features for named entity segmentation in §3.1.</S><S sid = NA ssid = NA>We now discuss our approach to named entity recognition on Twitter data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  D11-1141.txt | Citing Article:  P14-2114.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Firstly, a named entity recognizer (Ritter et al, 2011) is employed to identify named entities.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In addition we take a dis- identify the types of the entities they contain.</S><S sid = NA ssid = NA>Compared with the state-of-the-art newstrained Stanford Named Entity Recognizer (Finkel et al., 2005), T-SEG obtains a 52% increase in F1 score.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  D11-1141.txt | Citing Article:  P12-1109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The social text serves as a very valuable information source for many NLP applications, such as the information extraction (Ritter et al, 2011), retrieval (Subramaniam et al, 2009), summarization (Liu et al, 2011a), sentiment analysis (Celikyilmaz et al, 2010), etc.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Following Collins and Singer (1999), Downey et al. (2007) and Elsner et al.</S><S sid = NA ssid = NA>In addition Finin et. al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  D11-1141.txt | Citing Article:  P13-1114.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We generate part-of-speech information over the original raw text using a Twitter part-of-speech tagger (Ritter et al, 2011).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Generate the word we,i from Mult(o,;e,i).</S><S sid = NA ssid = NA>LabeledLDA) for classifying developed in parallel, Gimpell et al. (2011) build a named entities has a similar effect, in that informaPOS tagger for tweets using 20 coarse-grained tags. tion about an entity’s distribution of possible types Benson et. al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  D11-1141.txt | Citing Article:  W12-2106.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Using the UW Twitter NLP tools (Ritter et al, 2011).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>NLP tools are available at:</S><S sid = NA ssid = NA>There has been relatively little previous work on building NLP tools for Twitter or similar text styles.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  D11-1141.txt | Citing Article:  W12-2106.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>To study the diversity of named entities (NEs) in retweets, we used UW Twitter NLP Tools (Ritter et al., 2011) to extract NEs from RT-data.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>All tools in §2 are used as features for named entity segmentation in §3.1.</S><S sid = NA ssid = NA>NLP tools are available at:</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  D11-1141.txt | Citing Article:  W12-2106.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We then standardized variants (i.e. "fb" as a variant of "Facebook"), and manually categorized them against the 10-class schema defined by Ritter et al. (2011).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Following Collins and Singer (1999), Downey et al. (2007) and Elsner et al.</S><S sid = NA ssid = NA>In addition Finin et. al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  D11-1141.txt | Citing Article:  P14-2062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Leaving out the dedicated test set to avoid in-sample bias, we evaluate our models across three data sets: RITTER (the 10% test split of the data in Ritter et al (2011) used in Derczynski et al (2013)), the test set from Foster et al.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Following Collins and Singer (1999), Downey et al. (2007) and Elsner et al.</S><S sid = NA ssid = NA>In addition Finin et. al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  D11-1141.txt | Citing Article:  P14-2062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Us1532</S><S sid = NA ssid = NA>On a 4-fold cross validation over 800 tweets, T-POS outperforms the Stanford tagger, obtaining a 26% reduction in error.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  D11-1141.txt | Citing Article:  P14-2062.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Us1532</S><S sid = NA ssid = NA>On a 4-fold cross validation over 800 tweets, T-POS outperforms the Stanford tagger, obtaining a 26% reduction in error.</S> | Discourse Facet:  NA | Annotator: Automatic


