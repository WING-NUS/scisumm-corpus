Citance Number: 1 | Reference Article:  P99-1059.txt | Citing Article:  W01-1404.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For certain lexicalized context-free models we even obtain higher time complexities when the size of the grammar is not to be considered as a parameter (Eisner and Satta, 1999).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Among items of the same width, those of type .L should be considered last.</S><S sid = NA ssid = NA>Standard context-free parsing algorithms are inefficient in such a case.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P99-1059.txt | Citing Article:  W01-1404.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We can reduce the generative power of context free transduction grammars by a syntactic restriction that corresponds to the bi lexical context-free grammars (Eisner and Satta, 1999).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Efficient Parsing For Bilexical Context-Free Grammars And Head Automaton Grammars</S><S sid = NA ssid = NA>The reader is assumed to be familiar with context-free grammars.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P99-1059.txt | Citing Article:  P14-1100.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>While this criterion is in general NP-hard (Desper and Gascuel, 2005), for projective trees we find that a bi lexical parsing algorithm can be used to find an exact solution efficiently (Eisner and Satta, 1999).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In general, G has p = 0(IVD13) = 0(t3).</S><S sid = NA ssid = NA>We omit the formal proof that G and H admit isomorphic derivations and hence generate the same languages, observing only that if (x, y) = (bib2 • • • bi,b3+1- • • bk) E L(Ha)— a condition used in defining La above—then A[a] [bi] • • • B3[MaB3+1[bi+11 • • • Bk[bk], for any A, B1, .</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P99-1059.txt | Citing Article:  P14-1100.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>However, if we restrict u to be in U, as we do in the above, then maximizing c(u) over U can be solved using the bi lexical parsing algorithm from Eisner and Satta (1999).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The algorithm makes a single pass through the possible items, setting the bit for each if it can be derived using any rule in (b) from items whose bits are already set.</S><S sid = NA ssid = NA>For a bilexical grammar, the worst case is IPI = VD I 3 ' I VT12, which is large for a large vocabulary VT. We may improve the analysis somewhat by observing that when parsing d1 • • • dn, the CKY algorithm only considers nonterminals of the form A[di]; by restricting to the relevant productions we obtain 0(n3 • IVDI3 • min(n, IVTI)2)• We observe that in practical applications we always have n < IVTI• Let us then restrict our analysis to the (infinite) set of input instances of the parsing problem that satisfy relation n < WTI.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P99-1059.txt | Citing Article:  P09-1087.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The algorithmic complexity of (Wu, 1996) is O (n3+4 (m1)), though Huang et al (2005) present a more efficient factorization inspired by (Eisner and Satta, 1999) that yields an overall complexity of O (n3+3 (m1)), i.e., O(n3m).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>But in this paper we show that the complexity is not so bad after all: grammars where an 0(n3) algorithm was previously known (Eisner, 1997), the grammar constant can be reduced without harming the 0(n3) property.</S><S sid = NA ssid = NA>8 Split head automaton grammars in time 0 (n3 ) For many bilexical CFGs or HAGs of practical significance, just as for the bilexical version of link grammars (Lafferty et al., 1992), it is possible to parse length-n inputs even faster, in time 0(n3) (Eisner, 1997).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P99-1059.txt | Citing Article:  P09-1087.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>While it seems difficult to improve the asymptotic running time of the Eisner algorithm beyond what is presented in (Eisner and Satta, 1999), McDonald et al.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>With this assumption, the asymptotic time complexity of the CKY algorithm becomes 0(n5 • IVD13).</S><S sid = NA ssid = NA>8 Split head automaton grammars in time 0 (n3 ) For many bilexical CFGs or HAGs of practical significance, just as for the bilexical version of link grammars (Lafferty et al., 1992), it is possible to parse length-n inputs even faster, in time 0(n3) (Eisner, 1997).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P99-1059.txt | Citing Article:  P13-1044.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Since we are interested in projecting the fractional parse onto the space of projective spanning trees, we can simply employ a dynamic programming parsing algorithm (Eisner and Satta, 1999) where the weight of each edge is given as the fraction of the edge variable.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We use dynamic programming to assemble such subderivations into a full parse.</S><S sid = NA ssid = NA>This declarative specification, like CKY, may be implemented by bottom-up dynamic programming.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P99-1059.txt | Citing Article:  P06-2122.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The new model, which bilexicalizes within languages, allows us to use the "hook trick" (Eisner and Satta, 1999) and therefore reduces complexity.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Formally, a flip state is one that allows entry on a —> transition and that either allows exit on a transition or is a final state.</S><S sid = NA ssid = NA>A split head automaton Ha is said to be g-split if its set of flip states, denoted C Qa, has size < g. The languages that can be recognized by g-split HAs are those that can be written as 1..g 1 Li x R, where the Li and Ri are regular languages over VT. Eisner (1997) actually defined (g-split) bilexical grammars in terms of the latter property.6 We now present our result: Figure 3 specifies an 0(n3g2t2) recognition algorithm for a head automaton grammar H in which every H, is g-split.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P99-1059.txt | Citing Article:  D08-1017.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), but not in the non projective case (McDonald and Satta, 2007), where finding the highest-scoring tree becomes NP-hard.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>If the HAs in H happen to be deterministic, then in each binary production given by (ii) above, symbol A is fully determined by a, b, and C. In this case p = 0(t2), so the parser will operate in time 0(n4t2).</S><S sid = NA ssid = NA>Standard context-free parsing algorithms are inefficient in such a case.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P99-1059.txt | Citing Article:  W05-1504.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Throughout this paper we will use split bi lexical grammars, or SBGs (Eisner, 2000), a notationally simpler variant of split head-automaton grammars, or SHAGs (Eisner and Satta, 1999).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We are concerned here with head automaton grammars H such that every Ha is split.</S><S sid = NA ssid = NA>A split head automaton Ha is said to be g-split if its set of flip states, denoted C Qa, has size < g. The languages that can be recognized by g-split HAs are those that can be written as 1..g 1 Li x R, where the Li and Ri are regular languages over VT. Eisner (1997) actually defined (g-split) bilexical grammars in terms of the latter property.6 We now present our result: Figure 3 specifies an 0(n3g2t2) recognition algorithm for a head automaton grammar H in which every H, is g-split.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P99-1059.txt | Citing Article:  W05-1504.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>On this point, see (Eisner and Satta, 1999, and footnote 6).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For the usual case, split head automaton grammars or equivalent bilexical CFGs, we replace the 0(n3) algorithm of (Eisner, 1997) by one with a smaller grammar constant.</S><S sid = NA ssid = NA>But in this paper we show that the complexity is not so bad after all: grammars where an 0(n3) algorithm was previously known (Eisner, 1997), the grammar constant can be reduced without harming the 0(n3) property.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P99-1059.txt | Citing Article:  W05-1504.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>First, we observe without details that we can easily achieve this by starting instead with the algorithm of Eisner (2000), rather than Eisner and Satta (1999), and again refusing to add long tree dependencies.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For a practical speedup, add h\j as an antecedent to the MID rule (and fill in the parse table from right to left).</S><S sid = NA ssid = NA>We introduce next a grammar formalism that captures lexical dependencies among pairs of words in VT.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P99-1059.txt | Citing Article:  W05-1504.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The obvious reduction for unsplit head automaton grammars, say, is only O(n4) O (n3k), following (Eisner and Satta, 1999).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>7 Head automaton grammars in time 0(n4) In this section we show that a length-n string generated by a head automaton grammar (Alshawi, 1996) can be parsed in time 0(n4).</S><S sid = NA ssid = NA>Efficient Parsing For Bilexical Context-Free Grammars And Head Automaton Grammars</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P99-1059.txt | Citing Article:  E09-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In the case of dependency parsers it is also possible to use grammars (Eisner and Satta, 1999), but many algorithms use a data-driven approach instead, making individual decisions about which dependencies to create by using probabilistic models (Eisner, 1996) or classifiers (Yamada and Matsumoto, 2003).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Several recent real-world parsers have improved state-of-the-art parsing accuracy by relying on probabilistic or weighted versions of bilexical grammars (Alshawi, 1996; Eisner, 1996; Charniak, 1997; Collins, 1997).</S><S sid = NA ssid = NA>stochastic parsers use grammars, where each word type idiosyncratically prefers particular complements with parhead words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P99-1059.txt | Citing Article:  N06-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>It achieves its speed in part because it uses the Eisner and Satta (1999) algorithm for n3 bi lexical parsing, but also because dependency parsing has a much lower grammar constant than does standard PCFG parsing after all, there are no phrasal constituents to consider.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Standard context-free parsing algorithms are inefficient in such a case.</S><S sid = NA ssid = NA>But in this paper we show that the complexity is not so bad after all: grammars where an 0(n3) algorithm was previously known (Eisner, 1997), the grammar constant can be reduced without harming the 0(n3) property.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P99-1059.txt | Citing Article:  H05-1036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Folding introduces new intermediate items, perhaps exploiting the distributive law; applications include parsing speedups such as (Eisner and Satta, 1999), as well as well-known techniques for speeding up multi-way database joins, constraint programming, or marginalization of graphical models.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our dynamic programming techniques for cheaply attaching head information to derivations can also be exploited in parsing formalisms other than rewriting systems.</S><S sid = NA ssid = NA>We present parsing algorithms for two bilexical formalisms, improvthe prior upper bounds of For a comspecial case that was known to allow (Eisner, 1997), we present an algorithm with an improved grammar constant.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P99-1059.txt | Citing Article:  N09-1012.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In the sections that follow, we frame various dependency models as a particular variety of CFGs known as split-head bi lexical CFGs (Eisner and Satta, 1999).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For the usual case, split head automaton grammars or equivalent bilexical CFGs, we replace the 0(n3) algorithm of (Eisner, 1997) by one with a smaller grammar constant.</S><S sid = NA ssid = NA>We do this by providing a translation from head automaton grammars to bilexical CFGs.4 This result improves on the head-automaton parsing algorithm given by Alshawi, which is analogous to the CKY algorithm on bilexical CFGs and is likewise 0(n5) in practice (see §3).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P99-1059.txt | Citing Article:  D08-1016.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The scoring follows the parametrization of a weighted split head-automaton grammar (Eisner and Satta, 1999).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We are concerned here with head automaton grammars H such that every Ha is split.</S><S sid = NA ssid = NA>A split head automaton Ha is said to be g-split if its set of flip states, denoted C Qa, has size < g. The languages that can be recognized by g-split HAs are those that can be written as 1..g 1 Li x R, where the Li and Ri are regular languages over VT. Eisner (1997) actually defined (g-split) bilexical grammars in terms of the latter property.6 We now present our result: Figure 3 specifies an 0(n3g2t2) recognition algorithm for a head automaton grammar H in which every H, is g-split.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P99-1059.txt | Citing Article:  P05-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>As shown by Eisner (Eisner and Satta,1999) the dynamic programming algorithms for bi lexicalized PCFGs require O (m3) states, so a n-best parser would require O (nm3) states.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We use dynamic programming to assemble such subderivations into a full parse.</S><S sid = NA ssid = NA>This declarative specification, like CKY, may be implemented by bottom-up dynamic programming.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P99-1059.txt | Citing Article:  W06-2929.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The best known parsing algorithm for such a model is O (n3) (Eisner and Satta, 1999).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>But in this paper we show that the complexity is not so bad after all: grammars where an 0(n3) algorithm was previously known (Eisner, 1997), the grammar constant can be reduced without harming the 0(n3) property.</S><S sid = NA ssid = NA>We present parsing algorithms for two bilexical formalisms, improvthe prior upper bounds of For a comspecial case that was known to allow (Eisner, 1997), we present an algorithm with an improved grammar constant.</S> | Discourse Facet:  NA | Annotator: Automatic


