We formulate dependency parsing as a graphical model with the novel ingredient of global constraints.
We show how to apply loopy belief propagation (BP), a simple and effective tool for approximate learning and inference.
As a parsing algorithm, BP is both asymptotically and empirically efficient.
Even with second-order features or latent variables, which would make exact parsing considerably slower or NP-hard, BP needs only O(n3) time with a small constant factor.
Furthermore, such features significantly improve parse accuracy over exact first-order methods.
Incorporating additional features would increase the runtime additively rather than multiplicatively.