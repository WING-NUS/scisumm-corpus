Prototype-Driven Learning For Sequence Models
We investigate prototype-driven learning for primarily unsupervised sequence modeling.
Prior knowledge is specified declaratively, by providing a few canonical examples of each target annotation label.
This sparse prototype information is then propagated across a corpus using distributional similarity features in a log-linear generative model.
On part-of-speech induction in English and Chinese, as well as an information extraction task, prototype features provide substantial error rate reductions over competitive baselines and outperform previous work.
For example, we can achieve an English part-of-speech tagging accuracy of 80.5% using only three examples of each tag and no dictionary constraints.
We also compare to semi-supervised learning and discuss the system's error trends.
Prototype-driven learning (PDL) optimizes the joint marginal likelihood of data labeled with prototype input features for each label.
We ask the user to suggest a few prototypes (examples) for each class and use those as features.
