Citance Number: 1 | Reference Article:  W06-3108.txt | Citing Article:  W07-0723.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Decompounding German nominal compounds will improve translation quality (Koehn and Knight, 2003) Re-ordering models based on word forms and parts-of-speech will improve translation quality (Zens and Ney, 2006).</S> | Reference Offset:  ['22','175'] | Reference Text:  <S sid = 22 ssid = >This evaluation consists of two parts: first we will evaluate the prediction capabilities of the model on a word-aligned corpus and second we will show improved translation quality compared to the baseline system.</S><S sid = 175 ssid = >More fine-tuning of the reordering model toward translation quality might also result in improvements.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W06-3108.txt | Citing Article:  P12-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Despite their high perplexities, reordered LMs yield some improvements when integrated to a PSMT baseline that already includes a discriminative phrase orientation model (Zens and Ney, 2006).</S> | Reference Offset:  ['43','138'] | Reference Text:  <S sid = 43 ssid = >We use a state-of-the-art phrase-based translation system (Zens and Ney, 2004; Zens et al., 2005) including the following models: an n-gram language model, a phrase translation model and a word-based lexicon model.</S><S sid = 138 ssid = >Despite that we observe the same tendencies as for two orientation classes.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W06-3108.txt | Citing Article:  N10-1129.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Unlike previous discriminative local orientation models (Zens and Ney, 2006), our framework permits the definition of global features.</S> | Reference Offset:  ['0','1'] | Reference Text:  <S sid = 0 ssid = >Discriminative Reordering Models For Statistical Machine Translation</S><S sid = 1 ssid = >We present discriminative reordering models for phrase-based statistical machine translation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W06-3108.txt | Citing Article:  P09-2061.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Adopting the idea of predicting the orientation, (Zens and Ney, 2006) started exploiting the context and grammar which may relate to phrase reorderings.</S> | Reference Offset:  ['11','26'] | Reference Text:  <S sid = 11 ssid = >We adopt the idea of predicting the orientation, but we propose to use a maximum-entropy based model.</S><S sid = 26 ssid = >The idea of predicting the orientation is adopted from (Tillmann and Zhang, 2005) and (Koehn et al., 2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W06-3108.txt | Citing Article:  P09-2061.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Figure 2: Classification results with respect to d. We used GIZA++ to produce alignments, enabling us to compare using a DPR model against a baseline lexicalized reordering model (Koehn et al., 2005) that uses MLE orientation prediction and a discriminative model (Zens and Ney, 2006) that utilizes an ME framework.</S> | Reference Offset:  ['15','47'] | Reference Text:  <S sid = 15 ssid = >In (Koehn et al., 2005) several variants of the orientation model have been tried.</S><S sid = 47 ssid = >This very simple reordering model is widely used, for instance in (Och et al., 1999; Koehn, 2004; Zens et al., 2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W06-3108.txt | Citing Article:  P09-2061.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Contrasting the direct use of the reordering probabilities used in (Zens and Ney, 2006), we utilize the probabilities to adjust the word distance-based reordering cost, where the reordering cost of a sentence is computed as P o (f, e).</S> | Reference Offset:  ['8','145'] | Reference Text:  <S sid = 8 ssid = >One reason is that most phrase-based systems use a very simple reordering model.</S><S sid = 145 ssid = >The word-class based features are not used for the translation experiments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W06-3108.txt | Citing Article:  D11-1017.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In addition to the regular distance distortion model, we incorporate a maximum entropy based lexicalized phrase reordering model (Zens and Ney, 2006) as a feature used in decoding.</S> | Reference Offset:  ['43','144'] | Reference Text:  <S sid = 43 ssid = >We use a state-of-the-art phrase-based translation system (Zens and Ney, 2004; Zens et al., 2005) including the following models: an n-gram language model, a phrase translation model and a word-based lexicon model.</S><S sid = 144 ssid = >The features for the maximum-entropy based reordering model are based on the source and target language words within a window of one.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W06-3108.txt | Citing Article:  D10-1054.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Zens and Ney (2006) and Xiong et al (2006) utilized contextual information to improve phrase reordering.</S> | Reference Offset:  ['9','47'] | Reference Text:  <S sid = 9 ssid = >Usually, the costs for phrase movements are linear in the distance, e.g. see (Och et al., 1999; Koehn, 2004; Zens et al., 2005).</S><S sid = 47 ssid = >This very simple reordering model is widely used, for instance in (Och et al., 1999; Koehn, 2004; Zens et al., 2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W06-3108.txt | Citing Article:  W11-2102.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In addition to the regular distance distortion model, we incorporate a maximum entropy based lexicalized phrase reordering model (Zens and Ney, 2006).</S> | Reference Offset:  ['43','144'] | Reference Text:  <S sid = 43 ssid = >We use a state-of-the-art phrase-based translation system (Zens and Ney, 2004; Zens et al., 2005) including the following models: an n-gram language model, a phrase translation model and a word-based lexicon model.</S><S sid = 144 ssid = >The features for the maximum-entropy based reordering model are based on the source and target language words within a window of one.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W06-3108.txt | Citing Article:  W11-2149.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = ></S> | Reference Offset:  ['44','178'] | Reference Text:  <S sid = 44 ssid = >The latter two models are used for both directions: p(f|e) and p(e|f).</S><S sid = 178 ssid = >HR0011-06-C-0023, and was partly funded by the European Union under the integrated project TC-STAR (Technology and Corpora for Speech to Speech Translation, IST2002-FP6-506738, http://www.tc-star.org).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W06-3108.txt | Citing Article:  W11-2149.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The first contrastive submission is a phrase-based system, enhanced with a triplet lexicon model and a discriminative word lexicon model (Mauser et al, 2009) both trained on in-domain news commentary data only as well as a sentence-level single-word lex icon model and a discriminative reordering model (Zens and Ney, 2006a).</S> | Reference Offset:  ['20','43'] | Reference Text:  <S sid = 20 ssid = >Then, we will present the discriminative reordering model in Section 4.</S><S sid = 43 ssid = >We use a state-of-the-art phrase-based translation system (Zens and Ney, 2004; Zens et al., 2005) including the following models: an n-gram language model, a phrase translation model and a word-based lexicon model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W06-3108.txt | Citing Article:  W11-2149.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following models were applied: n-gram posteriors (Zens and Ney, 2006b), sentence length model, a 6-gram LM and single word lexicon models in both normal and inverse direction.</S> | Reference Offset:  ['43','44'] | Reference Text:  <S sid = 43 ssid = >We use a state-of-the-art phrase-based translation system (Zens and Ney, 2004; Zens et al., 2005) including the following models: an n-gram language model, a phrase translation model and a word-based lexicon model.</S><S sid = 44 ssid = >The latter two models are used for both directions: p(f|e) and p(e|f).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W06-3108.txt | Citing Article:  W11-2149.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Moreover, we tested the impact of the discriminative reordering model (Zens and Ney, 2006a).</S> | Reference Offset:  ['20','48'] | Reference Text:  <S sid = 20 ssid = >Then, we will present the discriminative reordering model in Section 4.</S><S sid = 48 ssid = >In this section, we will describe the proposed discriminative reordering model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W06-3108.txt | Citing Article:  P13-1032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The classifier can be trained with maximum likelihood like Moses lexicalized reordering (Koehn et al, 2007) and hierarchical lexical ized reordering model (Galley and Manning, 2008) or be trained under maximum entropy framework (Zens and Ney, 2006).</S> | Reference Offset:  ['2','164'] | Reference Text:  <S sid = 2 ssid = >The models are trained using the maximum entropy principle.</S><S sid = 164 ssid = >This model is trained on the word aligned bilingual corpus using the maximum entropy principle.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W06-3108.txt | Citing Article:  P13-1032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Figure 3 is an illustration of (Zens and Ney, 2006). j is the source word position which is aligned to the last target word of the current phrase.</S> | Reference Offset:  ['74','75'] | Reference Text:  <S sid = 74 ssid = >We use the source position j which is aligned to the last word of the target phrase in target position i.</S><S sid = 75 ssid = >The illustration in Figure 1 contains such an example.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W06-3108.txt | Citing Article:  P13-1032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >j is the source word position which is aligned to the first target word position of the next phrase. (Zens and Ney, 2006) proposed a maximum entropy classifier to predict the orientation of the next phrase given the current phrase.</S> | Reference Offset:  ['55','74'] | Reference Text:  <S sid = 55 ssid = >Now, the model has to predict if the start position of the next phrase jâ€² is to the left or to the right of the current phrase.</S><S sid = 74 ssid = >We use the source position j which is aligned to the last word of the target phrase in target position i.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W06-3108.txt | Citing Article:  W12-0704.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Clustered word classes have also been used in a discriminate reordering model (Zens and Ney, 2006), and were shown to reduce the classification error rate. Word clusters have also been used for unsupervised and semi-supervised parsing.</S> | Reference Offset:  ['131','166'] | Reference Text:  <S sid = 131 ssid = >Adding the features based on word classes, the classification error rate can be further improved to 2.1%.</S><S sid = 166 ssid = >We have shown that the model is able to predict the orientation very well, e.g. for Arabic-English the classification error rate is only 2.1%.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W06-3108.txt | Citing Article:  W12-0704.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >One way to approach reordering is by extending the translation model, either by adding extra models, such as lexicalized (Koehn et al, 2005) or discriminative (Zens and Ney, 2006) reordering models or by directly modelling reordering in hierarchical (Chiang, 2007) or syntactical translation models (Yamada and Knight, 2002).</S> | Reference Offset:  ['0','1'] | Reference Text:  <S sid = 0 ssid = >Discriminative Reordering Models For Statistical Machine Translation</S><S sid = 1 ssid = >We present discriminative reordering models for phrase-based statistical machine translation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W06-3108.txt | Citing Article:  W09-2307.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Using these Chinese grammatical relations, we improve a phrase orientation classifier (introduced by Zens and Ney (2006)) that decides the ordering of two phrases when translated into English by adding path features designed over the Chinese typed dependencies.</S> | Reference Offset:  ['98','139'] | Reference Text:  <S sid = 98 ssid = >We use the Arabic-English, the Chinese-English and the Japanese-English data.</S><S sid = 139 ssid = >Again, using more features always helps to improve the performance.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W06-3108.txt | Citing Article:  W09-2307.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To achieve this, we train a discriminative phrase orientation classifier following the work by Zens and Ney (2006), and we use the grammatical relations between words as extra features to build the classifier.</S> | Reference Offset:  ['43','103'] | Reference Text:  <S sid = 43 ssid = >We use a state-of-the-art phrase-based translation system (Zens and Ney, 2004; Zens et al., 2005) including the following models: an n-gram language model, a phrase translation model and a word-based lexicon model.</S><S sid = 103 ssid = >To train and evaluate the reordering model, we use the word aligned bilingual training corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


