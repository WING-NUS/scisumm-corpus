[
  {
    "citance_No": 1, 
    "citing_paper_id": "W07-0723", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Maria, Holmqvist | Sara, Stymne | Lars, Ahrenberg", 
    "raw_text": "and Ney, 2004)? Decompounding German nominal compounds will improve translation quality (Koehn and Knight, 2003)? Re-ordering models based on word forms and parts-of-speech will improve translation quality (Zens and Ney, 2006)", 
    "clean_text": "Decompounding German nominal compounds will improve translation quality (Koehn and Knight, 2003) Re-ordering models based on word forms and parts-of-speech will improve translation quality (Zens and Ney, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P12-1050", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Arianna, Bisazza | Marcello, Federico", 
    "raw_text": "Despite their high perplexities, reordered LMs yield some improvements when integrated to a PSMT baseline that already includes a discriminative phrase orientation model (Zens and Ney, 2006)", 
    "clean_text": "Despite their high perplexities, reordered LMs yield some improvements when integrated to a PSMT baseline that already includes a discriminative phrase orientation model (Zens and Ney, 2006).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "N10-1129", 
    "citing_paper_authority": 12, 
    "citing_paper_authors": "Spence, Green | Michel, Galley | Christopher D., Manning", 
    "raw_text": "Unlike previous discriminative local orientation models (Zens and Ney, 2006), our framework permits the definition of global features", 
    "clean_text": "Unlike previous discriminative local orientation models (Zens and Ney, 2006), our framework permits the definition of global features.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "P09-2061", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Yizhao, Ni | Craig, Saunders | Sandor, Szedmak | Mahesan, Niranjan", 
    "raw_text": "Adopting the idea of predicting the orientation, (Zens and Ney, 2006) started exploiting the context and grammar which may relate to phrase reorderings", 
    "clean_text": "Adopting the idea of predicting the orientation, (Zens and Ney, 2006) started exploiting the context and grammar which may relate to phrase reorderings.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P09-2061", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Yizhao, Ni | Craig, Saunders | Sandor, Szedmak | Mahesan, Niranjan", 
    "raw_text": "Figure 2: Classification results with respect to d. We used GIZA++ to produce alignments, enabling us to compare using a DPR model against a baseline lexicalized reordering model (Koehn et al., 2005) that uses MLE orientation prediction and a discriminative model (Zens and Ney, 2006) that utilizes an ME framework", 
    "clean_text": "Figure 2: Classification results with respect to d. We used GIZA++ to produce alignments, enabling us to compare using a DPR model against a baseline lexicalized reordering model (Koehn et al., 2005) that uses MLE orientation prediction and a discriminative model (Zens and Ney, 2006) that utilizes an ME framework.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "P09-2061", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Yizhao, Ni | Craig, Saunders | Sandor, Szedmak | Mahesan, Niranjan", 
    "raw_text": "Contrasting the direct use of the reordering probabilities used in (Zens and Ney, 2006), we utilize the probabilities to adjust the word distance? based reordering cost, where the reordering cost of a sentence is computed as P o (f, e)= 243 Settings three? class setup five? class setup Classes d &lt; 0 d= 0 d& gt; 0 d?? 5? 5 &lt; d &lt; 0 d= 0 0 &lt; d &lt; 5 d? 5 Train 181, 583 755, 854 181, 279 82, 677 98, 907 755, 854 64, 881 116, 398 Test 5, 025 21, 106 5, 075 2, 239 2, 786 21, 120 1, 447 3, 629 Table 3: Data statistics for the classification experiments", 
    "clean_text": "Contrasting the direct use of the reordering probabilities used in (Zens and Ney, 2006), we utilize the probabilities to adjust the word distance-based reordering cost, where the reordering cost of a sentence is computed as P o (f, e).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "D11-1017", 
    "citing_paper_authority": 11, 
    "citing_paper_authors": "Jason, Katz-Brown | Slav, Petrov | Ryan, McDonald | Franz Josef, Och | David, Talbot | Hiroshi, Ichikawa | Masakazu, Seno | Hideto, Kazawa", 
    "raw_text": "In addition to the regular distance distortion model, we incorporate a maximum entropy based lexicalized phrase reordering model (Zens and Ney, 2006) as a feature used in decoding", 
    "clean_text": "In addition to the regular distance distortion model, we incorporate a maximum entropy based lexicalized phrase reordering model (Zens and Ney, 2006) as a feature used in decoding.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "D10-1054", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Zhongjun, He | Yao, Meng | Hao, Yu", 
    "raw_text": "Zens and Ney (2006) and Xiong et al (2006) utilized contextual information to improve phrase reordering", 
    "clean_text": "Zens and Ney (2006) and Xiong et al (2006) utilized contextual information to improve phrase reordering.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "W11-2102", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "David, Talbot | Hideto, Kazawa | Hiroshi, Ichikawa | Jason, Katz-Brown | Masakazu, Seno | Franz Josef, Och", 
    "raw_text": "In addition to the regular distance distortion model, we incorporate a maximum entropy based lexical ized phrase reordering model (Zens and Ney, 2006) .For parallel training data, we use an in-house collection of parallel documents", 
    "clean_text": "In addition to the regular distance distortion model, we incorporate a maximum entropy based lexicalized phrase reordering model (Zens and Ney, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "W11-2149", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Matthias, Huck | Joern, Wuebker | Christoph, Schmidt | Markus, Freitag | Stephan, Peitz | Daniel, Stein | Arnaud, Dagnelies | Saab, Mansour | Gregor, Leusch | Hermann, Ney", 
    "raw_text": "Tolexicalize reordering, a discriminative reordering model (Zens and Ney, 2006a) is used", 
    "clean_text": "", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "W11-2149", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Matthias, Huck | Joern, Wuebker | Christoph, Schmidt | Markus, Freitag | Stephan, Peitz | Daniel, Stein | Arnaud, Dagnelies | Saab, Mansour | Gregor, Leusch | Hermann, Ney", 
    "raw_text": "The first contrastive submission is a phrase-based system, enhanced with a triplet lexicon model and a disc rim inative word lexicon model (Mauser et al, 2009)? both trained on in-domain news commentary data only? as well as a sentence-level single-word lex icon model and a discriminative reordering model (Zens and Ney, 2006a)", 
    "clean_text": "The first contrastive submission is a phrase-based system, enhanced with a triplet lexicon model and a discriminative word lexicon model (Mauser et al, 2009) both trained on in-domain news commentary data only as well as a sentence-level single-word lex icon model and a discriminative reordering model (Zens and Ney, 2006a).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "W11-2149", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Matthias, Huck | Joern, Wuebker | Christoph, Schmidt | Markus, Freitag | Stephan, Peitz | Daniel, Stein | Arnaud, Dagnelies | Saab, Mansour | Gregor, Leusch | Hermann, Ney", 
    "raw_text": "Following models were applied: n-gram posteriors (Zens and Ney, 2006b), sentence length model, a 6-gram LM and single word lexicon models in both normal and inverse direction", 
    "clean_text": "Following models were applied: n-gram posteriors (Zens and Ney, 2006b), sentence length model, a 6-gram LM and single word lexicon models in both normal and inverse direction.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "W11-2149", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Matthias, Huck | Joern, Wuebker | Christoph, Schmidt | Markus, Freitag | Stephan, Peitz | Daniel, Stein | Arnaud, Dagnelies | Saab, Mansour | Gregor, Leusch | Hermann, Ney", 
    "raw_text": "Moreover, we tested the impact of the discriminative reordering model (Zens and Ney, 2006a)", 
    "clean_text": "Moreover, we tested the impact of the discriminative reordering model (Zens and Ney, 2006a).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "P13-1032", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Minwei, Feng | Jan-Thorsten, Peter | Hermann, Ney", 
    "raw_text": "The classifier can be trained with maximum likelihood like Moseslexicalized reordering (Koehn et al, 2007) and hierarchical lexical ized reordering model (Galley and Manning, 2008) or be trained under maximum entropy framework (Zens and Ney, 2006)", 
    "clean_text": "The classifier can be trained with maximum likelihood like Moses lexicalized reordering (Koehn et al, 2007) and hierarchical lexical ized reordering model (Galley and Manning, 2008) or be trained under maximum entropy framework (Zens and Ney, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "P13-1032", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Minwei, Feng | Jan-Thorsten, Peter | Hermann, Ney", 
    "raw_text": "Figure 3 is an illustration of (Zens and Ney, 2006). j is the source word position which is aligned to the last target word of the current phrase", 
    "clean_text": "Figure 3 is an illustration of (Zens and Ney, 2006). j is the source word position which is aligned to the last target word of the current phrase.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "P13-1032", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Minwei, Feng | Jan-Thorsten, Peter | Hermann, Ney", 
    "raw_text": "j?? is the source word position which is aligned to the first target word position of the next phrase. (Zens and Ney, 2006) proposed a maximum entropy classifier to predict the orientation of the next phrase given the current phrase", 
    "clean_text": "j is the source word position which is aligned to the first target word position of the next phrase. (Zens and Ney, 2006) proposed a maximum entropy classifier to predict the orientation of the next phrase given the current phrase.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "W12-0704", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Sara, Stymne", 
    "raw_text": "Clustered word classes have also been used in a discriminate re ordering model (Zens and Ney, 2006), and were shown to reduce the classification error rate. Word clusters have also been used for unsupervised and semi-supervised parsing", 
    "clean_text": "Clustered word classes have also been used in a discriminate reordering model (Zens and Ney, 2006), and were shown to reduce the classification error rate. Word clusters have also been used for unsupervised and semi-supervised parsing.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "W12-0704", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Sara, Stymne", 
    "raw_text": "One way to approach reordering is by extending the translation model, either by adding extra models, such as lexicalized (Koehn et al, 2005) or discriminative (Zens and Ney, 2006) reordering models or by directly modelling reordering in hierarchical (Chiang, 2007) or syntactical translation models (Yamada and Knight, 2002)", 
    "clean_text": "One way to approach reordering is by extending the translation model, either by adding extra models, such as lexicalized (Koehn et al, 2005) or discriminative (Zens and Ney, 2006) reordering models or by directly modelling reordering in hierarchical (Chiang, 2007) or syntactical translation models (Yamada and Knight, 2002).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "W09-2307", 
    "citing_paper_authority": 24, 
    "citing_paper_authors": "Pi-Chuan, Chang | Huihsin, Tseng | Daniel, Jurafsky | Christopher D., Manning", 
    "raw_text": "Using these Chinese grammatical relations, we improve a phrase orientation classifier (introduced by Zens and Ney (2006)) that decides the ordering of two phrases when translated into English by adding path features designed over the Chinese typed dependencies", 
    "clean_text": "Using these Chinese grammatical relations, we improve a phrase orientation classifier (introduced by Zens and Ney (2006)) that decides the ordering of two phrases when translated into English by adding path features designed over the Chinese typed dependencies.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "W09-2307", 
    "citing_paper_authority": 24, 
    "citing_paper_authors": "Pi-Chuan, Chang | Huihsin, Tseng | Daniel, Jurafsky | Christopher D., Manning", 
    "raw_text": "To achieve this, we train adiscriminative phrase orientation classifier following the work by Zens and Ney (2006), and we use the grammatical relations between words as extra features to build the classifier", 
    "clean_text": "To achieve this, we train a discriminative phrase orientation classifier following the work by Zens and Ney (2006), and we use the grammatical relations between words as extra features to build the classifier.", 
    "keep_for_gold": 0
  }
]