Applying Co-Training Methods To Statistical Parsing
We propose a novel Co-Training method for statistical parsing.
The algorithm takes as input a small corpus (9695 sentences) annotated with parse trees, a dictionary of possible lexicalized structures for each word in the training set and a large pool of unlabeled text.
The algorithm iteratively labels the entire data set with parse trees.
Using empirical results based on parsing the Wall Street Journal corpus we show that training a statistical parser on the combined labeled and unlabeled data strongly outperforms training only on the labeled data.
Our co-training a mostly unsupervised algorithm that replaces the human by having two (or more) parsers label training examples for each other.
