Citance Number: 1 | Reference Article:  W96-0214.txt | Citing Article:  W96-0111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In Goodman (1996), an efficient parsing strategy is given that maximizes the expected number of correct constituents.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We solve the second problem by a novel deterministic parsing strategy that maximizes the expected number of correct constituents, rather than the probability of a correct parse tree.</S><S sid = NA ssid = NA>Next, we present an algorithm for parsing, which returns the parse that is expected to have the largest number of correct constituents.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W96-0214.txt | Citing Article:  W96-0111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This research also shows the importance of testing on more than one small test set, as well as the importance of not making cross-corpus comparisons; if a new corpus is required, then previous algorithms should be duplicated for comparison.</S><S sid = NA ssid = NA>The construction is as follows.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W96-0214.txt | Citing Article:  W96-0111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Although Sima'an (1996) and Goodman (1996) also report experiments on unedited ATIS trees, their results do not refer to the most probable parse but to the most probable derivation and the maximum constituents parse respectively.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>First, one can find the most probable derivation.</S><S sid = NA ssid = NA>This parse tree is most probable.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W96-0214.txt | Citing Article:  W12-1904.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This does not scale well to large treebanks, forcing the use of implicit representations (Goodman, 1996) or heuristic subsets (Bod, 2001).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In other words, rather than using the large, explicit STSG, we can use this small PCFG that generates isomorphic derivations, with identical probabilities.</S><S sid = NA ssid = NA>Unlike a PCFG, the use of trees allows capturing large contexts, making the model more sensitive.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W96-0214.txt | Citing Article:  P09-1067.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Indeed, our methods were inspired by past work on variational decoding for DOP (Goodman, 1996) and for latent-variable parsing (Matsuzaki et al, 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Efficient Algorithms For Parsing The DOP Model</S><S sid = NA ssid = NA>Ten data sets were constructed by randomly splitting minimally edited ATIS (Hemphill et al., 1990) sentences into a 700 sentence training set, and 88 sentence test set, then discarding sentences of length > 30.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W96-0214.txt | Citing Article:  P02-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Although see (Goodman 1996) for an efficient algorithm for the DOP model, which we discuss in section 7 of this paper.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Efficient Algorithms For Parsing The DOP Model</S><S sid = NA ssid = NA>Khalil Sima'an (1996) implemented a version of the DOP model, which parses efficiently by limiting the number of trees used and by using an efficient most probable derivation model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W96-0214.txt | Citing Article:  P02-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>(Goodman 1996) gives a polynomial time conversion of a DOP model into an equivalent PCFG whose size is linear in the size of the training set.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this paper, we introduce a reduction of the DOP model to an exactly equivalent Probabilistic Context Free Grammar (PCFG) that is linear in the number of nodes in the training data.</S><S sid = NA ssid = NA>Thus he concludes that his algorithm runs in polynomial time.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W96-0214.txt | Citing Article:  P01-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This research also shows the importance of testing on more than one small test set, as well as the importance of not making cross-corpus comparisons; if a new corpus is required, then previous algorithms should be duplicated for comparison.</S><S sid = NA ssid = NA>The construction is as follows.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W96-0214.txt | Citing Article:  D07-1058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The relation between DOP and enrichment/conditioning models was clarified by Joshua Goodman, who devised an efficient PCFG transform of the DOP1 model (Goodman, 1996).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Efficient Algorithms For Parsing The DOP Model</S><S sid = NA ssid = NA>In theory, the DOP model has several advantages over other models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W96-0214.txt | Citing Article:  P05-1010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Although we omit the details, we can prove the NP-hardness by observing that a stochastic tree substitution grammar (STSG) can be represented by a PCFG-LA model in a similar way to one described by Goodman (1996a), and then using the NP-hardness of STSG parsing (Sima'an, 2002).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Now, use these trees to form a Stochastic Tree Substitution Grammar (STSG).</S><S sid = NA ssid = NA>We call a PCFG derivation isomorphic to a STSG derivation if for every substitution in the STSG there is a corresponding subderivation in the PCFG.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W96-0214.txt | Citing Article:  D11-1008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Implicit grammars Goodman (1996, 2003 ) defined a transformation for some versions of DOP to an equivalent PCFG-based model, with the number of rules extracted from each parse tree linear in the size of the trees.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this paper, we introduce a reduction of the DOP model to an exactly equivalent Probabilistic Context Free Grammar (PCFG) that is linear in the number of nodes in the training data.</S><S sid = NA ssid = NA>The PCFG is equivalent in two senses: first it generates the same strings with the same probabilities; second, using an isomorphism defined below, it generates the same trees with the same probabilities, although one must sum over several PCFG trees for each STSG tree.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W96-0214.txt | Citing Article:  D09-1039.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Probabilities for the PCFG rules are computed monolingually as in the standard Goodman reduction for DOP (Goodman, 1996).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A formal derivation of a very similar algorithm is given elsewhere (Goodman, 1996); only the intuition is given here.</S><S sid = NA ssid = NA>There are eight cases, one for each of the eight rules.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W96-0214.txt | Citing Article:  P10-1112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The difference is that Goodman (1996a) collapses our BEGIN and END rules into the binary productions, giving a larger grammar which is less convenient for weighting.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>There are eight cases, one for each of the eight rules.</S><S sid = NA ssid = NA>When using a chart parser, as Bod did, three problematic cases must be handled: â‚¬ productions, unary productions, and n-ary (n> 2) productions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W96-0214.txt | Citing Article:  P10-1112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This research also shows the importance of testing on more than one small test set, as well as the importance of not making cross-corpus comparisons; if a new corpus is required, then previous algorithms should be duplicated for comparison.</S><S sid = NA ssid = NA>The construction is as follows.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W96-0214.txt | Citing Article:  P11-2127.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This research also shows the importance of testing on more than one small test set, as well as the importance of not making cross-corpus comparisons; if a new corpus is required, then previous algorithms should be duplicated for comparison.</S><S sid = NA ssid = NA>The construction is as follows.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W96-0214.txt | Citing Article:  P98-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The most probable parse can be estimated by iterative Monte Carlo sampling (Bod 1995), but efficient algorithms exist only for sub-optimal solutions such as the most likely derivation of a sentence (Bod 1995, Simaa'as; an 1995) or the labelled recall parse of a sentence (Goodman 1996).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Bod (1993c) shows how to approximate this most probable parse using a Monte Carlo algorithm.</S><S sid = NA ssid = NA>First, one can find the most probable derivation.</S> | Discourse Facet:  NA | Annotator: Automatic


