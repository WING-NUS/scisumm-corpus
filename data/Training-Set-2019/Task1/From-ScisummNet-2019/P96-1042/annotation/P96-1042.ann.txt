Citance Number: 1 | Reference Article:  P96-1042.txt | Citing Article:  W97-0201.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The first author gratefully acknowledges the support of the Fulbright Foundation.</S><S sid = NA ssid = NA>The maximum likelihood estimate for each of the multinomial's distribution parameters, ai, is & = .</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P96-1042.txt | Citing Article:  W03-0403.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Active learning has been successfully applied to a number of natural language oriented tasks, including text categorization (Lewis and Gale, 1994) and part-of-speech tagging (Engelson and Dagan, 1996).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Previous research in sample selection has used either sequential selection (Seung, Opper, and Sompolinsky, 1992; Freund et al., 1993; Dagan and Engelson, 1995), or batch selection (Lewis and Catlett, 1994; Lewis and Gale, 1994).</S><S sid = NA ssid = NA>The paper compares the performance of several instantiations of the general scheme, including a batch selection method similar to that of Lewis and Gale (1994).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P96-1042.txt | Citing Article:  W07-1516.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Engelson and Dagan (1996) experimented with QBC using HMMs for POS tagging and found that selective sampling of sentences can significantly reduce the number of samples required to achieve desirable tag accuracies.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We compare the amount of training required by different selection methods to achieve a given tagging accuracy on the test set, where both the amount of training and tagging accuracy are measured over ambiguous words.'</S><S sid = NA ssid = NA>In bigram part-of-speech tagging the HMM model M contains three types of parameters: transition probabilities P(tiâ€”t) giving the probability of tag tj occuring after tag ti, lexical probabilities P(t1w) giving the probability of tag t labeling word w, and tag probabilities P(t) giving the marginal probability of a tag occurring.2 The values of these parameters are estimated from a tagged corpus which provides a training set of labeled examples (see Section 4.1).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P96-1042.txt | Citing Article:  W09-1118.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In all experiments, the agreement among the decision committee members is quantified by the Vote Entropy measure (Engelson and Dagan, 1996).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Vote entropy is maximized when all committee members disagree, and is zero when they all agree.</S><S sid = NA ssid = NA>Our approach to measuring disagreement is to use the vote entropy, the entropy of the distribution of classifications assigned to an example ('voted for') by the committee members.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P96-1042.txt | Citing Article:  H05-1107.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Active learning also has been applied to many NLP applications, including POS tagging (Engelson and Dagan, 1996) and parsing (Baldridge and Osborne, 2003).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this paper, we extend our previous work (Dagan and Engelson, 1995) where we applied the basic idea of the committee-based approach to probabilistic classification.</S><S sid = NA ssid = NA>This basic approach gives rise to a family of algorithms (including the original algorithm described in (Dagan and Engelson, 1995)) which we then describe.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P96-1042.txt | Citing Article:  C08-1109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This method is similar in spirit to active learning ((Dagan and Engelson, 1995) and (Engelson and Dagan, 1996)), which has been used to iteratively build up an annotated corpus, but it differs from active learning applications in that there are no iterative loops between the system and the human annotator(s).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this paper, we investigate and extend the committee-based sample selection approach to minimizing training cost (Dagan and Engelson, 1995).</S><S sid = NA ssid = NA>The selection method we used in our earlier work (Dagan and Engelson, 1995) is randomized sequential selection using this linear selection probability model, with parameters k, t and g. An alternative to sequential selection is batch selection.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P96-1042.txt | Citing Article:  W07-1502.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>A common metric to estimate the disagreement within an ensemble is the so-called vote entropy, the entropy of the distribution of labels li assigned to an example e by the ensemble of k classifiers (Engelson and Dagan, 1996).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our approach to measuring disagreement is to use the vote entropy, the entropy of the distribution of classifications assigned to an example ('voted for') by the committee members.</S><S sid = NA ssid = NA>We define the selection probability as a linear function of vote entropy: p = gD, where g is an entropy gain parameter.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P96-1042.txt | Citing Article:  P08-1098.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al, 2007).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In statistical NLP, probabilistic classifiers are often used to select a preferred analysis of the linguistic structure of a text (for example, its syntactic structure (Black et al., 1993), word categories (Church, 1988), or word senses (Gale, Church, and Yarowsky, 1993)).</S><S sid = NA ssid = NA>Our results suggest applying committee-based sample selection to other statistical NLP tasks which rely on estimating probabilistic parameters from an annotated corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P96-1042.txt | Citing Article:  W99-0620.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Engelson and Dagan (1996) investigated several plausible approaches to the selection function but were unable to find significant differences among them.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The selection method we used in our earlier work (Dagan and Engelson, 1995) is randomized sequential selection using this linear selection probability model, with parameters k, t and g. An alternative to sequential selection is batch selection.</S><S sid = NA ssid = NA>We also show that sample selection yields a significant reduction in the size of the model used by the tagger.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P96-1042.txt | Citing Article:  D07-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>QBC is based on the idea to select those examples for manual annotation on which a committee of classifiers disagree most in their predictions (Engelson and Dagan, 1996).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this paper, we extend our previous work (Dagan and Engelson, 1995) where we applied the basic idea of the committee-based approach to probabilistic classification.</S><S sid = NA ssid = NA>Most simply, we can use a committee of size two and select an example when the two models disagree on its classification.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P96-1042.txt | Citing Article:  D07-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This is measured by the vote entropy (Engelson and Dagan, 1996), i.e., the entropy of the distribution of classifications assigned to an example by the classifiers.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our approach to measuring disagreement is to use the vote entropy, the entropy of the distribution of classifications assigned to an example ('voted for') by the committee members.</S><S sid = NA ssid = NA>We define the selection probability as a linear function of vote entropy: p = gD, where g is an entropy gain parameter.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P96-1042.txt | Citing Article:  D07-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Engelson and Dagan (1996) confirm this observation that, in general, different (and even more refined) selection methods still yield similar results.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We also find test set, comparable to other published results on bigram tagging. that, to a first approximation, all selection methods considered give similar results.</S><S sid = NA ssid = NA>A more general algorithm results from allowing (i) a larger number of committee members, k, in order to sample P(MIS) more precisely, and (ii) more refined example selection criteria.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P96-1042.txt | Citing Article:  C10-2143.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The first author gratefully acknowledges the support of the Fulbright Foundation.</S><S sid = NA ssid = NA>The maximum likelihood estimate for each of the multinomial's distribution parameters, ai, is & = .</S> | Discourse Facet:  NA | Annotator: Automatic


