Citance Number: 1 | Reference Article:  W02-0908.txt | Citing Article:  W02-1029.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Curran and Moens (2002b) evaluate thesaurus extractors based on several different models of context on large corpora.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our previous work (Curran and Moens, 2002) has evaluated thesaurus extraction performance and efficiency using several different context models.</S><S sid = NA ssid = NA>This approximation algorithm is dramatically faster than simple pairwise comparison, with only a small performance penalty, which means that complete thesaurus extraction on large corpora is now feasible.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W02-0908.txt | Citing Article:  W02-1029.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>All the systems use the JACCARD similarity metric and TTEST weighting function that were found to be most effective for thesaurus extraction by Curran and Moens (2002a).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Table 7 presents the example term results using the techniques we have described: JACCARD measure and TTEST weight functions; minimum cutoff of 30; and approximation algorithm with canonical vector size of 100 with TTESTLOG weighting.</S><S sid = NA ssid = NA>Vector-space thesaurus extraction systems can be separated into two components.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W02-0908.txt | Citing Article:  W02-1029.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Curran and Moens (2002b) have demonstrated that more complex and constrained contexts can yield superior performance, since the correlation between context and target term is stronger than simple window methods.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Some systems define the context as a window of words surrounding each thesaurus term (McDonald, 2000).</S><S sid = NA ssid = NA>Most systems extract co-occurrence and syntactic information from the words surrounding the target term, which is then converted into a vector-space representation of the contexts that each target term appears in (Pereira et al., 1993; Ruge, 1997; Lin, 1998b).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W02-0908.txt | Citing Article:  W09-0203.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We worked with an implementation of the log likelihood ratio (g-Score) as proposed by Dunning (1993) and two variants of the t-score, one considering all values (t-score) and one where only positive values (t-score+) are kept following the results of Curran and Moens (2002).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>1 5 + 128, and with at most 200 synonyms, the maximum INVR score is 5.878.</S><S sid = NA ssid = NA>As an example, with a maximum cutoff of 10,000 and a canonical vector size of 70, the total DIRECT score of 1841 represents a 3.9% performance penalty over full extraction, for an 89% reduction in execution time.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W02-0908.txt | Citing Article:  W06-1708.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Another venue of research may be to exploit different thesauri, such as the ones automatically derived as in (Curran and Moens, 2002).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our previous work (Curran and Moens, 2002) has evaluated thesaurus extraction performance and efficiency using several different context models.</S><S sid = NA ssid = NA>There is a clear need for methods to extract thesauri automatically or tools that assist in the manual creation and updating of these semantic resources.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W02-0908.txt | Citing Article:  P06-2111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Several researchers (Curran and Moens (2002), Lin (1998), van der Plas and Bouma (2005)) have used large monolingual corpora to extract distributionally similar words.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Many systems extract grammatical relations using either a broad coverage parser (Lin, 1998a) or shallow statistical tools (Grefenstette, 1994; Curran and Moens, 2002).</S><S sid = NA ssid = NA>Early experiments in thesaurus extraction (Grefenstette, 1994) suffered from the limited size of available corpora, but more recent experiments have used much larger corpora with greater success (Lin, 1998a).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W02-0908.txt | Citing Article:  P06-2111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Monolingual syntax-based distributional similarity is used in many proposals to find semantically related words (Curran and Moens (2002), Lin (1998), van der Plas and Bouma (2005)).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Other measures, such as LIN and JACCARD have previously been used for thesaurus extraction (Lin, 1998a; Grefenstette, 1994).</S><S sid = NA ssid = NA>Alternatively, some systems are based on the observation that related terms appear together in particular contexts.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W02-0908.txt | Citing Article:  P06-2111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Curran and Moens (2002) report on a large scale evaluation experiment, where they evaluated the performance of various commonly used methods.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our previous work (Curran and Moens, 2002) has evaluated thesaurus extraction performance and efficiency using several different context models.</S><S sid = NA ssid = NA>We would also like to expand our evaluation to include direct methods used by others (Lin, 1998a) and using the extracted thesaurus in NLP tasks.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W02-0908.txt | Citing Article:  P06-2111.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Vander Plas and Bouma (2005) present a similar experiment for Dutch, in which they tested most of the best performing measures according to Curran and Moens (2002).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The best performance across all measures was shared by JACCARD and DICE†, which produced identical results for the 70 words.</S><S sid = NA ssid = NA>Finally, we have generalised some set measures using similar reasoning to Grefenstette (1994).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W02-0908.txt | Citing Article:  P06-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Curran and Moens (2002) show that synonymy extraction for lexical semantic resources using distributional similarity produces continuing gains in accuracy as the volume of input data increases.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The use of semantic resources is comin modern but methods to extract lexical semantics have only recently begun to perform well enough for practical use.</S><S sid = NA ssid = NA>Our previous work (Curran and Moens, 2002) has evaluated thesaurus extraction performance and efficiency using several different context models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W02-0908.txt | Citing Article:  P06-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Curran and Moens (2002) introduces a vector of canonical attributes (of bounded length k m), selected from the full vector, to represent the term.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The canonical vector must contain attributes that best describe the thesaurus term in a bounded number of entries.</S><S sid = NA ssid = NA>We can do this by introducing another, much shorter vector of canonical attributes, with a bounded length k. If our approximate comparison returns at most p positive results for each term, then the time complexity becomes O(n2k + npm), which, since k is constant, is O(n2 + npm).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W02-0908.txt | Citing Article:  P06-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Comparisons made with these low frequency terms are unreliable (Curran and Moens, 2002).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Introducing a minimum cutoff that ignores low frequency potential synonyms can eliminate many unnecessary comparisons.</S><S sid = NA ssid = NA>In fact, using a cutoff increases the average value of m across the terms because it removes low frequency terms with few attributes.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W02-0908.txt | Citing Article:  W05-1202.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Recently, there has been much interest in finding words which are distribution ally similar e.g., Lin (1998), Lee (1999), Curran and Moens (2002), Weeds (2003) and Geffet and Dagan (2004).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Many systems extract grammatical relations using either a broad coverage parser (Lin, 1998a) or shallow statistical tools (Grefenstette, 1994; Curran and Moens, 2002).</S><S sid = NA ssid = NA>Our previous work (Curran and Moens, 2002) has evaluated thesaurus extraction performance and efficiency using several different context models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W02-0908.txt | Citing Article:  W09-1706.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In these experiments we have used a variant of Dice, proposed by Curran and Moens (2002).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In these experiments we have proposed new measure and weight functions that, as our evaluation has shown, significantly outperform existing similarity functions.</S><S sid = NA ssid = NA>Our previous work (Curran and Moens, 2002) has evaluated thesaurus extraction performance and efficiency using several different context models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W02-0908.txt | Citing Article:  D09-1089.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Pereira et al (1993), Curran and Moens (2002) and Lin (1998) use syntactic features in the vector definition.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Most systems extract co-occurrence and syntactic information from the words surrounding the target term, which is then converted into a vector-space representation of the contexts that each target term appears in (Pereira et al., 1993; Ruge, 1997; Lin, 1998b).</S><S sid = NA ssid = NA>This consists of four passes over the sentence, associating each noun with the modifiers and verbs from the syntactic contexts they appear in: The relation tuple is then converted to root form using the Sussex morphological analyser (Minnen et al., 2000) and the POS tags are removed.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W02-0908.txt | Citing Article:  P08-3001.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Also, because it has been shown (Curran and Moens, 2002) that negative PMI values worsen the distributional similarity performance, we bound PMI so that PMI (wi, cj)= 0 if PMI (wi, cj) < 0.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our previous work (Curran and Moens, 2002) has evaluated thesaurus extraction performance and efficiency using several different context models.</S><S sid = NA ssid = NA>In these experiments we have proposed new measure and weight functions that, as our evaluation has shown, significantly outperform existing similarity functions.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W02-0908.txt | Citing Article:  P06-1038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>A first major algorithmic approach is to represent word contexts as vectors in some space and use similarity measures and automatic clustering in that space (Curran and Moens, 2002).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Vector-space thesaurus extraction systems can be separated into two components.</S><S sid = NA ssid = NA>Much of the existing work on thesaurus extraction and word clustering is based on the observation that related terms will appear in similar contexts.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W02-0908.txt | Citing Article:  W05-1011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Curran and Moens (2002b) have demonstrated that dramatically increasing the quantity of text used to extract contexts significantly improves synonym quality.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The first component extracts the contexts from raw text and compiles them into a statistical description of the contexts each potential thesaurus term appears in.</S><S sid = NA ssid = NA>Each measure is averaged over the extracted synonym lists for all 70 thesaurus terms.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W02-0908.txt | Citing Article:  W05-1011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For these experiments we use the JACCARD (1) measure and the TTEST (2) weight, as Curran and Moens (2002a) found them to have the best performance in their comparison of many distance measures.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Therefore, we have evaluated the weight functions using the JACCARD measure, and evaluated the measure functions using the TTEST weight because they produced the best results in our previous experiments.</S><S sid = NA ssid = NA>The best performance across all measures was shared by JACCARD and DICE†, which produced identical results for the 70 words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W02-0908.txt | Citing Article:  W05-1011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Curran and Moens (2002a) propose an initial heuristic comparison to reduce the number of full O(m) vector comparisons.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>However, what is needed is some algorithmic reduction that bounds the number of full O(m) vector comparisons performed.</S><S sid = NA ssid = NA>Curran and Moens (2002)), which would increase both number of attributes for each term and the total number of terms above the minimum cutoff, this is not nearly fast enough.</S> | Discourse Facet:  NA | Annotator: Automatic


