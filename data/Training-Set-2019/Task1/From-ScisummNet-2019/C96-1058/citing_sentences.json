[
  {
    "citance_No": 1, 
    "citing_paper_id": "P14-2107", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Hao, Zhang | Ryan, McDonald", 
    "raw_text": "In 656 (a) l= l+ l 1 (b) l= l 1+ l 2 Figure 2: Structures and rules for parsing with the (Eisner, 1996) algorithm", 
    "clean_text": "Structures and rules for parsing with the (Eisner, 1996) algorithm.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P14-2107", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Hao, Zhang | Ryan, McDonald", 
    "raw_text": "Inthat work, as here, inference is simply the Eisner first-order parsing model (Eisner, 1996) shown in Figure 2. In order to score higher-order features, each chart item maintains a list of signatures, which represent subtrees consistent with the chart item", 
    "clean_text": "In that work, as here, inference is simply the Eisner first-order parsing model (Eisner, 1996) shown in Figure 2. In order to score higher-order features, each chart item maintains a list of signatures, which represent subtrees consistent with the chart item.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P01-1010", 
    "citing_paper_authority": 17, 
    "citing_paper_authors": "Rens, Bod", 
    "raw_text": "context-free rulesCharniak (1996) Collins (1996), Eisner (1996) context-free rules, headwords Charniak (1997) context-free rules, headwords, grandparent nodes Collins (2000) context-free rules, headwords, grandparent nodes/rules, bi grams, two-level rules, two-level bi grams, non headwords Bod (1992) all fragments within parse trees Scope of Statistical Dependencies Model Figure 4", 
    "clean_text": "context-free rules Charniak (1996) Collins (1996), Eisner (1996) context-free rules, headwords Charniak (1997) context-free rules, headwords, grandparent nodes Collins (2000) context-free rules, headwords, grandparent nodes/rules, bi grams, two-level rules, two-level bi grams, non headwords Bod (1992) all fragments within parse trees Scope of Statistical Dependencies Model Figure 4.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "P09-1042", 
    "citing_paper_authority": 28, 
    "citing_paper_authors": "Kuzman, Ganchev | Jennifer, Gillenwater | Ben, Taskar", 
    "raw_text": "Viterbi decoding is done using Eisner? s algorithm (Eisner, 1996) .We also used a generative model based on dependency model with valence (Klein and Manning, 2004)", 
    "clean_text": "Viterbi decoding is done using Eisner's algorithm (Eisner, 1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "D07-1070", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "David A., Smith | Jason M., Eisner", 
    "raw_text": "1 of these edges, using an O (n3) dynamic programming algorithm (Eisner, 1996) for projective trees", 
    "clean_text": "1 of these edges, using an O (n3) dynamic programming algorithm (Eisner, 1996) for projective trees.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "D07-1070", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "David A., Smith | Jason M., Eisner", 
    "raw_text": "But the denominator Zi is a normalizing constant that sums over all parses; it is found by a dependency-parsing variant of the inside algorithm, following (Eisner, 1996)", 
    "clean_text": "But the denominator Zi is a normalizing constant that sums over all parses; it is found by a dependency-parsing variant of the inside algorithm, following (Eisner, 1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "D09-1059", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Richard, Johansson", 
    "raw_text": "In a slightly more general formulation, it was first published by Eisner (1996)", 
    "clean_text": "In a slightly more general formulation, it was first published by Eisner (1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "P09-1043", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Yi, Zhang | Rui, Wang", 
    "raw_text": "ForMSTParser, we use 1st order features and a projective decoder (Eisner, 1996) .When incorporating HPSG features, two set tings are used", 
    "clean_text": "For MST Parser, we use 1st order features and a projective decoder (Eisner, 1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "W08-2124", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Xavier, Llu&iacute;s | Llu&iacute;s, M&agrave;rquez", 
    "raw_text": "It combines on line Peceptron learning (Collins, 2002) with a parsing model based on the Eisner algorithm (Eisner, 1996), extended so asto jointly assign syntactic and semantic labels", 
    "clean_text": "It combines online Peceptron learning (Collins, 2002) with a parsing model based on the Eisner algorithm (Eisner, 1996), extended so as to jointly assign syntactic and semantic labels.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "N10-1145", 
    "citing_paper_authority": 18, 
    "citing_paper_authors": "Michael, Heilman | Noah A., Smith", 
    "raw_text": "The child nodes for a given parent are represented in a head-outward fashion such that the left and right children are separate lists, with the left and right-most elements as the last members of their respective lists, as in most generative dependency models (Eisner, 1996)", 
    "clean_text": "The child nodes for a given parent are represented in a head-outward fashion such that the left and right children are separate lists, with the left and right-most elements as the last members of their respective lists, as in most generative dependency models (Eisner, 1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "D11-1116", 
    "citing_paper_authority": 17, 
    "citing_paper_authors": "Stephen, Tratz | Eduard, Hovy", 
    "raw_text": "? Eisner (1996) algorithm with non-projective rewriting and second order features", 
    "clean_text": "Eisner (1996) algorithm with non-projective rewriting and second order features.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "D11-1116", 
    "citing_paper_authority": 17, 
    "citing_paper_authors": "Stephen, Tratz | Eduard, Hovy", 
    "raw_text": "Examples of this include McDonald and Pereira? s (2006) rewriting of projective trees produced by the Eisner (1996) algorithm, and Nivre and Nilsson? s (2005) pseudo projective approach that creates projective trees with specially marked arcs that are later transformed into non-projective dependencies. Descriptive dependency labels", 
    "clean_text": "Examples of this include McDonald and Pereira's (2006) rewriting of projective trees produced by the Eisner (1996) algorithm, and Nivre and Nilsson's (2005) pseudo projective approach that creates projective trees with specially marked arcs that are later transformed into non-projective dependencies. Descriptive dependency labels.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "D07-1100", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Tetsuji, Nakagawa", 
    "raw_text": "Thebest projective parse tree is obtained using the Eisner algorithm (Eisner, 1996) with the scores, and the best non-projective one is obtained using the Chu Liu-Edmonds (CLE) algorithm (McDonald et al, 2005b)", 
    "clean_text": "The best projective parse tree is obtained using the Eisner algorithm (Eisner, 1996) with the scores, and the best non-projective one is obtained using the Chu Liu-Edmonds (CLE) algorithm (McDonald et al, 2005b).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "H05-1066", 
    "citing_paper_authority": 186, 
    "citing_paper_authors": "Ryan, McDonald | Fernando, Pereira | Kiril, Ribarov | Jan, Haji&#x10D;", 
    "raw_text": "Using this representation, the parsing algorithm of Eisner (1996) is sufficient for searching over all projective trees in O (n3) time", 
    "clean_text": "Using this representation, the parsing algorithm of Eisner (1996) is sufficient for searching over all projective trees in O (n3) time.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "H05-1066", 
    "citing_paper_authority": 186, 
    "citing_paper_authors": "Ryan, McDonald | Fernando, Pereira | Kiril, Ribarov | Jan, Haji&#x10D;", 
    "raw_text": "Most previous dependency parsing models have focused on projective trees, including the work of Eisner (1996), Collins et al (1999), Yamada and Matsumoto (2003), Nivre and Scholz (2004), and McDonald et al (2005)", 
    "clean_text": "Most previous dependency parsing models have focused on projective trees, including the work of Eisner (1996), Collins et al (1999), Yamada and Matsumoto (2003), Nivre and Scholz (2004), and McDonald et al (2005).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "H05-1066", 
    "citing_paper_authority": 186, 
    "citing_paper_authors": "Ryan, McDonald | Fernando, Pereira | Kiril, Ribarov | Jan, Haji&#x10D;", 
    "raw_text": "This formalization generalizes standard projective parsing mod els based on the Eisner algorithm (Eisner, 1996) to yield efficient O (n2) exact parsing methods for non projective languages like Czech", 
    "clean_text": "This formalization generalizes standard projective parsing models based on the Eisner algorithm (Eisner, 1996) to yield efficient O (n2) exact parsing methods for non projective languages like Czech.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "H05-1066", 
    "citing_paper_authority": 186, 
    "citing_paper_authors": "Ryan, McDonald | Fernando, Pereira | Kiril, Ribarov | Jan, Haji&#x10D;", 
    "raw_text": "2.2.2 Projective TreesIt is well known that projective dependency parsing using edge based factorization can be handled with the Eisner algorithm (Eisner, 1996)", 
    "clean_text": "It is well known that projective dependency parsing using edge based factorization can be handled with the Eisner algorithm (Eisner, 1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "P11-2125", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Gholamreza, Haffari | Marzieh, Razavi | Anoop, Sarkar", 
    "raw_text": "The (Eisner, 1996) algorithm is typically used for projective parsing", 
    "clean_text": "The (Eisner, 1996) algorithm is typically used for projective parsing.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "P11-2125", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Gholamreza, Haffari | Marzieh, Razavi | Anoop, Sarkar", 
    "raw_text": "In this paper, we work with both first-order and second-order models, we train the models using MIRA, and we use the (Eisner, 1996) algorithm for inference", 
    "clean_text": "In this paper, we work with both first-order and second-order models, we train the models using MIRA, and we use the (Eisner, 1996) algorithm for inference.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "W09-1212", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Xavier, Llu&iacute;s | Stefan, Bott | Llu&iacute;s, M&agrave;rquez", 
    "raw_text": "The Eisner (1996) algorithm and its variants are commonly used in data-driven dependency parsing", 
    "clean_text": "The Eisner (1996) algorithm and its variants are commonly used in data-driven dependency parsing.", 
    "keep_for_gold": 0
  }
]