Citance Number: 1 | Reference Article:  W01-0501.txt | Citing Article:  P14-2021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Similarly, in a case study on co-training for natural language tasks, Pierce and Cardie (2001) find that the degradation in data quality from automatic labelling prevents these systems from performing comparably to their fully-supervised counterparts.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This case study of co-training for natural language learning addresses the scalability question using the task of base noun phrase identification.</S><S sid = NA ssid = NA>Cardie et al. (2000)) systems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W01-0501.txt | Citing Article:  D07-1051.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Pierce and Cardie (2001) have shown, however, that for tasks which require large numbers of labeled examples such as most NLP tasks co training might be inadequate because it tends to generate noisy data.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This study examines the learning behavior of co-training on natural language processing tasks that typically require large numbers of training instances to achieve usable performance levels.</S><S sid = NA ssid = NA>In particular, many natural language learning tasks contrast sharply with the classification tasks previously studied in conjunction with co-training in that they require hundreds of thousands, rather than hundreds, of training examples.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W01-0501.txt | Citing Article:  N06-2014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We observed the same pattern in co-training; its accuracy peaked after two iterations (85.1%) and then performance degraded drastically (68% after five iterations) due in part to an increase in mislabeled data in the training set (as previously observed in (Pierce and Cardie, 2001)) and in part because the data skew is not controlled for.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This enables bootstrapping from a small set of labeled training data via a large set of unlabeled data.</S><S sid = NA ssid = NA>A straightforward solution to this problem is to have a human anized, as co-training achieves 95.03% accuracy, just 0.14% away from the goal, after 600 iterations (and reaches 95.12% after 800 iterations).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W01-0501.txt | Citing Article:  W03-0407.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For naive co-training, new samples will always be added, and so there is a possibility that the noise accumulated at later stages will start to degrade performance (see Pierce and Cardie (2001)).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We leave evaluation of this possibility to future work.</S><S sid = NA ssid = NA>Finally, we note that the accuracy of automatically accumulated training data is an important issue for many bootstrapping learning methods (e.g.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W01-0501.txt | Citing Article:  W03-0407.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The opposite behaviour has been observed in other applications of co-training (Pierce and Cardie, 2001).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Cardie and Pierce (1998)), memory-based sequence learning (e.g.</S><S sid = NA ssid = NA>Co-Training.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W01-0501.txt | Citing Article:  W03-1015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This work was supported in part by DARPA TIDES contract N66001-00-C-8009, and NSF Grants 9454149, 0081334, and 0074896.</S><S sid = NA ssid = NA>Many NLL tasks contrast in two ways with the web page classification task studied in previous work on co-training.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W01-0501.txt | Citing Article:  W03-1015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The drop in F-measure is potentially due to the pollution of the labeled data by mislabeled instances (Pierce and Cardie, 2001).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For initial labeled data, the first L instances of the training data are given their correct labels.</S><S sid = NA ssid = NA>This procedure preserves the distribution of labels in the labeled data as instances are labeled and added.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W01-0501.txt | Citing Article:  W07-1502.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>However, it has been shown (Pierce and Cardie, 2001) that semi-supervised learning is brittle for NLP tasks where typically large amounts of high quality annotations are needed to train appropriate classifiers.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>On one hand, the goal of weakly supervised learning is to bootstrap a classifier from small amounts of labeled data and large amounts of unlabeled data, often by automatically labeling some of the unlabeled data.</S><S sid = NA ssid = NA>This study examines the learning behavior of co-training on natural language processing tasks that typically require large numbers of training instances to achieve usable performance levels.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W01-0501.txt | Citing Article:  W06-2209.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The use of the disagreement between taggers for selecting candidates for manual correction is reminiscent of corrected co-training (PierceandCardie, 2001).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Corrected Co-Training.</S><S sid = NA ssid = NA>This neatly dovetails with the criterion for selecting instances to label in CT. We envision a learner that would alternate between selecting its most certain unlabeled examples to label and present to the human for acknowledgment, and selecting its most uncertain examples to present to the human for annotation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W01-0501.txt | Citing Article:  P03-1042.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For other work on co-training, see (Muslea et al 200; Pierce and Cardie 2001).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Cardie et al. (2000)) systems.</S><S sid = NA ssid = NA>Argamon et al. (1999)), and memory-based learning (e.g.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W01-0501.txt | Citing Article:  W04-2405.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The fact that no improvement was obtained agrees with previous observations that classifiers that are too accurate can not be improved with bootstrapping (Pierce and Cardie,2001).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For example, with L = 200 the co-training classifiers appear not to be accurate enough to sustain co-training, while with L = 1000, they are too accurate, in the sense that co-training contributes very little accuracy before the labeled data deteriorates (Figure 5).</S><S sid = NA ssid = NA>Cardie and Pierce (1998)), memory-based sequence learning (e.g.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W01-0501.txt | Citing Article:  W04-2405.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>A comparative analysis of words that benefit from basic/smoothed co-training with global parameter settings, versus words with little or no improvement obtained through bootstrapping reveals several observations: (1) Words with accurate basic classifiers can not be improved through co-training, which agrees with previous observations (Pierce and Cardie, 2001).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Words to the left and right of the focus word are included for context.</S><S sid = NA ssid = NA>For this task, they suggest the following two views: (1) the words contained in the text of the page; for example, research interests or publications; (2) the words contained in links pointing to the page; for example, my advisor.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W01-0501.txt | Citing Article:  W09-2208.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Algorithms such as co-training (Blum and Mitchell, 1998) (Collins and Singer, 1999) (Pierce and Cardie, 2001) and the Yarowsky algorithm (Yarowsky, 1995) make assumptions about the data that permit such an approach.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Yarowsky (1995), Riloff and Jones (1999)), suggesting that the rewards of understanding and dealing with this issue may be significant.</S><S sid = NA ssid = NA>Collins and Singer (1999) were concerned that the CT algorithm does not strongly enforce the requirement that hypothesis functions should be compatible with the unlabeled data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W01-0501.txt | Citing Article:  W03-0310.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The table further indicates that co-training for machine translation suffers the same problem reported in Pierce and Cardie (2001): gains above the accuracy of the initial corpus are achieved, but decline as after acer tain number of machine translations are added to the training set.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Co-Training.</S><S sid = NA ssid = NA>This enables bootstrapping from a small set of labeled training data via a large set of unlabeled data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W01-0501.txt | Citing Article:  P09-1117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Pierce and Cardie (2001) showed that the quality of the automatically labeled training data is crucial for co-training to perform well because too many tagging errors prevent a high performing model from being learned.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Furthermore, our experiments support the hypothesis that labeled data quality is a crucial issue for co-training.</S><S sid = NA ssid = NA>Co-Training.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W01-0501.txt | Citing Article:  P09-1117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>To address the problem of data pollution by tagging errors, Pierce and Cardie (2001) propose corrected co-training.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We address this problem with a moderately supervised variant, corrected co-training, that employs a human annotator to correct the errors made during bootstrapping.</S><S sid = NA ssid = NA>Corrected Co-Training.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W01-0501.txt | Citing Article:  P09-1117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Thus, these bootstrapping methods will presumably not find the most useful unlabeled examples but require a human to review data points of limited training utility (Pierce and Cardie, 2001).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Finally, we note that the accuracy of automatically accumulated training data is an important issue for many bootstrapping learning methods (e.g.</S><S sid = NA ssid = NA>This enables bootstrapping from a small set of labeled training data via a large set of unlabeled data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W01-0501.txt | Citing Article:  N03-1031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This work was supported in part by DARPA TIDES contract N66001-00-C-8009, and NSF Grants 9454149, 0081334, and 0074896.</S><S sid = NA ssid = NA>Many NLL tasks contrast in two ways with the web page classification task studied in previous work on co-training.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W01-0501.txt | Citing Article:  N03-1031.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In their experiments on NP identifiers, Pierce and Cardie (2001) observed a similar effect.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>To apply co-training, the base NP classification task must first be factored into views.</S><S sid = NA ssid = NA>Cardie and Pierce (1998)), memory-based sequence learning (e.g.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W01-0501.txt | Citing Article:  H05-1107.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Co-training has been applied to a number of NLP applications ,including POS-tagging (Clark et al, 2003), parsing (Sarkar, 2001), word sense disambiguation (Mihalcea, 2004), and base noun phrase detection (Pierce and Cardie, 2001).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Cardie et al. (2000)) systems.</S><S sid = NA ssid = NA>Base noun phrase identification is a crucial component of systems that employ partial syntactic analysis, including information retrieval (e.g.</S> | Discourse Facet:  NA | Annotator: Automatic


