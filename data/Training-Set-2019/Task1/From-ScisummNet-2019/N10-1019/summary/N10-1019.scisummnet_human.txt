Using Mostly Native Data to Correct Errors in Learners&rsquo; Writing
We present results from a range of experiments on article and preposition error correction for non-native speakers of English.
We first compare a language model and error-specific classifiers (all trained on large English corpora) with respect to their performance in error detection and correction.
We then combine the language model and the classifiers in a meta-classification approach by combining evidence from the classifiers and the language model as input features to the meta-classifier.
The meta-classifier in turn is trained on error-annotated learner data, optimizing the error detection and correction performance on this domain.
The meta-classification approach results in substantial gains over the classifier-only and language-model-only scenario.
Since the meta-classifier requires error-annotated data for training, we investigate how much training data is needed to improve results over the baseline of not using a meta-classifier.
All evaluations are conducted on a large error-annotated corpus of learner English.
We remove sentences from the data where some other error appears immediately next to a preposition or determiner error.
