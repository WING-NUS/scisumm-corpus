[
  {
    "citance_No": 1, 
    "citing_paper_id": "P02-1034", 
    "citing_paper_authority": 111, 
    "citing_paper_authors": "Michael John, Collins | Nigel, Duffy", 
    "raw_text": "We used a feature set which included the current, next, and previous word; the previous two tags; various capitalization and other features of the word being tagged (the full feature set is described in (Collins 2002a))", 
    "clean_text": "We used a feature set which included the current, next, and previous word; the previous two tags; various capitalization and other features of the word being tagged (the full feature set is described in (Collins 2002a)).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P02-1034", 
    "citing_paper_authority": 111, 
    "citing_paper_authors": "Michael John, Collins | Nigel, Duffy", 
    "raw_text": "In contrast, the parameter estimation methods in this paper have a strong theoretical basis (see (Cristianini and Shawe-Taylor 2000) chapter 2 and (Freund& amp; Schapire 1999) for statistical theory underlying the perceptron) .For related work on the voted perceptron algorithm applied to NLP problems, see (Collins 2002a) and (Collins 2002b)", 
    "clean_text": "For related work on the voted perceptron algorithm applied to NLP problems, see (Collins 2002a) and (Collins 2002b).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P02-1034", 
    "citing_paper_authority": 111, 
    "citing_paper_authors": "Michael John, Collins | Nigel, Duffy", 
    "raw_text": "(Collins 2002a) describes experiments on the same named-entity dataset as in this paper, but using explicit features rather than kernels", 
    "clean_text": "(Collins 2002a) describes experiments on the same named-entity dataset as in this paper, but using explicit features rather than kernels.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "P02-1034", 
    "citing_paper_authority": 111, 
    "citing_paper_authors": "Michael John, Collins | Nigel, Duffy", 
    "raw_text": "(Collins 2002b) describes how the voted per ceptron can be used to train maximum-entropy style taggers, and also gives a more thorough discussion of the theory behind the perceptron algorithm applied to ranking tasks", 
    "clean_text": "(Collins 2002b) describes how the voted perceptron can be used to train maximum-entropy style taggers, and also gives a more thorough discussion of the theory behind the perceptron algorithm applied to ranking tasks.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P14-1021", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Wenduan, Xu | Stephen, Clark | Yue, Zhang", 
    "raw_text": "We use the averaged perceptron (Collins, 2002) to train a global linear model and score each action", 
    "clean_text": "We use the averaged perceptron (Collins, 2002) to train a global linear model and score each action.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "H05-1059", 
    "citing_paper_authority": 39, 
    "citing_paper_authors": "Yoshimasa, Tsuruoka | Jun'ichi, Tsujii", 
    "raw_text": "We split the Penn Treebank corpus (Marcus et al, 1994) into training, development and test sets as in (Collins, 2002)", 
    "clean_text": "We split the Penn Treebank corpus (Marcus et al, 1994) into training, development and test sets as in (Collins, 2002).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "H05-1059", 
    "citing_paper_authority": 39, 
    "citing_paper_authors": "Yoshimasa, Tsuruoka | Jun'ichi, Tsujii", 
    "raw_text": "Networks (Toutanova et al, 2003) 97.24 Perceptron (Collins, 2002) 97.11 SVM (Gimenez and Marquez, 2003) 97.05 HMM (Brants, 2000) 96.48 Easiest-first 97.10 Full Bidirectional 97.15Table 3: POS tagging accuracy on the test set (Sections 22-24 of the WSJ, 5462 sentences) .provided by (Toutanova et al, 2003) except for complex features such as crude company-name detection features because they are specific to the Penn Tree bank and we could not find the exact implementation details", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "H05-1059", 
    "citing_paper_authority": 39, 
    "citing_paper_authors": "Yoshimasa, Tsuruoka | Jun'ichi, Tsujii", 
    "raw_text": "Aperceptron algorithm gives 97.11% (Collins, 2002)", 
    "clean_text": "A perceptron algorithm gives 97.11% (Collins, 2002).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "H05-1059", 
    "citing_paper_authority": 39, 
    "citing_paper_authors": "Yoshimasa, Tsuruoka | Jun'ichi, Tsujii", 
    "raw_text": "sabine/chunklink/ Current word wi& amp; ti Previous word wi? 1& amp; ti Word two back wi? 2& amp; ti Next word wi+1& amp; ti Word two ahead wi+2& amp; ti Bigram features wi? 2 ,wi? 1& amp; ti wi? 1 ,wi& amp; ti wi ,wi+1& amp; ti wi+1 ,wi+2& amp; ti Current POS pi& amp; ti Previous POS pi? 1& amp; ti POS two back pi? 2& amp; ti Next POSpi+1& amp; ti POS two ahead pi+2& amp; ti Bigram POS features pi? 2, pi? 1& amp; ti pi? 1, pi& amp; ti pi ,pi+1& amp; ti pi+1 ,pi+2& amp; ti Trigram POS features pi? 2, pi? 1, pi& amp; ti pi? 1, pi ,pi+1& amp; ti pi ,pi+1 ,pi+2& amp; ti Previous tag ti? 1& amp; ti Tag two back ti? 2& amp; ti Next tag ti+1& amp; ti Tag two ahead ti+2& amp; ti Bigram tag features ti? 2, ti? 1& amp; ti ti? 1 ,ti+1& amp; ti ti+1 ,ti+2& amp; ti Table 4: Feature templates used in chunking experiments. (Collins, 2002) and used POS-trigrams as well", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "P12-2001", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Xiao, Chen | Chunyu, Kit", 
    "raw_text": "NP VP where g (t, s) is a scoring function to evaluate the event that t is the parse of s. Following Collins (2002), this scoring function is formulated in the linear form g (t, s)=??? (t, s), (2) where? (t, s) is a vector of features and? the vector of their associated weights", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "P12-2001", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Xiao, Chen | Chunyu, Kit", 
    "raw_text": "Theparameters? of each parsing model are estimated from a training set using an averaged perceptron algorithm, following Collins (2002) and Huang (2008)", 
    "clean_text": "Theparameters? of each parsing model are estimated from a training set using an averaged perceptron algorithm, following Collins (2002) and Huang (2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "N10-1085", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Aaron, Dunlop | Margaret, Mitchell | Brian, Roark", 
    "raw_text": "Since we have no gold-standard MSAs, we instead align the ordered NPs with the current model and treat the least cost alignment of the correct ordering as the reference for training. We trained this model using the averaged perceptron algorithm (Collins, 2002)", 
    "clean_text": "We trained this model using the averaged perceptron algorithm (Collins, 2002).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "C08-1049", 
    "citing_paper_authority": 14, 
    "citing_paper_authors": "Wenbin, Jiang | Haitao, Mi | Qun, Liu", 
    "raw_text": "We adopt the perceptron algorithm (Collins, 2002) to train the re ranker", 
    "clean_text": "We adopt the perceptron algorithm (Collins, 2002) to train the re-ranker.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "C08-1049", 
    "citing_paper_authority": 14, 
    "citing_paper_authors": "Wenbin, Jiang | Haitao, Mi | Qun, Liu", 
    "raw_text": "of Collins (2002) to alleviate overfittingon the training corpus and obtain more stable performance", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "C08-1049", 
    "citing_paper_authority": 14, 
    "citing_paper_authors": "Wenbin, Jiang | Haitao, Mi | Qun, Liu", 
    "raw_text": "Collins (2002)? s perceptron training algorithm were adopted again, to learn a discriminative classifier, mapping from inputs x? X to outputs y? Y. Here x is a character sequence, and y is the sequence of classification result of each character in x. For segmentation, the classification result is a positional tag, while for Joint S& amp; T, it is an extended tag with POS information", 
    "clean_text": "Collins (2002)'s perceptron training algorithm were adopted again, to learn a discriminative classifier, mapping from inputs x X to outputs y Y.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "N03-1028", 
    "citing_paper_authority": 136, 
    "citing_paper_authors": "Fei, Sha | Fernando, Pereira", 
    "raw_text": "The generalized perceptron proposed by Collins (2002) is closely related to CRFs, but the best CRF training methods seem to have a slight edge over the generalized perceptron", 
    "clean_text": "The generalized perceptron proposed by Collins (2002) is closely related to CRFs, but the best CRF training methods seem to have a slight edge over the generalized perceptron.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "N03-1028", 
    "citing_paper_authority": 136, 
    "citing_paper_authors": "Fei, Sha | Fernando, Pereira", 
    "raw_text": "We compare those algorithms to generalized iterative scaling (GIS) (Darroch and Ratcliff, 1972), non-preconditioned CG, and voted perceptron training (Collins, 2002)", 
    "clean_text": "We compare those algorithms to generalized iterative scaling (GIS) (Darroch and Ratcliff, 1972), non-preconditioned CG, and voted perceptron training (Collins, 2002).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "N03-1028", 
    "citing_paper_authority": 136, 
    "citing_paper_authors": "Fei, Sha | Fernando, Pereira", 
    "raw_text": "Unlike other methods discussed so far, voted perceptrontraining (Collins, 2002) attempts to minimize the difference between the global feature vector for a training in stance and the same feature vector for the best-scoring labeling of that instance according to the current model", 
    "clean_text": "Unlike other methods discussed so far, voted perceptron training (Collins, 2002) attempts to minimize the difference between the global feature vector for a training instance and the same feature vector for the best-scoring labeling of that instance according to the current model.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "N03-1028", 
    "citing_paper_authority": 136, 
    "citing_paper_authors": "Fei, Sha | Fernando, Pereira", 
    "raw_text": "Instead of taking just the final weight vector, the voted perceptronalgorithm takes the average of the? t. Collins (2002) reported and we confirmed that this averaging reduces over fitting considerably", 
    "clean_text": "Collins (2002) reported and we confirmed that this averaging reduces over fitting considerably.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "N03-1028", 
    "citing_paper_authority": 136, 
    "citing_paper_authors": "Fei, Sha | Fernando, Pereira", 
    "raw_text": "Minor variants support voted perceptron (Collins, 2002) and MEMMs (McCallum et al, 2000) with the same efficient feature encoding", 
    "clean_text": "Minor variants support voted perceptron (Collins, 2002) and MEMMs (McCallum et al, 2000) with the same efficient feature encoding.", 
    "keep_for_gold": 0
  }
]