Intelligent Selection of Language Model Training Data
We address the problem of selecting non-domain-specific language model training data to build auxiliary language models for use in tasks such as machine translation.
Our approach is based on comparing the cross-entropy, according to domain-specific and non-domain-specifc language models, for each sentence of the text source used to produce the latter language model.
We show that this produces better language models, trained on less data, than both random data selection and two other previously proposed methods.
In cross-entropy difference selection, a sentence's score is the in-domain cross-entropy minus the background cross-entropy. This technique has been used to supplement European parliamentary text (48M words) with newswire data (3.4B words).
