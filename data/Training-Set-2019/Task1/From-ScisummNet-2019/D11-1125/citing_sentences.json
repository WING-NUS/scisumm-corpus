[
  {
    "citance_No": 1, 
    "citing_paper_id": "P14-2127", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Kai, Zhao | Liang, Huang | Haitao, Mi | Abe, Ittycheriah", 
    "raw_text": "We evaluate MAXFORCE for HIERO over two CHEN corpora, IWSLT09 and FBIS, and compare the performance with vanilla n-best MERT (Och, 2003) from Moses (Koehn et al, 2007), Hypergraph MERT (Kumar et al, 2009), and PRO (Hopkins and May, 2011) from cdec", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P13-1110", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Vladimir, Eidelman | Yuval, Marton | Philip, Resnik", 
    "raw_text": "RAMPION settings were as described in (Gimpel and Smith, 2012), and PRO settings as described in (Hopkins and May, 2011), with PRO requiring regularization tuning in order to be competitive with the other optimizers", 
    "clean_text": "RAMPION settings were as described in (Gimpel and Smith, 2012), and PRO settings as described in (Hopkins and May, 2011), with PRO requiring regularization tuning in order to be competitive with the other optimizers.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P12-1002", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Patrick, Simianer | Stefan, Riezler | Chris, Dyer", 
    "raw_text": "We found similar fluctuations for the cdec implementations of PRO (Hopkins and May, 2011) or hyper graph-MERT (Kumar et al, 2009) both of which depend on hyper graph sampling", 
    "clean_text": "We found similar fluctuations for the cdec implementations of PRO (Hopkins and May, 2011) or hyper graph-MERT (Kumar et al, 2009) both of which depend on hyper graph sampling.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "W12-3134", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Juri, Ganitkevitch | Yuan, Cao | Jonathan, Weese | Matt, Post | Chris, Callison-Burch", 
    "raw_text": "Additionally, we present Joshua? s implementation of the pairwise ranking optimization (Hopkins and May, 2011) approach to translation model tuning", 
    "clean_text": "Additionally, we present Joshua's implementation of the pairwise ranking optimization (Hopkins and May, 2011) approach to translation model tuning.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "W12-3134", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Juri, Ganitkevitch | Yuan, Cao | Jonathan, Weese | Matt, Post | Chris, Callison-Burch", 
    "raw_text": "in Joshua Pairwise ranking optimization (PRO) proposed by (Hopkins and May, 2011) is a new method for discriminative parameter tuning in statistical machine translation", 
    "clean_text": "Pairwise ranking optimization (PRO) proposed by (Hopkins and May, 2011) is a new method for discriminative parameter tuning in statistical machine translation.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "W12-3134", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Juri, Ganitkevitch | Yuan, Cao | Jonathan, Weese | Matt, Post | Chris, Callison-Burch", 
    "raw_text": "III and Marcu, 2006): the classifier used by Hopkins and May (2011) .2? Maximum entropy classifier (Manning and Klein, 2003): the Stanford toolkit for maxi mum entropy classification.3 The user may specify which classifier he wants touse and the classifier-specific parameters in the J PRO configuration file", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "P13-1031", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Spence, Green | Sida, Wang | Daniel, Cer | Christopher D., Manning", 
    "raw_text": "It optimizes a logistic objective identical to that of PRO (Hopkinsand May, 2011) with stochastic gradient descent, al though other objectives are possible", 
    "clean_text": "It optimizes a logistic objective identical to that of PRO (Hopkins and May, 2011) with stochastic gradient descent, although other objectives are possible.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "P13-1031", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Spence, Green | Sida, Wang | Daniel, Cer | Christopher D., Manning", 
    "raw_text": "We cast MT tuning as pairwise ranking (Herbrich et al, 1999, inter alia), which Hopkinsand May (2011) applied to MT. The pairwise approach results in simple, convex loss functions suit able for on line learning", 
    "clean_text": "We cast MT tuning as pairwise ranking (Herbrich et al, 1999, inter alia), which Hopkins and May (2011) applied to MT.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "N12-1047", 
    "citing_paper_authority": 59, 
    "citing_paper_authors": "Colin, Cherry | George, Foster", 
    "raw_text": "Introduced by Hopkins and May (2011), Pairwise Ranking Optimization (PRO) aims to handle large feature sets inside the traditional MERT architecture", 
    "clean_text": "Introduced by Hopkins and May (2011), Pairwise Ranking Optimization (PRO) aims to handle large feature sets inside the traditional MERT architecture.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "N12-1047", 
    "citing_paper_authority": 59, 
    "citing_paper_authors": "Colin, Cherry | George, Foster", 
    "raw_text": "The expectations needed to calculate the gradient EP~w [~hi (e)? i (e)]? EP~w [? i (e)] EP~w [~hi (e)] 2Hopkins and May (2011) advocate a maximum-entropy version of PRO, which is what we evaluate in our empirical comparison", 
    "clean_text": "Hopkins and May (2011) advocate a maximum-entropy version of PRO, which is what we evaluate in our empirical comparison.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "N12-1047", 
    "citing_paper_authority": 59, 
    "citing_paper_authors": "Colin, Cherry | George, Foster", 
    "raw_text": "We used sparse feature templates that are equivalent to the PBMT set described in (Hopkins and May, 2011) :tgtunal picks out each of the 50 most frequent tar get words to appear unaligned in the phrase table; count bin uniquely bins joint phrase pair counts with upper bounds 1,2,4,8,16,32,64,128,1k,10k,?; word pair fires when each of the 80 most frequent words in each language appear aligned 1-1 to each other, to some other word, or not 1-1; and length bin captures each possible phrase length and length pair", 
    "clean_text": "We used sparse feature templates that are equivalent to the PBMT set described in (Hopkins and May, 2011).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "P13-2121", 
    "citing_paper_authority": 11, 
    "citing_paper_authors": "Kenneth, Heafield | Ivan, Pouzyrevsky | Jonathan H., Clark | Philipp, Koehn", 
    "raw_text": "Feature weights were re tuned with PRO (Hopkins and May, 2011 )forCzechEnglish and batch MIRA (Cherry and Foster, 2012) for French-English and SpanishEnglishbecause these worked best for the baseline", 
    "clean_text": "Feature weights were re-tuned with PRO (Hopkins and May, 2011) for Czech-English and batch MIRA (Cherry and Foster, 2012) for French-English and Spanish-English because these worked best for the baseline.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "N12-1062", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Marzieh, Bazrafshan | Tagyoung, Chung | Daniel, Gildea", 
    "raw_text": "Hopkins and May (2011 )presented a method that uses a binary classifier", 
    "clean_text": "Hopkins and May (2011) presented a method that uses a binary classifier.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "N12-1062", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Marzieh, Bazrafshan | Tagyoung, Chung | Daniel, Gildea", 
    "raw_text": "Hopkins and May (2011) introduced the method of pairwise ranking optimization (PRO), which casts the problem of tuning as a ranking problem be tween pairs of translation candidates", 
    "clean_text": "Hopkins and May (2011) introduced the method of pairwise ranking optimization (PRO), which casts the problem of tuning as a ranking problem between pairs of translation candidates.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "N12-1062", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Marzieh, Bazrafshan | Tagyoung, Chung | Daniel, Gildea", 
    "raw_text": "Following Hopkins and May (2011), we used the following parameters for the sampling task: For each sentence, the decoder generates the 1500 best candidate translations (k= 1500), and the sampler samples 5000 pairs (n= 5000)", 
    "clean_text": "Following Hopkins and May (2011), we used the following parameters for the sampling task: For each sentence, the decoder generates the 1500 best candidate translations (k= 1500), and the sampler samples 5000 pairs (n= 5000).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "P13-1083", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Akihiro, Tamura | Taro, Watanabe | Eiichiro, Sumita | Hiroya, Takamura | Manabu, Okumura", 
    "raw_text": "Parameters are tuned on the development data using xBLEU (Rosti et al, 2011) as an objective and L-BFGS (Liu and Nocedal, 1989) as an optimization toolkit, since it is stable and less prone to randomness, unlike MERT (Och, 2003) or PRO (Hopkins and May, 2011)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "N12-1026", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Taro, Watanabe", 
    "raw_text": "Like Hopkins and May (2011), we optimize ranking in nbest lists, but learn parameters in an on line fashion", 
    "clean_text": "Like Hopkins and May (2011), we optimize ranking in n-best lists, but learn parameters in an online fashion.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "N12-1026", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Taro, Watanabe", 
    "raw_text": "Unlike Hopkins and May (2011), we do not randomly sample from all the pairs in the n-best translations, but extract pairs by selecting one oracle translation and one other translation in the nbests other than those in ORACLE (?)", 
    "clean_text": "Unlike Hopkins and May (2011), we do not randomly sample from all the pairs in the n-best translations, but extract pairs by selecting one oracle translation and one other translation in the n-bests other than those in ORACLE.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "N12-1026", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Taro, Watanabe", 
    "raw_text": "Hopkins and May (2011) applied a MERT-like procedure in Alg", 
    "clean_text": "Hopkins and May (2011) applied a MERT-like procedure in Alg 1 in which Equation 4 was solved to obtain new parameters in each iteration.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "N12-1026", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Taro, Watanabe", 
    "raw_text": "4Hopkins and May (2011) minimized logistic loss sampled from the merged n-bests, and sentence-BLEU was used for determining ranks", 
    "clean_text": "Hopkins and May (2011) minimized logistic loss sampled from the merged n-bests, and sentence-BLEU was used for determining ranks.", 
    "keep_for_gold": 1
  }
]