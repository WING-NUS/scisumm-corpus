Citance Number: 1 | Reference Article:  N10-1020.txt | Citing Article:  D10-1074.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Recent studies have also developed approaches to summarize conversations (Murray and Carenini, 2008) and to model conversation structures (dialogue acts) from online Twitter conversations (Ritter et al, 2010).</S> | Reference Offset:  ['0','9'] | Reference Text:  <S sid = 0 ssid = >Unsupervised Modeling of Twitter Conversations</S><S sid = 9 ssid = >These acts are useful in many applications, including conversational agents (Wilks, 2006), dialogue systems (Allen et al., 2007), dialogue summarization (Murray et al., 2006), and flirtation detection (Ranganath et al., 2009).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N10-1020.txt | Citing Article:  P14-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Ritter et al (2010) extends LMHMM to allow words to be emitted from two additional sources: the topic of current dialogue, or a background LM shared across all dialogues.</S> | Reference Offset:  ['9','105'] | Reference Text:  <S sid = 9 ssid = >These acts are useful in many applications, including conversational agents (Wilks, 2006), dialogue systems (Allen et al., 2007), dialogue summarization (Murray et al., 2006), and flirtation detection (Ranganath et al., 2009).</S><S sid = 105 ssid = >Rather than attempt to set them using an expensive grid search, we treat the concentration parameters as additional hidden variables and sample each in turn, conditioned on the current assignment to all other variables.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N10-1020.txt | Citing Article:  P14-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Ritter et al (2010) finds these alternate sources are important in non-task-oriented domains, where events are diffuse and fleeting.</S> | Reference Offset:  ['7','9'] | Reference Text:  <S sid = 7 ssid = >Automatic detection of dialogue structure is an important first step toward deep understanding of human conversations.</S><S sid = 9 ssid = >These acts are useful in many applications, including conversational agents (Wilks, 2006), dialogue systems (Allen et al., 2007), dialogue summarization (Murray et al., 2006), and flirtation detection (Ranganath et al., 2009).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N10-1020.txt | Citing Article:  P14-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Ritter et al (2010) proposes an evaluation based on rank correlation coefficient, which measures the degree of similarity between any two orderings over sequential data.</S> | Reference Offset:  ['157','163'] | Reference Text:  <S sid = 157 ssid = >Therefore, we also propose a novel quantitative evaluation that measures the intrinsic quality of a conversation model by its ability to predict the ordering of posts in a conversation.</S><S sid = 163 ssid = >The Kendall r rank correlation coefficient measures the similarity between two permutations based on their agreement in pairwise orderings: where n+ is the number of pairs that share the same order in both permutations, and n_ is the number that do not.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N10-1020.txt | Citing Article:  P14-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Ritter et al (2010) limits their dataset by choosing Twitter dialogues containing 3 to 6 posts (utterances), making it tractable to enumerate all permutations.</S> | Reference Offset:  ['44','160'] | Reference Text:  <S sid = 44 ssid = >The resulting corpus consists of about 1.3 million conversations, with each conversation containing between 2 and 243 posts.</S><S sid = 160 ssid = >For each conversation in the test set, we generate all n! permutations of the posts.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N10-1020.txt | Citing Article:  W11-0501.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To overcome this, we plan to use an unsupervised learning approach to discover dialogue acts (Ritter et al, 2010).</S> | Reference Offset:  ['1','54'] | Reference Text:  <S sid = 1 ssid = >We propose the first unsupervised approach to the problem of modeling dialogue acts in an open domain.</S><S sid = 54 ssid = >We propose two models to discover dialogue acts in an unsupervised manner.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N10-1020.txt | Citing Article:  D11-1061.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To date, our paper most closely relates to works on semantic role labeling (SRL) on social media (Liu et al., 2010) and conversation modeling (Ritter et al, 2010).</S> | Reference Offset:  ['9','10'] | Reference Text:  <S sid = 9 ssid = >These acts are useful in many applications, including conversational agents (Wilks, 2006), dialogue systems (Allen et al., 2007), dialogue summarization (Murray et al., 2006), and flirtation detection (Ranganath et al., 2009).</S><S sid = 10 ssid = >Dialogue act tagging has traditionally followed an annotate-train-test paradigm, which begins with the design of annotation guidelines, followed by the collection and labeling of corpora (Jurafsky et al., 1997; Dhillon et al., 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N10-1020.txt | Citing Article:  D11-1061.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >On a different ground, Ritter et al (2010) propose a probabilistic model to discover dialogue acts in Twitter conversations and to classify tweets in a conversation according to those acts.</S> | Reference Offset:  ['54','61'] | Reference Text:  <S sid = 54 ssid = >We propose two models to discover dialogue acts in an unsupervised manner.</S><S sid = 61 ssid = >Our goals are not so different: we wish to discover the sequential dialogue structure of conversation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N10-1020.txt | Citing Article:  W12-2108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Twitter also provides a wealth of user dialog, and a variety of dialog acts have been observed (Ritter et al, 2010) and predicted (Ritter et al, 2011).</S> | Reference Offset:  ['9','11'] | Reference Text:  <S sid = 9 ssid = >These acts are useful in many applications, including conversational agents (Wilks, 2006), dialogue systems (Allen et al., 2007), dialogue summarization (Murray et al., 2006), and flirtation detection (Ranganath et al., 2009).</S><S sid = 11 ssid = >Only then can one train a tagger to automatically recognize dialogue acts (Stolcke et al., 2000).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N10-1020.txt | Citing Article:  P11-1040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Examples of such approaches include methods for conversation structure analysis (Ritter et al, 2010) and exploration of geographic language variation (Eisenstein et al, 2010) from Twitter messages.</S> | Reference Offset:  ['9','30'] | Reference Text:  <S sid = 9 ssid = >These acts are useful in many applications, including conversational agents (Wilks, 2006), dialogue systems (Allen et al., 2007), dialogue summarization (Murray et al., 2006), and flirtation detection (Ranganath et al., 2009).</S><S sid = 30 ssid = >Similar datasets include the NUS SMS corpus (How and Kan, 2005), several IRC chat corpora (Elsner and Charniak, 2008; Forsyth and Martell, 2007), and blog datasets (Yano et al., 2009; Gamon et al., 2008), which can display conversational structure in the blog comments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N10-1020.txt | Citing Article:  P11-1040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Second, we employ a MIRA-based binary classifier (Ritter et al, 2010) to predict whether a message mentions a concert event.</S> | Reference Offset:  ['9','109'] | Reference Text:  <S sid = 9 ssid = >These acts are useful in many applications, including conversational agents (Wilks, 2006), dialogue systems (Allen et al., 2007), dialogue summarization (Murray et al., 2006), and flirtation detection (Ranganath et al., 2009).</S><S sid = 109 ssid = >As computing this probability exactly is intractable in our model, we employ a recently proposed Chibbstyle estimator (Murray and Salakhutdinov, 2008; Wallach et al., 2009).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N10-1020.txt | Citing Article:  D11-1065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Twitter has previously been proposed as a candidate for modeling conversations, see for example Ritter et al (2010).</S> | Reference Offset:  ['0','109'] | Reference Text:  <S sid = 0 ssid = >Unsupervised Modeling of Twitter Conversations</S><S sid = 109 ssid = >As computing this probability exactly is intractable in our model, we employ a recently proposed Chibbstyle estimator (Murray and Salakhutdinov, 2008; Wallach et al., 2009).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N10-1020.txt | Citing Article:  W12-1607.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the context of Twitter conversations, Ritter et al (2010) suggests using dialogue act tags as a middle layer towards conversation reconstruction.</S> | Reference Offset:  ['0','9'] | Reference Text:  <S sid = 0 ssid = >Unsupervised Modeling of Twitter Conversations</S><S sid = 9 ssid = >These acts are useful in many applications, including conversational agents (Wilks, 2006), dialogue systems (Allen et al., 2007), dialogue summarization (Murray et al., 2006), and flirtation detection (Ranganath et al., 2009).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N10-1020.txt | Citing Article:  D12-1009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Specifically, we build off the Bayesian block HMMs used by Ritter et al (2010) for modeling Twitter conversations, which will be our primary baseline.</S> | Reference Offset:  ['0','9'] | Reference Text:  <S sid = 0 ssid = >Unsupervised Modeling of Twitter Conversations</S><S sid = 9 ssid = >These acts are useful in many applications, including conversational agents (Wilks, 2006), dialogue systems (Allen et al., 2007), dialogue summarization (Murray et al., 2006), and flirtation detection (Ranganath et al., 2009).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N10-1020.txt | Citing Article:  D12-1009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We show that M4 increases thread reconstruction accuracy by up to 15% compared to the HMM of Ritter et al (2010), and we reduce variation of information against speech act annotations by an average of 18% from HMM and LDA baselines.</S> | Reference Offset:  ['35','114'] | Reference Text:  <S sid = 35 ssid = >The Twitter API provides a link from each reply to the post it is responding to, allowing accurate thread reconstruction without requiring a conversation disentanglement step (Elsner and Charniak, 2008).</S><S sid = 114 ssid = >There is reason to believe that integrating out multinomials and using sparse priors will improve the performance of the conversation model, as improvements have been observed when using a Bayesian HMM for unsupervised part-of-speech tagging (Goldwater and Griffiths, 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  N10-1020.txt | Citing Article:  D12-1009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Under the block HMM, as utilized by Ritter et al (2010), messages in a conversation flow according to a Markov process, where the words of messages are generated according to language models associated with a state in a hidden Markov model.</S> | Reference Offset:  ['24','59'] | Reference Text:  <S sid = 24 ssid = >Woszczyna and Waibel (1994) propose an unsupervised Hidden Markov Model (HMM) for dialogue structure in a meeting scheduling domain, but model dialogue state at the word level.</S><S sid = 59 ssid = >A news story is modeled by first generating a sequence of hidden topics according to a Markov model, with each topic generating an observed sentence according to a topic-specific language model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  N10-1020.txt | Citing Article:  D12-1009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >If it is desired to have a large number of latent topics as is common in LDA, this model could be combined with a standard topic model without sequential dependencies, as explored by Ritter et al (2010).</S> | Reference Offset:  ['59','92'] | Reference Text:  <S sid = 59 ssid = >A news story is modeled by first generating a sequence of hidden topics according to a Markov model, with each topic generating an observed sentence according to a topic-specific language model.</S><S sid = 92 ssid = >To that end, we adopt a Latent Dirichlet Allocation, or LDA, framework (Blei et al., 2003) similar to approaches used recently in summarization (DaumÂ´e III and Marcu, 2006; Haghighi and Vanderwende, 2009).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  N10-1020.txt | Citing Article:  D12-1009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Unsupervised HMMs were applied to conversational data by Ritter et al (2010) who experimented with Twitter conversations.</S> | Reference Offset:  ['0','178'] | Reference Text:  <S sid = 0 ssid = >Unsupervised Modeling of Twitter Conversations</S><S sid = 178 ssid = >We have presented an approach that allows the unsupervised induction of dialogue structure from naturally-occurring open-topic conversational data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  N10-1020.txt | Citing Article:  D12-1009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Second, we use the Twitter data set created by Ritter et al (2010).</S> | Reference Offset:  ['63','123'] | Reference Text:  <S sid = 63 ssid = >An initial conversation model can be created by simply applying the content modeling framework to conversation data.</S><S sid = 123 ssid = >Smoothing parameters are set using grid search on a development set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  N10-1020.txt | Citing Article:  D12-1009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our work is motivated by the Bayesian HMM approach of Ritter et al (2010) - the model we refer to as the block HMM (BHMM) - and we consider this our primary baseline.</S> | Reference Offset:  ['24','114'] | Reference Text:  <S sid = 24 ssid = >Woszczyna and Waibel (1994) propose an unsupervised Hidden Markov Model (HMM) for dialogue structure in a meeting scheduling domain, but model dialogue state at the word level.</S><S sid = 114 ssid = >There is reason to believe that integrating out multinomials and using sparse priors will improve the performance of the conversation model, as improvements have been observed when using a Bayesian HMM for unsupervised part-of-speech tagging (Goldwater and Griffiths, 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


