Minimum Risk Annealing For Training Log-Linear Models
When training the parameters for a natural language system, one would prefer to minimize 1-best loss (error) on an evaluation set.
Since the error surface for many natural language problems is piecewise constant and riddled with local minima, many systems instead optimize log-likelihood, which is conveniently differentiable and convex.
We propose training instead to minimize the expected loss, or risk.
We define this expectation using a probability distribution over hypotheses that we gradually sharpen (anneal) to focus on the 1-best hypothesis.
Besides the linear loss functions used in previous work, we also describe techniques for optimizing nonlinear functions such as precision or the BLEU metric.
We present experiments training log-linear combinations of models for dependency parsing and for machine translation.
In machine translation, annealed minimum risk training achieves significant improvements in BLEU over standard minimum error training.
We also show improvements in labeled dependency parsing.
We use a linearization technique to approximate the expectation of log BLEU score.
We present a deterministic annealing training procedure, whose objective is to minimize the expected error (together with the entropy regularization technique).
We observe test set gains by minimum risk annealing, which incorporates a temperature parameter that trades off between the smoothness of the objective and the degree it reflects the underlying piecewise constant error surface.
