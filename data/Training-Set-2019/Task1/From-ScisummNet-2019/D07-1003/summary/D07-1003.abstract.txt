This paper presents a syntax-driven approach to question answering, specifically the answer-sentence selection problem for short-answer questions.
Rather than using syntactic features to augment existing statistical classifiers (as in previous work), we build on the idea that questions and their (correct) answers relate to each other via loose but predictable syntactic transformations.
We propose a probabilistic quasi-synchronous grammar, inspired by one proposed for machine translation (D. Smith and Eisner, 2006), and parameterized by mixtures of a robust nonlexical syntax/alignment model with a(n optional) lexical-semantics-driven log-linear model.
Our model learns soft alignments as a hidden variable in discriminative training.
Experimental results using the TREC dataset are shown to significantly outperform strong state-of-the-art baselines.
