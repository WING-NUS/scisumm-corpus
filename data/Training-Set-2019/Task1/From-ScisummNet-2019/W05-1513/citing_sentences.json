[
  {
    "citance_No": 1, 
    "citing_paper_id": "W05-1515", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Joseph P., Turian | I. Dan, Melamed", 
    "raw_text": "Thisapproach differs from that of Yamada and Matsumoto (2003) and Sagae and Lavie (2005), who parallelize according to the POS tag of one of the child items", 
    "clean_text": "This approach differs from that of Yamada and Matsumoto (2003) and Sagae and Lavie (2005), who parallelize according to the POS tag of one of the child items.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "W05-1515", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Joseph P., Turian | I. Dan, Melamed", 
    "raw_text": "Like Yamada and Matsumoto (2003) and Sagae and Lavie (2005), our parser is driven by classifiers", 
    "clean_text": "Like Yamada and Matsumoto (2003) and Sagae and Lavie (2005), our parser is driven by classifiers.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "W05-1515", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Joseph P., Turian | I. Dan, Melamed", 
    "raw_text": "Although training timeis still a concern in our setup, the situation is ameliorated by generating training examples in advance and inducing one-vs-all classifiers in parallel, a technique similar in spirit to the POS-tag parallelization in Yamada and Matsumoto (2003) and Sagae and Lavie (2005)", 
    "clean_text": "Although training time is still a concern in our setup, the situation is ameliorated by generating training examples in advance and inducing one-vs-all classifiers in parallel, a technique similar in spirit to the POS-tag parallelization in Yamada and Matsumoto (2003) and Sagae and Lavie (2005).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "P06-2089", 
    "citing_paper_authority": 17, 
    "citing_paper_authors": "Kenji, Sagae | Alon, Lavie", 
    "raw_text": "We present a statistical parser that is based on a shift-reduce algorithm, like the parsers of Sagae and Lavie (2005) and Nivre and Scholz (2004), but performs a best-first search instead of pursuing a single analysis path in deterministic fashion", 
    "clean_text": "We present a statistical parser that is based on a shift-reduce algorithm, like the parsers of Sagae and Lavie (2005) and Nivre and Scholz (2004), but performs a best-first search instead of pursuing a single analysis path in deterministic fashion.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P06-2089", 
    "citing_paper_authority": 17, 
    "citing_paper_authors": "Kenji, Sagae | Alon, Lavie", 
    "raw_text": "2.1 A Shift-Reduce Algorithm for Deterministic Constituent Parsing In its deterministic form, our parsing algorithm is the same single-pass shift-reduce algorithm as the one used in the classifer-based parser of Sagaeand Lavie (2005)", 
    "clean_text": "A Shift-Reduce Algorithm for Deterministic Constituent Parsing In its deterministic form, our parsing algorithm is the same single-pass shift-reduce algorithm as the one used in the classifer-based parser of Sagae and Lavie (2005).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "P06-2089", 
    "citing_paper_authority": 17, 
    "citing_paper_authors": "Kenji, Sagae | Alon, Lavie", 
    "raw_text": "Sagae and Lavie (2005) built two deterministic parsers this way, one using support vector machines, and one usingk-nearest neighbors", 
    "clean_text": "Sagae and Lavie (2005) built two deterministic parsers this way, one using support vector machines, and one using k-nearest neighbors.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "P06-2089", 
    "citing_paper_authority": 17, 
    "citing_paper_authors": "Kenji, Sagae | Alon, Lavie", 
    "raw_text": "Figure 1: Features used for classification, with features 1 to 13 taken from Sagae and Lavie (2005)", 
    "clean_text": "Features used for classification, with features 1 to 13 taken from Sagae and Lavie (2005).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "P06-2089", 
    "citing_paper_authority": 17, 
    "citing_paper_authors": "Kenji, Sagae | Alon, Lavie", 
    "raw_text": "Training the maximum entropy classifier with such a large number (1.9 million) of training in stances and features required more memory than was available (the maximum training set size we were able to train with 2GB of RAM was about 200,000 instances), so we employed the training set splitting idea used by Yamada and Matsumoto (2003) and Sagae and Lavie (2005)", 
    "clean_text": "Training the maximum entropy classifier with such a large number (1.9 million) of training instances and features required more memory than was available (the maximum training set size we were able to train with 2GB of RAM was about 200,000 instances), so we employed the training set splitting idea used by Yamada and Matsumoto (2003) and Sagae and Lavie (2005).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "P06-2089", 
    "citing_paper_authority": 17, 
    "citing_paper_authors": "Kenji, Sagae | Alon, Lavie", 
    "raw_text": "For comparison, Sagae and Lavie (2005) report that training support vector machines for one-against-all multi-class classification on the same set of features for their deterministic parser took 62 hours, and training a k-nearest neighbors classifier took 11 minutes. When given perfectly tagged text (gold part-of speech tags extracted from the Penn Treebank), our parser has labeled constituent precision and re call of 89.40% and 88.79% respectively over all sentences in the test set, and 90.01% and 89.32% over sentences with length of at most 40 words", 
    "clean_text": "For comparison, Sagae and Lavie (2005) report that training support vector machines for one-against-all multi-class classification on the same set of features for their deterministic parser took 62 hours, and training a k-nearest neighbors classifier took 11 minutes.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "P06-2089", 
    "citing_paper_authority": 17, 
    "citing_paper_authors": "Kenji, Sagae | Alon, Lavie", 
    "raw_text": "More interestingly, it parses all 2,416 sentences (more than 50,000 words) in only 46 seconds, 10 times faster than the deter minis tic SVM parser of Sagae and Lavie (2005)", 
    "clean_text": "More interestingly, it parses all 2,416 sentences (more than 50,000 words) in only 46 seconds, 10 times faster than the deterministic SVM parser of Sagae and Lavie (2005).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "W06-3603", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Joseph P., Turian | I. Dan, Melamed", 
    "raw_text": "Like our work, Ratnaparkhi (1999) and Sagae and Lavie (2005) generate examples off-line, but their parsing strategies are essentially shift-reduce so each sentence generates only O (n) training examples", 
    "clean_text": "Like our work, Ratnaparkhi (1999) and Sagae and Lavie (2005) generate examples off-line, but their parsing strategies are essentially shift-reduce so each sentence generates only O (n) training examples.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "W07-2211", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "P. S., Newman", 
    "raw_text": "One extrapolation is to a very fast stochastic parser by Sagae and Lavie (2005)", 
    "clean_text": "One extrapolation is to a very fast stochastic parser by Sagae and Lavie (2005).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "P06-1033", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Jens, Nilsson | Joakim, Nivre | Johan, Hall", 
    "raw_text": "63.73 82.10 63.20 75.14 90.89 92.79 80.02 81.40 Table 6: Detailed results for SVM; T= transformation; P= unlabeled precision, R= unlabeled recall costly to train (Sagae and Lavie, 2005)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "P06-1110", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Joseph P., Turian | I. Dan, Melamed", 
    "raw_text": "We choose a single correct path from each training parse tree, and the training examples correspond to all candidate inferences considered in every state along this path.4 In the deterministic setting there is only one correct path, so example generation is identical to that of Sagae and Lavie (2005)", 
    "clean_text": "In the deterministic setting there is only one correct path, so example generation is identical to that of Sagae and Lavie (2005).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "P06-1110", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Joseph P., Turian | I. Dan, Melamed", 
    "raw_text": "Like our work, Ratnaparkhi (1999) and Sagae and Lavie (2005 )gener ate examples off-line, but their parsing strategies are essentially shift-reduce so each sentence generates only O (n) training examples", 
    "clean_text": "Like our work, Ratnaparkhi (1999) and Sagae and Lavie (2005) generate examples off-line, but their parsing strategies are essentially shift-reduce so each sentence generates only O (n) training examples.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "I08-2087", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Dakun, Zhang | Le, Sun | Wenbo, Li", 
    "raw_text": "This model is inspired by Sagae and Lavie (2005), in which a stack-based representation of monolingual parsing trees is used", 
    "clean_text": "This model is inspired by Sagae and Lavie (2005), in which a stack-based representation of monolingual parsing trees is used.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "I08-2087", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Dakun, Zhang | Le, Sun | Wenbo, Li", 
    "raw_text": "Sagae and Lavie (2005) propose a constituency based parsing method to determine sentence dependency structures", 
    "clean_text": "Sagae and Lavie (2005) propose a constituency based parsing method to determine sentence dependency structures.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "P06-1054", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Mengqiu, Wang | Kenji, Sagae | Teruko, Mitamura", 
    "raw_text": "In our approach, which is based on the shift-reduce parser for English reported in (Sagae and Lavie,2005), the parsing task is transformed into a succession of classification tasks", 
    "clean_text": "In our approach, which is based on the shift-reduce parser for English reported in (Sagae and Lavie,2005), the parsing task is transformed into a succession of classification tasks.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "P06-1054", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Mengqiu, Wang | Kenji, Sagae | Teruko, Mitamura", 
    "raw_text": "A simple transformation process as describe din (Sagae and Lavie, 2005) is employed to convert between arbitrary branching trees and binary trees", 
    "clean_text": "A simple transformation process as described in (Sagae and Lavie, 2005) is employed to convert between arbitrary branching trees and binary trees.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "P06-1054", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Mengqiu, Wang | Kenji, Sagae | Teruko, Mitamura", 
    "raw_text": "Sagae and Lavie (2005) have shown that this algorithm has linear time complexity, assuming that classification takes constant time", 
    "clean_text": "Sagae and Lavie (2005) have shown that this algorithm has linear time complexity, assuming that classification takes constant time.", 
    "keep_for_gold": 0
  }
]