Automatic evaluation of machine translation, based on computing n-gram similarity between system output and human reference translations, has revolutionized the development of MT systems.
We explore the use of syntactic information, including constituent labels and head-modier dependencies, in computing similarity between output and reference.
Our results show that adding syntactic information to the evaluation metric improves both sentence-level and corpus-level correlation with human judgments.
