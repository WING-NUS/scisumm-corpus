[
  {
    "citance_No": 1, 
    "citing_paper_id": "W11-2127", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Jacob, Andreas | Nizar, Habash | Owen, Rambow", 
    "raw_text": "MERT was done only for the baseline system; these same weights were used for all experiments to control for the effect of MERT instability. In the future, we plan to experiment with approach specific optimization and to use recent published suggestions on controlling for optimizer instability (Clark et al, 2011)", 
    "clean_text": "MERT was done only for the baseline system; these same weights were used for all experiments to control for the effect of MERT instability. In the future, we plan to experiment with approach specific optimization and to use recent published suggestions on controlling for optimizer instability (Clark et al, 2011).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P14-1010", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Mohammad, Salameh | Colin, Cherry | Grzegorz, Kondrak", 
    "raw_text": "Fol lowing Clark et al (2011), we report average scores over five random tuning replications to ac count for optimizer instability", 
    "clean_text": "Following Clark et al (2011), we report average scores over five random tuning replications to account for optimizer instability.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P14-1010", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Mohammad, Salameh | Colin, Cherry | Grzegorz, Kondrak", 
    "raw_text": "We measure statistical significance using MultEval (Clark et al, 2011), which implements a stratified approximate randomization test to account for multiple tuning replications", 
    "clean_text": "We measure statistical significance using MultEval (Clark et al, 2011), which implements a stratified approximate randomization test to account for multiple tuning replications.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "W12-3154", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Barry, Haddow | Philipp, Koehn", 
    "raw_text": "Tuning of models used minimum error rate training (Och, 2003), repeated 3 times and averaged (Clarketal., 2011)", 
    "clean_text": "Tuning of models used minimum error rate training (Och, 2003), repeated 3 times and averaged (Clark et al., 2011).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P14-1137", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Deyi, Xiong | Min, Zhang", 
    "raw_text": "In order to alleviate the impact of MERT (Och, 2003 )instability, we followed the suggestion of Clark et al (2011) to run MERT three times and report aver age BLEU/NIST scores over the three runs for all our experiments", 
    "clean_text": "In order to alleviate the impact of MERT (Och, 2003) instability, we followed the suggestion of Clark et al (2011) to run MERT three times and report average BLEU/NIST scores over the three runs for all our experiments.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "W12-3150", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Philip, Williams | Philipp, Koehn", 
    "raw_text": "Following the recommendation of Clark et al (2011), we ran the optimization three times and repeated evaluation with each set of feature weights", 
    "clean_text": "Following the recommendation of Clark et al (2011), we ran the optimization three times and repeated evaluation with each set of feature weights.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "P13-1078", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "lemao,  | Taro, Watanabe | Eiichiro, Sumita | Tiejun, Zhao", 
    "raw_text": "and MaxIter, which are common in other tuning toolkits such as MIRA and can be tuned5 on a development test dataset. Since both MERT and PRO tuning toolkits involve randomness in their implementations, all BLEU scores reported in the experiments are the average of five tuning runs, as suggested by Clark et al (2011) for fairer comparisons", 
    "clean_text": "Since both MERT and PRO tuning toolkits involve randomness in their implementations, all BLEU scores reported in the experiments are the average of five tuning runs, as suggested by Clark et al (2011) for fairer comparisons.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "W12-3130", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Bushra, Jawaid | Amir, Kamran | Ond&#x159;ej, Bojar", 
    "raw_text": "Aware of the low stability of MERT (Clark et al,2011), we run MERT three times and report the average BLEU score including the standard deviation. The last column in Table 3 lists the average number of distinct candidates per sentence in the n best lists during MERT, dubbed? effective n-best list size?", 
    "clean_text": "Aware of the low stability of MERT (Clark et al, 2011), we run MERT three times and report the average BLEU score including the standard deviation.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "W12-3152", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Matt, Post | Chris, Callison-Burch | Miles, Osborne", 
    "raw_text": "We tuned with minimum error rate training (Och, 2003) using Z-MERT (Zaidan, 2009) and present the mean BLEU score on test data over three separate runs (Clark et al, 2011) .MBR re ranking (Kumar and Byrne, 2004) was applied to Joshua? s 300-best (unique) output, and evaluation was conducted with case-insensitive BLEU with four references", 
    "clean_text": "We tuned with minimum error rate training (Och, 2003) using Z-MERT (Zaidan, 2009) and present the mean BLEU score on test data over three separate runs (Clark et al, 2011).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "P13-1082", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Rico, Sennrich | Holger, Schwenk | Walid, Aransa", 
    "raw_text": "We account for optimizer instability by running 3 independent MERT runs per system, and performing significance testing with MultEval (Clark et al, 2011)", 
    "clean_text": "We account for optimizer instability by running 3 independent MERT runs per system, and performing significance testing with MultEval (Clark et al, 2011).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "P13-1079", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Conghui, Zhu | Taro, Watanabe | Eiichiro, Sumita | Tiejun, Zhao", 
    "raw_text": "The BLEU scores reported in this paper are the average of 5 independent runs of independent batch-MIRA weight training, as suggested by (Clark et al, 2011)", 
    "clean_text": "The BLEU scores reported in this paper are the average of 5 independent runs of independent batch-MIRA weight training, as suggested by (Clark et al, 2011).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "W12-3148", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Milo&scaron;, Stanojevi&#263; | Amir, Kamran | Petra, Galu&scaron;&ccaron;&aacute;kov&aacute; | Ale&scaron;, Tamchyna | Ond&#x159;ej, Bojar", 
    "raw_text": "In order to make the results more reliable, it is necessary to repeat the experiment several times (Clark et al, 2011)", 
    "clean_text": "In order to make the results more reliable, it is necessary to repeat the experiment several times (Clark et al, 2011).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "W12-3126", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Dennis, Mehay | Chris, Brew", 
    "raw_text": "All boldfaced results were found to be significantly better than the baseline at? the 95% confidence level using method described in (Clark et al, 2011) with 3 separate MERT tuning runs for each system", 
    "clean_text": "All boldfaced results were found to be significantly better than the baseline at the 95% confidence level using method described in (Clark et al, 2011) with 3 separate MERT tuning runs for each system.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "W12-3126", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Dennis, Mehay | Chris, Brew", 
    "raw_text": "While is generally not useful to test experimental manipulations based on a single tuning run (Clark et al., 2011) and with different monolingual languagemodelling data, we note these figures simply to situ ate our results within the state of the art", 
    "clean_text": "While is generally not useful to test experimental manipulations based on a single tuning run (Clark et al., 2011) and with different monolingual language modelling data, we note these figures simply to situate our results within the state of the art.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "P14-2074", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Desmond, Elliott | Frank, Keller", 
    "raw_text": "We collected the BLEU, TER, and Meteor scores using MultEval (Clark et al, 2011), and the ROUGE-SU4 scores using the RELEASE-1.5.5.pl script", 
    "clean_text": "We collected the BLEU, TER, and Meteor scores using MultEval (Clark et al, 2011), and the ROUGE-SU4 scores using the RELEASE-1.5.5.pl script.", 
    "keep_for_gold": 0
  }
]