[
  {
    "citance_No": 1, 
    "citing_paper_id": "D10-1049", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Gabor, Angeli | Percy, Liang | Dan, Klein", 
    "raw_text": "We use the model of Liang et al (2009) to automatically induce the correspondences between words in the text and the actual database records mentioned. We break up the full generation process into a sequence of local decisions, training a log-linear classifier for each type of decision", 
    "clean_text": "We use the model of Liang et al (2009) to automatically induce the correspondences between words in the text and the actual database records mentioned.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "D10-1049", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Gabor, Angeli | Percy, Liang | Dan, Klein", 
    "raw_text": "achieved a BLEU score of 51.5 on the combined task of content selection and generation, which is more than a two-fold improvement over a model similar to that of Liang et al (2009)", 
    "clean_text": "We achieved a BLEU score of 51.5 on the combined task of content selection and generation, which is more than a two-fold improvement over a model similar to that of Liang et al (2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "D10-1049", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Gabor, Angeli | Percy, Liang | Dan, Klein", 
    "raw_text": "We used the dataset created by Liang et al (2009) .The world state is summarized by records which aggregate measurements over selected time intervals. The dataset consists of 29,528 scenarios, each containing on average |s|= 36 records and 28.7 words", 
    "clean_text": "We used the dataset created by Liang et al (2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "D10-1049", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Gabor, Angeli | Percy, Liang | Dan, Klein", 
    "raw_text": "By defining features on the en tire field set, we can capture any correlation structure over the fields; in contrast, Liang et al (2009) generates a sequence of fields in which a field can only depend on the previous one", 
    "clean_text": "By defining features on the entire field set, we can capture any correlation structure over the fields; in contrast, Liang et al (2009) generates a sequence of fields in which a field can only depend on the previous one.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "D10-1049", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Gabor, Angeli | Percy, Liang | Dan, Klein", 
    "raw_text": "In particular, a word is chosen from the parameters learned in the model of Liang et al (2009)", 
    "clean_text": "In particular, a word is chosen from the parameters learned in the model of Liang et al (2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "D10-1049", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Gabor, Angeli | Percy, Liang | Dan, Klein", 
    "raw_text": "Intuitively, we know what was generated but not why it was generated. We use the model of Liang et al (2009) to impute the decisions d. They introduce a generative model p (a ,w|s), where the latent alignment a specifies (1) the sequence of records that were chosen, (2) the sequence of fields that were chosen, and (3) which words in the text were spanned by the chosen records and fields", 
    "clean_text": "We use the model of Liang et al (2009) to impute the decisions.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "D10-1049", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Gabor, Angeli | Percy, Liang | Dan, Klein", 
    "raw_text": "We found that using the unsupervised model of Liang et al (2009) to automatically produce aligned training scenarios (Section 4.3.1) was less effective than it was in the other two domains due to two factors: (i) there are fewer training examples in SUMTIME and unsupervised learning typically works better with a large amount of data; and (ii) the alignment model does not exploit the temporal structure in the SUMTIME world state", 
    "clean_text": "We found that using the unsupervised model of Liang et al (2009) to automatically produce aligned training scenarios (Section 4.3.1) was less effective than it was in the other two domains due to two factors: (i) there are fewer training examples in SUMTIME and unsupervised learning typically works better with a large amount of data; and (ii) the alignment model does not exploit the temporal structure in the SUMTIME world state.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "D10-1049", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Gabor, Angeli | Percy, Liang | Dan, Klein", 
    "raw_text": "Liang et al (2009) introduces a generative model of the text given the world state, and in some ways is similar in spirit to our model", 
    "clean_text": "Liang et al (2009) introduces a generative model of the text given the world state, and in some ways is similar in spirit to our model.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "C10-2062", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "Joohyun, Kim | Raymond J., Mooney", 
    "raw_text": "However, this approach is somewhat ad hoc and does not exploit a well-defined probabilistic generative model or real EM training. On the other hand, Liang et al (2009) introduced a probabilistic generative model for learning semantic correspondences in ambiguous training data consisting of sentences paired with observed world states", 
    "clean_text": "On the other hand, Liang et al (2009) introduced a probabilistic generative model for learning semantic correspondences in ambiguous training data consisting of sentences paired with observed world states.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "C10-2062", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "Joohyun, Kim | Raymond J., Mooney", 
    "raw_text": "However, this approach does not use the power of a probabilistic correspondence between an NL and MRs during training. On the other hand, Liang et al (2009) proposed a probabilistic generative approach to pro duce a Viterbi alignment between NL and MRs. They use a hierarchical semi-Markov generative model that first determines which facts to discuss and then generates words from the predicates and arguments of the chosen facts", 
    "clean_text": "On the other hand, Liang et al (2009) proposed a probabilistic generative approach to produce a Viterbi alignment between NL and MRs. They use a hierarchical semi-Markov generative model that first determines which facts to discuss and then generates words from the predicates and arguments of the chosen facts.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "C10-2062", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "Joohyun, Kim | Raymond J., Mooney", 
    "raw_text": "Chen et al (2010) recently reported results on utilizing the improved alignment produced by Liang et al (2009)? s model to initialize their own iterative retraining method", 
    "clean_text": "Chen et al (2010) recently reported results on utilizing the improved alignment produced by Liang et al (2009)'s model to initialize their own iterative retraining method.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "C10-2062", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "Joohyun, Kim | Raymond J., Mooney", 
    "raw_text": "Motivated by this prior research, our approach combines the generative alignment model of Liang et al (2009) with the generative semantic parsing model of Lu et al (2008) in order to fully exploit the NL syntax and its relationship to the MR semantics", 
    "clean_text": "Motivated by this prior research, our approach combines the generative alignment model of Liang et al (2009) with the generative semantic parsing model of Lu et al (2008) in order to fully exploit the NL syntax and its relationship to the MR semantics.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "C10-2062", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "Joohyun, Kim | Raymond J., Mooney", 
    "raw_text": "This follows the experimental scheme of Chen et al (2010), which demonstrated that an improved NL? MR 544 S S: pass (PLAYER, PLAYER) PLAYER PLAYER :pink10pink10 passes the ball to PLAYER PLAYER :pink11pink11 Figure 1: Sample hybrid tree from English sportscasting dataset where (w, m)= (pink10 passes the ball to pink11, pass (pink10 ,pink11)) matching from Liang et al (2009) results in better overall parsing and generation", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "C10-2062", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "Joohyun, Kim | Raymond J., Mooney", 
    "raw_text": "Like Liang et al (2009)? s generative alignment model, our model is designed to estimate P (w|s), where w is an NL sentence and s is a world state containing a set of possible MR logical forms that can be matched to w. However, our approach is intended to support both determining the most likely match between an NL and its MR in its world state, and semantic parsing ,i.e. finding the Natural Langu age Meaning Representati on Purple9 p repa res t o at tack pass (Purple Player9, Purple Player6 )defense (PinkPlayer6, PinkPlayer6) Purple9 p asses to Purple6 Purple6 &apos; s pas s was defend ed b y Pink6 turn over (purple 6, p ink6) ball stopped upl e6s pass was defended by ink6 Pink 6 makes a short pass to Pink3 kick (PinkPlayer6) pass (PinkPlayer6, Pink Player3) Pinkgoa lie n ow h as th e ball play mod e (f ree_ kick _r) pass (PinkPlayer3, Pink Player1) Figure 2: Sample trace from Robocup English data", 
    "clean_text": "Like Liang et al (2009)'s generative alignment model, our model is designed to estimate P (w|s), where w is an NL sentence and s is a world state containing a set of possible MR logical forms that can be matched to w.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "C10-2062", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "Joohyun, Kim | Raymond J., Mooney", 
    "raw_text": "1 The natural language generation model covers the roles of both the field choice model and word choice models of Liang et al (2009)", 
    "clean_text": "The natural language generation model covers the roles of both the field choice model and word choice models of Liang et al (2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "C10-2062", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "Joohyun, Kim | Raymond J., Mooney", 
    "raw_text": "2We also tried using a Markov model to order arguments like Liang et al (2009), but preliminary experimental results showed that this additional component actually decreased performance rather than improving it", 
    "clean_text": "We also tried using a Markov model to order arguments like Liang et al (2009), but preliminary experimental results showed that this additional component actually decreased performance rather than improving it.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "C10-2062", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "Joohyun, Kim | Raymond J., Mooney", 
    "raw_text": "English Korean Chen and Mooney (2008) 0.681 0.753 Liang et al (2009) 0.757 0.694 Chen et al (2010) 0.793 0.841 Our model 0.832 0.800 Our model w/ IGSLinit 0.885 0.895 Table 2: NL? MR Matching Results (F-measure)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "C10-2062", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "Joohyun, Kim | Raymond J., Mooney", 
    "raw_text": "In particular, our proposed model outperforms the generative alignment model of Liang et al (2009), indicating that the extra linguistic information and MR grammatical structure used by Lu et al (2008)? s generative language model make our overall model more effective than a simple Markov+ bag-of-words model for language generation", 
    "clean_text": "In particular, our proposed model outperforms the generative alignment model of Liang et al (2009), indicating that the extra linguistic information and MR grammatical structure used by Lu et al (2008)'s generative language model make our overall model more effective than a simple Markov+ bag-of-words model for language generation.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "C10-2062", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "Joohyun, Kim | Raymond J., Mooney", 
    "raw_text": "For English, the improvement in matching is almost 10 percentage points in F measure, but the semantic parsing result trained with this more accurate matching shows only 1 point improvement. Compared to Liang et al (2009), our more ac curate (i.e. higher F-measure) matchings provide a clear improvement in both semantic parsing and tactical generation", 
    "clean_text": "Compared to Liang et al (2009), our more accurate (i.e. higher F-measure) matchings provide a clear improvement in both semantic parsing and tactical generation.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "C10-2062", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "Joohyun, Kim | Raymond J., Mooney", 
    "raw_text": "Our approach also learns competitive semantic parsers and improved language generators compared to previous methods. In particular, we showed that our alignments provide a better foundation for learning accurate semantic parsers and tactical generators compared to those of Liang et al (2009), whose generative model is limited by a simple bag-of-words assumption", 
    "clean_text": "In particular, we showed that our alignments provide a better foundation for learning accurate semantic parsers and tactical generators compared to those of Liang et al (2009), whose generative model is limited by a simple bag-of-words assumption.", 
    "keep_for_gold": 0
  }
]