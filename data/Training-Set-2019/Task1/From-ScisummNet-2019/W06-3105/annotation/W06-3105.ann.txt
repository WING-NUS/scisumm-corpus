Citance Number: 1 | Reference Article:  W06-3105.txt | Citing Article:  W07-0715.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>DeNero et al (2006) tried a different generative phrase translation model analogous to IBM word-translation Model 3 (Brown et al., 1993), and again found that the standard model outperformed their generative model.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this work, we first define a novel (but not radical) generative phrase-based model analogous to IBM Model 3.</S><S sid = NA ssid = NA>The generative process modeled has four steps:2 The corresponding probabilistic model for this generative process is: where P(e, ¯fi , ¯ei, a|f) factors into a segmentation model σ, a translation model φ and a distortion model d. The parameters for each component of this model are estimated differently: ing function based on absolute sentence position akin to the one used in IBM model 3.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W06-3105.txt | Citing Article:  W07-0715.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>DeNero et al (2006) attribute the inferiority of their model and the Marcu and Wong model to a hidden segmentation variable, which enables the EM algorithm to maximize the probability of the training data without really improving the quality of the model.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The generative process modeled has four steps:2 The corresponding probabilistic model for this generative process is: where P(e, ¯fi , ¯ei, a|f) factors into a segmentation model σ, a translation model φ and a distortion model d. The parameters for each component of this model are estimated differently: ing function based on absolute sentence position akin to the one used in IBM model 3.</S><S sid = NA ssid = NA>Significant approximation and pruning is required to train a generative phrase model and table – such as φEM – with hidden segmentation and alignment variables using the expectation maximization algorithm (EM).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W06-3105.txt | Citing Article:  P11-1131.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This avoids segmentation problems encountered by DeNero et al (2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Using word alignments, we can address both problems.4 In particular, we can determine for any aligned segmentation ( 1I1, eI1, a) whether it is compatible with the word-level alignment for the sentence pair.</S><S sid = NA ssid = NA>Koehn et al. (2003a) showed that translation quality is very sensitive to how this table is extracted from the training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W06-3105.txt | Citing Article:  D08-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>140k sentences up to a certain length. DeNero et al (2006) have explored estimation using EM of phrase pair probabilities under a conditional translation model based on the original source-channel formulation.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>At the core of a phrase-based statistical machine translation system is a phrase table containing pairs of source and target language phrases, each weighted by a conditional translation probability.</S><S sid = NA ssid = NA>We tested on the first 1,000 unique sentences of length 5 to 15 in the corpus and trained on sentences of length 1 to 60 starting after the first 10,000.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W06-3105.txt | Citing Article:  D08-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>These segmentations into bilingual containers (where segmentations are taken inside the containers) are different from the monolingual segmentations used in earlier comparable conditional models (e.g., (DeNero et al, 2006)) which must generate the alignment on top of the segmentations.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>While in some cases, such as idiomatic vs. literal translations, two segmentations may be in true competition, we show that the most common result is for different segmentations to be recruited for different examples, overfitting the training data and overly determinizing the phrase translation estimates.</S><S sid = NA ssid = NA>We show that estimates are overly determinized because segmentations are used in unintuitive ways for the sake of data likelihood.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W06-3105.txt | Citing Article:  D08-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>As it has been found out by (DeNero et al, 2006), it is not easy to come up with a simple, effective prior distribution over segmentations that allows for improved phrase pair estimates.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We first propose a simple generative, phrase-based model and verify that its estimates are inferior to those given by surface statistics.</S><S sid = NA ssid = NA>We define a phrase pair to be compatible with a word-alignment if no word in either phrase is aligned with a word outside the other phrase (Zens et al., 2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W06-3105.txt | Citing Article:  D08-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In contrast with the model of (DeNero et al, 2006), who define the segmentations over the source sentence f alone, our model employs bilingual containers thereby segmenting both source and target sides simultaneously.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>That is, they do not merely learn correspondences between phrases, but also segmentations of the source and target sentences.</S><S sid = NA ssid = NA>In this work, we first define a novel (but not radical) generative phrase-based model analogous to IBM Model 3.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W06-3105.txt | Citing Article:  D08-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Therefore, unlike (DeNeroet al, 2006), our model does not need to generate the word-alignments explicitly, as these are embedded in the segmentations.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The key virtue of competition in word alignment is that, to a first approximation, only one source word should generate each target word.</S><S sid = NA ssid = NA>Unlike previous attempts to train a similar model (Marcu and Wong, 2002), we allow information from a word-alignment model to inform our approximation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W06-3105.txt | Citing Article:  D08-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We did not explore mere EM without any smoothing or ITG prior, as we expect it will directly over fit the training data as reported by (DeNero et al, 2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Thus, after training, the parameters of the phrase translation model φEM can be used directly for decoding.</S><S sid = NA ssid = NA>Koehn et al. (2003a) showed that translation quality is very sensitive to how this table is extracted from the training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W06-3105.txt | Citing Article:  D08-1066.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The most similar efforts to ours, mainly (DeNero et al, 2006), conclude that segmentation variables in the generative translation model lead to overfitting while attaining higher likelihood of the training data than the heuristic estimator.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In cases of true ambiguity in the language pair to be translated, parameter estimates that explain the ambiguity using segmentation variables can in some cases yield higher data likelihoods by determinizing phrase translation estimates.</S><S sid = NA ssid = NA>The performance gap stems primarily from the addition of a hidden segmentation variable, which increases the capacity for overfitting during maximum likelihood training with EM.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W06-3105.txt | Citing Article:  D09-1107.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>While theoretically sound, this approach is computationally challenging both in practice (DeNero et al, 2008) and in theory (DeNero and Klein, 2008), may suffer from reference reachability problems (DeNero et al, 2006), and in the end may lead to inferior translation quality (Koehn et al, 2003).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Koehn et al. (2003a) showed that translation quality is very sensitive to how this table is extracted from the training data.</S><S sid = NA ssid = NA>In practice, however, this approach has not led to an improvement in BLEU.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W06-3105.txt | Citing Article:  D09-1107.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>thus generates a number of potentially overlapping in (DeNero et al, 2006), the ambiguity in word alignment is less prevalent than in phrase segmentation.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We define a phrase pair to be compatible with a word-alignment if no word in either phrase is aligned with a word outside the other phrase (Zens et al., 2002).</S><S sid = NA ssid = NA>Specifically, in the task of word alignment, heuristic approaches such as the Dice coefficient consistently underperform their re-estimated counterparts, such as the IBM word alignment models (Brown et al., 1993).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W06-3105.txt | Citing Article:  W10-1711.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For the German English, French English and English French language tasks we applied a forced alignment procedure to train the phrase translation model with the EM algorithm ,similar to the one described in (DeNero et al,2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The learned parameters from this model will be used to translate sentences from English to French.</S><S sid = NA ssid = NA>The generative process we modeled produces a phrase-aligned English sentence from a French sentence where the former is a translation of the latter.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W06-3105.txt | Citing Article:  P09-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This explicitly avoids the degenerate solutions of maximum likelihood estimation (DeNero et al, 2006), without resort to the heuristic estimator of Koehn et al (2003).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Koehn et al. (2003a) showed that translation quality is very sensitive to how this table is extracted from the training data.</S><S sid = NA ssid = NA>The top-performing diag-and extraction heuristic (Zens et al., 2002) serves as the baseline for evaluation.1 Each approach – the generative model and heuristic baseline – produces an estimated conditional distribution of English phrases given French phrases.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W06-3105.txt | Citing Article:  P09-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Phrasal SCFG models are subject to a degenerate maximum likelihood solution in which all probability mass is placed on long, or whole sentence, phrase translations (DeNero et al, 2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>As more probability mass is reserved for fewer translations, many of the alternative translations under φH are assigned prohibitively small probabilities.</S><S sid = NA ssid = NA>A maximum phrase length of three was used for all experiments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W06-3105.txt | Citing Article:  D08-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>DeNero et al (2006) instead proposed an exponential-time dynamic program pruned using word alignments.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This training loop necessitates approximation because summing over all possible segmentations and alignments for each sentence is intractable, requiring time exponential in the length of the sentences.</S><S sid = NA ssid = NA>We define a phrase pair to be compatible with a word-alignment if no word in either phrase is aligned with a word outside the other phrase (Zens et al., 2002).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W06-3105.txt | Citing Article:  D09-1037.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The heuristic method is inconsistent in the limit (Johnson, 2002) while EM is degenerate, placing disproportionate probability mass on the largest rules in order to describe the data with as few a rules as possible (DeNero et al, 2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The top-performing diag-and extraction heuristic (Zens et al., 2002) serves as the baseline for evaluation.1 Each approach – the generative model and heuristic baseline – produces an estimated conditional distribution of English phrases given French phrases.</S><S sid = NA ssid = NA>Koehn et al. (2003a) showed that translation quality is very sensitive to how this table is extracted from the training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W06-3105.txt | Citing Article:  W10-2915.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>A remaining challenge is to design more appropriate statistical models which tie segmentations together unless sufficient evidence of true non-compositionality is present; perhaps such models could properly combine the benefits of both current approaches.</S><S sid = NA ssid = NA>However, the results in figure 1 of the following section show that OEM trained on twice as much data as OH still underperforms the heuristic, indicating a larger issue than decreased training set size.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W06-3105.txt | Citing Article:  D11-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>If we only use the features as traditional SCFG systems, the bi parsing may end with a derivation consists of some giant rules or rules with rare source/target sides, which is called degenerate solution (DeNero et al, 2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>That is, they do not merely learn correspondences between phrases, but also segmentations of the source and target sentences.</S><S sid = NA ssid = NA>To test the relative performance of OEM and OH, we evaluated each using an end-to-end translation system from English to French.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W06-3105.txt | Citing Article:  P08-1024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This is illustrated in Table 2, which shows the conditional probabilities for rules, obtained by locally normalising the rule feature weights for a simple grammar extracted from the ambiguous pair of sentences presented in DeNero et al (2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Koehn et al. (2003a) showed that translation quality is very sensitive to how this table is extracted from the training data.</S><S sid = NA ssid = NA>The ambiguous process of translation can be modeled either by the latent segmentation variable or the phrase translation probabilities.</S> | Discourse Facet:  NA | Annotator: Automatic


