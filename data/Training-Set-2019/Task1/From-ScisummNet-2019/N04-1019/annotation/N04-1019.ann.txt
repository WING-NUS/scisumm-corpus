Citance Number: 1 | Reference Article:  N04-1019.txt | Citing Article:  W04-3254.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >It also corresponds to the pyramid measure proposed by Nenkova and Passonneau (2004), which also considers an estimation of the maximum value reachable.</S> | Reference Offset:  ['144','148'] | Reference Text:  <S sid = 144 ssid = >The expected value for the point of divergence of scores, in terms of number of summaries in the pyramid, is 5.5.</S><S sid = 148 ssid = >First, this corresponds to the difference in PAL scores (D31041) we find when we use a different one of our three PAL annotations (see Table 1).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N04-1019.txt | Citing Article:  P14-2041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In evaluations of summarization algorithms, it is common practice to derive the gold standard con tent importance scores from human summaries, as done, for example, in the pyramid method, where the importance of a content element corresponds to the number of reference human summaries that make use of it (Nenkova and Passonneau, 2004).</S> | Reference Offset:  ['3','21'] | Reference Text:  <S sid = 3 ssid = >Our method quantifies the relative importance of facts to be conveyed.</S><S sid = 21 ssid = >Evaluation involves comparison of a peer summary (baseline, or produced by human or system) by comparing its content to a gold standard, or model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N04-1019.txt | Citing Article:  C08-1087.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To evaluate our system, we use the pyramid evaluation method (Nenkova and Passonneau, 2004) at sentence level.</S> | Reference Offset:  ['97','117'] | Reference Text:  <S sid = 97 ssid = >In (Passonneau, 2004), we report our method for computing reliability for coreference annotations, and the use of a distance metric that allows us to weight disagreements.</S><S sid = 117 ssid = >For the DUC scores there was always a single model, and no attempt to evaluate the model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N04-1019.txt | Citing Article:  C08-1087.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Each fact in the citation summary of a paper is a summarization content unit (SCU) (Nenkova and Passonneau, 2004), and the fact distribution matrix, created by annotation, provides the information about the importance of each fact in the citation summary.</S> | Reference Offset:  ['6','131'] | Reference Text:  <S sid = 6 ssid = >Our approach acknowledges the fact that no single best model summary exists, and takes this as a foundation rather than an obstacle.</S><S sid = 131 ssid = >This makes sense in light of the fact that a score is dominated by the higher weight SCUS that appear in a summary.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N04-1019.txt | Citing Article:  C10-1101.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >These annotations give the list of nuggets covered by each sentence in each citation summary, which are equivalent to the summarization content unit (SCU) as described in (Nenkova and Passonneau, 2004).</S> | Reference Offset:  ['19','90'] | Reference Text:  <S sid = 19 ssid = >A more detailed account of the work described here, but not including the study of distributional properties of pyramid scores, can be found in (Passonneau and Nenkova, 2003).</S><S sid = 90 ssid = >SCU annotation involves two types of choices: extracting a contributor from a sentence, and assigning it to an SCU.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N04-1019.txt | Citing Article:  N09-1070.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The second metric we use is similar to Pyramid (Nenkova and Passonneau, 2004), which has been used for summarization evaluation.</S> | Reference Offset:  ['30','97'] | Reference Text:  <S sid = 30 ssid = >The use of a single model summary is one of the surprises – all research in summarization evaluation has indicated that no single good model exists.</S><S sid = 97 ssid = >In (Passonneau, 2004), we report our method for computing reliability for coreference annotations, and the use of a distance metric that allows us to weight disagreements.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N04-1019.txt | Citing Article:  N07-2055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the PYRAMID scheme for manual evaluation of summaries (Nenkova and Passonneau, 2004), machine-generated summaries were compared with human-written ones at the nugget level.</S> | Reference Offset:  ['34','104'] | Reference Text:  <S sid = 34 ssid = >Human-written summaries can score as low as 0.1 while machine summaries can score as high as 0.5.</S><S sid = 104 ssid = >Here we use three DUC 2003 summary sets for which four human summaries were written.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N04-1019.txt | Citing Article:  N07-2055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >the PYRAMID framework (Nenkova and Passonneau, 2004) was used for manual summary evaluations.</S> | Reference Offset:  ['19','208'] | Reference Text:  <S sid = 19 ssid = >A more detailed account of the work described here, but not including the study of distributional properties of pyramid scores, can be found in (Passonneau and Nenkova, 2003).</S><S sid = 208 ssid = >The pyramid method not only assigns a score to a summary, but also allows the investigator to find what important information is missing, and thus can be directly used to target improvements of the summarizer.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N04-1019.txt | Citing Article:  D09-1032.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Pyramid evaluation: The pyramid evaluation method (Nenkova and Passonneau, 2004) has been developed for reliable and diagnostic assessment of content selection quality in summarization and has been used in several large scale evaluations (Nenkova et al, 2007).</S> | Reference Offset:  ['0','207'] | Reference Text:  <S sid = 0 ssid = >Evaluating Content Selection In Summarization: The Pyramid Method</S><S sid = 207 ssid = >The strengths of pyramid scores are that they are reliable, predictive, and diagnostic.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N04-1019.txt | Citing Article:  P06-1108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Paralleling work in summarization, it is hypothesized that the quality of a rewritten story can be defined by the presence or absence of 'semantic content units' that are crucial details of the text that may have a variety of syntactic forms (Nenkova and Passonneau, 2004).</S> | Reference Offset:  ['39','92'] | Reference Text:  <S sid = 39 ssid = >Our analysis of summary content is based on Summarization Content Units, or SCUs and we will now proceed to define the concept.</S><S sid = 92 ssid = >Our approach is to separate syntactic from semantic agreement, as in (Klavans et al., 2003).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N04-1019.txt | Citing Article:  N06-1057.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Three most noticeable efforts in manual evaluation are SEE (Lin and Hovy, 2001), Factoid (Van Halteren and Teufel, 2003), and the Pyramid method (Nenkova and Passonneau, 2004).</S> | Reference Offset:  ['63','201'] | Reference Text:  <S sid = 63 ssid = >We do not attempt to represent the subsumption or implicational relations that Halteren and Teufel assign to factoids (Halteren and Teufel, 2003).</S><S sid = 201 ssid = >Lin and Hovy (2002) and Lin and Hovy (2003) were the first to systematically point out problems with the large scale DUC evaluation and to look to solutions by seeking more robust automatic alternatives.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N04-1019.txt | Citing Article:  N06-1057.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This paraphrase matching process is observed in the Pyramid annotation procedure shown in (Nenkova and Passonneau, 2004) over three summary sets (10 summaries each).</S> | Reference Offset:  ['57','64'] | Reference Text:  <S sid = 57 ssid = >The label, which is subject to revision throughout the annotation process, has three functions.</S><S sid = 64 ssid = >After the annotation procedure is completed, the final SCUs can be partitioned in a pyramid.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N04-1019.txt | Citing Article:  N06-1057.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >From an in-depth analysis on the manually created SCUs of the DUC2003 summary set D30042 (Nenkova and Passonneau, 2004), we find that 54.48% of 1746 cases where a non-stop word from one SCU did not match with its supposedly human-aligned pairing SCUs are in need of some level of paraphrase matching support.</S> | Reference Offset:  ['39','109'] | Reference Text:  <S sid = 39 ssid = >Our analysis of summary content is based on Summarization Content Units, or SCUs and we will now proceed to define the concept.</S><S sid = 109 ssid = >Then we present results demonstrating the need for at least five summaries per pyramid, given this corpus of 100-word summaries.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N04-1019.txt | Citing Article:  N07-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To assess the quality of system responses, we adopt the nugget-based methodology used previously for many types of complex questions (Voorhees, 2003), which shares similarities with the pyramid evaluation scheme used in summarization (Nenkova and Passonneau, 2004).</S> | Reference Offset:  ['7','25'] | Reference Text:  <S sid = 7 ssid = >In machine translation, the rankings from the automatic BLEU method (Papineni et al., 2002) have been shown to correlate well with human evaluation, and it has been widely used since and has even been adapted for summarization (Lin and Hovy, 2003).</S><S sid = 25 ssid = >The procedure used for evaluating summaries in DUC is the following: The final score is based on the content unit coverage.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N04-1019.txt | Citing Article:  P11-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >They report a marginal increase in the automatic word overlap metric ROUGE (Lin, 2004), but a decline in manual Pyramid (Nenkova and Passonneau, 2004).</S> | Reference Offset:  ['32','97'] | Reference Text:  <S sid = 32 ssid = >Additionally, the task of determining the percentage overlap between two text units turns out to be difficult to annotate reliably – (Lin and Hovy, 2002) report that humans agreed with their own prior judgment in only 82% of the cases.</S><S sid = 97 ssid = >In (Passonneau, 2004), we report our method for computing reliability for coreference annotations, and the use of a distance metric that allows us to weight disagreements.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  N04-1019.txt | Citing Article:  P11-1049.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Pyramid (Nenkova and Passonneau, 2004) is a manually evaluated measure of recall on facts or Semantic Content Units appearing in the reference summaries.</S> | Reference Offset:  ['11','179'] | Reference Text:  <S sid = 11 ssid = >Our method involves semantic matching of content units to which differential weights are assigned based on their frequency in a corpus of summaries.</S><S sid = 179 ssid = >Thus scores can be computed using one, two and so on up to five reference summaries.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  N04-1019.txt | Citing Article:  P07-2015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The pyramid method (Nenkova and Passonneau, 2004) addresses the problem by using multiple human summaries to create a gold-standard.</S> | Reference Offset:  ['21','179'] | Reference Text:  <S sid = 21 ssid = >Evaluation involves comparison of a peer summary (baseline, or produced by human or system) by comparing its content to a gold standard, or model.</S><S sid = 179 ssid = >Thus scores can be computed using one, two and so on up to five reference summaries.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  N04-1019.txt | Citing Article:  W05-0906.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >As an example, the Pyramid Method (Nenkova and Passonneau, 2004), represents a good first attempt at a realistic model of human variations.</S> | Reference Offset:  ['30','117'] | Reference Text:  <S sid = 30 ssid = >The use of a single model summary is one of the surprises – all research in summarization evaluation has indicated that no single good model exists.</S><S sid = 117 ssid = >For the DUC scores there was always a single model, and no attempt to evaluate the model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  N04-1019.txt | Citing Article:  P12-1106.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Nenkova and Passonneau (2004) proposed a manual evaluation method that was based on the idea that there is no single best model summary for a collection of documents.</S> | Reference Offset:  ['2','30'] | Reference Text:  <S sid = 2 ssid = >It incorporates the idea that no single best model summary for a collection of documents exists.</S><S sid = 30 ssid = >The use of a single model summary is one of the surprises – all research in summarization evaluation has indicated that no single good model exists.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  N04-1019.txt | Citing Article:  C10-2122.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In recent years the Pyramids evaluation method (Nenkova and Passonneau, 2004) was introduced.</S> | Reference Offset:  ['29','97'] | Reference Text:  <S sid = 29 ssid = >There are numerous problems with the DUC human evaluation method.</S><S sid = 97 ssid = >In (Passonneau, 2004), we report our method for computing reliability for coreference annotations, and the use of a distance metric that allows us to weight disagreements.</S> | Discourse Facet:  NA | Annotator: Automatic


