Citance Number: 1 | Reference Article:  W11-2123.txt | Citing Article:  W11-2138.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We used common tools for phrase-based translation Moses (Koehn et al, 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modelling and GIZA++ (Och and Ney, 2000) for word alignments.</S> | Reference Offset:  ['21','97'] | Reference Text:  <S sid = 21 ssid = >Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.</S><S sid = 97 ssid = >SRILM (Stolcke, 2002) is widely used within academia.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W11-2123.txt | Citing Article:  P14-2022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The language model was compiled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run.</S> | Reference Offset:  ['0','223'] | Reference Text:  <S sid = 0 ssid = >KenLM: Faster and Smaller Language Model Queries</S><S sid = 223 ssid = >The binary language model from Section 5.2 and text phrase table were forced into disk cache before each run.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W11-2123.txt | Citing Article:  W12-3145.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >While this increases the number of LM queries, we exploit the language model state in formation in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states.</S> | Reference Offset:  ['0','151'] | Reference Text:  <S sid = 0 ssid = >KenLM: Faster and Smaller Language Model Queries</S><S sid = 151 ssid = >In this section, we extend state to optimize left-to-right queries.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W11-2123.txt | Citing Article:  W12-3131.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our translation system uses cdec (Dyer et al,2010), an implementation of the hierarchical phrase based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference.</S> | Reference Offset:  ['21','268'] | Reference Text:  <S sid = 21 ssid = >Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.</S><S sid = 268 ssid = >For example, syntactic decoders (Koehn et al., 2007; Dyer et al., 2010; Li et al., 2009) perform dynamic programming parametrized by both backward- and forward-looking state.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W11-2123.txt | Citing Article:  W12-3154.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The three data sets in use in this paper are summarised in Table 1.The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al, 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime.</S> | Reference Offset:  ['12','21'] | Reference Text:  <S sid = 12 ssid = >Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.</S><S sid = 21 ssid = >Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W11-2123.txt | Citing Article:  P12-2058.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011).</S> | Reference Offset:  ['108','133'] | Reference Text:  <S sid = 108 ssid = >Compared with SRILM, IRSTLM adds several features: lower memory consumption, a binary file format with memory mapping, caching to increase speed, and quantization.</S><S sid = 133 ssid = >When two partial hypotheses have equal state (including that of other features), they can be recombined and thereafter efficiently handled as a single packed hypothesis.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W11-2123.txt | Citing Article:  W11-2139.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Inference was carried out using the language modeling library described by Heafield (2011).</S> | Reference Offset:  ['19','274'] | Reference Text:  <S sid = 19 ssid = >These packages are further described in Section 3.</S><S sid = 274 ssid = >We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W11-2123.txt | Citing Article:  P13-2003.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We used the MADA ATB segmentation for Arabic (Roth et al, 2008) and true casing for English, phrases of maximal length 7, KneserNey smoothing, and lexicalized reordering (Koehn et al, 2005), and a 5-gram language model, trained on GigaWordv.5 using KenLM (Heafield, 2011).</S> | Reference Offset:  ['21','268'] | Reference Text:  <S sid = 21 ssid = >Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.</S><S sid = 268 ssid = >For example, syntactic decoders (Koehn et al., 2007; Dyer et al., 2010; Li et al., 2009) perform dynamic programming parametrized by both backward- and forward-looking state.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W11-2123.txt | Citing Article:  W12-3134.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language models by Heafield (2011) and Pauls and Klein (2011)? both language model implementations are now integrated with Joshua.</S> | Reference Offset:  ['114','264'] | Reference Text:  <S sid = 114 ssid = >Later, BerkeleyLM (Pauls and Klein, 2011) described ideas similar to ours.</S><S sid = 264 ssid = >For even larger models, storing counts (Talbot and Osborne, 2007; Pauls and Klein, 2011; Guthrie and Hepple, 2010) is a possibility.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W11-2123.txt | Citing Article:  W12-3134.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets.</S> | Reference Offset:  ['92','283'] | Reference Text:  <S sid = 92 ssid = >To quantize, we use the binning method (Federico and Bertoldi, 2006) that sorts values, divides into equally sized bins, and averages within each bin.</S><S sid = 283 ssid = >Nicola Bertoldi and Marcello Federico assisted with IRSTLM.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W11-2123.txt | Citing Article:  W12-3134.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult to compile SRILM toolkit (Stolcke, 2002).</S> | Reference Offset:  ['51','114'] | Reference Text:  <S sid = 51 ssid = >This differs from other implementations (Stolcke, 2002; Pauls and Klein, 2011) that use hash tables as nodes in a trie, as explained in the next section.</S><S sid = 114 ssid = >Later, BerkeleyLM (Pauls and Klein, 2011) described ideas similar to ours.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W11-2123.txt | Citing Article:  W12-3160.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >This was used to create a KenLM (Heafield, 2011).</S> | Reference Offset:  ['0','189'] | Reference Text:  <S sid = 0 ssid = >KenLM: Faster and Smaller Language Model Queries</S><S sid = 189 ssid = >The same numbers were used for each data structure.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W11-2123.txt | Citing Article:  W12-3706.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the Opinum system we query the M p, M n models with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable for a web application.</S> | Reference Offset:  ['1','103'] | Reference Text:  <S sid = 1 ssid = >We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S sid = 103 ssid = >IRSTLM (Federico et al., 2008) is an open-source toolkit for building and querying language models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W11-2123.txt | Citing Article:  W11-2147.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our base line is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al, 2007) for translation model training and decoding, GIZA++ (Ochand Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights.</S> | Reference Offset:  ['12','21'] | Reference Text:  <S sid = 12 ssid = >Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.</S><S sid = 21 ssid = >Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W11-2123.txt | Citing Article:  E12-1083.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011).</S> | Reference Offset:  ['13','103'] | Reference Text:  <S sid = 13 ssid = >IRSTLM 5.60.02 (Federico et al., 2008) is a sorted trie implementation designed for lower memory consumption.</S><S sid = 103 ssid = >IRSTLM (Federico et al., 2008) is an open-source toolkit for building and querying language models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W11-2123.txt | Citing Article:  P12-1002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Simaan,2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well.3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using kenlm (Heafield, 2011).</S> | Reference Offset:  ['97','199'] | Reference Text:  <S sid = 97 ssid = >SRILM (Stolcke, 2002) is widely used within academia.</S><S sid = 199 ssid = >For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W11-2123.txt | Citing Article:  D12-1108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3.</S> | Reference Offset:  ['0','131'] | Reference Text:  <S sid = 0 ssid = >KenLM: Faster and Smaller Language Model Queries</S><S sid = 131 ssid = >Dynamic programming efficiently scores many hypotheses by exploiting the fact that an N-gram language model conditions on at most N âˆ’ 1 preceding words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W11-2123.txt | Citing Article:  P12-2006.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Research efforts to increase search efficiency for phrase-based MT (Koehn et al, 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al, 2006) to additional early pruning techniques (Delaney et al, 2006), (Moore and Quirk, 2007) and more efficient language model (LM) querying (Heafield, 2011).</S> | Reference Offset:  ['21','268'] | Reference Text:  <S sid = 21 ssid = >Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.</S><S sid = 268 ssid = >For example, syntactic decoders (Koehn et al., 2007; Dyer et al., 2010; Li et al., 2009) perform dynamic programming parametrized by both backward- and forward-looking state.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W11-2123.txt | Citing Article:  P13-2073.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For English language modeling, we use English Giga word Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011).</S> | Reference Offset:  ['199','205'] | Reference Text:  <S sid = 199 ssid = >For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid = 205 ssid = >We evaluate the time and memory consumption of each data structure by computing perplexity on 4 billion tokens from the English Gigaword corpus (Parker et al., 2009).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W11-2123.txt | Citing Article:  P13-1109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with KneserNey smoothing.</S> | Reference Offset:  ['0','199'] | Reference Text:  <S sid = 0 ssid = >KenLM: Faster and Smaller Language Model Queries</S><S sid = 199 ssid = >For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S> | Discourse Facet:  NA | Annotator: Automatic


