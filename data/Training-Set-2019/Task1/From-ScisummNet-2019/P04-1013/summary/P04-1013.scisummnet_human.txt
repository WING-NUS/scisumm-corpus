Discriminative Training Of A Neural Network Statistical Parser
Discriminative methods have shown significant improvements over traditional generative methods in many machine learning applications, but there has been difficulty in extending them to natural language parsing.
One problem is that much of the work on discriminative methods conflates changes to the learning method with changes to the parameterization of the problem.
We show how a parser can be trained with a discriminative learning method while still parameterizing the problem according to a generative probability model.
We present three methods for training a neural network to estimate the probabilities for a statistical parser, one generative, one discriminative, and one where the probability model is generative but the training criteria is discriminative.
The latter model outperforms the previous two, achieving state-of-the-art levels of performance (90.1% F-measure on constituents).
We provide a discussion of why generative models are better than models parameterized to estimate the a posteriori probability directly.
We use neural networks to induce latent left-corner parser states.
We find that discriminative training was too slow, and reports accuracy higher than generative models by discriminatively reranking the output of the generative model.
