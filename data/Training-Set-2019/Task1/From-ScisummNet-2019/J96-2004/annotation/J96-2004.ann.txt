Citance Number: 1 | Reference Article:  J96-2004.txt | Citing Article:  W96-0402.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>As advocated by Carletta (1996), we have used the Kappa coefficient (Siegel and Castellan, 1988) as a measure of coder agreement.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We first explain what effect chance expected agreement has on each of these measures, and then argue that we should adopt the kappa statistic (Siegel and Castellan 1988) as a uniform measure of reliability.</S><S sid = NA ssid = NA>(For complete instructions on how to calculate K, see Siegel and Castellan [1988].)</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  J96-2004.txt | Citing Article:  P14-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Following the works of Carletta (1996) and Artstein and Poesio (2008), there is an increasing consensus within the field that in order to properly gauge the reliability of an annotation effort, chance-corrected measures of inter-annotator agreement should be used.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For instance, consider the following arguments for reliability.</S><S sid = NA ssid = NA>Unfortunately, as a field we have not yet come to agreement about how to show reliability of judgments.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  J96-2004.txt | Citing Article:  P14-1088.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>(Cohen, 1960, introduced to computational linguistics by Carletta, 1996) and pi (Scott, 1955).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We discuss what is wrong with reliability measures as they are currently used for discourse and dialogue work in computational linguistics and cognitive science, and argue that we would be better off as afield adopting techniques from content analysis.</S><S sid = NA ssid = NA>We discuss what is wrong with reliability measures as they are currently used for discourse and dialogue work in computational linguistics and cognitive science, and argue that we would be better off as afield adopting techniques from content analysis.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  J96-2004.txt | Citing Article:  W04-0216.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The reliability of the annotation was evaluated using the kappa statistic (Carletta, 1996).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Assessing Agreement On Classification Tasks: The Kappa Statistic</S><S sid = NA ssid = NA>We first explain what effect chance expected agreement has on each of these measures, and then argue that we should adopt the kappa statistic (Siegel and Castellan 1988) as a uniform measure of reliability.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  J96-2004.txt | Citing Article:  P09-1101.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>(Carletta 1996) is another method of comparing inter-annotator agreement.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In Passonneau and Litman, the reason for comparing to the majority opinion is less clear.</S><S sid = NA ssid = NA>Comparing naive and expert coding as KID do can be a useful exercise, but rather than assessing the naive coders' accuracy, it in fact measures how well the instructions convey what these researchers think they do.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  J96-2004.txt | Citing Article:  D08-1021.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We measured inter-annotator agreement with the Kappa statistic (Carletta, 1996) using the 1,391 items that two annotators scored in common.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Assessing Agreement On Classification Tasks: The Kappa Statistic</S><S sid = NA ssid = NA>In assessing the amount of agreement among coders of category distinctions, the kappa statistic normalizes for the amount of expected chance agreement and allows a single measure to be calculated over multiple coders.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  J96-2004.txt | Citing Article:  P09-1095.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>To measure inter-annotator agreement, we compute Cohen's Kappa (Carletta, 1996) from the two sets of annotations.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In assessing the amount of agreement among coders of category distinctions, the kappa statistic normalizes for the amount of expected chance agreement and allows a single measure to be calculated over multiple coders.</S><S sid = NA ssid = NA>Measure (3) still falls foul of the same problem with expected chance agreement as measure (2) because it does not take into account the number of categories occurring in the coding scheme.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  J96-2004.txt | Citing Article:  W07-1707.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Obtained percent agreement of 0.988 and coefficient (Carletta, 1996) of 0.975 suggest high convergence of both annotations.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We suggest that this measure be adopted more widely within our own research community.</S><S sid = NA ssid = NA>One would expect measure (1)'s results to be high under any circumstances, and it is not affected by the density of boundaries.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  J96-2004.txt | Citing Article:  D10-1055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>P (A) is the observed agreement between annotators and P (E) is the probability of agreement due to chance (Carletta, 1996).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>When there is no agreement other than that which would be expected by chance, K is zero.</S><S sid = NA ssid = NA>In assessing the amount of agreement among coders of category distinctions, the kappa statistic normalizes for the amount of expected chance agreement and allows a single measure to be calculated over multiple coders.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  J96-2004.txt | Citing Article:  W07-2007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Annotation was highly reliable with a kappa (Carletta, 1996) of 3.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Silverman et al. treat all coders indistinguishably, although they do build an interesting argument about how agreement levels shift when a number of less-experienced transcribers are added to a pool of highly experienced ones.</S><S sid = NA ssid = NA>Currently, computational linguists and cognitive scientists working in the area of discourse and dialogue argue that their subjective judgments are reliable using several different statistics, none of which are easily interpretable or comparable to each other.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  J96-2004.txt | Citing Article:  W03-2802.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>With the help of the kappa coefficient (Carletta, 1996) proposes to represent the dialog success independently from the task intrinsic complexity, thus opening the way to task generic comparative evaluation.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Whether we have reached (or will be able to reach) a reasonable level of agreement in our work as a field remains to be seen; our point here is merely that if, as a community, we adopt clearer statistics, we will be able to compare results in a standard way across different coding schemes and experiments and to evaluate current developments—and that will illuminate both our individual results and the way forward.</S><S sid = NA ssid = NA>The concerns of these researchers are largely the same as those in the field of content analysis (see especially Krippendorff [1980] and Weber [1985]), which has been through the same problems as we are currently facing and in which strong arguments have been made for using the kappa coefficient of agreement (Siegel and Castellan 1988) as a measure of reliability!</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  J96-2004.txt | Citing Article:  W04-2703.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>To test the reliability of the annotation, we first considered the kappa statistic (Siegel and Castellan, 1988) which is used extensively in empirical studies of dis course (Carletta, 1996).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We first explain what effect chance expected agreement has on each of these measures, and then argue that we should adopt the kappa statistic (Siegel and Castellan 1988) as a uniform measure of reliability.</S><S sid = NA ssid = NA>(For complete instructions on how to calculate K, see Siegel and Castellan [1988].)</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  J96-2004.txt | Citing Article:  N10-1113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Inter-annotator agreement was sufficient (κ = 0.77 on average (Carletta, 1996)).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>At one time, it was considered sufficient when working with such judgments to show examples based on the authors' interpretation (paradigmatically, (Grosz and Sidner [1986], but also countless others).</S><S sid = NA ssid = NA>If there are two categories occurring in equal proportions, on average the coders would agree with each other half of the time: each time the second coder makes a choice, there is a fifty/fifty chance of coming up with the same category as the first coder.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  J96-2004.txt | Citing Article:  W04-0807.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In addition to raw inter-tagger agreement, the kappa statistic, which removes from the agreement rate the amount of agreement that is expected by chance (Carletta, 1996), was also determined.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>When there is no agreement other than that which would be expected by chance, K is zero.</S><S sid = NA ssid = NA>In assessing the amount of agreement among coders of category distinctions, the kappa statistic normalizes for the amount of expected chance agreement and allows a single measure to be calculated over multiple coders.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  J96-2004.txt | Citing Article:  W06-1318.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Following the suggestions in (Carletta, 1996), Core et al. consider kappa scores above 0.67 to indicate significant agreement and scores above 0.8 reliable agreement.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For instance, consider the following arguments for reliability.</S><S sid = NA ssid = NA>KID make no comment about the meaning of their figures other than to say that the amount of agreement they show is reasonable; Silverman et al. simply point out that where figures are calculated over different numbers of categories, they are not comparable.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  J96-2004.txt | Citing Article:  W11-2035.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In order to test the reliability of our annotation, we double coded about 18% of the data, namely 21 sub-dialogues comprising 213 pronouns, on which we computed the Kappa coefficient (Carletta, 1996).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The concerns of these researchers are largely the same as those in the field of content analysis (see especially Krippendorff [1980] and Weber [1985]), which has been through the same problems as we are currently facing and in which strong arguments have been made for using the kappa coefficient of agreement (Siegel and Castellan 1988) as a measure of reliability!</S><S sid = NA ssid = NA>It is possible, and sometimes useful, to test whether or not K is significantly different from chance, but more importantly, interpretation of the scale of agreement is possible.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  J96-2004.txt | Citing Article:  W03-2115.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>As reported elsewhere the resulting Kappa statistics (Carletta, 1996) over the annotated data yields 0.7.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>It is more important to ask how different the results are from random and whether or not the data produced by coding is too noisy to use for the purpose for which it was collected.</S><S sid = NA ssid = NA>Currently, computational linguists and cognitive scientists working in the area of discourse and dialogue argue that their subjective judgments are reliable using several different statistics, none of which are easily interpretable or comparable to each other.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  J96-2004.txt | Citing Article:  P10-1142.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Since co-reference is a clustering task, any general-purpose method for evaluating a response partition against a key partition (e.g., Kappa (Carletta, 1996)) can be used for coreference scoring (see Popescu-Belis et al (2004)).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Krippendorff's a is more general than Siegel and Castellan's K in that Krippendorff extends the argument from category data to interval and ratio scales; this extension might be useful for, for instance, judging the reliability of TOBI break index coding, since some researchers treat these codes as inherently scalar (Silverman et al. 1992).</S><S sid = NA ssid = NA>KID make no comment about the meaning of their figures other than to say that the amount of agreement they show is reasonable; Silverman et al. simply point out that where figures are calculated over different numbers of categories, they are not comparable.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  J96-2004.txt | Citing Article:  C04-1034.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The reliability for the two annotation tasks (statistics (Carletta, 1996)) was of 0.94 and 0.90 respectively.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Assessing Agreement On Classification Tasks: The Kappa Statistic</S><S sid = NA ssid = NA>However, the four measures of reliability bear no relationship to each other.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  J96-2004.txt | Citing Article:  S12-1048.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>From the first effort an inter-annotator agreement (Carletta, 1996) of 0.89 for Cohen's kappa was obtained.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>On the other hand, Passonneau and Litman note that their figures are not properly interpretable and attempt to overcome this failing to some extent by showing that the agreement which they have obtained at least significantly differs from random agreement.</S><S sid = NA ssid = NA>Assessing Agreement On Classification Tasks: The Kappa Statistic</S> | Discourse Facet:  NA | Annotator: Automatic


