We present a probabilistic parsing model for German trained on the Negra treebank.
We observe that existing lexicalized parsing models using head-head dependencies, while successful for English, fail to outperform an unlexicalized baseline model for German.
Learning curves show that this effect is not due to lack of training data.
We propose an alternative model that uses sister-head dependencies instead of head-head dependencies.
This model outperforms the baseline, achieving a labeled precision and recall of up to 74%.
This indicates that sister-head dependencies are more appropriate for treebanks with very flat structures such as Negra.
