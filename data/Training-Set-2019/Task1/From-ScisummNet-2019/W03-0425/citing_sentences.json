[
  {
    "citance_No": 1, 
    "citing_paper_id": "W03-1026", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Hongyan, Jing | Radu, Florian | Xiaoqiang, Luo | Tong, Zhang | Abraham, Ittycheriah", 
    "raw_text": "Classifier combination has been shown to be effective in improving the performance of NLP applications, and have been investigated by Brill and Wu (1998) and van Halteren et al (2001) for part-of-speech tagging, Tjong Kim Sang et al (2000) for base noun phrase chunking, and Florian et al (2003a) for word sense disambiguation", 
    "clean_text": "Classifier combination has been shown to be effective in improving the performance of NLP applications, and have been investigated by Brill and Wu (1998) and van Halteren et al (2001) for part-of-speech tagging, Tjong Kim Sang et al (2000) for base noun phrase chunking, and Florian et al (2003a) for word sense disambiguation.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "W03-1026", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Hongyan, Jing | Radu, Florian | Xiaoqiang, Luo | Tong, Zhang | Abraham, Ittycheriah", 
    "raw_text": "We also applied the classifier combination technique discussed in this paper to English and German (Florian et al, 2003b)", 
    "clean_text": "We also applied the classifier combination technique discussed in this paper to English and German (Florian et al, 2003b).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P05-1001", 
    "citing_paper_authority": 36, 
    "citing_paper_authors": "Rie Kubota, Ando | Tong, Zhang", 
    "raw_text": "Previous best results: FIJZ03 (Florian et al, 2003), CN03 (Chieu and Ng, 2003), KSNM03 (Klein et al, 2003)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "D07-1083", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Jun, Suzuki | Akinori, Fujino | Hideki, Isozaki", 
    "raw_text": "More over, the experimental results shown in Tables 3 and F? =1 additional resources ASO-semi 89.31 unlabeled data (27M words) (Ando and Zhang, 2005) (Florian et al, 2003) 88.76 their own large gazetteers, 2M-word labeled data (Chieu and Ng, 2003) 88.31 their own large gazetteers, very elaborated features HySOL 88.14 unlabeled data (17M words) supplied gazetters HySOL 87.20 unlabeled data (17M words) Table 5: Previous top systems in NER (CoNLL 2003) experiments F? =1 additional resources ASO-semi 94.39 unlabeled data (Ando and Zhang, 2005) (15M words: WSJ) HySOL 94.30 unlabeled data (17M words: Reuters) (Zhang et al, 2002) 94.17 full parser output (Kudo and Matsumoto, 2001) 93.91? Table 6: Previous top systems in Chunking (CoNLL-2000) experiments 4 indicate that HySOL is rather robust with respect to the hyper-parameter since we can obtain fairly good performance without a prior distribution", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "W06-2919", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Partha Pratim, Talukdar | Thorsten, Brants | Mark Y., Liberman | Fernando, Pereira", 
    "raw_text": "System F1 (Precision, Recall) Florian et al (2003), best single, no list 89.94 (91.37, 88.56) Zhang and Johnson (2003), no list 90.26 (91.00, 89.53) CRF baseline, no list 89.52 (90.39, 88.66) Table 6: Baseline comparison on 4 categories (LOC, ORG, PER, MISC) on Test-a dataset", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "P06-2060", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Nanda, Kambhatla", 
    "raw_text": "Numerous methods for combining classifiers have been proposed and utlized to improve the performance of different NLP tasks such as part of speech tagging (Brill and Wu, 1998), identifying base noun phrases (Tjong Kim Sang et al, 2000), named entity extraction (Florian et al, 2003), etc. Ho et al (1994) investigated different approaches for rerank ing the outputs of a committee of classifiers and also explored union and intersection methods for reducing the set of predicted categories", 
    "clean_text": "Numerous methods for combining classifiers have been proposed and utlized to improve the performance of different NLP tasks such as part of speech tagging (Brill and Wu, 1998), identifying base noun phrases (Tjong Kim Sang et al, 2000), named entity extraction (Florian et al, 2003), etc.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "P08-1076", 
    "citing_paper_authority": 28, 
    "citing_paper_authors": "Jun, Suzuki | Hideki, Isozaki", 
    "raw_text": "test additional resources JESS-CM (CRF/HMM) 94.48 89.92 1G-word unlabeled data 93.66 89.36 37M-word unlabeled data (Ando and Zhang, 2005) 93.15 89.31 27M-word unlabeled data (Florian et al, 2003) 93.87 88.76 own large gazetteers, 2M-word labeled data (Suzuki et al, 2007) N/A 88.41 27M-word unlabeled data [sup", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "W05-0611", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Antal, van den Bosch | Walter, Daelemans", 
    "raw_text": "Boosting (Freund and Schapire, 1996) has been applied to optimize chunking systems (Carreras et al, 2002), as well as voting over sets of different classifiers (Florian et al, 2003)", 
    "clean_text": "Boosting (Freund and Schapire, 1996) has been applied to optimize chunking systems (Carreras et al, 2002), as well as voting over sets of different classifiers (Florian et al, 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "W06-2202", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Claudio, Giuliano | Alberto, Lavelli | Lorenza, Romano", 
    "raw_text": "Metric \u000f ?train/test R P F1 T 0 76.7 90.5 83.1 228 OR 1 70.4/83.9 78.2 88.1 82.8 74 2.5 83.6/95.6 76.4 62.6 68.8 33 5 90.5/97.2 75.3 66.5 70.7 14 Florian et al (2003) 88.5 89.0 88.8 baseline 50.9 71.9 59.6 Table 7: Filtering Rate, Micro-averaged Recall, Precision, F1 and total computation time for CoNLL-2003 (English)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "D10-1098", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Laura, Chiticariu | Rajasekar, Krishnamurthy | Yunyao, Li | Frederick, Reiss | Shivakumar, Vaithyanathan", 
    "raw_text": "1009 customized CoreNER for CoNLL03 and Enron3 with the corresponding best published results by (Florian et al, 2003) and (Minkov et al, 2005)", 
    "clean_text": "Next, we compare the extraction quality of the customized CoreNER for CoNLL03 and Enron3 with the corresponding best published results by (Florian et al, 2003) and (Minkov et al, 2005).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "D10-1098", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Laura, Chiticariu | Rajasekar, Krishnamurthy | Yunyao, Li | Frederick, Reiss | Shivakumar, Vaithyanathan", 
    "raw_text": "4 These results demonstrate that high-quality annotators can be built by customizing CoreNERorig using NERL, with the final extraction quality matching that of state-of-the art machine learning-based extractors. It is worthwhile noting that the best published results for CoNLL03 (Florian et al, 2003) were obtained by using four different classifiers (Robust Risk Minimization, Maximum Entropy, Transformation-based learning, and Hidden MarkovModel) and trying six different classifier combination methods", 
    "clean_text": "It is worthwhile noting that the best published results for CoNLL03 (Florian et al, 2003) were obtained by using four different classifiers (Robust Risk Minimization, Maximum Entropy, Transformation-based learning, and Hidden MarkovModel) and trying six different classifier combination methods.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "W05-0610", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Yaoyong, Li | Kalina, Bontcheva | Hamish, Cunningham", 
    "raw_text": "Table 1 presents the results of our system using three learning algorithms, the uneven margins SVM, the standard SVM and the PAUM on the CONLL 2003 test set, together with the results of three participating systems in the CoNLL-2003 shared task: the best system (Florian et al, 2003), the SVM-based system (Mayfield et al, 2003) and the Perceptron-based system (Carreras et al, 2003) .Firstly, our uneven margins SVM system performed significantly better than the other SVM based system", 
    "clean_text": "Table 1 presents the results of our system using three learning algorithms, the uneven margins SVM, the standard SVM and the PAUM on the CONLL 2003 test set, together with the results of three participating systems in the CoNLL-2003 shared task: the best system (Florian et al, 2003), the SVM-based system (Mayfield et al, 2003) and the Perceptron-based system (Carreras et al, 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "I08-1071", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Wisam, Dakka | Silviu, Cucerzan", 
    "raw_text": "Since the introduction of this task in MUC-6 (Grishman and Sundheim, 1996), numerous systems using various ways of exploiting entity-specific and local context features were proposed, from relatively simple character based models such as Cucerzan and Yarowsky (2002) and Klein et al (2003) to complex models making use of various lexical, syntactic ,morpho logical, and orthographical information, such as Wacholder et al (1997), Fleischman and Hovy (2002), and Florian et al (2003)", 
    "clean_text": "Since the introduction of this task in MUC-6 (Grishman and Sundheim, 1996), numerous systems using various ways of exploiting entity-specific and local context features were proposed, from relatively simple character based models such as Cucerzan and Yarowsky (2002) and Klein et al (2003) to complex models making use of various lexical, syntactic ,morpho logical, and orthographical information, such as Wacholder et al (1997), Fleischman and Hovy (2002), and Florian et al (2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "W03-0419", 
    "citing_paper_authority": 127, 
    "citing_paper_authors": "Erik F., Tjong Kim Sang | Fien, De Meulder", 
    "raw_text": "Florian et al (2003) employed the same technique in a combination of learners", 
    "clean_text": "Florian et al (2003) employed the same technique in a combination of learners.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "W03-0419", 
    "citing_paper_authority": 127, 
    "citing_paper_authors": "Erik F., Tjong Kim Sang | Fien, De Meulder", 
    "raw_text": "Transformation-based learning (Florian et al., 2003), Support Vector Machines (Mayfield et al, 2003) and Conditional Random Fields (McCallum and Li, 2003) were applied by one system each", 
    "clean_text": "Transformation-based learning (Florian et al., 2003), Support Vector Machines (Mayfield et al, 2003) and Conditional Random Fields (McCallum and Li, 2003) were applied by one system each.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "W03-0419", 
    "citing_paper_authority": 127, 
    "citing_paper_authors": "Erik F., Tjong Kim Sang | Fien, De Meulder", 
    "raw_text": "Florian et al (2003) tested different methods for combining the results of four systems and found that robust risk minimization worked best", 
    "clean_text": "Florian et al (2003) tested different methods for combining the results of four systems and found that robust risk minimization worked best.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "W03-0419", 
    "citing_paper_authority": 127, 
    "citing_paper_authors": "Erik F., Tjong Kim Sang | Fien, De Meulder", 
    "raw_text": "One participating team has used externally trained named entity recognition systems for English as a part in a combined system (Florian et al, 2003)", 
    "clean_text": "One participating team has used externally trained named entity recognition systems for English as a part in a combined system (Florian et al, 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "W03-0419", 
    "citing_paper_authority": 127, 
    "citing_paper_authors": "Erik F., Tjong Kim Sang | Fien, De Meulder", 
    "raw_text": "The inclusion of extra named entity recognition systems seems to have worked well (Florian et al, 2003)", 
    "clean_text": "The inclusion of extra named entity recognition systems seems to have worked well (Florian et al, 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "W03-0419", 
    "citing_paper_authority": 127, 
    "citing_paper_authors": "Erik F., Tjong Kim Sang | Fien, De Meulder", 
    "raw_text": "For English, the combined classifier of Florian et al (2003) achieved the highest overall F? =1 rate", 
    "clean_text": "For English, the combined classifier of Florian et al (2003) achieved the highest overall F1 rate.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "W03-0419", 
    "citing_paper_authority": 127, 
    "citing_paper_authors": "Erik F., Tjong Kim Sang | Fien, De Meulder", 
    "raw_text": "Florian et al (2003) have also obtained the highest F? =1 rate for the German data", 
    "clean_text": "Florian et al (2003) have also obtained the highest F1 rate for the German data.", 
    "keep_for_gold": 0
  }
]