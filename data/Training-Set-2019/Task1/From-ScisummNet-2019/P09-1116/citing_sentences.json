[
  {
    "citance_No": 1, 
    "citing_paper_id": "P14-2131", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Mohit, Bansal | Kevin, Gimpel | Karen, Livescu", 
    "raw_text": "We tried two kinds of indicator features: Bucket features: For both parent and child vectors in a potential dependency, we fire one indicator feature per dimension of each embedding 4 A similar effect, when changing distributional context window sizes, was found by Lin and Wu (2009)", 
    "clean_text": "We tried two kinds of indicator features: Bucket features: For both parent and child vectors in a potential dependency, we fire one indicator feature per dimension of each embedding. A similar effect, when changing distributional context window sizes, was found by Lin and Wu (2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "C10-2137", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Ang, Sun | Ralph, Grishman", 
    "raw_text": "Lin and Wu (2009) further explored a two-stage cluster-based approach: first clustering phrases and then relying on a supervised learner to identify useful clusters and assign proper weights to cluster features", 
    "clean_text": "Lin and Wu (2009) further explored a two-stage cluster-based approach: first clustering phrases and then relying on a supervised learner to identify useful clusters and assign proper weights to cluster features.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P11-1001", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Andreas, Zollmann | Stephan, Vogel", 
    "raw_text": "Our phrase pair clustering approach is similar inspirit to the work of Lin and Wu (2009), who use K means to cluster (monolingual) phrases and use the resulting clusters as features in discriminative classifiers for a named-entity-recognition and a query classification task", 
    "clean_text": "Our phrase pair clustering approach is similar inspirit to the work of Lin and Wu (2009), who use K means to cluster (monolingual) phrases and use the resulting clusters as features in discriminative classifiers for a named-entity-recognition and a query classification task.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "P11-1001", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Andreas, Zollmann | Stephan, Vogel", 
    "raw_text": "Another distinction is that Lin and Wu (2009) work with phrase types instead of phrase instances, obtaining a phrase type? s contexts by averaging the contexts of all its phrase instances", 
    "clean_text": "Another distinction is that Lin and Wu (2009) work with phrase types instead of phrase instances, obtaining a phrase type's contexts by averaging the contexts of all its phrase instances.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P11-2112", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Jun, Suzuki | Hideki, Isozaki | Masaaki, Nagata", 
    "raw_text": "= 1e+ 04) 94.91 91.02 3,720M 5.94M (Ando and Zhang, 2005) 93.15 89.31 27M N/A (Suzuki and Isozaki, 2008) 94.48 89.92 1,000M N/A (Ratinov and Roth, 2009) 93.50 90.57 N/A N/A (Turian et al, 2010) 93.95 90.36 37M N/A (Lin and Wu, 2009) N/A 90.90 700,000M N/A Dependency parser dev", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "C10-1058", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Alpa, Jain | Marco, Pennacchiotti", 
    "raw_text": "Our clustering approach is related to Lin and Wu? s work (Lin and Wu, 2009)", 
    "clean_text": "Our clustering approach is related to Lin and Wu's work (Lin and Wu, 2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "P12-1041", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Mohit, Bansal | Dan, Klein", 
    "raw_text": "We check the match of not just the top 1 cluster ids, but also farther down in the 20 sized lists because, as discussed in Lin and Wu (2009), the soft cluster assignments often reveal different senses of a word", 
    "clean_text": "We check the match of not just the top 1 cluster ids, but also farther down in the 20 sized lists because, as discussed in Lin and Wu (2009), the soft cluster assignments often reveal different senses of a word.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "P11-1097", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Stefan, R&uuml;d | Massimiliano, Ciaramita | Jens, M&uuml;ller | Hinrich, Sch&uuml;tze", 
    "raw_text": "Lin and Wu (2009) report an F1 score of 90.90 on the original split of the CoNLL data", 
    "clean_text": "Lin and Wu (2009) report an F1 score of 90.90 on the original split of the CoNLL data.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "P10-1040", 
    "citing_paper_authority": 133, 
    "citing_paper_authors": "Joseph P., Turian | Lev, Ratinov | Yoshua, Bengio", 
    "raw_text": "representations Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce", 
    "clean_text": "Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "P10-1040", 
    "citing_paper_authority": 133, 
    "citing_paper_authors": "Joseph P., Turian | Lev, Ratinov | Yoshua, Bengio", 
    "raw_text": "Lin and Wu (2009) finds that the representations that are good for NER are poor for search query classification, and vice-versa", 
    "clean_text": "Lin and Wu (2009) finds that the representations that are good for NER are poor for search query classification, and vice-versa.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "P10-1040", 
    "citing_paper_authority": 133, 
    "citing_paper_authors": "Joseph P., Turian | Lev, Ratinov | Yoshua, Bengio", 
    "raw_text": "System Dev Test MUC7 Baseline 90.03 84.39 67.48 Baseline+Nonlocal 91.91 86.52 71.80 HLBL 100-dim 92.00 88.13 75.25 Gazetteers 92.09 87.36 77.76 C& amp; W 50-dim 92.27 87.93 75.74 Brown, 1000 clusters 92.32 88.52 78.84 C& amp; W 200-dim 92.46 87.96 75.51 C& amp; W+HLBL 92.52 88.56 78.64 Brown+HLBL 92.56 88.93 77.85 Brown+C& amp; W 92.79 89.31 80.13 HLBL+Gaz 92.91 89.35 79.29 C& amp; W+Gaz 92.98 88.88 81.44 Brown+Gaz 93.25 89.41 82.71Lin and Wu (2009), 3.4B 88.44 Ando and Zhang (2005), 27M 93.15 89.31 Suzuki and Isozaki (2008), 37M 93.66 89.36 Suzuki and Isozaki (2008), 1B 94.48 89.92 All (Brown+C& amp; W+HLBL+Gaz), 37M 93.17 90.04 82.50 All+Nonlocal, 37M 93.95 90.36 84.15Lin and Wu (2009), 700B 90.90 Table 3: Final NER F1 results, showing the cumulative effect of adding word representations, non-local features, and gazetteers to the baseline", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "P10-1040", 
    "citing_paper_authority": 133, 
    "citing_paper_authors": "Joseph P., Turian | Lev, Ratinov | Yoshua, Bengio", 
    "raw_text": "We compare to the state-of-the-art methods of Ando and Zhang (2005), Suzuki and Isozaki (2008), and? for NER? Lin and Wu (2009)", 
    "clean_text": "We compare to the state-of-the-art methods of Ando and Zhang (2005), Suzuki and Isozaki (2008), and for NER Lin and Wu (2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "P10-1040", 
    "citing_paper_authority": 133, 
    "citing_paper_authors": "Joseph P., Turian | Lev, Ratinov | Yoshua, Bengio", 
    "raw_text": "Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce", 
    "clean_text": "Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "N12-1095", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Mohit, Bansal | John, DeNero | Dekang, Lin", 
    "raw_text": "K-Means clustering algorithm described in Lin and Wu (2009)", 
    "clean_text": "K-Means clustering algorithm described in Lin and Wu (2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "N12-1095", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Mohit, Bansal | John, DeNero | Dekang, Lin", 
    "raw_text": "Following Lin and Wu (2009), each word to be clustered is represented as a feature vector describing the distributional context of that word", 
    "clean_text": "Following Lin and Wu (2009), each word to be clustered is represented as a feature vector describing the distributional context of that word.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "N12-1095", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Mohit, Bansal | John, DeNero | Dekang, Lin", 
    "raw_text": "We follow Lin and Wu (2009) in applying various thresholds during K-Means, such as a frequency threshold for the initial vocabulary, a total count threshold for the feature vectors, and a thresh old for PMI scores", 
    "clean_text": "We follow Lin and Wu (2009) in applying various thresholds during K-Means, such as a frequency threshold for the initial vocabulary, a total count threshold for the feature vectors, and a threshold for PMI scores.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "N12-1095", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Mohit, Bansal | John, DeNero | Dekang, Lin", 
    "raw_text": "In addition to the features described in Lin and Wu (2009), we introduce features from a bilingual parallel corpus that encode reverse-translation information from the source-language (Spanish or Japanesein our experiments)", 
    "clean_text": "In addition to the features described in Lin and Wu (2009), we introduce features from a bilingual parallel corpus that encode reverse-translation information from the source-language (Spanish or Japanese in our experiments).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "N12-1095", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Mohit, Bansal | John, DeNero | Dekang, Lin", 
    "raw_text": "Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs", 
    "clean_text": "Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "N12-1052", 
    "citing_paper_authority": 32, 
    "citing_paper_authors": "Oscar, T&auml;ckstr&ouml;m | Ryan, McDonald | Jakob, Uszkoreit", 
    "raw_text": "As an alternative to clustering words, Lin and Wu (2009) proposed a phrase clustering approach that obtained the state-of-the-art result for English NER", 
    "clean_text": "As an alternative to clustering words, Lin and Wu (2009) proposed a phrase clustering approach that obtained the state-of-the-art result for English NER.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "W12-1914", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Grzegorz, Chrupa{l}a", 
    "raw_text": "Second, learning categories has been cast as unsupervised part-of-speech tagging task (recent work includes Ravi and Knight (2009), Lee et al (2010), Lamar et al (2010), Christodoulopoulos et al (2011)), and primarily motivated as useful for tagging under-resourced languages. Finally, learning categories has also been re searched from the point of view of feature learning, where the induced categories provide an inter me diate level of representation, abstracting away and generalizing over word form features in an NLPap plication (Brown et al 1992, Miller et al 2004, Lin and Wu 2009, Turian et al 2010, Chrupala 2011, Ta ?ckstro? m et al 2012)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }
]