Citance Number: 1 | Reference Article:  E06-1038.txt | Citing Article:  P06-2109.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>McDonald (McDonald, 2006) independently proposed a new machine learning approach.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this section we described a discriminative online learning approach to sentence compression, the core of which is a decoding algorithm that searches the entire space of compressions.</S><S sid = NA ssid = NA>In this paper we have described a new system for sentence compression.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  E06-1038.txt | Citing Article:  P14-1117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>However, while the former problem can be solved efficiently using the dynamic programming approach of McDonald (2006), there are no efficient algorithms to recover maximum weighted non projective subtrees in a general directed graph.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The algorithm then either shifts (considers new words and subtrees for x), reduces (combines subtrees from x into possibly new tree constructions) or drops (drops words and subtrees from x) on each step of the algorithm.</S><S sid = NA ssid = NA>We define a dynamic programming table C[i] which represents the highest score for any compression that ends at word xi for sentence x.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  E06-1038.txt | Citing Article:  P14-1117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>McDonald (2006) provides a Viterbi-like dynamic programming algorithm to recover the highest scoring sequence of order-preserving bigrams from a lattice, either in unconstrained form or with a specific length constraint.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>That is, the algorithm will return the highest scoring compression regardless of length.</S><S sid = NA ssid = NA>We define a dynamic programming table C[i] which represents the highest score for any compression that ends at word xi for sentence x.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  E06-1038.txt | Citing Article:  P14-1117.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For consistent comparisons with the other systems, our re-implementation does not include the k-best inference strategy presented in McDonald (2006) for learning with MIRA.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>When looking at importance, we see that our system actually does the best – even better than humans.</S><S sid = NA ssid = NA>The judges were told all three compressions were automatically generated and the order in which they were presented was randomly chosen for each sentence.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  E06-1038.txt | Citing Article:  P14-2054.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The third one is the bigram model proposed by McDonald (McDonald, 2006) which adopts dynamic programming for efficient inference.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We define a dynamic programming table C[i] which represents the highest score for any compression that ends at word xi for sentence x.</S><S sid = NA ssid = NA>We simply augment the dynamic programming table and calculate C[i][r], which is the score of the best compression of length r that ends at word xi.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  E06-1038.txt | Citing Article:  P08-1035.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This work was supported by NSF ITR grants 0205448 and 0428193.</S><S sid = NA ssid = NA>We also add a feature indicating if yj−1 and yj were actually adjacent in the original sentence or not and we conjoin this feature with the above POS features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  E06-1038.txt | Citing Article:  C10-1037.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Some utilize a parser to identify and later keep certain important relations but do not require a complete parse (Clarke & Lapata, 2008), or use a syntactic representation to extract features (McDonald, 2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>To do this we parse every sentence twice, once with a dependency parser (McDonald et al., 2005b) and once with a phrase-structure parser (Charniak, 2000).</S><S sid = NA ssid = NA>However, we use these syntactic constraints as soft evidence in our model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  E06-1038.txt | Citing Article:  P09-1093.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This work was supported by NSF ITR grants 0205448 and 0428193.</S><S sid = NA ssid = NA>We also add a feature indicating if yj−1 and yj were actually adjacent in the original sentence or not and we conjoin this feature with the above POS features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  E06-1038.txt | Citing Article:  N07-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>One successful recent approach (McDonald, 2006) combines a discriminative framework with a set of features that capture information similar to the K&M model.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Note that these features are meant to capture the same information in both the source and channel models of Knight and Marcu (2000).</S><S sid = NA ssid = NA>This is similar in purpose to the source model from the noisy-channel system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  E06-1038.txt | Citing Article:  W11-1606.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This phenomenon has been noted and discussed in the task of pairwise sentence fusion (Daume III and Marcu, 2004) and also in sentence compression (McDonald, 2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>P(x|y) is the channel model, the probability that the long sentence is an expansion of the compressed sentence.</S><S sid = NA ssid = NA>Discriminative Sentence Compression With Soft Syntactic Evidence</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  E06-1038.txt | Citing Article:  W11-1606.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This work was supported by NSF ITR grants 0205448 and 0428193.</S><S sid = NA ssid = NA>We also add a feature indicating if yj−1 and yj were actually adjacent in the original sentence or not and we conjoin this feature with the above POS features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  E06-1038.txt | Citing Article:  W09-1802.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Better statistical methods have been developed for producing high quality compression candidates (McDonald, 2006), that maintain linguistic quality, some recent work even uses ILPs for exact inference (Clarke and Lapata, 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our learning algorithm may unnecessarily lower the score of some perfectly valid compressions just because they were not the exact compression chosen by the human annotator.</S><S sid = NA ssid = NA>When looking at importance, we see that our system actually does the best – even better than humans.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  E06-1038.txt | Citing Article:  W06-2932.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>This has been shown through their successful use in many standard natural language processing tasks, including machine translation (Ding and Palmer, 2005), sentence compression (McDonald, 2006), and textual inference (Haghighi et al, 2005).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>It is not unique to use soft syntactic features in this way, as it has been done for many problems in language processing.</S><S sid = NA ssid = NA>We focus on the particular instantiation of sentence compression when the goal is to produce the compressed version solely by removing words or phrases from the original, which is the most common setting in the literature (Knight and Marcu, 2000; Riezler et al., 2003; Turner and Charniak, 2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  E06-1038.txt | Citing Article:  W09-1801.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>McDonald (2006) uses the outputs of two parsers (a phrase-based and a dependency parser) as features in a discriminative model that decomposes over pairs of consecutive words.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>To do this we parse every sentence twice, once with a dependency parser (McDonald et al., 2005b) and once with a phrase-structure parser (Charniak, 2000).</S><S sid = NA ssid = NA>We present a model for sentence compression that uses a discriminative largemargin learning framework coupled with a novel feature set defined on compressed bigrams as well as deep syntactic representations provided by auxiliary dependency and phrase-structure parsers.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  E06-1038.txt | Citing Article:  W09-1801.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This work was supported by NSF ITR grants 0205448 and 0428193.</S><S sid = NA ssid = NA>We also add a feature indicating if yj−1 and yj were actually adjacent in the original sentence or not and we conjoin this feature with the above POS features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  E06-1038.txt | Citing Article:  C08-1018.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Note that their model is a strong baseline: it performed significantly better than competitive approaches (McDonald, 2006) across a variety of compression corpora.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This system has many advantages over previous approaches.</S><S sid = NA ssid = NA>We compared our system to the decision tree model of Knight and Marcu instead of the noisy-channel model since both performed nearly as well in their evaluation, and the compression rate of the decision tree model is nearer to our system (around 57-58%).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  E06-1038.txt | Citing Article:  D11-1108.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Both these systems reported results outperforming previous systems such as McDonald (2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Results are shown in Table 1.</S><S sid = NA ssid = NA>This is quantified in the standard deviation of the two systems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  E06-1038.txt | Citing Article:  P13-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For example, such solvers are unable to take advantage of efficient dynamic programming routines for sentence compression (McDonald, 2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We define a dynamic programming table C[i] which represents the highest score for any compression that ends at word xi for sentence x.</S><S sid = NA ssid = NA>Another advantage is distance.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  E06-1038.txt | Citing Article:  P13-1020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The same framework can be readily adapted to other compression models that are efficiently decodable, such as the semi-Markov model of McDonald (2006), which would allow incorporating a language model for the compression.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The noisy-channel model defines the problem as finding the compressed sentence with maximum conditional probability P(y) is the source model, which is a PCFG plus bigram language model.</S><S sid = NA ssid = NA>Let the score of a compression y for a sentence x as In particular, we are going to factor this score using a first-order Markov assumption on the words in the compressed sentence Finally, we define the score function to be the dot product between a high dimensional feature representation and a corresponding weight vector Note that this factorization will allow us to define features over two adjacent words in the compression as well as the words in-between that were dropped from the original sentence to create the compression.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  E06-1038.txt | Citing Article:  D07-1008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>McDonald (2006) also presents a sentence compression model that uses a discriminative large margin algorithm.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This system uses discriminative large-margin learning techniques coupled with a decoding algorithm that searches the space of all compressions.</S><S sid = NA ssid = NA>In Section 3 we present our discriminative large-margin model for sentence compression, including the learning framework and an efficient decoding algorithm for searching the space of compressions.</S> | Discourse Facet:  NA | Annotator: Automatic


