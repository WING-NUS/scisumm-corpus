[
  {
    "citance_No": 1, 
    "citing_paper_id": "P06-2109", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Yuya, Unno | Takashi, Ninomiya | Yusuke, Miyao | Jun'ichi, Tsujii", 
    "raw_text": "McDonald (McDonald, 2006) independently proposed a new machine learning approach", 
    "clean_text": "McDonald (McDonald, 2006) independently proposed a new machine learning approach.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P14-1117", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Kapil, Thadani", 
    "raw_text": "How ever, while the former problem can be solved efficiently using the dynamic programming approach of McDonald (2006), there are no efficient algorithms to recover maximum weighted non projective subtrees in a general directed graph", 
    "clean_text": "However, while the former problem can be solved efficiently using the dynamic programming approach of McDonald (2006), there are no efficient algorithms to recover maximum weighted non projective subtrees in a general directed graph.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P14-1117", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Kapil, Thadani", 
    "raw_text": "McDonald (2006) provides a Viterbi-like dynamic programming algorithm to recover the highest scoring sequence of order-preserving bi grams from a lattice, either in unconstrained form or with a specific length constraint", 
    "clean_text": "McDonald (2006) provides a Viterbi-like dynamic programming algorithm to recover the highest scoring sequence of order-preserving bigrams from a lattice, either in unconstrained form or with a specific length constraint.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "P14-1117", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Kapil, Thadani", 
    "raw_text": "+ i), 10 while the hyper parameter? was tuned using the 9 For consistent comparisons with the other systems, our reimplementation does not include the k-best inference strategy presented in McDonald (2006) for learning with MIRA", 
    "clean_text": "For consistent comparisons with the other systems, our re-implementation does not include the k-best inference strategy presented in McDonald (2006) for learning with MIRA.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P14-2054", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Xian, Qian | Yang, Liu", 
    "raw_text": "model proposed by McDonald (McDonald, 2006) which adopts dynamic programming for efficient inference", 
    "clean_text": "The third one is the bigram model proposed by McDonald (McDonald, 2006) which adopts dynamic programming for efficient inference.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "P08-1035", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Tadashi, Nomoto", 
    "raw_text": "We tackle the issues by harnessing CRFs with what we might call dependency truncation, whose goal is to restrict CRFs to working with candidates that conform to the grammar. Thus, unlike McDonald (2006), Clarke and Lapata (2006) and Cohn and Lapata (2007), we do not insist on finding a globally optimal solution in the space of 2n possible compressions for an n word long sentence", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "C10-1037", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Katja, Filippova", 
    "raw_text": "Some utilize a parser to identify and later keep certain important relations but do not require a complete parse (Clarke& amp; Lapata, 2008), or use a syntactic representation to extract features (McDonald, 2006)", 
    "clean_text": "Some utilize a parser to identify and later keep certain important relations but do not require a complete parse (Clarke & Lapata, 2008), or use a syntactic representation to extract features (McDonald, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "P09-1093", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Tsutomu, Hirao | Jun, Suzuki | Hideki, Isozaki", 
    "raw_text": "Nomoto (2007) and McDonald (2006) employed the random field based approach", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "N07-1023", 
    "citing_paper_authority": 33, 
    "citing_paper_authors": "Michel, Galley | Kathleen R., McKeown", 
    "raw_text": "One successful recent approach (McDonald, 2006) combines a discriminative framework with a set of features that capture information similar to the K& amp; M model", 
    "clean_text": "One successful recent approach (McDonald, 2006) combines a discriminative framework with a set of features that capture information similar to the K&M model.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "W11-1606", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Kapil, Thadani | Kathleen R., McKeown", 
    "raw_text": "III and Marcu, 2004) and also in sentence compression (McDonald, 2006)", 
    "clean_text": "This phenomenon has been noted and discussed in the task of pairwise sentence fusion (Daume III and Marcu, 2004) and also in sentence compression (McDonald, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "W11-1606", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Kapil, Thadani | Kathleen R., McKeown", 
    "raw_text": "(b) Compression example from McDonald (2006) TapeWare, which supports DOS and NetWare 286, is a value-added process that lets you directly connect the QA150-EXAT to a file server and issue a command from any workstation to back up the server Human compression# 1 TapeWare supports DOS and NetWare 286 Human compression# 2 TapeWare lets you connect the QA150-EXAT to a file server (hypothesized) Table 1: Examples of text-to-text generation problems with multiple valid human-generated outputs that differ significantly in semantic content", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "W09-1802", 
    "citing_paper_authority": 21, 
    "citing_paper_authors": "Dan, Gillick | Benoit, Favre", 
    "raw_text": "Betterstatistical methods have been developed for producing high quality compression candidates (McDonald, 2006), that maintain linguistic quality, some re cent work even uses ILPs for exact inference (Clarkeand Lapata, 2008)", 
    "clean_text": "Better statistical methods have been developed for producing high quality compression candidates (McDonald, 2006), that maintain linguistic quality, some recent work even uses ILPs for exact inference (Clarke and Lapata, 2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "W06-2932", 
    "citing_paper_authority": 66, 
    "citing_paper_authors": "Ryan, McDonald | Kevin, Lerman | Fernando, Pereira", 
    "raw_text": "This has been shown through their successful use in many standard natural language processing tasks, including machine translation (Ding and Palmer, 2005), sentence compression (McDonald, 2006), and textual inference (Haghighi et al, 2005) .In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler", 
    "clean_text": "This has been shown through their successful use in many standard natural language processing tasks, including machine translation (Ding and Palmer, 2005), sentence compression (McDonald, 2006), and textual inference (Haghighi et al, 2005).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "W09-1801", 
    "citing_paper_authority": 24, 
    "citing_paper_authors": "Andre, Martins | Noah A., Smith", 
    "raw_text": "McDonald (2006) uses the outputs of two parsers (a phrase-based and a dependency parser) as features in a discriminative model that decomposes over pairs of consecutive words", 
    "clean_text": "McDonald (2006) uses the outputs of two parsers (a phrase-based and a dependency parser) as features in a discriminative model that decomposes over pairs of consecutive words.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "W09-1801", 
    "citing_paper_authority": 24, 
    "citing_paper_authors": "Andre, Martins | Noah A., Smith", 
    "raw_text": "Ratio P R F1 P R F1 HedgeTrimmer 57.64% 0.7099 0.5925 0.6459 0.7195 0.6547 0.6367 McDonald (2006) 71.40% 0.7444 0.7697 0.7568 0.7711 0.7852 0.7696 NoBigram 71.20% 0.7399 0.7626 0.7510 0.7645 0.7730 0.7604 Bigram 71.35% 0.7472 0.7720 0.7594 0.7737 0.7848 0.7710 Table 1: Results for sentence compression in the Clarke? s test dataset (441 sentences) for our implementation of the baseline systems (HedgeTrimmer and the system described in McDonald, 2006), and the two variants of our model, NoBigram and Bigram", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "C08-1018", 
    "citing_paper_authority": 28, 
    "citing_paper_authors": "Trevor, Cohn | Mirella, Lapata", 
    "raw_text": "Note that their model is a strong baseline: it performed significantly better than competitive approaches (McDonald, 2006) across a variety of compression corpora. Evaluation Methodology Sentencecompression output is commonly evaluated by eliciting human judgments", 
    "clean_text": "Note that their model is a strong baseline: it performed significantly better than competitive approaches (McDonald, 2006) across a variety of compression corpora.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "D11-1108", 
    "citing_paper_authority": 13, 
    "citing_paper_authors": "Juri, Ganitkevitch | Chris, Callison-Burch | Courtney, Napoles | Benjamin, van Durme", 
    "raw_text": "Boththese systems reported results outperforming previous systems such as McDonald (2006)", 
    "clean_text": "Both these systems reported results outperforming previous systems such as McDonald (2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "P13-1020", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Miguel, Almeida | Andre, Martins", 
    "raw_text": "For example, such solvers are unable to take advantage of efficient dynamic programming routines for sentence compression (McDonald, 2006)", 
    "clean_text": "For example, such solvers are unable to take advantage of efficient dynamic programming routines for sentence compression (McDonald, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "P13-1020", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Miguel, Almeida | Andre, Martins", 
    "raw_text": "Here, we employ the AD3 algorithm, in a 4The same framework can be readily adapted to other compression models that are efficiently decodable, such as the semi-Markov model of McDonald (2006), which would allow incorporating a language model for the compression.similar manner as described in? 2, but with an additional component for the sentence compressor, and slight modifications in the other components", 
    "clean_text": "The same framework can be readily adapted to other compression models that are efficiently decodable, such as the semi-Markov model of McDonald (2006), which would allow incorporating a language model for the compression.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "D07-1008", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Trevor, Cohn | Mirella, Lapata", 
    "raw_text": "McDonald (2006) also presents a sentence compression model that uses a discriminative large margin algorithm", 
    "clean_text": "McDonald (2006) also presents a sentence compression model that uses a discriminative large margin algorithm.", 
    "keep_for_gold": 0
  }
]