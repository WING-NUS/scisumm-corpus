[
  {
    "citance_No": 1, 
    "citing_paper_id": "P03-1011", 
    "citing_paper_authority": 46, 
    "citing_paper_authors": "Daniel, Gildea", 
    "raw_text": "Theapproach of optimizing a small number of meta pa rameters has been applied to machine translation byOch and Ney (2002)", 
    "clean_text": "The approach of optimizing a small number of meta parameters has been applied to machine translation by Och and Ney (2002).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P14-1012", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Shixiang, Lu | Zhenbiao, Chen | Bo, Xu", 
    "raw_text": "Features Learning for SMTEach translation rule in the phrase-based translation model has a set number of features that are combined in the log-linear model (Och and Ney, 2002), and our semi-supervised DAE features can also be combined in this model", 
    "clean_text": "Each translation rule in the phrase-based translation model has a set number of features that are combined in the log-linear model (Och and Ney, 2002), and our semi-supervised DAE features can also be combined in this model.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "H05-1097", 
    "citing_paper_authority": 20, 
    "citing_paper_authors": "David, Vickrey | Lukas, Biewald | Marc, Teyssier | Daphne, Koller", 
    "raw_text": "Incorporatingthe discriminative alignment model into the source channel model also improves performance, but not nearly as much as using the word-translation model. An alternate way to optimize weights over translation features is described in Och and Ney (2002) .They consider a number of translation features, including the language model and generative and discriminative alignment models", 
    "clean_text": "An alternate way to optimize weights over translation features is described in Och and Ney (2002).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "D11-1004", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Michel, Galley | Chris, Quirk", 
    "raw_text": "MERT directly optimizes the evaluation metric under which systems are being evaluated, yielding superior performance (Och, 2003) when compared to a likelihood-based discriminative method (Och and Ney, 2002)", 
    "clean_text": "MERT directly optimizes the evaluation metric under which systems are being evaluated, yielding superior performance (Och, 2003) when compared to a likelihood-based discriminative method (Och and Ney, 2002).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "D11-1004", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Michel, Galley | Chris, Quirk", 
    "raw_text": "Smoothing the objective function may allow differentiation and standard ML learning techniques (Och and Ney,2002)", 
    "clean_text": "Smoothing the objective function may allow differentiation and standard ML learning techniques (Och and Ney, 2002).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "P11-2074", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Bing, Xiang | Abraham, Ittycheriah", 
    "raw_text": "For example, the system described in (Koehnet al, 2003) is a widely known one using small number of features in a maximum-entropy (log-linear) model (Och and Ney, 2002)", 
    "clean_text": "For example, the system described in (Koehnet al, 2003) is a widely known one using small number of features in a maximum-entropy (log-linear) model (Och and Ney, 2002).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "H05-1098", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "David, Chiang | Adam, Lopez | Nitin, Madnani | Christof, Monz | Philip, Resnik | Michael, Subotin", 
    "raw_text": "The model is a log-linear model (Och and Ney, 2002) over synchronous CFG derivations", 
    "clean_text": "The model is a log-linear model (Och and Ney, 2002) over synchronous CFG derivations.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "P07-1001", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Yonggang, Deng | Yuqing, Gao", 
    "raw_text": "These feature weights are tuned on the dev set to achieve optimal translation performance using downhill simplex method (Och and Ney, 2002)", 
    "clean_text": "These feature weights are tuned on the dev set to achieve optimal translation performance using downhill simplex method (Och and Ney, 2002).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "D09-1040", 
    "citing_paper_authority": 21, 
    "citing_paper_authors": "Yuval, Marton | Chris, Callison-Burch | Philip, Resnik", 
    "raw_text": "For all baselines weused the phrase-based statistical machine translation system Moses (Koehn et al, 2007), with the default model features, weighted in a log-linear framework (Och and Ney, 2002)", 
    "clean_text": "For all baselines we used the phrase-based statistical machine translation system Moses (Koehn et al, 2007), with the default model features, weighted in a log-linear framework (Och and Ney, 2002).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "P06-1098", 
    "citing_paper_authority": 16, 
    "citing_paper_authors": "Taro, Watanabe | Hajime, Tsukada | Hideki, Isozaki", 
    "raw_text": "The former rep re sents the correspondence between two languages and the latter contributes to the fluency of English.In the state of the art statistical machine translation, the posterior probability Pr (eI1| f J1) is directly maximized using a log-linear combination of feature functions (Och and Ney, 2002): e? I1 =argmaxeI1 exp (? Mm=1 ?mhm (eI1, f J1))? e? I? 1 exp (? Mm=1 ?mhm (e? I? 1, f J1)) (3) where hm (eI1, f J1) is a feature function, such as a ngram language model or a translation model", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "P06-1098", 
    "citing_paper_authority": 16, 
    "citing_paper_authors": "Taro, Watanabe | Hajime, Tsukada | Hideki, Isozaki", 
    "raw_text": "When decoding, the denominator is dropped since it depends only on f J1. Feature function scaling factors? m are optimized based on a maximum likely approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003)", 
    "clean_text": "Feature function scaling factors \u03bbm are optimized based on a maximum likely approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "C10-1123", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Zhaopeng, Tu | Yang, Liu | Young-Sook, Hwang | Qun, Liu | Shouxun, Lin", 
    "raw_text": "We optimized feature weights using the minimum error rate training algorithm (OchandNey, 2002) on the NIST 2002 test set", 
    "clean_text": "We optimized feature weights using the minimum error rate training algorithm (Och and Ney, 2002) on the NIST 2002 test set.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "P06-1129", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Mu, Li | Muhua, Zhu | Yang, Zhang | Ming, Zhou", 
    "raw_text": "This is similar to what Och and Ney (2002) used for their maximum entropy-based statistical machine translation training", 
    "clean_text": "This is similar to what Och and Ney (2002) used for their maximum entropy-based statistical machine translation training.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "W05-0834", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Richard, Zens | Hermann, Ney", 
    "raw_text": "Discriminative Training.The training of the model scaling factors as described in (Och and Ney, 2002) was done on N -best lists", 
    "clean_text": "The training of the model scaling factors as described in (Och and Ney, 2002) was done on N-best lists.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "E06-1032", 
    "citing_paper_authority": 72, 
    "citing_paper_authors": "Chris, Callison-Burch | Miles, Osborne | Philipp, Koehn", 
    "raw_text": "feature functions (Och and Ney, 2002)", 
    "clean_text": "The remaining six entries were all fully automatic machine translation systems; in fact, they were all phrase-based statistical machine translation system that had been trained on the same parallel corpus and most used Bleu based minimum error rate training (Och, 2003) to optimize the weights of their log linear models' feature functions (Och and Ney, 2002).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "W07-0725", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Holger, Schwenk", 
    "raw_text": "=arg max p (e|f) =arg max e{ exp (? i ?ihi (e, f))} (1) The feature functions hi are the system models and the? i weights are typically optimized to maximize a scoring function on a development set (OchandNey, 2002)", 
    "clean_text": "The feature functions hi are the system models and the weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "W11-1007", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Bing, Xiang | Niyu, Ge | Abraham, Ittycheriah", 
    "raw_text": "Word order in the translation output relies on how the phrases are reordered based on both language model scores and distortion cost/penalty (Koehn et al, 2003), among all the features utilized in a maximum-entropy (log linear) model (Och and Ney, 2002)", 
    "clean_text": "Word order in the translation output relies on how the phrases are reordered based on both language model scores and distortion cost/penalty (Koehn et al, 2003), among all the features utilized in a maximum-entropy (log linear) model (Och and Ney, 2002).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "D07-1045", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Holger, Schwenk | Marta Ruiz, Costa-Juss&agrave; | Jos&eacute; A. R., Fonollosa", 
    "raw_text": "=arg max p (e|f) =arg max e{ exp (? i ?ihi (e, f))} (1) The feature functions hi are the system models and the? i weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "C10-1124", 
    "citing_paper_authority": 19, 
    "citing_paper_authors": "Jakob, Uszkoreit | Jay, Ponte | Ashok C., Popat | Moshe, Dubiner", 
    "raw_text": "To translate the input documents into English we use phrase-based statistical machine translation systems based on the log-linear formulation of the problem (Och and Ney, 2002) .We train the systems on the Europarl Cor pus (Koehn, 2002), the DGT Multilingual Translation Memory (European Commission Directorate-General for Translation, 2007) and the United Nations ODS corpus (United Nations, 2006)", 
    "clean_text": "To translate the input documents into English we use phrase-based statistical machine translation systems based on the log-linear formulation of the problem (Och and Ney, 2002).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "W06-2601", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Vanessa, Sandrini | Marcello, Federico | Mauro, Cettolo", 
    "raw_text": "In fact, only recently, log-probability features have been deployed in ME models for statistical machine translation (Och and Ney, 2002) .This paper focuses on ME models for two text tagging tasks: Named Entity Recognition (NER) and Text Chuncking (TC)", 
    "clean_text": "In fact, only recently, log-probability features have been deployed in ME models for statistical machine translation (Och and Ney, 2002).", 
    "keep_for_gold": 0
  }
]