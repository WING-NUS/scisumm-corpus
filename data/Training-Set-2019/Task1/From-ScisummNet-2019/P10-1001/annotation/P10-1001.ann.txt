Citance Number: 1 | Reference Article:  P10-1001.txt | Citing Article:  P11-2123.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Regarding dependency parsing of the English PTB, currently Koo and Collins (2010) and Zhang and Nivre (2011) hold the best results, with 93.0 and 92.9 unlabeled attachment score, respectively.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We evaluate our parsers on the Penn Treebank and Prague Dependency Treebank, achieving unlabeled attachment scores of 93.04% and 87.38%, respectively.</S><S sid = NA ssid = NA>Accuracy is measured with unlabeled attachment score (UAS): the percentage of words with the correct head.8</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P10-1001.txt | Citing Article:  P14-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For example, Koo and Collins (2010) and Zhang and McDonald (2012) show that incorporating higher-order features into a graph-based parser only leads to modest increase in parsing accuracy.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Following standard practice for higher-order dependency parsing (McDonald and Pereira, 2006; Carreras, 2007), Models 1 and 2 evaluate not only the relevant third-order parts, but also the lower-order parts that are implicit in their third-order factorizations.</S><S sid = NA ssid = NA>Interestingly, grandchild interactions appear to provide important information: for example, when Model 2 is used without grandchild-based features (“Model 2, no-G” in Table 3), its accuracy suffers noticeably.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P10-1001.txt | Citing Article:  P14-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Previous work on graph-based dependency parsing mostly adopts linear models and perceptron-based training procedures, which lack probabilistic explanations of dependency trees and do not need to compute likelihood of labeled training 1Higher-order models of Carreras (2007) and Koo and Collins (2010) can achieve higher accuracy, but has much higher time cost (O (n4)).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Following standard practice for higher-order dependency parsing (McDonald and Pereira, 2006; Carreras, 2007), Models 1 and 2 evaluate not only the relevant third-order parts, but also the lower-order parts that are implicit in their third-order factorizations.</S><S sid = NA ssid = NA>All three of the “†” models are based on versions of the Carreras (2007) parser, so modifying these methods to work with our new third-order parsing algorithms would be an interesting topic for future research.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P10-1001.txt | Citing Article:  P14-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>06/21/98).</S><S sid = NA ssid = NA>The algorithm resembles the first-order parser, except that every recursive construction must also set the grandparent indices of the smaller g-spans; fortunately, this can be done deterministically in all cases.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P10-1001.txt | Citing Article:  P14-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Kooand Collins (2010) propose a third-order graph based parser.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This section presents three parsing algorithms based on this idea: Model 0, a second-order parser, and Models 1 and 2, which are third-order parsers.</S><S sid = NA ssid = NA>In addition, it seems that grandchild interactions are particularly useful in Czech, while sibling interactions are less important: consider that Model 0, a second-order grandchild parser with no sibling-based features, can easily outperform “Model 2, no-G,” a third-order sibling parser with no grandchild-based features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P10-1001.txt | Citing Article:  D11-1022.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In the second column, [Ma08] denotes Martins et al (2008), [KC10] is Koo and Collins (2010), [Ma10] is Martins et al (2010), and [Ko10] is Koo et al (2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Suzuki et al. (2009) and phrase-structure annotations in the case of Carreras et al.</S><S sid = NA ssid = NA>Note that Koo et al. (2008) is listed with standard features and semi-supervised features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P10-1001.txt | Citing Article:  P12-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>S (g, h, m, s) (4) For projective parsing, dynamic programming for this factorization was derived in Koo and Collins (2010) (Model 1 in that paper), and for non projective parsing, dual decomposition was used for this factorization in Koo et al (2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Model 0 dynamic-programming algorithm.</S><S sid = NA ssid = NA>For the purposes of improving parsing performance, it is desirable to increase the size and variety of the parts used by the factorization.1 At the same time, the need for more expressive factorizations 1For examples of how performance varies with the degree of the parser’s factorization see, e.g., McDonald and Pereira (2006, Tables 1 and 2), Carreras (2007, Table 2), Koo et al. (2008, Tables 2 and 4), or Suzuki et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P10-1001.txt | Citing Article:  P12-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We use the Model 1 version of dpo3, a state-of-the art third-order dependency parser (Koo and Collins, 2010)).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This section presents three parsing algorithms based on this idea: Model 0, a second-order parser, and Models 1 and 2, which are third-order parsers.</S><S sid = NA ssid = NA>Efficient Third-Order Dependency Parsers</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P10-1001.txt | Citing Article:  P12-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For further details about the parser, see Koo and Collins (2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Due to space restrictions, we have been necessarily brief at some points in this paper; some additional details can be found in Koo (2010). on validation data.</S><S sid = NA ssid = NA>For concreteness, Figure 5 provides a pseudocode sketch of a bottom-up chart parser for Model 0; although the sketch omits many details, it suffices for the purposes of illustration.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P10-1001.txt | Citing Article:  P12-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The set of potential edges was pruned using the marginals produced by a first-order parser trained using exponentiated gradient descent (Collins et al, 2008) as in Koo and Collins (2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>(2008).</S><S sid = NA ssid = NA>In order to decrease training times, we follow Carreras et al. (2008) and eliminate unlikely dependencies using a form of coarse-to-fine pruning (Charniak and Johnson, 2005; Petrov and Klein, 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P10-1001.txt | Citing Article:  P12-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We also ran MST Parser (McDonald and Pereira, 2006), the Berkeley constituency parser (Petrov and Klein, 2007), and the unmodified dpo3 Model 1 (Koo and Collins, 2010) using Conversion 2 (the current recommendations) for comparison.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The term no-G indicates a parser that was trained and tested with the grandchild-based feature mappings fgch and fgsib deactivated; note that “Model 2, no-G” emulates the third-order sibling parser proposed by McDonald and Pereira (2006).</S><S sid = NA ssid = NA>In order to decrease training times, we follow Carreras et al. (2008) and eliminate unlikely dependencies using a form of coarse-to-fine pruning (Charniak and Johnson, 2005; Petrov and Klein, 2007).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P10-1001.txt | Citing Article:  P12-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>As some important relationships were represented as siblings and some as grandparents, there was a need to develop third-order parsers which could exploit both simultaneously (Koo and Collins, 2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In dependency grammar, syntactic relationships are represented as head-modifier dependencies: directed arcs between a head, which is the more “essential” word in the relationship, and a modifier, which supplements the meaning of the head.</S><S sid = NA ssid = NA>For example, Model 1 defines feature mappings for dependencies, siblings, grandchildren, and grand-siblings, so that the score of a dependency parse is given by: Above, y is simultaneously decomposed into several different types of parts; trivial modifications to the Model 1 parser allow it to evaluate all of the necessary parts in an interleaved fashion.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P10-1001.txt | Citing Article:  P13-2020.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>06/21/98).</S><S sid = NA ssid = NA>The algorithm resembles the first-order parser, except that every recursive construction must also set the grandparent indices of the smaller g-spans; fortunately, this can be done deterministically in all cases.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P10-1001.txt | Citing Article:  E12-1009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We extend Carreras (2007) graph based model with factors involving three edges similar to that of Koo and Collins (2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>It should be noted that unlike Model 1, Model 2 produces grand-sibling parts only for the outermost pair of grandchildren,4 similar to the behavior of the Carreras (2007) parser.</S><S sid = NA ssid = NA>There are several possibilities for further research involving our third-order parsing algorithms.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P10-1001.txt | Citing Article:  E12-1009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Figure 5 and 6 illustrate the third order factors, which are similar to the factors of Koo and Collins (2010).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The first parser, Model 0, factors each dependency tree into a set of grandchild parts—pairs of dependencies connected head-to-tail.</S><S sid = NA ssid = NA>Complexity would increase by factors of O(54) time and O(53) space, where 5 bounds the number of senses per word.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P10-1001.txt | Citing Article:  E12-1009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>While previous work using third order factors, cf. Koo and Collins (2010), was restricted to unlabeled and projective trees, our parser can produce labeled and non-projective dependency trees.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Unfortunately, designing efficient higherorder non-projective parsers is likely to be challenging, based on recent hardness results (McDonald and Pereira, 2006; McDonald and Satta, 2007).</S><S sid = NA ssid = NA>As a final note, the parsing algorithms described in this section fall into the category of projective dependency parsers, which forbid crossing dependencies.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P10-1001.txt | Citing Article:  P11-2112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>06/21/98).</S><S sid = NA ssid = NA>The algorithm resembles the first-order parser, except that every recursive construction must also set the grandparent indices of the smaller g-spans; fortunately, this can be done deterministically in all cases.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P10-1001.txt | Citing Article:  P12-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Koo and Collins (2010) further propose a third-order model that uses third-order features.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In addition, it seems that grandchild interactions are particularly useful in Czech, while sibling interactions are less important: consider that Model 0, a second-order grandchild parser with no sibling-based features, can easily outperform “Model 2, no-G,” a third-order sibling parser with no grandchild-based features.</S><S sid = NA ssid = NA>Note that Koo et al. (2008) is listed with standard features and semi-supervised features.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P10-1001.txt | Citing Article:  P12-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Table 7 shows the performance of the graph-based systems that were compared, where McDonald06 refers to the second-order parser of McDonald and Pereira (2006), Koo08-standard refers to the second-order parser with the features defined in Koo et al (2008), Koo10-model1 refers to the third-order parser with model1 of Koo and Collins (2010), Koo08-dep2c refers to the second-order parser with cluster-based features of (Koo et al, 2008), Suzuki09 refers to the parser of Suzuki et al.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Note that Koo et al. (2008) is listed with standard features and semi-supervised features.</S><S sid = NA ssid = NA>For the purposes of improving parsing performance, it is desirable to increase the size and variety of the parts used by the factorization.1 At the same time, the need for more expressive factorizations 1For examples of how performance varies with the degree of the parser’s factorization see, e.g., McDonald and Pereira (2006, Tables 1 and 2), Carreras (2007, Table 2), Koo et al. (2008, Tables 2 and 4), or Suzuki et al.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P10-1001.txt | Citing Article:  P12-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Koo10-model1 (Koo and Collins, 2010) used the third-order features and achieved the best reported result among the supervised parsers.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Note that Koo et al. (2008) is listed with standard features and semi-supervised features.</S><S sid = NA ssid = NA>Efficient Third-Order Dependency Parsers</S> | Discourse Facet:  NA | Annotator: Automatic


