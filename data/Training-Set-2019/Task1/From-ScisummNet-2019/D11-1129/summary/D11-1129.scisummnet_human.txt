Experimental Support for a Categorical Compositional Distributional Model of Meaning
Modeling compositional meaning for sentences using empirical distributional methods has been a challenge for computational linguists.
We implement the abstract categorical model of Coecke et al. (2010) using data from the BNC and evaluate it.
The implementation is based on unsupervised learning of matrices for relational words and applying them to the vectors of their arguments.
The evaluation is based on the word disambiguation task developed by Mitchell and Lapata (2008) for intransitive sentences, and on a similar new experiment designed for transitive sentences.
Our model matches the results of its competitors in the first experiment, and betters them in the second.
The general improvement in results with increase in syntactic complexity showcases the compositional power of our model.
We suggest that relational word vectors live in a space the dimensionality of which be a function of the arity of the relation.
