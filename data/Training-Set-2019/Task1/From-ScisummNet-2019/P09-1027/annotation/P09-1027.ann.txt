Citance Number: 1 | Reference Article:  P09-1027.txt | Citing Article:  P10-1061.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To overcome the shortcomings of available resources and to take advantage of ensemble systems, Wan (2008) and Wan (2009) explored methods for developing a hybrid system for Chinese using English and Chinese sentiment analyzers.</S> | Reference Offset:  ['17','49'] | Reference Text:  <S sid = 17 ssid = >In this study, we focus on the problem of cross-lingual sentiment classification, which leverages only English training data for supervised sentiment classification of Chinese product reviews, without using any Chinese resources.</S><S sid = 49 ssid = >Wan (2008) focuses on leveraging both Chinese and English lexicons to improve Chinese sentiment analysis by using lexicon-based methods.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P09-1027.txt | Citing Article:  P10-2048.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To reduce this kind of error introduced by the translator, Wan in (Wan, 2009) applied a co-training scheme.</S> | Reference Offset:  ['74','180'] | Reference Text:  <S sid = 74 ssid = >The co-training algorithm is then applied to learn two classifiers and finally the two classifiers are combined into a single sentiment classifier.</S><S sid = 180 ssid = >During the bootstrapping process of smoothed co-training, the classifier at each iteration is replaced with a majority voting scheme applied to all classifiers constructed at previous iterations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P09-1027.txt | Citing Article:  P10-2048.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >But as the conditional distribution can be quite different for the original language and the pseudo language produced by the machine translators, these two strategies give poor performance as reported in (Wan, 2009).</S> | Reference Offset:  ['20','65'] | Reference Text:  <S sid = 20 ssid = >SVM, NB), and the classification performance is far from satisfactory because of the language gap between the original language and the translated language.</S><S sid = 65 ssid = >As shown in our experiments, the above two methods do not perform well for Chinese sentiment classification, either, because the underlying distribution between the original language and the translated language are different.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P09-1027.txt | Citing Article:  P10-2048.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For comparsion, we use the same data set in (Wan, 2009): Test Set (Labeled Chinese Reviews): The data set contains a total of 886 labeled product reviews in Chinese (451 positive reviews and 435 negative ones).</S> | Reference Offset:  ['106','110'] | Reference Text:  <S sid = 106 ssid = >The following three datasets were collected and used in the experiments: Test Set (Labeled Chinese Reviews): In order to assess the performance of the proposed approach, we collected and labeled 886 product reviews (451 positive reviews + 435 negative reviews) from a popular Chinese IT product web site-IT1688.</S><S sid = 110 ssid = >Unlabeled Set (Unlabeled Chinese Reviews): We downloaded additional 1000 Chinese product reviews from IT168 and used the reviews as the unlabeled set.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P09-1027.txt | Citing Article:  P10-2048.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The features we used are bigrams and unigrams in the two languages as in (Wan, 2009).</S> | Reference Offset:  ['13','96'] | Reference Text:  <S sid = 13 ssid = >However, such resources in different languages are very imbalanced.</S><S sid = 96 ssid = >The English or Chinese features used in this study include both unigrams and bigrams5 and the feature weight is simply set to term frequency6.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P09-1027.txt | Citing Article:  P10-2048.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We compare our procedure with the co-training scheme reported in (Wan, 2009): CoTrain: The method with the best performance in (Wan, 2009).</S> | Reference Offset:  ['139','146'] | Reference Text:  <S sid = 139 ssid = >In the experiments, we first compare the proposed co-training approach (I=40 and p=n=5) with the eight baseline methods.</S><S sid = 146 ssid = >Actually, TSVM(ENCN2) is similar to CoTrain because CoTrain also combines the results of two classifiers in the same way.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P09-1027.txt | Citing Article:  W10-4116.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >More recently, Wan (2009) proposed a co training approach to tackle the problem of cross lingual sentiment classification by leveraging an available English corpus for Chinese sentiment classification.</S> | Reference Offset:  ['0','3'] | Reference Text:  <S sid = 0 ssid = >Co-Training for Cross-Lingual Sentiment Classification</S><S sid = 3 ssid = >This paper focuses on the problem of cross-lingual sentiment classification, which leverages an available English corpus for Chinese sentiment classification by using the English corpus as training data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P09-1027.txt | Citing Article:  D10-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Similarly, other multilingual sentiment approaches also require parallel text, often supplied via automatic translation; after the translated text is available, either monolingual analysis (Denecke, 2008) or co-training is applied (Wan, 2009).</S> | Reference Offset:  ['82','181'] | Reference Text:  <S sid = 82 ssid = >Google Translate applies statistical learning techniques to build a translation model based on both monolingual text in the target language and aligned text consisting of examples of human translations between the languages.</S><S sid = 181 ssid = >2) The feature distributions of the translated text and the natural text in the same language are still different due to the inaccuracy of the machine translation service.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P09-1027.txt | Citing Article:  D10-1103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >A related, yet more sophisticated technique is proposed in (Wan,2009), where a co-training approach is used to leverage resources from both a source and a target language.</S> | Reference Offset:  ['47','77'] | Reference Text:  <S sid = 47 ssid = >To date, several pilot studies have been performed to leverage rich English resources for sentiment analysis in other languages.</S><S sid = 77 ssid = >In order to overcome the language gap, we must translate one language into another language.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P09-1027.txt | Citing Article:  W11-1724.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Wan (2009) combined the annotated English reviews, unannotated Chinese reviews and their translations to co-train two separate classifiers for each language, respectively.</S> | Reference Offset:  ['23','60'] | Reference Text:  <S sid = 23 ssid = >First, machine translation services are used to translate English training reviews into Chinese reviews and also translate Chinese test reviews and additional unlabeled reviews into English reviews.</S><S sid = 60 ssid = >Given the labeled English reviews and unlabeled Chinese reviews, two straightforward methods for addressing the problem are as follows: 1) We first learn a classifier based on the labeled English reviews, and then translate Chinese reviews into English reviews.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P09-1027.txt | Citing Article:  W11-1724.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the document-level review polarity classification experiment, we used the dataset adopted in (Wan, 2009).</S> | Reference Offset:  ['33','100'] | Reference Text:  <S sid = 33 ssid = >In this paper we focus on document sentiment classification.</S><S sid = 100 ssid = >The output value of the SVM classifier for a review indicates the confidence level of the review’s classification.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P09-1027.txt | Citing Article:  W11-1724.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the review polarity classification experiment, we use unigram, bigram of Chinese words as features which is suggested by (Wan, 2009).</S> | Reference Offset:  ['86','166'] | Reference Text:  <S sid = 86 ssid = >In the context of cross-lingual sentiment classification, each labeled English review or unlabeled Chinese review has two views of features: English features and Chinese features.</S><S sid = 166 ssid = >In the above experiments, all features (unigram + bigram) are used.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P09-1027.txt | Citing Article:  C10-2127.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The original annotations 1104 can be produced either manually or automatically. Wan (2009) constructs a multilingual classifier using co-training.</S> | Reference Offset:  ['26','49'] | Reference Text:  <S sid = 26 ssid = >The SVM classifier is adopted as the basic classifier in the proposed approach.</S><S sid = 49 ssid = >Wan (2008) focuses on leveraging both Chinese and English lexicons to improve Chinese sentiment analysis by using lexicon-based methods.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P09-1027.txt | Citing Article:  P10-1114.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Methods which follow this two step approach include the EM-based approach by Rigutini et al (2005), the CCA approach by Fortuna and Shawe-Taylor (2005), the information bottleneck approach by Ling et al (2008), and the co-training approach by Wan (2009).</S> | Reference Offset:  ['53','57'] | Reference Text:  <S sid = 53 ssid = >To date, many semi-supervised learning algorithms have been developed for addressing the cross-domain text classification problem by transferring knowledge across domains, including Transductive SVM (Joachims, 1999), EM(Nigam et al., 2000), EM-based Naïve Bayes classifier (Dai et al., 2007a), Topic-bridged PLSA (Xue et al., 2008), Co-Clustering based classification (Dai et al., 2007b), two-stage approach (Jiang and Zhai, 2007).</S><S sid = 57 ssid = >A few novel models have been proposed to address the problem, e.g. the EM-based algorithm (Rigutini et al., 2005), the information bottleneck approach (Ling et al., 2008), the multilingual domain models (Gliozzo and Strapparava, 2005), etc.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P09-1027.txt | Citing Article:  P13-2094.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The proposed co-regression algorithm can make full use of both the features in the source language and the features in the target language in a unified framework similar to (Wan 2009).</S> | Reference Offset:  ['67','77'] | Reference Text:  <S sid = 67 ssid = >The co-training approach can make full use of both the English features and the Chinese features in a unified framework.</S><S sid = 77 ssid = >In order to overcome the language gap, we must translate one language into another language.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P09-1027.txt | Citing Article:  P10-3005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Wan (2009) constructs a multilingual classifier using co-training.</S> | Reference Offset:  ['26','49'] | Reference Text:  <S sid = 26 ssid = >The SVM classifier is adopted as the basic classifier in the proposed approach.</S><S sid = 49 ssid = >Wan (2008) focuses on leveraging both Chinese and English lexicons to improve Chinese sentiment analysis by using lexicon-based methods.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P09-1027.txt | Citing Article:  P12-1060.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Wan (2009) proposes to use ensemble method to train better Chinese sentiment classification model on English labeled data and their Chinese translation.</S> | Reference Offset:  ['129','132'] | Reference Text:  <S sid = 129 ssid = >Only English-to-Chinese translation is needed.</S><S sid = 132 ssid = >Only Chinese-to-English translation is needed.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P09-1027.txt | Citing Article:  P12-1060.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >SVM: We train a SVM classifier on the Chinese labeled data.MT-Cotrain: This is the co-training based approach described in (Wan, 2009).</S> | Reference Offset:  ['26','118'] | Reference Text:  <S sid = 26 ssid = >The SVM classifier is adopted as the basic classifier in the proposed approach.</S><S sid = 118 ssid = >In the experiments, the proposed co-training approach (CoTrain) is compared with the following baseline methods: SVM(CN): This method applies the inductive SVM with only Chinese features for sentiment classification in the Chinese view.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P09-1027.txt | Citing Article:  W10-3209.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Wan (2009) also leveraged an available English corpus for Chinese sentiment classification by using the co-training approach to make full use of both English and Chinese features in a unified framework.</S> | Reference Offset:  ['59','67'] | Reference Text:  <S sid = 59 ssid = >The purpose of our approach is to make use of the annotated English corpus for sentiment polarity identification of Chinese reviews in a supervised framework, without using any Chinese resources.</S><S sid = 67 ssid = >The co-training approach can make full use of both the English features and the Chinese features in a unified framework.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P09-1027.txt | Citing Article:  C10-2173.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Sentiment classification can be performed on words, sentences or documents, and is generally categorized into lexicon-based and corpus-based classification method (Wan, 2009).</S> | Reference Offset:  ['32','34'] | Reference Text:  <S sid = 32 ssid = >Sentiment classification can be performed on words, sentences or documents.</S><S sid = 34 ssid = >The methods for document sentiment classification can be generally categorized into lexicon-based and corpus-based.</S> | Discourse Facet:  NA | Annotator: Automatic


