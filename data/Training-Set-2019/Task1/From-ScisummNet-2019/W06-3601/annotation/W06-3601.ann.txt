Citance Number: 1 | Reference Article:  W06-3601.txt | Citing Article:  D10-1027.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In this framework, decoding without language model (LM decoding) is simply a linear-time depth-first search with memoization (Huang et al, 2006), since a tree of n words is also of size O (n) and we visit every node only once.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This can be done by a simple top-down traversal (or depth-first search) from the root of T*: at each node q in T*, try each possible rule r whose Englishside pattern t(r) matches the subtree T*η rooted at q, and recursively visit each descendant node qi in T*η that corresponds to a variable in t(r).</S><S sid = NA ssid = NA>The model is then extended to the general log-linear framework in order to rescore with other fealike language models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W06-3601.txt | Citing Article:  P08-1064.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Huang et al (2006) study a TSG-based tree-to-string alignment model.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Galley et al. (2004) presents a linear-time algorithm for automatic extraction of these xRs rules from a parallel corpora with word-alignment and parse-trees on the source-side, which will be used in our experiments in Section 6.</S><S sid = NA ssid = NA>Departing from the conventional noisy-channel approach of Brown et al. (1993), our basic model is a direct one: where e is the English input string and c* is the best Chinese translation according to the translation model Pr(c  |e).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W06-3601.txt | Citing Article:  W08-0308.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Liu et al (2006) and Huang et al (2006) then used the TTS transducer on the task of Chinese-to-English and English-to-Chinese translation, respectively, and achieved decent performance.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In accordance with our experiments, we also use English and Chinese as the source and target languages, opposite to the Foreign-to-English convention of Brown et al. (1993).</S><S sid = NA ssid = NA>Departing from the conventional noisy-channel approach of Brown et al. (1993), our basic model is a direct one: where e is the English input string and c* is the best Chinese translation according to the translation model Pr(c  |e).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W06-3601.txt | Citing Article:  W08-0308.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The formal description of a TTS transducer is describe din Graehl and Knight (2004), and our baseline approach follows the Extended Tree-to-String Transducer defined in (Huang et al, 2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Similarly, in the tree-transducer terminology, Graehl and Knight (2004) define extended tree transducers that have multi-level trees on the source-side.</S><S sid = NA ssid = NA>With the extended LHS of our transducer, there may be many different rules applicable at one tree node.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W06-3601.txt | Citing Article:  W08-0308.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The implementation of a TTS transducer can be done either top down with memoization to the visited subtrees (Huang et al, 2006), or with a bottom-up dynamic programming (DP) algorithm (Liu et al, 2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This problem can be solved by memoization (Cormen et al., 2001): we cache each subtree that has been visited before, so that every tree node is visited at most once.</S><S sid = NA ssid = NA>We also define a direct probability model and use a linear-time dynamic programming algorithm to search for the best derivation.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W06-3601.txt | Citing Article:  W08-0308.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>To speed up the decoding ,standard beam search is used. In Figure 3, BinaryCombine denotes the target size binarization (Huang et al, 2006) combination. The translation candidates of the template's variables, as well as its terminals, are combined pair wise in the order they appear in the RHS of the template.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In both cases, the source projection is the English tree in Figure 3 (b), and the target projection is the Chinese translation.</S><S sid = NA ssid = NA>2 would be While Section 3 will define the model formally, we first proceed with an example translation from English to Chinese (note in particular that the inverted phrases between source and target): 1Throughout this paper, we will use LHS and source-side interchangeably (so are RHS and target-side).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W06-3601.txt | Citing Article:  W08-0308.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Huang et al (2006) used character-based BLEU as a way of normalizing in consistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Since the target language is Chinese, we report character-based BLEU score instead of word-based to ensure our results are independent of Chinese tokenizations (although our language models are word-based).</S><S sid = NA ssid = NA>Galley et al. (2004) presents a linear-time algorithm for automatic extraction of these xRs rules from a parallel corpora with word-alignment and parse-trees on the source-side, which will be used in our experiments in Section 6.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W06-3601.txt | Citing Article:  D09-1136.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Huang et al (2006) used character-based BLEU as a way of normalizing inconsistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Since the target language is Chinese, we report character-based BLEU score instead of word-based to ensure our results are independent of Chinese tokenizations (although our language models are word-based).</S><S sid = NA ssid = NA>Galley et al. (2004) presents a linear-time algorithm for automatic extraction of these xRs rules from a parallel corpora with word-alignment and parse-trees on the source-side, which will be used in our experiments in Section 6.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W06-3601.txt | Citing Article:  P11-1086.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Note that it is also possible to integrate our rule Markov model with other decoding algorithms, for example, the more common non-incremental top-down/bottom-up approach (Huang et al, 2006), but it would involve a non-trivial change to the decoding algorithms to keep track of the vertical derivation history, which would result in significant overhead.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We are also investigating more principled algorithms for integrating n-gram language models during the search, rather than k-best rescoring.</S><S sid = NA ssid = NA>Similar algorithms have also been proposed for dependency-based translation (Lin, 2004; Ding and Palmer, 2005).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W06-3601.txt | Citing Article:  C10-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The formal description of a TTS transducer is given by Graehl and Knight (2004), and our baseline approach follows the Extended Tree-to-String Transducer defined by Huang et al (2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Similarly, in the tree-transducer terminology, Graehl and Knight (2004) define extended tree transducers that have multi-level trees on the source-side.</S><S sid = NA ssid = NA>With the extended LHS of our transducer, there may be many different rules applicable at one tree node.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W06-3601.txt | Citing Article:  C10-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>It is straightforward to generalize the algorithm for larger n-gram models and TTS templates with any number of children in the bottom using target-side binarized combination (Huang et al, 2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We are also investigating more principled algorithms for integrating n-gram language models during the search, rather than k-best rescoring.</S><S sid = NA ssid = NA>We also define |X  |to be the rank of the rule, i.e., the number of variables in it.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W06-3601.txt | Citing Article:  C10-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Huang et al (2006) used character based BLEU as a way of normalizing inconsistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Since the target language is Chinese, we report character-based BLEU score instead of word-based to ensure our results are independent of Chinese tokenizations (although our language models are word-based).</S><S sid = NA ssid = NA>Galley et al. (2004) presents a linear-time algorithm for automatic extraction of these xRs rules from a parallel corpora with word-alignment and parse-trees on the source-side, which will be used in our experiments in Section 6.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W06-3601.txt | Citing Article:  P07-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We push the idea behind this method further and make the following contributions in this paper: We generalize cube pruning and adapt it to two systems very different from Hiero: a phrase based system similar to Pharaoh (Koehn, 2004) and a tree-to-string system (Huang et al, 2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We compared our system with a state-of-the-art phrase-based system Pharaoh (Koehn, 2004) on the evaluation data.</S><S sid = NA ssid = NA>Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) are good at learning local translations that are pairs of (consecutive) sub-strings, but often insufficient in modeling the reorderings of phrases themselves, especially between language pairs with very different word-order.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W06-3601.txt | Citing Article:  P07-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We test our methods on two large-scale English-to-Chinese translation systems: a phrase-based system and our tree-to-string system (Huang et al, 2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Departing from the conventional noisy-channel approach of Brown et al. (1993), our basic model is a direct one: where e is the English input string and c* is the best Chinese translation according to the translation model Pr(c  |e).</S><S sid = NA ssid = NA>The English sentence (a) is first parsed into the tree in (b), which is then recursively converted into the Chinese string in (e) through five steps.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W06-3601.txt | Citing Article:  P07-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our data preparation follows Huang et al (2006): the training data is a parallel corpus of 28.3M words on the English side, and a trigram language model is trained on the Chinese side.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our training set is a Chinese-English parallel corpus with 1.95M aligned sentences (28.3M words on the English side).</S><S sid = NA ssid = NA>We also use the SRI Language Modeling Toolkit (Stolcke, 2002) to train a Chinese trigram model with Knesser-Ney smoothing on the Chinese side of the parallel corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W06-3601.txt | Citing Article:  P07-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We use the same test set as (Huang et al, 2006), which is a 140-sentence sub set of the NIST 2003 test set with 9-36 words on the English side.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our evaluation data consists of 140 short sentences (< 25 Chinese words) of the Xinhua portion of the NIST 2003 Chinese-to-English evaluation set.</S><S sid = NA ssid = NA>Our training set is a Chinese-English parallel corpus with 1.95M aligned sentences (28.3M words on the English side).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W06-3601.txt | Citing Article:  P07-1019.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For cube growing, we use a non-duplicate k-best method (Huang et al, 2006) to get 100-best unique translations according to LM to estimate the lower-boundheuristics. This preprocessing step takes on aver age 0.12 seconds per sentence, which is negligible in comparison to the +LM decoding time.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We devise a simple-yet-effective algorithm to generate non-duplicate k-best translations for n-gram rescoring.</S><S sid = NA ssid = NA>We finally take the best of all translations.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W06-3601.txt | Citing Article:  C10-1080.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Compared with its string-based counterparts, tree-based decoding is simpler and faster: there is no need for synchronous binarization (Huang et al, 2009b; Zhang et al, 2006) and tree parsing generally runs in linear time (Huang et al, 2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Besides being linguistically motivated, the need for EDL is also supported by empirical findings in MT that one-level rules are often inadequate (Fox, 2002; Galley et al., 2004).</S><S sid = NA ssid = NA>Galley et al. (2004) presents a linear-time algorithm for automatic extraction of these xRs rules from a parallel corpora with word-alignment and parse-trees on the source-side, which will be used in our experiments in Section 6.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W06-3601.txt | Citing Article:  N10-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Many of these systems exploit linguistically-derived syntactic information either on the target side (Galley et al, 2006), the source side (Huang et al, 2006), or both (Liu et al, 2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Besides being linguistically motivated, the need for EDL is also supported by empirical findings in MT that one-level rules are often inadequate (Fox, 2002; Galley et al., 2004).</S><S sid = NA ssid = NA>In accordance with our experiments, we also use English and Chinese as the source and target languages, opposite to the Foreign-to-English convention of Brown et al. (1993).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W06-3601.txt | Citing Article:  P10-4002.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The k-best extraction algorithm is also parameterized by an optional predicate that can filter out derivations at each node, enabling extraction of only derivations that yield different strings as in Huang et al (2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Unfortunately, different derivations may have the same yield (a problem called spurious ambiguity), due to multi-level LHS of our rules.</S><S sid = NA ssid = NA>We first word-align them by GIZA++, then parse the English side by a variant of Collins (1999) parser, and finally apply the rule-extraction algorithm of Galley et al. (2004).</S> | Discourse Facet:  NA | Annotator: Automatic


