Probabilistic CFG With Latent Annotations
This paper defines a generative probabilistic model of parse trees, which we call PCFG-LA.
This model is an extension of PCFG in which non-terminal symbols are augmented with latent variables.
Fine-grained CFG rules are automatically induced from a parsed corpus by training a PCFG-LA model using an EM-algorithm.
Because exact parsing with a PCFG-LA is NP-hard, several approximations are described and empirically compared.
In experiments using the Penn WSJ corpus, our automatically trained model gave a performance of 86.6% (F1, sentences <= 40 words), which is comparable to that of an unlexicalized PCFG parser created using extensive manual feature selection.
We use a markovized grammar to get a better unannotated parse forest during decoding, but we do not markovize the training data.
We right-binarize the tree bank data to construct grammars with only unary and binary productions.
