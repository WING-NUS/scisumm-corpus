[
  {
    "citance_No": 1, 
    "citing_paper_id": "W05-1512", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Detlef, Prescher", 
    "raw_text": "Matsuzaki et al (2005 )independently introduce a similar approach and present empirical results that rival ours", 
    "clean_text": "Matsuzaki et al (2005) independently introduce a similar approach and present empirical results that rival ours.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P14-1099", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Shay B., Cohen | Michael John, Collins", 
    "raw_text": "The results for EM and spectral are reported from Cohen et al (2013) .in Matsuzaki et al (2005)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "N10-1015", 
    "citing_paper_authority": 18, 
    "citing_paper_authors": "David, Burkett | John, Blitzer | Dan, Klein", 
    "raw_text": "These scores are the same as the variational rule scores of Matsuzaki et al (2005) .4 4.2 Alignment", 
    "clean_text": "These scores are the same as the variational rule scores of Matsuzaki et al (2005).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "W06-1638", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Markus, Dreyer | Jason M., Eisner", 
    "raw_text": "Following Matsuzaki et al (2005) and Prescher (2005), we may for example split NP without supervision into NP [0] and NP [1], which behave differently", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "W06-1638", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Markus, Dreyer | Jason M., Eisner", 
    "raw_text": "Matsuzaki et al (2005) introduced a model for such learning: PCFG-LA.2 They used EM to in duce fine-grained versions of a given tree bank? s nonterminals and rules", 
    "clean_text": "Matsuzaki et al (2005) introduced a model for such learning: PCFG-LA.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "W06-1638", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Markus, Dreyer | Jason M., Eisner", 
    "raw_text": "Lari and Young (1990) Words Parse tree Pereira and Schabes (1992) Words and partial brackets Parse tree Klein and Manning (2001) Part-of-speech tags Parse tree Chiang and Bikel (2002) Treebank tree Head child on each nonterminalMatsuzaki et al (2005) Treebank tree Integer feature on each nonterminal INHERIT model (this paper) Treebank tree and head child heuristics Integer feature on each nonterminal Table 1: Observed and hidden data in PCFG grammar learning", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "W06-1638", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Markus, Dreyer | Jason M., Eisner", 
    "raw_text": "Just as Collins manually split the S nonterminal label into S and SG for sentences with and without subjects, Matsuzaki et al (2005) split S into S [1], S [2],..", 
    "clean_text": "Just as Collins manually split the S nonterminal label into S and SG for sentences with and without subjects, Matsuzaki et al (2005) split S into S [1], S [2],.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "W06-1638", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Markus, Dreyer | Jason M., Eisner", 
    "raw_text": "Before extracting the backbone PCFG and running the con strained inside-outside (EM) training algorithm, we preprocessed the Treebank using center-parentbinarization Matsuzaki et al (2005)", 
    "clean_text": "Before extracting the backbone PCFG and running the constrained inside-outside (EM) training algorithm, we preprocessed the Treebank using center-parent binarization Matsuzaki et al (2005).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "W06-1638", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Markus, Dreyer | Jason M., Eisner", 
    "raw_text": "Matsuzaki et al (2005) used a markovized grammar to get a better unannotated parse forest during decoding, but they did not markovize the training data", 
    "clean_text": "Matsuzaki et al (2005) used a markovized grammar to get a better unannotated parse forest during decoding, but they did not markovize the training data.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "W06-1638", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Markus, Dreyer | Jason M., Eisner", 
    "raw_text": "Matsuzaki et al (2005) note that the best annotated parse is in fact NP-hard to find", 
    "clean_text": "Matsuzaki et al (2005) note that the best annotated parse is in fact NP-hard to find.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "W06-1638", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Markus, Dreyer | Jason M., Eisner", 
    "raw_text": "block of experiments in Table 3 used non-markovized gram mars, as in Matsuzaki et al (2005)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "W06-1638", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Markus, Dreyer | Jason M., Eisner", 
    "raw_text": "models are trained on a non-markovized tree bank (as in Matsuzaki et al (2005)); all others are trained on a markovized tree bank", 
    "clean_text": "'Basic' models are trained on a non-markovized tree bank (as in Matsuzaki et al (2005)); all others are trained on a markovized tree bank.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "W06-1638", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Markus, Dreyer | Jason M., Eisner", 
    "raw_text": "With these techniques we reach a parsing accuracy similar to Matsuzaki et al (2005), but with an order of magnitude less parameters, resulting in more efficient parsing", 
    "clean_text": "With these techniques we reach a parsing accuracy similar to Matsuzaki et al (2005), but with an order of magnitude less parameters, resulting in more efficient parsing.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "D08-1016", 
    "citing_paper_authority": 38, 
    "citing_paper_authors": "David A., Smith | Jason M., Eisner", 
    "raw_text": "We could also introduce new variables ,e.g., nonterminal refinements (Matsuzaki et al, 2005), or secondary linksMij (not constrainedby TREE/PTREE) that augment the parse with representations of control, binding, etc", 
    "clean_text": "We could also introduce new variables, e.g., nonterminal refinements (Matsuzaki et al, 2005), or secondary linksMij (not constrainedby TREE/PTREE) that augment the parse with representations of control, binding, etc.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "N10-1003", 
    "citing_paper_authority": 16, 
    "citing_paper_authors": "Slav, Petrov", 
    "raw_text": "We focus on the randomness introduced by the EM algorithm and refer the reader to Matsuzaki et al (2005) and Petrov et al (2006) for a more general introduction", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "N10-1003", 
    "citing_paper_authority": 16, 
    "citing_paper_authors": "Slav, Petrov", 
    "raw_text": "Computing the joint likelihood of the observed parse trees T and sentences w requires sum ming over all derivations t over split subcategories:? i P (wi, Ti)=? i? t: Ti P (wi, t) (1) Matsuzaki et al (2005) derive an EM algorithm for maximizing the joint likelihood, and Petrov et al", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "N10-1003", 
    "citing_paper_authority": 16, 
    "citing_paper_authors": "Slav, Petrov", 
    "raw_text": "Petrov and Klein (2007) present such an objective function, which maximizes the product of expected correct productions r: T? =argmax T? r? T E (r|w) (4) These expectations can be easily computed from the inside/outside scores, similarly as in the maximum bracket recall algorithm of Goodman (1996), or in the variational approximation of Matsuzaki et al (2005)", 
    "clean_text": "These expectations can be easily computed from the inside/outside scores, similarly as in the maximum bracket recall algorithm of Goodman (1996), or in the variational approximation of Matsuzaki et al (2005).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "P12-1046", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Hiroyuki, Shindo | Yusuke, Miyao | Akinori, Fujino | Masaaki, Nagata", 
    "raw_text": "The tree bank data is right-binarized (Matsuzaki et al, 2005) to construct grammars with only unary and binary productions", 
    "clean_text": "The tree bank data is right-binarized (Matsuzaki et al, 2005) to construct grammars with only unary and binary productions.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "W12-1904", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Francis, Ferraro | Benjamin, Van Durme | Matt, Post", 
    "raw_text": "Klein and Manning (2003) built on these observations, introducing a series of manual refinements that captured multiple linguistic phenomena, leading to accurate and fast unlexicalized parsing. Later, automated methods for nonterminal refinement were introduced, first splitting all categories equally (Matsuzaki et al, 2005), and later refining nonterminals to different degrees (Petrov et al,2006) in a split-merge EM framework", 
    "clean_text": "Later, automated methods for nonterminal refinement were introduced, first splitting all categories equally (Matsuzaki et al, 2005), and later refining nonterminals to different degrees (Petrov et al,2006) in a split-merge EM framework.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "D08-1091", 
    "citing_paper_authority": 11, 
    "citing_paper_authors": "Slav, Petrov | Dan, Klein", 
    "raw_text": "The resulting memory limitations alone can prevent the practical learning of highly split grammars (Matsuzaki et al, 2005)", 
    "clean_text": "The resulting memory limitations alone can prevent the practical learning of highly split grammars (Matsuzaki et al, 2005).", 
    "keep_for_gold": 0
  }
]