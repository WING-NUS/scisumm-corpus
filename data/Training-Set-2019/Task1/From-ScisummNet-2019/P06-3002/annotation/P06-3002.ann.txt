Citance Number: 1 | Reference Article:  P06-3002.txt | Citing Article:  N07-3010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Ideally, the application should tell us the granularity of our tagger: e.g. semantic class learners could greatly benefit from the high-granular word sets arising in both of our partitionings, which we endeavoured to lump into a coarser tagset here.</S><S sid = NA ssid = NA>It is time-linear with respect to the number of edges, making its application viable even for graphs with several million nodes and edges.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P06-3002.txt | Citing Article:  N07-3010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In Biemann (2006b), the tagger output was directly compared to supervised taggers for English, German and Finnish via information-theoretic measures.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The resulting taggers are evaluated against outputs of supervised taggers for various languages.</S><S sid = NA ssid = NA>We supported the claim of language-independence by validating the output of our system against supervised systems in three languages.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P06-3002.txt | Citing Article:  P10-2038.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We train SVM Tool and an unsupervised tagger, Unsupos (Biemann, 2006), on our training sections and apply them to the development, test and unlabeled sections.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Unlike in supervised scenarios, our task is not to train a tagger model from a small corpus of hand-tagged data, but from our clusters of derived syntactic categories and a considerably large, yet unlabeled corpus.</S><S sid = NA ssid = NA>Finally, we train a Viterbi tagger with this lexicon and augment it with an affix classifier for unknown words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P06-3002.txt | Citing Article:  W07-0212.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Nevertheless, many of the practically useful spell checkers incorporate context information and the current analysis on SpellNet can be extended for such spell-checkers by conceptualizing a network of words that capture the word co-occurrence patterns (Biemann, 2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>For assigning classes, we use the Chinese Whispers (CW) graph-clustering algorithm, which has been proven useful in NLP applications as described in (Biemann 2006).</S><S sid = NA ssid = NA>The word’s global context is the sum of all its contexts.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P06-3002.txt | Citing Article:  P10-1132.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Ideally, the application should tell us the granularity of our tagger: e.g. semantic class learners could greatly benefit from the high-granular word sets arising in both of our partitionings, which we endeavoured to lump into a coarser tagset here.</S><S sid = NA ssid = NA>It is time-linear with respect to the number of edges, making its application viable even for graphs with several million nodes and edges.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P06-3002.txt | Citing Article:  D08-1096.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Previous graph-theoretic work (Biemann, 2006) uses order 1 representations.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This work constructs an unsupervised POS tagger from scratch.</S><S sid = NA ssid = NA>(van Dongen, 2000; Biemann 2006), find the number of clusters automatically1.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P06-3002.txt | Citing Article:  P07-3008.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In order to test the argument above, and as an attempt to improve the results from the previous experiment, POS-tags were induced using Biemann's unsupervised POS-tagger (Biemann, 2006).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>This work constructs an unsupervised POS tagger from scratch.</S><S sid = NA ssid = NA>Using the resulting word clusters as a lexicon, a Viterbi POS tagger is trained, which is refined by a morphological component.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P06-3002.txt | Citing Article:  W10-2901.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Additionally, we used an unsupervised part-of-speech tagger (see (Biemann, 2006)) to tag the NEGRA corpus to be able to present a complete unsupervised parsing process relying on word strings only.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Unsupervised Part-Of-Speech Tagging Employing Efficient Graph Clustering</S><S sid = NA ssid = NA>An unsupervised part-of-speech (POS) tagging system that relies on graph clustering methods is described.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P06-3002.txt | Citing Article:  D12-1086.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Mintz (2003) only uses the most frequent 45 frames and Biemann (2006) clusters the most frequent 10,000 words using contexts formed from the most frequent 150-200 words.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Contexts in that sense are often restricted to the most frequent words.</S><S sid = NA ssid = NA>In a first stage, we employ a clustering algorithm on distributional similarity, which groups a subset of the most frequent 10,000 words of a corpus into several hundred clusters (partitioning 1).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P06-3002.txt | Citing Article:  W09-1707.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Ideally, the application should tell us the granularity of our tagger: e.g. semantic class learners could greatly benefit from the high-granular word sets arising in both of our partitionings, which we endeavoured to lump into a coarser tagset here.</S><S sid = NA ssid = NA>It is time-linear with respect to the number of edges, making its application viable even for graphs with several million nodes and edges.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P06-3002.txt | Citing Article:  W09-1707.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Ideally, the application should tell us the granularity of our tagger: e.g. semantic class learners could greatly benefit from the high-granular word sets arising in both of our partitionings, which we endeavoured to lump into a coarser tagset here.</S><S sid = NA ssid = NA>It is time-linear with respect to the number of edges, making its application viable even for graphs with several million nodes and edges.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P06-3002.txt | Citing Article:  P11-2009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Ideally, the application should tell us the granularity of our tagger: e.g. semantic class learners could greatly benefit from the high-granular word sets arising in both of our partitionings, which we endeavoured to lump into a coarser tagset here.</S><S sid = NA ssid = NA>It is time-linear with respect to the number of edges, making its application viable even for graphs with several million nodes and edges.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P06-3002.txt | Citing Article:  D07-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Perhaps due to the overly simplistic methods employed to compute morphological information, morphology has only been used as what Biemann (2006) called add-on's in existing POS induction algorithms, which remain primarily distributional in nature.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>An extension to this generic scheme is presented in (Clark, 2003), where morphological Proceedings of the COLING/ACL 2006 Student Research Workshop, pages 7–12, Sydney, July 2006. c�2006 Association for Computational Linguistics information is used for determining the word class of rare words.</S><S sid = NA ssid = NA>The words used to describe syntactic contexts will be called feature words in the remainder.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P06-3002.txt | Citing Article:  W12-0704.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Biemann (2006) described a graph-based clustering methods for word classes.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>An unsupervised part-of-speech (POS) tagging system that relies on graph clustering methods is described.</S><S sid = NA ssid = NA>For assigning classes, we use the Chinese Whispers (CW) graph-clustering algorithm, which has been proven useful in NLP applications as described in (Biemann 2006).</S> | Discourse Facet:  NA | Annotator: Automatic


