Citance Number: 1 | Reference Article:  W00-0712.txt | Citing Article:  N01-1024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our algorithm extends earlier approaches to morphology induction by combining various induced information sources: the semantic relatedness of the affixed forms usinga Latent Semantic Analysis approach to corpus based semantics (Schone and Jurafsky, 2000), affix frequency, syntactic context, and transitive closure.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Knowledge-Free Induction Of Morphology Using Latent Semantic Analysis</S><S sid = NA ssid = NA>We implement our approach using Latent Semantic Analysis and show that our semantics-only approach provides morphology induction results that rival a current state-of-the-art system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  W00-0712.txt | Citing Article:  N01-1024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Most of the existing algorithms described focus on approach falls into this category (expanding upon suffixing in inflectional languages (though our earlier approach (Schone and Jurafsky, 2000)), Jacquemin and Jean describe work on prefixes).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Existing induction algorithms all focus on identifying prefixes, suffixes, and word stems in inflectional languages (avoiding infixes and other language types like concatenative or agglutinative languages (Sproat, 1992)).</S><S sid = NA ssid = NA>Our algorithm also focuses on inflectional languages.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  W00-0712.txt | Citing Article:  N01-1024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In our earlier work, we (Schone and Jurafsky (2000)) generated a list of N candidate suffixes and used this list to identify word pairs which share the same stem but conclude with distinct candidate suffixes.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>To select candidate affixes, we, like Gaussier, identify p-similar words.</S><S sid = NA ssid = NA>Stage 3 Stage 4 ind wo-r\ pairs that are possible morphoWe next identify pairs of candidate affixes that descend from a common ancestor node in the trie.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  W00-0712.txt | Citing Article:  N01-1024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We had noted previously (Schone and Jurafsky, 2000), however, that errors can arise from strictly orthographic systems.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Hence, there is merit to considering subrules that arise while performing analysis on a particular rule.</S><S sid = NA ssid = NA>Several problems can arise using only stem-and-affix statistics: (1) valid affixes may be applied inappropriately (&quot;ally&quot; stemming to &quot;all&quot;), (2) morphological ambiguity may arise (&quot;rating&quot; conflating with &quot;rat&quot; instead of &quot;rate&quot;), and (3) non-productive affixes may get accidentally pruned (the relationship between &quot;dirty&quot; and &quot;dirt&quot; may be lost).1 Some of these problems could be resolved if one could incorporate word semantics.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  W00-0712.txt | Citing Article:  N01-1024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>As in our earlier approach (Schone and Jurafsky, 2000), we begin by generating, from an untagged corpus, a list of word pairs that might be morphological variants.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Our approach (see Figure 1) can be decomposed into four components: (1) initially selecting candidate affixes, (2) identifying affixes which are potential morphological variants of each other, (3) computing semantic vectors for words possessing these candidate affixes, and (4) selecting as valid morphological variants those words with similar semantic vectors.</S><S sid = NA ssid = NA>We here show that incorporating LSA-based semantics alone into the morphology-induction process can provide results that rival a state-ofthe-art system based on stem-and-affix statistics (Goldsmith's Linguistica). lError examples are from Goldsmith's Linguistica Our algorithm automatically extracts potential affixes from an untagged corpus, identifies word pairs sharing the same proposed stem but having different affixes, and uses LSA to judge semantic relatedness between word pairs.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  W00-0712.txt | Citing Article:  N01-1024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Using this final lexicon, we can now seek for suffixes in a manner equivalent to what we had done before (Schone and Jurafsky, 2000).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We compare our algorithm to Goldsmith's Linguistica (2000) by using CELEX's (Baayen, et al., 1993) suffixes as a gold standard.</S><S sid = NA ssid = NA>The words and parts of speech from his inflectional lexicon serve for building relational families of words and identifying sets of word pairs and suffixes therefrom.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  W00-0712.txt | Citing Article:  N01-1024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In order to obtain semantic representations of each word, we apply our previous strategy (Schone and Jurafsky (2000)).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>To obtain a NCS, we first calculate the cosine between each semantic vector, nw, and the semantic vectors from 200 randomly chosen words.</S><S sid = NA ssid = NA>First, to stay as close to the knowledge-free scenario as possible, we neither apply a stopword list nor remove capitalization.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  W00-0712.txt | Citing Article:  N01-1024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>To correlate these semantic vectors, we use normalized cosine scores (NCSs) as we had illustrated before (Schone and Jurafsky (2000)).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We use what we call a normalized cosine score (NCS) as a correlation.</S><S sid = NA ssid = NA>To obtain a NCS, we first calculate the cosine between each semantic vector, nw, and the semantic vectors from 200 randomly chosen words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  W00-0712.txt | Citing Article:  N01-1024.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We compare this improved algorithm to our former algorithm (Schone and Jurafsky (2000)) as well as to Goldsmith's Linguistica (2000).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We compare our algorithm to Goldsmith's Linguistica (2000) by using CELEX's (Baayen, et al., 1993) suffixes as a gold standard.</S><S sid = NA ssid = NA>Table 4 uses the above scoring mechanism to compare between Linguistica and our system (at various probability thresholds).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  W00-0712.txt | Citing Article:  P13-1118.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>They are either based solely on corpus statistics (Djean, 1998), measure semantic similarity between input and output lemma (Schone and Jurafsky, 2000), or bootstrap derivation rules starting from seed examples (Piasecki et al, 2012).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Latent Semantic Analysis (LSA) (Deerwester, et al., 1990); Landauer, et at., 1998) is a technique which automatically identifies semantic information from a corpus.</S><S sid = NA ssid = NA>We call these pairs rules.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  W00-0712.txt | Citing Article:  W02-0602.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Using latent semantic analysis, Schone and Jurafsky (2000) have previously demonstrated the success of using semantic information in morphological analysis.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Knowledge-Free Induction Of Morphology Using Latent Semantic Analysis</S><S sid = NA ssid = NA>Latent Semantic Analysis (LSA) (Deerwester, et al., 1990); Landauer, et at., 1998) is a technique which automatically identifies semantic information from a corpus.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  W00-0712.txt | Citing Article:  N03-2015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Many researchers, including Schone and Jurafsky (2000), Harris (1958), and Djean (1998), suggest looking for nodes with high branching (out-degree) or a large number of continuations.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We would like to be able to incorporate semantics for an arbitrarily large number of words and LSA quickly becomes impractical on large sets.</S><S sid = NA ssid = NA>DeJean (1998) uses an approach derived from Harris (1951) where word-splitting occurs if the number of distinct letters that follows a given sequence of characters surpasses a threshold.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  W00-0712.txt | Citing Article:  W02-0603.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In a different approach, Schone and Jurafsky (2000) utilize the context of each term to obtain a semantic representation for it using LSA.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Lastly, instead of generating a term-document matrix, we build a term-term matrix.</S><S sid = NA ssid = NA>We implement our approach using Latent Semantic Analysis and show that our semantics-only approach provides morphology induction results that rival a current state-of-the-art system.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  W00-0712.txt | Citing Article:  D10-1029.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The idea thata stem and stem+affix should be semantically similar has been exploited previously for morphological analysis (Schone and Jurafsky, 2000).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We introduce a semantic-based algorithm for learning morphology which only proposes affixes when the stem and stem-plusaffix are sufficiently similar semantically.</S><S sid = NA ssid = NA>We here show that incorporating LSA-based semantics alone into the morphology-induction process can provide results that rival a state-ofthe-art system based on stem-and-affix statistics (Goldsmith's Linguistica). lError examples are from Goldsmith's Linguistica Our algorithm automatically extracts potential affixes from an untagged corpus, identifies word pairs sharing the same proposed stem but having different affixes, and uses LSA to judge semantic relatedness between word pairs.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  W00-0712.txt | Citing Article:  W04-0107.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Next along the spectrum of orthographic similarity bias is the work of Schone and Jurafsky (2000), who first acquire a list of pairs of potential morphological variants (PPMVs) using an or tho graphic similarity technique due to Gaussier (1999), in which pairs of words from a corpus vocabulary with the same initial string are identified.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Gaussier splits words based on p-similarity â€” words that agree in exactly the first p characters.</S><S sid = NA ssid = NA>We call these pairs rules.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  W00-0712.txt | Citing Article:  P11-1090.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>To reduce the running time of the model we limit the space of considered morpheme boundaries as follows: Given the target side of the corpus, we derive a list of K most frequent prefixes and suffixes using a simple trie-based method proposed by (Schone and Jurafsky, 2000).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>(It should also be mentioned that we can identify potential prefixes by inserting words into the trie in reversed order.</S><S sid = NA ssid = NA>We compare our algorithm to Goldsmith's Linguistica (2000) by using CELEX's (Baayen, et al., 1993) suffixes as a gold standard.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  W00-0712.txt | Citing Article:  D09-1070.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Following Schone and Jurafsky (2000), clusters are evaluated for whether they capture inflectional paradigms using CELEX (Baayen et al, 1993).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We compare our algorithm to Goldsmith's Linguistica (2000) by using CELEX's (Baayen, et al., 1993) suffixes as a gold standard.</S><S sid = NA ssid = NA>Though our algorithm could be applied to any inflectional language, we here restrict it to English in order to perform evaluations against the human-labeled CELEX database (Baayen, et al., 1993).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  W00-0712.txt | Citing Article:  D09-1070.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Schone and Jurafsky (2000) attempts to cluster morphologically related words starting with an unrefined trie search (but with a parameter of minimum possible stem length and an upper bound on potential affix candidates) that is constrained by semantic similarity in a word context vector space.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Morphologically-related words frequently share similar semantics, so we want to see how well semantic vectors of PPMVs correlate.</S><S sid = NA ssid = NA>We insert words into a trie (Figure 2) and extract potential affixes by observing those places in the trie where branching occurs.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  W00-0712.txt | Citing Article:  D09-1070.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Schone and Jurafsky (2000) give definitions for correct (C), inserted (I), and deleted (D) words in model-derived conflation sets in relation to a gold standard.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>To evaluate an algorithm, we sum the number of correct (C), inserted (I) , and deleted (D) words it predicts for each hypothesized conflation set.</S><S sid = NA ssid = NA>We will refer to these vertex sets as conflation sets.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  W00-0712.txt | Citing Article:  W02-0606.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In current work, we are examining how to combine these two approaches.</S><S sid = NA ssid = NA>(It should also be mentioned that we can identify potential prefixes by inserting words into the trie in reversed order.</S> | Discourse Facet:  NA | Annotator: Automatic


