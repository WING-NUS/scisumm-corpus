Large-scale natural language generation requires the integration of vast mounts of knowledge: lexical, grammatical, and conceptual.
A robust generator must be able to operate well even when pieces of knowledge are missing.
It must also be robust against incomplete or inaccurate inputs.
To attack these problems, we have built a hybrid generator, in which gaps in symbolic knowledge are filled by statistical methods.
We describe algorithms and show experimental results.
We also discuss how the hybrid generation model can be used to simplify current generators and enhance their portability, even when perfect knowledge is in principle obtainable.
