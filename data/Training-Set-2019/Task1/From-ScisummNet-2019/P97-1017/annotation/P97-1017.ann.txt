Citance Number: 1 | Reference Article:  P97-1017.txt | Citing Article:  W98-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Extending this notion, (Knight and Graehl, 1997) built five probability distributions:.</S> | Reference Offset:  ['48','59'] | Reference Text:  <S sid = 48 ssid = >Extending this notion, we settled down to build five probability distributions: Given a katakana string o observed by OCR, we want to find the English word sequence w that maximizes the sum, over all e, j, and k, of Following (Pereira et al.. 1994; Pereira and Riley, 1996), we implement P(w) in a weighted finite-state acceptor (WFSA) and we implement the other distributions in weighted finite-state transducers (WFSTs).</S><S sid = 59 ssid = >This section describes how we designed and built each of our five models.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  P97-1017.txt | Citing Article:  W98-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Fortunately, the first two models of (Knight and Graehl, 1997) deal with English only, so we can reuse them directly for Arabic/English transliteration.</S> | Reference Offset:  ['8','167'] | Reference Text:  <S sid = 8 ssid = >However, the situation is more complicated for language pairs that employ very different alphabets and sound systems, such as Japanese/English and Arabic/English.</S><S sid = 167 ssid = >We also plan to explore probabilistic models for Arabic/English transliteration.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  P97-1017.txt | Citing Article:  W98-1005.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We applied the EM learning algorithm described in (Knight and Graehl, 1997) on this data, with one variation.</S> | Reference Offset:  ['89','97'] | Reference Text:  <S sid = 89 ssid = >We then applied the estimationmaximization (EM) algorithm (Baum, 1972) to generate symbol-mapping probabilities, shown in Figure 1.</S><S sid = 97 ssid = >Because no alignments are possible, such pairs are skipped by the learning algorithm; cases like these must be solved by dictionary lookup anyway.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  P97-1017.txt | Citing Article:  E12-1014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Transliterations can be generated for tokens in a source phrase (Knightand Graehl, 1997), with o (f, e) calculating phonetic similarity rather than orthographic.</S> | Reference Offset:  ['18','123'] | Reference Text:  <S sid = 18 ssid = >Here are a few more examples: Notice how the transliteration is more phonetic than orthographic; the letter h in Johnson does not produce any katakana.</S><S sid = 123 ssid = >We start with a katakana phrase as observed by OCR.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  P97-1017.txt | Citing Article:  I08-6004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Transliteration of a name, for the purpose of this work, is defined as its transcription in a different language, preserving the phonetics, perhaps in a different orthography [Knight and Graehl, 1997].</S> | Reference Offset:  ['1','8'] | Reference Text:  <S sid = 1 ssid = >It is challenging to translate names and technical terms across languages with different alphabets and sound inventories.</S><S sid = 8 ssid = >However, the situation is more complicated for language pairs that employ very different alphabets and sound systems, such as Japanese/English and Arabic/English.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  P97-1017.txt | Citing Article:  W10-2406.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In this work we treat the process of transliteration as a process of direct transduction from sequences of tokens in the source language to sequences of tokens in the target language with no modeling of the phonetics of either source or target language (Knight and Graehl, 1997).</S> | Reference Offset:  ['76','133'] | Reference Text:  <S sid = 76 ssid = >I A /\ r 0 0 r o o Next, we map English sound sequences onto Japanese sound sequences.</S><S sid = 133 ssid = >We have performed two large-scale experiments, one using a full-language P(w) model, and one using a personal name language model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  P97-1017.txt | Citing Article:  N06-1011.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Knight and Graehl (1997) build a generative model for back ward transliteration from Japanese to English.</S> | Reference Offset:  ['5','40'] | Reference Text:  <S sid = 5 ssid = >This method uses a generative model, incorporating several distinct stages in the transliteration process.</S><S sid = 40 ssid = >After initial experiments along these lines, we decided to step back and build a generative model of the transliteration process, which goes like this: This divides our problem into five sub-problems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  P97-1017.txt | Citing Article:  H05-1055.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Knight and Graehl, 1997) proposed a generative transliteration model to transliterate foreign names in Japanese back to English using finite state transducers.</S> | Reference Offset:  ['88','133'] | Reference Text:  <S sid = 88 ssid = >For each glossary entry, we converted English words into English sounds using the previous section's model, and we converted katakana words into Japanese sounds using the next section's model.</S><S sid = 133 ssid = >We have performed two large-scale experiments, one using a full-language P(w) model, and one using a personal name language model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  P97-1017.txt | Citing Article:  W09-0418.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For example, in order to translate names and technical terms, (Knight and Graehl, 1997) introduced a probabilistic model that replaces Japanese katakana words with phonetically equivalent English words.</S> | Reference Offset:  ['1','6'] | Reference Text:  <S sid = 1 ssid = >It is challenging to translate names and technical terms across languages with different alphabets and sound inventories.</S><S sid = 6 ssid = >Translators must deal with many problems, and one of the most frequent is translating proper names and technical terms.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  P97-1017.txt | Citing Article:  P08-2014.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Previous works usually take a generative approach, (Knight and Graehl, 1997).</S> | Reference Offset:  ['25','55'] | Reference Text:  <S sid = 25 ssid = >However, very little computational work has been done in this area; (Yamron et al., 1994) briefly mentions a patternmatching approach, while (Arbabi et al., 1994) discuss a hybrid neural-net/expert-system approach to (forward) transliteration.</S><S sid = 55 ssid = >The approach is modular.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  P97-1017.txt | Citing Article:  C04-1102.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >However, back transliteration is known to be a very difficult task (Knight and Graehl, 1997).</S> | Reference Offset:  ['158','168'] | Reference Text:  <S sid = 158 ssid = >When word separators ( • ) are removed from the katakana phrases, rendering the task exceedingly difficult for people, the machine's performance is unchanged.</S><S sid = 168 ssid = >Simply identifying which Arabic words to transliterate is a difficult task in itself; and while Japanese tends to insert extra vowel sounds, Arabic is usually written without any (short) vowels.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  P97-1017.txt | Citing Article:  C04-1102.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >There are several studies on transliteration (e.g., (Knight and Graehl, 1997)), and they tell us that machine transliteration of language pairs that employ very different alphabets and sound systems is extremely difficult, and that the technology is still to immature for use in practical processing.</S> | Reference Offset:  ['0','8'] | Reference Text:  <S sid = 0 ssid = >Machine Transliteration</S><S sid = 8 ssid = >However, the situation is more complicated for language pairs that employ very different alphabets and sound systems, such as Japanese/English and Arabic/English.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  P97-1017.txt | Citing Article:  P06-1103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Knight and Graehl, 1997) build a generative model for backward transliteration from Japanese to English.</S> | Reference Offset:  ['5','40'] | Reference Text:  <S sid = 5 ssid = >This method uses a generative model, incorporating several distinct stages in the transliteration process.</S><S sid = 40 ssid = >After initial experiments along these lines, we decided to step back and build a generative model of the transliteration process, which goes like this: This divides our problem into five sub-problems.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  P97-1017.txt | Citing Article:  N10-1065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >(Knight and Graehl, 1997) and extended Markov window (Jung et al, 2000) treat transliteration as a phonetic process rather than an orthographic process.</S> | Reference Offset:  ['5','18'] | Reference Text:  <S sid = 5 ssid = >This method uses a generative model, incorporating several distinct stages in the transliteration process.</S><S sid = 18 ssid = >Here are a few more examples: Notice how the transliteration is more phonetic than orthographic; the letter h in Johnson does not produce any katakana.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  P97-1017.txt | Citing Article:  W03-0104.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Back-transliteration is a difficult problem as exemplified in (Knight and Graehl 1997, Chen, et.al. 1998).</S> | Reference Offset:  ['21','25'] | Reference Text:  <S sid = 21 ssid = >Transliteration is not trivial to automate, but we will be concerned with an even more challenging problem—going from katakana back to English, i.e., back-transliteration.</S><S sid = 25 ssid = >However, very little computational work has been done in this area; (Yamron et al., 1994) briefly mentions a patternmatching approach, while (Arbabi et al., 1994) discuss a hybrid neural-net/expert-system approach to (forward) transliteration.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  P97-1017.txt | Citing Article:  W03-1508.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Finite state transducers that implement transformation rules for back-transliteration from Japanese to English have been described by Knight and Graehl (1997), and extended to Arabic by Glover-Stalls and Knight (1998).</S> | Reference Offset:  ['48','167'] | Reference Text:  <S sid = 48 ssid = >Extending this notion, we settled down to build five probability distributions: Given a katakana string o observed by OCR, we want to find the English word sequence w that maximizes the sum, over all e, j, and k, of Following (Pereira et al.. 1994; Pereira and Riley, 1996), we implement P(w) in a weighted finite-state acceptor (WFSA) and we implement the other distributions in weighted finite-state transducers (WFSTs).</S><S sid = 167 ssid = >We also plan to explore probabilistic models for Arabic/English transliteration.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  P97-1017.txt | Citing Article:  I08-1030.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Using machine transliteration can resolve part of UNK translation (Knight and Graehl, 1997).</S> | Reference Offset:  ['0','9'] | Reference Text:  <S sid = 0 ssid = >Machine Transliteration</S><S sid = 9 ssid = >Phonetic translation across these pairs is called transliteration.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  P97-1017.txt | Citing Article:  I08-1030.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Other proposed methods include paraphrasing (Callison-Burch et al, 2006) and transliteration (Knight and Graehl, 1997) that uses the feature of phonetic similarity.</S> | Reference Offset:  ['9','25'] | Reference Text:  <S sid = 9 ssid = >Phonetic translation across these pairs is called transliteration.</S><S sid = 25 ssid = >However, very little computational work has been done in this area; (Yamron et al., 1994) briefly mentions a patternmatching approach, while (Arbabi et al., 1994) discuss a hybrid neural-net/expert-system approach to (forward) transliteration.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  P97-1017.txt | Citing Article:  E09-1050.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >One usually distinguishes between two types of transliteration (Knight and Graehl, 1997): Forward transliteration, where an originally Hebrewterm is to be transliterated to English; and Backward transliteration, in which a foreign term that has already been transliterated into Hebrew is to be recovered.</S> | Reference Offset:  ['0','104'] | Reference Text:  <S sid = 0 ssid = >Machine Transliteration</S><S sid = 104 ssid = >They are more useful for English-to-Japanese forward transliteration.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  P97-1017.txt | Citing Article:  W02-1809.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Knight and Graehl (1997) have proposed to compose a set of weighted finite state transducers to solve the much more complicated problem of back-transliteration from Japanese Katakana to English.</S> | Reference Offset:  ['21','48'] | Reference Text:  <S sid = 21 ssid = >Transliteration is not trivial to automate, but we will be concerned with an even more challenging problem—going from katakana back to English, i.e., back-transliteration.</S><S sid = 48 ssid = >Extending this notion, we settled down to build five probability distributions: Given a katakana string o observed by OCR, we want to find the English word sequence w that maximizes the sum, over all e, j, and k, of Following (Pereira et al.. 1994; Pereira and Riley, 1996), we implement P(w) in a weighted finite-state acceptor (WFSA) and we implement the other distributions in weighted finite-state transducers (WFSTs).</S> | Discourse Facet:  NA | Annotator: Automatic


