[
  {
    "citance_No": 1, 
    "citing_paper_id": "P12-1081", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Emily, Pitler", 
    "raw_text": "S (g, h, m, s) (4) For projective parsing, dynamic programming for this factorization was derived in Koo and Collins (2010) (Model 1 in that paper), and for non projective parsing, dual decomposition was used for this factorization in Koo et al (2010) .This factorization should combine all the benefits of the sibling and grandparent factorizations described above? for Conversion 1, sibling scoring may help conjunctions and grandparent scoring may help prepositions, and for Conversion 2, grandparent scoring should help both, while sibling scoring may or may not add some additional gains", 
    "clean_text": "For projective parsing, dynamic programming for this factorization was derived in Koo and Collins (2010) (Model 1 in that paper), and for non projective parsing, dual decomposition was used for this factorization in Koo et al (2010).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "D12-1131", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Alexander M., Rush | Michael John, Collins | Roi, Reichart | Amir, Globerson", 
    "raw_text": "In NLP, Rush et al (2010) and Koo et al (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging", 
    "clean_text": "In NLP, Rush et al (2010) and Koo et al (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "D12-1131", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Alexander M., Rush | Michael John, Collins | Roi, Reichart | Amir, Globerson", 
    "raw_text": "If these were not there, the problem would factor into two parts, an optimization of F over the test corpus Y (X) and an optimization of g over possibleMRF assignments Z. The first problem factors naturally into sentence-level parsing problems and the second can be solved efficiently given our assumptions on the MRF topology G. Recent work has shown that a relaxation base don dual decomposition often produces an exact solution for such problems (Koo et al 2010)", 
    "clean_text": "Recent work has shown that a relaxation based on dual decomposition often produces an exact solution for such problems (Koo et al 2010).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "D12-1131", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Alexander M., Rush | Michael John, Collins | Roi, Reichart | Amir, Globerson", 
    "raw_text": "In our dual decomposition inference algorithm, we use K =200 maximum iterations and tune the decay rate fol lowing the protocol described by Koo et al (2010)", 
    "clean_text": "In our dual decomposition inference algorithm, we use K =200 maximum iterations and tune the decay rate following the protocol described by Koo et al (2010).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "D12-1131", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Alexander M., Rush | Michael John, Collins | Roi, Reichart | Amir, Globerson", 
    "raw_text": "First, we follow Koo et al (2010) and use lazy decoding as part of dual decomposition", 
    "clean_text": "First, we follow Koo et al (2010) and use lazy decoding as part of dual decomposition.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "P13-2109", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Andre, Martins | Miguel, Almeida | Noah A., Smith", 
    "raw_text": "Approximate parsers have there fore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011)", 
    "clean_text": "Approximate parsers have there fore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "P13-2109", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Andre, Martins | Miguel, Almeida | Noah A., Smith", 
    "raw_text": "While AD3 requires solving quadratic subproblems as an intermediate step, recent results (Martins et al, 2012) show that they can beaddressed with the same oracles used in the sub gradient method (Koo et al, 2010)", 
    "clean_text": "While AD requires solving quadratic subproblems as an intermediate step, recent results (Martins et al, 2012) show that they can be addressed with the same oracles used in the subgradient method (Koo et al, 2010).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "P13-2109", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Andre, Martins | Miguel, Almeida | Noah A., Smith", 
    "raw_text": "This opens the door for larger subproblems (such as the combination of trees and head automata in Koo et al, 2010) instead of a many-components approach (Martins et al, 2011), while still enjoying faster convergence", 
    "clean_text": "This opens the door for larger subproblems (such as the combination of trees and head automata in Koo et al, 2010) instead of a many-components approach (Martins et al, 2011), while still enjoying faster convergence.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "P13-2109", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Andre, Martins | Miguel, Almeida | Noah A., Smith", 
    "raw_text": "9Koo et al (2010) used an identical automaton for their second-order model, but leaving out the grand-sibling scores", 
    "clean_text": "Koo et al (2010) used an identical automaton for their second-order model, but leaving out the grand-sibling scores.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "P13-2109", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Andre, Martins | Miguel, Almeida | Noah A., Smith", 
    "raw_text": "includes the most accurate parsers among Nivre et al (2006), McDonald et al (2006), Martins et al (2010, 2011), Koo et al (2010), Rush and Petrov (2012), Zhang and McDonald (2012)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "D12-1030", 
    "citing_paper_authority": 14, 
    "citing_paper_authors": "Hao, Zhang | Ryan, McDonald", 
    "raw_text": "Our parser achieves the state-of-art unlabeled accuracy of 93.06% and labeled accuracy of 91.86% on the standard test set for English, at a faster speed than a reimplementation of the third-order model of Koo et al2010)", 
    "clean_text": "Our parser achieves the state-of-art unlabeled accuracy of 93.06% and labeled accuracy of 91.86% on the standard test set for English, at a faster speed than a reimplementation of the third-order model of Koo et al (2010).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "N12-1087", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Rajhans, Samdani | Ming-Wei, Chang | Dan, Roth", 
    "raw_text": "Few of many examples include type constraints between relations and entities (Roth and Yih, 2004), sentential and modifier constraints during sentence compression (Clarke and Lapata,2006), and agreement constraints between word alignment directions (Ganchev et al, 2008) or various parsing models (Koo et al, 2010)", 
    "clean_text": "Few of many examples include type constraints between relations and entities (Roth and Yih, 2004), sentential and modifier constraints during sentence compression (Clarke and Lapata,2006), and agreement constraints between word alignment directions (Ganchev et al, 2008) or various parsing models (Koo et al, 2010).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "P14-2032", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Mengqiu, Wang | Rob, Voigt | Christopher D., Manning", 
    "raw_text": "DD has been successfully applied to similar situations for combining local with global models; for example, in dependency parsing (Koo et al, 2010), bilingual sequence tagging (Wang et al, 2013) and word alignment (DeNero and Macherey, 2011)", 
    "clean_text": "DD has been successfully applied to similar situations for combining local with global models; for example, in dependency parsing (Koo et al, 2010), bilingual sequence tagging (Wang et al, 2013) and word alignment (DeNero and Macherey, 2011).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "P14-2032", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Mengqiu, Wang | Rob, Voigt | Christopher D., Manning", 
    "raw_text": "We also give an updated Viterbidecoding algorithm for CRF and a modified beam search algorithm for perceptron in Algorithm 1. T is the maximum number of iterations before early stopping, and? t is the learning rate at time t. We adopt a learning rate update rule from Koo et al (2010) where? t is defined as 1 N, where N is the number of times we observed a consecutive dual value increase from iteration 1 to t", 
    "clean_text": "We adopt a learning rate update rule from Koo et al (2010) where t is defined as 1/N, where N is the number of times we observed a consecutive dual value increase from iteration 1 to t.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "N12-1054", 
    "citing_paper_authority": 13, 
    "citing_paper_authors": "Alexander M., Rush | Slav, Petrov", 
    "raw_text": "We also note that while direct speed comparison are difficult, our parser is significantly faster than the published results for other high accuracy parsers ,e.g. Huang and Sagae (2010) and Koo et al (2010)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "P13-2019", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Guangyou, Zhou | Jun, Zhao", 
    "raw_text": "For the third-order features (e.g., grand-siblings and tri-siblings) described in (Koo et al, 2010), we will discuss it in future work", 
    "clean_text": "For the third-order features (e.g., grand-siblings and tri-siblings) described in (Koo et al, 2010), we will discuss it in future work.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "P13-1106", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Mengqiu, Wang | Wanxiang, Che | Christopher D., Manning", 
    "raw_text": ") Similar to previous work, we solve this DD problem by iteratively updating the sub-gradient as depicted in Algorithm 1. T is the maximum number of iterations before early stopping, and? t is the learning rate at time t. We adopt a learning rate update rule from Koo et al (2010) where? t is defined as 1N, where N is the number of times we observed a consecutive dual value increase from iteration 1 to t. A thorough introduction to the theoretical foundations of dual decomposition algorithms is beyond the scope of this paper; we encourage unfamiliar readers to read Rush and Collins (2012) for a full tutorial", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "P13-1106", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Mengqiu, Wang | Wanxiang, Che | Christopher D., Manning", 
    "raw_text": "Although we have seen more than a handful of recent papers that apply the dual decomposition method for joint inference problems, all of the past work deals with cases where the various model components have the same inference output space (e.g., dependency parsing (Koo et al, 2010), POS tagging (Rush et al., 2012), etc.)", 
    "clean_text": "Although we have seen more than a handful of recent papers that apply the dual decomposition method for joint inference problems, all of the past work deals with cases where the various model components have the same inference output space (e.g., dependency parsing (Koo et al, 2010), POS tagging (Rush et al., 2012), etc.).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "D11-1022", 
    "citing_paper_authority": 18, 
    "citing_paper_authors": "Andre, Martins | Noah A., Smith | Mario, Figueiredo | Pedro, Aguiar", 
    "raw_text": "(4 )gle factor in a factor graph (Smith and Eisner, 2008), or to a entire subgraph enclosing several factors (Koo et al, 2010), or even to a formula in Markov logic (Richardson and Domingos, 2006)", 
    "clean_text": "There is a lot of flexibility about how to decompose the model into S components: each set Rs can correspond to a single factor in a factor graph (Smith and Eisner, 2008), or to a entire subgraph enclosing several factors (Koo et al, 2010), or even to a formula in Markov logic (Richardson and Domingos, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "D11-1022", 
    "citing_paper_authority": 18, 
    "citing_paper_authors": "Andre, Martins | Noah A., Smith | Mario, Figueiredo | Pedro, Aguiar", 
    "raw_text": "While this is good enough when tight relaxations are frequent, as in the settings explored by Rush et al (2010), Koo et al (2010), and Rush and Collins (2011), it is hard to know when to stop when a relaxation gap exists. We would like to have similar guarantees concerning the relaxed primal P? .5 A general weakness of sub gradient algorithms is that they do not have this capacity, and so are usually stopped by specifying a maximum number of iterations", 
    "clean_text": "", 
    "keep_for_gold": 0
  }
]