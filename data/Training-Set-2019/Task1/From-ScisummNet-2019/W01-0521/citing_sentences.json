[
  {
    "citance_No": 1, 
    "citing_paper_id": "W01-0908", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Yuval, Krymolowski", 
    "raw_text": "Another approach to this topic, examining the effect of using lexicalbigram information, which is very corpus specific, appears in (Gildea, 2001)", 
    "clean_text": "Another approach to this topic, examining the effect of using lexical bigram information, which is very corpus specific, appears in (Gildea, 2001).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P14-1022", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "David, Hall | Greg, Durrett | Dan, Klein", 
    "raw_text": "Lexicalization allows us to capture bi lexical relationships along dependency arcs, but it has been previously shown that these add only marginal benefit to Collins? s model anyway (Gildea, 2001)", 
    "clean_text": "Lexicalization allows us to capture bi-lexical relationships along dependency arcs, but it has been previously shown that these add only marginal benefit to Collins's model anyway (Gildea, 2001).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "N06-1020", 
    "citing_paper_authority": 88, 
    "citing_paper_authors": "David, McClosky | Eugene, Charniak | Mark, Johnson", 
    "raw_text": "Gildea (2001) and Bacchiani et al (2006) show that out-of-domain training data can improve parsing ac 152 curacy", 
    "clean_text": "Gildea (2001) and Bacchiani et al (2006) show that out-of-domain training data can improve parsing accuracy.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "W10-3911", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Yasuhide, Miura | Eiji, Aramaki | Tomoko, Ohkuma | Masatsugu, Tonoike | Daigo, Sugihara | Hiroshi, Masuichi | Kazuhiko, Ohe", 
    "raw_text": "A statistical syntactic parser is known to perform badly if a text to be parsed belongs to a domain which differs from a domain on which the parser is trained (Gildea, 2001)", 
    "clean_text": "A statistical syntactic parser is known to perform badly if a text to be parsed belongs to a domain which differs from a domain on which the parser is trained (Gildea, 2001).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P10-1036", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Jonathan K., Kummerfeld | Jessika, Roesner | Tim, Dawborn | James, Haggerty | James R., Curran | Stephen, Clark", 
    "raw_text": "Previous work has shown that parsers typically perform poorly outside of their training domain (Gildea, 2001)", 
    "clean_text": "Previous work has shown that parsers typically perform poorly outside of their training domain (Gildea, 2001).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "P10-1036", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Jonathan K., Kummerfeld | Jessika, Roesner | Tim, Dawborn | James, Haggerty | James R., Curran | Stephen, Clark", 
    "raw_text": "When applying parsers out of domain they are typically slower and less accurate (Gildea, 2001)", 
    "clean_text": "When applying parsers out of domain they are typically slower and less accurate (Gildea, 2001).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "P11-2127", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Mohit, Bansal | Dan, Klein", 
    "raw_text": "For German, our parser outperformsDubey (2005) and we are not far behind latent variable parsers, for which parsing is substantially7These statistics can be further improved with standard parsing micro-optimization.8See Gildea (2001) and Petrov and Klein (2007) for the ex act experimental setup that we followed here", 
    "clean_text": "See Gildea (2001) and Petrov and Klein (2007) for the exact experimental setup that we followed here.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "W07-2203", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Rebecca, Watson | Ted, Briscoe | John, Carroll", 
    "raw_text": "We therefore, fol lowing Gildea (2001) and others, consider S, and also the baseline training data, B, as out-of domain training data", 
    "clean_text": "We therefore, following Gildea (2001) and others, consider S, and also the baseline training data, B, as out-of domain training data.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "W04-2410", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Mark, McLauchlan", 
    "raw_text": "Lexical information has a lot of promise for parse selection in theory, but there are practical problems such as sparse data and genre effects (Gildea, 2001)", 
    "clean_text": "Lexical information has a lot of promise for parse selection in theory, but there are practical problems such as sparse data and genre effects (Gildea, 2001).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "D09-1085", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Laura, Rimell | Stephen, Clark | Mark, Steedman", 
    "raw_text": "Second, it is well known that the accuracy of parsers trained on the Penn Treebank degrades when they are applied to different genres and do mains (Gildea, 2001)", 
    "clean_text": "Second, it is well known that the accuracy of parsers trained on the Penn Treebank degrades when they are applied to different genres and domains (Gildea, 2001).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "D08-1093", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Sujith, Ravi | Kevin, Knight | Radu, Soricut", 
    "raw_text": "Gildea (2001) studied how well WSJ-trained parsers do on the Brown corpus, for which a gold standard exists", 
    "clean_text": "Gildea (2001) studied how well WSJ-trained parsers do on the Brown corpus, for which a gold standard exists.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "D08-1093", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Sujith, Ravi | Kevin, Knight | Radu, Soricut", 
    "raw_text": "For the Brown corpus, the test set was formed from every tenth sentence in the corpus (Gildea, 2001)", 
    "clean_text": "For the Brown corpus, the test set was formed from every tenth sentence in the corpus (Gildea, 2001).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "I08-2096", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Tuomo, Kakkonen | Erkki, Sutinen", 
    "raw_text": "The genre dependency of parsers is an accepted fact and has been described by, among others, Sekine (1997) and Gildea (2001)", 
    "clean_text": "The genre dependency of parsers is an accepted fact and has been described by, among others, Sekine (1997) and Gildea (2001).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "D08-1091", 
    "citing_paper_authority": 11, 
    "citing_paper_authors": "Slav, Petrov | Dan, Klein", 
    "raw_text": "Note that the span features improve the performance of the unsplit baseline gram mar by 8%, but not surprisingly their contribution 6See Gildea (2001) for the exact setup.7This setup contains only sentences without annotation errors, as in (Arun and Keller, 2005)", 
    "clean_text": "See Gildea (2001) for the exact setup.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "W09-1008", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Marielu&iacute;sio, Sandra, Candito | Beno&icirc;t, Crabb&eacute; | Djam&eacute;, Seddah", 
    "raw_text": "For the lexicalized approach, the annotation of symbols with lexical head is known to be rarely fully used in practice (Gildea, 2001), what is really used being the category of the lexical head", 
    "clean_text": "For the lexicalized approach, the annotation of symbols with lexical head is known to be rarely fully used in practice (Gildea, 2001), what is really used being the category of the lexical head.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "W06-2902", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Ivan, Titov | James B., Henderson", 
    "raw_text": "For this reason we do not run experiments on the task considered in (Gildea, 2001) and (Roark and Bacchiani, 2003), where they are porting from the restricted domain of the WSJcorpus to the more varied domain of the Browncor pus as a whole", 
    "clean_text": "For this reason we do not run experiments on the task considered in (Gildea, 2001) and (Roark and Bacchiani, 2003), where they are porting from the restricted domain of the WSJ corpus to the more varied domain of the Brown corpus as a whole.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "W06-2902", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Ivan, Titov | James B., Henderson", 
    "raw_text": "In addition to portability experiments with the parsing model of (Collins, 1997), (Gildea, 2001) provided a comprehensive analysis of parser portability", 
    "clean_text": "In addition to portability experiments with the parsing model of (Collins, 1997), (Gildea, 2001) provided a comprehensive analysis of parser portability.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "W06-2902", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Ivan, Titov | James B., Henderson", 
    "raw_text": "Gildea (2001) only reports results on sentences of 40 or less words on all the Brown corpus sections combined, for which he reports 80.3% /81.0 %recall/precision when training only on data from the WSJ corpus, and 83.9% /84.8% when training on data from the WSJ corpus and all sections of the Brown corpus. (Roark and Bacchiani, 2003) performed experiments on supervised and unsupervised PCFGadaptation to the target domain", 
    "clean_text": "Gildea (2001) only reports results on sentences of 40 or less words on all the Brown corpus sections combined, for which he reports 80.3%/81.0% recall/precision when training only on data from the WSJ corpus, and 83.9%/84.8% when training on data from the WSJ corpus and all sections of the Brown corpus.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "W06-2902", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Ivan, Titov | James B., Henderson", 
    "raw_text": "They achieved very good improvement over their baseline and over (Gildea, 2001), but the absolute accuracies were still relatively low (as discussed above)", 
    "clean_text": "They achieved very good improvement over their baseline and over (Gildea, 2001), but the absolute accuracies were still relatively low (as discussed above).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "W10-2604", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Fei, Huang | Alexander, Yates", 
    "raw_text": "Past research in a variety of NLP tasks, like parsing (Gildea, 2001) and chunking (Huang and Yates, 2009), has shown that systems suffer from a drop-off in performance on out-of-domain tests", 
    "clean_text": "Past research in a variety of NLP tasks, like parsing (Gildea, 2001) and chunking (Huang and Yates, 2009), has shown that systems suffer from a drop-off in performance on out-of-domain tests.", 
    "keep_for_gold": 0
  }
]