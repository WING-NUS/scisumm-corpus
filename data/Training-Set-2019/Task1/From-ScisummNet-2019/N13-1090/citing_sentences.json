[
  {
    "citance_No": 1, 
    "citing_paper_id": "P13-1045", 
    "citing_paper_authority": 28, 
    "citing_paper_authors": "Richard, Socher | John, Bauer | Christopher D., Manning | Andrew Y., Ng", 
    "raw_text": "These vector representations capture interesting linear relationships (up to some accuracy), such as king ?man+woman? queen (Mikolov et al, 2013)", 
    "clean_text": "These vector representations capture interesting linear relationships (up to some accuracy), such as king-man+woman=queen (Mikolov et al, 2013).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P14-1129", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Jacob, Devlin | Rabih, Zbib | Zhongqiang, Huang | Thomas, Lamar | Richard M., Schwartz | John, Makhoul", 
    "raw_text": "How ever, not only are these techniques intractable to train with high-order context vectors, they also lack the neural network? s ability to semantically generalize (Mikolov et al, 2013) and learn non linear relationships. A number of recent papers have proposed methods for creating neural network translation/jointmodels, but nearly all of these works have obtained much smaller BLEU improvements than ours", 
    "clean_text": "However, not only are these techniques intractable to train with high-order context vectors, they also lack the neural network's ability to semantically generalize (Mikolov et al, 2013) and learn nonlinear relationships.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P14-1066", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Jianfeng, Gao | Xiaodong, He | Scott Wen-Tau, Yih | Li, Deng", 
    "raw_text": "Continuous space models have also been used for generating translations for new words (Mikolov et al 2013a) and ITG reordering (Li et al 2013)", 
    "clean_text": "Continuous space models have also been used for generating translations for new words (Mikolov et al 2013a) and ITG reordering (Li et al 2013).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "P14-1023", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Marco, Baroni | Georgiana, Dinu | Germ\u00c3\u00a1n, Kruszewski", 
    "raw_text": "Themcrae set (McRae et al, 1998) consists of 100 noun? verb pairs, with top performance reached by the DepDM system of Baroni and Lenci (2010), a count DSM relying on 241 syntactic information. Analogy While all the previous data sets are relatively standard in the DSM field to test traditional count models, our last benchmark was introduce din Mikolov et al (2013a) specifically to test predict models", 
    "clean_text": "While all the previous data sets are relatively standard in the DSM field to test traditional count models, our last benchmark was introduced in Mikolov et al (2013a) specifically to test predict models.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P14-1023", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Marco, Baroni | Georgiana, Dinu | Germ\u00c3\u00a1n, Kruszewski", 
    "raw_text": "Mikolov et al (2013a) reach top accuracy on the syntactic subset (an syn) with a CBOW predict model akin to ours (but trained on a corpus twice as large)", 
    "clean_text": "Mikolov et al (2013a) reach top accuracy on the syntactic subset (an syn) with a CBOW predict model akin to ours (but trained on a corpus twice as large).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "P14-1023", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Marco, Baroni | Georgiana, Dinu | Germ\u00c3\u00a1n, Kruszewski", 
    "raw_text": "Top accuracy on the entire data set (an) and on these mantic subset (ansem) was reached by Mikolov et al (2013c) using a skip-gram predict model", 
    "clean_text": "Top accuracy on the entire data set (an) and on the semantic subset (ansem) was reached by Mikolov et al (2013c) using a skip-gram predict model.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "P14-1023", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Marco, Baroni | Georgiana, Dinu | Germ\u00c3\u00a1n, Kruszewski", 
    "raw_text": "Note however that, because of the way the task is framed, performance also depends on the size of the vocabulary to be searched: Mikolov et al (2013a) pick the nearest neighbour among vectors for 1M words, Mikolov et al (2013c) among 700K words, and we among 300K words", 
    "clean_text": "Note however that, because of the way the task is framed, performance also depends on the size of the vocabulary to be searched: Mikolov et al (2013a) pick the nearest neighbour among vectors for 1M words, Mikolov et al (2013c) among 700K words, and we among 300K words.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "P14-1023", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Marco, Baroni | Georgiana, Dinu | Germ\u00c3\u00a1n, Kruszewski", 
    "raw_text": "A more cogent and interesting evaluation is reported in the third block of Table 2, where we see what happens if we use the single models with worst performance across tasks (recall from Section 2 above that, in any case, we are exploring a space of reasonable parameter settings, of the sort that an 242 name task measure source soa rg relatedness Pearson Rubenstein and Goodenough Hassan and Mihalcea (2011) (1965 )ws relatedness Spearman Finkelstein et al (2002) Halawi et al (2012 )wss relatedness Spearman Agirre et al (2009) Agirre et al (2009 )wsr relatedness Spearman Agirre et al (2009) Agirre et al (2009) men relatedness Spearman Bruni et al (2013) Bruni et al (2013 )toefl synonyms accuracy Landauer and Dumais Bullinaria and Levy (2012) (1997 )ap categorization purity Almuhareb (2006) Rothenh ?ausler and Sch ?utze (2009 )esslli categorization purity Baroni et al (2008) Katrenko and Adriaans (2008 )battig categorization purity Baroni et al (2010) Baroni and Lenci (2010) up sel pref Spearman Pad? o (2007) Herda ?gdelen and Baroni (2009 )mcraesel pref Spearman McRae et al (1998) Baroni and Lenci (2010) an analogy accuracy Mikolov et al (2013a) Mikolov et al (2013c) an syn analogy accuracy Mikolov et al (2013a) Mikolov et al (2013a )ansem analogy accuracy Mikolov et al (2013a) Mikolov et al (2013c) Table 1: Benchmarks used in experiments, with type of task, figure of merit (measure), original reference (source) and reference to current state-of-the-art system (soa)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "P14-1113", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Ruiji, Fu | Jiang, Guo | Bing, Qin | Wanxiang, Che | Haifeng, Wang | Ting, Liu", 
    "raw_text": "Word embeddings have been empirically shown to preserve linguistic regularities, such as the semantic relationship between words (Mikolov et al, 2013b)", 
    "clean_text": "Word embeddings have been empirically shown to preserve linguistic regularities, such as the semantic relationship between words (Mikolov et al, 2013b).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "P14-1113", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Ruiji, Fu | Jiang, Guo | Bing, Qin | Wanxiang, Che | Haifeng, Wang | Ting, Liu", 
    "raw_text": "More recently, Mikolov et al (2013a) propose two log-linear models, namely the Skip-gram and CBOW model, to efficiently induce word embeddings", 
    "clean_text": "More recently, Mikolov et al (2013a) propose two log-linear models, namely the Skip-gram and CBOW model, to efficiently induce word embeddings.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "P14-1113", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Ruiji, Fu | Jiang, Guo | Bing, Qin | Wanxiang, Che | Haifeng, Wang | Ting, Liu", 
    "raw_text": "Mikolov et al (2013b) observe that word embeddings preserve interesting linguistic regularities, capturing a considerable amount of syntac tic/semantic relations", 
    "clean_text": "Mikolov et al (2013b) observe that word embeddings preserve interesting linguistic regularities, capturing a considerable amount of syntactic/semantic relations.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "P14-1008", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Ran, Tian | Yusuke, Miyao | Takuya, Matsuzaki", 
    "raw_text": "The word vectors we use are from Mikolov et al (2013) 13 (Mikolov13), and additional results are also shown using Turian et al (2010) 14 (Turian10)", 
    "clean_text": "The word vectors we use are from Mikolov et al (2013) 13 (Mikolov13), and additional results are also shown using Turian et al (2010) 14 (Turian10).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "P14-1028", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Wenzhe, Pei | Tao, Ge | Baobao, Chang", 
    "raw_text": "Mikolov et al (2013b) show that pre-trained embedding scan capture interesting semantic and syntactic in formation such as king ?man+woman? queen on English data", 
    "clean_text": "Mikolov et al (2013b) show that pre-trained embeddings can capture interesting semantic and syntactic information such as king-man+woman=queen on English data.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "P14-2133", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Jacob, Andreas | Dan, Klein", 
    "raw_text": "These are precisely the kinds of distinctions between determiners that state-splitting in the Berkeley parser has shown to be useful (Petrov and Klein, 2007), and existing work (Mikolovetal., 2013b) has observed that such regular em bedding structure extends to many other parts of speech", 
    "clean_text": "These are precisely the kinds of distinctions between determiners that state-splitting in the Berkeley parser has shown to be useful (Petrov and Klein, 2007), and existing work (Mikolov et al., 2013b) has observed that such regular embedding structure extends to many other parts of speech.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "P14-2050", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Omer, Levy | Yoav, Goldberg", 
    "raw_text": "Among the state-of-the-art word embedding methods is the skip-gram with negative sampling model (SKIPGRAM), introduced by Mikolov et al (2013b) and implemented in the word2vec software", 
    "clean_text": "Among the state-of-the-art word embedding methods is the skip-gram with negative sampling model (SKIPGRAM), introduced by Mikolov et al (2013b) and implemented in the word2vec software.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "P14-2050", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Omer, Levy | Yoav, Goldberg", 
    "raw_text": "Our departure point is the skip-gram neural em bedding model introduced in (Mikolov et al,2013a) trained using the negative-sampling procedure presented in (Mikolov et al, 2013b)", 
    "clean_text": "Our departure point is the skip-gram neural embedding model introduced in (Mikolov et al, 2013a) trained using the negative-sampling procedure presented in (Mikolov et al, 2013b).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "P14-2050", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Omer, Levy | Yoav, Goldberg", 
    "raw_text": "We also tried using the sub sampling option (Mikolov et al, 2013b) with BOW contexts (not shown)", 
    "clean_text": "We also tried using the sub sampling option (Mikolov et al, 2013b) with BOW contexts (not shown).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "P14-2050", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Omer, Levy | Yoav, Goldberg", 
    "raw_text": "In particular, we found that DEPS perform dramatically worse than BOW contexts on analogy tasks as in (Mikolov et al, 2013c; Levy and Goldberg, 2014)", 
    "clean_text": "In particular, we found that DEPS perform dramatically worse than BOW contexts on analogy tasks as in (Mikolov et al, 2013c; Levy and Goldberg, 2014).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "P14-1132", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Angeliki, Lazaridou | Elia, Bruni | Marco, Baroni", 
    "raw_text": "Fur therm ore, their text-based vectors encode very rich information, such as~ king? ~man+ ~woman =~queen (Mikolov et al, 2013c)", 
    "clean_text": "Furthermore, their text-based vectors encode very rich information, such as king-man+woman=queen (Mikolov et al, 2013c).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "P14-1132", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Angeliki, Lazaridou | Elia, Bruni | Marco, Baroni", 
    "raw_text": "This method is similar to the one introduced by Mikolov et al (2013a) for estimating a translation matrix, only solved analytically", 
    "clean_text": "This method is similar to the one introduced by Mikolov et al (2013a) for estimating a translation matrix, only solved analytically.", 
    "keep_for_gold": 0
  }
]