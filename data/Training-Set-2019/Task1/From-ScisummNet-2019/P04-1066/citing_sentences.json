[
  {
    "citance_No": 1, 
    "citing_paper_id": "P13-1140", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Jiajun, Zhang | Chengqing, Zong", 
    "raw_text": "2 Following Moore (2004b), we use the threshold 10 on", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "N10-1014", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Adam, Pauls | Dan, Klein | David, Chiang | Kevin, Knight", 
    "raw_text": "(Moore, 2004) problem for rare words", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P11-2032", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Co&scedil;kun, Mermer | Murat, Sara&ccedil;lar", 
    "raw_text": "Problems with the standard EM estimation of IBM Model 1 was pointed out by Moore (2004 )anda number of heuristic changes to the estimation procedure, such as smoothing the parameter estimates, were shown to reduce the alignment error rate ,butthe effects on translation performance was not reported", 
    "clean_text": "Problems with the standard EM estimation of IBM Model 1 was pointed out by Moore (2004) and a number of heuristic changes to the estimation procedure, such as smoothing the parameter estimates, were shown to reduce the alignment error rate, but the effects on translation performance was not reported.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "I08-1033", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Ming-Hong, Bai | Keh-Jiann, Chen | Jason S., Chang", 
    "raw_text": "Finally, the IBM models (Moore, 2004) impose the limitation that each word in the target sentence can be generated by at most one word in the source sentence", 
    "clean_text": "Finally, the IBM models (Moore, 2004) impose the limitation that each word in the target sentence can be generated by at most one word in the source sentence.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P08-1112", 
    "citing_paper_authority": 14, 
    "citing_paper_authors": "Kuzman, Ganchev | Jo&atilde;o, Gra&ccedil;a | Ben, Taskar", 
    "raw_text": "phenomenon often occurs (Moore, 2004)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "D10-1058", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Shaojun, Zhao | Daniel, Gildea", 
    "raw_text": "Moore (2004) also suggested adding multiple empty words to the target sentence for IBM Model 1", 
    "clean_text": "Moore (2004) also suggested adding multiple empty words to the target sentence for IBM Model 1.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "N06-1014", 
    "citing_paper_authority": 108, 
    "citing_paper_authors": "Percy, Liang | Ben, Taskar | Dan, Klein", 
    "raw_text": "this example, COJO is a rare word that becomes a garbage collector (Moore, 2004) for the models in both directions", 
    "clean_text": "this example, COJO is a rare word that becomes a garbage collector (Moore, 2004) for the models in both directions.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "N07-2010", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Michael, Fleischman | Deb, Roy", 
    "raw_text": "Similar to Berger& amp; Lafferty (1999), the probability distribution of terms given a category is estimated using a normalized log-likelihood ratio (Moore, 2004), and query terms are sampled randomly from this distribution", 
    "clean_text": "Similar to Berger and Lafferty (1999), the probability distribution of terms given a category is estimated using a normalized log-likelihood ratio (Moore, 2004), and query terms are sampled randomly from this distribution.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "W10-4217", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Anja, Belz | Eric, Kow", 
    "raw_text": "Following Moore (2004a) rather than Munteanu& amp; Marcu, our current notion of co occurrence is that a data field and word co occur if they are present in the same pair of data fields and sentence (as identified by the method described in Section 4.1 above)", 
    "clean_text": "Following Moore (2004a) rather than Munteanu and Marcu, our current notion of co-occurrence is that a data field and word co-occur if they are present in the same pair of data fields and sentence (as identified by the method described in Section 4.1 above).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "W10-4217", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Anja, Belz | Eric, Kow", 
    "raw_text": "This allows us to compute the G2 score, for which we use the formulation from Moore (2004b) shown in Figure 2", 
    "clean_text": "This allows us to compute the G2 score, for which we use the formulation from Moore (2004b) shown in Figure 2.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "W10-4217", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Anja, Belz | Eric, Kow", 
    "raw_text": "2N (p (d, w) log p (d, w) p (d) p (w)+ p (d,? w) log p (d,? w) p (d) p (? w)+ p (? d, w) log p (? d, w) p (? d) p (w)+ p (? d,? w) log p (? d,? w) p (? d) p (? w)) Figure 2: Formula for computing G2 from Moore (2004b) (N is the sample size)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "D12-1116", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Amit, Singh", 
    "raw_text": "(Moore, 2004) translation limitation in the IBM model, due to which each word in the target document can be generated by at most one word in thequestion2", 
    "clean_text": "(Moore, 2004) translation limitation in the IBM model, due to which each word in the target document can be generated by at most one word in the question.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "D12-1116", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Amit, Singh", 
    "raw_text": "For the same reasons, it also alleviates another related limitation by enabling translation between contiguous words across the query and documents (Moore, 2004) .terms by proposing new ways of organizing monolingual parallel corpus and simultaneously leveraging external resources likeWikipedia from which one can derive these relationships reliably", 
    "clean_text": "For the same reasons, it also alleviates another related limitation by enabling translation between contiguous words across the query and documents (Moore, 2004).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "P06-1011", 
    "citing_paper_authority": 31, 
    "citing_paper_authors": "Dragos Stefan, Munteanu | Daniel, Marcu", 
    "raw_text": "Word Translation ProbabilitiesOur method for computing the probabilistic translation lexicon LLR-Lex is based on the the Log 2http: //www.fjoch.com/GIZA++.html Likelihood-Ratio (LLR) statistic (Dunning, 1993), which has also been used by Moore (2004a; 2004b) and Melamed (2000) as a measure of word association", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "P06-1011", 
    "citing_paper_authority": 31, 
    "citing_paper_authors": "Dragos Stefan, Munteanu | Daniel, Marcu", 
    "raw_text": "In the work of Moore (2004a) and Melamed (2000), two words co occur if they are present in a pair of aligned sentences in the parallel training corpus", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "P06-1011", 
    "citing_paper_authority": 31, 
    "citing_paper_authors": "Dragos Stefan, Munteanu | Daniel, Marcu", 
    "raw_text": "and \u0000, using the formula presented by Moore (2004b), which we do not repeat here due to lack of space", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "C10-1084", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Minh-Thang, Luong | Min-Yen, Kan", 
    "raw_text": "(Moore, 2004) that attract CCMs to align to them", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "W06-1607", 
    "citing_paper_authority": 25, 
    "citing_paper_authors": "George, Foster | Roland, Kuhn | Howard, Johnson", 
    "raw_text": "As also mentioned previously, there is relatively little published work on smoothing for statistical MT. For the IBM models, alignment probabilities need to be smoothed for combinations of sentence lengths and positions not encountered in training data (Garc? ?a-Varea et al, 1998) .Moore (2004) has found that smoothing to correct overestimated IBM1 lexical probabilities for rare words can improve word-alignment performance", 
    "clean_text": "Moore (2004) has found that smoothing to correct overestimated IBM1 lexical probabilities for rare words can improve word-alignment performance.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "P12-1033", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Ashish, Vaswani | Liang, Huang | David, Chiang", 
    "raw_text": "(Moore, 2004), aligning to many unrelated words", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "P12-1033", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Ashish, Vaswani | Liang, Huang | David, Chiang", 
    "raw_text": "Previous attempted remedies include early stopping, smoothing (Moore, 2004), and posterior regularization (Grac? a et al, 2010)", 
    "clean_text": "Previous attempted remedies include early stopping, smoothing (Moore, 2004), and posterior regularization (Graca et al, 2010).", 
    "keep_for_gold": 0
  }
]