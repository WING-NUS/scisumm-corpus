Citance Number: 1 | Reference Article:  J90-1003.txt | Citing Article:  A92-1013.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The results of these studies have important applications in lexicography, to detect lexico-syntactic regularities (Church and Hanks).</S> | Reference Offset:  ['0','168'] | Reference Text:  <S sid = 0 ssid = >Word Association Norms Mutual Information And Lexicography</S><S sid = 168 ssid = >It is hard to know what is important in such a concordance and what is not.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  J90-1003.txt | Citing Article:  A92-1013.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In (Church and Hanks, 1990) the significance of an association (x, y) is measured by the mutual information I (x, y) ,i.e. the probability of observing x and y together, compared with the probability of observing x and y independently.</S> | Reference Offset:  ['56','57'] | Reference Text:  <S sid = 56 ssid = >What is &quot;mutual information?&quot; According to Fano (1961), if two points (words), x and y, have probabilities P(x) and P(y), then their mutual information, I(x,y), is defined to be Informally, mutual information compares the probability of observing x and y together (the joint probability) with the probabilities of observing x and y independently (chance).</S><S sid = 57 ssid = >If there is a genuine association between x and y, then the joint probability P(x,y) will be much larger than chance P(x) P(y), and consequently I(x,y) » 0.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  J90-1003.txt | Citing Article:  P14-1045.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We also measured the syntagmatic association of neighbour a and neighbour b, with a mutual information measure (Church and Hanks, 1990), computed from the cooccurrence of two tokens within the same paragraph in Wikipedia.</S> | Reference Offset:  ['0','52'] | Reference Text:  <S sid = 0 ssid = >Word Association Norms Mutual Information And Lexicography</S><S sid = 52 ssid = >We propose an alternative measure, the association ratio, for measuring word association norms, based on the information theoretic concept of mutual information.'</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  J90-1003.txt | Citing Article:  S12-1094.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer et al, 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islamand Inkpen, 2006), Hyperspace Analogues to Language (Burgess et al, 1998) and distributional similarity (Lin, 1998) employ probabilistic approaches to decode the semantics of words.</S> | Reference Offset:  ['7','139'] | Reference Text:  <S sid = 7 ssid = >For example, Hindle (Church et al. 1989) has used a syntactic parser to select words in certain constructions of interest.</S><S sid = 139 ssid = >Hindle (Church et al. 1989) has found it helpful to preprocess the input with the Fidditch parser (Hindle 1983a, 1983b) to identify associations between verbs and arguments, and postulate semantic classes for nouns on this basis.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  J90-1003.txt | Citing Article:  P10-1135.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In this equation, pmi (i, p) is the pointwise mutual information score (Church and Hanks, 1990) between a pattern, p (e.g. consist-of), and a tuple, i (e.g. engine-car), and maxpmi is the maximum PMI score between all patterns and tuples.</S> | Reference Offset:  ['237','238'] | Reference Text:  <S sid = 237 ssid = >But on the other hand, the objective score can be misleading.</S><S sid = 238 ssid = >The score takes only distributional evidence into account.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  J90-1003.txt | Citing Article:  P06-1036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To this end we follow the method introduced by (Church and Hanks, 1990), i.e. by sliding a window of a given size over some texts.</S> | Reference Offset:  ['62','72'] | Reference Text:  <S sid = 62 ssid = >Joint probabilities, P(x,y), are estimated by counting the number of times that xis followed by y in a window of w words,f,(x,y), and normalizing by N. The window size parameter allows us to look at different scales.</S><S sid = 72 ssid = >The ideal window size is different in each case.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  J90-1003.txt | Citing Article:  P06-1036.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Like (Church and Hanks, 1990), we used mutual information to measure the cohesion between two words.</S> | Reference Offset:  ['52','191'] | Reference Text:  <S sid = 52 ssid = >We propose an alternative measure, the association ratio, for measuring word association norms, based on the information theoretic concept of mutual information.'</S><S sid = 191 ssid = >. from, and having noted that the two words are rarely Computational Linguistics Volume 16, Number 1, March 1990 27 Kenneth Church and Patrick Hanks Word Association Norms, Mutual Information, and Lexicography adjacent, we would now like to speed up the labor-intensive task of categorizing the concordance lines.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  J90-1003.txt | Citing Article:  P14-2030.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We approached this task by selecting target roles from the first experiment and ranking characteristic attributes for each using point wise mutual information (PMI) (Church and Hanks, 1990).</S> | Reference Offset:  ['0','191'] | Reference Text:  <S sid = 0 ssid = >Word Association Norms Mutual Information And Lexicography</S><S sid = 191 ssid = >. from, and having noted that the two words are rarely Computational Linguistics Volume 16, Number 1, March 1990 27 Kenneth Church and Patrick Hanks Word Association Norms, Mutual Information, and Lexicography adjacent, we would now like to speed up the labor-intensive task of categorizing the concordance lines.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  J90-1003.txt | Citing Article:  E95-1037.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Collocation has been applied successfully to many possible applications (Church et al, 1989).</S> | Reference Offset:  ['7','139'] | Reference Text:  <S sid = 7 ssid = >For example, Hindle (Church et al. 1989) has used a syntactic parser to select words in certain constructions of interest.</S><S sid = 139 ssid = >Hindle (Church et al. 1989) has found it helpful to preprocess the input with the Fidditch parser (Hindle 1983a, 1983b) to identify associations between verbs and arguments, and postulate semantic classes for nouns on this basis.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  J90-1003.txt | Citing Article:  C02-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Collocations were extracted according to the method described in (Church and Hanks, 1990) by moving a window on texts.</S> | Reference Offset:  ['4','62'] | Reference Text:  <S sid = 4 ssid = >This definition y) a rectangular window.</S><S sid = 62 ssid = >Joint probabilities, P(x,y), are estimated by counting the number of times that xis followed by y in a window of w words,f,(x,y), and normalizing by N. The window size parameter allows us to look at different scales.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  J90-1003.txt | Citing Article:  P11-1129.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use pointwise mutual information (PMI) (Church and Hanks, 1990) to measure the strength of the association between x and y, which is defined as follows PMI (x, y)= log (P (x, y) P (x) P (y)).</S> | Reference Offset:  ['0','52'] | Reference Text:  <S sid = 0 ssid = >Word Association Norms Mutual Information And Lexicography</S><S sid = 52 ssid = >We propose an alternative measure, the association ratio, for measuring word association norms, based on the information theoretic concept of mutual information.'</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  J90-1003.txt | Citing Article:  P06-2033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The information content of this set is defined as mutual information I (F (w)) (Church and Hanks, 1990).</S> | Reference Offset:  ['56','126'] | Reference Text:  <S sid = 56 ssid = >What is &quot;mutual information?&quot; According to Fano (1961), if two points (words), x and y, have probabilities P(x) and P(y), then their mutual information, I(x,y), is defined to be Informally, mutual information compares the probability of observing x and y together (the joint probability) with the probabilities of observing x and y independently (chance).</S><S sid = 126 ssid = >Given these estimates, we would compute the mutual information to be I(set; off) 6.2.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  J90-1003.txt | Citing Article:  P13-1174.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >PMI scores have been widely used in previous studies to measure association between words (Church and Hanks (1990)).</S> | Reference Offset:  ['7','85'] | Reference Text:  <S sid = 7 ssid = >For example, Hindle (Church et al. 1989) has used a syntactic parser to select words in certain constructions of interest.</S><S sid = 85 ssid = >This problem can be fixed by dividing f (x, y) by w — 1 (which has the consequence of subtracting log2 (w — 1) = 2 from our association ratio scores).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  J90-1003.txt | Citing Article:  P09-1072.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Computational linguists have demonstrated that a word's meaning is captured to some extent by the distribution of words and phrases with which it commonly co-occurs (Church and Hanks, 1990).</S> | Reference Offset:  ['59','105'] | Reference Text:  <S sid = 59 ssid = >If x and y are in complementary distribution, then P(x,y) will be much less than P(x) P(y), forcing I(x,y) « 0.</S><S sid = 105 ssid = >Both are frequent words [set occurs approximately 250 times in a million words and off occurs approximately 556 times in a million words .</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  J90-1003.txt | Citing Article:  W10-2007.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Following a very long tradition in computational linguistics (Church and Hanks, 1990), we use cooccurrence statistics for words in certain contexts to hypothesise a meaningful connection between the words.</S> | Reference Offset:  ['147','232'] | Reference Text:  <S sid = 147 ssid = >(drink IV and beer I 0 are found in 660 and Computational Linguistics Volume 16, Number 1, March 1990 25 Kenneth Church and Patrick Hanks Word Association Norms, Mutual Information, and Lexicography readers, which introduced an element of selectivity and so inevitably distortion (rare words and uses were collected but common uses of common words were not), or on small corpora of only a million words or so, which are reliably informative for only the most common uses of the few most frequent words of English.</S><S sid = 232 ssid = >In other words, they use two words to triangulate in on a word sense.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  J90-1003.txt | Citing Article:  D11-1077.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Early approaches to MWEs identification concentrated on their collocational behavior (Church and Hanks, 1990).</S> | Reference Offset:  ['191','198'] | Reference Text:  <S sid = 191 ssid = >. from, and having noted that the two words are rarely Computational Linguistics Volume 16, Number 1, March 1990 27 Kenneth Church and Patrick Hanks Word Association Norms, Mutual Information, and Lexicography adjacent, we would now like to speed up the labor-intensive task of categorizing the concordance lines.</S><S sid = 198 ssid = >Lexicographers have tended to use concordances impressionistically; semantic theorists, AI-ers, and others have concentrated on a few interesting examples, e.g. bachelor, and have not given much thought to how the results might be scaled up.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  J90-1003.txt | Citing Article:  N12-1056.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Church and Hanks (1990) suggested pointwise mutual information: PMI (wi ,wj)= log Pr (wi ,wj) Pr (wi) Pr (wj), showing linguistically appealing results using contexts defined by fixed width n-gram windows, and syntactic dependencies derived from automatically parsed corpora.</S> | Reference Offset:  ['6','56'] | Reference Text:  <S sid = 6 ssid = >Other windows are also possible.</S><S sid = 56 ssid = >What is &quot;mutual information?&quot; According to Fano (1961), if two points (words), x and y, have probabilities P(x) and P(y), then their mutual information, I(x,y), is defined to be Informally, mutual information compares the probability of observing x and y together (the joint probability) with the probabilities of observing x and y independently (chance).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  J90-1003.txt | Citing Article:  S12-1047.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Therefore, we propose a second baseline where pairs are rated according to their Pointwise Mutual Information (PMI) (Church and Hanks, 1990), which measures the statistical association between two words.</S> | Reference Offset:  ['25','52'] | Reference Text:  <S sid = 25 ssid = >This paper will propose an objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora.</S><S sid = 52 ssid = >We propose an alternative measure, the association ratio, for measuring word association norms, based on the information theoretic concept of mutual information.'</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  J90-1003.txt | Citing Article:  P98-1100.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Collocation: Collocations were extracted from a seven million word sample of the Longman English Language Corpus using the association ratio (Church and Hanks, 1990) and outputted to a lexicon.</S> | Reference Offset:  ['129','186'] | Reference Text:  <S sid = 129 ssid = >Sinclair's corpus is a fairly balanced sample of (mainly British) text; the AP corpus is an unbalanced sample of American journalese.</S><S sid = 186 ssid = >Of the 27 bare verbs (tagged `vb') in the list above, all but seven are listed in Collins Cobuild English Language Dictionary as occurring with from.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  J90-1003.txt | Citing Article:  W03-1805.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Word collocation Various collocation metrics have been proposed, including mean and variance (Smadja, 1994), the t-test (Church et al, 1991), the chi-square test, point wise mutual information (MI) (Church and Hanks, 1990), and binomial log likelihood ratio test (BLRT) (Dunning, 1993).</S> | Reference Offset:  ['66','219'] | Reference Text:  <S sid = 66 ssid = >That is, the mean separation is two, and the variance is zero.</S><S sid = 219 ssid = >A future step would be to examine other more balanced corpora and test how well the patterns hold up.</S> | Discourse Facet:  NA | Annotator: Automatic


