Citance Number: 1 | Reference Article:  N07-1047.txt | Citing Article:  P08-1065.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To explore this, we tested our model in conjunction with a recent L2P system that has been shown to predict phonemes with state-of-the-art word accuracy (Jiampojamarn et al, 2007).</S> | Reference Offset:  ['5','7'] | Reference Text:  <S sid = 5 ssid = >We also apply an HMM method in conjunction with a local classification model to predict a global phoneme sequence given a word.</S><S sid = 7 ssid = >Our system achieves state-of-the-art performance on several languages and data sets.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  N07-1047.txt | Citing Article:  D08-1113.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We would like to use features that look at wide context on the input side, which is inexpensive (Jiampojamarn et al, 2007).</S> | Reference Offset:  ['121','166'] | Reference Text:  <S sid = 121 ssid = >Our approach is to use an instance-based learning technique as a local predictor to generate a set of phoneme candidates for each letter chunk, given its context in a word.</S><S sid = 166 ssid = >We plan to explore other sequence prediction approaches, such as discriminative training methods (Collins, 2004), and sequence tagging with Support Vector Machines (SVM-HMM) (Altun et al., 2003) to incorporate more features (context information) into the phoneme generation model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  N07-1047.txt | Citing Article:  N10-1103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our system employs the many-to-many alignment described in (Jiampojamarn et al, 2007).</S> | Reference Offset:  ['21','139'] | Reference Text:  <S sid = 21 ssid = >Previous work has generally assumed one-to-one alignment for simplicity (Daelemans and Bosch, 1997; Black et al., 1998; Damper et al., 2005).</S><S sid = 139 ssid = >We show results comparing the one-to-one aligner described in Section 2.1 and the one-to-one aligner provided by the PRONALSYL challenge.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  N07-1047.txt | Citing Article:  P11-2013.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We use the many-to-many letter phoneme alignment algorithm (Jiampojamarn et al, 2007) to map each letter to multiple phonemes (1-to-2 alignment).</S> | Reference Offset:  ['37','146'] | Reference Text:  <S sid = 37 ssid = >We describe the letter-phoneme alignment methods including a standard one-to-one alignment method and our many-to-many approach in Section 2.</S><S sid = 146 ssid = >The alignment provided by the PRONALSYS one-to-one alignments is: e f a n Clearly, the latter alignment provides more information on how the graphemes map to the phonemes.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  N07-1047.txt | Citing Article:  W10-2405.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The M2M-aligner (Jiampojamarn et al, 2007) is based on the expectation maximization (EM) algorithm.</S> | Reference Offset:  ['22','68'] | Reference Text:  <S sid = 22 ssid = >An expectation maximization (EM) based algorithm (Dempster et al., 1977) is applied to train the aligners.</S><S sid = 68 ssid = >For each grapheme-/phoneme-sequence pair (x, y), the EM-many2many function (Algorithm 1) calls the Expectation-many2many function (Algorithm 2) to collect partial counts.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  N07-1047.txt | Citing Article:  W10-2405.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Our input string representation for a candidate pair is formed by first aligning the source and target words using M2M-aligner (Jiampojamarn et al., 2007).</S> | Reference Offset:  ['127','132'] | Reference Text:  <S sid = 127 ssid = >Since the candidate set is from the classifier, the search space is limited to a small number of candidate phonemes (1 to 5 phonemes in most cases).</S><S sid = 132 ssid = >For the English Celex data, we removed duplicate words as well as words shorter than four letters.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  N07-1047.txt | Citing Article:  W12-4411.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >DIRECTL+ (Jiampojamarn et al, 2010a) is an online discriminative training system that incorporates joint n-gram features and many-to-many alignments, which are generated by M2M-ALIGNER (Jiampojamarn et al, 2007).</S> | Reference Offset:  ['21','166'] | Reference Text:  <S sid = 21 ssid = >Previous work has generally assumed one-to-one alignment for simplicity (Daelemans and Bosch, 1997; Black et al., 1998; Damper et al., 2005).</S><S sid = 166 ssid = >We plan to explore other sequence prediction approaches, such as discriminative training methods (Collins, 2004), and sequence tagging with Support Vector Machines (SVM-HMM) (Altun et al., 2003) to incorporate more features (context information) into the phoneme generation model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  N07-1047.txt | Citing Article:  P10-1080.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Advanced L2P approaches, including the joint n-gram models (Bisani and Ney, 2008) and the joint discriminative approach (Jiampojamarn et al., 2007) eliminate the one-to-one constraint entirely, allowing for linking of multiple letters to multiple phonemes.</S> | Reference Offset:  ['37','73'] | Reference Text:  <S sid = 37 ssid = >We describe the letter-phoneme alignment methods including a standard one-to-one alignment method and our many-to-many approach in Section 2.</S><S sid = 73 ssid = >Normalization can be done over the whole table to create a joint distribution or per grapheme to create a conditional distribution.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  N07-1047.txt | Citing Article:  P10-1080.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >M2M-aligner (Jiampojamarn et al, 2007) is a many-to-many (M-M) alignment algorithm based on EM that allows for mapping of multiple letters to multiple phonemes.</S> | Reference Offset:  ['22','58'] | Reference Text:  <S sid = 22 ssid = >An expectation maximization (EM) based algorithm (Dempster et al., 1977) is applied to train the aligners.</S><S sid = 58 ssid = >The method applies the EM algorithm to estimate the probability of mapping a letter l to a phoneme p, P(l, p).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  N07-1047.txt | Citing Article:  E12-1015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >Extensions to this model are possible, for example the use of many-to-many alignments which have been shown to be very effective in letter-to-phoneme alignment tasks (Jiampojamarn et al 2007).</S> | Reference Offset:  ['28','141'] | Reference Text:  <S sid = 28 ssid = >Once we have our many-to-many alignments, we use that data to train a prediction model.</S><S sid = 141 ssid = >For both alignments, we use instancebased learning as the prediction model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  N07-1047.txt | Citing Article:  W09-2010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In our lexicon, the graphemes and phonemes of each word are aligned according to the method of Jiampojamarn et al (2007).</S> | Reference Offset:  ['82','87'] | Reference Text:  <S sid = 82 ssid = >Given a set of words and their phonemes, alignments are made across graphemes and phonemes.</S><S sid = 87 ssid = >Once many-to-many alignments are built across graphemes and phonemes, each word contains a set of letter chunks, each consisting of one or two letters aligned with phonemes.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  N07-1047.txt | Citing Article:  W09-2010.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >P(ht|ps,pos) is estimated from the frequency of ps, and its alignment with ht, in a version of CELEX in which the graphemic and phonemic representation of each word is many-many aligned using the method of Jiampojamarn et al (2007).</S> | Reference Offset:  ['37','153'] | Reference Text:  <S sid = 37 ssid = >We describe the letter-phoneme alignment methods including a standard one-to-one alignment method and our many-to-many approach in Section 2.</S><S sid = 153 ssid = >The results of our best system, which combines the HMM method with the many-to-many alignments (M-M+HMM), are better than the results reported in (Black et al., 1998) on both the CMUDict and German Celex data sets.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  N07-1047.txt | Citing Article:  W12-4410.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In order to extract source-target character mappings, we use m2m-aligner (Jiampojamarn et al, 2007), which implements a forward-backward algorithm to sum over probabilities of possible character sequence mappings, and uses Expectation Maximization to learn mapping probabilities.</S> | Reference Offset:  ['67','78'] | Reference Text:  <S sid = 67 ssid = >Partial counts are counts of all possible mappings from letters to phonemes that are collected in the -y table, while mapping probabilities (initially uniform) are maintained in the S table.</S><S sid = 78 ssid = >Expectation-many2many first calls the two functions to fill the Î± and Q tables, and then uses the probabilities to calculate partial counts for every possible mapping in the sequence pair.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  N07-1047.txt | Citing Article:  W12-4409.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >One of the most popular alignment tools is m2maligner (Jiampojamarn et al, 2007), which is released as an open source software.</S> | Reference Offset:  ['21','134'] | Reference Text:  <S sid = 21 ssid = >Previous work has generally assumed one-to-one alignment for simplicity (Daelemans and Bosch, 1997; Black et al., 1998; Damper et al., 2005).</S><S sid = 134 ssid = >For all of our experiments, our local classifier for predicting phonemes is the instance-based learning IB1 algorithm (Aha et al., 1991) implemented in the TiMBL package (Daelemans et al., 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  N07-1047.txt | Citing Article:  D11-1089.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To establish the substring alignment between katakana and Latin alphabet strings, we use the probabilistic model proposed by (Jiampojamarn et al., 2007).</S> | Reference Offset:  ['28','141'] | Reference Text:  <S sid = 28 ssid = >Once we have our many-to-many alignments, we use that data to train a prediction model.</S><S sid = 141 ssid = >For both alignments, we use instancebased learning as the prediction model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  N07-1047.txt | Citing Article:  W12-4412.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >To balance cost and benefit for English-to-Chinese (E2C) transliteration, this work compares the one-stage method with the two-stage one, using additional features of AV (Feng et al, 2004) and M2M-aligner as an initial alignment (Jiampojamarn et al, 2007), to explore where the best investment reward is.</S> | Reference Offset:  ['21','134'] | Reference Text:  <S sid = 21 ssid = >Previous work has generally assumed one-to-one alignment for simplicity (Daelemans and Bosch, 1997; Black et al., 1998; Damper et al., 2005).</S><S sid = 134 ssid = >For all of our experiments, our local classifier for predicting phonemes is the instance-based learning IB1 algorithm (Aha et al., 1991) implemented in the TiMBL package (Daelemans et al., 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  N07-1047.txt | Citing Article:  N12-1044.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >For the alignment of supplemental data with candidate outputs, we apply M2M ALIGNER (Jiampojamarn et al, 2007).</S> | Reference Offset:  ['21','127'] | Reference Text:  <S sid = 21 ssid = >Previous work has generally assumed one-to-one alignment for simplicity (Daelemans and Bosch, 1997; Black et al., 1998; Damper et al., 2005).</S><S sid = 127 ssid = >Since the candidate set is from the classifier, the search space is limited to a small number of candidate phonemes (1 to 5 phonemes in most cases).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  N07-1047.txt | Citing Article:  P08-1103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >The second hybrid approach (Jiampojamarn et al, 2007) also extends instance-based classification.</S> | Reference Offset:  ['96','134'] | Reference Text:  <S sid = 96 ssid = >In our system, a bigram letter chunking prediction automatically discovers double letters based on instance-based learning (Aha et al., 1991).</S><S sid = 134 ssid = >For all of our experiments, our local classifier for predicting phonemes is the instance-based learning IB1 algorithm (Aha et al., 1991) implemented in the TiMBL package (Daelemans et al., 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  N07-1047.txt | Citing Article:  P08-1103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >In the pipeline approach (Figure 1b), the input word is segmented into letter substrings by an instance-based classifier (Aha et al, 1991), which learns a letter segmentation model from many-to-many alignments (Jiampojamarn et al, 2007).</S> | Reference Offset:  ['96','134'] | Reference Text:  <S sid = 96 ssid = >In our system, a bigram letter chunking prediction automatically discovers double letters based on instance-based learning (Aha et al., 1991).</S><S sid = 134 ssid = >For all of our experiments, our local classifier for predicting phonemes is the instance-based learning IB1 algorithm (Aha et al., 1991) implemented in the TiMBL package (Daelemans et al., 2004).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  N07-1047.txt | Citing Article:  P08-1103.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset: NA | Citation Text:  <S sid =  ssid = >We ignored one-to-one alignments included in the PRONALSYL data sets, and instead induced many-to-many alignments using the method of Jiampojamarn et al (2007).</S> | Reference Offset:  ['142','160'] | Reference Text:  <S sid = 142 ssid = >Overall, our one-to-one alignments outperform the alignments provided by the data sets for all corpora.</S><S sid = 160 ssid = >On several languages and data sets, using the many-to-many alignments, word accuracy improvements ranged from 2.7% to 7.6%, as compared to one-to-one alignments.</S> | Discourse Facet:  NA | Annotator: Automatic


