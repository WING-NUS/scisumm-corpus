Citance Number: 1 | Reference Article:  D09-1058.txt | Citing Article:  D10-1069.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Finally, Suzuki et al (2009) present a very effective semi-supervised approach in which features from multiple generative models estimated on unlabeled data are combined in a discriminative system for structured prediction.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In this framework, a structured conditional model is constructed by incorporating a series of generative models, whose parameters are estimated from unlabeled data.</S><S sid = NA ssid = NA>An Empirical Study of Semi-supervised Structured Conditional Models for Dependency Parsing</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  D09-1058.txt | Citing Article:  P14-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The main choice in the approach is the partitioning of f(x, y) into components r1(x, y) ... rk(x, y), which in our experience is straightforward.</S><S sid = NA ssid = NA>5 can be defined as follows.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  D09-1058.txt | Citing Article:  P14-1043.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Both Suzuki et al (2009) and Chen et al (2013) adopt the higher order parsing model of Carreras (2007), and Suzuki et al (2009) also incorporate word cluster features proposed by Koo et al (2008) in their system.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In addition, we have described extensions that incorporate the cluster-based features of Koo et al. (2008), and that allow the use of second-order parsing models.</S><S sid = NA ssid = NA>These settings match the evaluation setting in previous work such as (McDonald et al., 2005a; Koo et al., 2008).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  D09-1058.txt | Citing Article:  D11-1116.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Also, some work has incorporated unsupervised word clusters as features, including that of Koo et al (2008) and Suzuki et al (2009), who utilized unsupervised word clusters created using the Brown et al (1992) hierarchical clustering algorithm.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>First, hierarchical word clusters are derived from unlabeled data using the Brown et al. clustering algorithm (Brown et al., 1992).</S><S sid = NA ssid = NA>These settings match the evaluation setting in previous work such as (McDonald et al., 2005a; Koo et al., 2008).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  D09-1058.txt | Citing Article:  N10-1116.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Although large amounts of unlabeled data are known to improve semi-supervised parsing (Suzuki et al, 2009), the best unsupervised systems use less data than is available for supervised training, relying on complex models instead: Headden et al's (2009) Extended Valence Grammar (EVG) combats data sparsity with smoothing alone, training on the same small subset of the tree-bank as the classic implementation of the DMV; Cohen and Smith (2009) use more complicated algorithms (variational EM and MBRdecoding) and stronger linguistic hints (tying related parts of speech and syntactically similar bilingual data).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>It is often straightforward to obtain large amounts of unlabeled data, making semi-supervised approaches appealing; previous work on semisupervised methods for dependency parsing includes (Smith and Eisner, 2007; Koo et al., 2008; Wang et al., 2008).</S><S sid = NA ssid = NA>These data sets are identical to the unlabeled data used in (Koo et al., 2008), and are disjoint from the training, development and test sets.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  D09-1058.txt | Citing Article:  P11-2112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We prepared a total of 3.72 billion token text data as unsupervised data following the instructions given in (Suzuki et al, 2009).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In addition, Table 4 shows the performance of our proposed method using 3.72 billion tokens of unlabeled data.</S><S sid = NA ssid = NA>Note that the English dependency parsing results shown in the table were achieved using 3.72 billion tokens of unlabeled data.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  D09-1058.txt | Citing Article:  P11-2112.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The main choice in the approach is the partitioning of f(x, y) into components r1(x, y) ... rk(x, y), which in our experience is straightforward.</S><S sid = NA ssid = NA>5 can be defined as follows.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  D09-1058.txt | Citing Article:  D11-1137.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The main choice in the approach is the partitioning of f(x, y) into components r1(x, y) ... rk(x, y), which in our experience is straightforward.</S><S sid = NA ssid = NA>5 can be defined as follows.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  D09-1058.txt | Citing Article:  D11-1137.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The main choice in the approach is the partitioning of f(x, y) into components r1(x, y) ... rk(x, y), which in our experience is straightforward.</S><S sid = NA ssid = NA>5 can be defined as follows.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  D09-1058.txt | Citing Article:  C10-2015.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The main choice in the approach is the partitioning of f(x, y) into components r1(x, y) ... rk(x, y), which in our experience is straightforward.</S><S sid = NA ssid = NA>5 can be defined as follows.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  D09-1058.txt | Citing Article:  P10-1040.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Semi-supervised models such as Ando and Zhang (2005), Suzuki and Isozaki (2008), and Suzuki et al (2009) achieve state-of-the-art accuracy.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Note that there are some similarities between our two-stage semi-supervised learning approach and the semi-supervised learning method introduced by (Blitzer et al., 2006), which is an extension of the method described by (Ando and Zhang, 2005).</S><S sid = NA ssid = NA>This paper has described an extension of the semi-supervised learning approach of (Suzuki and Isozaki, 2008) to the dependency parsing problem.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  D09-1058.txt | Citing Article:  E12-1009.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The main choice in the approach is the partitioning of f(x, y) into components r1(x, y) ... rk(x, y), which in our experience is straightforward.</S><S sid = NA ssid = NA>5 can be defined as follows.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  D09-1058.txt | Citing Article:  W12-3412.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Our system also compares favourably with the system of Carreras et al (2008) that relies on a more complex generative model, namely Tree Adjoining Grammars, and the system of Suzuki et al (2009) that makes use of external data (unannotated text).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>These settings match the evaluation setting in previous work such as (McDonald et al., 2005a; Koo et al., 2008).</S><S sid = NA ssid = NA>To facilitate comparisons with previous work, we used exactly the same training, development and test sets as those described in (McDonald et al., 2005a; McDonald et al., 2005b; McDonald and Pereira, 2006; Koo et al., 2008).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  D09-1058.txt | Citing Article:  P13-1104.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The main choice in the approach is the partitioning of f(x, y) into components r1(x, y) ... rk(x, y), which in our experience is straightforward.</S><S sid = NA ssid = NA>5 can be defined as follows.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  D09-1058.txt | Citing Article:  P12-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Suzuki 2009 (Suzuki et al, 2009) reported the best reported result by combining a Semi supervised Structured Conditional Model (Suzuki and Isozaki, 2008) with the method of (Koo et al, 2008).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The first extension is to combine our method with the cluster-based semi-supervised method of (Koo et al., 2008).</S><S sid = NA ssid = NA>This paper has described an extension of the semi-supervised learning approach of (Suzuki and Isozaki, 2008) to the dependency parsing problem.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  D09-1058.txt | Citing Article:  P12-1023.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Suzuki et al (2009) presented a semi supervised learning approach.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Note that there are some similarities between our two-stage semi-supervised learning approach and the semi-supervised learning method introduced by (Blitzer et al., 2006), which is an extension of the method described by (Ando and Zhang, 2005).</S><S sid = NA ssid = NA>This paper has described an extension of the semi-supervised learning approach of (Suzuki and Isozaki, 2008) to the dependency parsing problem.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  D09-1058.txt | Citing Article:  D12-1133.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The main choice in the approach is the partitioning of f(x, y) into components r1(x, y) ... rk(x, y), which in our experience is straightforward.</S><S sid = NA ssid = NA>5 can be defined as follows.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  D09-1058.txt | Citing Article:  P11-2123.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Suzuki et al (2009) also experiment with the same method combined with semi-supervised learning.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Note that there are some similarities between our two-stage semi-supervised learning approach and the semi-supervised learning method introduced by (Blitzer et al., 2006), which is an extension of the method described by (Ando and Zhang, 2005).</S><S sid = NA ssid = NA>The first extension is to combine our method with the cluster-based semi-supervised method of (Koo et al., 2008).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  D09-1058.txt | Citing Article:  P14-2106.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The main choice in the approach is the partitioning of f(x, y) into components r1(x, y) ... rk(x, y), which in our experience is straightforward.</S><S sid = NA ssid = NA>5 can be defined as follows.</S> | Discourse Facet:  NA | Annotator: Automatic


