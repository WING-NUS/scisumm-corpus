Batch Tuning Strategies for Statistical Machine Translation
There has been a proliferation of recent work on SMT tuning algorithms capable of handling larger feature sets than the traditional MERT approach.
We analyze a number of these algorithms in terms of their sentence-level loss functions, which motivates several new approaches, including a Structured SVM.
We perform empirical comparisons of eight different tuning strategies, including MERT, in a variety of settings.
Among other results, we find that a simple and efficient batch version of MIRA performs at least as well as training online, and consistently outperforms other options.
