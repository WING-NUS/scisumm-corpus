A comparison of Bayesian estimators for unsupervised Hidden Markov Model POS taggers
There is growing interest in applying Bayesian techniques to NLP problems.
There are a number of different estimators for Bayesian models, and it is useful to know what kinds of tasks each does well on.
This paper compares a variety of different Bayesian estimators for Hidden Markov Model POS taggers with various numbers of hidden states on data sets of different sizes.
Recent papers have given contradictory results when comparing Bayesian estimators to Expectation Maximization (EM) for unsupervised HMM POS tagging, and we show that the difference in reported results is largely due to differences in the size of the training data and the number of states in the HMM.
We investigate a variety of samplers for HMMs, including some that these earlier papers did not study.
We find that all of Gibbs samplers do well with small data sets and few states, and that Variational Bayes does well on large data sets and is competitive with the Gibbs samplers.
In terms of times of convergence, we find that Variational Bayes was the fastest of all the estimators, especially on large data sets, and that explicit Gibbs sampler (both pointwise and sentence-blocked) were generally faster than their collapsed counterparts on large data sets.
We consider three evaluation criteria: M-to-1 and 1-to-1 are the tagging accuracies under the best many-to-one map and the greedy one-to-one map respectively; VI is a map-free information theoretic criterion.
We induce a many-to-one mapping of state identifiers to PoS tags from one half of the corpus and evaluate on the second half, which is referred to as cross-validation accuracy.
We show that sparse priors can gain 4% (.62 to .66 with a 1M word corpus) in cross-validated many to-one accuracy.
