Citance Number: 1 | Reference Article:  J92-4003.txt | Citing Article:  P95-1025.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>On the other hand, the thesaurus-based method of Yarowsky (1992) may suffer from loss of information (since it is semi-class-based) as well as data sparseness since H Classes used in Resnik (1992) are based on the WordNet taxonomy while classes of Brown et al (1992) and Pereira et al (1993) are derived from statistical data collected from corpora.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We used these classes to construct an interpolated 3-gram class model using the same training text and held-out data as we used for the word-based language model we discussed above.</S><S sid = NA ssid = NA>We then interpolated the class-based estimators with the word-based estimators and found the perplexity of the test data to be 236, which is a small improvement over the perplexity of 244 we obtained with the word-based model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 2 | Reference Article:  J92-4003.txt | Citing Article:  P14-1100.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>For all languages we use Brown clustering (Brown et al, 1992) to construct a log (C)+ C feature vector where the first log (C) elements indicate which mer gable cluster the word belongs to, and the last C elements indicate the cluster identity.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>If we continue the algorithm for V - 1 merges, then we will have a single cluster which, of course, will be the entire vocabulary.</S><S sid = NA ssid = NA>Let and let The average mutual information remaining after V — k merges is We use the notation i+ j to represent the cluster obtained by merging Ck(i) and Ck(i)• If we know Ik.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 3 | Reference Article:  J92-4003.txt | Citing Article:  C10-2013.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We use the word clusters computed by Candito and Crabbe (2009) using Percy Liang's implementation of the Brown unsupervised clustering algorithm (Brown et al, 1992).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>To make even this suboptimal algorithm practical one must exercise a certain care in implementation.</S><S sid = NA ssid = NA>Although we have described this algorithm as one for finding clusters, we actually determine much more.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 4 | Reference Article:  J92-4003.txt | Citing Article:  W01-1004.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The authors would like to thank John Lafferty for his assistance in constructing word classes described in this paper.</S><S sid = NA ssid = NA>Of the 6.799 x 1010 2-grams that might have occurred in the data, only 14,494,217 actually did occur and of these, 8,045,024 occurred only once each.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 5 | Reference Article:  J92-4003.txt | Citing Article:  P13-1041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>One of the obvious syntagmas is words, and words are grouped into equivalence classes or clusters, thus reducing the model parameters of a statistical NLP system (Brown et al, 1992).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words.</S><S sid = NA ssid = NA>We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 6 | Reference Article:  J92-4003.txt | Citing Article:  P13-1041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Agglomerative clustering algorithm by Brown et al (1992) is used for this purpose.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We have used this algorithm to divide the 260,741-word vocabulary of Table 1 into 1,000 classes.</S><S sid = NA ssid = NA>We used these classes to construct an interpolated 3-gram class model using the same training text and held-out data as we used for the word-based language model we discussed above.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 7 | Reference Article:  J92-4003.txt | Citing Article:  P13-1041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Formally, as mentioned in Brown et al (1992), let C be a hard clustering function which maps vocabulary V to one of the K clusters.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Suppose that we partition a vocabulary of V words into C classes using a function, 7r, which maps a word, wi, into its class, ci.</S><S sid = NA ssid = NA>Let and let The average mutual information remaining after V — k merges is We use the notation i+ j to represent the cluster obtained by merging Ck(i) and Ck(i)• If we know Ik.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 8 | Reference Article:  J92-4003.txt | Citing Article:  P13-1041.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Note this is different from the likelihood estimation of Brown et al (1992).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Thus, the order-n parameters are We call this method of parameter estimation sequential maximum likelihood estimation.</S><S sid = NA ssid = NA>Maximum likelihood estimation of the parameters of a consistent n-gram language model is an interesting topic, but is beyond the scope of this paper.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 9 | Reference Article:  J92-4003.txt | Citing Article:  P12-1081.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The features were all derived from the publicly available clusters produced by running the Brown clustering algorithm (Brown et al, 1992) over the BLLIP corpus with the Penn Treebank sentences excluded.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We measured the perplexity of the Brown corpus with respect to this model and found it to be 271.</S><S sid = NA ssid = NA>The Brown corpus contains 1,014,312 words and has a perplexity of 244 with respect to our interpolated model.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 10 | Reference Article:  J92-4003.txt | Citing Article:  D11-1144.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Popular clustering algorithms used prevalently in many NER systems are, for example, the combination of distributional and morphological similarity work of (Clark 2003) or the classic N-gram language model based clustering algorithm of (Brown et al 1992).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We used these classes to construct an interpolated 3-gram class model using the same training text and held-out data as we used for the word-based language model we discussed above.</S><S sid = NA ssid = NA>Class-Based N-Gram Models Of Natural Language</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 11 | Reference Article:  J92-4003.txt | Citing Article:  E12-1033.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>As a state of-the-art clustering method, we consider Brown clustering (Brown et al 1992) as implemented in the SRILM-toolkit (Stolcke, 2002).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In Section 4, we apply mutual information to two other forms of word clustering.</S><S sid = NA ssid = NA>Thus, the probability of a transition between the state W1W2 • • ' Wn-1 and the state w2w3 • • • wn is Pr (w I W1102 • • • wn-i ) .</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 12 | Reference Article:  J92-4003.txt | Citing Article:  C00-2121.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Methods that use bigrams (Brown et al, 1992) or trigrams (Martin et al, 1998) cluster words considering as a word's context he one or two immediately adjacent words and employ as clustering criteria the minimal loss of average information and the perplexity improvement respectively.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We then merge that pair of classes for which the loss in average mutual information is least.</S><S sid = NA ssid = NA>Initially, we assign each word to a distinct class and compute the average mutual information between adjacent classes.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 13 | Reference Article:  J92-4003.txt | Citing Article:  C00-2121.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>Brown et al (1992) also proposed a window method introducing the concept of semantic stickiness of two words as the relatively frequent close occurrence between them (less than 500 words distance).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Then, by examining the probability that two words will appear within a reasonable distance of one another, we use it to find classes that have some loose semantic coherence.</S><S sid = NA ssid = NA>We arrange the words in the vocabulary in order of frequency with the most frequent words first and assign each of the first C words to its own, distinct class.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 14 | Reference Article:  J92-4003.txt | Citing Article:  D10-1056.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We find that the oldest system tested (Brown et al, 1992) produces the best prototypes, and that using these prototypes as input to Haghighi and Klein's system yields state of-the-art performance on WSJ and improvements on seven of the eight non-English corpora.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We measure the performance of our model on the Brown corpus, which contains a variety of English text and is not included in either our training or held-out data (Kiera and Francis 1967).</S><S sid = NA ssid = NA>To tackle this problem successfully, we must be able to estimate the probability with which any particular string of English words will be presented as input to the noisy channel.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 15 | Reference Article:  J92-4003.txt | Citing Article:  D10-1056.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The systems are as follows:1 [brown]: Class-based n-grams (Brown et al,1992).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>Class-Based N-Gram Models Of Natural Language</S><S sid = NA ssid = NA>We can expect from these data that maximum likelihood estimates will assign a probability of 0 to about 3.8 percent of the class 3-grams and to about .02 percent of the class 2-grams in a new sample of English text.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 16 | Reference Article:  J92-4003.txt | Citing Article:  D10-1056.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>We found that the oldest system (Brown et al, 1992) yielded the best prototypes, and that using these prototypes gave state-of-the-art performance on WSJ, as well as improvements on nearly all of the non-English corpora.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>In all three applications, given a signal y, we seek to determine the string of English words, w, which gave rise to it.</S><S sid = NA ssid = NA>We measure the performance of our model on the Brown corpus, which contains a variety of English text and is not included in either our training or held-out data (Kiera and Francis 1967).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 17 | Reference Article:  J92-4003.txt | Citing Article:  W97-0211.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA></S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>The authors would like to thank John Lafferty for his assistance in constructing word classes described in this paper.</S><S sid = NA ssid = NA>Of the 6.799 x 1010 2-grams that might have occurred in the data, only 14,494,217 actually did occur and of these, 8,045,024 occurred only once each.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 18 | Reference Article:  J92-4003.txt | Citing Article:  N10-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>The idea was introduced by Brown et al (1992) and used in different applications, including speech recognition, named entity tagging, machine translation, query expansion, text categorization, and word sense disambiguation.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>One language model may surpass another as part of a speech recognition system but perform less well in a translation system.</S><S sid = NA ssid = NA>Figure 1 shows a model that has long been used in automatic speech recognition (Bahl, Jelinek, and Mercer 1983) and has recently been proposed for machine translation (Brown et al. 1990) and for automatic spelling correction (Mays, Demerau, and Mercer 1990).</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 19 | Reference Article:  J92-4003.txt | Citing Article:  N10-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>To this end, the Brown algorithm (Brown et al, 1992) is applied to pairwise word co-occurrence statistics based on different definitions of word co-occurrence.</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words.</S><S sid = NA ssid = NA>We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words.</S> | Discourse Facet:  NA | Annotator: Automatic


Citance Number: 20 | Reference Article:  J92-4003.txt | Citing Article:  N10-1046.txt | Citation Marker Offset:  NA | Citation Marker: NA | Citation Offset:  NA | Citation Text:  <S sid = NA ssid = NA>In order to cluster lexical items, we use the algorithm proposed by Brown et al (1992), as implemented in the SRILM toolkit (Stolcke, 2002).</S> | Reference Offset:  NA | Reference Text:  <S sid = NA ssid = NA>First, we use it to find pairs of words that function together as a single lexical entity.</S><S sid = NA ssid = NA>The algorithm, then, is of order V3.</S> | Discourse Facet:  NA | Annotator: Automatic


