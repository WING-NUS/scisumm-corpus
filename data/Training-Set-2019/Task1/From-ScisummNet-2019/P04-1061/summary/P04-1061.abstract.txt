We present a generative model for the unsupervised learning of dependency structures.
We also describe the multiplicative combination of this dependency model with a model of linear constituency.
The product model outperforms both components on their respective evaluation metrics, giving the best published figures for unsupervised dependency parsing and unsupervised constituency parsing.
We also demonstrate that the combined model works and is robust cross-linguistically, being able to exploit either attachment or distributional regularities that are salient in the data.
