Effective Self-Training For Parsing
We present a simple, but surprisingly effective, method of self-training a two-phase parser-reranker system using readily available unlabeled data.
We show that this type of bootstrapping is possible for parsing when the bootstrapped parses are processed by a discriminative reranker.
Our improved model achieves an f-score of 92.1%, an absolute 1.1% improvement (12% error reduction) over the previous best result for Wall Street Journal parsing.
Finally, we provide some analysis to better understand the phenomenon.
We presented a very effective method for self-training a two-stage parsing system consisting of a first-stage generative lexicalized parser and a second-stage discriminative reranker.
Self-training can suffer from over-fitting, in which errors in the original model are repeated and amplified in the new model.
