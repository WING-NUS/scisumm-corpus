+/-EffectWordNet: Sense-level Lexicon Acquisition  for Opinion Inference


Yoonjung Choi and Janyce Wiebe 
Department of Computer Science 
University of Pittsburgh
yjchoi, wiebe@cs.pitt.edu






Abstract

Recently, work in NLP was initiated on a 
type of opinion  inference that arises when 
opinions are  expressed  toward events 
which have positive  or negative  effects 
on entities (+/-effect  events).  This paper 
addresses methods  for creating  a lexicon 
of such events, to support such work on 
opinion inference.  Due to significant 
sense ambiguity, our goal is to develop  a 
sense-level rather than word-level  lexicon. 
To maximize the effectiveness of different 
types of  information,  we  combine a 
graph-based  method using WordNet1 
relations and a standard  classifier  using 
gloss information. A hybrid between the 
two gives the best results. Further,  we 
provide evidence  that the model is an 
effective way to guide manual annotation 
to find +/-effect senses that are not in the 
seed set.


1   Introduction

Opinion mining (or sentiment analysis) identifies 
positive or negative opinions  in many kinds of 
texts such as reviews, blogs, and news articles.  It 
has been exploited  in many application areas such 
as  review mining, election analysis,  and infor- 
mation extraction.  While most previous research 
focusses on explicit  opinion expressions, recent 
work addresses  a type of opinion inference that 
arises when opinions are expressed toward events 
which have positive  or negative effects on enti- 
ties (Deng et al., 2013; Deng and Wiebe, 2014). 
We call such events +/-effect  events.2   Deng and 
Wiebe (2014) show how sentiments toward one

1 WordNet 3.0, http://wordnet.princeton.edu/
     2 While the term goodFor/badFor  is used in previous pa- 
pers (Deng et al., 2013; Deng and Wiebe, 2014; Deng et al.,
2014), we have since decided that +/-effect is a better term.


entity may be propagated  to other entities via 
opinion inference rules. They give the following 
example:

(1) The bill  would curb skyrocketing 
health care costs.

  The writer  expresses an explicit negative senti- 
ment (by skyrocketing) toward the object (health 
care costs). The event, curb, has a negative effect 
on costs, since they are reduced.  We can reason 
that the writer is positive toward the event because 
it has a negative effect on costs, toward which the 
writer is negative. From there, we can reason that 
the writer is positive toward  the bill, since it is 
the agent of the positive event.  Deng and Wiebe 
(2014) show that such inferences may be exploited 
to significantly improve explicit sentiment analy- 
sis systems.
  However, to achieve its results, the system de- 
veloped by Deng and Wiebe (2014) requires that 
all instances of +/-effect  events in the corpus be 
manually  provided  as input.  For the system to 
be fully automatic, it needs to be able to recog- 
nize +/-effect events automatically. This paper 
addresses methods  for creating lexicons of such 
events, to support such work on opinion inference. 
We have discovered that there is significant  sense 
ambiguity, meaning that words often have mix- 
tures of senses among the classes +effect,  -effect, 
and Null.  Thus, we develop a sense-level rather 
than word-level lexicon.
  One of  our goals is to investigate  whether 
the +/-effect  property tends to be shared among 
semantically-related   senses, and another is to 
use a method that applies to all word senses, not 
just to the senses of words in a given  word-level 
lexicon. Thus, we build a graph-based model  in 
which each node is a WordNet  sense, and edges 
represent  semantic  WordNet relations between 
senses.  In addition, we hypothesized that glosses 
also contain useful information.  Thus, we develop


a supervised  gloss classifier  and define a hybrid 
model which gives the best overall performance. 
Finally,  because  all  WordNet verb senses are 
incorporated into the model, we investigate the 
ability of the method to identify unlabeled  senses 
that are likely to be +/-effect  senses. We find that 
by iteratively labeling the top-weighted unlabeled 
senses and rerunning the model, it may be used as 
an effective method for guiding annotation efforts.


2   Background

There are many varieties of +/-effect  events, in- 
cluding creation/destruction (changes in states in- 
volving existence),  gain/loss  (changes in states 
involving possession), and benefit/injury  (Anand 
and Reschke, 2010; Deng et al., 2013). The cre- 
ation,  gain, and benefit classes are +effect  events. 
For example, baking a cake has a positive  effect on 
the cake because it is created;3 increasing the tax 
rate has a positive effect on the tax rate; and com- 
forting the child has a positive effect on the child. 
The antonymous  classes of each are -effect events: 
destroying the building has a negative effect on the 
building;  demand decreasing has a negative effect 
on demand; and killing Bill has a negative  effect 
on Bill.4
  While sentiment (Esuli and Sebastiani, 2006; 
Wilson et al., 2005; Su and Markert, 2009) and 
connotation lexicons (Feng et al., 2011; Kang et 
al., 2014) are related, sentiment, connotation, and
+/-effects  are not the same;  a single  event  may 
have different sentiment and +/-effect polarities, 
for example. Consider the following example:

perpetrate:
S: (v) perpetrate, commit, pull (perform 
an act, usually with a negative connota- 
tion) “perpetrate  a crime”; “pull a bank 
robbery”

  This  sense   of  perpetuate has   a   negative 
connotation, and is  an objective  term in 
SentiWordNet.    However, it has  a  positive 
effect on the object, a crime, since performing  a 
crime brings it into existence.


  3 Deng et al. (2013) point out that +/-effect objects are not 
equivalent to benefactive/malefactive semantic roles. An ex- 
ample they give is She baked a cake for me: a cake is the ob- 
ject of the +effect event baked as just noted, while me is the 
filler of its benefactive semantic role (Ziga and Kittil, 2010).
  

As we mentioned, the +/-effect ambiguity can- 
not be avoided in a word-level   lexicon.  In the
+/-effect corpus of Deng et al. (2013),5  +/-effect
events and their agents and objects are annotated 
at the word level. In that corpus, 1,411 +/-effect in- 
stances are annotated; 196 different  +effect  words 
and 286 different -effect words appear in these 
instances. Among them, 10 words appear  in 
both +effect and -effect instances, accounting for
9.07% of all annotated instances. They show that
+/-effect  events (and the inferences that motivate 
this work)  appear frequently  in sentences with ex- 
plicit sentiment. Further, all instances of +/-effect 
words that are not identified  as +/-effect  events are 
false hits from the perspective of a recognition sys- 
tem.
  The following is an example of a word with 
senses of different classes:

purge:
S: (v) purge (oust politically) “Deng 
Xiao Ping was  purged several  times 
throughout his lifetime” -effect
S: (v) purge (clear of a charge) +effect 
S: (v) purify, purge, sanctify (make pure 
or free from sin or guilt) “he left the 
monastery purified”  +effect
S: (v) purge (rid of impurities) “purge 
the water”; “purge your mind” +effect


  This is part of the WordNet output for the word 
purge.   In the first sense,  the polarity is -effect 
since it has a negative effect  on the object, Deng 
Xizo Ping. However, the other cases have positive 
effect on the object. Moreover,  although  a word 
may not have both +effect  and -effect senses, it 
may have mixtures  of ((+effect or -effect) and 
Null). A purely word-based approach is blind to 
these cases.


3   Related Work

Lexicons  are widely used in sentiment analysis 
and opinion  mining.  Several works such as Hatzi- 
vassiloglou  and McKeown (1997), Turney and 
Littman (2003), Kim and Hovy (2004), Strappar- 
ava and Valitutti (2004), and Peng and Park (2011) 
have tackled automatic lexicon expansion or ac- 
quistion. However, in most such work, the lexi- 
cons are word-level  rather than sense-level.


4 Their annotation manual, which gives additional cases, is	 	


available with the annotated data at http://mpqa.cs.pitt.edu/.


5 Called the goodFor/badFor  corpus in that paper.


  For the related (but different) tasks of  de- 
veloping subjectivity, sentiment  and connota- 
tion lexicons, some do take  a  sense-level   ap- 
proach.  Esuli and Sebastiani  (2006) construct 
SentiWordNet.  They assume that terms with 
the same polarity  tend to have similar glosses. So, 
they first expand a manually  selected seed set of 
senses  using WordNet lexical relations such as 
also-see and direct antonymy and train two clas- 
sifiers, one for positive and another for negative. 
As features, a vectorial  representation of glosses 
is adopted. These classifiers were applied to all 
WordNet senses to measure positive, negative, and 
objective scores. In extending their work (Esuli 
and Sebastiani, 2007), the PageRank algorithm  is 
applied to rank senses in terms of how strongly 
they are positive  or negative.  In the graph, each 
sense is one node, and two nodes are connected 
when they contain the same words in their Word- 
Net glosses. Moreover,   a random-walk   step is 
adopted to refine the scores in their recent work 
(Baccianella et al., 2010).  In contrast, our ap- 
proach  uses WordNet  relations and graph propa- 
gation in addition to gloss classification.

  Gyamfi et al. (2009) construct a classifier to la- 
bel the subjectivity of word senses. The hierarchi- 
cal structure and domain information  in WordNet 
are exploited  to define features in terms of sim- 
ilarity (using the LCS metric in Resnik (1995)) 
of target  senses and a seed set of senses.   Also, 
the similarity of glosses in WordNet is consid- 
ered. Even though they investigated the hierarchi- 
cal structure by LCS values, WordNet relations are 
not exploited directly.

  Su and Markert (2009) adopt a semi-supervised 
mincut method to recognize the subjectivity of 
word senses. To construct a graph, each node cor- 
responds to one WordNet  sense and is connected 
to two classification  nodes (one for subjectivity 
and another for objectivity) via a weighted  edge 
that is assigned by a classifier.   For this classifier, 
WordNet glosses,   relations, and monosemous 
features are considered.   Also, several WordNet 
relations (e.g.,  antonymy, similiar-to,  direct 
hypernym, etc.) are used to connect two nodes. 
Although  they make use of both WordNet glosses 
and relations,  and gloss information is utilized 
for a classifier,   this classifier is generated only 
for weighting edges  between   sense  nodes  and 
classification nodes, not for classifying all senses.
  

Kang et al. (2014) present a unified model that 
assigns connotation polarities  to both words and 
senses. They formulate  the induction  process as 
collective  inference over pairwise-Markov Ran- 
dom Fields,  and apply loopy belief propagation 
for inference. Their approach relies on selectional 
preferences of connotative predicates; the polarity 
of a connotation  predicate suggests the polarity  of 
its arguments.  We have not discovered an analo- 
gous type of predicate for the problem we address.
  Goyal et al. (2010) generate a lexicon of patient 
polarity verbs (PPVs) that impart positive or neg- 
ative states on their patients.  They harvest PPVs 
from a Web corpus by co-occurance with Kind and 
Evil agents and by bootstrapping over conjunc- 
tions of verbs. Riloff et al. (2013) learn positive 
sentiment phrases and negative situation  phrases 
from a corpus  of tweets with hashtag “sarcasm”. 
However, both of these methods  are word-level 
rather than sense-level.
  Ours is the first NLP research into developing 
a sense-level lexicon  for events that have negative 
or positive effects on entities.


4	+/-Effect Word-Level Seed Lexicon 
and Sense Annotations

To create the corpus used in this work, we devel- 
oped a word-level seed lexicon, and then manually 
annotated all the senses of the words in that lexi- 
con.
FrameNet6  is based  on a  theory of meaning
called Frame Semantics. In FrameNet,  a Lexical 
Unit (LU) is a pairing  of a word with a meaning, 
i.e., it corresponds to a sense in WordNet. Each 
LU of a polysemous  word belongs to a different 
semantic frame, which is a description  of a type 
of event, relation, or entity and, where appropri- 
ate, its participants. For instance, in the Creating 
frame, the definition  is that a Cause leads to the 
formation of a Created  entity. It has a positive 
effect on the object, Created entity. This frame 
contains about 10 LUs such as assemble,  create, 
yield, and so on. FrameNet consists of about 1,000 
semantic frames and about 10,000 LUs.
  FrameNet is a useful resource to select +/-effect 
words since  each semantic  frame covers multi- 
ple LUs.   We  believe  that using FrameNet  to 
find +/-effect  words is easier than finding  +/-effect 
words without any information  since words may

6 FrameNet, https://framenet.icsi.berkeley.edu/fndrupal/


be filtered by semantic frames. To select +/-effect 
words, an annotator (who is not a co-author)  first 
identified  promising  frames as +/-effect and ex- 
tracted all LUs from them. Then, he went through 
them and picked out the LUs which he judged to 
be +effect or -effect. In total, 736 +effect LUs and
601 -effect LUs were selected from 463 semantic 
frames.
  While Deng et al. (2013) and Deng and Wiebe 
(2014) specifically focus on events affecting  ob- 
jects (i.e., themes), we do not want to limit the 
lexicon to only that case. Sometimes, events have 
positive or negative effects on agents or other en- 
tities as well.  Thus, in this paper, we consider 
a  sense  to be +effect (-effect) if  it has  +effect 
(-effect) on an entity, which may be the agent, the 
theme, or some other entity.
  In a previous paper (Choi et al., 2014), we con- 
ducted a study  of the sense-level +/-effect  prop- 
erty.   For the evaluation,  two annotators  (who 
are co-authors of that paper) independently anno- 
tated  senses of selected words,  where some are 
from pure +effect (-effect) words (i.e., all senses 
of the words are classified  into the same class) 
and some are from mixed words (i.e., the words 
have both +effect and -effect senses). In the agree- 
ment study, we calculated percent agreement and 
κ (Artstein  and Poesio, 2008), and achieved 0.84 
percent agreement and 0.75 κ value.
  For a seed set and an evaluation  set in this pa- 
per, we need annotated sense-level +/-effect data. 
Mappings  between FrameNet and WordNet are 
not perfect. Thus, we opted to manually anno- 
tate the senses of the words in the word-level lexi- 
con. We first extracted all words from 736 +effect 
LUs and 601 -effect LUs; this extracts 606 +effect 
words and 537 -effect words (the number of words 
is smaller than the number of LUs because one 
word can have more than one LU). Among them,
14 words (e.g., crush, order, etc.) are in both the
+effect word set and the -effect word set. That is, 
these words have both +effect  and -effect  mean- 
ings. Recall that this annotator was focusing on 
frames, not on words - he did not look at all the 
senses of all the words. As we will see just below, 
when all the senses of all the words are annotated, 
a much higher percentage of the words have both
  

A different annotator (a co-author)  then went 
through all senses of all the words from the pre- 
vious  step and manually  annotated each sense as 
to whether it is +effect, -effect, or Null. Note that 
this annotator participated in an agreement study 
with positive results in Choi et al. (2014).
  For the experiments in this paper, we divided 
this annotated data into two equal-sized sets. One 
is a fixed  test set that is used to evaluate both the 
graph model and the gloss classifier. The other set 
is used as a seed set by the graph model,  and as a 
training  set by the gloss classifer.  Table 1 shows 
the distribution of the data. In total, there are 258
+effect  senses, 487 -effect senses, and 880 Null 
senses.  To avoid too large a bias toward the Null 
class,7  we randomly chose half (i.e., the Null set 
contains 440 senses). Half of each set is used as 
seed and training  data, and the other half is used 
for evaluation.


+
ef
fe
ct
-
ef
fe
ct
N
ull
# 
an
no
tat
ed 
da
ta
2
5
8
4
8
7
8
8
0
# 
S
ee
d/
Tr
ai
n
S
et
# 
Te
st
Se
t
1
2
9
1
2
9
2
4
3
2
4
4
2
2
0
2
2
0

Table 1: Distribution of annotated data.


5   Graph-based Semi-Supervised
Learning for WordNet Relations

WordNet (Miller et al., 1990) is organized by se- 
mantic  relations  such as hypernymy,  troponymy, 
grouping, and so on. These semantic relations can 
be used to build a network. Since the most fre- 
quently encoded relation is the super-subordinate 
relation, most verb senses are arranged  into hi- 
erarchies;  verb  senses towards  the bottom of the 
graph express increasingly specific manner. Thus, 
by following this hierarchical information, we hy- 
pothesized that +/-effect polarity tends to propa- 
gate. We use a graph-based semi-supervised  learn- 
ing (GSSL) method to carry out the label propaga- 
tion.

5.1   Graph Formulation
We formulate a graph for semi-supervised learning 
as follows. Let G = {X, E, W } be the undirected 
graph in which X is the set of nodes, E is the set


+effect and -effect senses.  We will also see that	 	
7 As mentioned in the introduction,  we want our method


many of the senses are revealed  to be Null, show-
ing that +effect vs. Null and -effect vs. Null ambi- 
guities are quite prevalent.



to be able to identify unlabeled  senses that are likely to be
+/-effect  senses (see Section 8); we resize the Null class to 
support this goal.


of edges, and W represents the edge weights (i.e., 
the weight of edge Eij  is Wij ). The weight matrix 
is a non-negative  matrix.
  Each data point in X  = {x1,  ...  ,xn} is one 
sense.   The labeled data of X  is represented  as 
XL = {x1, ... ,xl } and the unlabeled data is rep- 
resented  as XU   = {xl+1, ...  ,xn}).  The labeled 
data XL is associated with labels YL  = {y1,  ...
,yl },  where yi   ∈  {1, ..., c} (c is the number 
of classes).  As is typical in such settings, l   n: 
n is 13,767, i.e., the number of verb senses  in 
WordNet.  Seed/TrainSet in Table 1 is the labeled 
data.
  To connect two nodes, WordNet  relations  are 
utilized.  We first connect  nodes by the hierar- 
chical relations.  Since hypernym relations repre- 
sent more  general senses and troponym relations 
represent more specific  verb senses, we hypothe- 
sized that hypernyms or troponyms of a verb sense


solving a linear system. Thus, we adopt the LGC 
method in this paper. Although  there are some ro- 
bust GSSL methods for handling noisy labels, we 
do not need to handle noisy labels because our in- 
put is the annotated data.
  Let F  be a  n × c matrix to save  the output 
values of label propagation.	So, we can label
each instance  xi  as a label yi   = argmaxj≤cFij
after the label propagation. The initial discrete la-
bel matrix Y , which is also n × c, is defined  as 
Yij   = 1 if xi  is labeled  as yi   = j in YL,  and 
Yij    = 0 otherwise. The vertex degree  matrix 
D  = diag([D11, ..., Dnn])  is defined by Dii   =
j=1 Wij .
LGC defines the cost function Q which inte-
grates two penalty components, global smooth- 
ness and local fitting (µ is the regularization pa- 
rameter):


tends to have its same polarity. Verb groups rela-


1  n 	n


Fi	Fj	2


tions that represent verb senses having  a similar


Q =	  Wij  √    −  	


meaning  are also promising. Even though verb- 
group coverage is not large, its relations are reli-


2
i=1  j=1


Dii

n


Djj


able since they are manually grouped. The entail-


+µ  Fi − Yi


ment relation is defined as the verb Y is entailed 
by X if you must be doing Y by doing X . Since 
pairs connected by this relation  are co-extensive, 
we can assume that both are the same type of 
event.  The synonym relation is not used because 
it is already defined in senses (i.e., each node in 
the graph is a synset), and the antonym relation is 
also not applied since the weight matrix should be 
non-negative. The weight value of all edges is 1.0.

5.2   Label Propagation

Given a constructed graph, the label inference (or 
prediction) task is to propagate the seed labels to 
the unlabeled nodes. One of the classic GSSL la- 
bel propagation methods is the local and global 
consistency (LGC) method suggested by Zhou et 
al. (2004). The LGC method is a graph transduc- 
tion algorithm which is sufficiently  smooth with 
respect to the intrinsic structure revealed by known 
labeled and unlabeled data. The cost function typ- 
ically involves  a tradeoff between the smoothness 
of the predicted labels over the entire graph and 
the accuracy of the predicted labels in fitting the 
given labeled nodes XL.  LGC fits in a univariate 
regularization framework,  where the output ma- 
trix is treated as the only variable in optimization, 
and the optimal solutions can be easily obtained by


i=1

  The first part of the cost function is the 
smoothness constraint: a good classifying  func- 
tion should not change too much between nearby 
points. That is, if xi  and xj  are connected with 
an edge, the difference between them should be 
small. The second is the fitting constraint:  a good 
classifying function should not change too much 
from the initial label assignment. The final label 
prediction matrix F can be obtained by minimiz- 
ing the cost function Q.

5.3   Experimental  Results
Note that, in the rest of this paper, all tables except 
the last one give results on the same fixed test set 
(TestSet in Table 1).

We can apply the graph model in two ways.

• UniGraph: All three classes (+effect, -effect, 
and Null) are represented in one graph.

• BiGraph: Two separate graphs are first con- 
structed and then combined. One graph is for 
classifying +effect and Other (i.e., -effect or 
Null).  This graph is called +eGraph. The 
other graph, called -eGraph, is for classify- 
ing -effect and Other (i.e., +effect or Null).




H
+
T
+
V
+
E
+e
ff
ec
t
P 
R 
F
0.
6
5
3
0.
6
6
0
0.
6
5
6
0.
64
2
0.
68
0
0.
66
0
0.
65
1
0.
68
3
0.
66
7
-
ef
fe
ct
P
R 
F
0.
7
8
4
0.
5
4
7
0.
6
4
4
0.
77
9
0.
61
2
0.
68
6
0.
78
6
0.
60
4
0.
68
3
N
u
ll
P
R 
F
0.
5
5
7
0.
7
3
5
0.
6
3
4
0.
58
3
0.
69
5
0.
63
4
0.
56
4
0.
69
1
0.
62
1

Table 3: Effect of each relation





Table 2: Results of UniGraph,  BiGraph, and Bi- 
Graph*.


These are combined  into one model  as fol- 
lows.  Nodes that are labeled  as +effect  by
+eGraph and Other by -eGraph are regarded 
as  +effect, and nodes  that are  labeled   as
-effect by -eGraph and Other by +eGraph are 
regarded  as -effect. If nodes are labeled  as
+effect by +eGraph and -effect by -eGraph, 
they are deemed to be Null.  Nodes that are 
labeled Other by both graphs are also consid- 
ered as Null.

  We  had two  motivations for  experimenting 
with the BiGraph model: (1) SVM, the super- 
vised learning  method  used for gloss classifica- 
tion, tends to have better performance on binary 
classification  tasks, and (2) the two graphs of the 
combined model can “negotiate”  with each other 
via constraints.
  In Table 2, we calculate precision (P), recall (R), 
and f-measure (F) for all three classes. The base- 
line shown in the top row is the accuracy of a ma- 
jority class classifier. The first two columns of Ta- 
ble 2 show the results of UniGraph  and BiGraph 
when they are built using the hypernym, troponym, 
and verb group relations.  UniGraph outperforms 
BiGraph in this experiment.
  To improve the results by performing  some- 
thing possible with BiGraph (but not UniGraph), 
constraints are added when determining  the class. 
As we explained,  the label of  instance  xi   is 
determined by Fi  in the graph. When the label 
of xi is decided to be j, we can say that its con- 
fidence value is Fij . There are two constraints  as 
follows.


• If a sense is labeled as +effect  (-effect),  but 
the confidence value is less than a threshold, 
we count it as Null.

• If a sense is labeled  as both +effect and -effect 
by BiGraph, we choose the label with the 
higher confidence value only if the higher one 
is larger than a threshold and the lower one is 
less than a threshold.

  The thresholds are determined on Seed/TrainSet 
by running BiGraph several times with different 
thresholds, and choosing the one that gives the 
best performance on Seed/TrainSet.  (The chosen 
value is 0.025 for +effect and 0.03 for -effect).
  As can be seen in Table 2, BiGraph with con- 
straints (called BiGraph*) outperforms not only 
BiGraph without any constraints  but also Uni- 
Graph. Especially, for BiGraph*, the recall of the 
Null class is considerably increased, showing that 
constraints not only help overall, but are particu- 
larly important for detecting Null cases.
  Table 3 gives ablation results, showing the con- 
tribution of each WordNet  relation in BiGraph*. 
With only hierarchical information (i.e., hyper- 
nym (H) and troponym  (T) relations), it already 
shows good performance for all classes.   How- 
ever, they cannot cover some senses. Among  the
13,767 verb senses  in WordNet, 1,707 (12.4%) 
cannot be labeled because there are not sufficient 
hierarchical links to propagate polarity informa- 
tion.   When adding the verb group (+V) rela- 
tion, it shows improvement  in both +effect and
-effect.   Especially, the recall for +effect and
-effect is significantly  increased. In addition, the 
coverage of the 13,767 verb senses increases  to
95.1%. For entailment (+E), whereas adding it 
shows  a slight improvement in +effect (and in- 
creases coverage  by 1.1 percentage points), the


performance is decreased a little bit in the -effect 
and Null classes. Since the average f-measure for 
all classes is the highest with hypernym (H), tro- 
ponym (T), and verb group (V) relations (not en- 
tailment), we only consider these three relations 
when constructing the graph.

6   Supervised Learning applied to
WordNet  Glosses

In WordNet,  each sense contains a gloss consist- 
ing of a definition  and optional example sentences. 
Since a gloss consists of several words and there 
are no direct links between glosses, we believe that 
a word vector representation is appropriate to uti- 
lize gloss information  as in Esuli and Sebastiani 
(2006). For that, we adopt an SVM classifier.

6.1   Features

Two different  feature types are used.
  Word Features (WF): The bag-of-words 
model is applied. We do not ignore stop words 
for several reasons. Since most definitions  and ex- 
amples are not long, each gloss contains  a small 
number of words. Also, among them, the total vo- 
cabulary of WordNet  glosses is not large. More- 
over, some prepositions such as against are some- 
times useful to determine the polarity (+effect or
-effect).
Sentiment  Features  (SF): Some glosses  of
+effect (-effect) senses contain positive (negative) 
words.  For instance, the definition of {hurt#4, 
injure#4} is “cause damage or affect negatively.” 
It contains  a negative  word, negatively. Since  a 
given event may positively (negatively) affect enti- 
ties, some definitions or examples already contain 
positive (negative) words to express this. Thus, as 
features, we check how many positive (negative) 
words a given gloss contains. To detect sentiment 
words, the subjectivity lexicon provided by Wil- 
son et al. (2005)8  is utilized.

6.2   Gloss Classifier

We have three classes, +effect,  -effect,  and Null. 
Since SVM  shows better performance on binary 
classification  tasks, we generate two binary clas- 
sifiers, one (+eClassifier) to determine whether 
a  given sense  is +effect or Other, and another 
(-eClassifier) to classify  whether a given  sense is
-effect or Other. Then, they are combined  as in
BiGraph.

8 Available at http://mpqa.cs.pitt.edu/


6.3   Experimental  Results

Seed/TrainSet in Table 1 is used to train the two 
classifiers,  and TestSet is utilized for the evalua- 
tion. So, the training  set for +eClassifier consists 
of 129 +effect instances and 463 Other instances, 
and the training set for -eClassifier contains 243
-effect instances and 349 Other instances.  As a 
baseline, we adopt a majority  class classifier.
  Table 4 shows the results on TestSet. Perfor- 
mance is better for the -effect than for the +effect 
class, perhaps because the -effect  class has more 
instances.
  When sentiment features (SF)  are   added, 
all  metric values increase,  providing evidence 
that sentiment  features are helpful to determine
+/-effect  classes.


W
F
W
F
+
S
F
baseli
ne 
accur
acy
0
.
4
1
1
a
c
c
u
ra
c
y
0.
5
0
9
0
.
5
3
9
+e
ff
ec
t
P
R
 
F
0.
5
4
1
0.
3
5
4
0.
4
2
8
0
.
5
8
8
0
.
3
9
3
0
.
4
7
2
-
ef
fe
ct
P
R
 
F
0.
6
1
6
0.
5
0
0
0.
5
5
2
0
.
6
7
2
0
.
5
1
1
0
.
5
8
0
N
u
ll
P
R
 
F
0.
4
3
2
0.
6
1
2
0.
5
0
7
0
.
4
5
1
0
.
6
5
7
0
.
5
3
5

Table 4: Results of the gloss classifier.



7   Hybrid Method

To use more combined knowledge, the gloss clas- 
sifier and BiGraph*  can be combined. That is, for 
WordNet gloss information,  the gloss classifier is 
utilized, and for WordNet relations, BiGraph* is 
used. With the Hybrid method, we can  see not 
only the effect of propagation by WordNet rela- 
tions but also the usefulness of gloss information 
and sentiment features. Also, while BiGraph* 
cannot cover all senses in WordNet, the Hybrid 
method can.
  The outputs of  the gloss classifier and Bi- 
Graph* are combined   as follows.  The label of 
the gloss classifier is one of +effect, -effect, Null, 
or Both (when  a given  sense is classified  as both
+effect by +eClassifier and -effect by -eClassifier). 
Possible labels of BiGraph* are +effect, -effect, 
Null, Both, or None (when  a given sense is not


labeled by BiGraph*). There are five rules:

• If both labels are +effect (-effect),  it is +effect
(-effect).

• If one of them is Both and the other is +effect
(-effect), it is +effect (-effect).

• If the label of BiGraph* is None, believe the 
label of the gloss classifier

• If both labels are Both, it is Null

• Otherwise, it is Null

  The results  for Hybrid are  given in the first 
row of the lower half of Table 5; the results for 
BiGraph* are in the first row of the upper half, 
for comparison. Generally,  the Hybrid method 
shows better performance than the gloss classifier 
and BiGraph*. In the Hybrid method, since more
+/-effect senses are detected  than by BiGraph*, 
while precision is decreased,  recall is increased 
by more. However, by the same token, the over- 
all performance for the Null class is decreased. 
Actually, that is expected since the Null class is 
determined by the Other class in the gloss clas- 
sifier and BiGraph*.  Through this experiment, we 
see that the Hybrid method is better for classifying
+/-effect  senses.

7.1   Model Comparison

To provide evidence for our assumption that dif- 
ferent models are needed for different information 
to maximize  effectiveness, we compare the hy- 
brid method with the supervised learning and the 
graph-based learning  (GSSL)  methods, each uti- 
lizing both WordNet  relations and gloss informa- 
tion.
  Supervised Learning (onlySL): The gloss clas- 
sifier is trained with word features and sentiment 
features for WordNet Gloss.  To exploit  Word- 
Net relations in supervised  learning, especially 
the hierarchical information, we use least  com- 
mon subsumer (LCS) values  as in Gyamfi et al. 
(2009), which, recall, performs supervised learn- 
ing of subjective/objective senses. The values are 
calculated  as follows. For a target  sense t and a 
seed set S, the maximum LCS value  between a 
target sense and a member  of the seed set is found 
as:

Score(t, S)= maxs∈S LC S(t, s)
  

With this LCS feature and the features described 
in Section 6, we run SVM on the same training and 
test data. For LCS values, the similarity  using the 
information  content proposed by Resnik (1995) is 
measured. WordNet Similarity9 package provides 
pre-computed pairwise similarity  values for that.
  Table 6 shows results of onlySL. Compared to 
Table 4, while +effect and Null classes show a 
slight improvement,  the performance is degraded 
for -effect. This means that the added feature is 
rather harmful to -effect. Even though the hierar- 
chical feature is very helpful to expand +/-effect, 
it is not helpful for onlySL since SVM cannot cap- 
ture propagation according to the hierarchy.
  Graph-based Learning (onlyGraph): In Sec- 
tion 5, the graph is constructed by using Word- 
Net relations. To apply WordNet gloss informa- 
tion in onlyGraph, we calculate a cosine similarity 
between glosses.  If the similarity value is higher 
than a threshold,  two nodes are connected with this 
similarity value. The threshold is determined by 
training and testing on Seed/TrainSet (the chosen 
value is 0.3).
  Comparing Tables 2 and 6, BiGraph* generally 
outperforms onlyGraph (the exception is precision 
of +effect). By gloss similarity, many nodes are 
connected to each other. However, since uncertain 
connections  can cause incorrect  propagation  in the 
graph, this negatively affects the performance.
  Through this experiment, we see that since each 
type of information  has a different  character, we 
need different  models to maximize the effective- 
ness of each type. Thus, the hybrid method with 
different models can have better performance.



H
y
br
id
o
nl
y
S
L
o
nl
y
Gr
ap
h
+e
ff
ec
t
P 
R 
F
0
.
6
1
0
0
.
7
3
5
0
.
6
6
7
0
.
5
8
4
0
.
4
0
0
0
.
4
7
5
0
.
7
0
1
0
.
3
6
4
0
.
4
8
0
-
ef
fe
ct
P
R 
F
0
.
7
1
7
0
.
6
6
9
0
.
6
9
2
0
.
7
7
8
0
.
3
1
6
0
.
4
4
9
0
.
6
5
1
0
.
5
6
2
0
.
6
0
3
N
u
ll
P
R 
F
0
.
5
5
6
0
.
5
2
0
0
.
5
3
8
0
.
4
4
0
0
.
8
1
3
0
.
5
7
1
0
.
4
7
3
0
.
6
7
9
0
.
5
5
7

Table 6: Comparison to onlySL and onlyGraph.



9 WordNet Similarity,
http://wn-similarity.sourceforge.net/




+
e
f
f
e
c
t
-
e
f
f
e
c
t
N
u
l
l

P
R
F
P
R
F
P
R
F
Bi
Gr
ap
h*
In
iti
al
1
s
t
2
n
d
3
r
d
4
t
h
0.
6
4
2
0.
6
3
6
0.
6
4
2
0.
6
3
6
0.
6
8
1
0.
68
0
0.
68
4
0.
70
1
0.
70
8
0.
67
4
0.
66
0
0.
66
3
0.
67
0
0.
67
0
0.
67
8
0.
77
9
0.
77
0
0.
74
8
0.
77
9
0.
75
6
0.
61
2
0.
63
2
0.
65
6
0.
65
2
0.
67
4
0.
68
6
0.
69
4
0.
69
9
0.
71
0
0.
71
2
0.
58
3
0.
59
1
0.
60
5
0.
59
9
0.
58
9
0.
69
5
0.
67
2
0.
65
5
0.
66
9
0.
66
9
0.
63
4
0.
62
9
0.
62
9
0.
63
2
0.
62
6
H
y
b
r
i
d
In
iti
al
1
s
t
2
n
d
3
r
d
4
t
h
0.
6
1
0
0.
6
1
4
0.
6
1
3
0.
6
1
6
0.
6
8
8
0.
73
5
0.
71
3
0.
74
3
0.
73
9
0.
68
1
0.
66
7
0.
67
2
0.
67
2
0.
67
2
0.
68
4
0.
71
7
0.
72
8
0.
71
6
0.
71
7
0.
71
2
0.
66
9
0.
68
1
0.
69
7
0.
70
6
0.
76
4
0.
69
2
0.
70
4
0.
70
6
0.
71
2
0.
73
2
0.
55
6
0.
56
2
0.
55
9
0.
55
9
0.
56
5
0.
52
0
0.
52
3
0.
49
7
0.
49
4
0.
52
7
0.
53
8
0.
54
2
0.
52
6
0.
52
5
0.
54
5

Table 5: Results of an iterative approach.




8   Guided Annotation

Recall that Seed/TrainSet and TestSet,  the data 
used so far, are all the senses of the words in a 
word-level +/-effect lexicon. This section presents 
evidence that our method can guide annotation ef- 
forts to find other words that have +/-effect senses. 
A bonus is that the method pinpoints particular
+/-effect senses of those words.
  All unlabeled  data are senses of words that are 
not included in the original lexicon. Since pre- 
sumably the majority of verbs do not have any
+/-effect  senses, a sense randomly  selected from 
WordNet is very likely to be Null. We explore an 
iterative approach to guided annotation, using Bi- 
Graph*  and Hybrid as the method for assigning 
labels.
  The system  is initially  created  as described 
above using Seed/TrainSet  as the initial seed set. 
Each iteration  has four steps: 1) rank all unlabeled 
data (i.e., the data other than TestSet and the cur- 
rent seed set) based on the Fij  confidence values 
(see Section 5.3); 2) choose the top 5% and manu- 
ally annotate them (the same annotator  as above 
did this); 3) add them to the seed set; 4) rerun 
the system using the expanded seed set.  We per- 
formed four iterations in this paper.
  The upper and lower parts of Table 5 show the 
intial results and the results after each iteration for 
BiGraph* and Hybrid. Recall that these are results 
on the fixed set, TestSet.  Overall  for both mod- 
els, f-measure increases for both the +effect and
-effect classes as more  seeds are added,  mainly 
due to improvements in recall. The evaluation on 
the fixed set is also useful in the annotation process 
because it trades off +/-effect vs. Null annotations.


If the new manual annotations were biased, in that 
they incorrectly label Null senses as +/-effect, then 
the f-measure results would instead degrade on the 
fixed TestSet, since the system is created each time 
using the increased seed set.
  We now consider the accuracy of the system 
on the newly labeled  annotated data in Step 2. 
Note that our method is similar to Active Learn- 
ing (Tong and Koller, 2001), in that both auto- 
matically identify which unlabeled instances the 
human should annotate next. However, in active 
learning, the goal is to find instances that are diffi- 
cult for a supervised learning system. In our case, 
the goal is to find needles in the haystack of Word- 
Net senses.  In Step 3, we add the newly labeled 
senses to the seed set, enabling the model to find 
unlabeled  senses close to the new seeds when the 
system is rerun for the next iteration.
  We  assess the system’s  accuracy  on the newly 
labeled data by comparing the system’s labels with 
the human’s new labels. Accuracy for +effect and
-effect is calculated such as:


# annotated +effect
Accuracy+ef f ect  = # top 5% +effect data
# annotated -effect
Accuracy−ef f ect  = # top 5% -effect data


That is, the accuracy means that out of the top 5% 
of the +effect  (-effect)  data as scored by the sys- 
tem, what percentage are correct  as judged  by a 
human annotator. Table 7 shows the accuracy for 
each iteration  in the top part and the number of 
senses labeled  in the bottom part. As can be seen, 
the accuracies range between 60% and 78%; these


values are much higher  than what would be ex- 
pected if labeling  senses of words randomly cho- 
sen from WordNet.10   The annotator spent, on av- 
erage, approximately  an hour to label 100 senses. 
For finding new words with +/-effect usages,  it 
would be much more cost-effective if a significant 
percentage of the data chosen for annotation  are 
senses of words that in fact have +/-effect  senses.











Table 7: Accuracy and frequency of the top 5% for 
each iteration


9   Conclusion and Future Work

In this paper, we investigated methods for creat- 
ing a sense-level +/-effect  lexicon. To maximize 
the effectiveness of each type of information, we 
combined  a graph-based method  using WordNet 
relations  and a standard classifier  using gloss in- 
formation. A hybrid between the two gives the 
best results. Further, we provide evidence that the 
model is an effective  way to guide manual anno- 
tation to find +/-effect words that are not in the 
seed word-level lexicon. This is important,  as the 
likelihood  that a random WordNet sense (and thus 
word) is +effect or -effect is not large.
  So as not to limit the inferences that may be 
drawn, our annotations  include events  that are
+effect or -effect either the agent or object. In fu- 
ture work, we plan to exploit corpus-based meth- 
ods using patterns as in Goyal et al. (2010) com- 
bined with semantic role labeling to refine the lex- 
icon to distinguish which is the affected entity. 
Further, to actually exploit the acquired lexicon to 
process corpus data, an appropriate coarse-grained 
sense disambiguation  process must be added,  as 
Akkaya et al. (2009) and Akkaya et al. (2011) did 
for subjective/objective classification.
  We hope the general methodology  will be ef- 
fective for other semantic properties. In opin- 
ion mining and sentiment analysis this is partic-

10 For reference, in 5th iteration, the +effect accuracy is
60.18% and the -effect accuracy is 69.93%, and in 6th itera- 
tion, the +effect accuracy is 59.81% and the -effect accuracy 
is 69.12%.


ularly needed, because different  meanings of pos- 
itive and negative are appropriate for different ap- 
plications. This is a way to create lexicons that are 
customized with respect to one’s own definitions.
  It would be promising  to combine our method 
with other methods to enable it to find +effect 
and -effect senses that are outside  the coverage 
of WordNet. However,  a WordNet-based  lexicon 
gives a substantial base to build from.


Acknowledgments

This work was supported in part by DARPA-BAA-
12-47 DEFT grant #12475008 and National  Sci- 
ence Foundation grant #IIS-0916046.   We would 
like to thank the reviewers for their helpful sug- 
gestions and comments.


References

Cem Akkaya, Janyce  Wiebe, and Rada Mihalcea.
2009. Subjectivity word sense disambiguation.  In
Proceedings of EMNLP  2009, pages 190–199.

Cem Akkaya, Janyce Wiebe,  Alexander  Conrad, and 
Rada Mihalcea. 2011. Improving the impact of sub- 
jectivity word sense disambiguation  on contextual 
opinion analysis. In Proceedings of CoNLL 2011, 
pages 87–96.

Pranna Anand and Kevin Reschke. 2010. Verb classes 
as evaluativity  functor  classes. In Interdisciplinary 
Workshop on Verbs. The Identification  and Repre- 
sentation of Verb Features.

Ron Artstein and Massimo Poesio. 2008. Inter-coder 
agreement for computational linguistics. Comput. 
Linguist., 34(4):555–596.

Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas- 
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical 
resource for sentiment analysis and opinion mining. 
In Proceedings of LREC, pages 2200–2204.

Yoonjung Choi, Lingjia Deng, and Janyce  Wiebe.
2014.  Lexical acquisition for opinion inference: 
A sense-level lexicon of benefactive and malefac- 
tive  events.    In Proceedings of the 5th Workshop 
on Computational Approaches to Subjectivity,  Sen- 
timent and Social Media Analysis  (WASSA), pages
107–112. Association for Computational Linguis- 
tics.

Lingjia Deng and Janyce Wiebe.  2014.  Sentiment 
propagation via implicature constraints. In Proceed- 
ings of EACL.

Lingjia Deng, Yoonjung Choi, and Janyce  Wiebe.
2013. Benefactive/malefactive  event and writer atti- 
tude annotation. In Proceedings of 51st ACL, pages
120–125.


Lingjia Deng, Janyce  Wiebe, and Yoonjung Choi.
2014. Joint inference and disambiguation of implicit 
sentiments via implicature constraints. In Proceed- 
ings of COLING,  page 7988.

Andrea Esuli and Fabrizio  Sebastiani.   2006. Senti- 
wordnet: A publicly available lexical resource for 
opinion mining. In Proceedings of 5th LREC, pages
417–422.

Andrea Esuli and Fabrizio  Sebastiani.  2007. Pager- 
anking wordnet synsets: An application to opinion 
mining. In Proceedings of ACL, pages 424–431.

Song Feng, Ritwik Bose, and Yejin Choi. 2011. Learn- 
ing general connotation of words using graph-based 
algorithms. In Proceedings of EMNLP, pages 1092–
1103.

Amit Goyal, Ellen Riloff, and Hal DaumeIII.  2010.
Automatically producing plot unit representations 
for narrative text. In Proceedings of EMNLP, pages
77–86.

Yaw Gyamfi, Janyce Wiebe, Rada Mihalcea, and Cem 
Akkaya. 2009. Integrating knowledge for subjectiv- 
ity sense labeling.   In Proceedings of NAACL HLT
2009, pages 10–18.

Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec- 
tives. In Proceedings of ACL, pages 174–181.

Jun Seok Kang, Song Feng, Leman Akoglu, and Yejin 
Choi. 2014. Connotationwordnet: Learning conno- 
tation over the word+sense network.  In Proceedings 
of the 52nd ACL, page 15441554.

Soo-Min Kim and Eduard Hovy. 2004. Determining 
the sentiment of opinions. In Proceedings of 20th 
COLING, pages 1367–1373.

George A. Miller, Richard Beckwith, Christiane Fell- 
baum, Derek Gross, and Katherine Miller.   1990. 
Wordnet: An on-line lexical database. International 
Journal of Lexicography, 13(4):235–312.

Wei Peng and Dae Hoon Park.  2011. Generate adjec- 
tive sentiment dictionary for social media sentiment 
analysis using constrained nonnegative matrix fac- 
torization. In Proceedings of ICWSM.

Philip Resnik. 1995.  Using information  content to 
evaluate semantic similarity. In Proceedings of 14th 
IJCAI,  pages 448–453.

Ellen Riloff,  Ashequl Qadir, Prafulla Surve,  Lalin- 
dra De Silva, Nathan Gilbert, and Ruihong Huang.
2013. Sarcasm as contrast  between a positive  sen- 
timent and negative situation.  In Proceedings of 
EMNLP,  pages 704–714.

Carlo Strapparava  and Alessandro  Valitutti.	2004.
Wordnet-affect: An affective extension of wordnet. 
In Proceedings of 4th LREC, pages 1083–1086.


Fangzhong Su and Katja Markert. 2009. Subjectiv- 
ity recognition on word senses via semi-supervised 
mincuts.	In Proceedings  of NAACL HLT 2009, 
pages 1–9.

Simon Tong and Daphne Koller. 2001. Support vector 
machin active learning with applications to text clas- 
sification. Journal of Machine Learning Research,
2:45–66.

Peter Turney and Michael Littman. 2003. Measuring 
praise and criticism: Inference of semantic orienta- 
tion from association.  ACM Transactions on Infor- 
mation  Systems, 21(4):315–346.

Theresa Wilson, Janyce Wiebe, , and Paul Hoffmann.
2005.  Recognizing contextual polarity in phrase- 
level sentiment analysis. In Proceedings of HLT- 
EMNLP,  pages 347–354.

Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, 
Jason  Weston, and Bernhard  Scholkopf.   2004. 
Learning with local and global consistency. Ad- 
vances in Neural Information  Processing Systems,
16:321–329.

Fernando Ziga and Seppo Kittil.  2010. Benefactives 
and malefactives, Typological  perspectives and case 
studies. John Benjamins Publishing.
