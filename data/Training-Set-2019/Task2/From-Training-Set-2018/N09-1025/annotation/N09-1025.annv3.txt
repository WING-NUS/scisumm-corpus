Citance Number: 1 | Reference Article: N09-1025 | Citing Article: C10-2052 | Citation Marker Offset: '22' | Citation Marker: Chiang et al., 2009 | Citation Offset: '22' | Citation Text: <S sid ="22" ssid = "22">We explore the different options for context and feature se 1 See (Chan et al., 2007) for the relevance of word sense disambiguation and (Chiang et al., 2009) for the role of prepositions in MT. 454 Coling 2010: Poster Volume, pages 454462, Beijing, August 2010 lection, the influence of different preprocessing methods, and different levels of sense granularity.</S> | Reference Offset: ['23'] | Reference Text: <S sid ="23" ssid = "9">Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.</S> | Discourse Facet: Method_Citation | 
Citance Number: 1 | Reference Article: N09-1025 | Citing Article: D11-1081 | Citation Marker Offset: '6' | Citation Marker: Chiang et al., 2009 | Citation Offset: '6' | Citation Text: <S sid ="6" ssid = "6">Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al., 2006; Tillmann and Zhang, 2006; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009).</S> | Reference Offset: ['9'] | Reference Text: <S sid ="9" ssid = "9">We add more than 250 features to improve a syntax- based MT systemalready the highest-scoring single system in the NIST 2008 ChineseEnglish common-data trackby +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.</S> | Discourse Facet: Method_Citation | 
Citance Number: 1 | Reference Article: N09-1025 | Citing Article: D11-1081 | Citation Marker Offset: '9' | Citation Marker: Chiang et al., 2009 | Citation Offset: '9' | Citation Text: <S sid ="9" ssid = "9">Overfitting problem often comes when training many features on a small data (Watanabe et al., 880 2007; Chiang et al., 2009).</S> | Reference Offset: ['23'] | Reference Text: <S sid ="23" ssid = "9">Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.</S> | Discourse Facet: Method_Citation | 
Citance Number: 1 | Reference Article: N09-1025 | Citing Article: D11-1081 | Citation Marker Offset: '25' | Citation Marker: Chiang et al., 2009 | Citation Offset: '25' | Citation Text: <S sid ="25" ssid = "25">(3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3 . Generally, this is done by deleting a node X0,1 . ing to constructing the oracle reference (Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2009), which is nontrivial for SMT and needs to be determined experimentally.</S> | Reference Offset: ['51'] | Reference Text: <S sid ="51" ssid = "24">We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).</S> | Discourse Facet: Method_Citation | 
Citance Number: 1 | Reference Article: N09-1025 | Citing Article: D11-1081 | Citation Marker Offset: '159' | Citation Marker: Chiang et al., 2009 | Citation Offset: '159' | Citation Text: <S sid ="159" ssid = "11">Following (Chiang et al., 2009), we only use 100 most frequent words for word context feature.</S> | Reference Offset: ['32' '33'] | Reference Text: <S sid ="32" ssid = "5">Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning (Chiang, 2007). | Discourse Facet: Method_Citation | 
Citance Number: 1 | Reference Article: N09-1025 | Citing Article: D11-1081 | Citation Marker Offset: '239' | Citation Marker: Chiang et al., 2009 | Citation Offset: '239' | Citation Text: <S sid ="239" ssid = "91">Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008).</S> | Reference Offset: ['29'] | Reference Text: <S sid ="29" ssid = "2">Hiero (Chiang, 2005) is a hierarchical, string-to- string translation system.</S> | Discourse Facet: Method_Citation | 
Citance Number: 1 | Reference Article: N09-1025 | Citing Article: D11-1125 | Citation Marker Offset: '13' | Citation Marker: Chiang et al., 2009 | Citation Offset: '13' | Citation Text: <S sid ="13" ssid = "13">The MIRA technique of Chiang et al. has been shown to perform well on large-scale tasks with hundreds or thousands of features (2009).</S> | Reference Offset: ['51'] | Reference Text: <S sid ="51" ssid = "24">We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).</S> | Discourse Facet: Method_Citation | 
Citance Number: 1 | Reference Article: N09-1025 | Citing Article: D11-1125 | Citation Marker Offset: '78', '79' | Citation Marker: Chiang et al., 2009 | Citation Offset: '77', '78', '79' | Citation Text: <S sid ="77" ssid = "2">The most prominent example of a tuning method that performs well on high-dimensionality candidate spaces is the MIRA-based approach used by Watanabe et al.</S>   <S sid ="78" ssid = "3">(2007) and Chiang et al.</S>   <S sid ="79" ssid = "4">(2008b; 2009).</S> | Reference Offset: ['14'] | Reference Text: <S sid ="14" ssid = "14">Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.</S> | Discourse Facet: Method_Citation | 
Citance Number: 1 | Reference Article: N09-1025 | Citing Article: D11-1125 | Citation Marker Offset: '208' | Citation Marker: Chiang et al., 2009 | Citation Offset: '207', '208', '209' | Citation Text: <S sid ="207" ssid = "55">We used the following feature classes in SBMT and PBMT extended scenarios:  Discount features for rule frequency bins (cf. Chiang et al. (2009), Section 4.1)  Target word insertion features9 We used the following feature classes in SBMT extended scenarios only (cf.</S> | Reference Offset: ['9'] | Reference Text: <S sid ="9" ssid = "9">We add more than 250 features to improve a syntax- based MT systemalready the highest-scoring single system in the NIST 2008 ChineseEnglish common-data trackby +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.</S> | Discourse Facet: Method_Citation | 
Citance Number: 1 | Reference Article: N09-1025 | Citing Article: D11-1125 | Citation Marker Offset: '210' | Citation Marker: Chiang et al., 2009 | Citation Offset: '210', '211' | Citation Text: <S sid ="210" ssid = "58">Chiang et al. (2009), Section 4.1):10  Rule overlap features  Node count features 9 For ChineseEnglish and UrduEnglish SBMT these features only fired when the inserted target word was unaligned to any source word.</S> | Reference Offset: ['9'] | Reference Text: <S sid ="9" ssid = "9">We add more than 250 features to improve a syntax- based MT systemalready the highest-scoring single system in the NIST 2008 ChineseEnglish common-data trackby +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.</S> | Discourse Facet: Method_Citation | 
Citance Number: 1 | Reference Article: N09-1025 | Citing Article: D11-1125 | Citation Marker Offset: '227' | Citation Marker: Chiang et al., 2009 | Citation Offset: '227', '228', '229' | Citation Text: <S sid ="227" ssid = "16">5.4.1 MERT We used David Chiangs CMERT implementation We for the most part follow the MIRA algorithm for machine translation as described by Chiang et al. (2009)12 but instead of using the 10-best of each of the best hw , hw +g, and hw -g, we use the 30-best according to hw .13 We use the same sentence-level BLEU calculated in the context of previous 1-best translations as Chiang et al. (2008b; 2009). | Reference Offset: ['14'] | Reference Text: <S sid ="14" ssid = "14">Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.</S> | Discourse Facet: Method_Citation | 
Citance Number: 1 | Reference Article: N09-1025 | Citing Article: N12-1006 | Citation Marker Offset: '102' | Citation Marker: Chiang et al., 2009 | Citation Offset: '102', '103' | Citation Text: <S sid ="102" ssid = "7">Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al. (2009) </S> | Reference Offset: ['51'] | Reference Text: <S sid ="51" ssid = "24">We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).</S> | Discourse Facet: Method_Citation | 
Citance Number: 1 | Reference Article: N09-1025 | Citing Article: P12-1001 | Citation Marker Offset: '147' | Citation Marker: Chiang et al., 2009 | Citation Offset: '147' | Citation Text: <S sid ="147" ssid = "64">Alternatively, by using the large- margin optimizer in (Chiang et al., 2009) and moving it into the for-each loop (lines 49), one can get an online algorithm such PMOMIRA.</S> | Reference Offset: ['14'] | Reference Text: <S sid ="14" ssid = "14">Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.</S> | Discourse Facet: Method_Citation | 
Citance Number: 1 | Reference Article: N09-1025 | Citing Article: P13-1110 | Citation Marker Offset: '36' | Citation Marker: Chiang et al., 2009 | Citation Offset: '36' | Citation Text: <S sid ="36" ssid = "16">Online large-margin algorithms, such as MIRA, have also gained prominence in SMT, thanks to their ability to learn models in high-dimensional feature spaces (Watanabe et al., 2007; Chiang et al., 2009).</S> | Reference Offset: ['51'] | Reference Text: <S sid ="51" ssid = "24">We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).</S> | Discourse Facet: Method_Citation | 
Citance Number: 2 | Reference Article: N09-1025 | Citing Article: P13-1110 | Citation Marker Offset: '102' | Citation Marker: Chiang et al., 2009 | Citation Offset: '102' | Citation Text: <S sid ="102" ssid = "56">task Corpus Sentences Tokens En Zh/Ar MT05 1082 35k 33k training 1M 23.7M 22.8M tune (MT06) 1797 55k 49k baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and RAMPION (Gimpel and Smith, 2012).</S> | Reference Offset: ['14'] | Reference Text: <S sid ="14" ssid = "14">Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.</S> | Discourse Facet: Method_Citation | 
Citance Number: 3 | Reference Article: N09-1025 | Citing Article: P13-1110 | Citation Marker Offset: '115' | Citation Marker: Chiang et al., 2009 | Citation Offset: '115' | Citation Text: <S sid ="115" ssid = "9">The bound constraint B was set to 1.4 The approximate sentence-level BLEU cost i is computed in a manner similar to (Chiang et al., 2009), namely, in the context of previous 1-best translations of the tuning set.</S> | Reference Offset: ['65'] | Reference Text: <S sid ="65" ssid = "38">Following Chiang et al. (2008), we calculate the sentence B scores in (1), (2), and (3) in the context of some previous 1-best translations.</S> | Discourse Facet: Method_Citation | 
Citance Number: 4 | Reference Article: N09-1025 | Citing Article: P13-1110 | Citation Marker Offset: '120' | Citation Marker: Chiang et al., 2009 | Citation Offset: '120' | Citation Text: <S sid ="120" ssid = "14">For experiments with a larger feature set, we introduced additional lexical and non-lexical sparse Boolean features of the form commonly found in the literature (Chiang et al., 2009; Watan 4 We also conducted an investigation into the setting of the B parameter.</S> | Reference Offset: ['23'] | Reference Text: <S sid ="23" ssid = "9">Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.</S> | Discourse Facet: Method_Citation | 
Citance Number: 5 | Reference Article: N09-1025 | Citing Article: P28_n09 | Citation Marker Offset: '51' | Citation Marker: Chiang et al., 2009 | Citation Offset: '51' | Citation Text: <S sid ="51" ssid = "23">Recently, a simple method was presented in (Chiang eta!., 2009), which keeps partial English and Urdu words in the training data for alignment training.</S> | Reference Offset: ['29'] | Reference Text: <S sid ="29" ssid = "2">Hiero (Chiang, 2005) is a hierarchical, string-to- string translation system.</S> | Discourse Facet: Method_Citation | 
Citance Number: 6 | Reference Article: N09-1025 | Citing Article: P134_n09 | Citation Marker Offset: '27' | Citation Marker: Chiang et al., 2009 | Citation Offset: '27', '28' | Citation Text: <S sid ="27" ssid = "27">First, we used features proposed by Chiang et al. (2009):  phrase pair count bin features (bins 1, 2, 3, 45, 69, 10+)  target word insertion features  source word deletion features  word translation features  phrase length feature (source, target, both) Table 4:Sparse domain features When combining the domain features and the other sparse features, we see roughly additive gains (Table 5). | Reference Offset: ['9'] | Reference Text: <S sid ="9" ssid = "9">We add more than 250 features to improve a syntax- based MT systemalready the highest-scoring single system in the NIST 2008 ChineseEnglish common-data trackby +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.</S> | Discourse Facet: Method_Citation | 
Citance Number: 7 | Reference Article: N09-1025 | Citing Article: Pmert_n09 | Citation Marker Offset: '15' | Citation Marker: Chiang et al., 2009 | Citation Offset: '15' | Citation Text: <S sid ="15" ssid = "15">These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization.</S> | Reference Offset: ['9'] | Reference Text: <S sid ="9" ssid = "9">We add more than 250 features to improve a syntax- based MT systemalready the highest-scoring single system in the NIST 2008 ChineseEnglish common-data trackby +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.</S> | Discourse Facet: Method_Citation | 
Citance Number: 8 | Reference Article: N09-1025 | Citing Article: Pmert_n09 | Citation Marker Offset: '16' | Citation Marker: Chiang et al., 2009 | Citation Offset: '16' | Citation Text: <S sid ="16" ssid = "16">In particular, recent work (Chiang et al., 2009) has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation.</S> | Reference Offset: ['9'] | Reference Text: <S sid ="9" ssid = "9">We add more than 250 features to improve a syntax- based MT systemalready the highest-scoring single system in the NIST 2008 ChineseEnglish common-data trackby +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.</S> | Discourse Facet: Method_Citation | 
Citance Number: 9 | Reference Article: N09-1025 | Citing Article: PMTS_n09 | Citation Marker Offset: '218' | Citation Marker: Chiang et al., 2009 | Citation Offset: '218' | Citation Text: <S sid ="218" ssid = "109">This observation conrms previous ndings (Chiang et al., 2009) regarding the inability of the MERT algorithm to converge on an optimal set of weights for a reasonably large number of parameters.</S> | Reference Offset: ['51'] | Reference Text: <S sid ="51" ssid = "24">We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).</S> | Discourse Facet: Method_Citation | 
Citance Number: 10 | Reference Article: N09-1025 | Citing Article: PMTS_n09 | Citation Marker Offset: '245' | Citation Marker: Chiang et al., 2009 | Citation Offset: '245' | Citation Text: <S sid ="245" ssid = "136">The MERT algorithm is known to be unable to learn optimal weights for large parameter settings (Chiang et al., 2009).</S> | Reference Offset: ['65'] | Reference Text: <S sid ="65" ssid = "38">Following Chiang et al. (2008), we calculate the sentence B scores in (1), (2), and (3) in the context of some previous 1-best translations.</S> | Discourse Facet: Method_Citation | 
Citance Number: 11 | Reference Article: N09-1025 | Citing Article: PMTS_n09 | Citation Marker Offset: '252' | Citation Marker: Chiang et al., 2009 | Citation Offset: '252' | Citation Text: <S sid ="252" ssid = "6">We would also like to look into alternative tuning techniques, especially ones based on the MIRA algorithm to improve the quality of log-linear mixture adaptation in large parameter settings (Chiang et al., 2009).</S> | Reference Offset: ['51'] | Reference Text: <S sid ="51" ssid = "24">We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).</S> | Discourse Facet: Method_Citation | 
Citance Number: 12 | Reference Article: N09-1025 | Citing Article: PSMPT_n09 | Citation Marker Offset: '73' | Citation Marker: Chiang et al., 2009 | Citation Offset: '73', '74' | Citation Text: <S sid ="73" ssid = "6">Chiang et w(X  (, ,  )) = ii (2) i al. 2009) define new translational features using neighbouring word contexts of the source phrase, which are directly integrated into the translation model of Hiero system.</S> | Reference Offset: ['29'] | Reference Text: <S sid ="29" ssid = "2">Hiero (Chiang, 2005) is a hierarchical, string-to- string translation system.</S> | Discourse Facet: Method_Citation | 
Citance Number: 13 | Reference Article: N09-1025 | Citing Article: PTASL_n09 | Citation Marker Offset: '79' | Citation Marker: Chiang et al., 2009 | Citation Offset: '79' | Citation Text: <S sid ="79" ssid = "79">For example, Chiang et al. [3] designed many target-side syntax features to improve the string-to-tree translation.</S> | Reference Offset: ['51'] | Reference Text: <S sid ="51" ssid = "24">We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).</S> | Discourse Facet: Method_Citation | 
Citance Number: 14 | Reference Article: N09-1025 | Citing Article: PTASL_n09 | Citation Marker Offset: '86' | Citation Marker: Chiang et al., 2009 | Citation Offset: '86' | Citation Text: <S sid ="86" ssid = "86">Moreover, to establish a strong baseline, we also include the discount feature used by Chiang et al. [3] in our baseline string-to-tree model.</S> | Reference Offset: ['29'] | Reference Text: <S sid ="29" ssid = "2">Hiero (Chiang, 2005) is a hierarchical, string-to- string translation system.</S> | Discourse Facet: Method_Citation | 
Citance Number: 15 | Reference Article: N09-1025 | Citing Article: W10-1757 | Citation Marker Offset: '48' | Citation Marker: Chiang et al., 2009 | Citation Offset: '48' | Citation Text: <S sid ="48" ssid = "25">When the number of features is too large, even popular reranking algorithms such as SVM (Shen et al., 2004) and MIRA (Watanabe et al., 2007; Chiang et al., 2009) may fail.</S> | Reference Offset: ['23'] | Reference Text: <S sid ="23" ssid = "9">Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.</S> | Discourse Facet: Method_Citation | 
Citance Number: 16 | Reference Article: N09-1025 | Citing Article: W10-1757 | Citation Marker Offset: '209' | Citation Marker: Chiang et al., 2009 | Citation Offset: '209' | Citation Text: <S sid ="209" ssid = "25">Recent work by (Chiang et al., 2009) describes new features for hierarchical phrase-based MT, while (Collins and Koo, 2005) describes features for parsing.</S> | Reference Offset: ['32', '33'] | Reference Text: <S sid ="32" ssid = "5">Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning (Chiang, 2007). | Discourse Facet: Method_Citation | 
Citance Number: 17 | Reference Article: N09-1025 | Citing Article: W10-1761 | Citation Marker Offset: '53' | Citation Marker: Chiang et al., 2009 | Citation Offset: '53', '54' | Citation Text: <S sid ="53" ssid = "30">Chiang et al. (2009) added thousands of linguistically-motivated features to hierarchical and syntax systems, however, the source syntax features are derived from the research above.</S> | Reference Offset: ['9'] | Reference Text: <S sid ="9" ssid = "9">We add more than 250 features to improve a syntax- based MT systemalready the highest-scoring single system in the NIST 2008 ChineseEnglish common-data trackby +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement.</S> | Discourse Facet: Method_Citation | 
Citance Number: 18 | Reference Article: N09-1025 | Citing Article: W10-1761 | Citation Marker Offset: '92' | Citation Marker: Chiang et al., 2009 | Citation Offset: '92' | Citation Text: <S sid ="92" ssid = "25">A non-exhaustive sample is given below: [X  Ls, i, i] Terminal Symbol (X  Ls)  G X  ADJ A1 Akt # X1 Act N P  N E1 X2 # X1 X2 T OP  N E1 letzter X2 # X1 Last X2 [X    Fj,k Ls, i, j] [X  Fj,k  Ls, i, j + 1] Non-Terminal Symbol [X    Fj,k Ls, i, j] [X, j, Rj,k ] [X  Fj,k  Ls, i, Rj,k ] [X    Ls, i, Ri,j ] [Fi,j = Ls] the log function (hm = log hm) [X  Ls, i, Ri,j ] log p(tPIPEs) = m hm(t, s) (3) m Goal [X  Ls, 0, PIPEV PIPE  1] An advanta ge of our model over (Marto n and Resnik, 2008; Chiang et al., 2008, 2009) is the number of feature function s remains the same,This model allows translation rules to take ad vantage of both syntactic label and word context.</S> | Reference Offset: ['23'] | Reference Text: <S sid ="23" ssid = "9">Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.</S> | Discourse Facet: Method_Citation | 
Citance Number: 19 | Reference Article: N09-1025 | Citing Article: W10-1761 | Citation Marker Offset: '97' | Citation Marker: Chiang et al., 2009 | Citation Offset: '97' | Citation Text: <S sid ="97" ssid = "30">The model also differs from (Marton and Resnik, 2008; Chiang et al., 2008, 2009) by adding informative labels to rule non-terminals and requiring them to match the source span label.</S> | Reference Offset: ['32','33'] | Reference Text: <S sid ="32" ssid = "5">Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning (Chiang, 2007). | Discourse Facet: Method_Citation | 
