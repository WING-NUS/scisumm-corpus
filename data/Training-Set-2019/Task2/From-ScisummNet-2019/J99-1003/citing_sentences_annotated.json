[
  {
    "citance_No": 1, 
    "citing_paper_id": "P00-1055", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Antonio, Ribeiro | Gabriel, Lopes | Joao, Mexia", 
    "raw_text": "Melamed (1999) aligns texts using correspondence points taken either from orthographic cognates (Michel Simard et al., 1992) or from a seed translation lexicon. However, although the heuristics both approaches use to filter noisy points may be intuitively quite acceptable, they are not theoretically supported by Statistics", 
    "clean_text": "Melamed (1999) aligns texts using correspondence points taken either from orthographic cognates (Michel Simard et al., 1992) or from a seed translation lexicon.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P00-1055", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Antonio, Ribeiro | Gabriel, Lopes | Joao, Mexia", 
    "raw_text": "The latter approach uses other filtering parameters: maxi mum point ambiguity level, point dispersion and angle deviation (Melamed, 1999, pp", 
    "clean_text": "The latter approach uses other filtering parameters: maximum point ambiguity level, point dispersion and angle deviation (Melamed, 1999, pp. 115-116).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P00-1055", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Antonio, Ribeiro | Gabriel, Lopes | Joao, Mexia", 
    "raw_text": "with their neighbours. Melamed (1999) also filtered candidate correspondence points obtained from orthographic cognates", 
    "clean_text": "Melamed (1999) also filtered candidate correspondence points obtained from orthographic cognates.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "P00-1055", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Antonio, Ribeiro | Gabriel, Lopes | Joao, Mexia", 
    "raw_text": "This approach is similar to Melamed (1999) but, in contrast, it is statistically sup ported and uses no heuristics", 
    "clean_text": "This approach is similar to Melamed (1999) but, in contrast, it is statistically supported and uses no heuristics.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P13-1155", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Hany, Hassan | Arul, Menezes", 
    "raw_text": "We use a similarity function proposed in (Con tractor et al, 2010) which is based on Longest Common Subsequence Ratio (LCSR) (Melamed, 1999)", 
    "clean_text": "We use a similarity function proposed in (Contractor et al, 2010) which is based on Longest Common Subsequence Ratio (LCSR) (Melamed, 1999).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "W05-0606", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "Wesley, Mackay | Grzegorz, Kondrak", 
    "raw_text": "The former includes string edit distance (Wagner and Fischer, 1974), longest common subsequence ratio (Melamed, 1999), and measures based on shared character n-grams (Brew and McKelvie, 1996)", 
    "clean_text": "The former includes string edit distance (Wagner and Fischer, 1974), longest common subsequence ratio (Melamed, 1999), and measures based on shared character n-grams (Brew and McKelvie, 1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "D08-1047", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Naoaki, Okazaki | Yoshimasa, Tsuruoka | Sophia, Ananiadou | Jun'ichi, Tsujii", 
    "raw_text": "Eight baseline systems were prepared for comparison: Levenshtein distance (LD), normalized Levenshtein distance (NLD), Dicecoefficient on letter bi grams (DICE) (AdamsonandBoreham, 1974), Longest Common Substring Ratio (LCSR) (Melamed, 1999), Longest Common Prefix Ratio (PREFIX) (Kondrak, 2005), Porter? s stemmer (Porter, 1980), Morpha (Minnen et al,2001), and CST? s lemmatiser (Dalianis and Jonge 3LRSPL table includes trivial spelling variants that can be handled using simple character/string operations", 
    "clean_text": "Eight baseline systems were prepared for comparison: Levenshtein distance (LD), normalized Levenshtein distance (NLD), Dicecoefficient on letter bigrams (DICE) (Adamson and Boreham, 1974), Longest Common Substring Ratio (LCSR) (Melamed, 1999), Longest Common Prefix Ratio (PREFIX) (Kondrak, 2005), Porter's stemmer (Porter, 1980), Morpha (Minnen et al,2001), and CST's lemmatiser (Dalianis and Jonge 3LRSPL table includes trivial spelling variants that can be handled using simple character/string operations.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "P07-1083", 
    "citing_paper_authority": 14, 
    "citing_paper_authors": "Shane, Bergsma | Grzegorz, Kondrak", 
    "raw_text": "Other popular measures include Dice? s Coefficient (DICE) (Adamson and Boreham, 1974), and the length-normalized measures Longest Common Subsequence Ratio (LCSR) (Melamed, 1999), and Longest Common Prefix Ratio (PREFIX) (Kondrak,2005)", 
    "clean_text": "Other popular measures include Dice's Coefficient (DICE) (Adamson and Boreham, 1974), and the length-normalized measures Longest Common Subsequence Ratio (LCSR) (Melamed, 1999), and Longest Common Prefix Ratio (PREFIX) (Kondrak,2005).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "P07-1083", 
    "citing_paper_authority": 14, 
    "citing_paper_authors": "Shane, Bergsma | Grzegorz, Kondrak", 
    "raw_text": "We adopt an improved definition (suggested by Melamed (1999) for the French-English Canadian Hansards) that does not over-propose shorter word pairs: (e, f) are cognate if they are translations and their LCSR? 0.58", 
    "clean_text": "We adopt an improved definition (suggested by Melamed (1999) for the French-English Canadian Hansards) that does not over-propose shorter word pairs.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "P07-1083", 
    "citing_paper_authority": 14, 
    "citing_paper_authors": "Shane, Bergsma | Grzegorz, Kondrak", 
    "raw_text": "Ourlabelled set is then generated from pairs with LCSR? 0.58 (using the cutoff from Melamed (1999))", 
    "clean_text": "Our labelled set is then generated from pairs with LCSR 0.58 (using the cutoff from Melamed (1999)).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "P01-1050", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Daniel, Marcu", 
    "raw_text": "A few of the errors were genuine, and could be explained by failures of the sentence alignment program that was used to create the corpus (Melamed, 1999)", 
    "clean_text": "A few of the errors were genuine, and could be explained by failures of the sentence alignment program that was used to create the corpus (Melamed, 1999).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "D09-1075", 
    "citing_paper_authority": 12, 
    "citing_paper_authors": "Tagyoung, Chung | Daniel, Gildea", 
    "raw_text": "The Korean-English parallel data was collected from news websites and sentence aligned using two different tools described by Moore (2002) and Melamed (1999)", 
    "clean_text": "The Korean-English parallel data was collected from news websites and sentence aligned using two different tools described by Moore (2002) and Melamed (1999).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "C04-1137", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Grzegorz, Kondrak | Bonnie Jean, Dorr", 
    "raw_text": "The longest common subsequence ratio (Melamed, 1999) (LCSR) is computed by dividing Measure Zantac/ Zantac/ Xanax/ Xanax Contac Contac EDIT 3 2 4 NED 0.500 0.333 0.667 LCSR 0.500 0.667 0.333 BIGRAM 0.222 0.600 0.000 TRIGRAM-2B 0.000 0.333 0.000 SOUNDEX 3 1 3 EDITEX 5 2 7 ALINE 9.542 9.333 8.958 BI-SIM 0.417 0.583 0.250 TRI-SIM 0.333 0.500 0.167 PREFIX 0.000 0.000 0.000 Table 2: Examples of values returned by various measures", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "D09-1129", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Aminul, Islam | Diana Zaiu, Inkpen", 
    "raw_text": "Melamed (1999) normalized LCS by dividing the length of the longest common subsequence by the length of the longer string and called it longest common subsequence ratio (LCSR)", 
    "clean_text": "Melamed (1999) normalized LCS by dividing the length of the longest common subsequence by the length of the longer string and called it longest common subsequence ratio (LCSR).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "P09-1096", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Govind, Kothari | Sumit, Negi | Tanveer A., Faruquie | Venkatesan T., Chakaravarthy | L. Venkata, Subramaniam", 
    "raw_text": "The Longest Common Subsequence Ratio (LCSR) (Melamed, 1999) of two strings is the ratio of the length of their LCS and the length of the longer string", 
    "clean_text": "The Longest Common Subsequence Ratio (LCSR) (Melamed, 1999) of two strings is the ratio of the length of their LCS and the length of the longer string.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "P04-3006", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Chris, Pike | I. Dan, Melamed", 
    "raw_text": "Our work is based on a modification of the SIMR bi text mapping algorithm (Melamed, 1999)", 
    "clean_text": "Our work is based on a modification of the SIMR bitext mapping algorithm (Melamed, 1999).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "W12-0114", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Mireia, Ginest&Atilde;&shy;-Rosell | Johanna, Geiss | Anthony, Hartley | Bogdan, Babych | Reinhard, Rapp | Kurt, Eberle | Serge, Sharoff | Martin, Thomas", 
    "raw_text": "For sentence alignment, the length-based Gale& amp; Church aligner (1993) can be used, or? alternatively? Dan Melamed? s GSA-algorithm (Geometric Sentence Alignment; Melamed, 1999)", 
    "clean_text": "For sentence alignment, the length-based Gale & Church aligner (1993) can be used, or - alternatively - Dan Melamed's GSA-algorithm (Geometric Sentence Alignment; Melamed, 1999).", 
    "keep_for_gold": 0
  }
]