[
  {
    "citance_No": 1, 
    "citing_paper_id": "A92-1013", 
    "citing_paper_authority": 11, 
    "citing_paper_authors": "Roberto, Basili | Maria Teresa, Pazienza | Paola, Velardi", 
    "raw_text": "The results of these studies have important applications in lexicography, to detect lexicosyntactic regularities (Church and Hanks, 19901 (Calzolari and Bindi,1990), such as, for example~ support verbs (e.g.& quot; make-decision& quot;) prepositional verbs (e.g.& quot; rely-upon& quot;) idioms, semantic relations (e.g.& quot ;part_of& quot;) and fixed expressions (e.g.& quot; kick the bucket& quot;)", 
    "clean_text": "The results of these studies have important applications in lexicography, to detect lexico-syntactic regularities (Church and Hanks).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "A92-1013", 
    "citing_paper_authority": 11, 
    "citing_paper_authors": "Roberto, Basili | Maria Teresa, Pazienza | Paola, Velardi", 
    "raw_text": "In (Calzolari and Bindi, 1990), (Church and Hanks, 1990) the significance of an association (x, y) is measured by the mutual information I (x, y) ,i.e. the probability of observing x and y together, compared with the probability of observing x and y independently", 
    "clean_text": "In (Church and Hanks, 1990) the significance of an association (x, y) is measured by the mutual information I (x, y) ,i.e. the probability of observing x and y together, compared with the probability of observing x and y independently.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P14-1045", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Philippe, Muller | C\u00c3\u00a9cile, Fabre | Cl\u00c3\u00a9mentine, Adam", 
    "raw_text": "For each pair neighbour a /neighbour b, we computed a set of features from Wikipedia (the corpus used to derive the distributional similarity): We first computed the frequencies of each item in the corpus, freq a and freq b, from which we derive? freq min, freq max: the min and max of freq a and freq b;? freq?: the combination of the two, or log (freq a? freq b) 482 We also measured the syntagmatic association of neighbour a and neighbour b, with a mutual information measure (Church and Hanks, 1990), computed from the co occurrence of two tokens within the same paragraph in Wikipedia", 
    "clean_text": "We also measured the syntagmatic association of neighbour a and neighbour b, with a mutual information measure (Church and Hanks, 1990), computed from the cooccurrence of two tokens within the same paragraph in Wikipedia.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "S12-1094", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Carmen, Banea | Samer, Hassan | Michael, Mohler | Rada, Mihalcea", 
    "raw_text": "On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer et al, 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islamand Inkpen, 2006), Hyperspace Analogues to Language (Burgess et al, 1998) and distributional similarity (Lin, 1998) employ probabilistic approaches to decode the semantics of words", 
    "clean_text": "On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer et al, 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islamand Inkpen, 2006), Hyperspace Analogues to Language (Burgess et al, 1998) and distributional similarity (Lin, 1998) employ probabilistic approaches to decode the semantics of words.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P10-1135", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Ashwin, Ittoo | Gosse, Bouma", 
    "raw_text": "(3 )rpi (p)=? i? I (pmi (i, p )maxpmi? r? (i)) |I| In this equation ,pmi (i, p) is the point wise mutual information score (Church and Hanks, 1990) be tween a pattern, p (e.g. consist-of), and a tuple, i (e.g. engine-car), and maxpmi is the maximumPMI score between all patterns and tuples", 
    "clean_text": "In this equation, pmi (i, p) is the pointwise mutual information score (Church and Hanks, 1990) between a pattern, p (e.g. consist-of), and a tuple, i (e.g. engine-car), and maxpmi is the maximum PMI score between all patterns and tuples.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "P06-1036", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Olivier, Ferret | Michael, Zock", 
    "raw_text": "To this end we follow the method introduced by (Church and Hanks, 1990) ,i.e. by sliding a window of a given size over some texts", 
    "clean_text": "To this end we follow the method introduced by (Church and Hanks, 1990), i.e. by sliding a window of a given size over some texts.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "P06-1036", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Olivier, Ferret | Michael, Zock", 
    "raw_text": "Like (Church and Hanks, 1990), we used mutual information to measure the cohesion between two words", 
    "clean_text": "Like (Church and Hanks, 1990), we used mutual information to measure the cohesion between two words.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "P14-2030", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Charley, Beller | Rebecca, Knowles | Craig, Harman | Shane, Bergsma | Margaret, Mitchell | Benjamin, van Durme", 
    "raw_text": "We approached this task by selecting target roles from the first experiment and ranking char ac teristic attributes for each using point wise mutual information (PMI) (Church and Hanks, 1990)", 
    "clean_text": "We approached this task by selecting target roles from the first experiment and ranking characteristic attributes for each using point wise mutual information (PMI) (Church and Hanks, 1990).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "E95-1037", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Kuang-Hua, Chen", 
    "raw_text": "Collocation has been applied successfully to many possible applications (Church et al, 1989) ,e.g, lexicography (Church and Hanks, 1990), information retrieval (Salton, 1986a), text input (Yamashinand Obashi, 1988), etc. This paper will touch on its feasibility in topic identification", 
    "clean_text": "Collocation has been applied successfully to many possible applications (Church et al, 1989).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "C02-1033", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Olivier, Ferret", 
    "raw_text": "Collocations were extracted according to the method described in (Church and Hanks, 1990) by moving a window on texts", 
    "clean_text": "Collocations were extracted according to the method described in (Church and Hanks, 1990) by moving a window on texts.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "P11-1129", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Deyi, Xiong | Min, Zhang | Haizhou, Li", 
    "raw_text": "We use point wise mutual information (PMI) (Church and Hanks, 1990) to measure the strength of the association between x and y, which is defined as follows PMI (x, y)= log (P (x, y) P (x) P (y)) (12) 5In this paper, we require that word x and y occur in the same sentence. Zhou (2004) proposes a new language model enhanced with MI trigger pairs", 
    "clean_text": "We use pointwise mutual information (PMI) (Church and Hanks, 1990) to measure the strength of the association between x and y, which is defined as follows PMI (x, y)= log (P (x, y) P (x) P (y)).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "P06-2033", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Albert, Gatt | Kees, van Deemter", 
    "raw_text": "The information content of this set is defined as mutual information I (F (w)) (Church and Hanks, 1990)", 
    "clean_text": "The information content of this set is defined as mutual information I (F (w)) (Church and Hanks, 1990).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "P13-1174", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Song, Feng | Jun Seok, Kang | Polina, Kuznetsova | Yejin, Choi", 
    "raw_text": "In this graph ,connotative predicates p reside on one side of the graph and their co-occurring arguments a reside on the other side of the graph based on Google Web 1Tcorpus.7 The weight on the edges between the predicates p and arguments a are defined using Point-wise Mutual Information (PMI) as follows: w (p? a):= PMI (p, a) =log2 P (p, a) P (p) P (a) PMI scores have been widely used in previous studies to measure association between words (e.g., Turney (2001), Church and Hanks (1990))", 
    "clean_text": "PMI scores have been widely used in previous studies to measure association between words (Church and Hanks (1990)).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "P09-1072", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Kai-min K., Chang | Vladimir L., Cherkassky | Tom M., Mitchell | Marcel Adam, Just", 
    "raw_text": "Computational linguists have demonstrated that a word? s meaning is captured to some extent by the distribution of words and phrases with which it commonly co-occurs (Church and Hanks, 1990)", 
    "clean_text": "Computational linguists have demonstrated that a word's meaning is captured to some extent by the distribution of words and phrases with which it commonly co-occurs (Church and Hanks, 1990).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "W10-2007", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Gerhard, Kremer | Marco, Baroni", 
    "raw_text": "Following a very long tradition in computational linguistics (Church and Hanks, 1990), we use co occurrence statistics for words in certain contexts to hypothesise a meaningful connection between the words", 
    "clean_text": "Following a very long tradition in computational linguistics (Church and Hanks, 1990), we use cooccurrence statistics for words in certain contexts to hypothesise a meaningful connection between the words.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "D11-1077", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Yulia, Tsvetkov | Shuly, Wintner", 
    "raw_text": "Early approaches to MWEs identification concentrated on their col locational behavior (Church and Hanks, 1990)", 
    "clean_text": "Early approaches to MWEs identification concentrated on their collocational behavior (Church and Hanks, 1990).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "N12-1056", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Brian, Kjersten | Benjamin, Van Durme", 
    "raw_text": "Church and Hanks (1990) suggested point wise mutual information: PMI (wi ,wj)= log Pr (wi ,wj) Pr (wi) Pr (wj), showing linguistically appealing results using contexts defined by fixed width n-gram windows, and syntactic dependencies derived from automatically parsed corpora", 
    "clean_text": "Church and Hanks (1990) suggested pointwise mutual information: PMI (wi ,wj)= log Pr (wi ,wj) Pr (wi) Pr (wj), showing linguistically appealing results using contexts defined by fixed width n-gram windows, and syntactic dependencies derived from automatically parsed corpora.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "S12-1047", 
    "citing_paper_authority": 12, 
    "citing_paper_authors": "Keith, Holyoak | David, Jurgens | Peter D., Turney | Saif, Mohammad", 
    "raw_text": "Therefore, we propose a second baseline where pairs are rated according to their Pointwise Mutual Information (PMI) (Church and Hanks, 1990), which measures the statistical association between two words", 
    "clean_text": "Therefore, we propose a second baseline where pairs are rated according to their Pointwise Mutual Information (PMI) (Church and Hanks, 1990), which measures the statistical association between two words.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "P98-1100", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Amanda C., Jobbins | Lindsay J., Evett", 
    "raw_text": "Collocation: Collocations were extracted from a seven million word sample of the Longman English Language Corpus using the association ratio (Church and Hanks, 1990) and outputted to a lexicon", 
    "clean_text": "Collocation: Collocations were extracted from a seven million word sample of the Longman English Language Corpus using the association ratio (Church and Hanks, 1990) and outputted to a lexicon.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "W03-1805", 
    "citing_paper_authority": 14, 
    "citing_paper_authors": "Takashi, Tomokiyo | Matthew, Hurst", 
    "raw_text": "Word collocation Various collocation metrics have been proposed, including mean and variance (Smadja, 1994), the t-test (Church et al, 1991), the chi-square test, point wise mutual information (MI) (Church and Hanks, 1990), and binomial log likelihood ratio test (BLRT) (Dunning, 1993)", 
    "clean_text": "Word collocation Various collocation metrics have been proposed, including mean and variance (Smadja, 1994), the t-test (Church et al, 1991), the chi-square test, point wise mutual information (MI) (Church and Hanks, 1990), and binomial log likelihood ratio test (BLRT) (Dunning, 1993).", 
    "keep_for_gold": 0
  }
]