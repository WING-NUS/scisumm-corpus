[
  {
    "citance_No": 1, 
    "citing_paper_id": "P14-2021", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Julie, Tibshirani | Christopher D., Manning", 
    "raw_text": "Similarly, in a case study on co-training for natural language tasks, Pierce and Cardie (2001) find that the degradation in data quality from automatic la belling prevents these systems from performing comparably to their fully-supervised counterparts. In this work we argue that incorrect examples should be explicitly modelled during training, and present a simple extension of logistic regression that incorporates the possibility of mis labelling directly into the objective", 
    "clean_text": "Similarly, in a case study on co-training for natural language tasks, Pierce and Cardie (2001) find that the degradation in data quality from automatic labelling prevents these systems from performing comparably to their fully-supervised counterparts.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "D07-1051", 
    "citing_paper_authority": 24, 
    "citing_paper_authors": "Katrin, Tomanek | Joachim, Wermter | Udo, Hahn", 
    "raw_text": "Pierce and Cardie (2001) have shown, however, that for tasks which require large numbers of labeled examples? such as most NLP tasks? co training might be inadequate because it tends to generate noisy data", 
    "clean_text": "Pierce and Cardie (2001) have shown, however, that for tasks which require large numbers of labeled examples such as most NLP tasks co training might be inadequate because it tends to generate noisy data.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "N06-2014", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Sangyun, Hahn | Richard, Ladner | Mari, Ostendorf", 
    "raw_text": "We observed the same pattern in co-training; its accuracy peaked after two iterations (85.1%) and then performance degraded drastically (68% after five iterations) due in part to an increase in mislabeled data in the training set (as previously observed in (Pierce and Cardie, 2001)) and in part because the data skew is not controlled for", 
    "clean_text": "We observed the same pattern in co-training; its accuracy peaked after two iterations (85.1%) and then performance degraded drastically (68% after five iterations) due in part to an increase in mislabeled data in the training set (as previously observed in (Pierce and Cardie, 2001)) and in part because the data skew is not controlled for.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "W03-0407", 
    "citing_paper_authority": 25, 
    "citing_paper_authors": "Stephen, Clark | James R., Curran | Miles, Osborne", 
    "raw_text": "For naive co-training, new samples will always be added, and so there is a possibility that the noise accumulated at later stages will start to degrade performance (see Pierce and Cardie (2001))", 
    "clean_text": "For naive co-training, new samples will always be added, and so there is a possibility that the noise accumulated at later stages will start to degrade performance (see Pierce and Cardie (2001)).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "W03-0407", 
    "citing_paper_authority": 25, 
    "citing_paper_authors": "Stephen, Clark | James R., Curran | Miles, Osborne", 
    "raw_text": "The opposite behaviour has been observed in other applications of co-training (Pierce and Cardie, 2001) .Whether this robustness is a property of the tagging problem or our approach is left for future work", 
    "clean_text": "The opposite behaviour has been observed in other applications of co-training (Pierce and Cardie, 2001).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "W03-1015", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Vincent, Ng | Claire, Cardie", 
    "raw_text": "ters such as the pool size and the growth size (Pierce and Cardie, 2001), we evaluate the algorithm under different parameter settings, as described below. Evaluation", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "W03-1015", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Vincent, Ng | Claire, Cardie", 
    "raw_text": "The drop in F-measure is potentially due to the pollution of the labeled data by mislabeled instances (Pierce and Cardie, 2001)", 
    "clean_text": "The drop in F-measure is potentially due to the pollution of the labeled data by mislabeled instances (Pierce and Cardie, 2001).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "W07-1502", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Katrin, Tomanek | Joachim, Wermter | Udo, Hahn", 
    "raw_text": "How ever, it has been shown (Pierce and Cardie, 2001) that semi-supervised learning is brittle for NLP tasks where typically large amounts of high quality annotations are needed to train appropriate classifiers", 
    "clean_text": "However, it has been shown (Pierce and Cardie, 2001) that semi-supervised learning is brittle for NLP tasks where typically large amounts of high quality annotations are needed to train appropriate classifiers.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "W06-2209", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Andreas, Vlachos", 
    "raw_text": "The use of the disagreement between taggers for selecting candidates for manual correction is reminiscent of corrected co-training (PierceandCardie, 2001)", 
    "clean_text": "The use of the disagreement between taggers for selecting candidates for manual correction is reminiscent of corrected co-training (PierceandCardie, 2001).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "P03-1042", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Yunbo, Cao | Hang, Li | Li, Lian", 
    "raw_text": "For other work on co-training, see (Muslea et al 200; Pierce and Cardie 2001)", 
    "clean_text": "For other work on co-training, see (Muslea et al 200; Pierce and Cardie 2001).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "W04-2405", 
    "citing_paper_authority": 12, 
    "citing_paper_authors": "Rada, Mihalcea", 
    "raw_text": "Thefact that no improvement was obtained agrees with previous observations that classifiers that are too accurate can not be improved with bootstrapping (Pierce and Cardie,2001)", 
    "clean_text": "The fact that no improvement was obtained agrees with previous observations that classifiers that are too accurate can not be improved with bootstrapping (Pierce and Cardie,2001).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "W04-2405", 
    "citing_paper_authority": 12, 
    "citing_paper_authors": "Rada, Mihalcea", 
    "raw_text": "Overall, the high est error reduction is achieved with smoothed co-training using global parameter settings, where an average error reduction of 9.8% is observed with respect to the basic classifier. A comparative analysis of words that benefit from ba sic/smoothed co-training with global parameter settings, versus words with little or no improvement obtained through bootstrapping reveals several observations: (1) Words with accurate basic classifiers can not be improved through co-training, which agrees with previous observations (Pierce and Cardie, 2001)", 
    "clean_text": "A comparative analysis of words that benefit from basic/smoothed co-training with global parameter settings, versus words with little or no improvement obtained through bootstrapping reveals several observations: (1) Words with accurate basic classifiers can not be improved through co-training, which agrees with previous observations (Pierce and Cardie, 2001).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "W09-2208", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Wenhui, Liao | Sriharsha, Veeramachaneni", 
    "raw_text": "Algorithms such as co-training (Blum and Mitchell, 1998) (Collins and Singer, 1999) (Pierce and Cardie, 2001) and the Yarowsky algorithm (Yarowsky, 1995) make assumptions about the data that permit such an ap proach. The main requirement for the automatically generated training data in addition to high accuracy, is that it covers regions in the feature space with low probability density", 
    "clean_text": "Algorithms such as co-training (Blum and Mitchell, 1998) (Collins and Singer, 1999) (Pierce and Cardie, 2001) and the Yarowsky algorithm (Yarowsky, 1995) make assumptions about the data that permit such an approach.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "W03-0310", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Chris, Callison-Burch | Miles, Osborne", 
    "raw_text": "The German to English translation model improves the most? exhibiting a 2.5% improvement in accuracy. The table further indicates that co-training for ma chine translation suffers the same problem reported in Pierce and Cardie (2001): gains above the accuracy of the initial corpus are achieved, but decline as after acer tain number of machine translations are added to the training set", 
    "clean_text": "The table further indicates that co-training for machine translation suffers the same problem reported in Pierce and Cardie (2001): gains above the accuracy of the initial corpus are achieved, but decline as after acer tain number of machine translations are added to the training set.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "P09-1117", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Katrin, Tomanek | Udo, Hahn", 
    "raw_text": "Pierce and Cardie (2001) showed that the quality of the automatically labeled training data is crucial for co-training to perform well because too many tagging errors prevent a high performing model from being learned", 
    "clean_text": "Pierce and Cardie (2001) showed that the quality of the automatically labeled training data is crucial for co-training to perform well because too many tagging errors prevent a high performing model from being learned.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "P09-1117", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Katrin, Tomanek | Udo, Hahn", 
    "raw_text": "Also, the size of the seed set is an important parameter. When it is chosen too small data quality gets deteriorated quickly, when it is chosen too large no improvement over the initial model can be expected. To address the problem of data pollution by tagging errors, Pierce and Cardie (2001) propose corrected co-training", 
    "clean_text": "To address the problem of data pollution by tagging errors, Pierce and Cardie (2001) propose corrected co-training.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "P09-1117", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Katrin, Tomanek | Udo, Hahn", 
    "raw_text": "Thus, these boot strapping methods will presumably not find the most useful unlabeled examples but require a human to review data points of limited training utility (Pierce and Cardie, 2001)", 
    "clean_text": "Thus, these bootstrapping methods will presumably not find the most useful unlabeled examples but require a human to review data points of limited training utility (Pierce and Cardie, 2001).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "N03-1031", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Mark, Steedman | Rebecca, Hwa | Stephen, Clark | Miles, Osborne | Anoop, Sarkar | Julia, Hockenmaier | Paul, Ruhlen | Steven, Baker | Jeremiah, Crim", 
    "raw_text": "Empirical studies were conducted to compare selection methods under both co-training and a semi-supervised framework called corrected co-training (PierceandCardie, 2001), in which the selected examples are manually checked and corrected before being added to the 1In the context of training parsers, a labeled example is a sentence with its parse tree", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "N03-1031", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Mark, Steedman | Rebecca, Hwa | Stephen, Clark | Miles, Osborne | Anoop, Sarkar | Julia, Hockenmaier | Paul, Ruhlen | Steven, Baker | Jeremiah, Crim", 
    "raw_text": "In their experiments on NP identifiers, Pierce and Cardie (2001) observed a similar effect", 
    "clean_text": "In their experiments on NP identifiers, Pierce and Cardie (2001) observed a similar effect.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "H05-1107", 
    "citing_paper_authority": 14, 
    "citing_paper_authors": "Chenhai, Xi | Rebecca, Hwa", 
    "raw_text": "Co-training has been applied to a number of NLP applications ,including POS-tagging (Clark et al, 2003), parsing (Sarkar, 2001), word sense disambiguation (Mihalcea, 2004), and base noun phrase detection (Pierce and Cardie, 2001)", 
    "clean_text": "Co-training has been applied to a number of NLP applications ,including POS-tagging (Clark et al, 2003), parsing (Sarkar, 2001), word sense disambiguation (Mihalcea, 2004), and base noun phrase detection (Pierce and Cardie, 2001).", 
    "keep_for_gold": 0
  }
]