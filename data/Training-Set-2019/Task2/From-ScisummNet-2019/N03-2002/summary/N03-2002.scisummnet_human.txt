Factored Language Models And Generalized Parallel Backoff
We introduce factored language models (FLMs) and generalized parallel backoff (GPB).
An FLM represents words as bundles of features (e.g. , morphological classes, stems, data-driven clusters, etc.), and induces a probability model covering sequences of bundles rather than just words.
GPB extends standard backoff to general conditional probability tables where variables might be heterogeneous types, where no obvious natural (temporal) backoff order exists, and where multiple dynamic backoff strategies are allowed.
These methodologies were implemented during the JHU 2002 workshop as extensions to the SRI language modeling toolkit.
This paper provides initial perplexity results on both CallHome Arabic and on Penn Treebank Wall Street Journal articles.
Significantly, FLMs with GPB can produce bigrams with significantly lower perplexity, sometimes lower than highly-optimized baseline trigrams.
In a multi-pass speech recognition context, where bigrams are used to create first-pass bigram lattices or N-best lists, these results are highly relevant.
We show that factored language models are able to outperform standard n-gram techniques in terms of perplexity.
A factored language model (FLM) is based on a representation of words as feature vectors and can utilize a variety of additional information sources in addition to words, such as part-of-speech (POS) information, morphological information, or semantic features, in a unified and principled framework.
