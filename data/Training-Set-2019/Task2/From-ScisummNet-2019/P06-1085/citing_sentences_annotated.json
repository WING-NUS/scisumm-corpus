[
  {
    "citance_No": 1, 
    "citing_paper_id": "N07-1048", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Mathias, Creutz | Teemu, Hirsim&auml;ki | Mikko, Kurimo | Antti, Puurula | Janne, Pylkkonen | Vesa, Siivola | Matti, Varjokallio | Ebru, Arisoy | Murat, Sara&ccedil;lar | Andreas, Stolcke", 
    "raw_text": "Similarly, Goldwater et al (2006) use a hierarchical Dirichlet model in combination with morph bigram probabilities", 
    "clean_text": "Similarly, Goldwater et al (2006) use a hierarchical Dirichlet model in combination with morph bigram probabilities.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "N10-1083", 
    "citing_paper_authority": 47, 
    "citing_paper_authors": "Taylor, Berg-Kirkpatrick | Alexandre, Bouchard-C&ocirc;t&eacute; | John, DeNero | Dan, Klein", 
    "raw_text": "This is the same set-up used by Liang and Klein (2009), Goldwater et al (2006), and Johnson and Goldwater (2009) .This corpus consists of 9790 child-directed utterances transcribed using a phonetic representation", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "N10-1082", 
    "citing_paper_authority": 13, 
    "citing_paper_authors": "Percy, Liang | Michael I., Jordan | Dan, Klein", 
    "raw_text": "USM: We learned a USM model on the Bernstein-Ratner corpus from the CHILDES database used in Goldwater et al (2006) (9790 sentences) for word segmentation", 
    "clean_text": "USM: We learned a USM model on the Bernstein-Ratner corpus from the CHILDES database used in Goldwater et al (2006) (9790 sentences) for word segmentation.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "N09-1069", 
    "citing_paper_authority": 17, 
    "citing_paper_authors": "Percy, Liang | Dan, Klein", 
    "raw_text": "We applied this model on the Bernstein-Ratner corpus from the CHILDES database used in Goldwater et al (2006) (9790 sentences) and the Academia Sinica (AS) corpus from the first SIGHAN Chinese word segmentation bakeoff (we used the first 100K sentences)", 
    "clean_text": "We applied this model on the Bernstein-Ratner corpus from the CHILDES database used in Goldwater et al (2006) (9790 sentences) and the Academia Sinica (AS) corpus from the first SIGHAN Chinese word segmentation bakeoff (we used the first 100K sentences).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "N09-1069", 
    "citing_paper_authority": 17, 
    "citing_paper_authors": "Percy, Liang | Dan, Klein", 
    "raw_text": "Indeed, as we have shown, applying on line EM does require some care, and some variants (including incremental EM) can fail catastrophically in face of local optima. Stepwise EM provides finer control via its optimization parameters and has proven quite successful. One family of methods that resembles incremental EM includes collapsed samplers for Bayesian models? for example, Goldwater et al (2006) and Goldwater and Griffiths (2007)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "D09-1075", 
    "citing_paper_authority": 12, 
    "citing_paper_authors": "Tagyoung, Chung | Daniel, Gildea", 
    "raw_text": "Unsupervised monolingual segmentation has been studied as a model of language acquisition (Goldwateret al, 2006), and as model of learning morphology in European languages (Goldsmith, 2001)", 
    "clean_text": "Unsupervised monolingual segmentation has been studied as a model of language acquisition (Goldwater et al, 2006), and as model of learning morphology in European languages (Goldsmith, 2001).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "P10-1096", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "Elif, Yamangil | Stuart M., Shieber", 
    "raw_text": "We start at a random derivation of the corpus, and at every iteration re sample a derivation by amending the current one through local changes made at the node level, in the style of Goldwater et al (2006)", 
    "clean_text": "We start at a random derivation of the corpus, and at every iteration resample a derivation by amending the current one through local changes made at the node level, in the style of Goldwater et al (2006).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "P08-1031", 
    "citing_paper_authority": 13, 
    "citing_paper_authors": "S. R. K., Branavan | Harr, Chen | Jacob, Eisenstein | Regina, Barzilay", 
    "raw_text": "We employ Gibbs sampling, previously used in NLP by Finkel et al (2005) and Goldwater et al (2006), among others. This technique repeatedly samples from the conditional distributions of each hidden variable ,eventu ally converging on a Markov chain whose stationary distribution is the posterior distribution of the hid den variables in the model (Gelman et al, 2004)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "P08-1046", 
    "citing_paper_authority": 21, 
    "citing_paper_authors": "Mark, Johnson", 
    "raw_text": "Weevaluated the f-score of the recovered word constituents (Goldwater et al, 2006b)", 
    "clean_text": "We evaluated the f-score of the recovered word constituents (Goldwater et al, 2006b).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "P08-1046", 
    "citing_paper_authority": 21, 
    "citing_paper_authors": "Mark, Johnson", 
    "raw_text": "Johnson et al (2007a) presented an adaptor grammar that defines a unigram model of word segmentation and showed that it performs as well as the unigram DP word segmentation model presented by (Goldwater et al, 2006a)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "P08-1046", 
    "citing_paper_authority": 21, 
    "citing_paper_authors": "Mark, Johnson", 
    "raw_text": "As reported in Goldwater et al (2006a) and Goldwater et al (2007), a unigram word segmentation model tends to under segment and misanalyse collocations as individual words", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "P08-1046", 
    "citing_paper_authority": 21, 
    "citing_paper_authors": "Mark, Johnson", 
    "raw_text": "based unsupervised morphological analysis model presented by Goldwater et al (2006b)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "P08-1046", 
    "citing_paper_authority": 21, 
    "citing_paper_authors": "Mark, Johnson", 
    "raw_text": "Goldwater et al (2006a) showed that modeling dependencies between adjacent words dramatically improves word segmentation accuracy", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "P08-1046", 
    "citing_paper_authority": 21, 
    "citing_paper_authors": "Mark, Johnson", 
    "raw_text": "Wethen investigated adaptor grammars that incorporate one additional kind of information, and found that modeling collocations provides the greatest improvement in word segmentation accuracy, resulting in a model that seems to capture many of the same inter word dependencies as the bigram model of Goldwater et al (2006b)", 
    "clean_text": "We then investigated adaptor grammars that incorporate one additional kind of information, and found that modeling collocations provides the greatest improvement in word segmentation accuracy, resulting in a model that seems to capture many of the same inter word dependencies as the bigram model of Goldwater et al (2006b).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "P08-1046", 
    "citing_paper_authority": 21, 
    "citing_paper_authors": "Mark, Johnson", 
    "raw_text": "It? s straight forward to design an adaptor grammar that can capture a finite number of concatenative paradigm classes (Goldwater et al, 2006b; Johnson et al, 2007a)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "C10-1060", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Mark, Johnson | Katherine, Demuth", 
    "raw_text": "that have a fixed finite vector of parameters. Goldwater et al (2006) introduced two nonparametric Bayesian models of word segmentation, which are discussed in more detail in (Goldwater et al, 2009)", 
    "clean_text": "Goldwater et al (2006) introduced two nonparametric Bayesian models of word segmentation, which are discussed in more detail in (Goldwater et al, 2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "C10-1060", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Mark, Johnson | Katherine, Demuth", 
    "raw_text": "Goldwater et al (2006) and Goldwater et al (2009) demonstrated the importance of contextual dependencies for word segmentation, and pro posed a bigram model in order to capture some of these", 
    "clean_text": "Goldwater et al (2006) and Goldwater et al (2009) demonstrated the importance of contextual dependencies for word segmentation, and proposed a bigram model in order to capture some of these.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "W12-2304", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Ruey-Cheng, Chen | Jieh, Hsiang | Chiung-Min, Tsai", 
    "raw_text": "Goldwater et al (2006) used hierarchical Dirichletprocesses (HDP) to induce contextual word mod els", 
    "clean_text": "Goldwater et al (2006) used hierarchical Dirichlet processes (HDP) to induce contextual word models.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "W08-0704", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Mark, Johnson", 
    "raw_text": "While there? s no reason why these methods can? t be used to learn the syntax and semantics of human languages, much of the work to date has focused on lower-level learning problems such as morphological structure learning (Goldwater et al, 2006b) and word segmentation, where the learner is given unsegmented broad-phonemic utterance transcriptions 20 and has to identify the word boundaries (Goldwater et al, 2006a; Goldwater et al, 2007)", 
    "clean_text": "While there is no reason why these methods cannot be used to learn the syntax and semantics of human languages, much of the work to date has focused on lower-level learning problems such as morphological structure learning (Goldwater et al, 2006b) and word segmentation, where the learner is given unsegmented broad-phonemic utterance transcriptions and has to identify the word boundaries (Goldwater et al, 2006a; Goldwater et al, 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "W08-0704", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Mark, Johnson", 
    "raw_text": "it confirmed the importance of modeling con textual dependencies above the word level for word segmentation (Goldwater et al, 2006a), 2", 
    "clean_text": "It confirmed the importance of modeling contextual dependencies above the word level for word segmentation (Goldwater et al, 2006a).", 
    "keep_for_gold": 0
  }
]