This paper presents a comprehensive empirical comparison between two approaches for developing a base noun phrase chunker: human rule writing and active learning using interactive realtime human annotation.
Several novel variations on active learning are investigated, and underlying cost models for cross-modal machine learning comparison are presented and explored.
Results show that it is more efficient and more successful by several measures to train a system using active learning annotation rather than hand-crafted rule writing at a comparable level of human labor investment.
