[
  {
    "citance_No": 1, 
    "citing_paper_id": "N06-1015", 
    "citing_paper_authority": 20, 
    "citing_paper_authors": "Simon, Lacoste-Julien | Ben, Taskar | Dan, Klein | Michael I., Jordan", 
    "raw_text": "Moreover, including predictions of bi-directional IBM Model 4 and model of Liang et al (2006) as features, we achieve an absolute AER of 3.8 on the EnglishFrench Hansards alignment task? the best AER result published on this task to date", 
    "clean_text": "Moreover, including predictions of bi-directional IBM Model 4 and model of Liang et al (2006) as features, we achieve an absolute AER of 3.8 on the English French Hansards alignment task - the best AER result published on this task to date.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "N06-1015", 
    "citing_paper_authority": 20, 
    "citing_paper_authors": "Simon, Lacoste-Julien | Ben, Taskar | Dan, Klein | Michael I., Jordan", 
    "raw_text": "By also including as features the posteriors of the model of Liang et al (2006), we achieve AER of 3.8, and 96.7/95.5precision/recall", 
    "clean_text": "By also including as features the posteriors of the model of Liang et al (2006), we achieve AER of 3.8, and 96.7/95.5 precision/recall.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P14-1139", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Yin-Wen, Chang | Alexander M., Rush | John, DeNero | Michael John, Collins", 
    "raw_text": "Liang et al (2006) propose agreement-based learning, which jointly learns probabilities by maximizing a combination of likelihood and agreement between two directional models", 
    "clean_text": "Liang et al (2006) propose agreement-based learning, which jointly learns probabilities by maximizing a combination of likelihood and agreement between two directional models.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "P14-1139", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Yin-Wen, Chang | Alexander M., Rush | John, DeNero | Michael John, Collins", 
    "raw_text": "Training is performed using the agreement-based learning method which encourages the directional models to overlap (Liang et al, 2006)", 
    "clean_text": "Training is performed using the agreement-based learning method which encourages the directional models to overlap (Liang et al, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "W12-3115", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Meritxell, Gonz&aacute;lez | Llu&iacute;s, M&agrave;rquez | Daniele, Pighin", 
    "raw_text": "Concerning the former, we trained an unsupervised model with the Berkeleyaligner4, an implementation of the symmetric word-alignment model described by Liang et al (2006)", 
    "clean_text": "Concerning the former, we trained an unsupervised model with the Berkeley aligner, an implementation of the symmetric word-alignment model described by Liang et al (2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "D11-1149", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Wei, Lu | Hwee Tou, Ng", 
    "raw_text": "The state-of-the-art unsupervised Berkeley aligner (Liang et al, 2006) with default setting is used to construct word alignments", 
    "clean_text": "The state-of-the-art unsupervised Berkeley aligner (Liang et al, 2006) with default setting is used to construct word alignments.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "P11-1103", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Alexandra, Birch | Miles, Osborne", 
    "raw_text": "We used version two of the Berkeley alignment model (Liang et al, 2006), with the posterior threshold set at 0.5", 
    "clean_text": "We used version two of the Berkeley alignment model (Liang et al, 2006), with the posterior threshold set at 0.5.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "P11-1103", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Alexandra, Birch | Miles, Osborne", 
    "raw_text": "The reordering metrics require alignments which were created using the Berkeley word alignment package version 1.1 (Liang et al, 2006), with the posterior probability to being 0.5.We first extracted the LRscore Kendall? s tau distance from the monotone for the Chinese-Englishtest set and this value was 66.1%", 
    "clean_text": "The reordering metrics require alignments which were created using the Berkeley word alignment package version 1.1 (Liang et al., 2006), with the posterior probability to being 0.5.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "N10-1014", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Adam, Pauls | Dan, Klein | David, Chiang | Kevin, Knight", 
    "raw_text": "As is standard in unsupervised alignment mod els, we initialized the translation parameters pt by first training 5 iterations of IBM Model 1 using the joint training algorithm of Liang et al (2006), and then trained our model for 5 EM iterations", 
    "clean_text": "As is standard in unsupervised alignment models, we initialized the translation parameters pt by first training 5 iterations of IBM Model 1 using the joint training algorithm of Liang et al (2006), and then trained our model for 5 EM iterations.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "W12-3105", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Maja, Popovi&#x107; | Ond&#x159;ej, Bojar | Mark, Fishel | Rico, Sennrich", 
    "raw_text": "Previous evaluation of Addicter shows that hypothesis-reference alignment coverage (in terms of discovered word pairs) directly influences error analysis quality; to increase alignment cover age we used Berkeley aligner (Liang et al, 2006) and trained it on and applied it to the whole set of reference-hypothesis pairs for every language pair", 
    "clean_text": "Previous evaluation of Addicter shows that hypothesis-reference alignment coverage (in terms of discovered word pairs) directly influences error analysis quality; to increase alignment coverage we used Berkeley aligner (Liang et al, 2006) and trained it on and applied it to the whole set of reference-hypothesis pairs for every language pair.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "W08-0303", 
    "citing_paper_authority": 18, 
    "citing_paper_authors": "Jan, Niehues | Stephan, Vogel", 
    "raw_text": "They could reach an AER of 3.8 on the same task, but only if they also included the posteriors of the model of Liang et al (2006)", 
    "clean_text": "They could reach an AER of 3.8 on the same task, but only if they also included the posteriors of the model of Liang et al (2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "D10-1065", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Jinxi, Xu | Antti-Veikko I., Rosti", 
    "raw_text": "We used two well studied unsupervised aligners, GIZA++ (Och and Ney, 2003) and HMM (Liang et al, 2006) and one supervised aligner, ITG (Haghighi et al, 2009) as representatives in this work", 
    "clean_text": "We used two well studied unsupervised aligners, GIZA++ (Och and Ney, 2003) and HMM (Liang et al, 2006) and one supervised aligner, ITG (Haghighi et al, 2009) as representatives in this work.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "D10-1065", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Jinxi, Xu | Antti-Veikko I., Rosti", 
    "raw_text": "We used three aligners in this work: GIZA++ (Och and Ney, 2003), jointly trained HMM (Liang et al, 2006), and ITG (Haghighi et al, 2009)", 
    "clean_text": "We used three aligners in this work: GIZA++ (Och and Ney, 2003), jointly trained HMM (Liang et al, 2006), and ITG (Haghighi et al, 2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "D10-1065", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Jinxi, Xu | Antti-Veikko I., Rosti", 
    "raw_text": "The HMM aligner used in this work was due to Liang et al (2006)", 
    "clean_text": "The HMM aligner used in this work was due to Liang et al (2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "P07-1039", 
    "citing_paper_authority": 17, 
    "citing_paper_authors": "Yanjun, Ma | Nicolas, Stroppa | Andy, Way", 
    "raw_text": "First, it is really difficult to build a reliable and objective gold-standard set, especially for languages as different as Chinese and English.Second, an increase in AER does not necessarily imply an improvement in translation quality (Liang et al., 2006) and vice-versa (Vilar et al, 2006)", 
    "clean_text": "Second, an increase in AER does not necessarily imply an improvement in translation quality (Liang et al., 2006) and vice-versa (Vilar et al, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "D08-1011", 
    "citing_paper_authority": 26, 
    "citing_paper_authors": "Xiaodong, He | Mei, Yang | Jianfeng, Gao | Patrick, Nguyen | Robert C., Moore", 
    "raw_text": "(5) As suggested by Liang et al (2006), we can group the distortion parameters{ c (d)} ,d= i i &apos;, into a few buckets", 
    "clean_text": "As suggested by Liang et al (2006), we can group the distortion parameters into a few buckets.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "D08-1084", 
    "citing_paper_authority": 17, 
    "citing_paper_authors": "Bill, MacCartney | Michel, Galley | Christopher D., Manning", 
    "raw_text": "On the RTE2 test set, the asymmetric alignment from H to P scored 68% in F1; GROW scored 58%; and all other alternatives scored below 52% .As an additional experiment, we tested the CrossEM aligner (Liang et al, 2006) from the BerkeleyAligner package on the MSR data", 
    "clean_text": "As an additional experiment, we tested the Cross EM aligner (Liang et al, 2006) from the Berkeley Aligner package on the MSR data.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "P12-1002", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Patrick, Simianer | Stefan, Riezler | Chris, Dyer", 
    "raw_text": "Exceptions where discriminative SMT has been used on large training data are Liang et al (2006a) who trained 1.5 million features on 67,000 sentences, Blunsom et al", 
    "clean_text": "Exceptions where discriminative SMT has been used on large training data are Liang et al (2006a) who trained 1.5 million features on 67,000 sentences.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "P12-1002", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Patrick, Simianer | Stefan, Riezler | Chris, Dyer", 
    "raw_text": "HI MID LOW Figure 3: Multipartite pairwise ranking. Training data for discriminative learning are prepared by comparing a 100-best list of translations against a single reference using smoothed per sentence BLEU (Liang et al, 2006a)", 
    "clean_text": "Training data for discriminative learning are prepared by comparing a 100-best list of translations against a single reference using smoothed per sentence BLEU (Liang et al, 2006a).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "N10-1140", 
    "citing_paper_authority": 16, 
    "citing_paper_authors": "Michel, Galley | Christopher D., Manning", 
    "raw_text": "We performed word alignment using a cross EM word aligner (Liang et al, 2006)", 
    "clean_text": "We performed word alignment using a cross EM word aligner (Liang et al, 2006).", 
    "keep_for_gold": 0
  }
]