[
  {
    "citance_No": 1, 
    "citing_paper_id": "D07-1091", 
    "citing_paper_authority": 91, 
    "citing_paper_authors": "Philipp, Koehn | Hieu, Hoang", 
    "raw_text": "Factored translation models have also been used for the integration of CCG super tags (Birch et al, 2007), domain adaptation (Koehn and Schroeder, 2007) and for the improvement of English-Czech translation (Bojar, 2007)", 
    "clean_text": "Factored translation models have also been used for the integration of CCG supertags (Birch et al, 2007), domain adaptation (Koehn and Schroeder, 2007) and for the improvement of English-Czech translation (Bojar, 2007).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P13-2058", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Felix, Hieber | Laura, Jehl | Stefan, Riezler", 
    "raw_text": "The novel aspect of task alternation introduced in this paper can be applied to all approaches incorporating SMT for sentence retrieval from comparable data. For our baseline system we use in-domain language models (Bertoldi and Federico, 2009) and meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007)", 
    "clean_text": "The novel aspect of task alternation introduced in this paper can be applied to all approaches incorporating SMT for sentence retrieval from comparable data. For our baseline system we use in-domain language models (Bertoldi and Federico, 2009) and meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "W08-0327", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Lo&iuml;c, Dugast | Jean, Senellart | Philipp, Koehn", 
    "raw_text": "Work on domain adaptation for statistical machine translation (Koehn and Schroeder, 2007) tries to bring solutions to this issue", 
    "clean_text": "Work on domain adaptation for statistical machine translation (Koehn and Schroeder, 2007) tries to bring solutions to this issue.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "W09-0421", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Maria, Holmqvist | Sara, Stymne | Jody, Foo | Lars, Ahrenberg", 
    "raw_text": "Based on Koehn and Schroeder (2007) we adapted our system from last year, which was focused on Europarl, to perform well on test data 2Machinese syntax, from Connexor Oyhttp: //www", 
    "clean_text": "Based on Koehn and Schroeder (2007) we adapted our system from last year, which was focused on Europarl, to perform well on test data.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "W09-0405", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Yu, Chen | Michael, Jellinghaus | Andreas, Eisele | Yi, Zhang | Sabine, Hunsicker | Silke, Theison | Christian, Federmann | Hans, Uszkoreit", 
    "raw_text": "Although the Moses decoder is able to work with two phrase tables at once (Koehn and Schroeder, 2007), it is difficult to use this method when there is more than one additional model", 
    "clean_text": "Although the Moses decoder is able to work with two phrase tables at once (Koehn and Schroeder, 2007), it is difficult to use this method when there is more than one additional model.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "W08-0321", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Nguyen, Bach | Qin, Gao | Stephan, Vogel", 
    "raw_text": "Genre adaptation is one of the major challenges in statistical machine translation since translation models suffer from data sparseness (Koehn and Schroeder, 2007)", 
    "clean_text": "Genre adaptation is one of the major challenges in statistical machine translation since translation models suffer from data sparseness (Koehn and Schroeder, 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "W11-2211", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Matthias, Huck | David, Vilar | Daniel, Stein | Hermann, Ney", 
    "raw_text": "Combining multiple translation models has been investigated for domain adaptation by Foster and Kuhn (2007) and Koehn and Schroeder (2007) before", 
    "clean_text": "Combining multiple translation models has been investigated for domain adaptation by Foster and Kuhn (2007) and Koehn and Schroeder (2007) before.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "E12-1083", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Katharina, Waeschle | Stefan, Riezler", 
    "raw_text": "The pooled model for pairing data from abstracts and claims is trained on data composed of 250,000 sentences from each text section. Another approach to exploit commonalities between tasks is to train separate language and translation models9 on the sentences from each task and combine the models in the global log-linear model of the SMT framework, following Foster and Kuhn (2007) and Koehn and Schroeder (2007)", 
    "clean_text": "The pooled model for pairing data from abstracts and claims is trained on data composed of 250,000 sentences from each text section. Another approach to exploit commonalities between tasks is to train separate language and translation models on the sentences from each task and combine the models in the global log-linear model of the SMT framework, following Foster and Kuhn (2007) and Koehn and Schroeder (2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "N10-1062", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Abby, Levenberg | Chris, Callison-Burch | Miles, Osborne", 
    "raw_text": "See, for example, Koehn and Schroeder (2007) or Bertoldi and Federico (2009)", 
    "clean_text": "See, for example, Koehn and Schroeder (2007) or Bertoldi and Federico (2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "W11-2130", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Barry, Haddow | Abhishek, Arun | Philipp, Koehn", 
    "raw_text": "Separate 5-gram language models were built from the target side of the two data set sand then they were interpolated using weights chosen to minimise the perplexity on the tuning set (Koehn and Schroeder, 2007)", 
    "clean_text": "Separate 5-gram language models were built from the target side of the two data set sand then they were interpolated using weights chosen to minimise the perplexity on the tuning set (Koehn and Schroeder, 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "W09-0414", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Jos&eacute; A. R., Fonollosa | Maxim, Khalilov | Marta Ruiz, Costa-Juss&agrave; | Jos&eacute;e B., Mari&ntilde;o | Carlos A., Henr&iacute;quez Q. | Adolfo, Hern&aacute;ndez H. | Rafael E., Banches", 
    "raw_text": "We implemented a TM interpolation strategy following the ideas proposed in (Schwenk and Est ?ve, 2008), where the authors present a promising technique of target LMs linear interpolation; in (Koehn and Schroeder, 2007) where a log-linear combination of TMs is performed; and specifically in (Foster and Kuhn, 2007) where the authors present various ways of TM combination and analyze in detail the TM domain adaptation", 
    "clean_text": "We implemented a TM interpolation strategy following the ideas proposed in (Schwenk and Estve, 2008), where the authors present a promising technique of target LMs linear interpolation; in (Koehn and Schroeder, 2007) where a log-linear combination of TMs is performed; and specifically in (Foster and Kuhn, 2007) where the authors present various ways of TM combination and analyze in detail the TM domain adaptation.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "C10-2124", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Germ&aacute;n, Sanchis | Francisco, Casacuberta", 
    "raw_text": "In (Koehn and Schroeder, 2007), different ways to combine available data belonging to two different sources was explored; in (Bertoldi and Federico, 2009) similar experiments were performed, but cons id ering only additional source data", 
    "clean_text": "In (Koehn and Schroeder, 2007), different ways to combine available data belonging to two different sources was explored; in (Bertoldi and Federico, 2009) similar experiments were performed, but considering only additional source data.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "W10-1759", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Kashif, Shah | Lo&iuml;c, Barrault | Holger, Schwenk", 
    "raw_text": "(Koehn and Schroeder, 2007) used two language models and two translation models: one in-domain and other out-of-domain to adapt the system", 
    "clean_text": "(Koehn and Schroeder, 2007) used two language models and two translation models: one in-domain and other out-of-domain to adapt the system.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "P13-1126", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Boxing, Chen | Roland, Kuhn | George, Foster", 
    "raw_text": "(Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework", 
    "clean_text": "(Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "P13-1126", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Boxing, Chen | Roland, Kuhn | George, Foster", 
    "raw_text": "One is the log-linear combination of TMs trained on each sub corpus (Koehn and Schroeder, 2007), with weights of each model tuned under minimal error rate training using MIRA", 
    "clean_text": "One is the log-linear combination of TMs trained on each subcorpus (Koehn and Schroeder, 2007), with weights of each model tuned under minimal error rate training using MIRA.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "W11-2133", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Nick, Ruiz | Marcello, Federico", 
    "raw_text": "Researchers such as Foster and Kuhn (2007 )andKoehn and Schroeder (2007) have investigated mixture model approaches to adaptation", 
    "clean_text": "Researchers such as Foster and Kuhn (2007) and Koehn and Schroeder (2007) have investigated mixture model approaches to adaptation.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "W11-2133", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Nick, Ruiz | Marcello, Federico", 
    "raw_text": "Koehn and Schroeder (2007) learn mixture weights for language models trained with in-domain and out of-domain data respectively by minimizing the perplexity of a tuning (development) set and interpolating the models", 
    "clean_text": "Koehn and Schroeder (2007) learn mixture weights for language models trained with in-domain and out of-domain data respectively by minimizing the perplexity of a tuning (development) set and interpolating the models.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "W12-3153", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Laura, Jehl | Felix, Hieber | Stefan, Riezler", 
    "raw_text": "Further approaches to domain adaptation for SMTinclude adaptation using in-domain language mod els (Bertoldi and Federico, 2009), meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007), or translation model adaptation 5http: //www.turk.com 411 using self-translations of in-domain source language texts (Ueffing et al, 2007)", 
    "clean_text": "Further approaches to domain adaptation for SMT include adaptation using in-domain language models (Bertoldi and Federico, 2009), meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007), or translation model adaptation using self-translations of in-domain source language texts (Ueffing et al, 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "W12-3153", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Laura, Jehl | Felix, Hieber | Stefan, Riezler", 
    "raw_text": "There were three different adaptation measures: First, the turker-generated development set was used for optimizing the weights of the decoding meta parameters, as introduced by Koehn and Schroeder (2007)", 
    "clean_text": "There were three different adaptation measures: First, the turker-generated development set was used for optimizing the weights of the decoding meta parameters, as introduced by Koehn and Schroeder (2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "W12-3153", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Laura, Jehl | Felix, Hieber | Stefan, Riezler", 
    "raw_text": "We also compared the effect of using only the in-domain language model to that of adding the in-domain language model as an extra feature while keeping the NIST language model.17 There was no signif 17The weights for both language models were optimized along with all other translation feature weights, rather than running an extra optimization step to interpolate between both language models, since Koehn and Schroeder (2007) showed that 415 Run Translation Model Language Model Dev Set BLEU% 1 NIST NIST NIST 13.90", 
    "clean_text": "", 
    "keep_for_gold": 0
  }
]