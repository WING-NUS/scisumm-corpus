[
  {
    "citance_No": 1, 
    "citing_paper_id": "P14-2131", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Mohit, Bansal | Kevin, Gimpel | Karen, Livescu", 
    "raw_text": "Several researchers have made their trained representations publicly avail 809 Representation Source Corpus Types, Tokens V D Time BROWN Koo et al (2008) BLLIP 317K, 43M 316,710? 2.5 days? SENNA Collobert et al (2011) Wikipedia 8.3M, 1.8B 130,000 50 2 months? TURIAN Turian et al (2010) RCV1 269K, 37M 268,810 50 few weeks? HUANG Huang et al (2012) Wikipedia 8.3M, 1.8B 100,232 50? CBOW, SKIP, SKIP DEP Mikolov et al (2013a) BLLIP 317K, 43M 316,697 100 24mins", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P13-1088", 
    "citing_paper_authority": 14, 
    "citing_paper_authors": "Karl Moritz, Hermann | Philip, Blunsom", 
    "raw_text": "We conclude with some qualitative analysis to get a better idea of whether the combination of CCG and RAE can learn semantically expressive embeddings. In our experiments we use the hyperbolic tan gent as nonlinearity g. Unless stated otherwise we use word-vectors of size 50, initialized using the embeddings provided by Turian et al (2010) based on the model of Collobert and Weston (2008) .4 We use the C& amp; C parser (Clark and Curran, 2007) to generate CCG parse trees for the data used in our experiments", 
    "clean_text": "Unless stated otherwise we use word-vectors of size 50, initialized using the embeddings provided by Turian et al (2010) based on the model of Collobert and Weston (2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P13-1088", 
    "citing_paper_authority": 14, 
    "citing_paper_authors": "Karl Moritz, Hermann | Philip, Blunsom", 
    "raw_text": "Experiment 1: Semi-Supervised Training In the first experiment, we use the semi-supervised training strategy described previously and initialize our models with the embeddings provided byTurian et al (2010)", 
    "clean_text": "In the first experiment, we use the semi-supervised training strategy described previously and initialize our models with the embeddings provided by Turian et al (2010).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "P13-1017", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Nan, Yang | Shujie, Liu | Mu, Li | Ming, Zhou | Nenghai, Yu", 
    "raw_text": "Follow (Turian et al., 2010), we randomly initialize all parameters to [-0.1, 0.1], and use stochastic gradient descent to minimize the ranking loss with a fixed learning rate 0.01", 
    "clean_text": "Follow (Turian et al., 2010), we randomly initialize all parameters to [-0.1, 0.1], and use stochastic gradient descent to minimize the ranking loss with a fixed learning rate 0.01.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "W12-1908", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Oscar, T&auml;ckstr&ouml;m", 
    "raw_text": "Just as monolingual word clusters are broadly applicable as feature sin monolingual models for linguistic structure prediction (Turian et al, 2010), the resulting cross-lingual word clusters can be used as features in various cross lingual direct transfer models", 
    "clean_text": "Just as monolingual word clusters are broadly applicable as feature sin monolingual models for linguistic structure prediction (Turian et al, 2010), the resulting cross-lingual word clusters can be used as features in various cross lingual direct transfer models.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "W12-1908", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Oscar, T&auml;ckstr&ouml;m", 
    "raw_text": "256 cross-lingual word clusters and the same feature templates as Ta ?ckstro? m et al (2012), with the exception that the transition factors are not conditioned on the input.3 The features used are similar to those used by Turian et al (2010), but include cross-lingual rather than monolingual word clusters", 
    "clean_text": "256 cross-lingual word clusters and the same feature templates as Ta?ckstro?m et al (2012), with the exception that the transition factors are not conditioned on the input. The features used are similar to those used by Turian et al (2010), but include cross-lingual rather than monolingual word clusters.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "P11-2008", 
    "citing_paper_authority": 67, 
    "citing_paper_authors": "Kevin, Gimpel | Nathan, Schneider | Brendan, O'Connor | Dipanjan, Das | Daniel P., Mills | Jacob, Eisenstein | Michael, Heilman | Dani, Yogatama | Jeffrey, Flanigan | Noah A., Smith", 
    "raw_text": "Each term? s feature vector is its row in U; following Turian et al (2010), we standardize and scale the standard deviation to 0.1", 
    "clean_text": "Each term's feature vector is its row in U; following Turian et al. (2010), we standardize and scale the standard deviation to 0.1.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "S12-1101", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Sneha, Jha | Hansen A., Schwartz | Lyle, Ungar", 
    "raw_text": "Further details and evaluations of these embeddings are discussed in Turian et al (2010)", 
    "clean_text": "Further details and evaluations of these embeddings are discussed in Turian et al (2010).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "P14-1062", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Nal, Kalchbrenner | Edward, Grefenstette | Philip, Blunsom", 
    "raw_text": "5.2. As the dataset is rather small, we use lower-dimensional word vectors with d= 32 that are initialised with embed dings trained in an unsupervised way to predict contexts of occurrence (Turian et al, 2010)", 
    "clean_text": "As the dataset is rather small, we use lower-dimensional word vectors with d=32 that are initialised with embeddings trained in an unsupervised way to predict contexts of occurrence (Turian et al, 2010).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "P14-2012", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Thien Huu, Nguyen | Ralph, Grishman", 
    "raw_text": "Each dimension of the word em beddings expresses a latent feature of the words, hopefully reflecting useful semantic and syntactic regularities (Turian et al, 2010)", 
    "clean_text": "Each dimension of the word embeddings expresses a latent feature of the words, hopefully reflecting useful semantic and syntactic regularities (Turian et al, 2010).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "P14-2012", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Thien Huu, Nguyen | Ralph, Grishman", 
    "raw_text": "Weevalu ate C& amp; W word embeddings with 25, 50 and 100 dimensions as well as HLBL word embeddings with 50 and 100 dimensions that are introduced in Turian et al (2010) and can be downloaded here 4. The fact that we utilize the large, general", 
    "clean_text": "We evaluate C&W word embeddings with 25, 50 and 100 dimensions as well as HLBL word embeddings with 50 and 100 dimensions that are introduced in Turian et al (2010) and can be downloaded here.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "P14-2012", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Thien Huu, Nguyen | Ralph, Grishman", 
    "raw_text": "This is because the RCV1 corpus used to induce the word embeddings (Turian et al, 2010) does not cover spoken language words in cts very well", 
    "clean_text": "This is because the RCV1 corpus used to induce the word embeddings (Turian et al, 2010) does not cover spoken language words in cts very well.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "P14-2037", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Tom\u00c3\u00a1\u00c5\u00a1, Ko\u00c4\u008disk\u00c3\u00bd | Karl Moritz, Hermann | Philip, Blunsom", 
    "raw_text": "Thesuccess of distributed approaches to a number of tasks, such as listed above, supports this notion and its implied benefits (see also Turian et al (2010) and Collobert and Weston (2008)) .While most work employing distributed representations has focused on monolingual tasks ,multilingual representations would also be useful for several NLP-related tasks", 
    "clean_text": "The success of distributed approaches to a number of tasks, such as listed above, supports this notion and its implied benefits (see also Turian et al (2010) and Collobert and Weston (2008)).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "P14-1022", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "David, Hall | Greg, Durrett | Dan, Klein", 
    "raw_text": "A given preterminal unary at position i inthe sentence includes features on the words (suf fixes) at position i? 1, i, and i+ 1. Because the lexicon is especially sensitive to morphological effects, we also fire features on all prefixes and suf 1 Experiments with the Brown clusters (Brown et al, 1992) provided by Turian et al (2010) in lieu of suffixes werenot promising", 
    "clean_text": "Experiments with the Brown clusters (Brown et al, 1992) provided by Turian et al (2010) in lieu of suffixes were not promising.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "W11-0315", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Fei, Huang | Alexander, Yates | Arun, Ahuja | Doug, Downey", 
    "raw_text": "Following Turian et al (2010), we use Percy Liang? s implementation of this algorithm for our comparison, and we test runs with 100, 320, and 1000 clusters", 
    "clean_text": "Following Turian et al (2010), we use Percy Liang's implementation of this algorithm for our comparison, and we test runs with 100, 320, and 1000 clusters.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "P12-1092", 
    "citing_paper_authority": 41, 
    "citing_paper_authors": "Eric H., Huang | Andrew Y., Ng | Christopher D., Manning | Richard, Socher", 
    "raw_text": "We downloaded these embeddings from Turian et al (2010)", 
    "clean_text": "We downloaded these embeddings from Turian et al (2010).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "P12-1092", 
    "citing_paper_authority": 41, 
    "citing_paper_authors": "Eric H., Huang | Andrew Y., Ng | Christopher D., Manning | Richard, Socher", 
    "raw_text": "Our model uses a similar neural network architecture as these models and uses the ranking-loss training objective proposed by Collobert and Weston (2008), but introduces a new way to combine local and global context to train word embeddings. Besides language modeling, word embeddings induced by neural language models have been useful in chunking, NER (Turian et al, 2010), parsing (Socher et al, 2011b), sentiment analysis (Socher et al., 2011c) and paraphrase detection (Socher et al,2011a)", 
    "clean_text": "Besides language modeling, word embeddings induced by neural language models have been useful in chunking, NER (Turian et al, 2010), parsing (Socher et al, 2011b), sentiment analysis (Socher et al., 2011c) and paraphrase detection (Socher et al, 2011a).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "W11-2155", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Christian, Rishj | Anders, S&oslash;gaard", 
    "raw_text": "In the present work, we went with the pre-trained models for jUnsupos3, which have the following characteristics4: Lang Corpus# Sents# Tags cs LCC 4 M 539de Wortschatz 40 M 396 en Medline 2004 34 M 480 es LCC 4.5 M 415 fr LCC 3 M 359For the Brown algorithm, we are contrasting cluster count choices of 320 and 1000, based on reports of other successful applications [Turian et al, 2010] 5, with clustering models trained on monolingual data from the Europarl corpus and the News Commentary corpus", 
    "clean_text": "For the Brown algorithm, we are contrasting cluster count choices of 320 and 1000, based on reports of other successful applications [Turian et al, 2010], with clustering models trained on monolingual data from the Europarl corpus and the News Commentary corpus.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "P11-1097", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Stefan, R&uuml;d | Massimiliano, Ciaramita | Jens, M&uuml;ller | Hinrich, Sch&uuml;tze", 
    "raw_text": "Turian et al (2010) show that adapting fromCoNLL to MUC-7 (Chinchor, 1998) data (thus between different newswire sources), the best unsupervised feature (Brown clusters) improves F1 from .68 to .79", 
    "clean_text": "Turian et al (2010) show that adapting from CoNLL to MUC-7 (Chinchor, 1998) data (thus between different newswire sources), the best unsupervised feature (Brown clusters) improves F1 from .68 to .79.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "P11-1097", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Stefan, R&uuml;d | Massimiliano, Ciaramita | Jens, M&uuml;ller | Hinrich, Sch&uuml;tze", 
    "raw_text": "The robustness of this simple approach is well documented ;e.g., Turian et al (2010) show that the baseline model (gazetteer features without unsupervised features) produces an F1 of .778 against .788 of the best unsupervised word representation feature", 
    "clean_text": "The robustness of this simple approach is well documented; e.g., Turian et al (2010) show that the baseline model (gazetteer features without unsupervised features) produces an F1 of .778 against .788 of the best unsupervised word representation feature.", 
    "keep_for_gold": 0
  }
]