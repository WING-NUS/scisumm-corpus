[
  {
    "citance_No": 1, 
    "citing_paper_id": "P06-2123", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Ruiqiang, Zhang | Genichiro, Kikui | Eiichiro, Sumita", 
    "raw_text": "In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al, 2005), using CRFs for the IOB tagging, yielded a very high R-oov in all of the four corpora used, but the R-iv rates were lower", 
    "clean_text": "In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al, 2005), using CRFs for the IOB tagging, yielded a very high R-oov in all of the four corpora used, but the R-iv rates were lower.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P06-2123", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Ruiqiang, Zhang | Genichiro, Kikui | Eiichiro, Sumita", 
    "raw_text": "Using the same approach as in (Tseng et al, 2005), we extracted the most frequent words tagged with? B?, indicating a prefix, and the last words tagged with? I?, denoting a suffix", 
    "clean_text": "Using the same approach as in (Tseng et al, 2005), we extracted the most frequent words tagged with 'B', indicating a prefix, and the last words tagged with 'I', denoting a suffix.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P14-1106", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Junhui, Li | Yuval, Marton | Philip, Resnik | Hal, Daum\u00c3\u00a9 III", 
    "raw_text": "For training we use 1.6M sentence pairs of the non-UN and non-HK Hansards portions of NISTMT training corpora, segmented with the Stanfordsegmenter (Tseng et al, 2005)", 
    "clean_text": "For training we use 1.6M sentence pairs of the non-UN and non-HK Hansards portions of NISTMT training corpora, segmented with the Stanford segmenter (Tseng et al, 2005).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "E09-1031", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Martin, Forst | Ji, Fang", 
    "raw_text": "(Tseng et al, 2005) achieve an SF of 95.0%, 95.3% and 86.3% on PKU data from the Sighan Bakeoff 2005, PKU data from the Sighan Bakeoff 2003 and CTB data from the Sighan Bakeoff 2003 respectively", 
    "clean_text": "(Tseng et al, 2005) achieve an SF of 95.0%, 95.3% and 86.3% on PKU data from the Sighan Bakeoff 2005, PKU data from the Sighan Bakeoff 2003 and CTB data from the Sighan Bakeoff 2003 respectively.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "N09-2054", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Zhongqiang, Huang | Vladimir, Eidelman | Mary P., Harper", 
    "raw_text": "While state-of-the-art tagging systems have achieved accuracies above 97% in English, Chinese POS tagging (Tseng et al, 2005b; Huang et al, 2007) has proven to be more challenging, and it is the focus of this study. The value of the latent variable approach for tagging is that it can learn more fine grained tags to better model the training data", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "N09-2054", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Zhongqiang, Huang | Vladimir, Eidelman | Mary P., Harper", 
    "raw_text": "They are selected from similar sources to the newswire articles, and arenormalized (Zhang and Kahn, 2008) and word segmented (Tseng et al, 2005a)", 
    "clean_text": "They are selected from similar sources to the newswire articles, and are normalized (Zhang and Kahn, 2008) and word segmented (Tseng et al, 2005a).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "P11-1033", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Bin, Lu | Chenhao, Tan | Claire, Cardie | Benjamin, K. Tsou", 
    "raw_text": "The Chinese sentences are segmented using the Stanford Chinese word segmenter (Tseng et al, 2005)", 
    "clean_text": "The Chinese sentences are segmented using the Stanford Chinese word segmenter (Tseng et al, 2005).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "P12-1097", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Chang, Liu | Hwee Tou, Ng", 
    "raw_text": "We use the Stanford Chinese word segmenter (Tseng et al, 2005) and POS tagger (Toutanova et al., 2003) for preprocessing and Cilin for synonym definition during matching", 
    "clean_text": "We use the Stanford Chinese word segmenter (Tseng et al, 2005) and POS tagger (Toutanova et al., 2003) for preprocessing and Cilin for synonym definition during matching.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "P12-1027", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Xu, Sun | Houfeng, Wang | Wenjie, Li", 
    "raw_text": "Pre Rec F-score MSR Best05 (Tseng et al, 2005)? 96.2 96.6 96.4 CRF+ rule-system (Zhang et al, 2006)? 97.2 96.9 97.1 Semi-Markovperceptron (Zhang and Clark, 2007)? N/A N/A 97.2 Semi-Markov CRF (Gao et al, 2007)? N/A N/A 97.2 Latent-variable CRF (Sun et al, 2009b)? 97.3 97.3 97.3 Our method (A Single CRF)? 97.6 97.2 97.4 CU Best05 (Tseng et al, 2005)? 94.1 94.6 94.3 CRF+ rule-system (Zhang et al, 2006)? 95.2 94.9 95.1 Semi-perceptron (Zhang and Clark, 2007)? N/A N/A 95.1 Latent-variable CRF (Sun et al, 2009b)? 94.7 94.4 94.6 Our method (A Single CRF)? 94.8 94.7 94.8 PKU Best05 (Chen et al, 2005) N/A 95.3 94.6 95.0 CRF+ rule-system (Zhang et al, 2006)? 94.7 95.5 95.1semi-perceptron (Zhang and Clark, 2007)? N/A N/A 94.5 Latent-variable CRF (Sun et al, 2009b)? 95.6 94.8 95.2 Our method (A Single CRF)? 95.8 94.9 95.4 Table 3: Comparing our method with the state-of-the-art CWS systems", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "P14-2016", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Gintar&egrave;, Grigonyt&egrave; | Timothy, Baldwin", 
    "raw_text": "Morphological segmentation for these two languages was carried out using MeCab (MeCab, 2011) and the Stanford Word Segmenter (Tseng et al, 2005), respectively", 
    "clean_text": "Morphological segmentation for these two languages was carried out using MeCab (MeCab, 2011) and the Stanford Word Segmenter (Tseng et al, 2005), respectively.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "W11-2128", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Yuval, Marton | Ahmed, El Kholy | Nizar, Habash", 
    "raw_text": "English-Chinese: For training we used the LDC Sinorama and FBIS tests (LDC2005T10 and LDC2003E14), and segmented the Chinese side with the Stanford Segmenter (Tseng et al, 2005)", 
    "clean_text": "English-Chinese: For training we used the LDC Sinorama and FBIS tests (LDC2005T10 and LDC2003E14), and segmented the Chinese side with the Stanford Segmenter (Tseng et al, 2005).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "I08-4028", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Xihong, Wu | Xiaojun, Lin | Xinhao, Wang | Chunyao, Wu | Yaozhong, Zhang | Dianhai, Yu", 
    "raw_text": "Recently, Maximum Entropy model (ME) and CRFs (Low et al, 2005) (Tseng et al, 2005) (HaiZhao et al, 2006) turned out to be promising in natural language processing tracks, and obtain excellent performances on most of the test corpora of Bakeoff 2005 and Bakeoff 2006", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "D09-1040", 
    "citing_paper_authority": 21, 
    "citing_paper_authors": "Yuval, Marton | Chris, Callison-Burch | Philip, Resnik", 
    "raw_text": "For the English-Chinese (E2C) baseline system, we trained on the LCD Sinorama and FBIStests (LCD2005T10 and LCD2003E14), and segmented the Chinese side with the Stanford Segmenter (Tseng et al, 2005)", 
    "clean_text": "For the English-Chinese (E2C) baseline system, we trained on the LCD Sinorama and FBIStests (LCD2005T10 and LCD2003E14), and segmented the Chinese side with the Stanford Segmenter (Tseng et al, 2005).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "I08-4015", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Zhenxing, Wang | Changning, Huang | Jingbo, Zhu", 
    "raw_text": "Since Chinese Word Segmentation was firstly treated as a character-based tagging task in (Xue and Converse, 2002), this method has been widely accepted and further developed by researchers (Peng et al, 2004), (Tseng et al, 2005), (Low et al., 2005), (Zhao et al, 2006)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "N12-1046", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Ferhan, Ture | Douglas W., Oard | Philip, Resnik", 
    "raw_text": "The Arabic text was preprocessed to produce two different segmentations (simple punctuation tokenization with orthographic normalization, and LDC? s ATBv3 representation (Maamouri et al,2008)), represented together using cdec? s lattice in put format (Dyer et al, 2008) .The Zh-En system was trained on parallel training text consisting of the non-UN portions and non HK Hansards portions of the NIST training corpora. Chinese was automatically segmented by the Stan ford segmenter (Tseng et al, 2005), and traditional characters were simplified", 
    "clean_text": "Chinese was automatically segmented by the Stanford segmenter (Tseng et al, 2005), and traditional characters were simplified.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "D11-1090", 
    "citing_paper_authority": 17, 
    "citing_paper_authors": "Weiwei, Sun | Jia, Xu", 
    "raw_text": "This character-by-character method was first proposed by (Xue, 2003), and a number of discriminative sequential learning algorithms have been exploited, including structured perceptron (Jiang et al, 2009), the Passive-Aggressive algorithm (Sun, 2010), conditional random fields (CRFs) (Tseng et al, 2005), and latent variable CRFs (Sun et al, 2009)", 
    "clean_text": "This character-by-character method was first proposed by (Xue, 2003), and a number of discriminative sequential learning algorithms have been exploited, including structured perceptron (Jiang et al, 2009), the Passive-Aggressive algorithm (Sun, 2010), conditional random fields (CRFs) (Tseng et al, 2005), and latent variable CRFs (Sun et al, 2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "W08-0304", 
    "citing_paper_authority": 11, 
    "citing_paper_authors": "Daniel, Cer | Daniel, Jurafsky | Christopher D., Manning", 
    "raw_text": "The Chinese data was word segmented using the GALE Y2 retest release of the Stanford CRFsegmenter (Tseng et al, 2005)", 
    "clean_text": "The Chinese data was word segmented using the GALE Y2 retest release of the Stanford CRF segmenter (Tseng et al, 2005).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "P13-1110", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Vladimir, Eidelman | Yuval, Marton | Philip, Resnik", 
    "raw_text": "For training we used the non-UN and non-HK Hansards portions of the NIST training corpora, which was segmented using the Stanfordsegmenter (Tseng et al, 2005)", 
    "clean_text": "For training we used the non-UN and non-HK Hansards portions of the NIST training corpora, which was segmented using the Stanford segmenter (Tseng et al, 2005).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "W08-0303", 
    "citing_paper_authority": 18, 
    "citing_paper_authors": "Jan, Niehues | Stephan, Vogel", 
    "raw_text": "The FBIS-corpus was used as training corpus and all Chinese sentences were word segmented with the Stanford Segmenter (Tseng et al, 2005)", 
    "clean_text": "The FBIS-corpus was used as training corpus and all Chinese sentences were word segmented with the Stanford Segmenter (Tseng et al, 2005).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "P11-2028", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Maoxi, Li | Chengqing, Zong | Hwee Tou, Ng", 
    "raw_text": "Stanford Chinese word segmenter (STANFORD): The Stanford Chinese word seg menter is another well-known CWS tool (Tseng et al., 2005)", 
    "clean_text": "Stanford Chinese word segmenter (STANFORD): The Stanford Chinese word segmenter is another well-known CWS tool (Tseng et al., 2005).", 
    "keep_for_gold": 0
  }
]