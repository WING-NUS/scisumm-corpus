[
  {
    "citance_No": 1, 
    "citing_paper_id": "W06-1422", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Ehud, Reiter | Anja, Belz", 
    "raw_text": "In our work (Belz and Reiter, 2006), we used several different evaluation techniques (human and corpus-based) to evaluate the output of five NLG systems which generated wind descriptions for weather forecasts", 
    "clean_text": "In our work (Belz and Reiter, 2006), we used several different evaluation techniques (human and corpus-based) to evaluate the output of five NLG systems which generated wind descriptions for weather forecasts.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "W06-1422", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Ehud, Reiter | Anja, Belz", 
    "raw_text": "As we showed previously (Belz and Reiter, 2006) that there are significant inter-subject differences in ratings, one thing we want to determine is how many subjects are needed to get reliable and reproducible results. Task performance", 
    "clean_text": "As we showed previously (Belz and Reiter, 2006) that there are significant inter-subject differences in ratings, one thing we want to determine is how many subjects are needed to get reliable and reproducible results.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P14-1116", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Dimitra, Gkatzia | Helen, Hastie | Oliver, Lemon", 
    "raw_text": "A previous study byBelz and Reiter (2006) has demonstrated that automatic metrics can correlate highly with human ratings if the training dataset is of high quality", 
    "clean_text": "A previous study by Belz and Reiter (2006) has demonstrated that automatic metrics can correlate highly with human ratings if the training dataset is of high quality.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "W08-1119", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Ehud, Reiter | Albert, Gatt | Francois, Portet | Marian, van der Meulen", 
    "raw_text": "Data-to-texts systems have been evaluated in a number of ways, including human ratings (the most common technique) (Reiter et al, 2005), BLEU-like scores against human texts (Belz and Reiter, 2006), post-edit analyses (Sripada et al, 2005), and persuasive effectiveness (Carenini and Moore, 2006) .However, again to the best of our knowledge no previous data-to-text system has been evaluated by asking users to make decisions based on the generated texts, and measuring the quality of these decisions", 
    "clean_text": "Data-to-texts systems have been evaluated in a number of ways, including human ratings (the most common technique) (Reiter et al, 2005), BLEU-like scores against human texts (Belz and Reiter, 2006), post-edit analyses (Sripada et al, 2005), and persuasive effectiveness (Carenini and Moore, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "N07-1021", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Anja, Belz", 
    "raw_text": "3.4.1 Evaluation methods The two automatic metrics used in the evaluations, NIST and BLEU have been shown to correlate highly with expert judgments (Pearson correlation coefficients 0.82 and 0.79 respectively) in this do main (Belz and Reiter, 2006)", 
    "clean_text": "The two automatic metrics used in the evaluations, NIST and BLEU have been shown to correlate highly with expert judgments (Pearson correlation coefficients 0.82 and 0.79 respectively) in this domain (Belz and Reiter, 2006).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "N07-1021", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Anja, Belz", 
    "raw_text": "The first was an experiment with 9 subjects experienced in reading marine fore casts (Belz and Reiter, 2006), the second is a new experiment with 14 similarly experienced subjects3", 
    "clean_text": "The first was an experiment with 9 subjects experienced in reading marine forecasts (Belz and Reiter, 2006), the second is a new experiment with 14 similarly experienced subjects.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "W09-0604", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Erwin, Marsi | Emiel, Krahmer | Iris, Hendrickx | Walter, Daelemans", 
    "raw_text": "However, the important thing to keep in mind is that compression of a given sentence is a problem for which there are usually multiple solutions (Belz and Reiter, 2006)", 
    "clean_text": "However, the important thing to keep in mind is that compression of a given sentence is a problem for which there are usually multiple solutions (Belz and Reiter, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "W08-1113", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Mary Ellen, Foster", 
    "raw_text": "unlike, for example, the corpus used by Belz and Reiter (2006), where the human judges sometimes preferred generated output to the corpus data", 
    "clean_text": "This suggests that, for this generation task, the data in the corpus can indeed be treated as a gold standard - unlike, for example, the corpus used by Belz and Reiter (2006), where the human judges sometimes preferred generated output to the corpus data.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "P09-2025", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Aoife, Cahill", 
    "raw_text": "Belz and Reiter (2006) and Reiter and Belz (2009) describe comparison experiments between the automatic evaluation of system output and human (expert and non-expert) evaluation of the same data (English weather forecasts)", 
    "clean_text": "Belz and Reiter (2006) and Reiter and Belz (2009) describe comparison experiments between the automatic evaluation of system output and human (expert and non-expert) evaluation of the same data (English weather forecasts).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "N07-1022", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Yuk Wah, Wong | Raymond J., Mooney", 
    "raw_text": "While it is found that BLEU and NIST correlate quite well with human judgments in evaluating NLG systems (Belz and Reiter, 2006), it is best to support these figures with human evaluation, which we did on a small scale", 
    "clean_text": "While it is found that BLEU and NIST correlate quite well with human judgments in evaluating NLG systems (Belz and Reiter, 2006), it is best to support these figures with human evaluation, which we did on a small scale.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "W09-0603", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Anja, Belz | Eric, Kow", 
    "raw_text": "The two automatic metrics used in the evaluations, NIST2 and BLEU3, have been shown to correlate well with expert judgments (Pearson? s r= 0.82 and 0.79 respectively) in the SUMTIME domain (Belz and Reiter, 2006)", 
    "clean_text": "The two automatic metrics used in the evaluations, NIST2 and BLEU3, have been shown to correlate well with expert judgments (Pearson's r= 0.82 and 0.79 respectively) in the SUMTIME domain (Belz and Reiter, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "E06-1045", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Mary Ellen, Foster | Jon, Oberlander", 
    "raw_text": "When Belz and Reiter (2006 )performeda similar study comparing natural-language generation systems that used different text-planning strategies, they also found similar results: auto mated measures tended to favour majority-choice strategies, while human judges preferred those that made weighted choices", 
    "clean_text": "When Belz and Reiter (2006) performed a similar study comparing natural-language generation systems that used different text-planning strategies, they also found similar results: automated measures tended to favour majority-choice strategies, while human judges preferred those that made weighted choices.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "E06-1045", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Mary Ellen, Foster | Jon, Oberlander", 
    "raw_text": "opinions of the behaviours annotated in the corpus: it may be that subjects actually prefer the generated facial displays to the displays in the corpus, as was foundby Belz and Reiter (2006)", 
    "clean_text": "it may be that subjects actually prefer the generated facial displays to the displays in the corpus, as was found by Belz and Reiter (2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "E09-1014", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Aoife, Cahill | Martin, Forst", 
    "raw_text": "Belz and Reiter (2006) carry out a comparison of automatic evaluation metrics against human domain experts and human non-experts in the domain of weather forecast statements", 
    "clean_text": "Belz and Reiter (2006) carry out a comparison of automatic evaluation metrics against human domain experts and human non-experts in the domain of weather forecast statements.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "P13-1138", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Ravikumar, Kondadadi | Blake Stephen, Howald | Frank, Schilder", 
    "raw_text": "The variability of the generated texts ranges from a close similarity to slightly shorter not an uncommon (Belz and Reiter, 2006), but not necessarily detrimental, observation for NLG systems (van Deemter et al, 2005)", 
    "clean_text": "The variability of the generated texts ranges from a close similarity to slightly shorter - not an uncommon (Belz and Reiter, 2006), but not necessarily detrimental, observation for NLG systems (van Deemter et al, 2005).", 
    "keep_for_gold": 0
  }
]