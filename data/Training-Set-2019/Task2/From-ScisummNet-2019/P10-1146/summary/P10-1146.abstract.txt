Statistical translation models that try to capture the recursive structure of language have been widely adopted over the last few years. 
These models make use of varying amounts of information from linguistic theory: some use none at all, some use information about the grammar of the target language, some use information about the grammar of the source language. 
But progress has been slower on translation models that are able to learn the relationship between the grammars of both the source and target language. 
We discuss the reasons why this has been a challenge, review existing attempts to meet this challenge, and show how some old and new ideas can be combined into a simple approach that uses both source and target syntax for significant improvements in translation accuracy.
