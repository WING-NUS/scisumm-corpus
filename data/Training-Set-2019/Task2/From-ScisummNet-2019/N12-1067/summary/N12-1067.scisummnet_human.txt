Better Evaluation for Grammatical Error Correction
We present a novel method for evaluating grammatical error correction.
The core of our method, which we call MaxMatch (M2), is an algorithm for efficiently computing the sequence of phrase-level edits between a source sentence and a system hypothesis that achieves the highest overlap with the gold-standard annotation.
This optimal edit sequence is subsequently scored using F1 measure.
We test our M2 scorer on the Helping Our Own (HOO) shared task data and show that our method results in more accurate evaluation for grammatical error correction.
We propose an alternative evaluation scheme which operates in terms of tokens rather than character offsets.
