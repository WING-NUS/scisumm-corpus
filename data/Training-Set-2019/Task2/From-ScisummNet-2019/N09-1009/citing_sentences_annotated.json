[
  {
    "citance_No": 1, 
    "citing_paper_id": "D12-1121", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Vasant, Honavar | Kewei, Tu", 
    "raw_text": "Cohen et al (2008) and Cohen and Smith (2009) employed the logistic normal prior to model the correlations between grammar symbols", 
    "clean_text": "Cohen et al (2008) and Cohen and Smith (2009) employed the logistic normal prior to model the correlations between grammar symbols.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P10-2034", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Tomoharu, Iwata | Daichi, Mochihashi | Hiroshi, Sawada", 
    "raw_text": "To our knowledge, the only grammar induction work on non-parallel corpora is (Cohen and Smith, 2009), but their method does not model a common grammar, and requires prior information such as part-of-speech tags", 
    "clean_text": "To our knowledge, the only grammar induction work on non-parallel corpora is (Cohen and Smith, 2009), but their method does not model a common grammar, and requires prior information such as part-of-speech tags.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "N10-1116", 
    "citing_paper_authority": 16, 
    "citing_paper_authors": "Valentin I., Spitkovsky | Hiyan, Alshawi | Daniel, Jurafsky", 
    "raw_text": "Althoughlarge amounts of unlabeled data are known to improve semi-supervised parsing (Suzuki et al, 2009), the best unsupervised systems use less data than is available for supervised training, relying on complex models instead: Headden et al? s (2009) Extended Valence Grammar (EVG) combats data sparsity with smoothing alone, training on the same small subset of the tree-bank as the classic implementation of theDMV; Cohen and Smith (2009) use more complicated algorithms (variational EM and MBRdecoding) and stronger linguistic hints (tying related parts of speech and syntactically similar bilingual data) .We explore what can be achieved through judicious use of data and simple, scalable techniques", 
    "clean_text": "Cohen and Smith (2009) use more complicated algorithms (variational EM and MBR decoding) and stronger linguistic hints (tying related parts of speech and syntactically similar bilingual data).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "D10-1117", 
    "citing_paper_authority": 20, 
    "citing_paper_authors": "Philip, Blunsom | Trevor, Cohn", 
    "raw_text": "Attach-Right 38.4 31.7 EM (Klein and Manning, 2004) 46.1 35.9 Dirichlet (Cohen et al, 2008) 46.1 36.9 LN (Cohen et al, 2008) 59.4 40.5 SLN, TIE V& amp; N (Cohen and Smith, 2009) 61.3 41.4DMV (Headden III et al, 2009) 55.7? =8.0 DMV smoothed (Headden III et al, 2009) 61.2? =1.2 EVG smoothed (Headden III et al, 2009) 65.0? =5.7 L-EVG smoothed (Headden III et al, 2009) 68.8? =4.5 Less is More (Spitkovsky et al, 2010a) 56.2 44.1 Leap Frog (Spitkovsky et al, 2010a) 57.1 45.0 Viterbi EM (Spitkovsky et al, 2010b) 65.3 47.9 Hypertext Markup (Spitkovsky et al, 2010c) 69.3 50.4Adaptor Grammar (Cohen et al, 2010) 50.2 TSG-DMV (Pcfg) 65.9? =2.4 53.1? =2.4 TSG-DMV (Pcfg, Psh) 65.1? =2.2 51.5? =2.0 LexTSG-DMV (Plcfg, Pcfg) 67.2? =1.4 55.2? =2.2 LexTSG-DMV (Plcfg, Pcfg, Psh) 67.7? =1.5 55.7? =2.0 Supervised MLE (Cohen and Smith, 2009) 84.5 68.8Table 4: Mean and variance for the head attachment accuracy of our TSG-DMV models (highlighted) with varying back off paths, and many other high performing models. Citations indicate where the model and result were re ported", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P11-1067", 
    "citing_paper_authority": 14, 
    "citing_paper_authors": "Roy, Schwartz | Omri, Abend | Roi, Reichart | Ari, Rappoport", 
    "raw_text": "Cohenand Smith (2009, 2010) further extended it by using a shared logistic normal prior which provided a new way to encode the knowledge that some POS tags are more similar than others", 
    "clean_text": "Cohen and Smith (2009, 2010) further extended it by using a shared logistic normal prior which provided a new way to encode the knowledge that some POS tags are more similar than others.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "P11-1067", 
    "citing_paper_authority": 14, 
    "citing_paper_authors": "Roy, Schwartz | Omri, Abend | Roi, Reichart | Ari, Rappoport", 
    "raw_text": "We experimented with three DMV-based algorithms: a replication of (Klein and Manning, 2004), as appears in (Cohen et al, 2008) (henceforth ,km04), Cohen and Smith (2009) (henceforth ,cs09), and Spitkovsky et al", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "P11-1067", 
    "citing_paper_authority": 14, 
    "citing_paper_authors": "Roy, Schwartz | Omri, Abend | Roi, Reichart | Ari, Rappoport", 
    "raw_text": "bbdk10: (Berg-Kirkpatrick et al, 2010) ,bc10: (Blunsom and Cohn, 2010) ,cs09: (Cohen and Smith, 2009) ,gggtp10: (Gillenwater et al, 2010) ,km04: A replication of (Klein and Manning, 2004) ,saj10a: (Spitkovsky et al, 2010a) ,saj10c: (Spitkovsky et al, 2010c) ,saj10b?: A lightly supervised algorithm (Spitkovsky et al, 2010b)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "P10-1131", 
    "citing_paper_authority": 18, 
    "citing_paper_authors": "Taylor, Berg-Kirkpatrick | Dan, Klein", 
    "raw_text": "Cohen and Smith (2009) present a model for jointly learning English and Chinesedependency grammars without bi texts", 
    "clean_text": "Cohen and Smith (2009) present a model for jointly learning English and Chinese dependency grammars without bitexts.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "P10-1131", 
    "citing_paper_authority": 18, 
    "citing_paper_authors": "Taylor, Berg-Kirkpatrick | Dan, Klein", 
    "raw_text": "Whilesome choices of prior structure can greatly complicate inference (Cohen and Smith, 2009), we choose a hierarchical Gaussian form for the drift term, which allows the gradient of the observed data likelihood to be easily computed using standard dynamic programming methods", 
    "clean_text": "While some choices of prior structure can greatly complicate inference (Cohen and Smith, 2009), we choose a hierarchical Gaussian form for the drift term, which allows the gradient of the observed data likelihood to be easily computed using standard dynamic programming methods.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "P10-1131", 
    "citing_paper_authority": 18, 
    "citing_paper_authors": "Taylor, Berg-Kirkpatrick | Dan, Klein", 
    "raw_text": "While this progression of model structure is similar to that explored in Cohen and Smith (2009), Cohen and Smith saw their largest improvements from tying together parameters for the varieties of coarse parts-of-speech monolinugally, and then only moderate improvements from allowing cross-linguistic influence on top of monolingual sharing", 
    "clean_text": "While this progression of model structure is similar to that explored in Cohen and Smith (2009), Cohen and Smith saw their largest improvements from tying together parameters for the varieties of coarse parts-of-speech monolinugally, and then only moderate improvements from allowing cross-linguistic influence on top of monolingual sharing.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "P10-1131", 
    "citing_paper_authority": 18, 
    "citing_paper_authors": "Taylor, Berg-Kirkpatrick | Dan, Klein", 
    "raw_text": "Evaluating our LINGUISTIC model on the same test sets as (Cohen and Smith, 2009), sentences of length10 or less in section 23 of PTB and sections 271 300 of CTB, we achieved an accuracy of 56.6 for Chinese and 60.3 for English", 
    "clean_text": "Evaluating our LINGUISTIC model on the same test sets as (Cohen and Smith, 2009), sentences of length 10 or less in section 23 of PTB and sections 271 - 300 of CTB, we achieved an accuracy of 56.6 for Chinese and 60.3 for English.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "P10-1131", 
    "citing_paper_authority": 18, 
    "citing_paper_authors": "Taylor, Berg-Kirkpatrick | Dan, Klein", 
    "raw_text": "The best models of Cohen and Smith (2009) achieved accuracies of 52.0 and 62.0 respectively on these same test sets", 
    "clean_text": "The best models of Cohen and Smith (2009) achieved accuracies of 52.0 and 62.0 respectively on these same test sets.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "N10-1083", 
    "citing_paper_authority": 47, 
    "citing_paper_authors": "Taylor, Berg-Kirkpatrick | Alexandre, Bouchard-C&ocirc;t&eacute; | John, DeNero | Dan, Klein", 
    "raw_text": "These are the same training, development, and test sets used by Cohen and Smith (2009)", 
    "clean_text": "These are the same training, development, and test sets used by Cohen and Smith (2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "N10-1083", 
    "citing_paper_authority": 47, 
    "citing_paper_authors": "Taylor, Berg-Kirkpatrick | Alexandre, Bouchard-C&ocirc;t&eacute; | John, DeNero | Dan, Klein", 
    "raw_text": "These models generate a hidden alignment vector z and an observed foreign sentence y, all conditioned on an observed English sentence e. The likelihood of both models takes the form: P (y ,z|e)=? j p (zj =i|zj? 1)? ?yj ,ei, ALIGN 4Using additional bilingual data, Cohen and Smith (2009) achieve an accuracy of 62.0 for English, and an accuracy of 52.0 for Chinese, still below our results", 
    "clean_text": "Using additional bilingual data, Cohen and Smith (2009) achieve an accuracy of 62.0 for English, and an accuracy of 52.0 for Chinese, still below our results.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "N10-1083", 
    "citing_paper_authority": 47, 
    "citing_paper_authors": "Taylor, Berg-Kirkpatrick | Alexandre, Bouchard-C&ocirc;t&eacute; | John, DeNero | Dan, Klein", 
    "raw_text": "Model Inference Reg Eval POS Induction? Many-1 W SJ Basic-HMM EM? 63.1 (1.3) Feature-MRF LBFGS 0.1 59.6 (6.9) Feature-HMM EM 1.0 68.1 (1.7) LBFGS 1.0 75.5 (1.1) Grammar Induction? Dir W SJ 10 Basic-DMV EM? 47.8 Feature-DMV EM 0.05 48.3 LBFGS 10.0 63.0 (Cohen and Smith, 2009) 61.3 C T B 10 Basic-DMV EM? 42.5 Feature-DMV EM 1.0 49.9 LBFGS 5.0 53.6 (Cohen and Smith, 2009) 51.9 Word Alignment? AER N IS T ChE n Basic-Model 1 EM? 38.0 Feature-Model 1 EM? 35.6 Basic-HMM EM? 33.8 Feature-HMM EM? 30.0 Word Segmentation? F1 B R Basic-Unigram EM? 76.9 (0.1) Feature-Unigram EM 0.2 84.5 (0.5) LBFGS 0.2 88.0 (0.1) (Johnson and Goldwater, 2009) 87 Table 1: Locally normalized feature-based models outperform all proposed baselines for all four tasks", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "D10-1005", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "Jordan, Boyd-Graber | Philip, Resnik", 
    "raw_text": "This has been done successfully in multilingual settings (Cohen and Smith, 2009)", 
    "clean_text": "This has been done successfully in multilingual settings (Cohen and Smith, 2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "D10-1067", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Roi, Reichart | Ari, Rappoport", 
    "raw_text": "To day? s best unsupervised dependency parsers, which are rooted in this model, train on short sentences only: both Headen et al, (2009) and Cohen and Smith (2009) train on WSJ10 even when the test set includes longer sentences", 
    "clean_text": "Today's best unsupervised dependency parsers, which are rooted in this model, train on short sentences only: both Headen et al., (2009) and Cohen and Smith (2009) train on WSJ10 even when the test set includes longer sentences.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "N10-1081", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Shay B., Cohen | David M., Blei | Noah A., Smith", 
    "raw_text": "Choosing such distributions is motivated by their ability to make the variational bound tight (similar to Cohen et al, 2008, and Cohen and Smith, 2009)", 
    "clean_text": "Choosing such distributions is motivated by their ability to make the variational bound tight (similar to Cohen et al, 2008, and Cohen and Smith, 2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "N10-1081", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Shay B., Cohen | David M., Blei | Noah A., Smith", 
    "raw_text": "This work was supported by the following grants: ONR 175-6343 and NSF CAREER 0745520 to Blei; NSF IIS-0836431 and IIS-0915187 to Smith.7The performance of Cohen and Smith (2009), like the performance of Headden et al (2009), is greater than what we re port, but those developments are orthogonal to the contributions of this paper", 
    "clean_text": "The performance of Cohen and Smith (2009), like the performance of Headden et al (2009), is greater than what we report, but those developments are orthogonal to the contributions of this paper.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "D11-1018", 
    "citing_paper_authority": 13, 
    "citing_paper_authors": "John, DeNero | Jakob, Uszkoreit", 
    "raw_text": "Some success in this area has been demonstrated via generative models (Klein and Manning, 2002), which often benefit from well chosen priors (Cohen and Smith, 2009) or posterior constraints (Ganchev et al, 2009)", 
    "clean_text": "Some success in this area has been demonstrated via generative models (Klein and Manning, 2002), which often benefit from well chosen priors (Cohen and Smith, 2009) or posterior constraints (Ganchev et al, 2009).", 
    "keep_for_gold": 0
  }
]