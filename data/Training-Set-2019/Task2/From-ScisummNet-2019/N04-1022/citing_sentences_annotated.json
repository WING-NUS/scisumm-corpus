[
  {
    "citance_No": 1, 
    "citing_paper_id": "P13-2075", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Nan, Duan", 
    "raw_text": "Minimum Bayes Risk (MBR) techniques have been successfully applied to a wide range of natural language processing tasks, such as statistical machine translation (Kumar and Byrne, 2004), automatic speech recognition (Goel and Byrne, 2000), parsing (Titov and Henderson, 2006), etc. This work makes further exploration along this line of research, by applying MBR technique to question answering (QA) .The function of a typical factoid question answering system is to automatically give answers to questions in most case asking about entities, which usually consists of three key components :question understanding, passage retrieval, and answer extraction", 
    "clean_text": "Minimum Bayes Risk (MBR) techniques have been successfully applied to a wide range of natural language processing tasks, such as statistical machine translation (Kumar and Byrne, 2004), automatic speech recognition (Goel and Byrne, 2000), parsing (Titov and Henderson, 2006), etc.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "W10-1756", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Abhishek, Arun | Barry, Haddow | Philipp, Koehn", 
    "raw_text": "This solution is often referred to as the Minimum Bayes Risk (MBR) solution (Kumar and Byrne,2004)", 
    "clean_text": "This solution is often referred to as the Minimum Bayes Risk (MBR) solution (Kumar and Byrne,2004).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "D07-1005", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Shankar, Kumar | Franz Josef, Och | Wolfgang, Macherey", 
    "raw_text": "This list is then rescored using Minimum Bayes-Risk (MBR) decoding (Kumar and Byrne, 2004)", 
    "clean_text": "This list is then rescored using Minimum Bayes-Risk (MBR) decoding (Kumar and Byrne, 2004).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "P09-1064", 
    "citing_paper_authority": 12, 
    "citing_paper_authors": "John, DeNero | David, Chiang | Kevin, Knight", 
    "raw_text": "The corresponding minimum Bayes risk (MBR) procedure maximizes the expected similarity score of a system? s translations relative to the model? s distribution over possible translations (Kumar and Byrne, 2004)", 
    "clean_text": "The corresponding minimum Bayes risk (MBR) procedure maximizes the expected similarity score of a system 's translations relative to the model 's distribution over possible translations (Kumar and Byrne, 2004).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P11-1127", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Jes&uacute;s, Gonz&aacute;lez-Rubio | Alfons, Juan | Francisco, Casacuberta", 
    "raw_text": "Consensus decoding procedures select translations for a single system with a mini mum Bayes risk (MBR) (Kumar and Byrne, 2004)", 
    "clean_text": "Consensus decoding procedures select translations for a single system with a minimum Bayes risk (MBR) (Kumar and Byrne, 2004).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "P11-1127", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Jes&uacute;s, Gonz&aacute;lez-Rubio | Alfons, Juan | Francisco, Casacuberta", 
    "raw_text": "In SMT, MBR decoding allows to minimize the loss of the output for a single translation system.MBR is generally implemented by re-ranking an N best list of translations produced by a first pass decoder (Kumar and Byrne, 2004)", 
    "clean_text": "In SMT, MBR decoding allows to minimize the loss of the output for a single translation system. MBR is generally implemented by re-ranking an N best list of translations produced by a first pass decoder (Kumar and Byrne, 2004).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "P11-1127", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Jes&uacute;s, Gonz&aacute;lez-Rubio | Alfons, Juan | Francisco, Casacuberta", 
    "raw_text": "For each system, we report the performance of max-derivation decoding (MAX) and 1000-best3 MBR decoding (Kumar and Byrne, 2004)", 
    "clean_text": "For each system, we report the performance of max-derivation decoding (MAX) and 1000-best3 MBR decoding (Kumar and Byrne, 2004).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "W10-1710", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Christian, Hardmeier | Arianna, Bisazza | Marcello, Federico", 
    "raw_text": "Finally, we used Minimum Bayes Risk decoding (Kumar and Byrne, 2004) based on the BLEU score (Papineni et al, 2002)", 
    "clean_text": "Finally, we used Minimum Bayes Risk decoding (Kumar and Byrne, 2004) based on the BLEU score (Papineni et al, 2002).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "W12-3136", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Ahmed, Thabet | Francisco, Guzman | Stephan, Vogel | Preslav, Nakov", 
    "raw_text": "We experimented with two decoding settings: (1) monotone at punctuation reordering (Tillmannand Ney, 2003), and (2) minimum Bayes risk decoding (Kumar and Byrne, 2004)", 
    "clean_text": "We experimented with two decoding settings: (1) monotone at punctuation reordering (Tillmannand Ney, 2003), and (2) minimum Bayes risk decoding (Kumar and Byrne, 2004).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "W09-0426", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Chris, Dyer | Hendra, Setiawan | Yuval, Marton | Philip, Resnik", 
    "raw_text": "Although during minimum error training we assume a decoder that uses the maximum derivation decision rule, we find benefits to translating using a minimum risk decision rule on a test set (Kumar and Byrne, 2004)", 
    "clean_text": "Although during minimum error training we assume a decoder that uses the maximum derivation decision rule, we find benefits to translating using a minimum risk decision rule on a test set (Kumar and Byrne, 2004).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "W11-2160", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Jonathan, Weese | Juri, Ganitkevitch | Chris, Callison-Burch | Matt, Post | Adam, Lopez", 
    "raw_text": "We decoded the test set to produce a 300-best list of unique translations, then chose the best candidate for each sentence using Minimum Bayes Risk re ranking (Kumar and Byrne, 2004)", 
    "clean_text": "We decoded the test set to produce a 300-best list of unique translations, then chose the best candidate for each sentence using Minimum Bayes Risk reranking (Kumar and Byrne, 2004).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "W10-1757", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Kevin, Duh | Katsuhito, Sudoh | Hajime, Tsukada | Hideki, Isozaki | Masaaki, Nagata", 
    "raw_text": "Modifying the multi task objective to incorporate application-specific loss/decoding, such as Minimum Bayes Risk (Kumar and Byrne, 2004)? Using multi task learning to aid large-scale feature engineering and visualization", 
    "clean_text": "Modifying the multitask objective to incorporate application-specific loss/decoding, such as Minimum Bayes Risk (Kumar and Byrne, 2004).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "W11-2139", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Chris, Dyer | Kevin, Gimpel | Jonathan H., Clark | Noah A., Smith", 
    "raw_text": "The system submitted for manual evaluation used segment-level MBR decoding with 1? BLEU as the loss function, approximated over a 500-best list for each sentence. This reliably results in a small but consistent improvement in translation quality, but is much more time consuming to compute (Kumar and Byrne, 2004)", 
    "clean_text": "This reliably results in a small but consistent improvement in translation quality, but is much more time consuming to compute (Kumar and Byrne, 2004).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "W09-0416", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Jinhua, Du | Yifan, He | Sergio, Penkale | Andy, Way", 
    "raw_text": "For the system combination task, we first use the minimum Bayes-risk (MBR) (Kumar and Byrne, 2004) decoder to select the best hypo the sis as the alignment reference for the Confusion Network (CN) (Mangu et al, 2000)", 
    "clean_text": "For the system combination task, we first use the minimum Bayes-risk (MBR) (Kumar and Byrne, 2004) decoder to select the best hypothesis as the alignment reference for the Confusion Network (CN) (Mangu et al, 2000).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "W10-1727", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Sara, Stymne | Maria, Holmqvist | Lars, Ahrenberg", 
    "raw_text": "To these systems we added minimum Bayes risk (MBR) decoding (Kumar and Byrne, 2004)", 
    "clean_text": "To these systems we added minimum Bayes risk (MBR) decoding (Kumar and Byrne, 2004).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "P13-2071", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Nadir, Durrani | Alexander, Fraser | Helmut, Schmid | Hieu, Hoang | Philipp, Koehn", 
    "raw_text": "Moses Baseline: We trained a Moses system (Koehn et al, 2007) with the following settings: maximum sentence length 80 ,grow-diag-final and symmetrization of GIZA++ alignments, an interpolated KneserNey smoothed 5-gramlanguage model with KenLM (Heafield, 2011) used at runtime ,msd-bidirectional-felexicalized reordering, sparse lexical and domain features (Hasler et al, 2012), distortion limit of 6, 100-best translation options, minimum bayes-risk decoding (Kumar and Byrne, 2004), cube-pruning (Huangand Chiang, 2007) and the no-reordering-over punctuation heuristic", 
    "clean_text": "Moses Baseline: We trained a Moses system (Koehn et al, 2007) with the following settings: maximum sentence length 80, grow-diag-final and symmetrization of GIZA++ alignments, an interpolated KneserNey smoothed 5-gram language model with KenLM (Heafield, 2011) used at runtime, msd-bidirectional-felexicalized reordering, sparse lexical and domain features (Hasler et al, 2012), distortion limit of 6, 100-best translation options, minimum bayes-risk decoding (Kumar and Byrne, 2004), cube-pruning (Huangand Chiang, 2007) and the no-reordering-over punctuation heuristic.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "W12-3131", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Michael, Denkowski | Greg, Hanneman | Alon, Lavie", 
    "raw_text": "Thisdevelopment set is chosen for its known stability and reliability. Our baseline translation system uses Viterbide coding while our final system uses segment-level Minimum Bayes-Risk decoding (Kumar and Byrne, 2004) over 500-best lists using 1 BLEU as the loss function", 
    "clean_text": "Our baseline translation system uses Viterbi decoding while our final system uses segment-level Minimum Bayes-Risk decoding (Kumar and Byrne, 2004) over 500-best lists using 1 BLEU as the loss function.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "W12-3131", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Michael, Denkowski | Greg, Hanneman | Alon, Lavie", 
    "raw_text": "With large training data, moving to a 5-gram language model ,increasing the cube pruning pop limit to 1000, and using Minimum Bayes-Risk decoding (Kumar and Byrne, 2004) over 500-best lists collectively show a slight improvement", 
    "clean_text": "With large training data, moving to a 5-gram language model, increasing the cube pruning pop limit to 1000, and using Minimum Bayes-Risk decoding (Kumar and Byrne, 2004) over 500-best lists collectively show a slight improvement.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "C10-1036", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Nan, Duan | Mu, Li | Dongdong, Zhang | Ming, Zhou", 
    "raw_text": "Kumar and Byrne (2004) first introduced MBR decoding to SMT field and developed it on the N-best list translations", 
    "clean_text": "Kumar and Byrne (2004) first introduced MBR decoding to SMT field and developed it on the N-best list translations.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "W09-0424", 
    "citing_paper_authority": 47, 
    "citing_paper_authors": "Zhifei, Li | Chris, Callison-Burch | Chris, Dyer | Sanjeev P., Khudanpur | Lane, Schwartz | Wren N. G., Thornton | Jonathan, Weese | Omar F., Zaidan", 
    "raw_text": "The translation scores for four different systems are reported in Table 1.5 Baseline: In this system, we use the GIZA++toolkit (Och and Ney, 2003), a suffix-array architecture (Lopez, 2007), the SRILM toolkit (Stolcke, 2002), and minimum error rate training (Och, 2003) to obtain word-alignments, a translation model, language models, and the optimal weights for combining these models, respectively. Minimum Bayes Risk Rescoring: In this system, we re-ranked the n-best output of our base line system using Minimum Bayes Risk (Kumarand Byrne, 2004)", 
    "clean_text": "Minimum Bayes Risk Rescoring: In this system, we re-ranked the n-best output of our baseline system using Minimum Bayes Risk (Kumarand Byrne, 2004).", 
    "keep_for_gold": 0
  }
]