Learning To Recognize Features Of Valid Textual Entailments
This paper advocates a new architecture for textual inference in which finding a good alignment is separated from evaluating entailment.
Current approaches to semantic inference in question answering and textual entailment have approximated the entailment problem as that of computing the best alignment of the hypothesis to the text, using a locally decomposable matching score.
We argue that there are significant weaknesses in this approach, including flawed assumptions of monotonicity and locality.
Instead we propose a pipelined approach where alignment is followed by a classification step, in which we extract features representing high-level characteristics of the entailment problem, and pass the resulting feature vector to a statistical classifier trained on development data.
We report results on data from the 2005 Pascal RTE Challenge which surpass previously reported results for alignment-based systems.
We emphasize that there is more to inferential validity than close lexical or structural correspondence: negations, models, non-factive and implicative verbs, and other linguistic constructs can affect validity in ways hard to capture in alignment.
