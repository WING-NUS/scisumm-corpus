Broad-coverage annotated treebanks necessary to train parsers do not exist for many resource-poor languages. 
The wide availability of parallel text and accurate parsers in English has opened up the possibility of grammar induction through partial transfer across bitext. 
We consider generative and discriminative models for dependency grammar induction that use word-level alignments and a source language parser (English) to constrain the space of possible target trees. 
Unlike previous approaches, our framework does not require full projected parses, allowing partial, approximate transfer through linear expectation constraints on the space of distributions over trees. 
We consider several types of constraints that range from generic dependency conservation to language-specific annotation rules for auxiliary verb analysis. 
We evaluate our approach on Bulgarian and Spanish CoNLL shared task data and show that we consistently outperform unsupervised methods and can outperform supervised learning for limited training data.
