[
  {
    "citance_No": 1, 
    "citing_paper_id": "W05-1621", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Nikiforos, Karamanis | Chris S., Mellish", 
    "raw_text": "This methodology is very similar to the way [Barzilay and Lee, 2004] evaluate their probabilistic TS model in comparison to the approach of [Lapata, 2003]", 
    "clean_text": "This methodology is very similar to the way [Barzilay and Lee, 2004] evaluate their probabilistic TS model in comparison to the approach of [Lapata, 2003].", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "W05-1621", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Nikiforos, Karamanis | Chris S., Mellish", 
    "raw_text": "Thus, there might exist more than one equally good solution for TS, a view shared by almost all TS researchers, but which has not been accounted for in the evaluation methodologies of [Karamanis et al, 2004] and [Barzilay and Lee, 2004] .2 Collecting sentence orderings defined by many experts in our domain enables us to investigate the possibility that there might exist many good solutions for TS", 
    "clean_text": "Thus, there might exist more than one equally good solution for TS, a view shared by almost all TS researchers, but which has not been accounted for in the evaluation methodologies of [Karamanis et al, 2004] and [Barzilay and Lee, 2004].", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P14-1004", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Ke, Zhai | Jason, Williams", 
    "raw_text": "developed for discourse analysis by Barzilay and Lee (2004)", 
    "clean_text": "The simplest formulation we consider is an HMM where each state contains a unigram language model (LM), proposed by Chotimongkol (2008) for task-oriented dialogue and originally developed for discourse analysis by Barzilay and Lee (2004).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "D10-1037", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Christina, Sauper | Aria, Haghighi | Regina, Barzilay", 
    "raw_text": "In this work, we also assume a content model, which we fix to be the document-level HMM as used in Barzilay and Lee (2004)", 
    "clean_text": "In this work, we also assume a content model, which we fix to be the document-level HMM as used in Barzilay and Lee (2004).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "D08-1057", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Stephen, Wan | Robert, Dale | Mark, Dras | C&eacute;cile L., Paris", 
    "raw_text": "Corpus-based methods inspired by the notion of schemata have been explored in the past by Lapata (2003) and Barzilay and Lee (2004) for ordering sentences extracted in a multi-document sum marisation application", 
    "clean_text": "Corpus-based methods inspired by the notion of schemata have been explored in the past by Lapata (2003) and Barzilay and Lee (2004) for ordering sentences extracted in a multi-document summarisation application.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "D08-1057", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Stephen, Wan | Robert, Dale | Mark, Dras | C&eacute;cile L., Paris", 
    "raw_text": "Barzilay and Lee (2004) showed that it is possible to obtain schema-like knowledge automatically from a corpus for the purposes of extracting sentences and ordering them. However, their work represents patterns at the sentence level, and is thus not directly comparable to our work, given our focus on sentence generation", 
    "clean_text": "Barzilay and Lee (2004) showed that it is possible to obtain schema-like knowledge automatically from a corpus for the purposes of extracting sentences and ordering them. However, their work represents patterns at the sentence level, and is thus not directly comparable to our work, given our focus on sentence generation.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "D08-1057", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Stephen, Wan | Robert, Dale | Mark, Dras | C&eacute;cile L., Paris", 
    "raw_text": "Like Barzilay and Lee (2004), this model was used to order extracted sentences in summaries", 
    "clean_text": "Like Barzilay and Lee (2004), this model was used to order extracted sentences in summaries.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "P11-1100", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Ziheng, Lin | Hwee Tou, Ng | Min-Yen, Kan", 
    "raw_text": "Barzilay and Lee (2004) pro posed a domain-dependent HMM model to capture topic shift in a text, where topics are represented by hidden states and sentences are observations", 
    "clean_text": "Barzilay and Lee (2004) proposed a domain-dependent HMM model to capture topic shift in a text, where topics are represented by hidden states and sentences are observations.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "P06-1049", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Danushka, Bollegala | Naoaki, Okazaki | Mitsuru, Ishizuka", 
    "raw_text": "Although she has not compared her method with chronological ordering, it could be applied to generic domains, not relying on the chronological clue provided by news paper articles. Barzilay and Lee (2004) have proposed content models to deal with topic transition in domain specific text", 
    "clean_text": "Barzilay and Lee (2004) have proposed content models to deal with topic transition in domain specific text.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "N12-2010", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Annie, Louis", 
    "raw_text": "These methods identify regularities in words (Barzilay and Lee, 2004), entity co reference (Barzi lay and Lapata, 2008) and discourse relations (Pitlerand Nenkova, 2008) from a large collection of articles and use these patterns to predict the coherence", 
    "clean_text": "These methods identify regularities in words (Barzilay and Lee, 2004).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "D12-1049", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Huidong, Jin | Wray, Buntine | Lan, Du", 
    "raw_text": "To make a tool like the HMM work at higher levels, one needs to make stronger assumptions, for instance as signing each sentence a single topic and then topic specific word models can be used: the hidden topic Markov model (Gruber et al2007) that models the transitional topic structure; a global model based on the generalised Mallows model (Chen et al2009), and a HMM based content model (Barzilay and Lee, 2004)", 
    "clean_text": "To make a tool like the HMM work at higher levels, one needs to make stronger assumptions, for instance as signing each sentence a single topic and then topic specific word models can be used: the hidden topic Markov model (Gruber et al2007) that models the transitional topic structure; a global model based on the generalised Mallows model (Chen et al2009), and a HMM based content model (Barzilay and Lee, 2004).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "N09-3014", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Thade, Nahnsen", 
    "raw_text": "Approaches to sentence ordering can generally be categorized as knowledge-rich or knowledge-lean. Knowledge-rich approaches rely on manually created representations of sentence orderings using do 78 main communication knowledge. Barzilay and Lee (2004)? s knowledge-lean approach attempts to automate the inference of knowledge-rich information using a distributional view of content", 
    "clean_text": "Barzilay and Lee (2004)'s knowledge-lean approach attempts to automate the inference of knowledge-rich information using a distributional view of content.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "N09-3014", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Thade, Nahnsen", 
    "raw_text": "This is specific to their approach as both Lapata (2003)? s and Barzilay and Lee (2004)? s approaches are not tailored to summarization and therefore do not experience the topic bias problem", 
    "clean_text": "This is specific to their approach as both Lapata (2003)'s and Barzilay and Lee (2004)'s approaches are not tailored to summarization and therefore do not experience the topic bias problem.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "N09-3014", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Thade, Nahnsen", 
    "raw_text": "Coreference+Syntax+Salience+ and Coreference? Syntax+Salience+ are the Barzilay and Lapata (2008) model, HMM-based Content Models is theBarzilay and Lee (2004) paper and Latent Semantic Analysis is the Barzilay and Lapata (2008) implementation of Peter W. Foltz and Landauer (1998)", 
    "clean_text": "The results for Coreference+Syntax+Salience+ and HMM-Based Content Models are reproduced from Barzilay and Lapata (2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "N09-3014", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Thade, Nahnsen", 
    "raw_text": "When compared to the results obtained by Barzilay and Lapata (2008) and Barzilayand Lee (2004), it would appear that direct sentence to-sentence similarity (as suggested by the Barzilayand Lapata baseline score) or capturing topic sequences are essential for acquiring the correct sequence of sentences in the earthquake dataset", 
    "clean_text": "When compared to the results obtained by Barzilay and Lapata (2008) and Barzilay and Lee (2004), it would appear that direct sentence to-sentence similarity (as suggested by the Barzilay and Lapata baseline score) or capturing topic sequences are essential for acquiring the correct sequence of sentences in the earthquake dataset.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "S12-1030", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Michael, Roth | Anette, Frank", 
    "raw_text": "This assumption builds onthe success of previous research, where comparable and parallel texts have been exploited for a range of related learning tasks ,e.g., unsupervised discourse segmentation (Barzilay and Lee, 2004) and bootstrapping semantic analyzers (Titov and Kozhevnikov, 2010) .For our purposes, we are interested in finding corresponding PAS across comparable texts that are known to talk about the same events, and hence involve the same set of underlying event participants. By aligning predicates in such texts, we can investigate the factors that determine discourse coherence in the realization patterns for the involved participants", 
    "clean_text": "This assumption builds on the success of previous research, where comparable and parallel texts have been exploited for a range of related learning tasks, e.g., unsupervised discourse segmentation (Barzilay and Lee, 2004).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "N09-1041", 
    "citing_paper_authority": 51, 
    "citing_paper_authors": "Aria, Haghighi | Lucy, Vanderwende", 
    "raw_text": "However, as Barzilay and Lee (2004) ob serve, the content of document collections is highly structured, consisting of several topical themes, each with its own vocabulary and ordering preferences. For concreteness consider the DUC 2006document collection describing the opening of Star Wars: Episode 1 (see figure 2 (a))", 
    "clean_text": "However, as Barzilay and Lee (2004) observe, the content of document collections is highly structured, consisting of several topical themes, each with its own vocabulary and ordering preferences.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "W06-3309", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Jimmy, Lin | Damianos, Karakos | Dina, Demner-Fushman | Sanjeev P., Khudanpur", 
    "raw_text": "(Barzilayand Lee, 2004)", 
    "clean_text": "An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in (Barzilay and Lee, 2004).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "W06-3309", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Jimmy, Lin | Damianos, Karakos | Dina, Demner-Fushman | Sanjeev P., Khudanpur", 
    "raw_text": "Following Ruch et al (2003) and Barzilay and Lee (2004), we employed Hidden Markov Modelsto model the discourse structure of MEDLINE abstracts", 
    "clean_text": "Following Ruch et al (2003) and Barzilay and Lee (2004), we employed Hidden Markov Models to model the discourse structure of MEDLINE abstracts.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "W06-3309", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Jimmy, Lin | Damianos, Karakos | Dina, Demner-Fushman | Sanjeev P., Khudanpur", 
    "raw_text": "An interesting aspect of our generative approachis that we model HMM outputs as Gaussianvec tors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in (Barzilay and Lee,2004)", 
    "clean_text": "An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in (Barzilay and Lee,2004).", 
    "keep_for_gold": 0
  }
]