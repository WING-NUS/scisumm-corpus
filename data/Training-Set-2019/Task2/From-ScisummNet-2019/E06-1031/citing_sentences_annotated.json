[
  {
    "citance_No": 1, 
    "citing_paper_id": "W06-3112", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Karolina, Owczarzak | Declan, Groves | Josef, van Genabith | Andy, Way", 
    "raw_text": "Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al, 2006), which employs a version of edit distance for word substitution and reordering; METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy; and a linear regression model developed by (Russo-Lassner et al., 2005), which makes use of stemming, Word Net synonymy, verb class synonymy, matching noun phrase heads, and proper name matching", 
    "clean_text": "Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al, 2006), which employs a version of edit distance for word substitution and reordering; METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy; and a linear regression model developed by (Russo-Lassner et al., 2005), which makes use of stemming, Word Net synonymy, verb class synonymy, matching noun phrase heads, and proper name matching.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P14-2124", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Chi-kiu, Lo | Meriem, Beloucif | Markus, Saers | Dekai, Wu", 
    "raw_text": "Surface-form oriented metrics such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al, 2006), WER (Nie? en et al, 2000), and TER (Snover et al, 2006) do not correctly reflect the meaning similarities of the input sentence", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "W07-0734", 
    "citing_paper_authority": 45, 
    "citing_paper_authors": "Alon, Lavie | Abhaya, Agarwal", 
    "raw_text": "Meteor, as well as several other proposed metrics such as GTM (Melamed et al, 2003), TER (Snover et al, 2006) and CDER (Leusch et al, 2006) aim to address some of these weaknesses", 
    "clean_text": "Meteor, as well as several other proposed metrics such as GTM (Melamed et al, 2003), TER (Snover et al, 2006) and CDER (Leusch et al, 2006) aim to address some of these weaknesses.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "W07-0707", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Maja, Popovi&#x107; | Hermann, Ney", 
    "raw_text": "The CDERmea sure (Leusch et al, 2006) is based on edit distance, such as the well-known WER, but allows reordering of blocks", 
    "clean_text": "The CDER measure (Leusch et al, 2006) is based on edit distance, such as the well-known WER, but allows reordering of blocks.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "W12-4206", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Chi-kiu, Lo | Dekai, Wu", 
    "raw_text": "With such simple weighting scheme, the cost of evaluating translation of languages with fewer resources available is greatly re duced. For the past decade, the task of measuring the performance of MT systems has relied heavily on lexical n-gram based MT evaluation metrics, such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006) and WER (Nie? en et al, 2000) because of their support on fast and inexpensive evaluation", 
    "clean_text": "For the past decade, the task of measuring the performance of MT systems has relied heavily on lexical n-gram based MT evaluation metrics, such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006) and WER (Nieen et al, 2000) because of their support on fast and inexpensive evaluation.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "D07-1007", 
    "citing_paper_authority": 71, 
    "citing_paper_authors": "Marine, Carpuat | Dekai, Wu", 
    "raw_text": "In addition to the widely used BLEU (Papineni et al, 2002) and NIST (Doddington, 2002) scores, we also evaluate translation quality with the recently proposed Meteor (Banerjee and Lavie, 2005) and four edit-distance style metrics, Word Error Rate (WER), Position independent word Error Rate (PER) (Tillmann et al., 1997), CDER, which allows block reordering (Leusch et al, 2006), and Translation Edit Rate (TER) (Snover et al, 2006)", 
    "clean_text": "In addition to the widely used BLEU (Papineni et al, 2002) and NIST (Doddington, 2002) scores, we also evaluate translation quality with the recently proposed Meteor (Banerjee and Lavie, 2005) and four edit-distance style metrics, Word Error Rate (WER), Position independent word Error Rate (PER) (Tillmann et al., 1997), CDER, which allows block reordering (Leusch et al, 2006), and Translation Edit Rate (TER) (Snover et al, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "W12-3129", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Anand Karthik, Tumuluru | Chi-kiu, Lo | Dekai, Wu", 
    "raw_text": "For the past decade, MT evaluation has relied heavily on inexpensive automatic metrics such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006), WER (Nie? en et al, 2000), and TER (Snover et al, 2006)", 
    "clean_text": "For the past decade, MT evaluation has relied heavily on inexpensive automatic metrics such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006), WER (Nie?en et al, 2000), and TER (Snover et al, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "W12-3129", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Anand Karthik, Tumuluru | Chi-kiu, Lo | Dekai, Wu", 
    "raw_text": "Other lexical similarity based automatic MTevaluation metrics, like NIST (Doddington, 2002), ME TEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006), WER (Nie? en et al, 2000), and TER (Snover et al, 2006), also perform wellin capturing translation fluency, but share the same problem that although evaluation with these metrics can bedone very quickly at low cost, their underlying assump tion\u0014that a good translation is one that shares the same lexical choices as the reference translation\u0014is not justified semantically", 
    "clean_text": "Other lexical similarity based automatic MT evaluation metrics, like NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006), WER (Nie?en et al, 2000), and TER (Snover et al, 2006), also perform well in capturing translation fluency, but share the same problem that although evaluation with these metrics can be done very quickly at low cost, their underlying assumption that a good translation is one that shares the same lexical choices as the reference translation is not justified semantically.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "W08-0312", 
    "citing_paper_authority": 13, 
    "citing_paper_authors": "Abhaya, Agarwal | Alon, Lavie", 
    "raw_text": "Meteor, as well as several other proposed metrics such as GTM (Melamed et al, 2003), TER (Snover et al, 2006) and CDER (Leusch et al, 2006) aim to address some of these weaknesses", 
    "clean_text": "Meteor, as well as several other proposed metrics such as GTM (Melamed et al, 2003), TER (Snover et al, 2006) and CDER (Leusch et al, 2006) aim to address some of these weaknesses.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "W11-1002", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Chi-kiu, Lo | Dekai, Wu", 
    "raw_text": "For years, the task of measuring the performance of MT systems has been dominated by lexical ngram based machine translation evaluation metrics, such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006) and WER (Nie? en et al, 2000)", 
    "clean_text": "For years, the task of measuring the performance of MT systems has been dominated by lexical ngram based machine translation evaluation metrics, such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006) and WER (Nie?en et al, 2000).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "W07-0714", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Karolina, Owczarzak | Josef, van Genabith | Andy, Way", 
    "raw_text": "Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al, 2006), which employs a version of edit distance for word substitution and reordering; or METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy", 
    "clean_text": "Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al, 2006), which employs a version of edit distance for word substitution and reordering; or METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "W07-0411", 
    "citing_paper_authority": 13, 
    "citing_paper_authors": "Karolina, Owczarzak | Josef, van Genabith | Andy, Way", 
    "raw_text": "Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al, 2006), which employs a version of edit distance for word substitution and reordering; or METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "P13-2067", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Chi-kiu, Lo | Karteek, Addanki | Markus, Saers | Dekai, Wu", 
    "raw_text": "Tuning against edit distance based metrics such as CDER (Leusch et al., 2006), WER (Nie? en et al, 2000), and TER (Snover et al, 2006) also fails to sufficiently bias SMT systems towards producing translations that preserve semantic information", 
    "clean_text": "Tuning against edit distance based metrics such as CDER (Leusch et al., 2006), WER (Nie?en et al, 2000), and TER (Snover et al, 2006) also fails to sufficiently bias SMT systems towards producing translations that preserve semantic information.", 
    "keep_for_gold": 0
  }
]