Transformation Based Learning In The Fast Lane
Transformation-based learning has been successfully employed to solve many natural language processing problems.
It achieves state-of-the-art performance on many natural language processing tasks and does not overtrain easily.
However, it does have a serious drawback: the training time is often intorelably long, especially on the large corpora which are often used in NLP.
In this paper, we present a novel and realistic method for speeding up the training time of a transformation-based learner without sacrificing performance.
The paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems: a standard transformation-based learner, and the ICA system (Hepple, 2000).
The results of these experiments show that our system is able to achieve a significant improvement in training time while still achieving the same performance as a standard transformation-based learner.
This is a valuable contribution to systems and algorithms which utilize transformation-based learning at any part of the execution.
we propose fnTBL toolkit, which implements several optimizations in rule learning to drastically speed up the time needed for training.
