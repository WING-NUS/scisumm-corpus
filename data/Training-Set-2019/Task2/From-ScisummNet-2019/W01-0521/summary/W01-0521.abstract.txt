Most work in statistical parsing has focused on a single corpus: the Wall Street Journal portion of the Penn Treebank.
While this has allowed for quantitative comparison of parsing techniques, it has left open the question of how other types of text might affect parser performance, and how portable parsing models are across corpora.
We examine these questions by comparing results for the Brown and WSJ corpora, and also consider which parts of the parser's probability model are particularly tuned to the corpus on which it was trained.
This leads us to a technique for pruning parameters to reduce the size of the parsing model.
