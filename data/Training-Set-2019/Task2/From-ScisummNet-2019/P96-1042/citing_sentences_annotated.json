[
  {
    "citance_No": 1, 
    "citing_paper_id": "W97-0201", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Hwee Tou, Ng", 
    "raw_text": "For example, in (Engelson and Dagan, 1996), POS top 80% noun 5.14 verb 8.75 adj 5.87 adv 4.22 top 90% 4.48 6.89 4.75 3.79 top 95% 4.07 5.77 4.08 3.48 top 99% 3.51 4.53 3.47 2.96 bottom 20% bottom I0% 2.86 2.71 3.43 3.15 2.86 2.72 2.55 2.46 bottom 5% bottom 1% 2.59 2.44 2.94 2.67 2.63 2.44 2.38 2.31 Table 4: Average number of senses per polysemous word in the Brown corpus for the top 80%, ..., top 99%, and the bottom 20%,.", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "W03-0403", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Jason, Baldridge | Miles, Osborne", 
    "raw_text": "Active learning has been successfully applied to a number of natural language oriented tasks, including text categorization (Lewis and Gale, 1994) and part-of-speech tagging (Engelson and Dagan, 1996)", 
    "clean_text": "Active learning has been successfully applied to a number of natural language oriented tasks, including text categorization (Lewis and Gale, 1994) and part-of-speech tagging (Engelson and Dagan, 1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "W07-1516", 
    "citing_paper_authority": 16, 
    "citing_paper_authors": "Eric K., Ringger | Peter, McClanahan | Robbie, Haertel | George, Busby | Marc, Carmen | James, Carroll | Kevin, Seppi | Deryle, Lonsdale", 
    "raw_text": "Engelson and Dagan (1996) experimented with QBC using HMMs for POS tagging and found that selective sampling of sentences can significantly reduce the number of samples required to achieve desirable tag accuracies", 
    "clean_text": "Engelson and Dagan (1996) experimented with QBC using HMMs for POS tagging and found that selective sampling of sentences can significantly reduce the number of samples required to achieve desirable tag accuracies.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "W09-1118", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Fredrik, Olsson | Katrin, Tomanek", 
    "raw_text": "In all experiments, the agreement among the decision committee members is quantified by the Vote Entropy measure (Engelson and Dagan, 1996): V E (e)=? 1log k? l V (l, e) k log V (l, e) k (1) where k is the number of members in the committee, and V (l, e) is the number of members assigning label l to instance e. If an instance obtains a low Vote Entropy value, it means that the committee members are in high agreement concerning its classification, and thus also that it is less a informative one", 
    "clean_text": "In all experiments, the agreement among the decision committee members is quantified by the Vote Entropy measure (Engelson and Dagan, 1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "H05-1107", 
    "citing_paper_authority": 14, 
    "citing_paper_authors": "Chenhai, Xi | Rebecca, Hwa", 
    "raw_text": "Active learning also has been applied to many NLP applications, including POStagging (Engelson and Dagan, 1996) and pars 852ing (Baldridge and Osborne, 2003)", 
    "clean_text": "Active learning also has been applied to many NLP applications, including POS tagging (Engelson and Dagan, 1996) and parsing (Baldridge and Osborne, 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "C08-1109", 
    "citing_paper_authority": 44, 
    "citing_paper_authors": "Joel R., Tetreault | Martin, Chodorow", 
    "raw_text": "Estimated Overall Rates Sample Proportion* Sub-Corpus Proportion Hits 0.80* 0.10= 0.08 FP 0.20* 0.10= 0.02 Misses 0.30* 0.90= 0.27 Precision 0.08/ (0.08+ 0.02)= 0.80 Recall 0.08/ (0.08+ 0.27)= 0.23 Table 3: Sampling Calculations (Hypothetical) 870 This method is similar in spirit to active learning ((Dagan and Engelson, 1995) and (Engelson and Dagan, 1996)), which has been used to iteratively build up an annotated corpus, but it differs from active learning applications in that there are no iterative loops between the system and the human annotator (s)", 
    "clean_text": "This method is similar in spirit to active learning ((Dagan and Engelson, 1995) and (Engelson and Dagan, 1996)), which has been used to iteratively build up an annotated corpus, but it differs from active learning applications in that there are no iterative loops between the system and the human annotator(s).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "W07-1502", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Katrin, Tomanek | Joachim, Wermter | Udo, Hahn", 
    "raw_text": "A common metric to estimate the disagreement within an ensemble is the so-called vote entropy, the entropy of the distribution of labels li assigned to an example e by the ensemble of k classifiers (Engelson and Dagan, 1996): D (e)=? 1log k ?li V (li, e) k log V (li, e) k Our AL component employs such an ensemble based approach (Tomanek et al, 2007)", 
    "clean_text": "A common metric to estimate the disagreement within an ensemble is the so-called vote entropy, the entropy of the distribution of labels li assigned to an example e by the ensemble of k classifiers (Engelson and Dagan, 1996).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "P08-1098", 
    "citing_paper_authority": 8, 
    "citing_paper_authors": "Roi, Reichart | Katrin, Tomanek | Udo, Hahn | Ari, Rappoport", 
    "raw_text": "AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al, 2007) .However, AL is designed in such a way that it selects examples for manual annotation with respect to a single learning algorithm or classifier", 
    "clean_text": "AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al, 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "W99-0620", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Tadashi, Nomoto | Yuji, Matsumoto", 
    "raw_text": "Engelson and Dagan (1996) investigated several plausible approaches to the selection function but were unable to find significant differences among them", 
    "clean_text": "Engelson and Dagan (1996) investigated several plausible approaches to the selection function but were unable to find significant differences among them.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "D07-1051", 
    "citing_paper_authority": 24, 
    "citing_paper_authors": "Katrin, Tomanek | Joachim, Wermter | Udo, Hahn", 
    "raw_text": "QBC is based on the idea to select those examples for manual annotation on which a committee of classifiers disagree most in their predictions (Engelsonand Dagan, 1996)", 
    "clean_text": "QBC is based on the idea to select those examples for manual annotation on which a committee of classifiers disagree most in their predictions (Engelson and Dagan, 1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "D07-1051", 
    "citing_paper_authority": 24, 
    "citing_paper_authors": "Katrin, Tomanek | Joachim, Wermter | Udo, Hahn", 
    "raw_text": "This is measured by the vote entropy (Engelson and Dagan,1996) ,i.e., the entropy of the distribution of classifications assigned to an example by the classifiers", 
    "clean_text": "This is measured by the vote entropy (Engelson and Dagan, 1996), i.e., the entropy of the distribution of classifications assigned to an example by the classifiers.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "D07-1051", 
    "citing_paper_authority": 24, 
    "citing_paper_authors": "Katrin, Tomanek | Joachim, Wermter | Udo, Hahn", 
    "raw_text": "Engelson and Dagan (1996) confirm this observation that, in general, different (and even more refined) selection methods still yield similar results", 
    "clean_text": "Engelson and Dagan (1996) confirm this observation that, in general, different (and even more refined) selection methods still yield similar results.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "C10-2143", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Katrin, Tomanek | Udo, Hahn", 
    "raw_text": "While the efficiency of AL has already been shown for many NLP tasks based on measuring the number of tokens or sentences that are saved in comparison to random sampling (e.g., Engelson and Dagan (1996), Tomanek et al (2007) or Settles and Craven (2008)), it is obvious that just counting tokens under the assumption of uniform annotation costs for each token is empirically questionable, from a linguistic perspective, at least", 
    "clean_text": "", 
    "keep_for_gold": 0
  }
]