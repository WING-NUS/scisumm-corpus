[
  {
    "citance_No": 1, 
    "citing_paper_id": "W03-0201", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Andrew, Olney | Max, Louwerse | Eric, Matthews | Johanna, Marineau | Heather, Hite-Mitchell | Arthur C., Graesser", 
    "raw_text": "A bootstrapping approach using machine learning is a possible alternative that will be explored in the future (Abney 2002)", 
    "clean_text": "A bootstrapping approach using machine learning is a possible alternative that will be explored in the future (Abney 2002).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P12-2053", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Qi, Tan | Pei, Yang | Kam-Fai, Wong | Wei, Gao", 
    "raw_text": "d? D? CDW, CDL (d) |D| (5) Abney (2002) suggests that the disagreement rate of two independent hypotheses upper-bounds the error rate of either hypothesis", 
    "clean_text": "Abney (2002) suggests that the disagreement rate of two independent hypotheses upper-bounds the error rate of either hypothesis.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "W04-2405", 
    "citing_paper_authority": 12, 
    "citing_paper_authors": "Rada, Mihalcea", 
    "raw_text": "In recent work, (Abney,2002) shows that the independence assumption can be relaxed, and co-training is still effective under a weaker independence assumption", 
    "clean_text": "In recent work, (Abney, 2002) shows that the independence assumption can be relaxed, and co-training is still effective under a weaker independence assumption.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "W04-2405", 
    "citing_paper_authority": 12, 
    "citing_paper_authors": "Rada, Mihalcea", 
    "raw_text": "However, as theoretically shown in (Abney, 2002), and then empirically in (Clark et al,2003), co-training still works under a weaker independence assumption, and the results we obtain concur with these previous observations", 
    "clean_text": "However, as theoretically shown in (Abney, 2002), and then empirically in (Clark et al,2003), co-training still works under a weaker independence assumption, and the results we obtain concur with these previous observations.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P03-1042", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Yunbo, Cao | Hang, Li | Li, Lian", 
    "raw_text": "Dasgupta et al (2001) and Abney (2002) con ducted theoretical analyses on the performance (generalization error) of co-training", 
    "clean_text": "Dasgupta et al (2001) and Abney (2002) conducted theoretical analyses on the performance (generalization error) of co-training.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "P03-1042", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Yunbo, Cao | Hang, Li | Li, Lian", 
    "raw_text": "Abney (2002) refined Dasgupta et als result by relaxing the view independence assumption with a new constraint", 
    "clean_text": "Abney (2002) refined Dasgupta et als result by relaxing the view independence assumption with a new constraint.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "W03-0403", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Jason, Baldridge | Miles, Osborne", 
    "raw_text": "Rather than comparing the two learners on whether they categorically select the same preferred parse on a number of examples, we can view active learning as the inverse of agreement-based co-training (Abney, 2002)", 
    "clean_text": "Rather than comparing the two learners on whether they categorically select the same preferred parse on a number of examples, we can view active learning as the inverse of agreement-based co-training (Abney, 2002).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "N03-1031", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Mark, Steedman | Rebecca, Hwa | Stephen, Clark | Miles, Osborne | Anoop, Sarkar | Julia, Hockenmaier | Paul, Ruhlen | Steven, Baker | Jeremiah, Crim", 
    "raw_text": "Further avenues to explore include the development of selection methods toefficiently approximate maximizing the objective function of parser agreement on unlabeled data, following the work of Dasgupta et al (2002) and Abney (2002)", 
    "clean_text": "Further avenues to explore include the development of selection methods to efficiently approximate maximizing the objective function of parser agreement on unlabeled data, following the work of Dasgupta et al (2002) and Abney (2002).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "W03-1504", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Llu&iacute;s, M&agrave;rquez | Adri&agrave; de, Gispert | Xavier, Carreras | Llu&iacute;s, Padr&oacute;", 
    "raw_text": "We have implemented the Greedy Agreement Algorithm (Abney, 2002) which, based on two independent views of the data, is able to learn two binary classifiers from a set of hand-typed seed rules", 
    "clean_text": "We have implemented the Greedy Agreement Algorithm (Abney, 2002) which, based on two independent views of the data, is able to learn two binary classifiers from a set of hand-typed seed rules.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "W03-1504", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Llu&iacute;s, M&agrave;rquez | Adri&agrave; de, Gispert | Xavier, Carreras | Llu&iacute;s, Padr&oacute;", 
    "raw_text": "See (Abney, 2002) for a formal proof that this algorithm tends to gradually reduce the classification error given the adequate seed rules. For its extreme simplicity and potentially good results, this algorithm is very appealing for the NEC task", 
    "clean_text": "See (Abney, 2002) for a formal proof that this algorithm tends to gradually reduce the classification error given the adequate seed rules.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "W03-1504", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Llu&iacute;s, M&agrave;rquez | Adri&agrave; de, Gispert | Xavier, Carreras | Llu&iacute;s, Padr&oacute;", 
    "raw_text": "In fact, results are reported to be competitive against more sophisticated methods (Co-DL, Co Boost, etc.) for this specific task in (Abney, 2002) .Three important questions arise from the algorithm", 
    "clean_text": "In fact, results are reported to be competitive against more sophisticated methods (Co-DL, Co Boost, etc.) for this specific task in (Abney, 2002).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "W03-1504", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Llu&iacute;s, M&agrave;rquez | Adri&agrave; de, Gispert | Xavier, Carreras | Llu&iacute;s, Padr&oacute;", 
    "raw_text": "Third, how the algorithm, presented in (Abney, 2002) for binary classification, can be extended to a multi class problem", 
    "clean_text": "Third, how the algorithm, presented in (Abney, 2002) for binary classification, can be extended to a multi class problem.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "W03-1504", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Llu&iacute;s, M&agrave;rquez | Adri&agrave; de, Gispert | Xavier, Carreras | Llu&iacute;s, Padr&oacute;", 
    "raw_text": "These results are comparable to the ones presented in (Abney, 2002) ,taking into account, apart from the language change, that we have introduced a fourth class to be treated the same as the other three", 
    "clean_text": "These results are comparable to the ones presented in (Abney, 2002), taking into account, apart from the language change, that we have introduced a fourth class to be treated the same as the other three.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "W06-2207", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Mihai, Surdeanu | Jordi, Turmo | Alicia, Ageno", 
    "raw_text": "Forthe same recall point, Collins yields a classification model with much higher precision, with differences ranging from 5% in the REUTERS collection to 20% in the AP collection. Theorem 5 in (Abney, 2002) provides a theoretical explanation for these results: if certain in dependence conditions between the classifier rules are satisfied and the precision of each rule is larger than a threshold T, then the precision of the final classifier is larger than T. Although the rule independence conditions are certainly not satisfied inour real-world evaluation, the above theorem indicates that there is a strong relation between the precision of the classifier rules on labeled data and the precision of the final classifier", 
    "clean_text": "Theorem 5 in (Abney, 2002) provides a theoretical explanation for these results: if certain independence conditions between the classifier rules are satisfied and the precision of each rule is larger than a threshold T, then the precision of the final classifier is larger than T.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "W04-2402", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Rie Kubota, Ando", 
    "raw_text": "Abney (2002) presents an analysis to relax the (fairly strong) conditional independence assumption to weak rule dependence. Nigam and Ghani (2000) study the effectiveness of co training through experiments on the text categorization task", 
    "clean_text": "Abney (2002) presents an analysis to relax the (fairly strong) conditional independence assumption to weak rule dependence.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "W09-2202", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Sriharsha, Veeramachaneni | Ravikumar, Kondadadi", 
    "raw_text": "Although this result was previously observed in a different context by Abney in (Abney, 2002), he does not use it to derive a semi-supervised learning algorithm", 
    "clean_text": "Although this result was previously observed in a different context by Abney in (Abney, 2002), he does not use it to derive a semi-supervised learning algorithm.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "W06-1624", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Weilin, Wu | Ruzhan, Lu | Jianyong, Duan | Hui, Liu | Feng, Gao | Yu-Quan, Chen", 
    "raw_text": "Semantic Classifiers Bootstrapping refers to a problem of inducing a classifier given a small set of labeled data and a large set of unlabeled data (Abney, 2002)", 
    "clean_text": "Semantic Classifiers Bootstrapping refers to a problem of inducing a classifier given a small set of labeled data and a large set of unlabeled data (Abney, 2002).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "E06-3004", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Zornitsa, Kozareva", 
    "raw_text": "We used bootstrapping (Abney, 2002) which refers to a problem setting in which one is given a small set of labeled data and a large set of unlabeled data, and the task is to induce a classifier", 
    "clean_text": "We used bootstrapping (Abney, 2002) which refers to a problem setting in which one is given a small set of labeled data and a large set of unlabeled data, and the task is to induce a classifier.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "N03-1023", 
    "citing_paper_authority": 20, 
    "citing_paper_authors": "Vincent, Ng | Claire, Cardie", 
    "raw_text": "Overall, our results suggest that single-view1Abney (2002) argues that the conditional independence assumption is remarkably strong and is rarely satisfied in real data sets, showing that a weaker independence assumption suffices", 
    "clean_text": "Abney (2002) argues that the conditional independence assumption is remarkably strong and is rarely satisfied in real data sets, showing that a weaker independence assumption suffices.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "N03-1023", 
    "citing_paper_authority": 20, 
    "citing_paper_authors": "Vincent, Ng | Claire, Cardie", 
    "raw_text": "Co-training algorithms such as CoBoost (Collins and Singer, 1999) and Greedy Agreement (Abney, 2002) that explicitly trade classifier agreement on unlabeled data against error on labeled data may be more robust to the underlying assumptions of co-training and can conceivably perform better than the Blum and Mitchell algorithm for problems without a natural feature split.9 Other less studied single-view weakly supervised algorithms in theNLP community such as co-training with different learning algorithms (Goldman and Zhou, 2000) and graphmincuts (Blum and Chawla, 2001) can be similarly applied to these problems to further test our original hypothesis", 
    "clean_text": "Co-training algorithms such as CoBoost (Collins and Singer, 1999) and Greedy Agreement (Abney, 2002) that explicitly trade classifier agreement on unlabeled data against error on labeled data may be more robust to the underlying assumptions of co-training and can conceivably perform better than the Blum and Mitchell algorithm for problems without a natural feature split.", 
    "keep_for_gold": 0
  }
]