[
  {
    "citance_No": 1, 
    "citing_paper_id": "D08-1066", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Markos, Mylonakis | Khalil, Sima'an", 
    "raw_text": "Birch et al (Birchetal., 2006) provide soft measures for including word alignments in the estimation process and obtain improved results only on small data sets. Coming up-to-date, (Blunsom et al, 2008) at tempt a related estimation problem to (Marcu and Wong, 2002), using the expanded phrase pair set of (Chiang, 2005a), working with an exponential model and concentrating on marginalizing out the latent segmentation variable", 
    "clean_text": "Coming up-to-date, (Blunsom et al, 2008) attempt a related estimation problem to (Marcu and Wong, 2002), using the expanded phrase pair set of (Chiang, 2005a), working with an exponential model and concentrating on marginalizing out the latent segmentation variable.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "D08-1066", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Markos, Mylonakis | Khalil, Sima'an", 
    "raw_text": "(Blunsom et al,2008))", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "D08-1066", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Markos, Mylonakis | Khalil, Sima'an", 
    "raw_text": "Secondly, as (Blunsom et al, 2008) show, marginalizing out the different segmentations during decoding leads to improved performance", 
    "clean_text": "Secondly, as (Blunsom et al, 2008) show, marginalizing out the different segmentations during decoding leads to improved performance.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "D09-1107", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Matti, K&auml;&auml;ri&auml;inen", 
    "raw_text": "The version presented in (Blunsom et al., 2008) scales to more than a hundred thousand short training sentences, but does not integrate alanguage model and thus has performance that improves upon Hiero without a language model only", 
    "clean_text": "The version presented in (Blunsom et al., 2008) scales to more than a hundred thousand short training sentences, but does not integrate a language model and thus has performance that improves upon Hiero without a language model only.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "D09-1107", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Matti, K&auml;&auml;ri&auml;inen", 
    "raw_text": "In practice, this problem can be circumvented by discarding the training sentence pairs with unreachable reference translations, but this may mean a significant reduction in the amount of training data (24% in (Blunsom et al., 2008)) .Another closely related approach is the independently developed discriminative block bi gram prediction model presented in (Tillmann and Zhang, 2007)", 
    "clean_text": "In practice, this problem can be circumvented by discarding the training sentence pairs with unreachable reference translations, but this may mean a significant reduction in the amount of training data (24% in (Blunsom et al., 2008)).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "W11-2130", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Barry, Haddow | Abhishek, Arun | Philipp, Koehn", 
    "raw_text": "The training methods in (Arun et al, 2692010) are very resource intensive, with the experiments running for 48 hours on around 40 cores, on a pruned phrase table derived from Europarl, and a 3-gram language model. Instead of using expected BLEU as a training objective, Blunsom et al (2008) trained their model to directly maximise the log-likelihood of the disc rim inative model, estimating feature expectations from a packed chart", 
    "clean_text": "Instead of using expected BLEU as a training objective, Blunsom et al (2008) trained their model to directly maximise the log-likelihood of the discriminative model, estimating feature expectations from a packed chart.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "W09-1114", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Abhishek, Arun | Chris, Dyer | Barry, Haddow | Philip, Blunsom | Adam, Lopez | Philipp, Koehn", 
    "raw_text": "This is almost certainly a side effect of the MERT training approach that was used to construct the models so as to maximise the performance of the model on its single best derivation, without regard to the shape of the rest of the distribution (Blunsom et al, 2008)", 
    "clean_text": "This is almost certainly a side effect of the MERT training approach that was used to construct the models so as to maximise the performance of the model on its single best derivation, without regard to the shape of the rest of the distribution (Blunsom et al, 2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "C10-1097", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Derya, Ozkan | Kenji, Sagae | Louis-Philippe, Morency", 
    "raw_text": "In recent years, great research has shown the strength of latent variable models for natural language processing (Blunsom et al, 2008)", 
    "clean_text": "In recent years, great research has shown the strength of latent variable models for natural language processing (Blunsom et al, 2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "D11-1081", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Xinyan, Xiao | Yang, Liu | Qun, Liu | Shouxun, Lin", 
    "raw_text": "We use the forest to train a log-linear model with a latent variable as describe in Blunsom et al (2008)", 
    "clean_text": "We use the forest to train a log-linear model with a latent variable as describe in Blunsom et al (2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "P09-1065", 
    "citing_paper_authority": 14, 
    "citing_paper_authors": "Yang, Liu | Haitao, Mi | Yang, Feng | Qun, Liu", 
    "raw_text": "Of ten, there are many derivations that are distinct yet produce the same translation. Blunsom et al (2008) present a latent variable model that describes the relationship between translation and derivation clearly", 
    "clean_text": "Of ten, there are many derivations that are distinct yet produce the same translation. Blunsom et al (2008) present a latent variable model that describes the relationship between translation and derivation clearly.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "P09-1065", 
    "citing_paper_authority": 14, 
    "citing_paper_authors": "Yang, Liu | Haitao, Mi | Yang, Feng | Qun, Liu", 
    "raw_text": "(6) as max-derivation decoding, which are first termed by Blunsom et al (2008)", 
    "clean_text": "(6) as max-derivation decoding, which are first termed by Blunsom et al (2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "P09-1065", 
    "citing_paper_authority": 14, 
    "citing_paper_authors": "Yang, Liu | Haitao, Mi | Yang, Feng | Qun, Liu", 
    "raw_text": "Blunsom et al (2008) first distinguish between max-derivation decoding and max-translation decoding explicitly", 
    "clean_text": "Blunsom et al (2008) first distinguish between max-derivation decoding and max-translation decoding explicitly.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "P09-1065", 
    "citing_paper_authority": 14, 
    "citing_paper_authors": "Yang, Liu | Haitao, Mi | Yang, Feng | Qun, Liu", 
    "raw_text": "Both Mi et al (2008 )andBlunsom et al (2008) use a translation hyper graph to represent search space", 
    "clean_text": "Both Mi et al (2008) and Blunsom et al (2008) use a translation hyper graph to represent search space.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "W11-2139", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Chris, Dyer | Kevin, Gimpel | Jonathan H., Clark | Noah A., Smith", 
    "raw_text": "3.1.1 Phase 1 training For the first model, which includes the sparse parse features, we learn weights in order to optimize penalized conditional log likelihood (Blunsom et al, 2008)", 
    "clean_text": "For the first model, which includes the sparse parse features, we learn weights in order to optimize penalized conditional log likelihood (Blunsom et al, 2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "W11-2139", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Chris, Dyer | Kevin, Gimpel | Jonathan H., Clark | Noah A., Smith", 
    "raw_text": "See Blunsom et al (2008) for more information.5Ideally,? would have been tuned to optimize held-out likelihood or BLEU; however, the evaluation deadline prevented us from doing this.model are easily defined and straightforward to compute with dynamic programming", 
    "clean_text": "See Blunsom et al (2008) for more information.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "E09-1061", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Adam, Lopez", 
    "raw_text": "Applying this to logic MONOTONE, the result will be a maximization (over all possible derivations D) of the algebraic expression in Equation 1.We might also want to calculate the total prob ability of all possible derivations, which is useful for parameter estimation (Blunsom et al, 2008)", 
    "clean_text": "We might also want to calculate the total probability of all possible derivations, which is useful for parameter estimation (Blunsom et al, 2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "D09-1023", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Kevin, Gimpel | Noah A., Smith", 
    "raw_text": "10 Optimization problems of this form are by now widely known in NLP (Koo and Collins, 2005), and have recently been used for machine translation as well (Blunsom et al, 2008) .Such problems are typically solved using variations of gradient ascent; in our experiments, we will use an on line method called stochastic gradient ascent (SGA)", 
    "clean_text": "Optimization problems of this form are by now widely known in NLP (Koo and Collins, 2005), and have recently been used for machine translation as well (Blunsom et al, 2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "P10-1049", 
    "citing_paper_authority": 22, 
    "citing_paper_authors": "Joern, Wuebker | Arne, Mauser | Hermann, Ney", 
    "raw_text": "For the hierarchical phrase-based approach, (Blunsom et al, 2008) present a discriminative rule model and show the difference between using only the viterbi alignment in training and using the full sum over all possible derivations", 
    "clean_text": "For the hierarchical phrase-based approach, (Blunsom et al, 2008) present a discriminative rule model and show the difference between using only the viterbi alignment in training and using the full sum over all possible derivations.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "C10-1081", 
    "citing_paper_authority": 18, 
    "citing_paper_authors": "Ding, Liu | Daniel, Gildea", 
    "raw_text": "The first method maximizes data likelihood as is standard in EM, while the second method maximizes conditional likelihood for a log linear model following Blunsom et al (2008)", 
    "clean_text": "The first method maximizes data likelihood as is standard in EM, while the second method maximizes conditional likelihood for a log linear model following Blunsom et al (2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "P09-1067", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Zhifei, Li | Jason M., Eisner | Sanjeev P., Khudanpur", 
    "raw_text": "4Note that the marginalization for a particular y would betractable; it is used at training time in certain training objective functions ,e.g., maximizing the conditional likelihood of a reference translation (Blunsom et al, 2008)", 
    "clean_text": "Note that the marginalization for a particular y would be tractable; it is used at training time in certain training objective functions, e.g., maximizing the conditional likelihood of a reference translation (Blunsom et al, 2008).", 
    "keep_for_gold": 0
  }
]