In this paper we address the problem of modeling compositional meaning for phrases and sentences using distributional methods.
We experiment with several possible combinations of representation and composition, exhibiting varying degrees of sophistication.
Some are shallow while others operate over syntactic structure, rely on parameter learning, or require access to very large corpora.
We find that shallow approaches are as good as more computationally intensive alternatives with regards to two particular tests: (1) phrase similarity and (2) paraphrase detection.
The sizes of the involved training corpora and the generated vectors are not as important as the fit between the meaning representation and compositional method. 