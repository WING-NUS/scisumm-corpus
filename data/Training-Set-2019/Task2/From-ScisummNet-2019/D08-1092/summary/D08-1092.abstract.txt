We show that jointly parsing a bitext can substantially improve parse quality on both sides.
In a maximum entropy bitext parsing model, we define a distribution over source trees, target trees, and node-to-node alignments between them.
Features include monolingual parse scores and various measures of syntactic divergence.
Using the translated portion of the Chinese treebank, our model is trained iteratively to maximize the marginal likelihood of training tree pairs, with alignments treated as latent variables.
The resulting bitext parser outperforms state-of-the-art monolingual parser baselines by 2.5 F at predicting English side trees and 1.8 F at predicting Chinese side trees (the highest published numbers on these corpora).
Moreover, these improved trees yield a 2.4 BLEU increase when used in a downstream MT evaluation.