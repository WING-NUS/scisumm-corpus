We describe new algorithms for training tagging models, as an alternative to maximum-entropy models or conditional random fields (CRFs).
The algorithms rely on Viterbi decoding of training examples, combined with simple additive updates.
We describe theory justifying the algorithms through a modification of the proof of convergence of the perceptron algorithm for classification problems.
We give experimental results on part-of-speech tagging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger.
