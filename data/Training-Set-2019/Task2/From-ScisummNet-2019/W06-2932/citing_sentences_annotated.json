[
  {
    "citance_No": 1, 
    "citing_paper_id": "W06-2920", 
    "citing_paper_authority": 228, 
    "citing_paper_authors": "Sabine, Buchholz | Erwin, Marsi", 
    "raw_text": "for all Swedish auxiliary and model verbs dby having no projectivity constraint eselective projectivity constraint for Japanesefseveral approaches to non-projectivity gusing some FEATS components to create some finer-grained POSTAG values hreattachment rules for some types of non-projectivity ihead automaton grammar jdetermined the maximally allowed distance for relations kthrough special parser actions lpseudo-projectivizing training data only m Greedy Prepend Algorithmnbut two separate learners used for unlabeled parsing versus labeling oboth foward and backward, then combined into a single tree with CLEpbut two separate SVMs used for unlabeled parsing versus labeling qforward parsing for Japanese and Turkish, backward for the rest rattaching remaining unattached tokens through exhaustive search (not for submitted runs) 156 sequence classifier can label all children of a token together (McDonald et al, 2006)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "W06-2920", 
    "citing_paper_authority": 228, 
    "citing_paper_authors": "Sabine, Buchholz | Erwin, Marsi", 
    "raw_text": "Introduce through post-processing ,e.g. through reattachment rules (Bick, 2006) or if the change increases overall parse tree probability (McDonald et al, 2006)", 
    "clean_text": "Introduce through post-processing, e.g. through reattachment rules (Bick, 2006) or if the change increases overall parse tree probability (McDonald et al, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "W06-2920", 
    "citing_paper_authority": 228, 
    "citing_paper_authors": "Sabine, Buchholz | Erwin, Marsi", 
    "raw_text": "Table 5 shows the official results for submitted parser outputs.31 The two participant groups with the highest total score are McDonald et al (2006) and Nivre et al (2006)", 
    "clean_text": "Table 5 shows the official results for submitted parser outputs. The two participant groups with the highest total score are McDonald et al (2006) and Nivre et al (2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "W06-2920", 
    "citing_paper_authority": 228, 
    "citing_paper_authors": "Sabine, Buchholz | Erwin, Marsi", 
    "raw_text": "Even though McDonald et al (2006) and Nivre et al (2006) obtained very similar overall scores, a more detailed look at their performance shows clear differences", 
    "clean_text": "Even though McDonald et al (2006) and Nivre et al (2006) obtained very similar overall scores, a more detailed look at their performance shows clear differences.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "W08-1007", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Johan, Hall | Joakim, Nivre", 
    "raw_text": "The high est score on parsing German in the CoNLL-X shared task was obtained by the system of McDonald et al (2006) with a LAS of 87.34 based on the TIGER tree bank, but we want to stress that these results are not comparable due to different data sets (anda different policy regarding the inclusion of punctuation) .The constituency versions were evaluated according to the labeled recall (LR), labeled precision (LP) and labeled F-score (LF)", 
    "clean_text": "The highest score on parsing German in the CoNLL-X shared task was obtained by the system of McDonald et al (2006) with a LAS of 87.34 based on the TIGER tree bank, but we want to stress that these results are not comparable due to different data sets (and a different policy regarding the inclusion of punctuation). The constituency versions were evaluated according to the labeled recall (LR), labeled precision (LP) and labeled F-score (LF).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "W09-1210", 
    "citing_paper_authority": 11, 
    "citing_paper_authors": "Bernd, Bohnet", 
    "raw_text": "McDonald et al (2006) use an additional algorithm", 
    "clean_text": "McDonald et al (2006) use an additional algorithm.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "W12-3407", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Iakes, Goenaga | Mar&Atilde;&not;a Jes&Atilde;&ordm;s, Aranzabe | Arantza, D&iacute;az de Ilarraza | Kepa, Bengoetxea | Koldo, Gojenola", 
    "raw_text": "Regarding the data-driven parsers, we have made use of MaltParser (Nivre et al, 2007b) and MST Parser (McDonald et al, 2006), two state of the art dependency parsers representing two dominant approaches in data-driven dependency parsing, and that have been successfully applied to typologically different languages and tree banks (McDonald and Nivre, 2007)", 
    "clean_text": "Regarding the data-driven parsers, we have made use of MaltParser (Nivre et al, 2007b) and MST Parser (McDonald et al, 2006), two state of the art dependency parsers representing two dominant approaches in data-driven dependency parsing, and that have been successfully applied to typologically different languages and tree banks (McDonald and Nivre, 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "I08-1012", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Wenliang, Chen | Daisuke, Kawahara | Kiyotaka, Uchimoto | Yujie, Zhang | Hitoshi, Isahara", 
    "raw_text": "In fact, our approach can also be applied to other parsers, such as (Yamada and Matsumoto, 2003)? s parser, (McDonald et al., 2006)? s parser, and so on", 
    "clean_text": "In fact, our approach can also be applied to other parsers, such as (Yamada and Matsumoto, 2003)'s parser, (McDonald et al., 2006)'s parser, and so on.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "N07-1050", 
    "citing_paper_authority": 12, 
    "citing_paper_authors": "Joakim, Nivre", 
    "raw_text": "But whereas the spanning tree parser of McDonald et al (2006) and the pseudo-projective parser of Nivre et al (2006) achieve this performance only with special preorpost-processing,7 the approach presented here derives a labeled non-projective graph in a single incremental process and hence at least has the advantage of simplicity", 
    "clean_text": "But whereas the spanning tree parser of McDonald et al (2006) and the pseudo-projective parser of Nivre et al (2006) achieve this performance only with special pre or post-processing, the approach presented here derives a labeled non-projective graph in a single incremental process and hence at least has the advantage of simplicity.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "N07-1050", 
    "citing_paper_authority": 12, 
    "citing_paper_authors": "Joakim, Nivre", 
    "raw_text": "Moreover, it has better time complexity than the approximate second-order spanning tree parsing of McDonald et al (2006), which has exponential complexity in the worst case (although this does not appear to be a problem in practice)", 
    "clean_text": "Moreover, it has better time complexity than the approximate second-order spanning tree parsing of McDonald et al (2006), which has exponential complexity in the worst case (although this does not appear to be a problem in practice).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "N07-1050", 
    "citing_paper_authority": 12, 
    "citing_paper_authors": "Joakim, Nivre", 
    "raw_text": "We have shown that, for languages with a7McDonald et al (2006) use post-processing for non projective dependencies and for labeling", 
    "clean_text": "McDonald et al (2006) use post-processing for non-projective dependencies and for labeling.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "D07-1122", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Wenliang, Chen | Yujie, Zhang | Hitoshi, Isahara", 
    "raw_text": "As described in (McDonald et al, 2006), we treat the labeling of dependencies as a sequence labeling problem", 
    "clean_text": "As described in (McDonald et al, 2006), we treat the labeling of dependencies as a sequence labeling problem.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "W11-0314", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Felice, Dell'Orletta | Giulia, Venturi | Simonetta, Montemagni", 
    "raw_text": "ULISSE was tested against the output of two really different data? driven parsers: the first? order Maximum Spanning Tree (MST) parser (McDonald et al., 2006) and the DeSR parser (Attardi, 2006) using Support Vector Machine as learning algorithm", 
    "clean_text": "ULISSE was tested against the output of two really different data driven parsers: the first order Maximum Spanning Tree (MST) parser (McDonald et al., 2006) and the DeSR parser (Attardi, 2006) using Support Vector Machine as learning algorithm.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "D07-1015", 
    "citing_paper_authority": 16, 
    "citing_paper_authors": "Terry, Koo | Amir, Globerson | Xavier, Carreras | Michael John, Collins", 
    "raw_text": "5It should be noted that McDonald et al (2006) use a richer feature set that is incomparable to our features", 
    "clean_text": "It should be noted that McDonald et al (2006) use a richer feature set that is incomparable to our features.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "D10-1069", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Slav, Petrov | Pi-Chuan, Chang | Michael, Ringgaard | Hiyan, Alshawi", 
    "raw_text": "706 Training on Evaluating on WSJ Section 22 Evaluating on QuestionBank WSJ Sections 02-21 F1 UAS LAS POS F1 UAS LAS POS Nivre et al (2007)? 88.42 84.89 95.00? 74.14 62.81 88.48 McDonald et al (2006)? 89.47 86.43 95.00? 80.01 67.00 88.48 Charniak (2000) 90.27 92.33 89.86 96.71 83.01 85.61 73.59 90.49 Charniak and Johnson (2005) 91.92 93.56 91.24 96.69 84.47 87.13 75.94 90.59 Petrov et al (2006) 90.70 92.91 90.48 96.27 85.52 88.17 79.10 90.57 Petrov (2010) 92.10 93.85 91.60 96.44 86.62 88.77 79.92 91.08 Our shift-reduce parser? 88.24 84.69 95.00? 72.23 60.06 88.48 Our shift-reduce parser (gold POS)? 90.51 88.53 100.00? 78.30 68.92 100.00 Table 1: Parsing accuracies for parsers trained on newswire data and evaluated on newswire and question test sets", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "D10-1069", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Slav, Petrov | Pi-Chuan, Chang | Michael, Ringgaard | Hiyan, Alshawi", 
    "raw_text": "The dependency parsers that we compare are the deterministic shift-reduce MaltParser (Nivre et al, 2007) and the second-order minimum spanning tree algorithm based MstParser (McDonald et al, 2006)", 
    "clean_text": "The dependency parsers that we compare are the deterministic shift-reduce MaltParser (Nivre et al, 2007) and the second-order minimum spanning tree algorithm based MstParser (McDonald et al, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "D10-1069", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Slav, Petrov | Pi-Chuan, Chang | Michael, Ringgaard | Hiyan, Alshawi", 
    "raw_text": "As one 707 Evaluating on Training on WSJ+ QB Training on QuestionBank QuestionBank F1 UAS LAS POS F1 UAS LAS POS Nivre et al (2007)? 83.54 78.85 91.32? 79.72 73.44 88.80 McDonald et al (2006)? 84.95 80.17 91.32? 82.52 77.20 88.80 Charniak (2000) 89.40 90.30 85.01 94.17 79.70 76.69 69.69 87.84 Petrov et al (2006) 90.96 90.98 86.90 94.01 86.62 84.09 78.92 87.56 Petrov (2010) 92.81 92.23 88.84 94.48 87.72 85.07 80.08 87.79 Our shift-reduce parser? 83.70 78.27 91.32? 80.44 74.29 88.80 Our shift-reduce parser (gold POS)? 89.39 86.60 100.00? 87.31 84.15 100.00 Table 2: Parsing accuracies for parsers trained on newswire and question data and evaluated on a question test set", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "D10-1004", 
    "citing_paper_authority": 26, 
    "citing_paper_authors": "Andre, Martins | Noah A., Smith | Eric P., Xing | Pedro, Aguiar | Mario, Figueiredo", 
    "raw_text": "Entries marked with? are the highest reported in the literature, to the best of our knowledge, beating (sometimes slightly) McDonald et al (2006), Martins et al (2008), Martins et al (2009), and, in the case of English Proj., also the third-order parser of Koo and Collins (2010), which achieves 93.04% on that dataset (their experiments in Czech are not comparable, since the datasets are different)", 
    "clean_text": "Entries marked with are the highest reported in the literature, to the best of our knowledge, beating (sometimes slightly) McDonald et al (2006), Martins et al (2008), Martins et al (2009), and, in the case of English Proj., also the third-order parser of Koo and Collins (2010), which achieves 93.04% on that dataset (their experiments in Czech are not comparable, since the datasets are different).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "P08-1108", 
    "citing_paper_authority": 67, 
    "citing_paper_authors": "Joakim, Nivre | Ryan, McDonald", 
    "raw_text": "The specific graph-based model studied in this work is that presented by McDonald et al (2006), which factors scores over pairs of arcs (instead of just single arcs) and uses near exhaustive search for unlabeled parsing coupled with a separate classifier to label each arc. We call this system MSTParser, or simply MST for short, which is also the name of the freely available implementation.2 2.3 Transition-Based Models", 
    "clean_text": "The specific graph-based model studied in this work is that presented by McDonald et al (2006), which factors scores over pairs of arcs (instead of just single arcs) and uses near exhaustive search for unlabeled parsing coupled with a separate classifier to label each arc. We call this system MSTParser, or simply MST for short, which is also the name of the freely available implementation.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "P08-1108", 
    "citing_paper_authority": 67, 
    "citing_paper_authors": "Joakim, Nivre | Ryan, McDonald", 
    "raw_text": "More precisely, dependency arcs (or pairs of arcs) are first represented by a high dimensional feature vector f (i, j, l)? Rk, where f is typically a bi nary feature vector over properties of the arc as well as the surrounding input (McDonald et al, 2005a; McDonald et al, 2006)", 
    "clean_text": "More precisely, dependency arcs (or pairs of arcs) are first represented by a high dimensional feature vector f (i, j, l) Rk, where f is typically a binary feature vector over properties of the arc as well as the surrounding input (McDonald et al, 2005a; McDonald et al, 2006).", 
    "keep_for_gold": 0
  }
]