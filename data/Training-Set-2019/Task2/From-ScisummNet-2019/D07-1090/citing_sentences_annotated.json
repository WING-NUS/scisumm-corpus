[
  {
    "citance_No": 1, 
    "citing_paper_id": "D07-1005", 
    "citing_paper_authority": 9, 
    "citing_paper_authors": "Shankar, Kumar | Franz Josef, Och | Wolfgang, Macherey", 
    "raw_text": "5-gram word language models in English are trained on a variety of monolingual corpora (Brants et al, 2007)", 
    "clean_text": "5-gram word language models in English are trained on a variety of monolingual corpora (Brants et al, 2007).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P14-1075", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Hayato, Kobayashi", 
    "raw_text": "In the case of language models, we often have to remove low-frequency words because of a lack of computational resources, since the feature space of k grams tends to be so large that we sometimes need cutoffs even in a distributed environment (Brantset al, 2007)", 
    "clean_text": "In the case of language models, we often have to remove low-frequency words because of a lack of computational resources, since the feature space of k grams tends to be so large that we sometimes need cutoffs even in a distributed environment (Brantset al, 2007).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P08-1058", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "David, Talbot | Thorsten, Brants", 
    "raw_text": "To scale LMs to larger corpora with higher-order dependencies, researchers? Work completed while this author was at Google Inc. have considered alternative parameterizations such as class-based models (Brown et al, 1992), model reduction techniques such as entropy-based pruning (Stolcke, 1998), novel represent ion schemes such as suffix arrays (Emami et al, 2007), Golomb Coding (Church et al, 2007) and distributed language mod els that scale more readily (Brants et al, 2007) .In this paper we propose a novel randomized language model", 
    "clean_text": "To scale LMs to larger corpora with higher-order dependencies, researchers have considered alternative parameterizations such as class-based models (Brown et al, 1992), model reduction techniques such as entropy-based pruning (Stolcke, 1998), novel represent ion schemes such as suffix arrays (Emami et al, 2007), Golomb Coding (Church et al, 2007) and distributed language models that scale more readily (Brants et al, 2007).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "P08-1058", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "David, Talbot | Thorsten, Brants", 
    "raw_text": "(Emami et al, 2007), (Brants et al, 2007), (Church et al, 2007)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P08-1058", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "David, Talbot | Thorsten, Brants", 
    "raw_text": "Here we choose to work with stupid back off smoothing (Brants et al, 2007) since this is significantly more efficient to train and deploy in a distributed framework than a context dependent smoothing scheme such as Kneser-Ney", 
    "clean_text": "Here we choose to work with stupid back off smoothing (Brants et al, 2007) since this is significantly more efficient to train and deploy in a distributed framework than a context dependent smoothing scheme such as Kneser-Ney.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "P08-1058", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "David, Talbot | Thorsten, Brants", 
    "raw_text": "Previous work (Brants et al, 2007) has shown it to be appropriate to large-scale language modeling", 
    "clean_text": "Previous work (Brants et al, 2007) has shown it to be appropriate to large-scale language modeling.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "D11-1080", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Christof, Monz", 
    "raw_text": "Brants et al (2007) have shown that each doubling of the training data from the news domain (used to build the language model), leads to improvements of approximately 0.5 BLEU points", 
    "clean_text": "Brants et al (2007) have shown that each doubling of the training data from the news domain (used to build the language model), leads to improvements of approximately 0.5 BLEU points.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "D11-1023", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Amit, Goyal", 
    "raw_text": "For example, Brants et al (2007) used 1500 machines for a day to compute the relative frequencies of n-grams from 1.8TB of web data", 
    "clean_text": "For example, Brants et al (2007) used 1500 machines for a day to compute the relative frequencies of n-grams from 1.8TB of web data.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "E09-1044", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Gonzalo, Iglesias | Adri&agrave; de, Gispert | Eduardo R., Banga | William, Byrne", 
    "raw_text": "We build sentence specific zero-cutoff stupid-back off (Brants et al., 2007) 5-gram language models, estimated using? 4.7B words of English newswire text, and apply them to rescore each 10000-best list", 
    "clean_text": "We build sentence specific zero-cutoff stupid-back off (Brants et al., 2007) 5-gram language models, estimated using 4.7B words of English newswire text, and apply them to rescore each 10000-best list.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "S12-1021", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Karl Moritz, Hermann | Stephen G., Pulman | Philip, Blunsom", 
    "raw_text": "However, adding more data in an unsupervised sense is unlikely to significantly improve results (Brants et al, 2007) .Alternatively, it would be possible to add specific training data that included the noun compounds from the evaluation data sets", 
    "clean_text": "However, adding more data in an unsupervised sense is unlikely to significantly improve results (Brants et al, 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "W12-2706", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Brian, Strope | Leif, Johnson | Preethi, Jyothi | Ciprian, Chelba", 
    "raw_text": "We aim to allow the estimation of large scale distributed models, similar in size to the ones in Brantset al (2007)", 
    "clean_text": "We aim to allow the estimation of large scale distributed models, similar in size to the ones in Brantset al (2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "W12-2706", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Brian, Strope | Leif, Johnson | Preethi, Jyothi | Ciprian, Chelba", 
    "raw_text": "In relation to language models, Brants et al (2007) recently proposed a distributed MapReduceinfrastructure to build Ngram language models having up to 300 billion n-grams", 
    "clean_text": "In relation to language models, Brants et al (2007) recently proposed a distributed MapReduce infrastructure to build Ngram language models having up to 300 billion n-grams.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "P10-2041", 
    "citing_paper_authority": 49, 
    "citing_paper_authors": "Robert C., Moore | William H., Lewis", 
    "raw_text": "Log-linear interpolation is particularly popular in statistical machine translation (e.g., Brants et al., 2007), because the interpolation weights can easily be discriminatively trained to optimize an end-to-end translation objective function (such as BLEU) by making the log probability according to each language model a separate feature function in the overall translation model. The normal practice when using multiple languages models in machine translation seems to be to train models on as much data as feasible from each source, and to depend on feature weight optimization to down-weight the impact of data that is less well-matched to the translation application", 
    "clean_text": "Log-linear interpolation is particularly popular in statistical machine translation (e.g., Brants et al., 2007), because the interpolation weights can easily be discriminatively trained to optimize an end-to-end translation objective function (such as BLEU) by making the log probability according to each language model a separate feature function in the overall translation model.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "E09-1019", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Mathias, Creutz | Sami, Virpioja | Anna, Kovaleva", 
    "raw_text": "This was expected, as it has been observed before that very simple smoothing techniques can perform well on large data sets, such as web data (Brants et al, 2007)", 
    "clean_text": "This was expected, as it has been observed before that very simple smoothing techniques can perform well on large data sets, such as web data (Brants et al, 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "E12-1058", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Tim, Gollub | Dennis, Hoppe | Benno, Stein", 
    "raw_text": "Sincestandard spelling correction dictionaries (e.g. ASpell) are not able to capture the rich language used in web queries, large-scale knowledge sources such as Wikipedia (Li et al 2011), query logs (Chen et al 2007), and large n-gram corpora (Brants et al., 2007) are employed", 
    "clean_text": "Since standard spelling correction dictionaries (e.g. ASpell) are not able to capture the rich language used in web queries, large-scale knowledge sources such as Wikipedia (Li et al 2011), query logs (Chen et al 2007), and large n-gram corpora (Brants et al., 2007) are employed.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "D09-1078", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Robert C., Moore | Chris, Quirk", 
    "raw_text": "Since that time, however, increasingly large amounts of language model training data have be come available ranging from approximately one billion words (the Gigaword corpora from the Linguistic Data Consortium) to trillions of words (Brants et al, 2007)", 
    "clean_text": "Since that time, however, increasingly large amounts of language model training data have become available ranging from approximately one billion words (the Gigaword corpora from the Linguistic Data Consortium) to trillions of words (Brants et al, 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "P09-2086", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Taro, Watanabe | Hajime, Tsukada | Hideki, Isozaki", 
    "raw_text": "We implemented an N -gramindexer/estimatorusing MPI inspired by the MapReduce implementation of N -gram language model index ing/estimation pipeline (Brants et al, 2007)", 
    "clean_text": "We implemented an N -gram indexer/estimator using MPI inspired by the MapReduce implementation of N-gram language model indexing/estimation pipeline (Brants et al, 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "P13-1151", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "David, Kauchak", 
    "raw_text": "In machine translation, improved language models have resulted in significant improvements in translation performance (Brantsetal., 2007)", 
    "clean_text": "In machine translation, improved language models have resulted in significant improvements in translation performance (Brants et al., 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "D12-1107", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Kenneth, Heafield | Philipp, Koehn | Alon, Lavie", 
    "raw_text": "This makes the storage requirement for higher-quality modifiedKneser-Ney smoothing comparable to stupid back off (Brants et al 2007)", 
    "clean_text": "This makes the storage requirement for higher-quality modified Kneser-Ney smoothing comparable to stupid back off (Brants et al 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "P12-1016", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Spence, Green | John, DeNero", 
    "raw_text": "Our distributed 4-gram language model was trained on 600 million words of Arabic text, also collected from many sources including the Web (Brants et al, 2007)", 
    "clean_text": "Our distributed 4-gram language model was trained on 600 million words of Arabic text, also collected from many sources including the Web (Brants et al, 2007).", 
    "keep_for_gold": 0
  }
]