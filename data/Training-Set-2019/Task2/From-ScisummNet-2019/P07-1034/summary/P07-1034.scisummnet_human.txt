Instance Weighting for Domain Adaptation in NLP
Domain adaptation is an important problem in natural language processing (NLP) due to the lack of labeled data in novel domains.
In this paper, we study the domain adaptation problem from the instance weighting perspective.
We formally analyze and characterize the domain adaptation problem from a distributional view, and show that there are two distinct needs for adaptation, corresponding to the different distributions of instances and classification functions in the source and the target domains.
We then propose a general instance weighting framework for domain adaptation.
Our empirical results on three NLP tasks show that incorporating and exploiting more information from the target domain through instance weighting is effective.
We weigh training instances based on their similarity to unlabeled target domain data.
We find that balanced bootstrapping is more effective in domain adaptation than standard bootstrapping.
In our instance weighting, we assign larger weights to transferable instances so that the model trained on the source domain can adapt more effectively to the target domain.
