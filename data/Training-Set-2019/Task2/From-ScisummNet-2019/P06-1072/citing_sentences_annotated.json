[
  {
    "citance_No": 1, 
    "citing_paper_id": "W06-1638", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Markus, Dreyer | Jason M., Eisner", 
    "raw_text": "of (Smith and Eisner, 2006)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "I08-1012", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Wenliang, Chen | Daisuke, Kawahara | Kiyotaka, Uchimoto | Yujie, Zhang | Hitoshi, Isahara", 
    "raw_text": "(Smith and Eisner, 2006) presents an approach to improve the accuracy of a dependency grammar induction models by EM from unlabeled data", 
    "clean_text": "(Smith and Eisner, 2006) presents an approach to improve the accuracy of a dependency grammar induction models by EM from unlabeled data.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P09-2001", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Shay B., Cohen | Noah A., Smith", 
    "raw_text": "For example, Smith and Eisner (2006) have penalized the approximate posterior over dependency structures in a natural language grammar induction task to avoid long range dependencies between words. Grac? a et al (2007) added linear constraints on expected values of features of the hidden variables in an alignment task", 
    "clean_text": "For example, Smith and Eisner (2006) have penalized the approximate posterior over dependency structures in a natural language grammar induction task to avoid long range dependencies between words.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "P09-2001", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Shay B., Cohen | Noah A., Smith", 
    "raw_text": "We follow the idea of annealing proposed in Rose et al (1990) and Smith and Eisner (2006) for the? by gradually loosening hard constraints on? as the variational EM algorithm proceeds", 
    "clean_text": "We follow the idea of annealing proposed in Rose et al (1990) and Smith and Eisner (2006) for the ? by gradually loosening hard constraints on ? as the variational EM algorithm proceeds.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P09-2001", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Shay B., Cohen | Noah A., Smith", 
    "raw_text": "This is a strict model reminiscent of the successful application of structural bias to grammar induction (Smith and Eisner, 2006)", 
    "clean_text": "This is a strict model reminiscent of the successful application of structural bias to grammar induction (Smith and Eisner, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "C08-1091", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Roi, Reichart | Ari, Rappoport", 
    "raw_text": "based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of (Seginer, 2007)", 
    "clean_text": "These include the constituent-context model (CCM) (Klein and Manning, 2002), its extension using a dependency model (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar-based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of (Seginer, 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "P09-1041", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Gregory, Druck | Gideon S., Mann | Andrew, McCallum", 
    "raw_text": "This demonstrates that constructing an appropriate neighbor hood function can be delicate and challenging. Smith and Eisner (2006) propose structural annealing (SA), in which a strong bias for local dependency attachments is enforced early in learning, and then gradually relaxed", 
    "clean_text": "Smith and Eisner (2006) propose structural annealing (SA), in which a strong bias for local dependency attachments is enforced early in learning, and then gradually relaxed.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "P09-1041", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Gregory, Druck | Gideon S., Mann | Andrew, McCallum", 
    "raw_text": "Finally, note that structural annealing (Smith and Eisner, 2006) provides 66.7% accuracy onWSJ10 when choosing the best performing annealing schedule (Smith, 2006)", 
    "clean_text": "Finally, note that structural annealing (Smith and Eisner, 2006) provides 66.7% accuracy on WSJ10 when choosing the best performing annealing schedule (Smith, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "W09-1120", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Roi, Reichart | Ari, Rappoport", 
    "raw_text": "els (Bod, 2006a; Bod, 2006b), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) that we use in this work", 
    "clean_text": "These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) that we use in this work.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "P08-1088", 
    "citing_paper_authority": 56, 
    "citing_paper_authors": "Aria, Haghighi | Percy, Liang | Taylor, Berg-Kirkpatrick | Dan, Klein", 
    "raw_text": "As we run EM, we gradually increase the number of edges to retain. In our context, bootstrapping has a similar motivation to the annealing approach of Smith and Eisner (2006), which also tries to alter the space of hidden outputs in the E-step over time to facilitate learning in the M-step, though of course the use of boot strapping in general is quite widespread (Yarowsky, 1995)", 
    "clean_text": "In our context, bootstrapping has a similar motivation to the annealing approach of Smith and Eisner (2006), which also tries to alter the space of hidden outputs in the E-step over time to facilitate learning in the M-step, though of course the use of bootstrapping in general is quite widespread (Yarowsky, 1995).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "W09-1108", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Dmitry, Davidov | Roi, Reichart | Ari, Rappoport", 
    "raw_text": "These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U) DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here", 
    "clean_text": "These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U) DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "N10-1116", 
    "citing_paper_authority": 16, 
    "citing_paper_authors": "Valentin I., Spitkovsky | Hiyan, Alshawi | Daniel, Jurafsky", 
    "raw_text": "Analogously, Baby Steps induces an early structural locality bias (Smith and Eisner, 2006), then relaxes it, as if annealing (Smith and Eisner, 2004)", 
    "clean_text": "Analogously, Baby Steps induces an early structural locality bias (Smith and Eisner, 2006), then relaxes it, as if annealing (Smith and Eisner, 2004).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "P11-1067", 
    "citing_paper_authority": 14, 
    "citing_paper_authors": "Roy, Schwartz | Omri, Abend | Roi, Reichart | Ari, Rappoport", 
    "raw_text": "Smith and Eisner (2006) used a structural locality bias, experimenting on five languages", 
    "clean_text": "Smith and Eisner (2006) used a structural locality bias, experimenting on five languages.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "P09-1004", 
    "citing_paper_authority": 15, 
    "citing_paper_authors": "Omri, Abend | Roi, Reichart | Ari, Rappoport", 
    "raw_text": "No table examples are (Clark, 2003) for unsupervisedPOS tagging and (Smith and Eisner, 2006) for unsupervised dependency parsing", 
    "clean_text": "Notable examples are (Clark, 2003) for unsupervised POS tagging and (Smith and Eisner, 2006) for unsupervised dependency parsing.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "P10-2036", 
    "citing_paper_authority": 14, 
    "citing_paper_authors": "Jennifer, Gillenwater | Kuzman, Ganchev | Jo&atilde;o, Gra&ccedil;a | Fernando, Pereira | Ben, Taskar", 
    "raw_text": "Following the example of Smith and Eisner (2006), we strip punctuation from the sentences and keep only sentences of length? 10", 
    "clean_text": "Following the example of Smith and Eisner (2006), we strip punctuation from the sentences and keep only sentences of length > 10.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "D09-1086", 
    "citing_paper_authority": 27, 
    "citing_paper_authors": "David A., Smith | Jason M., Eisner", 
    "raw_text": "Our model is thus a form of quasi-synchronous grammar (QG) (Smith and Eisner, 2006a)", 
    "clean_text": "Our model is thus a form of quasi-synchronous grammar (QG) (Smith and Eisner, 2006a).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "D09-1086", 
    "citing_paper_authority": 27, 
    "citing_paper_authors": "David A., Smith | Jason M., Eisner", 
    "raw_text": "These alignment classes are called configurations (Smith and Eisner, 2006a, and following)", 
    "clean_text": "These alignment classes are called configurations (Smith and Eisner, 2006a, and following).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "D09-1086", 
    "citing_paper_authority": 27, 
    "citing_paper_authors": "David A., Smith | Jason M., Eisner", 
    "raw_text": "Thus, our generative model is a quasi-synchronous grammar, exactly as in (Smith and Eisner, 2006a)", 
    "clean_text": "Thus, our generative model is a quasi-synchronous grammar, exactly as in (Smith and Eisner, 2006a).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "D09-1086", 
    "citing_paper_authority": 27, 
    "citing_paper_authors": "David A., Smith | Jason M., Eisner", 
    "raw_text": "(? 2) are incorporated into the generative process as in Smith and Eisner (2006a)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "W11-1109", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Anders, S&oslash;gaard", 
    "raw_text": "For each si? s, ti is assumed to have been built the following way: The arguments of a head h in direction d are generated one after another with the probability that no more arguments of h should be generated in direction d conditioned on h, d and whether this would be the first argument of h in direction d. The POS tag of the argument of h is generated given h and d. Klein and Manning (2004) use expectation maximization (EM) to estimate probabilities with manually tuned linguistically-biased priors. Smith and Eisner (2005) use contrastive estimation instead of EM, while Smith and Eisner (2006) use structural annealing which penalizes long-distance dependencies initially, gradually weakening the penalty during training", 
    "clean_text": "Smith and Eisner (2005) use contrastive estimation instead of EM, while Smith and Eisner (2006) use structural annealing which penalizes long-distance dependencies initially, gradually weakening the penalty during training.", 
    "keep_for_gold": 0
  }
]