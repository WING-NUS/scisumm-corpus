A Language Modeling Approach To Predicting Reading Difficulty
We demonstrate a new research approach to the problem of predicting the reading difficulty of a text passage, by recasting readability in terms of statistical language modeling.
We derive a measure based on an extension of multinomial naive Bayes classification that combines multiple language models to estimate the most likely grade level for a given passage.
The resulting classifier is not specific to any particular subject and can be trained with relatively little labeled data.
We perform predictions for individual Web pages in English and compare our performance to widely-used semantic variables from traditional readability measures.
We show that with minimal changes, the classifier may be retrained for use with French Web documents.
For both English and French, the classifier maintains consistently good correlation with labeled grade level (0.63 to 0.79) across all test sets.
Some traditional semantic variables such as type-token ratio gave the best performance on commercial calibrated test passages, while our language modeling approach gave better accuracy for Web documents and very short passages (less than 10 words).
We use a smoothed unigram language model to predict the grade reading levels of web page documents and short passages.
