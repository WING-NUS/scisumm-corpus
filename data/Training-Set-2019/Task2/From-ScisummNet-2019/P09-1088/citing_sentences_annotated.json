[
  {
    "citance_No": 1, 
    "citing_paper_id": "N10-1028", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Philip, Blunsom | Trevor, Cohn", 
    "raw_text": "Blunsom et al (2009) describe a blocked sampler following John son et al (2007) which uses the Metropolis-Hastings algorithm to correct proposal samples drawn from an approximating SCFG, however this is discounted as impractical due to the O (|f |3|e|3) complexity", 
    "clean_text": "Blunsom et al (2009) describe a blocked sampler following John son et al (2007) which uses the Metropolis-Hastings algorithm to correct proposal samples drawn from an approximating SCFG, however this is discounted as impractical due to the O (|f |3|e|3) complexity.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "N10-1028", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Philip, Blunsom | Trevor, Cohn", 
    "raw_text": "(Blunsom et al, 2009) 49.6 44 -110.3 Slice (a=0.15 ,b=1) LBinit", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "N10-1028", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Philip, Blunsom | Trevor, Cohn", 
    "raw_text": "In the following experiments we compare the slice sampler and the Gibbs sampler (Blunsom et al, 2009), in terms of mixing and translation quality", 
    "clean_text": "In the following experiments we compare the slice sampler and the Gibbs sampler (Blunsom et al, 2009), in terms of mixing and translation quality.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "N10-1028", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Philip, Blunsom | Trevor, Cohn", 
    "raw_text": "We evaluate three initialisers: M4: the symmetrised output of GIZA++factorised into ITG form (as used in Blunsom et al (2009)); M1: the output of a heavily pruned ITG parser using the IBM Model 1 prior for the rule probabilities; 9 and LB: left-branching monotone derivations.10We experiment with the Chinese? English translation task from IWSLT, as used in Blunsom et al (2009) .11 Figure 1 shows LLH curves for the samplers initialised with the M1 and LB derivations, plus the curve for Gibbs sampler with the M4initialiser.12Table 1 gives BLEU scores on Test-05 for phrase based translation models built from the 1500thsample for the various models along with the average time per sample and their final log-likelihood", 
    "clean_text": "We evaluate three initialisers: M4: the symmetrised output of GIZA++factorised into ITG form (as used in Blunsom et al (2009)).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P13-2070", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Tingting, Li | Tiejun, Zhao | Andrew, Finch | Chunyue, Zhang", 
    "raw_text": "Following (Blunsom et al, 2009) we used a vague gamma prior? (10? 4, 104), and sampled new values from a log-normal distribution whose mean was the value of the parameter, and variance was 0.3. We used the Metropolis-Hastingsalgorithm to determine whether this new sample would be accepted", 
    "clean_text": "Following (Blunsom et al, 2009) we used a vague gamma prior (10? 4, 104), and sampled new values from a log-normal distribution whose mean was the value of the parameter, and variance was 0.3.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "W10-3813", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Yanjun, Ma | Andy, Way", 
    "raw_text": "Recent progress in better parameteri sation and approximate inference (Blunsom et al., 2009) can only augment the performance of these models to a similar level as the baseline where bidirectional word alignments are combined with heuristics and subsequently used to induce translation equivalence (e.g", 
    "clean_text": "Recent progress in better parameterisation and approximate inference (Blunsom et al., 2009) can only augment the performance of these models to a similar level as the baseline where bidirectional word alignments are combined with heuristics and subsequently used to induce translation equivalence (e.g. (Koehn et al., 2003)).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "W11-2167", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Baskaran, Sankaran | Gholamreza, Haffari | Anoop, Sarkar", 
    "raw_text": "Other recent approaches use Gibbs sampler for learning the SCFG by exploring a fixed grammar having pre-defined rule templates (Blunsom et al, 2008) or by reasoning over the space of derivations (Blunsom et al, 2009)", 
    "clean_text": "Other recent approaches use Gibbs sampler for learning the SCFG by exploring a fixed grammar having pre-defined rule templates (Blunsom et al, 2008) or by reasoning over the space of derivations (Blunsom et al, 2009).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "W11-2167", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Baskaran, Sankaran | Gholamreza, Haffari | Anoop, Sarkar", 
    "raw_text": "We explore a subset of the space of rules being cons id ered by (Blunsom et al, 2009) ?i.e., only those rules satisfying the word alignments and heuristically grown phrase alignments", 
    "clean_text": "We explore a subset of the space of rules being considered by (Blunsom et al, 2009) i.e., only those rules satisfying the word alignments and heuristically grown phrase alignments.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "P13-1033", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Yang, Feng | Trevor, Cohn", 
    "raw_text": "For this we adopt the approach of Blunsom et al (2009b), who present a method for maintaining table counts without needing to record the table assignments for each translation decision", 
    "clean_text": "For this we adopt the approach of Blunsom et al (2009b), who present a method for maintaining table counts without needing to record the table assignments for each translation decision.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "P13-1033", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Yang, Feng | Trevor, Cohn", 
    "raw_text": "We apply the technique from Blunsom et al (2009a) of using multiple processors to perform approximate Gibbs sampling which they showed achieved equivalent performance to the exact Gibbs sampler", 
    "clean_text": "We apply the technique from Blunsom et al (2009a) of using multiple processors to perform approximate Gibbs sampling which they showed achieved equivalent performance to the exact Gibbs sampler.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "P13-1033", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Yang, Feng | Trevor, Cohn", 
    "raw_text": "Our ArEn training data comprises several LDCcorpora,6 using the same experimental setup as in Blunsom et al (2009a)", 
    "clean_text": "Our ArEn training data comprises several LDCcorpora, using the same experimental setup as in Blunsom et al (2009a).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "D11-1081", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Xinyan, Xiao | Yang, Liu | Qun, Liu | Shouxun, Lin", 
    "raw_text": "We use this method motivated by Gibbs Sampler (Blunsom et al, 2009) which has been used for efficiently learning rules", 
    "clean_text": "We use this method motivated by Gibbs Sampler (Blunsom et al, 2009) which has been used for efficiently learning rules.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "W10-2915", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Markos, Mylonakis | Khalil, Sima'an", 
    "raw_text": "Of these, some concentrate on evaluating word-alignment, directly such as (Zhang et al, 2008) or indirectly by evaluating a heuristically trained hierarchical translation system from sampled phrasal alignments (Blunsom et al, 2009)", 
    "clean_text": "Of these, some concentrate on evaluating word-alignment, directly such as (Zhang et al, 2008) or indirectly by evaluating a heuristically trained hierarchical translation system from sampled phrasal alignments (Blunsom et al, 2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "W10-2915", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Markos, Mylonakis | Khalil, Sima'an", 
    "raw_text": "Our smoothing distribution of phrase pairs for all pre-terminals considers source-target phrase lengths drawn from a Poisson distribution with unit mean, drawing subsequently the words of each of the phrases uniformly from the vocabulary of each language, similar to (Blunsom et al, 2009)", 
    "clean_text": "Our smoothing distribution of phrase pairs for all pre-terminals considers source-target phrase lengths drawn from a Poisson distribution with unit mean, drawing subsequently the words of each of the phrases uniformly from the vocabulary of each language, similar to (Blunsom et al, 2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "D10-1052", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Hendra, Setiawan | Chris, Dyer | Philip, Resnik", 
    "raw_text": "Recent work ,e.g. by Blunsom et al (2009) and Haghihi et al", 
    "clean_text": "Recent work, e.g. by Blunsom et al (2009) and Haghihi et al. (2009) just to name a few, show that alignment models that bear closer resemblance to state-of-the art translation model consistently yields not only a better alignment quality but also an improved translation quality.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "P11-1001", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Andreas, Zollmann | Stephan, Vogel", 
    "raw_text": "Blunsom et al (2009) present a nonparametric PSCFG translation model that directly induces a grammar from parallel sentences without the use of or constraints from a word-alignment model, and 8 Cohn and Blunsom (2009) achieve the same for tree-to-string grammars, with encouraging results on small data", 
    "clean_text": "Blunsom et al (2009) present a nonparametric PSCFG translation model that directly induces a grammar from parallel sentences without the use of or constraints from a word-alignment model, and Cohn and Blunsom (2009) achieve the same for tree-to-string grammars, with encouraging results on small data.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "D11-1018", 
    "citing_paper_authority": 13, 
    "citing_paper_authors": "John, DeNero | Jakob, Uszkoreit", 
    "raw_text": "Also related to STIR is previous work on bilingual grammar induction from parallel corpora using ITG (Blunsom et al, 2009)", 
    "clean_text": "Also related to STIR is previous work on bilingual grammar induction from parallel corpora using ITG (Blunsom et al, 2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "P13-1077", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Trevor, Cohn | Gholamreza, Haffari", 
    "raw_text": "Another strand of related research isin estimating a broader class of synchronous gram mars than ITGs, such as SCFGs (Blunsom et al, 2009b; Levenberg et al, 2012)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "P13-1077", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Trevor, Cohn | Gholamreza, Haffari", 
    "raw_text": "We discard sentences with length above 30 from the datasets for all experiments.9 Sampler configuration Samplers are initialised with trees created from GIZA++ alignments constructed using a SCFGfactorisation method (Blunsom et al, 2009a)", 
    "clean_text": "Samplers are initialised with trees created from GIZA++ alignments constructed using a SCFG factorisation method (Blunsom et al, 2009a).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "P13-1077", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Trevor, Cohn | Gholamreza, Haffari", 
    "raw_text": "For AREN experiments the language model is trained on English data as (Blunsom et al, 2009a), and for FA-EN and UR EN the English data are the target sides of the bilingual training data", 
    "clean_text": "For AREN experiments the language model is trained on English data as (Blunsom et al, 2009a), and for FA-EN and UR EN the English data are the target sides of the bilingual training data.", 
    "keep_for_gold": 0
  }
]