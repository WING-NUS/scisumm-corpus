[
  {
    "citance_No": 1, 
    "citing_paper_id": "W09-0432", 
    "citing_paper_authority": 25, 
    "citing_paper_authors": "Nicola, Bertoldi | Marcello, Federico", 
    "raw_text": "As already observed in previous literature (Macherey et al, 2008), first iterations of the tuning process produces very bad weights (even closeto 0); this exceptional performance drop is attributed to an over-fitting on the candidate reposi tory. Configurations exploiting the small development set (c, d) show a slower and more unstable convergence; however, their final performance in Table 3 result only slightly lower than that obtained with the standard dev sets (a, b)", 
    "clean_text": "As already observed in previous literature (Macherey et al, 2008), first iterations of the tuning process produces very bad weights (even close to 0); this exceptional performance drop is attributed to an over-fitting on the candidate repository.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "W12-3159", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Tagyoung, Chung | Michel, Galley", 
    "raw_text": "Recent efforts ex tended MERT to work on lattices (Macherey et al,2008) and hyper graphs (Kumar et al, 2009)", 
    "clean_text": "Recent efforts extended MERT to work on lattices (Macherey et al, 2008) and hypergraphs (Kumar et al, 2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "W12-3159", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Tagyoung, Chung | Michel, Galley", 
    "raw_text": "While this approach is similar in spirit to lattice-based MERT (Machereyet al, 2008), there is a crucial difference", 
    "clean_text": "While this approach is similar in spirit to lattice-based MERT (Macherey et al, 2008), there is a crucial difference.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "W12-6219", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Aurelien, Waite | Graeme, Blackwood | William, Byrne", 
    "raw_text": "A key property of the line optimisation is that it can consider a large set of hypotheses encoded as a weighted directed acyclic graph (Macherey et al, 2008), which is called a lattice", 
    "clean_text": "A key property of the line optimisation is that it can consider a large set of hypotheses encoded as a weighted directed acyclic graph (Macherey et al, 2008), which is called a lattice.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "W12-6219", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Aurelien, Waite | Graeme, Blackwood | William, Byrne", 
    "raw_text": "Notethat the upper envelope is completely defined by hypo the ses e4 ,e3, and e1, together with the intersection points? 1 and? 2 (after Macherey et al (2008), Fig", 
    "clean_text": "Note that the upper envelope is completely defined by hypotheses e4 ,e3, and e1, together with the intersection points ?1 and ?2 (after Macherey et al (2008), Fig. 1).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "W12-6219", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Aurelien, Waite | Graeme, Blackwood | William, Byrne", 
    "raw_text": "The first step of line op timisation is to compute this compact representation of the upper envelope. Macherey et al (2008) use methods from computational geometry to compute the upper envelope", 
    "clean_text": "Macherey et al (2008) use methods from computational geometry to compute the upper envelope.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "W12-6219", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Aurelien, Waite | Graeme, Blackwood | William, Byrne", 
    "raw_text": "Macherey et al (2008) describe a procedure for con ducting line optimisation directly over a word lattice encoding the hypotheses in Cs", 
    "clean_text": "Macherey et al (2008) describe a procedure for conducting line optimisation directly over a word lattice encoding the hypotheses in Cs.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "W12-6219", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Aurelien, Waite | Graeme, Blackwood | William, Byrne", 
    "raw_text": "The SweepLine algorithm (Bentley and Ottmann, 1979) is applied to the union to discard redundant linear functions and their associated hypotheses (Macherey et al, 2008)", 
    "clean_text": "The SweepLine algorithm (Bentley and Ottmann, 1979) is applied to the union to discard redundant linear functions and their associated hypotheses (Macherey et al, 2008).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "W12-6219", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Aurelien, Waite | Graeme, Blackwood | William, Byrne", 
    "raw_text": "(1) along the line? M1+ ?dM1. Macherey? s theorem (Macherey et al, 2008) states that an upper bound for the number of linear functions in the up per envelope at the final state is equal to the number of edges in the lattice", 
    "clean_text": "Macherey's theorem (Macherey et al, 2008) states that an upper bound for the number of linear functions in the upper envelope at the final state is equal to the number of edges in the lattice.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "W12-6219", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Aurelien, Waite | Graeme, Blackwood | William, Byrne", 
    "raw_text": "We compare feature weight optimisation using k best MERT (Och, 2003), lattice MERT (Macherey et al, 2008), and tropical geometry MERT", 
    "clean_text": "We compare feature weight optimisation using k best MERT (Och, 2003), lattice MERT (Macherey et al, 2008), and tropical geometry MERT.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "W12-6219", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Aurelien, Waite | Graeme, Blackwood | William, Byrne", 
    "raw_text": "Both TGMERT and LMERT converge to a small gain over MERTin fewer iterations, consistent with previous re ports (Macherey et al, 2008)", 
    "clean_text": "Both TGMERT and LMERT converge to a small gain over MERT in fewer iterations, consistent with previous reports (Macherey et al, 2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "W10-1702", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Boxing, Chen | George, Foster | Roland, Kuhn", 
    "raw_text": "Weights on feature functions are found by lattice MERT (Macherey et al, 2008)", 
    "clean_text": "Weights on feature functions are found by lattice MERT (Macherey et al, 2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "P09-1065", 
    "citing_paper_authority": 14, 
    "citing_paper_authors": "Yang, Liu | Haitao, Mi | Yang, Feng | Qun, Liu", 
    "raw_text": "In the future, we plan to optimize feature weights for max-translation decoding directly on the entire packed translation hyper graph rather than on n-best derivations, following the lattice based MERT (Macherey et al, 2008)", 
    "clean_text": "In the future, we plan to optimize feature weights for max-translation decoding directly on the entire packed translation hypergraph rather than on n-best derivations, following the lattice based MERT (Macherey et al, 2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "D11-1018", 
    "citing_paper_authority": 13, 
    "citing_paper_authors": "John, DeNero | Jakob, Uszkoreit", 
    "raw_text": "In all cases, the final log-linear models were optimized on the devset using lattice-based Minimum Error Rate Training (Macherey et al, 2008) .Table 2 shows that STIR improves over the base line system by a large margin of 3.84% BLEU (test)", 
    "clean_text": "In all cases, the final log-linear models were optimized on the dev set using lattice-based Minimum Error Rate Training (Macherey et al, 2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "W09-0426", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Chris, Dyer | Hendra, Setiawan | Yuval, Marton | Philip, Resnik", 
    "raw_text": "To tune the feature weights of our system, we used a variant of the minimum error training algorithm (Och, 2003) that computes the error statistics from the target sentences from the translation search space (represented by a packed forest) that are exactly those that are minimally discriminable by changing the feature weights along a single vector in the dimensions of the feature space (Macherey et al, 2008)", 
    "clean_text": "To tune the feature weights of our system, we used a variant of the minimum error training algorithm (Och, 2003) that computes the error statistics from the target sentences from the translation search space (represented by a packed forest) that are exactly those that are minimally discriminable by changing the feature weights along a single vector in the dimensions of the feature space (Macherey et al, 2008).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "W09-0426", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Chris, Dyer | Hendra, Setiawan | Yuval, Marton | Philip, Resnik", 
    "raw_text": "To tune the model parameters, we selected a set of compound words from a subset of the German development set, manually created a linguistically plausible segmentation of these words, and used this to select the parameters of the log-linear model using a lattice minimum error training algorithm to minimize WER (Macherey et al, 2008)", 
    "clean_text": "To tune the model parameters, we selected a set of compound words from a subset of the German development set, manually created a linguistically plausible segmentation of these words, and used this to select the parameters of the log-linear model using a lattice minimum error training algorithm to minimize WER (Macherey et al, 2008).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "D10-1059", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Samidh, Chatterjee | Nicola, Cancedda", 
    "raw_text": "Recognizing this shortcoming, Macherey et al (2008) extended the MERT algorithm so as to use the whole set of candidate translations compactly represented in the search lattice produced by the decoder, instead of only a N-best list of candidates extracted from it", 
    "clean_text": "Recognizing this shortcoming, Macherey et al (2008) extended the MERT algorithm so as to use the whole set of candidate translations compactly represented in the search lattice produced by the decoder, instead of only a N-best list of candidates extracted from it.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "D10-1059", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Samidh, Chatterjee | Nicola, Cancedda", 
    "raw_text": "But the Down hill Simplex Algorithm loses its robustness as the dimension goes up by more than 10 (Machereyet al, 2008)", 
    "clean_text": "But the Down hill Simplex Algorithm loses its robustness as the dimension goes up by more than 10 (Machereyet al, 2008).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "D10-1059", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Samidh, Chatterjee | Nicola, Cancedda", 
    "raw_text": "Macherey et al (2008) propose a new variation of MERT where the algorithm is tuned to work on the whole phrase lattice instead of N-best list only", 
    "clean_text": "Macherey et al (2008) propose a new variation of MERT where the algorithm is tuned to work on the whole phrase lattice instead of N-best list only.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "D10-1059", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Samidh, Chatterjee | Nicola, Cancedda", 
    "raw_text": "If we use either N-best lists or random samples to form the translation pool, and M is the size of the translation pool, then computing the envelope can be done in time O (M log M) using the SweepLine algorithm reproduced as Algorithm 1 in (Macherey et al, 2008)", 
    "clean_text": "If we use either N-best lists or random samples to form the translation pool, and M is the size of the translation pool, then computing the envelope can be done in time O (M log M) using the SweepLine algorithm reproduced as Algorithm 1 in (Macherey et al, 2008).", 
    "keep_for_gold": 0
  }
]