[
  {
    "citance_No": 1, 
    "citing_paper_id": "W03-1001", 
    "citing_paper_authority": 19, 
    "citing_paper_authors": "Christoph, Tillmann", 
    "raw_text": "A word link extension algorithm similar to the one presented in this paper is given in (Koehn et al, 2003)", 
    "clean_text": "A word link extension algorithm similar to the one presented in this paper is given in (Koehn et al, 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P14-2046", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Xing, Shi | Kevin, Knight | Heng, Ji", 
    "raw_text": "Following phrase-based methods in statistical machine translation (Koehn et al., 2003) and machine transliteration (FinchandSumita, 2008), we model substitution of longer sequences", 
    "clean_text": "Following phrase-based methods in statistical machine translation (Koehn et al., 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "W07-0701", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Arul, Menezes | Chris, Quirk", 
    "raw_text": "Modern phrasal SMT systems such as (Koehn et al., 2003) derive much of their power from being able to memorize and use long phrases", 
    "clean_text": "Modern phrasal SMT systems such as (Koehn et al., 2003) derive much of their power from being able to memorize and use long phrases.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "W07-0701", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Arul, Menezes | Chris, Quirk", 
    "raw_text": "We compared our system to Pharaoh, a leading phrasal SMT decoder (Koehn et al, 2003), and our tree let system", 
    "clean_text": "We compared our system to Pharaoh, a leading phrasal SMT decoder (Koehn et al, 2003), and our tree let system.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "W08-0404", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Michael, Subotin", 
    "raw_text": "The use of conditional probabilities in standard lexical models also gives us a straightforward way to generalize them in the same way as phrase models. Consider the lexical model pw (ry|rx), defined fol lowing Koehn et al (2003), with a denoting the most frequent word alignment observed for the rule in the training set", 
    "clean_text": "Consider the lexical model pw (ry|rx), defined following Koehn et al (2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "W08-0404", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Michael, Subotin", 
    "raw_text": "symmetrization (Koehn et al, 2003) .Thresholds for phrase extraction and decoder pruning were set to values typical for the baseline system (Chiang, 2007)", 
    "clean_text": "All conditions use word alignments produced by sequential iterations of IBM model 1, HMM, and IBM model 4 in GIZA++ , followed by 'diag-and' symmetrization (Koehn et al., 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "P09-1065", 
    "citing_paper_authority": 14, 
    "citing_paper_authors": "Yang, Liu | Haitao, Mi | Yang, Feng | Qun, Liu", 
    "raw_text": "(Koehn et al, 2003)", 
    "clean_text": "We obtained word alignments of training data by first running GIZA++ (Och and Ney, 2003) and then applying the refinement rule \"grow-diag-final-and\" (Koehn et al., 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "W06-3105", 
    "citing_paper_authority": 29, 
    "citing_paper_authors": "John, DeNero | Dan, Gillick | James, Zhang | Dan, Klein", 
    "raw_text": "At the core of a phrase-based statistical machine translation system is a phrase table containing pairs of source and target language phrases, each weighted by a conditional translation probability. Koehn et al (2003a) showed that translation quality is very sensitive to how this table is extracted from the training data", 
    "clean_text": "Koehn et al (2003a) showed that translation quality is very sensitive to how this table is extracted from the training data.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "W06-3105", 
    "citing_paper_authority": 29, 
    "citing_paper_authors": "John, DeNero | Dan, Gillick | James, Zhang | Dan, Klein", 
    "raw_text": "While the model and training regimen for? EM differ from the model from Marcu and Wong (2002), we achieved results similar to Koehn et al (2003a):? EM slightly underperformed? H. Figure 1 compares the BLEUscores using each estimate", 
    "clean_text": "we achieved results similar to Koehn et al (2003a).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "W04-3219", 
    "citing_paper_authority": 64, 
    "citing_paper_authors": "Chris, Quirk | Chris, Brockett | William B., Dolan", 
    "raw_text": "Recent work in SMT has shown that simple phrase-based MT systems can outperform more sophisticated word-based systems (e.g. Koehn et al. 2003)", 
    "clean_text": "Recent work in SMT has shown that simple phrase-based MT systems can outperform more sophisticated word-based systems (e.g. Koehn et al. 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "D11-1018", 
    "citing_paper_authority": 13, 
    "citing_paper_authors": "John, DeNero | Jakob, Uszkoreit", 
    "raw_text": "We apply STIR as a pre-ordering step in a state of-the-art phrase-based translation system from English to Japanese (Koehn et al, 2003)", 
    "clean_text": "We apply STIR as a pre-ordering step in a state of-the-art phrase-based translation system from English to Japanese (Koehn et al, 2003).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "C08-2032", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Takashi, Tsunakawa | Naoaki, Okazaki | Jun'ichi, Tsujii", 
    "raw_text": "This paper proposes a method for building abilingual lexicon through a pivot language by using phrase-based statistical machine translation (SMT) (Koehn et al, 2003)", 
    "clean_text": "This paper proposes a method for building a bilingual lexicon through a pivot language by using phrase-based statistical machine translation (SMT) (Koehn et al, 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "W12-4402", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Min, Zhang | Haizhou, Li | A, Kumaran | Ming, Liu", 
    "raw_text": "This system is based on phrase-based statistical machine transliteration (SMT) (Finch and Sumita, 2008), an approach initially developed for machine translation (Koehn et al, 2003), where the SMT system? s log-linear model is augmented with a set of features specifically suited to the task of transliteration", 
    "clean_text": "This system is based on phrase-based statistical machine transliteration (SMT) (Finch and Sumita, 2008), an approach initially developed for machine translation (Koehn et al, 2003), where the SMT system's log-linear model is augmented with a set of features specifically suited to the task of transliteration.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "W12-4402", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Min, Zhang | Haizhou, Li | A, Kumaran | Ming, Liu", 
    "raw_text": "In addition to the most popular techniques such as Phrase-Based Machine Transliteration (Koehnet al, 2003), CRF, re-ranking, DirecTL-pde coder, Non-Parametric Bayesian Co-segmentation (Finch et al, 2011), and Multi-to-Multi Joint Source Channel Model (Chen et al, 2011) in the News 2011, we are delighted to see that several new techniques have been proposed and explored with promising results reported ,includingRNN-based LM (Finch et al, 2012), English Segmentation algorithm (Zhang et al, 2012), JLIS re ranking method (Wu et al, 2012) ,improvedm2m-aligner (Okuno, 2012), multiple reference optimized CRF (Ammar et al, 2012), language dependent adaptation (Kondrak et al, 2012) and two-stage CRF (Kuo et al, 2012)", 
    "clean_text": "In addition to the most popular techniques such as Phrase-Based Machine Transliteration (Koehnet al, 2003), CRF, re-ranking, DirecTL-pde coder, Non-Parametric Bayesian Co-segmentation (Finch et al, 2011), and Multi-to-Multi Joint Source Channel Model (Chen et al, 2011) in the News 2011, we are delighted to see that several new techniques have been proposed and explored with promising results reported, including RNN-based LM (Finch et al, 2012), English Segmentation algorithm (Zhang et al, 2012), JLIS reranking method (Wu et al, 2012) ,improved m2m-aligner (Okuno, 2012), multiple reference optimized CRF (Ammar et al, 2012), language dependent adaptation (Kondrak et al, 2012) and two-stage CRF (Kuo et al, 2012).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "W06-3122", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Marian, Olteanu | Pasin, Suriyentrakorn | Dan, Moldovan", 
    "raw_text": "count (f?) 2http: //www.phramer.org/? Java-based open-source phrase based SMT system 3http: //www.isi.edu/licensed-sw/carmel/ 4http: //www.speech.sri.com/projects/srilm/ 5http: //www.iccs.inf.ed.ac.uk/ ?pkoehn/training.tgz 150? lexical weighting (Koehn et al, 2003): lex (f? |e?, a)= n ?i=1 1| {j| (i, j)? a}|?? (i, j)? a w (fi|ej) lex (e? |f?, a)= m ?j=1 1| {i| (i, j)? a}|?? (i, j)? a w (ej |fi)? phrase penalty:? (f? |e?)= e; log (? (f? |e?))= 1 2.2 Decoding", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "N07-2015", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Sa&scaron;a, Hasan | Richard, Zens | Hermann, Ney", 
    "raw_text": "Many research groups use a decoder based on a log-linear approach incorporating phrases as main paradigm (Koehn et al, 2003)", 
    "clean_text": "Many research groups use a decoder based on a log-linear approach incorporating phrases as main paradigm (Koehn et al, 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "P12-1018", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Graham, Neubig | Taro, Watanabe | Shinsuke, Mori | Tatsuya, Kawahara", 
    "raw_text": "In the formal ism presented above, this means that each ei must be included in at most one span, and for each span u= v. Traditionally, these models are run in both directions and combined using heuristics to create many-to-many alignments (Koehn et al, 2003)", 
    "clean_text": "Traditionally, these models are run in both directions and combined using heuristics to create many-to-many alignments (Koehn et al, 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "P11-2079", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Wang, Ling | Tiago, Lu&iacute;s | Jo&atilde;o, Gra&ccedil;a | Isabel, Trancoso | Lu&iacute;sa, Coheur", 
    "raw_text": "The translation quality of statistical phrase-based systems (Koehn et al, 2003) is heavily dependent on the quality of the translation and reordering models generated during the phrase extraction algorithm (Ling et al, 2010)", 
    "clean_text": "The translation quality of statistical phrase-based systems (Koehn et al, 2003) is heavily dependent on the quality of the translation and reordering models generated during the phrase extraction algorithm (Ling et al, 2010).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "P10-1064", 
    "citing_paper_authority": 12, 
    "citing_paper_authors": "Yifan, He | Yanjun, Ma | Josef, van Genabith | Andy, Way", 
    "raw_text": "Note that we remove the exact matches in the TM from our dataset, because ex act matches will be reused and not presented to the post-editor in a typical TM setting. As for the SMT system, we use a standard log-linear PB-SMT model (Och and Ney, 2002): GIZA++ implementation of IBM word alignment model 4,1 the refinement and phrase extraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003), a 5-gram language model with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke, 2002) on the English side of the training data, and Moses (Koehn et al, 2007) to decode", 
    "clean_text": "As for the SMT system, we use a standard log-linear PB-SMT model (Och and Ney, 2002): GIZA++ implementation of IBM word alignment model 4, the refinement and phrase extraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003), a 5-gram language model with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke, 2002) on the English side of the training data, and Moses (Koehn et al, 2007) to decode.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "W10-3707", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Santanu, Pal | Sudip Kumar, Naskar | Pavel, Pecina | Sivaji, Bandyopadhyay | Andy, Way", 
    "raw_text": "Secondly, the IBM Models only al low at most one word in the source language to correspond to a word in the target language (Marcu, 2001, Koehn et al, 2003)", 
    "clean_text": "", 
    "keep_for_gold": 0
  }
]