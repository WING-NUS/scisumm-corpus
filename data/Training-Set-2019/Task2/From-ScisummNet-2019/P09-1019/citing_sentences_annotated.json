[
  {
    "citance_No": 1, 
    "citing_paper_id": "N10-1141", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "John, DeNero | Shankar, Kumar | Ciprian, Chelba | Franz Josef, Och", 
    "raw_text": "=arg max d? Dsw (d) using standard max-sum (Viterbi) inference over D.We learn the weights of this consensus model using hyper graph-based minimum-error-rate training (Kumar et al, 2009)", 
    "clean_text": "We learn the weights of this consensus model using hyper graph-based minimum-error-rate training (Kumar et al, 2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "N10-1141", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "John, DeNero | Shankar, Kumar | Ciprian, Chelba | Franz Josef, Och", 
    "raw_text": "Kumar et al (2009) describes an efficient approximate algorithm for computing n-gram posterior probabilities", 
    "clean_text": "Kumar et al (2009) describes an efficient approximate algorithm for computing n-gram posterior probabilities.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "N10-1141", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "John, DeNero | Shankar, Kumar | Ciprian, Chelba | Franz Josef, Och", 
    "raw_text": "For each system, we report the performance of max-derivation decoding (MAX), hyper graph-based MBR (Kumar et al, 2009), and a linear version of forest-based consensus decoding (CON) (DeNero et al., 2009)", 
    "clean_text": "For each system, we report the performance of max-derivation decoding (MAX), hyper graph-based MBR (Kumar et al, 2009), and a linear version of forest-based consensus decoding (CON) (DeNero et al., 2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "N10-1141", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "John, DeNero | Shankar, Kumar | Ciprian, Chelba | Franz Josef, Och", 
    "raw_text": "These results show that while expansion is necessary for correctness, it does not affect performance. Table 6 compares exact n-gram posterior computation (Algorithm 1) to the approximation described by Kumar et al (2009)", 
    "clean_text": "Table 6 compares exact n-gram posterior computation (Algorithm 1) to the approximation described by Kumar et al (2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P14-2127", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Kai, Zhao | Liang, Huang | Haitao, Mi | Abe, Ittycheriah", 
    "raw_text": "We evaluate MAXFORCE for HIERO over two CHEN corpora, IWSLT09 and FBIS, and compare the performance with vanilla n-best MERT (Och, 2003) from Moses (Koehn et al, 2007), Hypergraph MERT (Kumar et al, 2009), and PRO (Hopkins and May, 2011) from cdec", 
    "clean_text": "We evaluate MAXFORCE for HIERO over two CHEN corpora, IWSLT09 and FBIS, and compare the performance with vanilla n-best MERT (Och, 2003) from Moses (Koehn et al, 2007), Hypergraph MERT (Kumar et al, 2009), and PRO (Hopkins and May, 2011) from cdec.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "P11-2031", 
    "citing_paper_authority": 61, 
    "citing_paper_authors": "Jonathan H., Clark | Chris, Dyer | Alon, Lavie | Noah A., Smith", 
    "raw_text": "Thecdec MERT implementation performs inference over the decoder search space which is structured as a hyper graph (Kumar et al, 2009)", 
    "clean_text": "The cdec MERT implementation performs inference over the decoder search space which is structured as a hyper graph (Kumar et al, 2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "N10-1139", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Cyril, Allauzen | Shankar, Kumar | Wolfgang, Macherey | Mehryar, Mohri | Michael D., Riley", 
    "raw_text": "These experiments demonstrate the efficiency of our algorithm which is shown empirically to be two orders of magnitude faster than Tromble et al (2008) and more than 3 times faster than even an approximation algorithm specifically designed for this problem (Kumar et al, 2009)", 
    "clean_text": "These experiments demonstrate the efficiency of our algorithm which is shown empirically to be two orders of magnitude faster than Tromble et al (2008) and more than 3 times faster than even an approximation algorithm specifically designed for this problem (Kumar et al, 2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "N12-1026", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Taro, Watanabe", 
    "raw_text": "MERT learns parameters from forests (Kumar et al, 2009) with 4 restarts and 8 random directions in each iteration", 
    "clean_text": "MERT learns parameters from forests (Kumar et al, 2009) with 4 restarts and 8 random directions in each iteration.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "P10-2006", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Graeme, Blackwood | Adri&agrave; de, Gispert | William, Byrne", 
    "raw_text": "While other fast MBR approximations are possible (Kumar et al, 2009), we show how the exact path posterior probabilities can be calculated and applied in the implementation of Equation (1) for efficient MBR decoding over lattices", 
    "clean_text": "While other fast MBR approximations are possible (Kumar et al, 2009), we show how the exact path posterior probabilities can be calculated and applied in the implementation of Equation (1) for efficient MBR decoding over lattices.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "P10-4002", 
    "citing_paper_authority": 67, 
    "citing_paper_authors": "Chris, Dyer | Adam, Lopez | Juri, Ganitkevitch | Jonathan, Weese | Ferhan, Ture | Philip, Blunsom | Hendra, Setiawan | Vladimir, Eidelman | Philip, Resnik", 
    "raw_text": "Rather than computing an error surface using k best approximations of the decoder search space ,cdec? s implementation performs inference over the full hyper graph structure (Kumar et al, 2009)", 
    "clean_text": "Rather than computing an error surface using k best approximations of the decoder search space, cdec's implementation performs inference over the full hyper graph structure (Kumar et al, 2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "W10-1707", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Vladimir, Eidelman | Chris, Dyer | Philip, Resnik", 
    "raw_text": "Then, max-marginals were computed using the forward-backward algorithm and used to prune out paths that were greater than a factor of 2.3 from the best path, as recommended by Dyer2This algorithm is equivalent to the hyper graph MERT algorithm described by Kumar et al (2009)", 
    "clean_text": "Then, max-marginals were computed using the forward-backward algorithm and used to prune out paths that were greater than a factor of 2.3 from the best path, as recommended by Dyer. This algorithm is equivalent to the hyper graph MERT algorithm described by Kumar et al (2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "P11-1126", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Nan, Duan | Mu, Li | Ming, Zhou", 
    "raw_text": "30 features are used to annotate each hypothesis in HM decoding, including: 8 n-gram posterior features computed from PB/DHPB forests for; 8 stemmed n-gram posterior features computed from stemmed PB/DHPB forests for; 4 n-gram posterior features and 1 length posterior feature computed from the mixture search space of HMde coder for; 1 LM feature; 1 word count feature; 1 dictionary-based feature; 2 grammar specified rule penalty features for either BTG HMD or SCFG-HMD; 4 count features for newly generated n-grams in HM decoding for. All n-gram posteriors are computed using the efficient algorithm proposed by Kumar et al (2009)", 
    "clean_text": "All n-gram posteriors are computed using the efficient algorithm proposed by Kumar et al (2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "W10-1756", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Abhishek, Arun | Barry, Haddow | Philipp, Koehn", 
    "raw_text": "We report results using the Moses implementation of Viterbi ,nbest MBR and lattice MBR decoding (Kumar et al, 2009)", 
    "clean_text": "We report results using the Moses implementation of Viterbi, nbest MBR and lattice MBR decoding (Kumar et al, 2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "W10-1756", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Abhishek, Arun | Barry, Haddow | Philipp, Koehn", 
    "raw_text": "(Kumar et al, 2009) mention that the linear approximation to BLEU used in their lattice MBR algorithm is not guaranteed to match corpus BLEU, especially on unseen test sets", 
    "clean_text": "(Kumar et al, 2009) mention that the linear approximation to BLEU used in their lattice MBR algorithm is not guaranteed to match corpus BLEU, especially on unseen test sets.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "W10-1756", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Abhishek, Arun | Barry, Haddow | Philipp, Koehn", 
    "raw_text": "However, due tothe unified nature of the training and decoding criterion in our approach, the minimum risk trained weights can be plugged directly into the sampler MBR decoder, whereas lattice MBR requires an additional expensive step of tuning the model hyper-parameters (Kumar et al, 2009)", 
    "clean_text": "However, due to the unified nature of the training and decoding criterion in our approach, the minimum risk trained weights can be plugged directly into the sampler MBR decoder, whereas lattice MBR requires an additional expensive step of tuning the model hyper-parameters (Kumar et al, 2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "P11-1125", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Taro, Watanabe | Eiichiro, Sumita", 
    "raw_text": "The feature weight vector w in Equation 1 is tuned by MERT over hyper graphs (Kumar et al, 2009) .We use three lower-cased 5-gram language mod 1253 els hilm (d): English Gigaword Fourthedition1, the English side of French-English 109 corpus and the news commentary Englishdata2", 
    "clean_text": "The feature weight vector w in Equation 1 is tuned by MERT over hyper graphs (Kumar et al, 2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "P12-1032", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Shujie, Liu | Chi-Ho, Li | Mu, Li | Ming, Zhou", 
    "raw_text": "These approaches include the work of Kumar and Byrne (2004), which re-ranks the n best output of a MT decoder, and the work of Tromble et al (2008) and Kumar et al (2009), which does MBR decoding for lattices and hyper graphs", 
    "clean_text": "These approaches include the work of Kumar and Byrne (2004), which re-ranks the n best output of a MT decoder, and the work of Tromble et al (2008) and Kumar et al (2009), which does MBR decoding for lattices and hyper graphs.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "W12-6219", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Aurelien, Waite | Graeme, Blackwood | William, Byrne", 
    "raw_text": "The line optimisation procedure can also be applied to a hyper graph representation of the hypo the ses (Kumar et al, 2009)", 
    "clean_text": "The line optimisation procedure can also be applied to a hyper graph representation of the hypotheses (Kumar et al, 2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "W12-6219", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Aurelien, Waite | Graeme, Blackwood | William, Byrne", 
    "raw_text": "The LMERT and TGMERToptimisation algorithms are particularly suitable for this realisation of hiero in that the lattice representation avoids the need to use the hyper graph formulation of MERT given by Kumar et al (2009) .MERToptimises the weights of the following features: target language model, source-to-target and target-to-source translation models, word and rule penalties, number of usages of the glue rule, word deletion scale factor, source-to-target and target-to source lexical models, and three count-based features that track the frequency of rules in the parallel data (Bender et al, 2007)", 
    "clean_text": "The LMERT and TGMERT optimisation algorithms are particularly suitable for this realisation of hiero in that the lattice representation avoids the need to use the hyper graph formulation of MERT given by Kumar et al (2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "D10-1059", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Samidh, Chatterjee | Nicola, Cancedda", 
    "raw_text": "More recently, this algorithm was extended to work with hyper graphs encoding a huge number of translations produced by MT systems based on Synchronous Context Free Grammars (Kumar et al, 2009)", 
    "clean_text": "More recently, this algorithm was extended to work with hyper graphs encoding a huge number of translations produced by MT systems based on Synchronous Context Free Grammars (Kumar et al, 2009).", 
    "keep_for_gold": 0
  }
]