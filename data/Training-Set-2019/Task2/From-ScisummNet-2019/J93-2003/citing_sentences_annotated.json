[
  {
    "citance_No": 1, 
    "citing_paper_id": "W93-0301", 
    "citing_paper_authority": 44, 
    "citing_paper_authors": "Ido, Dagan | Kenneth Ward, Church | William A., Gale", 
    "raw_text": "The program takes the output of char_align (Church, 1993), a robust alternative to sentence-based alignment programs, and applies word-level constraints using a version of Brown el al. &apos; s Model 2 (Brown et al, 1993), modified and extended to deal with robustness i sues", 
    "clean_text": "The program takes the output of char_align (Church, 1993), a robust alternative to sentence-based alignment programs, and applies word-level constraints using a version of Brown's Model 2 (Brown et al, 1993), modified and extended to deal with robustness issues.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "W06-3119", 
    "citing_paper_authority": 97, 
    "citing_paper_authors": "Andreas, Zollmann | Ashish, Venugopal", 
    "raw_text": "Recent work in machine translation has evolved from the traditional word (Brown et al, 1993) and phrase based (Koehn et al, 2003a) models to include hierarchical phrase models (Chiang, 2005) and bilingual synchronous grammars (Melamed, 2004) .These advances are motivated by the desire to integrate richer knowledge sources within the translation process with the explicit goal of producing mo refluent translations in the target language", 
    "clean_text": "Recent work in machine translation has evolved from the traditional word (Brown et al, 1993).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "W12-3124", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Antti-Veikko, Rosti | Xiaodong, He | Damianos, Karakos | Gregor, Leusch | Yuan, Cao | Markus, Freitag | Spyros, Matsoukas | Hermann, Ney | Jason, Smith | Bing, Zhang", 
    "raw_text": "The IBM Model 1 (Brownet al, 1993) and hidden Markov model (HMM) (Vo gel et al, 1996) are used to estimate the alignment", 
    "clean_text": "The IBM Model 1 (Brown et al, 1993) and hidden Markov model (HMM) (Vo gel et al, 1996) are used to estimate the alignment.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "D10-1062", 
    "citing_paper_authority": 6, 
    "citing_paper_authors": "Tagyoung, Chung | Daniel, Gildea", 
    "raw_text": "A special NULL word is typically used when learning word alignment (Brown et al, 1993)", 
    "clean_text": "A special NULL word is typically used when learning word alignment (Brown et al, 1993).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P07-1001", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Yonggang, Deng | Yuqing, Gao", 
    "raw_text": "For instance, the most relaxed IBM Model-1, which assumes that any source word can be generated by any target word equally regardless of distance, can be improved by demanding a Markov process of alignments as inHMM-based models (Vogel et al, 1996), or implementing a distribution of number of target words linked to a source word as in IBM fertility-based models (Brown et al, 1993)", 
    "clean_text": "For instance, the most relaxed IBM Model-1, which assumes that any source word can be generated by any target word equally regardless of distance, can be improved by demanding a Markov process of alignments as in HMM-based models (Vogel et al, 1996), or implementing a distribution of number of target words linked to a source word as in IBM fertility-based models (Brown et al, 1993).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "P07-1001", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Yonggang, Deng | Yuqing, Gao", 
    "raw_text": "It can be applied to complicated models such IBM Model-4 (Brown et al, 1993)", 
    "clean_text": "It can be applied to complicated models such IBM Model-4 (Brown et al, 1993).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "P07-1001", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Yonggang, Deng | Yuqing, Gao", 
    "raw_text": "We shall take HMM-based word alignment model (Vogel et al., 1996) as an example and follow the notation of (Brown et al, 1993)", 
    "clean_text": "We shall take HMM-based word alignment model (Vogel et al., 1996) as an example and follow the notation of (Brown et al, 1993).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "D12-1089", 
    "citing_paper_authority": 4, 
    "citing_paper_authors": "Daisy, Stanton | Richard, Zens | Peng, Xu", 
    "raw_text": "The baseline system uses the common phrase translation models, such as p (e? |f?) and p (f? |e?), lexical models, word and phrase penalty, distortion penalty as well as a lexicalized reordering model (Zens and Ney, 2006) .The word alignment was trained with six iterations of IBM model 1 (Brown et al 1993) and 6 iterations of the HMM alignment model (Vogel et al 1996) using a symmetric lexicon (Zens et al 2004)", 
    "clean_text": "The word alignment was trained with six iterations of IBM model 1 (Brown et al 1993).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "W11-1209", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Sanjika, Hewavitharana | Stephan, Vogel", 
    "raw_text": "The feature set we use is inspired by Munteanu and Marcu (2005) who define the features based on IBM Model-1 (Brown et al, 1993) alignments for source and target pairs", 
    "clean_text": "The feature set we use is inspired by Munteanu and Marcu (2005) who define the features based on IBM Model-1 (Brown et al, 1993) alignments for source and target pairs.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "P06-2061", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Shahram, Khadivi | Richard, Zens | Hermann, Ney", 
    "raw_text": "In (Brown et al, 1994), the authors proposed a method to integrate the IBM translation model 2 (Brown et al, 1993) with an ASR system", 
    "clean_text": "In (Brown et al, 1994), the authors proposed a method to integrate the IBM translation model 2 (Brown et al, 1993) with an ASR system.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "P06-2061", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Shahram, Khadivi | Richard, Zens | Hermann, Ney", 
    "raw_text": "Werescore the ASR N -best lists with the standard HMM (Vogel et al, 1996) and IBM (Brown et al, 1993) MTmodels", 
    "clean_text": "We rescore the ASR N -best lists with the standard HMM (Vogel et al, 1996) and IBM (Brown et al, 1993) MT models.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "P06-2061", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Shahram, Khadivi | Richard, Zens | Hermann, Ney", 
    "raw_text": "In (Brown et al, 1993), three alignment models are described that include fertility models, these are IBM Models 3, 4, and 5", 
    "clean_text": "In (Brown et al, 1993), three alignment models are described that include fertility models, these are IBM Models 3, 4, and 5.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "P13-1033", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Yang, Feng | Trevor, Cohn", 
    "raw_text": "Word based models have a long history in machine translation, starting with the venerable IBM translation models (Brown et al, 1993) and the hid den Markov model (Vogel et al, 1996)", 
    "clean_text": "Word based models have a long history in machine translation, starting with the venerable IBM translation models (Brown et al, 1993) and the hidden Markov model (Vogel et al, 1996).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "P06-2014", 
    "citing_paper_authority": 23, 
    "citing_paper_authors": "Colin, Cherry | Dekang, Lin", 
    "raw_text": "Originally introduced as a byproduct of training statistical translation model sin (Brown et al, 1993), word alignment has become the first step in training most statistical translation systems, and alignments are useful to a host of other tasks", 
    "clean_text": "Originally introduced as a byproduct of training statistical translation model since (Brown et al, 1993), word alignment has become the first step in training most statistical translation systems, and alignments are useful to a host of other tasks.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "P06-2014", 
    "citing_paper_authority": 23, 
    "citing_paper_authors": "Colin, Cherry | Dekang, Lin", 
    "raw_text": "The IBM models (Brown et al, 1993) benefit from a one-tomany constraint, where each target word has ex 105 the tax causes unrest l &apos; imp? t cause le malaise Figure 1: A cohesion constraint violation", 
    "clean_text": "The IBM models (Brown et al, 1993) benefit from a one-to-many constraint.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "N04-4003", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Michael, Paul | Eiichiro, Sumita | Seiichi, Yamamoto", 
    "raw_text": "The statistical machine translation framework (SMT) formulates the problem of translating a sentence from asource language S into a target language T as the maximization problem of the conditional probability: TM? LM =argmaxT p (S|T)? p (T), (1) where p (S|T) is called a translation model (TM), rep resenting the generation probability from T into S, p (T) is called a language model (LM) and represents the likelihood of the target language (Brown et al, 1993)", 
    "clean_text": "The statistical machine translation framework (SMT) formulates the problem of translating a sentence from a source language S into a target language T as the maximization problem of the conditional probability: TM LM =argmaxT p (S|T) p (T), (1) where p (S|T) is called a translation model (TM), rep resenting the generation probability from T into S, p (T) is called a language model (LM) and represents the likelihood of the target language (Brown et al, 1993).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "N07-1057", 
    "citing_paper_authority": 10, 
    "citing_paper_authors": "Fei, Xia | William H., Lewis", 
    "raw_text": "We then train IBM mod els (Brown et al, 1993) using the GIZA++ package (Och and Ney, 2000)", 
    "clean_text": "We then train IBM models (Brown et al, 1993) using the GIZA++ package (Och and Ney, 2000).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "W06-2402", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Patrik, Lambert | Rafael E., Banches", 
    "raw_text": "Statistical machine translation (SMT) was originally focused on word to word translation and was based on the noisy channel approach (Brown et al., 1993)", 
    "clean_text": "Statistical machine translation (SMT) was originally focused on word to word translation and was based on the noisy channel approach (Brown et al., 1993).", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "P10-4002", 
    "citing_paper_authority": 67, 
    "citing_paper_authors": "Chris, Dyer | Adam, Lopez | Juri, Ganitkevitch | Jonathan, Weese | Ferhan, Ture | Philip, Blunsom | Hendra, Setiawan | Vladimir, Eidelman | Philip, Resnik", 
    "raw_text": "Phrase-based models (Koehn et al, 2003), lexical translation models (Brown et al, 1993), and finite-state conditional random fields (Sha and Pereira, 2003) exemplify the former, and hierarchical phrase-based models the latter (Chiang, 2007)", 
    "clean_text": "lexical translation models (Brown et al, 1993) exemplify the former, and hierarchical phrase-based models the latter (Chiang, 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "W11-2146", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Sanjika, Hewavitharana | Nguyen, Bach | Qin, Gao | Vamshi, Ambati | Stephan, Vogel", 
    "raw_text": "We defined a feature set which includes: length ratio and length difference between source and target sentences, lexical probability scores similar to IBM model 1 (Brown et al, 1993), number of aligned/unaligned words and the length of the longest aligned word sequence", 
    "clean_text": "We defined a feature set which includes: length ratio and length difference between source and target sentences, lexical probability scores similar to IBM model 1 (Brown et al, 1993), number of aligned/unaligned words and the length of the longest aligned word sequence.", 
    "keep_for_gold": 0
  }
]