[
  {
    "citance_No": 1, 
    "citing_paper_id": "W12-0514", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Hanna, Fadida | Reshef, Shilon | Shuly, Wintner", 
    "raw_text": "Table 2 lists the BLEU (Papineni et al, 2002) and METEOR (Denkowski and Lavie, 2011) scores of both systems", 
    "clean_text": "Table 2 lists the BLEU (Papineni et al, 2002) and METEOR (Denkowski and Lavie, 2011) scores of both systems.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P14-1065", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Francisco, Guzm\u00c3\u00a1n | Shafiq R., Joty | Llu\u00c3\u00ads, M\u00c3\u00a0rquez | Preslav, Nakov", 
    "raw_text": "4http: //nlp.lsi.upc.edu/asiya/ To complement the set of individual metrics that participated at the WMT12 metrics task ,wealso computed the scores of other commonly used evaluation metrics: BLEU (Papineni et al, 2002), NIST (Doddington, 2002), TER (Snover et al, 2006), ROUGE-W (Lin, 2004), and three METEOR variants (Denkowski and Lavie, 2011): METEOR-ex (exact match), METEOR-st (+stemming) and METEOR-sy (+synonyms)", 
    "clean_text": "To complement the set of individual metrics that participated at the WMT12 metrics task, we also computed the scores of other commonly used evaluation metrics: BLEU (Papineni et al, 2002), NIST (Doddington, 2002), TER (Snover et al, 2006), ROUGE-W (Lin, 2004), and three METEOR variants (Denkowski and Lavie, 2011): METEOR-ex (exact match), METEOR-st (+stemming) and METEOR-sy (+synonyms).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "W12-3901", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Sokratis, Sofianopoulos | Marina, Vassiliou | George, Tambouratzis", 
    "raw_text": "7 For the current evaluation phase four automatic evaluation metrics have been employed ,i.e. BLEU (Papineni et al, 2002), NIST (NIST 2002), Meteor (Denkowski and Lavie, 2011) and TER (Snover et al, 2006)", 
    "clean_text": "For the current evaluation phase four automatic evaluation metrics have been employed, i.e. BLEU (Papineni et al, 2002), NIST (NIST 2002), Meteor (Denkowski and Lavie, 2011) and TER (Snover et al, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "W12-3119", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Chunyang, Wu | Hai, Zhao", 
    "raw_text": "Inverted Automatic Scores: For each Spanish system output sentence, we translate it to English and get its scores of BLEU and METEOR (Denkowski and Lavie, 2011)", 
    "clean_text": "Inverted Automatic Scores: For each Spanish system output sentence, we translate it to English and get its scores of BLEU and METEOR (Denkowski and Lavie, 2011).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "E12-1055", 
    "citing_paper_authority": 20, 
    "citing_paper_authors": "Rico, Sennrich", 
    "raw_text": "We report two translation measures: BLEU (Papineni et al 2002) and METEOR 1.3 (Denkowski and Lavie, 2011)", 
    "clean_text": "We report two translation measures: BLEU (Papineni et al 2002) and METEOR 1.3 (Denkowski and Lavie, 2011).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "W12-3131", 
    "citing_paper_authority": 5, 
    "citing_paper_authors": "Michael, Denkowski | Greg, Hanneman | Alon, Lavie", 
    "raw_text": "System performance is evaluated on newstest 2011 using BLEU (uncased and cased) (Papineni et al, 2002), Meteor (Denkowski and Lavie, 2011), and TER (Snover et al, 2006)", 
    "clean_text": "System performance is evaluated on newstest 2011 using BLEU (uncased and cased) (Papineni et al, 2002), Meteor (Denkowski and Lavie, 2011), and TER (Snover et al, 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "W12-4202", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Rui, Wang | Petya, Osenova | Kiril, Simov", 
    "raw_text": "The baseline results (non-factored model) under the standard evaluation metrics are shown in the first row of Table 3 in terms of BLEU (Papineni et al, 2002) and METEOR (Denkowski and Lavie, 2011)", 
    "clean_text": "The baseline results (non-factored model) under the standard evaluation metrics are shown in the first row of Table 3 in terms of BLEU (Papineni et al, 2002) and METEOR (Denkowski and Lavie, 2011).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "N12-1017", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Markus, Dreyer | Daniel, Marcu", 
    "raw_text": "The quotient is lower under the automatic metrics Meteor (Version 1.3, (Denkowski and Lavie, 2011)), BLEU and TERp (Snover et al, 2009)", 
    "clean_text": "The quotient is lower under the automatic metrics Meteor (Version 1.3, (Denkowski and Lavie, 2011)), BLEU and TERp (Snover et al, 2009).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "S12-1064", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Sascha, Fendrich | Katharina, W&Atilde;&curren;schle", 
    "raw_text": "The Meteor scoring tool (Denkowski and Lavie, 2011) for evaluating the output of statistical machine translation systems can be used to calculate the similarity of two sentences in the same language", 
    "clean_text": "The Meteor scoring tool (Denkowski and Lavie, 2011) for evaluating the output of statistical machine translation systems can be used to calculate the similarity of two sentences in the same language.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "P12-1018", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Graham, Neubig | Taro, Watanabe | Shinsuke, Mori | Tatsuya, Kawahara", 
    "raw_text": "We evaluate translation quality using BLEU score (Papineni et al, 2002), both on the word and character level (with n= 4), as well as METEOR (Denkowski and Lavie, 2011) on the word level", 
    "clean_text": "We evaluate translation quality using BLEU score (Papineni et al, 2002), both on the word and character level (with n= 4), as well as METEOR (Denkowski and Lavie, 2011) on the word level.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "W12-3102", 
    "citing_paper_authority": 51, 
    "citing_paper_authors": "Chris, Callison-Burch | Philipp, Koehn | Christof, Monz | Matt, Post | Radu, Soricut | Lucia, Specia", 
    "raw_text": "21 Metric IDs Participant AMBER National Research Council Canada (Chen et al, 2012) METEOR CMU (Denkowski and Lavie, 2011) SAGAN-STS FaMAF, UNC, Argentina (Castillo and Estrella, 2012) SEMPOS Charles University (Macha? c ?ek and Bojar, 2011) SIMBLEU University of Sheffield (Song and Cohn, 2011) SPEDE Stanford University (Wang and Manning, 2012) TERRORCAT University of Zurich, DFKI, Charles U (Fishel et al, 2012) BLOCKERRCATS, ENXERRCATS, WORD BLOCKERRCATS, XENERRCATS, POSF DFKI (Popovic, 2012) Table 6: Participants in the metrics task.the manual evaluation is useful for validating automatic evaluation metrics", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "P13-1138", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Ravikumar, Kondadadi | Blake Stephen, Howald | Frank, Schilder", 
    "raw_text": "We used BLEU? 4 (Papineni et al, 2002), METEOR (v.1.3) (Denkowski and Lavie, 2011) to evaluate the texts at document level", 
    "clean_text": "We used BLEU 4 (Papineni et al, 2002), METEOR (v.1.3) (Denkowski and Lavie, 2011) to evaluate the texts at document level.", 
    "keep_for_gold": 0
  }
]