[
  {
    "citance_No": 1, 
    "citing_paper_id": "W03-1101", 
    "citing_paper_authority": 19, 
    "citing_paper_authors": "Chin-Yew, Lin", 
    "raw_text": "In a recent study (Lin and Hovy, 2003a), we showed that the recall-based unigram co occurrence automatic scoring metric correlates highly with human evaluation and has high recall and precision in predicting the statistical significance of results comparing with its human counter part", 
    "clean_text": "In a recent study (Lin and Hovy, 2003a), we showed that the recall-based unigram co occurrence automatic scoring metric correlates highly with human evaluation and has high recall and precision in predicting the statistical significance of results comparing with its human counter part.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "P14-1087", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Jun-Ping, Ng | Yan, Chen | Min-Yen, Kan | Zhoujun, Li", 
    "raw_text": "Summarizationevaluation is done using ROUGE-2 (R-2) (Lin and Hovy, 2003), as it has previously been shown to correlate well with human assessment (Lin, 2004) and is of ten used to evaluate automatic text summarization", 
    "clean_text": "Summarization evaluation is done using ROUGE-2 (R-2) (Lin and Hovy, 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "N04-1019", 
    "citing_paper_authority": 60, 
    "citing_paper_authors": "Ani, Nenkova | Rebecca J., Passonneau", 
    "raw_text": "In ma chine translation, the rankings from the automatic BLEUmethod (Papineni et al, 2002) have been shown to correlate well with human evaluation, and it has been widely used since and has even been adapted for summarization (Lin and Hovy, 2003)", 
    "clean_text": "it has been widely used since and has even been adapted for summarization (Lin and Hovy, 2003).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "N04-1019", 
    "citing_paper_authority": 60, 
    "citing_paper_authors": "Ani, Nenkova | Rebecca J., Passonneau", 
    "raw_text": "Lin and Hovy (2003) have shown that a unigram co occurrence statistic, computed with stop words ignored, between a summary and a set of models can be used to assign scores for a test suite that highy correlates with the scores assigned by human evaluators at DUC", 
    "clean_text": "Lin and Hovy (2003) have shown that a unigram co-occurrence statistic, computed with stop words ignored, between a summary and a set of models can be used to assign scores for a test suite that highly correlates with the scores assigned by human evaluators at DUC.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "N04-1019", 
    "citing_paper_authority": 60, 
    "citing_paper_authors": "Ani, Nenkova | Rebecca J., Passonneau", 
    "raw_text": "Lin and Hovy (2002) and Lin and Hovy (2003) were the first to systematically point out problems with the large scale DUC evaluation and to look to solutions by seeking more robust automatic alternatives", 
    "clean_text": "Lin and Hovy (2002) and Lin and Hovy (2003) were the first to systematically point out problems with the large scale DUC evaluation and to look to solutions by seeking more robust automatic alternatives.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "N04-4001", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Michele, Banko | Lucy, Vanderwende", 
    "raw_text": "According to this model, 81% of summary sentences contained in a corpus of 300 human-written summaries of news articles on telecommunications were found to fit the cut-and-paste method, with the rest believed to have been composed from scratch.1 Another recent study (Lin and Hovy, 2003) investigated the extent to which extractive methods may be sufficient for summarization in the single-document case", 
    "clean_text": "Another recent study (Lin and Hovy, 2003) investigated the extent to which extractive methods may be sufficient for summarization in the single-document case.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "D12-1022", 
    "citing_paper_authority": 11, 
    "citing_paper_authors": "Kristian, Woodsend | Mirella, Lapata", 
    "raw_text": "Automatic evaluation was performed with ROUGE (Lin and Hovy, 2003) using TAC-2008parameter settings", 
    "clean_text": "Automatic evaluation was performed with ROUGE (Lin and Hovy, 2003) using TAC-2008parameter settings.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "C08-1062", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Wenjie, Li | Furu, Wei | Qin, Lu | Yanxiang, He", 
    "raw_text": "Among them, ROUGE5 (Lin and Hovy, 2003) is supposed to produce the most reliable scores in correspondence with human evaluations", 
    "clean_text": "Among them, ROUGE5 (Lin and Hovy, 2003) is supposed to produce the most reliable scores in correspondence with human evaluations.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "H05-1032", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Tadashi, Nomoto", 
    "raw_text": "Among some recent work with similar concerns, one notable is the Pyramid scheme (Nenkova and Passonneau, 2004) where one does not declare a particular human summary a absolute reference to compare summaries against, but rather makes every one of multiple human summaries at hand bear on evaluation; Rouge (Lin and Hovy, 2003) represents another such effort", 
    "clean_text": "Rouge (Lin and Hovy, 2003) represents another such effort.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "P07-2047", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Maofu, Liu | Wenjie, Li | Mingli, Wu | Qin, Lu", 
    "raw_text": "The automatic evaluation tool, ROUGE (Lin and Hovy, 2003), is run to evaluate the quality of the generated summaries (200 words in length)", 
    "clean_text": "The automatic evaluation tool, ROUGE (Lin and Hovy, 2003), is run to evaluate the quality of the generated summaries (200 words in length).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "W08-0112", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Fei, Liu | Yang, Liu", 
    "raw_text": "ROUGE (Lin and Hovy, 2003) has been adopted as a standard evaluation metric in various summarization tasks", 
    "clean_text": "ROUGE (Lin and Hovy, 2003) has been adopted as a standard evaluation metric in various summarization tasks.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "C10-2131", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Hideyuki, Shibuki | Takahiro, Nagai | Masahiro, Nakano | Rintaro, Miyazaki | Madoka, Ishioroshi | Tatsunori, Mori", 
    "raw_text": "Although ROUGE (Lin and Hovy, 2003) is one of the most popular methods for evaluation of summaries, it may not be appropriate for the evaluation of the mediatory summary because the scoring based on N-gram in this method can not be used to consider the fairness described in Section 2", 
    "clean_text": "Although ROUGE (Lin and Hovy, 2003) is one of the most popular methods for evaluation of summaries, it may not be appropriate for the evaluation of the mediatory summary because the scoring based on N-gram in this method can not be used to consider the fairness described in Section 2.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "W05-0905", 
    "citing_paper_authority": 11, 
    "citing_paper_authors": "Gabriel, Murray | Steve, Renals | Jean, Carletta | Johanna D., Moore", 
    "raw_text": "One semi automatic approach to evaluation is ROUGE (Linand Hovy, 2003), which is primarily based on n gram co-occurrence between automatic and human summaries", 
    "clean_text": "One semi automatic approach to evaluation is ROUGE (Lin and Hovy, 2003), which is primarily based on n gram co-occurrence between automatic and human summaries.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "W05-0905", 
    "citing_paper_authority": 11, 
    "citing_paper_authors": "Gabriel, Murray | Steve, Renals | Jean, Carletta | Johanna D., Moore", 
    "raw_text": "We used the ROUGE evaluation approach (Linand Hovy, 2003), which is based on n-gram co occurrence between machine summaries and? ideal? human summaries", 
    "clean_text": "We used the ROUGE evaluation approach (Lin and Hovy, 2003), which is based on n-gram co occurrence between machine summaries and ideal human summaries.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "W05-0905", 
    "citing_paper_authority": 11, 
    "citing_paper_authors": "Gabriel, Murray | Steve, Renals | Jean, Carletta | Johanna D., Moore", 
    "raw_text": "Lin (Lin and Hovy, 2003) has found that ROUGE-1 and ROUGE-2 correlate well with human judgments", 
    "clean_text": "Lin (Lin and Hovy, 2003) has found that ROUGE-1 and ROUGE-2 correlate well with human judgments.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "W05-0905", 
    "citing_paper_authority": 11, 
    "citing_paper_authors": "Gabriel, Murray | Steve, Renals | Jean, Carletta | Johanna D., Moore", 
    "raw_text": "According to (Lin and Hovy, 2003), ROUGE1 correlates particularly well with human judgments of informativeness", 
    "clean_text": "", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "W11-0503", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Lu, Wang | Claire, Cardie", 
    "raw_text": "Our experiments employ the aforementioned AMI meeting corpus: we compare our decision summaries to the manually generated decision abstracts for each meeting and evaluate performance using the ROUGE-1 (Lin and Hovy, 2003) text summarization evaluation metric. In the supervised summarization setting, our experiments demonstrate that with true clusterings of decision-related DAs, token-level summaries that employ limited discourse context can approach an upper bound for summaries extracted directly from DRDAs2? 0.4387 ROUGE-F1 vs. 0.5333", 
    "clean_text": "Our experiments employ the aforementioned AMI meeting corpus: we compare our decision summaries to the manually generated decision abstracts for each meeting and evaluate performance using the ROUGE-1 (Lin and Hovy, 2003) text summarization evaluation metric.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "W11-0503", 
    "citing_paper_authority": 3, 
    "citing_paper_authors": "Lu, Wang | Claire, Cardie", 
    "raw_text": "We use the ROUGE (Lin and Hovy, 2003) evaluation measure", 
    "clean_text": "We use the ROUGE (Lin and Hovy, 2003) evaluation measure.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "P10-2070", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Josef, Steinberger | Marco, Turchi | Mijail, Alexandrov-Kabadjov | Ralf, Steinberger | Nello, Cristianini", 
    "raw_text": "Our initial experimental results show that our approach is feasible, since it produces summaries, which when evaluated against the TAC 2009data1yield ROUGE scores (Lin and Hovy, 2003) comparable to the participating systems in the Summarization task at TAC 2009", 
    "clean_text": "Our initial experimental results show that our approach is feasible, since it produces summaries, which when evaluated against the TAC 2009 data yield ROUGE scores (Lin and Hovy, 2003) comparable to the participating systems in the Summarization task at TAC 2009.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "P10-2070", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Josef, Steinberger | Marco, Turchi | Mijail, Alexandrov-Kabadjov | Ralf, Steinberger | Nello, Cristianini", 
    "raw_text": "We used the standard ROUGEevaluation (Lin and Hovy, 2003) which has been also used for TAC", 
    "clean_text": "We used the standard ROUGE evaluation (Lin and Hovy, 2003) which has been also used for TAC.", 
    "keep_for_gold": 0
  }
]