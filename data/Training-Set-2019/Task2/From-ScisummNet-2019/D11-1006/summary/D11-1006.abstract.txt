We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data.
We first demonstrate that delexicalized parsers can be directly transferred between languages, producing significantly higher accuracies than unsupervised parsers.
We then use a constraint driven learning algorithm where constraints are drawn from parallel corpora to project the final parser.
Unlike previous work on projecting syntactic resources, we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers.
The projected parsers from our system result in state-of-the-art performance when compared to previously studied unsupervised and projected parsing systems across eight different languages.