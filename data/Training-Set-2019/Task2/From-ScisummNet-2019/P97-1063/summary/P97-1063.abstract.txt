Many multilingual NLP applications need to translate words between different languages, but cannot afford the computational expense of inducing or applying a full translation model.
For these applications, we have designed a fast algorithm for estimating a partial translation model, which accounts for translational equivalence only at the word level. The model's precision/recall trade-off can be directly controlled via one threshold parameter.
This feature makes the model more suitable for applications that are not fully statistical.
The model's hidden parameters can be easily conditioned on information extrinsic to the model, providing an easy way to integrate pre-existing knowledge such as part-of-speech, dictionaries, word order, etc..
Our model can link word tokens in parallel texts as well as other translation models in the literature.
Unlike other translation models, it can automatically produce dictionary-sized translation lexicons, and it can do so with over 99% accuracy.
