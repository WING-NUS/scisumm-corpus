[
  {
    "citance_No": 1, 
    "citing_paper_id": "W08-0334", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Andrew, Finch | Eiichiro, Sumita", 
    "raw_text": "In this respect our approach is similar to that of Foster and Kuhn (2007), however we used a probabilistic classifier to determine a vector of probabilities representing class-membership, rather than distance based weights", 
    "clean_text": "In this respect our approach is similar to that of Foster and Kuhn (2007), however we used a probabilistic classifier to determine a vector of probabilities representing class-membership, rather than distance based weights.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 2, 
    "citing_paper_id": "W08-0334", 
    "citing_paper_authority": 7, 
    "citing_paper_authors": "Andrew, Finch | Eiichiro, Sumita", 
    "raw_text": "Both Yamamoto and Sumita (2007) and Foster and Kuhn (2007), extended this to include the translation model", 
    "clean_text": "Both Yamamoto and Sumita (2007) and Foster and Kuhn (2007), extended this to include the translation model.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 3, 
    "citing_paper_id": "P14-1110", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Yuening, Hu | Ke, Zhai | Vladimir, Eidelman | Jordan, Boyd-Graber", 
    "raw_text": "Early efforts focus on building separate models (Foster and Kuhn, 2007) and adding features (Matsoukas et al, 2009) to model domain information", 
    "clean_text": "Early efforts focus on building separate models (Foster and Kuhn, 2007) and adding features (Matsoukas et al, 2009) to model domain information.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 4, 
    "citing_paper_id": "E12-1045", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Arianna, Bisazza | Marcello, Federico", 
    "raw_text": "Besides many works addressing holistic LM domain adaptation for SMT ,e.g. Foster and Kuhn (2007), recently methods were also proposed to explicitly adapt the LM to the discourse topic of a talk (Ruiz and Federico, 2011)", 
    "clean_text": "Besides many works addressing holistic LM domain adaptation for SMT, e.g. Foster and Kuhn (2007), recently methods were also proposed to explicitly adapt the LM to the discourse topic of a talk (Ruiz and Federico, 2011).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 5, 
    "citing_paper_id": "P13-1126", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Boxing, Chen | Roland, Kuhn | George, Foster", 
    "raw_text": "Both we restudied in (Foster and Kuhn, 2007), which concluded that the best approach was to combine sub models of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log linearly", 
    "clean_text": "Both we restudied in (Foster and Kuhn, 2007), which concluded that the best approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log linearly.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 6, 
    "citing_paper_id": "P13-1126", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Boxing, Chen | Roland, Kuhn | George, Foster", 
    "raw_text": "Thus, the variant of VSM adaptation tested here bears a superficial resemblance to domain adaptation based on mixture models for TMs, as in (Foster and Kuhn, 2007), in that both approaches rely on information about the sub corpora from which the data originate", 
    "clean_text": "Thus, the variant of VSM adaptation tested here bears a superficial resemblance to domain adaptation based on mixture models for TMs, as in (Foster and Kuhn, 2007), in that both approaches rely on information about the subcorpora from which the data originate.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 7, 
    "citing_paper_id": "P13-1126", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Boxing, Chen | Roland, Kuhn | George, Foster", 
    "raw_text": "For details, refer to (Foster and Kuhn, 2007)", 
    "clean_text": "For details, refer to (Foster and Kuhn, 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 8, 
    "citing_paper_id": "P13-1126", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Boxing, Chen | Roland, Kuhn | George, Foster", 
    "raw_text": "In (Foster and Kuhn, 2007), two kinds of linear mixture were described: linear mixture of language models (LMs), and linear mixture of translation models (TMs) .Some of the results reported above involved linear TM mixtures, but none of them involved linear LM mixtures", 
    "clean_text": "In (Foster and Kuhn, 2007), two kinds of linear mixture were described: linear mixture of language models (LMs), and linear mixture of translation models (TMs). Some of the results reported above involved linear TM mixtures, but none of them involved linear LM mixtures.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 9, 
    "citing_paper_id": "W09-0432", 
    "citing_paper_authority": 25, 
    "citing_paper_authors": "Nicola, Bertoldi | Marcello, Federico", 
    "raw_text": "In (Foster and Kuhn, 2007) two basic settings are investigated: cross-domain adaptation, in whicha small sample of parallel in-domain text is assumed, and dynamic adaptation, in which only the current input source text is considered", 
    "clean_text": "In (Foster and Kuhn, 2007) two basic settings are investigated: cross-domain adaptation, in which a small sample of parallel in-domain text is assumed, and dynamic adaptation, in which only the current input source text is considered.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 10, 
    "citing_paper_id": "W12-3155", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Christian, Buck | Nicola, Bertoldi | Mauro, Cettolo | Marcello, Federico", 
    "raw_text": "Although dynamic adaptation is closely related to static domain adaptation (Foster and Kuhn, 2007), in this scenario we are not interested in the quality of the final model", 
    "clean_text": "Although dynamic adaptation is closely related to static domain adaptation (Foster and Kuhn, 2007), in this scenario we are not interested in the quality of the final model.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 11, 
    "citing_paper_id": "D11-1033", 
    "citing_paper_authority": 36, 
    "citing_paper_authors": "Amittai, Axelrod | Xiaodong, He | Jianfeng, Gao", 
    "raw_text": "Foster and Kuhn (2007) interpolated the inand general-domain phrase tables together, assigning either linear or log-linear weights to the entries in the tables before combining overlapping entries; this is now standard practice", 
    "clean_text": "Foster and Kuhn (2007) interpolated the in and general-domain phrase tables together, assigning either linear or log-linear weights to the entries in the tables before combining overlapping entries; this is now standard practice.", 
    "keep_for_gold": 1
  }, 
  {
    "citance_No": 12, 
    "citing_paper_id": "D11-1033", 
    "citing_paper_authority": 36, 
    "citing_paper_authors": "Amittai, Axelrod | Xiaodong, He | Jianfeng, Gao", 
    "raw_text": "In this work, we directly com pare the approaches of (Foster and Kuhn, 2007) and (Koehn and Schroeder, 2007) on the systems generated from the methods mentioned in Section 2.1", 
    "clean_text": "In this work, we directly compare the approaches of (Foster and Kuhn, 2007) and (Koehn and Schroeder, 2007) on the systems generated from the methods mentioned in Section 2.1.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 13, 
    "citing_paper_id": "W10-1759", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Kashif, Shah | Lo&iuml;c, Barrault | Holger, Schwenk", 
    "raw_text": "(Foster and Kuhn, 2007) applied a mixture model approach to adapt the system to a new domain byusing weights that depend on text distances to mixture components", 
    "clean_text": "(Foster and Kuhn, 2007) applied a mixture model approach to adapt the system to a new domain by using weights that depend on text distances to mixture components.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 14, 
    "citing_paper_id": "E12-1026", 
    "citing_paper_authority": 2, 
    "citing_paper_authors": "Gennadi, Lembersky | Noam, Ordan | Shuly, Wintner", 
    "raw_text": "Specifically, we will interpolate the translation models as in Foster and Kuhn (2007), including a maximum a posteriori combination (Bacchiani et al 2006)", 
    "clean_text": "Specifically, we will interpolate the translation models as in Foster and Kuhn (2007), including a maximum a posteriori combination (Bacchiani et al 2006).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 15, 
    "citing_paper_id": "C10-1035", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Nan, Duan | Hong, Sun | Ming, Zhou", 
    "raw_text": "Foster and Kuhn (2007) presented an approach that resembles more to our work, in which they divided the training corpus into different components and integrated models trained on each component using the mixture modeling", 
    "clean_text": "Foster and Kuhn (2007) presented an approach that resembles more to our work, in which they divided the training corpus into different components and integrated models trained on each component using the mixture modeling.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 16, 
    "citing_paper_id": "E12-1055", 
    "citing_paper_authority": 20, 
    "citing_paper_authors": "Rico, Sennrich", 
    "raw_text": "Among other applications, language model perplexity has been used for domain adaptation (Foster and Kuhn, 2007)", 
    "clean_text": "Among other applications, language model perplexity has been used for domain adaptation (Foster and Kuhn, 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 17, 
    "citing_paper_id": "E12-1055", 
    "citing_paper_authority": 20, 
    "citing_paper_authors": "Rico, Sennrich", 
    "raw_text": "Mixture-modelling for language models is well established (Foster and Kuhn, 2007)", 
    "clean_text": "Mixture-modelling for language models is well established (Foster and Kuhn, 2007).", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 18, 
    "citing_paper_id": "E12-1055", 
    "citing_paper_authority": 20, 
    "citing_paper_authors": "Rico, Sennrich", 
    "raw_text": "Foster and Kuhn (2007) find that? both TM and LMadaptation are effective?, but that? combined LM and TM adaptation is not better than LM adaptation on its own? .A second strand of research in domain adaptation is data selection ,i.e. choosing a subset of the training data that is considered more relevant for the task at hand", 
    "clean_text": "Foster and Kuhn (2007) find that both TM and LM adaptation are effective, but that combined LM and TM adaptation is not better than LM adaptation on its own. A second strand of research in domain adaptation is data selection, i.e. choosing a subset of the training data that is considered more relevant for the task at hand.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 19, 
    "citing_paper_id": "W11-2211", 
    "citing_paper_authority": 1, 
    "citing_paper_authors": "Matthias, Huck | David, Vilar | Daniel, Stein | Hermann, Ney", 
    "raw_text": "Combining multiple translation models has been investigated for domain adaptation by Foster and Kuhn (2007) and Koehn and Schroeder (2007) before", 
    "clean_text": "Combining multiple translation models has been investigated for domain adaptation by Foster and Kuhn (2007) and Koehn and Schroeder (2007) before.", 
    "keep_for_gold": 0
  }, 
  {
    "citance_No": 20, 
    "citing_paper_id": "P13-2060", 
    "citing_paper_authority": 0, 
    "citing_paper_authors": "Majid, Razmara | Anoop, Sarkar", 
    "raw_text": "They used sampling without replacement to create a number of base models whose phrase-tables are combined with that of the baseline (trained on the full training-set) using linear mixture models (Foster and Kuhn, 2007)", 
    "clean_text": "They used sampling without replacement to create a number of base models whose phrase-tables are combined with that of the baseline (trained on the full training-set) using linear mixture models (Foster and Kuhn, 2007).", 
    "keep_for_gold": 0
  }
]