Learning Accurate Compact And Interpretable Tree Annotation
We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank.
Starting with a simple X-bar grammar, we learn a new grammar whose nonterminals are subsymbols of the original nonterminals.
In contrast with previous work, we are able to split various terminals to different degrees, as appropriate to the actual complexity in the data.
Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation.
On the other hand, our grammars are much more compact and substantially more accurate than previous work on automatic annotation.
Despite its simplicity, our best grammar achieves an F1 of 90.2% on the Penn Treebank, higher than fully lexicalized systems.
We use hierarchical EM training.
We show that in the domain of syntactic parsing with probabilistic context-free grammars, automatically induced grammar refinements can outperform sophisticated methods which exploit substantial manually articulated structure.
We introduce split-merge-smooth estimation.
