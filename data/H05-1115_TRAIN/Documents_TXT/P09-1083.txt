Answering Opinion Questions with Random Walks on Graphs
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 737–745,Suntec, Singapore, 2-7 August 2009. c©2009 ACL and AFNLP
Answering Opinion Questions with Random Walks on Graphs
Fangtao Li, Yang Tang, Minlie Huang, and Xiaoyan ZhuState Key Laboratory on Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and TechnologyDepartment of Computer Sci. and Tech., Tsinghua University, Beijing 100084, China
{fangtao06,tangyang9}@gmail.com,{aihuang,zxy-dcs}@tsinghua.edu.cn
Abstract
Opinion Question Answering (OpinionQA), which aims to find the authors’ sen-timental opinions on a specific target, ismore challenging than traditional fact-based question answering problems. Toextract the opinion oriented answers, weneed to consider both topic relevance andopinion sentiment issues. Current solu-tions to this problem are mostly ad-hoccombinations of question topic informa-tion and opinion information. In this pa-per, we propose an Opinion PageRankmodel and an Opinion HITS model to fullyexplore the information from different re-lations among questions and answers, an-swers and answers, and topics and opin-ions. By fully exploiting these relations,the experiment results show that our pro-posed algorithms outperform several stateof the art baselines on benchmark data set.A gain of over 10% in F scores is achievedas compared to many other systems.
1 Introduction
Question Answering (QA), which aims to pro-vide answers to human-generated questions auto-matically, is an important research area in natu-ral language processing (NLP) and much progresshas been made on this topic in previous years.However, the objective of most state-of-the-art QAsystems is to find answers to factual questions,such as “What is the longest river in the UnitedStates?” and “Who is Andrew Carnegie?” In fact,rather than factual information, people would alsolike to know about others’ opinions, thoughts andfeelings toward some specific objects, people andevents. Some examples of these questions are:“How is Bush’s decision not to ratify the KyotoProtocol looked upon by Japan and other US al-
lies?”(Stoyanov et al., 2005) and “Why do peo-ple like Subway Sandwiches?” from TAC 2008(Dang, 2008). Systems designed to deal with suchquestions are called opinion QA systems. Re-searchers (Stoyanov et al., 2005) have found thatopinion questions have very different character-istics when compared with fact-based questions:opinion questions are often much longer, morelikely to represent partial answers rather than com-plete answers and vary much more widely. Thesefeatures make opinion QA a harder problem totackle than fact-based QA. Also as shown in (Stoy-anov et al., 2005), directly applying previous sys-tems designed for fact-based QA onto opinion QAtasks would not achieve good performances.
Similar to other complex QA tasks (Chen et al.,2006; Cui et al., 2007), the problem of opinion QAcan be viewed as a sentence ranking problem. TheOpinion QA task needs to consider not only thetopic relevance of a sentence (to identify whetherthis sentence matches the topic of the question)but also the sentiment of a sentence (to identifythe opinion polarity of a sentence). Current solu-tions to opinion QA tasks are generally in ad hocstyles: the topic score and the opinion score areusually separately calculated and then combinedvia a linear combination (Varma et al., 2008) orjust filter out the candidate without matching thequestion sentiment (Stoyanov et al., 2005). How-ever, topic and opinion are not independent in re-ality. The opinion words are closely associatedwith their contexts. Another problem is that exist-ing algorithms compute the score for each answercandidate individually, in other words, they do notconsider the relations between answer candidates.The quality of a answer candidate is not only de-termined by the relevance to the question, but alsoby other candidates. For example, the good an-swer may be mentioned by many candidates.
In this paper, we propose two models to ad-dress the above limitations of previous sentence
737
ranking models. We incorporate both the topicrelevance information and the opinion sentimentinformation into our sentence ranking procedure.Meanwhile, our sentence ranking models couldnaturally consider the relationships between dif-ferent answer candidates. More specifically, ourfirst model, called Opinion PageRank, incorpo-rates opinion sentiment information into the graphmodel as a condition. The second model, calledOpinion HITS model, considers the sentences asauthorities and both question topic informationand opinion sentiment information as hubs. Theexperiment results on the TAC QA data set demon-strate the effectiveness of the proposed RandomWalk based methods. Our proposed method per-forms better than the best method in the TAC 2008competition.
The rest of this paper is organized as follows:Section 2 introduces some related works. We willdiscuss our proposed models in Section 3. In Sec-tion 4, we present an overview of our opinion QAsystem. The experiment results are shown in Sec-tion 5. Finally, Section 6 concludes this paper andprovides possible directions for future work.
2 Related Work
Few previous studies have been done on opin-ion QA. To our best knowledge, (Stoyanov etal., 2005) first created an opinion QA corpusOpQA. They find that opinion QA is a more chal-lenging task than factual question answering, andthey point out that traditional fact-based QA ap-proaches may have difficulty on opinion QA tasksif unchanged. (Somasundaran et al., 2007) arguesthat making finer grained distinction of subjectivetypes (sentiment and arguing) further improves theQA system. For non-English opinion QA, (Ku etal., 2007) creates a Chinese opinion QA corpus.They classify opinion questions into six types andconstruct three components to retrieve opinion an-swers. Relevant answers are further processed byfocus detection, opinion scope identification andpolarity detection. Some works on opinion min-ing are motivated by opinion question answering.(Yu and Hatzivassiloglou, 2003) discusses a nec-essary component for an opinion question answer-ing system: separating opinions from fact at boththe document and sentence level. (Soo-Min andHovy, 2005) addresses another important compo-nent of opinion question answering: finding opin-ion holders.
More recently, TAC 2008 QA track (evolvedfrom TREC) focuses on finding answers to opin-ion questions (Dang, 2008). Opinion questionsretrieve sentences or passages as answers whichare relevant for both question topic and questionsentiment. Most TAC participants employ a strat-egy of calculating two types of scores for answercandidates, which are the topic score measure andthe opinion score measure (the opinion informa-tion expressed in the answer candidate). How-ever, most approaches simply combined these twoscores by a weighted sum, or removed candidatesthat didn’t match the polarity of questions, in orderto extract the opinion answers.
Algorithms based on Markov Random Walkhave been proposed to solve different kinds ofranking problems, most of which are inspired bythe PageRank algorithm (Page et al., 1998) and theHITS algorithm (Kleinberg, 1999). These two al-gorithms were initially applied to the task of Websearch and some of their variants have been provedsuccessful in a number of applications, includingfact-based QA and text summarization (Erkan andRadev, 2004; Mihalcea and Tarau, 2004; Otter-bacher et al., 2005; Wan and Yang, 2008). Gener-ally, such models would first construct a directedor undirected graph to represent the relationshipbetween sentences and then certain graph-basedranking methods are applied on the graph to com-pute the ranking score for each sentence. Sen-tences with high scores are then added into theanswer set or the summary. However, to the bestof our knowledge, all previous Markov RandomWalk-based sentence ranking models only makeuse of topic relevance information, i.e. whetherthis sentence is relevant to the fact we are lookingfor, thus they are limited to fact-based QA tasks.To solve the opinion QA problems, we need toconsider both topic and sentiment in a non-trivialmanner.
3 Our Models for Opinion SentenceRanking
In this section, we formulate the opinion questionanswering problem as a topic and sentiment basedsentence ranking task. In order to naturally inte-grate the topic and opinion information into thegraph based sentence ranking framework, we pro-pose two random walk based models for solvingthe problem, i.e. an Opinion PageRank model andan Opinion HITS model.
738
3.1 Opinion PageRank Model
In order to rank sentence for opinion question an-swering, two aspects should be taken into account.First, the answer candidate is relevant to the ques-tion topic; second, the answer candidate is suitablefor question sentiment.
Considering Question Topic: We first intro-duce how to incorporate the question topic intothe Markov Random Walk model, which is simi-lar as the Topic-sensitive LexRank (Otterbacher etal., 2005). Given the set Vs = {vi} containing allthe sentences to be ranked, we construct a graphwhere each node represents a sentence and eachedge weight between sentence vi and sentence vjis induced from sentence similarity measure as fol-lows: p(i ? j) = f(i?j)P|Vs|
k=1 f(i?k), where f(i ? j)
represents the similarity between sentence vi andsentence vj , here is cosine similarity (Baeza-Yatesand Ribeiro-Neto, 1999). We define f(i? i) = 0to avoid self transition. Note that p(i ? j) is usu-ally not equal to p(j ? i). We also compute thesimilarity rel(vi|q) of a sentence vi to the questiontopic q using the cosine measure. This relevancescore is then normalized as follows to make thesum of all relevance values of the sentences equalto 1: rel'(vi|q) = rel(vi|q)P|Vs|
k=1 rel(vk|q).
The saliency score Score(vi) for sentence vican be calculated by mixing topic relevance scoreand scores of all other sentences linked with it asfollows: Score(vi) = µ
?j 6=i Score(vj) · p(j ?
i)+(1-µ)rel'(vi|q), where µ is the damping fac-tor as in the PageRank algorithm.
The matrix form is: p˜ = µM˜T p˜ + (1 -µ)~a, where p˜ = [Score(vi)]|Vs|×1 is the vec-tor of saliency scores for the sentences; M˜ =[p(i ? j)]|Vs|×|Vs| is the graph with each entrycorresponding to the transition probability; ~a =[rel'(vi|q)]|Vs|×1 is the vector containing the rel-evance scores of all the sentences to the ques-tion. The above process can be considered as aMarkov chain by taking the sentences as the statesand the corresponding transition matrix is given byA
'= µM˜T + (1- µ)~e~aT .
Considering Topics and Sentiments To-gether: In order to incorporate the opinion infor-mation and topic information for opinion sentenceranking in an unified framework, we propose anOpinion PageRank model (Figure 1) based on atwo-layer link graph (Liu and Ma, 2005; Wan andYang, 2008). In our opinion PageRank model, the
Figure 1: Opinion PageRank
first layer contains all the sentiment words from alexicon to represent the opinion information, andthe second layer denotes the sentence relationshipin the topic sensitive Markov Random Walk modeldiscussed above. The dashed lines between thesetwo layers indicate the conditional influence be-tween the opinion information and the sentencesto be ranked.
Formally, the new representation for the two-layer graph is denoted as G* = <Vs, Vo, Ess, Eso>,where Vs = {vi} is the set of sentences and Vo ={oj} is the set of sentiment words representing theopinion information; Ess = {eij |vi, vj ? Vs}corresponds to all links between sentences andEso = {eij |vi ? Vs, oj ? Vo} corresponds tothe opinion correlation between a sentence andthe sentiment words. For further discussions, welet p(oj) ? [0, 1] denote the sentiment strengthof word oj , and let ?(vi, oj) ? [0, 1] denote thestrength of the correlation between sentence vi andword oj . We incorporate the two factors into thetransition probability from vi to vj and the newtransition probability p(i ? j|Op(vi),Op(vj)) isdefined as f(i?j|Op(vi),Op(vj ))P|Vs|
k=1 f(i?k|Op(vi),Op(vk))when
?f 6=
0, and defined as 0 otherwise, where Op(vi) is de-noted as the opinion information of sentence vi,and f(i ? j|Op(vi),Op(vj)) is the new similar-ity score between two sentences vi and vj , condi-tioned on the opinion information expressed by thesentiment words they contain. We propose to com-pute the conditional similarity score by linearlycombining the scores conditioned on the sourceopinion (i.e. f(i ? j|Op(vi))) and the destina-tion opinion (i.e. f(i? j|Op(vj))) as follows:f(i? j|Op(vi),Op(vj))
= ? · f(i? j|Op(vi)) + (1- ?) · f(i? j|Op(vj))
= ? ·X
ok?Op(vi)
f(i? j) · pi(ok) · ?(ok, vi)
+ (1- ?) ·X
ok' ?Op(vj))
(i? j) · pi(ok' ) · ?(o
k' , vj) (1)
where ? ? [0, 1] is the combination weight con-trolling the relative contributions from the source
739
opinion and the destination opinion. In this study,for simplicity, we define p(oj) as 1, if oj ex-ists in the sentiment lexicon, otherwise 0. And?(vi, oj) is described as an indicative function. Inother words, if word oj appears in the sentence vi,?(vi, oj) is equal to 1. Otherwise, its value is 0.Then the new row-normalized matrix M˜* is de-fined as follows: M˜*ij = p(i ? j|Op(i),Opj).
The final sentence score for Opinion PageR-ank model is then denoted by: Score(vi) = µ ·?
j 6=i Score(vj) · M˜*ji + (1- µ) · rel
'(si|q).
The matrix form is: p˜ = µM˜*T p˜+ (1- µ) · ~a.The final transition matrix is then denoted as:A* = µM˜*T +(1-µ)~e~aT and the sentence scoresare obtained by the principle eigenvector of thenew transition matrix A*.
3.2 Opinion HITS Model
The word’s sentiment score is fixed in OpinionPageRank. This may encounter problem whenthe sentiment score definition is not suitable forthe specific question. We propose another opin-ion sentence ranking model based on the populargraph ranking algorithm HITS (Kleinberg, 1999).This model can dynamically learn the word senti-ment score towards a specific question. HITS al-gorithm distinguishes the hubs and authorities inthe objects. A hub object has links to many au-thorities, and an authority object has high-qualitycontent and there are many hubs linking to it. Thehub scores and authority scores are computed in arecursive way. Our proposed opinion HITS algo-rithm contains three layers. The upper level con-tains all the sentiment words from a lexicon, whichrepresent their opinion information. The lowerlevel contains all the words, which represent theirtopic information. The middle level contains allthe opinion sentences to be ranked. We considerboth the opinion layer and topic layer as hubs andthe sentences as authorities. Figure 2 gives the bi-partite graph representation, where the upper opin-ion layer is merged with lower topic layer togetheras the hubs, and the middle sentence layer is con-sidered as the authority.
Formally, the representation for the bipartitegraph is denoted as G# = <Vs, Vo, Vt, Eso, Est>,where Vs = {vi} is the set of sentences. Vo ={oj} is the set of all the sentiment words repre-senting opinion information, Vt = {tj} is the setof all the words representing topic information.Eso = {eij |vi ? Vs, oj ? Vo} corresponds to the
Figure 2: Opinion HITS model
correlations between sentence and opinion words.Each edge eij is associated with a weight owij de-noting the strength of the relationship between thesentence vi and the opinion word oj . The weightowij is 1 if the sentence vi contains word oj , other-wise 0. Est denotes the relationship between sen-tence and topic word. Its weight twij is calculatedby tf · idf (Otterbacher et al., 2005).
We define two matrixes O = (Oij)|Vs|×|Vo| andT = (Tij)|Vs|×|Vt| as follows, for Oij = owij ,and if sentence i contains word j, therefore owijis assigned 1, otherwise owij is 0. Tij = twij =tfj · idfj (Otterbacher et al., 2005).
Our new opinion HITS model is different fromthe basic HITS algorithm in two aspects. First,we consider the topic relevance when computingthe sentence authority score based on the topic hublevel as follows: Authsen(vi) ?
?twij>0
twij ·
topic score(j)·hubtopic(j), where topic score(j)is empirically defined as 1, if the word j is in thetopic set (we will discuss in next section), and 0.1otherwise.
Second, in our opinion HITS model, there aretwo aspects to boost the sentence authority score:we simultaneously consider both topic informa-tion and opinion information as hubs.
The final scores for authority sentence, hubtopic and hub opinion in our opinion HITS modelare defined as:
Auth(n+1)sen (vi) = (2)? ·
X
twij>0
twij · topic score(j) ·Hub(n)topic(tj)
+ (1- ?) ·X
owij>0
owij ·Hub(n)opinion(oj)
Hub(n+1)topic (ti) =
X
twki>0
twki ·Auth(n)sen(vi) (3)
Hub(n+1)opinion(oi) =
X
owki>0
owki ·Auth(n)sen(vi) (4)
740
Figure 3: Opinion Question Answering System
The matrix form is:
a(n+1) = ? · T · e · tTs · I · h
(n)t + (1- ?) · O · h
(n)o (5)
h(n+1)t = T
T · a(n) (6)h
(n+1)o = O
T · a(n) (7)
where e is a |Vt|×1 vector with all elements equalto 1 and I is a |Vt| × |Vt| identity matrix, ts =[topic score(j)]|Vt|×1 is the score vector for topicwords, a(n) = [Auth(n)sen(vi)]|Vs|×1 is the vectorauthority scores for the sentence in the nth itera-tion, and the same as h(n)t = [Hub
(n)topic(tj)]|Vt|×1,
h(n)o = [Hub
(n)opinion(tj)]|Vo|×1. In order to guaran-
tee the convergence of the iterative form, authorityscore and hub score are normalized after each iter-ation.
For computation of the final scores, the ini-tial scores of all nodes, including sentences, topicwords and opinion words, are set to 1 and theabove iterative steps are used to compute the newscores until convergence. Usually the convergenceof the iteration algorithm is achieved when the dif-ference between the scores computed at two suc-cessive iterations for any nodes falls below a giventhreshold (10e-6 in this study). We use the au-thority scores as the saliency scores in the Opin-ion HITS model. The sentences are then rankedby their saliency scores.
4 System DescriptionIn this section, we introduce the opinion questionanswering system based on the proposed graphmethods. Figure 3 shows five main modules:
Question Analysis: It mainly includes twocomponents. 1).Sentiment Classification: Weclassify all opinion questions into two categories:positive type or negative type. We extract several
types of features, including a set of pattern fea-tures, and then design a classifier to identify sen-timent polarity for each question (similar as (Yuand Hatzivassiloglou, 2003)). 2).Topic Set Expan-sion: The opinion question asks opinions abouta particular target. Semantic role labeling based(Carreras and Marquez, 2005) and rule based tech-niques can be employed to extract this target astopic word. We also expand the topic word withseveral external knowledge bases: Since all the en-tity synonyms are redirected into the same page inWikipedia (Rodrigo et al., 2007), we collect theseredirection synonym words to expand topic set.We also collect some related lists as topic words.For example, given question “What reasons didpeople give for liking Ed Norton’s movies?”, wecollect all the Norton’s movies from IMDB as thisquestion’s topic words.
Document Retrieval: The PRISE search en-gine, supported by NIST (Dang, 2008), is em-ployed to retrieve the documents with topic word.
Answer Candidate Extraction: We split re-trieved documents into sentences, and extract sen-tences containing topic words. In order to im-prove recall, we carry out the following process tohandle the problem of coreference resolution: Weclassify the topic word into four categories: male,female, group and other. Several pronouns are de-fined for each category, such as ”he”, ”him”, ”his”for male category. If a sentence is determined tocontain the topic word, and its next sentence con-tains the corresponding pronouns, then the nextsentence is also extracted as an answer candidate,similar as (Chen et al., 2006).
Answer Ranking: The answer candidatesare ranked by our proposed Opinion PageRankmethod or Opinion HITS method.
Answer Selection by Removing Redundancy:We incrementally add the top ranked sentence intothe answer set, if its cosine similarity with ev-ery extracted answer doesn’t exceed a predefinedthreshold, until the number of selected sentence(here is 40) is reached.
5 Experiments
5.1 Experiment Step
5.1.1 Dataset
We employ the dataset from the TAC 2008 QAtrack. The task contains a total of 87 squishy
741
opinion questions.1 These questions have simpleforms, and can be easily divided into positive typeor negative type, for example “Why do people likeMythbusters?” and “What were the specific ac-tions or reasons given for a negative attitude to-wards Mahmoud Ahmadinejad?”. The initial topicword for each question (called target in TAC) isalso provided. Since our work in this paper fo-cuses on sentence ranking for opinion QA, thesecharacteristics of TAC data make it easy to pro-cess question analysis. Answers for all questionsmust be retrieved from the TREC Blog06 collec-tion (Craig Macdonald and Iadh Ounis, 2006).The collection is a large sample of the blog sphere,crawled over an eleven-week period from Decem-ber 6, 2005 until February 21, 2006. We retrievethe top 50 documents for each question.
5.1.2 Evaluation MetricsWe adopt the evaluation metrics used in the TACsquishy opinion QA task (Dang, 2008). The TACassessors create a list of acceptable informationnuggets for each question. Each nugget will beassigned a normalized weight based on the num-ber of assessors who judged it to be vital. We usethese nuggets and corresponding weights to assessour approach. Three human assessors completethe evaluation process. Every question is scoredusing nugget recall (NR) and an approximation tonugget precision (NP) based on length. The finalscore will be calculated using F measure with TACofficial value ß = 3 (Dang, 2008). This means re-call is 3 times as important as precision:
F (ß = 3) =(32 + 1) ·NP ·NR
32 ·NP +NR
where NP is the sum of weights of nuggets re-turned in response over the total sum of weightsof all nuggets in nugget list, and NP = 1 -(length - allowance)/(length) if length is noless than allowance and 0 otherwise. Hereallowance = 100 × (?nuggets returned) andlength equals to the number of non-white char-acters in strings. We will use average F Score toevaluate the performance for each system.
5.1.3 BaselineThe baseline combines the topic score and opinionscore with a linear weight for each answer candi-date, similar to the previous ad-hoc algorithms:final score = (1- a) × opinion score + a× topic score
(8)13 questions were dropped from the evaluation due to no
correct answers found in the corpus
The topic score is computed by the cosine sim-ilarity between question topic words and answercandidate. The opinion score is calculated usingthe number of opinion words normalized by thetotal number of words in candidate sentence.
5.2 Performance Evaluation5.2.1 Performance on Sentimental Lexicons
Lexicon Neg Pos DescriptionName Size Size
1 HowNet 2700 2009 English translationof positive/negativeChinese words
2 Senti- 4800 2290 Words with a positiveWordNet or negative score
above 0.63 Intersec- 640 518 Words appeared in
tion both 1 and 24 Union 6860 3781 Words appeared in
1 or 25 All 10228 10228 All words appeared
in 1 or 2 withoutdistinguishing posor neg
Table 1: Sentiment lexicon description
For lexicon-based opinion analysis, the selec-tion of opinion thesaurus plays an important rolein the final performance. HowNet2 is a knowledgedatabase of the Chinese language, and provides anonline word list with tags of positive and negativepolarity. We use the English translation of thosesentiment words as the sentimental lexicon. Sen-tiWordNet (Esuli and Sebastiani, 2006) is anotherpopular lexical resource for opinion mining. Ta-ble 1 shows the detail information of our used sen-timent lexicons. In our models, the positive opin-ion words are used only for positive questions, andnegative opinion words just for negative questions.We initially set parameter ? in Opinion PageRankas 0 as (Liu and Ma, 2005), and other parameterssimply as 0.5, including µ in Opinion PageRank,? in Opinion HITS, and a in baseline. The exper-iment results are shown in Figure 4.
We can make three conclusions from Figure 4:1. Opinion PageRank and Opinion HITS are botheffective. The best results of Opinion PageRankand Opinion HITS respectively achieve around35.4% (0.199 vs 0.145), and 34.7% (0.195 vs0.145) improvements in terms of F score over thebest baseline result. We believe this is because ourproposed models not only incorporate the topic in-formation and opinion information, but also con-
2http://www.keenage.com/zhiwang/e zhiwang.html
742
0 15
0.2
0.25
HowNet SentiWordNet Intersection Union All
0
0.05
0.1
0.15
Baseline Opinion PageRank Opinion HITS
Figure 4: Sentiment Lexicon Performance
sider the relationship between different answers.The experiment results demonstrate the effective-ness of these relations. 2. Opinion PageRank andOpinion HITS are comparable. Among five sen-timental lexicons, Opinion PageRank achieves thebest results when using HowNet and Union lexi-cons, and Opinion HITS achieves the best resultsusing the other three lexicons. This may be be-cause when the sentiment lexicon is defined appro-priately for the specific question set, the opinionPageRank model performs better. While when thesentiment lexicon is not suitable for these ques-tions, the opinion HITS model may dynamicallylearn a temporal sentiment lexicon and can yielda satisfied performance. 3. Hownet achieves thebest overall performance among five sentimentlexicons. In HowNet, English translations of theChinese sentiment words are annotated by non-native speakers; hence most of them are commonand popular terms, which maybe more suitable forthe Blog environment (Zhang and Ye, 2008). Wewill use HowNet as the sentiment thesaurus in thefollowing experiments.
In baseline, the parameter a shows the relativecontributions for topic score and opinion score.We vary a from 0 to 1 with an interval of 0.1, andfind that the best baseline result 0.170 is achievedwhen a=0.1. This is because the topic informa-tion has been considered during candidate extrac-tion, the system considering more opinion infor-mation (lower a) achieves better. We will use thisbest result as baseline score in following experi-ments. Since F(3) score is more related with re-call, F score and recall will be demonstrated. Inthe next two sections, we will present the perfor-mances of the parameters in each model. For sim-plicity, we denote Opinion PageRank as PR, Opin-ion HITS as HITS, baseline as Base, Recall as r, Fscore as F.
0.22
0.24
0.26
PR_r PR_F Base_r Base_FF(3)
0.12
0.14
0.16
0.18
0.2
0 0.2 0.4 0.6 0.8 1 
Figure 5: Opinion PageRank Performance withvarying parameter ? (µ = 0.5)
0.22
0.24
0.26
PR_r PR_F Base_r Base_F
F(3)
0.12
0.14
0.16
0.18
0.2
0 0.2 0.4 0.6 0.8 1
 
Figure 6: Opinion PageRank Performance withvarying parameter µ (? = 0.2)
5.2.2 Opinion PageRank PerformanceIn Opinion PageRank model, the value ? com-bines the source opinion and the destination opin-ion. Figure 5 shows the experiment results on pa-rameter ?. When we consider lower ?, the systemperforms better. This demonstrates that the desti-nation opinion score contributes more than sourceopinion score in this task.
The value of µ is a trade-off between answerreinforcement relation and topic relation to calcu-late the scores of each node. For lower value of µ,we give more importance to the relevance to thequestion than the similarity with other sentences.The experiment results are shown in Figure 6. Thebest result is achieved when µ = 0.8. This fig-ure also shows the importance of reinforcementbetween answer candidates. If we don’t considerthe sentence similarity(µ = 0), the performancedrops significantly.
5.2.3 Opinion HITS PerformanceThe parameter ? combines the opinion hub scoreand topic hub score in the Opinion HITS model.The higher ? is, the more contribution is given
743
0.22
0.24
0.26
HITS_r HITS_F Base_r Base_FF(3)
0.12
0.14
0.16
0.18
0.2
0 0.2 0.4 0.6 0.8 1 
Figure 7: Opinion HITS Performance with vary-ing parameter ?
to topic hub level, while the less contribution isgiven to opinion hub level. The experiment resultsare shown in Figure 7. Similar to baseline param-eter a, since the answer candidates are extractedbased on topic information, the systems consider-ing opinion information heavily (a=0.1 in base-line, ?=0.2) perform best.
Opinion HITS model ranks the sentences by au-thority scores. It can also rank the popular opin-ion words and popular topic words from the topichub layer and opinion hub layer, towards a specificquestion. Take the question 1024.3 “What reasonsdo people give for liking Zillow?” as an example,its topic word is “Zillow”, and its sentiment polar-ity is positive. Based on the final hub scores, thetop 10 topic words and opinion words are shownas Table 2.
Opinion real, like, accurate, rich, right, interesting,Words better, easily, free, goodTopic zillow, estate, home, house, data, value,Words site, information, market, worth
Table 2: Question-specific popular topic wordsand opinion words generated by Opinion HITS
Zillow is a real estate site for users to see thevalue of houses or homes. People like it because itis easily used, accurate and sometimes free. Fromthe Table 2, we can see that the top topic wordsare the most related with question topic, and thetop opinion words are question-specific sentimentwords, such as “accurate”, “easily”, “free”, notjust general opinion words, like “great”, “excel-lent” and “good”.
5.2.4 Comparisons with TAC SystemsWe are also interested in the performance compar-ison with the systems in TAC QA 2008. From Ta-ble 3, we can see Opinion PageRank and Opinion
System Precision Recall F(3)OpPageRank 0.109 0.242 0.200
OpHITS 0.102 0.256 0.205System 1 0.079 0.235 0.186System 2 0.053 0.262 0.173System 3 0.109 0.216 0.172
Table 3: Comparison results with TAC 2008 ThreeTop Ranked Systems (system 1-3 demonstrate top3 systems in TAC)
HITS respectively achieve around 10% improve-ment compared with the best result in TAC 2008,which demonstrates that our algorithm is indeedperforming much better than the state-of-the-artopinion QA methods.
6 Conclusion and Future Works
In this paper, we proposed two graph based sen-tence ranking methods for opinion question an-swering. Our models, called Opinion PageRankand Opinion HITS, could naturally incorporatetopic relevance information and the opinion senti-ment information. Furthermore, the relationshipsbetween different answer candidates can be con-sidered. We demonstrate the usefulness of theserelations through our experiments. The experi-ment results also show that our proposed methodsoutperform TAC 2008 QA Task top ranked sys-tems by about 10% in terms of F score.
Our random walk based graph methods inte-grate topic information and sentiment informationin a unified framework. They are not limited tothe sentence ranking for opinion question answer-ing. They can be used in general opinion docu-ment search. Moreover, these models can be moregeneralized to the ranking task with two types ofinfluencing factors.
Acknowledgments: Special thanks to DerekHao Hu and Qiang Yang for their valuablecomments and great help on paper prepara-tion. We also thank Hongning Wang, MinZhang, Xiaojun Wan and the anonymous re-viewers for their useful comments, and thankHoa Trang Dang for providing the TAC eval-uation results. The work was supported by973 project in China(2007CB311003), NSFCproject(60803075), Microsoft joint project ”Opin-ion Summarization toward Opinion Search”, anda grant from the International Development Re-search Center, Canada.
744
ReferencesRicardo Baeza-Yates and Berthier Ribeiro-Neto. 1999.
Modern Information Retrieval. Addison Wesley,May.
Xavier Carreras and Lluis Marquez. 2005. Introduc-tion to the conll-2005 shared task: Semantic role la-beling.
Yi Chen, Ming Zhou, and Shilong Wang. 2006.Reranking answers for definitional qa using lan-guage modeling. In ACL-CoLing, pages 1081–1088.
Hang Cui, Min-Yen Kan, and Tat-Seng Chua. 2007.Soft pattern matching models for definitional ques-tion answering. ACM Trans. Inf. Syst., 25(2):8.
Hoa Trang Dang. 2008. Overview of the tac2008 opinion question answering and summariza-tion tasks (draft). In TAC.
Gu¨nes Erkan and Dragomir R. Radev. 2004. Lex-pagerank: Prestige in multi-document text summa-rization. In EMNLP.
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-wordnet: A publicly available lexical resource foropinion mining. In LREC.
Jon M. Kleinberg. 1999. Authoritative sources in ahyperlinked environment. J. ACM, 46(5):604–632.
Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen.2007. Question analysis and answer passage re-trieval for opinion question answering systems. InROCLING.
Tie-Yan Liu and Wei-Ying Ma. 2005. Webpage im-portance analysis using conditional markov randomwalk. In Web Intelligence, pages 515–521.
Rada Mihalcea and Paul Tarau. 2004. Textrank:Bringing order into texts. In EMNLP.
Jahna Otterbacher, Gu¨nes Erkan, and Dragomir R.Radev. 2005. Using random walks for question-focused sentence retrieval. In HLT/EMNLP.
Lawrence Page, Sergey Brin, Rajeev Motwani, andTerry Winograd. 1998. The pagerank citation rank-ing: Bringing order to the web. Technical report,Stanford University.
Swapna Somasundaran, Theresa Wilson, JanyceWiebe, and Veselin Stoyanov. 2007. Qa with at-titude: Exploiting opinion type analysis for improv-ing question answering in online discussions and thenews. In ICWSM.
Kim Soo-Min and Eduard Hovy. 2005. Identifyingopinion holders for question answering in opiniontexts. In AAAI 2005 Workshop.
Veselin Stoyanov, Claire Cardie, and Janyce Wiebe.2005. Multi-perspective question answering usingthe opqa corpus. In HLT/EMNLP.
Vasudeva Varma, Prasad Pingali, Rahul Katragadda,and et al. 2008. Iiit hyderabad at tac 2008. In TextAnalysis Conference.
X. Wan and J Yang. 2008. Multi-document summa-rization using cluster-based link analysis. In SIGIR,pages 299–306.
Hong Yu and Vasileios Hatzivassiloglou. 2003. To-wards answering opinion questions: Separating factsfrom opinions and identifying the polarity of opinionsentences. In EMNLP.
Min Zhang and Xingyao Ye. 2008. A generationmodel to unify topic relevance and lexicon-basedsentiment for opinion retrieval. In SIGIR, pages411–418.
745
