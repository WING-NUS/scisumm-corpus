Proceedings of the...
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 915–922, Vancouver, October 2005. c©2005 Association for Computational Linguistics
Using Random Walks for Question-focused Sentence Retrieval
Jahna Otterbacher1, Gu¨nes¸ Erkan2, Dragomir R. Radev1,21School of Information, 2Department of EECS
University of Michigan{jahna,gerkan,radev}@umich.edu
Abstract
We consider the problem of question-focused sentence retrieval from complexnews articles describing multi-event sto-ries published over time. Annotators gen-erated a list of questions central to under-standing each story in our corpus. Be-cause of the dynamic nature of the stories,many questions are time-sensitive (e.g.“How many victims have been found?”)Judges found sentences providing an an-swer to each question. To address thesentence retrieval problem, we apply astochastic, graph-based method for com-paring the relative importance of the tex-tual units, which was previously used suc-cessfully for generic summarization. Cur-rently, we present a topic-sensitive versionof our method and hypothesize that it canoutperform a competitive baseline, whichcompares the similarity of each sentenceto the input question via IDF-weightedword overlap. In our experiments, themethod achieves a TRDR score that is sig-nificantly higher than that of the baseline.
1 Introduction
Recent work has motivated the need for systemsthat support “Information Synthesis” tasks, in whicha user seeks a global understanding of a topic orstory (Amigo et al., 2004). In contrast to the clas-sical question answering setting (e.g. TREC-style
Q&A (Voorhees and Tice, 2000)), in which the userpresents a single question and the system returns acorresponding answer (or a set of likely answers), inthis case the user has a more complex informationneed.
Similarly, when reading about a complex newsstory, such as an emergency situation, users mightseek answers to a set of questions in order to un-derstand it better. For example, Figure 1 showsthe interface to our Web-based news summarizationsystem, which a user has queried for informationabout Hurricane Isabel. Understanding such storiesis challenging for a number of reasons. In particular,complex stories contain many sub-events (e.g. thedevastation of the hurricane, the relief effort, etc.) Inaddition, while some facts surrounding the situationdo not change (such as “Which area did the hurri-cane first hit?”), others may change with time (“Howmany people have been left homeless?”). There-fore, we are working towards developing a systemfor question answering from clusters of complex sto-ries published over time. As can be seen at the bot-tom of Figure 1, we plan to add a component to ourcurrent system that allows users to ask questions asthey read a story. They may then choose to receiveeither a precise answer or a question-focused sum-mary.
Currently, we address the question-focused sen-tence retrieval task. While passage retrieval (PR) isclearly not a new problem (e.g. (Robertson et al.,1992; Salton et al., 1993)), it remains important andyet often overlooked. As noted by (Gaizauskas et al.,2004), while PR is the crucial first step for questionanswering, Q&A research has typically not empha-
915
Hurricane Isabel's outer bands moving onshoreproduced on 09/18, 6:18 AM
2% SummaryThe North Carolina coast braced for a weakened but still potent Hurricane Isabel while already rain-soaked areas as faraway as Pennsylvania prepared for possibly ruinous flooding. (2:3) A hurricane warning was in effect from CapeFear in southern North Carolina to the Virginia-Maryland line, and tropical storm warnings extended from South Carolinato New Jersey. (2:14)
While the outer edge of the hurricane approached the North Carolina coast Wednesday, the center of the storm was still400 miles south-southeast of Cape Hatteras, N.C., late Wednesday morning. (3:10) BBC NEWS World AmericasHurricane Isabel prompts US shutdown (4:1)
Ask us:What states have been affected by the hurricane so far?
Around 200,000 people in coastal areas of North Carolina and Virginia were ordered to evacuate or risk getting trappedby flooding from storm surges up to 11 feet. (5:8) The storm was expected to hit with its full fury today, slamming intothe North Carolina coast with 105-mph winds and 45-foot wave crests, before moving through Virginia and bashing thecapital with gusts of about 60 mph. (7:6)
Figure 1: Question tracking interface to a summa-rization system.
sized it. The specific problem we consider differsfrom the classic task of PR for a Q&A system ininteresting ways, due to the time-sensitive nature ofthe stories in our corpus. For example, one challengeis that the answer to a user’s question may be up-dated and reworded over time by journalists in orderto keep a running story fresh, or because the factsthemselves change. Therefore, there is often morethan one correct answer to a question.We aim to develop a method for sentence re-
trieval that goes beyond finding sentences that aresimilar to a single query. To this end, we pro-pose to use a stochastic, graph-based method. Re-cently, graph-based methods have proved useful fora number of NLP and IR tasks such as documentre-ranking in ad hoc IR (Kurland and Lee, 2005)and analyzing sentiments in text (Pang and Lee,2004). In (Erkan and Radev, 2004), we introducedthe LexRank method and successfully applied it togeneric, multi-document summarization. Presently,we introduce topic-sensitive LexRank in creating asentence retrieval system. We evaluate its perfor-mance against a competitive baseline, which con-siders the similarity between each sentence and thequestion (using IDF-weighed word overlap). Wedemonstrate that LexRank significantly improvesquestion-focused sentence selection over the base-line.
2 Formal description of the problem
Our goal is to build a question-focused sentence re-trieval mechanism using a topic-sensitive version ofthe LexRank method. In contrast to previous PR sys-tems such as Okapi (Robertson et al., 1992), which
ranks documents for relevancy and then proceeds tofind paragraphs related to a question, we address thefiner-grained problem of finding sentences contain-ing answers. In addition, the input to our system isa set of documents relevant to the topic of the querythat the user has already identified (e.g. via a searchengine). Our system does not rank the input docu-ments, nor is it restricted in terms of the number ofsentences that may be selected from the same docu-ment.The output of our system, a ranked list of sen-
tences relevant to the user’s question, can be sub-sequently used as input to an answer selection sys-tem in order to find specific answers from the ex-tracted sentences. Alternatively, the sentences canbe returned to the user as a question-focused sum-mary. This is similar to “snippet retrieval” (Wu etal., 2004). However, in our system answers are ex-tracted from a set of multiple documents rather thanon a document-by-document basis.
3 Our approach: topic-sensitive LexRank
3.1 The LexRank method
In (Erkan and Radev, 2004), the concept of graph-based centrality was used to rank a set of sentences,in producing generic multi-document summaries.To apply LexRank, a similarity graph is producedfor the sentences in an input document set. In thegraph, each node represents a sentence. There areedges between nodes for which the cosine similar-ity between the respective pair of sentences exceedsa given threshold. The degree of a given node isan indication of how much information the respec-tive sentence has in common with other sentences.Therefore, sentences that contain the most salient in-formation in the document set should be very centralwithin the graph.Figure 2 shows an example of a similarity graph
for a set of five input sentences, using a cosine simi-larity threshold of 0.15. Once the similarity graph isconstructed, the sentences are then ranked accordingto their eigenvector centrality. As previously men-tioned, the original LexRank method performed wellin the context of generic summarization. Below,we describe a topic-sensitive version of LexRank,which is more appropriate for the question-focusedsentence retrieval problem. In the new approach, the
916
score of a sentence is determined by a mixture modelof the relevance of the sentence to the query and thesimilarity of the sentence to other high-scoring sen-tences.
3.2 Relevance to the question
In topic-sensitive LexRank, we first stem all of thesentences in a set of articles and compute word IDFsby the following formula:
idfw = log
(N + 1
0.5 + sfw
)(1)
whereN is the total number of sentences in the clus-ter, and sfw is the number of sentences that the wordw appears in.We also stem the question and remove the stop
words from it. Then the relevance of a sentence s tothe question q is computed by:
rel(s|q) =Xw?q
log(tfw,s + 1)× log(tfw,q + 1) × idfw (2)
where tfw,s and tfw,q are the number of times wappears in s and q, respectively. This model hasproven to be successful in query-based sentence re-trieval (Allan et al., 2003), and is used as our com-petitive baseline in this study (e.g. Tables 4, 5 and7).
3.3 The mixture model
The baseline system explained above does not makeuse of any inter-sentence information in a cluster.We hypothesize that a sentence that is similar tothe high scoring sentences in the cluster should alsohave a high score. For instance, if a sentence thatgets a high score in our baseline model is likely tocontain an answer to the question, then a related sen-tence, which may not be similar to the question it-self, is also likely to contain an answer.This idea is captured by the following mixture
model, where p(s|q), the score of a sentence s givena question q, is determined as the sum of its rele-vance to the question (using the same measure asthe baseline described above) and the similarity tothe other sentences in the document cluster:
p(s|q) = d rel(s|q)Pz?C rel(z|q)
+(1-d)Xv?C
sim(s, v)Pz?C sim(z, v)
p(v|q) (3)
where C is the set of all sentences in the cluster. Thevalue of d, which we will also refer to as the “ques-tion bias,” is a trade-off between two terms in the
Vertices:
Sentence IndexSentence Index SalienceSalience SentenceSentence
4 0.1973852892722677 Milan fire brigade officials said that...
1 0.03614457831325301 At least two people are dead, inclu...
0 0.28454242157110576 Officials said the plane was carryin...
2 0.1973852892722677 Italian police said the plane was car..
3 0.28454242157110576 Rescue officials said that at least th...
Graph
Figure 2: LexRank example: sentence similaritygraph with a cosine threshold of 0.15.
equation and is determined empirically. For highervalues of d, we give more importance to the rele-vance to the question compared to the similarity tothe other sentences in the cluster. The denominatorsin both terms are for normalization, which are de-scribed below. We use the cosine measure weightedby word IDFs as the similarity between two sen-tences in a cluster:
sim(x, y) =
Pw?x,y tfw,xtfw,y(idfw)
2
qPxi?x(tfxi,xidfxi )
2 ×qP
yi?y(tfyi,y idfyi )2
(4)
Equation 3 can be written in matrix notation asfollows:
p = [dA+ (1- d)B]Tp (5)
A is the square matrix such that for a given index i,all the elements in the ith column are proportionalto rel(i|q). B is also a square matrix such that eachentry B(i, j) is proportional to sim(i, j). Both ma-trices are normalized so that row sums add up to 1.Note that as a result of this normalization, all rowsof the resulting square matrixQ = [dA+(1-d)B]also add up to 1. Such a matrix is called stochasticand defines a Markov chain. If we view each sen-tence as a state in a Markov chain, thenQ(i, j) spec-ifies the transition probability from state i to state jin the corresponding Markov chain. The vector pwe are looking for in Equation 5 is the stationarydistribution of the Markov chain. An intuitive inter-pretation of the stationary distribution can be under-
917
stood by the concept of a random walk on the graphrepresentation of the Markov chain.With probability d, a transition is made from the
current node (sentence) to the nodes that are simi-lar to the query. With probability (1-d), a transitionis made to the nodes that are lexically similar to thecurrent node. Every transition is weighted accordingto the similarity distributions. Each element of thevector p gives the asymptotic probability of endingup at the corresponding state in the long run regard-less of the starting state. The stationary distributionof a Markov chain can be computed by a simple it-erative algorithm, called power method.1
A simpler version of Equation 5, where A is auniform matrix andB is a normalized binary matrix,is known as PageRank (Brin and Page, 1998; Pageet al., 1998) and used to rank the web pages by theGoogle search engine. It was also the model used torank sentences in (Erkan and Radev, 2004).
3.4 Experiments with topic-sensitive LexRank
We experimented with different values of d on ourtraining data. We also considered several thresholdvalues for inter-sentence cosine similarities, wherewe ignored the similarities between the sentencesthat are below the threshold. In the training phaseof the experiment, we evaluated all combinationsof LexRank with d in the range of [0, 1] (in incre-ments of 0.10) and with a similarity threshold rang-ing from [0, 0.9] (in increments of 0.05). We thenfound all configurations that outperformed the base-line. These configurations were then applied to ourdevelopment/test set. Finally, our best sentence re-trieval system was applied to our test data set andevaluated against the baseline. The remainder of thepaper will explain this process and the results in de-tail.
4 Experimental setup
4.1 Corpus
We built a corpus of 20 multi-document clusters ofcomplex news stories, such as plane crashes, polit-ical controversies and natural disasters. The data
1The stationary distribution is unique and the power methodis guaranteed to converge provided that the Markov chain isergodic (Seneta, 1981). A non-ergodic Markov chain can bemade ergodic by reserving a small probability for jumping toany other state from the current state (Page et al., 1998).
clusters and their characteristics are shown in Ta-ble 1. The news articles were collected from varioussources. “Newstracker” clusters were collected au-tomatically by our Web-based news summarizationsystem. The number of clusters randomly assignedto the training, development/test and test data setswere 11, 3 and 6, respectively.Next, we assigned each cluster of articles to an
annotator, who was asked to read all articles in thecluster. He or she then generated a list of factualquestions key to understanding the story. Once wecollected the questions for each cluster, two judgesindependently annotated nine of the training clus-ters. For each sentence and question pair in a givencluster, the judges were asked to indicate whetheror not the sentence contained a complete answerto the question. Once an acceptable rate of inter-judge agreement was verified on the first nine clus-ters (Kappa (Carletta, 1996) of 0.68), the remaining11 clusters were annotated by one judge each.In some cases, the judges did not find any sen-
tences containing the answer for a given question.Such questions were removed from the corpus. Thefinal number of questions annotated for answersover the entire corpus was 341, and the distributionsof questions per cluster can be found in Table 1.
4.2 Evaluation metrics and methods
To evaluate our sentence retrieval mechanism, weproduced extract files, which contain a list of sen-tences deemed to be relevant to the question, for thesystem and from human judgment. To compare dif-ferent configurations of our system to the baselinesystem, we produced extracts at a fixed length of 20sentences. While evaluations of question answeringsystems are often based on a shorter list of rankedsentences, we chose to generate longer lists for sev-eral reasons. One is that we are developing a PRsystem, of which the output can then be input to ananswer extraction system for further processing. Insuch a setting, we would most likely want to gener-ate a relatively longer list of candidate sentences. Aspreviously mentioned, in our corpus the questionsoften have more than one relevant answer, so ideally,our PR system would find many of the relevant sen-tences, sending them on to the answer componentto decide which answer(s) should be returned to theuser. Each system’s extract file lists the document
918
Cluster Sources Articles Questions Data set Sample question
Algerian terror AFP, UPI 2 12 train What is the condition under whichthreat GIA will take its action?Milan plane MSNBC, CNN, ABC, 9 15 train How many people were in thecrash Fox, USAToday building at the time of the crash?Turkish plane BBC, ABC, 10 12 train To where was the plane headed?crash FoxNews, YahooMoscow terror UPI, AFP, AP 7 7 train How many people were killed inattack the most recent explosion?Rhode Island MSNBC, CNN, ABC, Lycos, 10 8 train Who was to blame forclub fire Fox, BBC, Ananova the fire?FBI most AFP, UPI 3 14 train How much is the State Department offeringwanted for information leading to bin Laden’s arrest?Russia bombing AP, AFP 2 11 train What was the cause of the blast?Bali terror CNN, FoxNews, ABC, 10 30 train What were the motivationsattack BBC, Ananova of the attackers?Washington DC FoxNews, Ha’aretz, BBC, 8 28 train What kinds of equipment or weaponssniper BBC, Washington Times, CBS were used in the killings?GSPC terror Newstracker 8 29 train What are the charges againstgroup the GSPC suspects?China Novelty 43 25 18 train What was the magnitude of theearthquake earthquake in Zhangjiakou?Gulfair ABC, BBC, CNN, USAToday, 11 29 dev/test How many people
FoxNews, Washington Post were on board?David Beckham AFP 20 28 dev/test How long had Beckham been playing fortrade MU before he moved to RM?Miami airport Newstracker 12 15 dev/test How many concourses doesevacuation the airport have?US hurricane DUC d04a 14 14 test In which places had the hurricane landed?EgyptAir crash Novelty 4 25 29 test How many people were killed?Kursk submarine Novelty 33 25 30 test When did the Kursk sink?Hebrew University bombing Newstracker 11 27 test How many people were injured?Finland mall bombing Newstracker 9 15 test How many people were in the mall
at the time of the bombing?Putin visits Newstracker 12 20 test What issue concerned BritishEngland human rights groups?
Table 1: Corpus of complex news stories.
and sentence numbers of the top 20 sentences. The“gold standard” extracts list the sentences judged ascontaining answers to a given question by the anno-tators (and therefore have variable sizes) in no par-ticular order.2
We evaluated the performance of the systems us-ing two metrics - Mean Reciprocal Rank (MRR)(Voorhees and Tice, 2000) and Total ReciprocalDocument Rank (TRDR) (Radev et al., 2005).MRR, used in the TREC Q&A evaluations, is thereciprocal rank of the first correct answer (or sen-tence, in our case) to a given question. This measuregives us an idea of how far down we must look in theranked list in order to find a correct answer. To con-trast, TRDR is the total of the reciprocal ranks of allanswers found by the system. In the context of an-swering questions from complex stories, where thereis often more than one correct answer to a question,and where answers are typically time-dependent, weshould focus on maximizing TRDR, which gives us
2For clusters annotated by two judges, all sentences chosenby at least one judge were included.
a measure of how many of the relevant sentenceswere identified by the system. However, we reportboth the average MRR and TRDR over all questionsin a given data set.
5 LexRank versus the baseline system
In the training phase, we searched the parameterspace for the values of d (the question bias) and thesimilarity threshold in order to optimize the resultingTRDR scores. For our problem, we expected that arelatively low similarity threshold pair with a highquestion bias would achieve the best results. Table 2shows the effect of varying the similarity threshold.3
The notation LR[a, d] is used, where a is the simi-larity threshold and d is the question bias. The opti-mal range for the parameter a was between 0.14 and0.20. This is intuitive because if the threshold is toohigh, such that only the most lexically similar sen-tences are represented in the graph, the method doesnot find sentences that are related but are more lex-
3A threshold of -1 means that no threshold was used suchthat all sentences were included in the graph.
919
System Ave. MRR Ave. TRDR
LR[-1.0,0.65] 0.5270 0.8117LR[0.02,0.65] 0.5261 0.7950LR[0.16,0.65] 0.5131 0.8134LR[0.18,0.65] 0.5062 0.8020LR[0.20,0.65] 0.5091 0.7944LR[-1.0,0.80] 0.5288 0.8152LR[0.02,0.80] 0.5324 0.8043LR[0.16,0.80] 0.5184 0.8160LR[0.18,0.80] 0.5199 0.8154LR[0.20,0.80] 0.5282 0.8152
Table 2: Training phase: effect of similarity thresh-old (a) on Ave. MRR and TRDR.
System Ave. MRR Ave. TRDR
LR[0.02,0.65] 0.5261 0.7950LR[0.02,0.70] 0.5290 0.7997LR[0.02,0.75] 0.5299 0.8013LR[0.02,0.80] 0.5324 0.8043LR[0.02,0.85] 0.5322 0.8038LR[0.02,0.90] 0.5323 0.8077LR[0.20,0.65] 0.5091 0.7944LR[0.20,0.70] 0.5244 0.8105LR[0.20,0.75] 0.5285 0.8137LR[0.20,0.80] 0.5282 0.8152LR[0.20,0.85] 0.5317 0.8203LR[0.20,0.90] 0.5368 0.8265
Table 3: Training phase: effect of question bias (d)on Ave. MRR and TRDR.
ically diverse (e.g. paraphrases). Table 3 shows theeffect of varying the question bias at two differentsimilarity thresholds (0.02 and 0.20). It is clear that ahigh question bias is needed. However, a small prob-ability for jumping to a node that is lexically simi-lar to the given sentence (rather than the questionitself) is needed. Table 4 shows the configurationsof LexRank that performed better than the baselinesystem on the training data, based on mean TRDRscores over the 184 training questions. We appliedall four of these configurations to our unseen devel-opment/test data, in order to see if we could furtherdifferentiate their performances.
5.1 Development/testing phase
The scores for the four LexRank systems and thebaseline on the development/test data are shown in
System Ave. MRR Ave. TRDR
Baseline 0.5518 0.8297
LR[0.14,0.95] 0.5267 0.8305LR[0.18,0.90] 0.5376 0.8382LR[0.18,0.95] 0.5421 0.8382LR[0.20,0.95] 0.5404 0.8311
Table 4: Training phase: systems outperforming thebaseline in terms of TRDR score.
System Ave. MRR Ave. TRDR
Baseline 0.5709 1.0002
LR[0.14,0.95] 0.5882 1.0469LR[0.18,0.90] 0.5820 1.0288LR[0.18,0.95] 0.5956 1.0411LR[0.20,0.95] 0.6068 1.0601
Table 5: Development testing evaluation.
Cluster B-MRR LR-MRR B-TRDR LR-TRDR
Gulfair 0.5446 0.5461 0.9116 0.9797David Beckham trade 0.5074 0.5919 0.7088 0.7991Miami airport 0.7401 0.7517 1.7157 1.7028evacuation
Table 6: Average scores by cluster: baseline versusLR[0.20,0.95].
Table 5. This time, all four LexRank systems outper-formed the baseline, both in terms of average MRRand TRDR scores. An analysis of the average scoresover the 72 questions within each of the three clus-ters for the best system, LR[0.20,0.95], is shownin Table 6. While LexRank outperforms the base-line system on the first two clusters both in termsof MRR and TRDR, their performances are not sub-stantially different on the third cluster. Therefore,we examined properties of the questions within eachcluster in order to see what effect they might have onsystem performance.We hypothesized that the baseline system, which
compares the similarity of each sentence to the ques-tion using IDF-weighted word overlap, should per-form well on questions that provide many contentwords. To contrast, LexRank might perform bet-ter when the question provides fewer content words,since it considers both similarity to the query andinter-sentence similarity. Out of the 72 questions inthe development/test set, the baseline system outper-formed LexRank on 22 of the questions. In fact, theaverage number of content words among these 22questions was slightly, but not significantly, higherthan the average on the remaining questions (3.63words per question versus 3.46). Given this obser-vation, we experimented with two mixed strategies,in which the number of content words in a questiondetermined whether LexRank or the baseline systemwas used for sentence retrieval. We tried thresholdvalues of 4 and 6 content words, however, this didnot improve the performance over the pure strategyof system LR[0.20,0.95]. Therefore, we applied this
920
Ave. MRR Ave. TRDR
Baseline 0.5780 0.8673
LR[0.20,0.95] 0.6189 0.9906p-value na 0.0619
Table 7: Testing phase: baseline vs. LR[0.20,0.95].
system versus the baseline to our unseen test set of134 questions.
5.2 Testing phase
As shown in Table 7, LR[0.20,0.95] outperformedthe baseline system on the test data both in termsof average MRR and TRDR scores. The improve-ment in average TRDR score was statistically sig-nificant with a p-value of 0.0619. Since we are in-terested in a passage retrieval mechanism that findssentences relevant to a given question, providing in-put to the question answering component of our sys-tem, the improvement in average TRDR score isvery promising. While we saw in Section 5.1 thatLR[0.20,0.95] may perform better on some questionor cluster types than others, we conclude that it beatsthe competitive baseline when one is looking to op-timize mean TRDR scores over a large set of ques-tions. However, in future work, we will continueto improve the performance, perhaps by develop-ing mixed strategies using different configurationsof LexRank.
6 Discussion
The idea behind using LexRank for sentence re-trieval is that a system that considers only the sim-ilarity between candidate sentences and the inputquery, and not the similarity between the candidatesentences themselves, is likely to miss some impor-tant sentences. When using any metric to comparesentences and a query, there is always likely to bea tie between multiple sentences (or, similarly, theremay be cases where fewer than the number of de-sired sentences have similarity scores above zero).LexRank effectively provides a means to break suchties. An example of such a scenario is illustrated inTables 8 and 9, which show the top ranked sentencesby the baseline and LexRank, respectively for thequestion “What caused the Kursk to sink?” from theKursk submarine cluster. It can be seen that all topfive sentences chosen by the baseline system have
Rank Sentence Score Relevant?
1 The Russian governmental commission on the 4.2282 Naccident of the submarine Kursk sinking inthe Barents Sea on August 12 has rejected11 original explanations for the disaster,but still cannot conclude what caused the
tragedy indeed, Russian Deputy Premier IlyaKlebanov said here Friday.
2 There has been no final word on what caused 4.2282 Nthe submarine to sink while participatingin a major naval exercise, but DefenseMinister Igor Sergeyev said the theory
that Kursk may have collided with anotherobject is receiving increasingly
concrete confirmation.3 Russian Deputy Prime Minister Ilya Klebanov 4.2282 Y
said Thursday that collision with a bigobject caused the Kursk nuclear submarineto sink to the bottom of the Barents Sea.
4 Russian Deputy Prime Minister Ilya Klebanov 4.2282 Ysaid Thursday that collision with a big
object caused the Kursk nuclear submarineto sink to the bottom of the Barents Sea.
5 President Clinton’s national security adviser, 4.2282 NSamuel Berger, has provided his Russian
counterpart with a written summary of whatU.S. naval and intelligence officials believe
caused the nuclear-powered submarine Kursk tosink last month in the Barents Sea, officials
said Wednesday.
Table 8: Top ranked sentences using baseline systemon the question “What caused the Kursk to sink?”.
the same sentence score (similarity to the query), yetthe top ranking two sentences are not actually rele-vant according to the judges. To contrast, LexRankachieved a better ranking of the sentences since it isbetter able to differentiate between them. It shouldbe noted that both for the LexRank and baseline sys-tems, chronological ordering of the documents andsentences is preserved, such that in cases where twosentences have the same score, the one publishedearlier is ranked higher.
7 Conclusion
We presented topic-sensitive LexRank and appliedit to the problem of sentence retrieval. In a Web-based news summarization setting, users of our sys-tem could choose to see the retrieved sentences (asin Table 9) as a question-focused summary. As in-dicated in Table 9, each of the top three sentenceswere judged by our annotators as providing a com-plete answer to the respective question. While thefirst two sentences provide the same answer (a col-lision caused the Kursk to sink), the third sentenceprovides a different answer (an explosion caused thedisaster). While the last two sentences do not pro-vide answers according to our judges, they do pro-vide context information about the situation. Alter-natively, the user might prefer to see the extracted
921
Rank Sentence Score Relevant?
1 Russian Deputy Prime Minister Ilya Klebanov 0.0133 Ysaid Thursday that collision with a big
object caused the Kursk nuclear submarineto sink to the bottom of the Barents Sea.
2 Russian Deputy Prime Minister Ilya Klebanov 0.0133 Ysaid Thursday that collision with a big
object caused the Kursk nuclear submarineto sink to the bottom of the Barents Sea.
3 The Russian navy refused to confirm this, 0.0125 Ybut officers have said an explosion in thetorpedo compartment at the front of the
submarine apparently caused the Kursk to sink.4 President Clinton’s national security adviser, 0.0124 N
Samuel Berger, has provided his Russiancounterpart with a written summary of whatU.S. naval and intelligence officials believe
caused the nuclear-powered submarine Kursk tosink last month in the Barents Sea, officials
said Wednesday.5 There has been no final word on what caused 0.0123 N
the submarine to sink while participatingin a major naval exercise, but DefenseMinister Igor Sergeyev said the theory
that Kursk may have collided with anotherobject is receiving increasingly
concrete confirmation.
Table 9: Top ranked sentences using theLR[0.20,0.95] system on the question “What causedthe Kursk to sink?”
answers from the retrieved sentences. In this case,the sentences selected by our system would be sentto an answer identification component for furtherprocessing. As discussed in Section 2, our goal wasto develop a topic-sensitive version of LexRank andto use it to improve a baseline system, which hadpreviously been used successfully for query-basedsentence retrieval (Allan et al., 2003). In terms ofthis task, we have shown that over a large set of unal-tered questions written by our annotators, LexRankcan, on average, outperform the baseline system,particularly in terms of TRDR scores.
8 Acknowledgments
We would like to thank the members of the CLAIRgroup at Michigan and in particular Siwei Shen andYang Ye for their assistance with this project.
References
James Allan, Courtney Wade, and Alvaro Bolivar. 2003.Retrieval and novelty detection at the sentence level.In SIGIR ’03: Proceedings of the 26th annual interna-tional ACM SIGIR conference on Research and devel-opment in informaion retrieval, pages 314–321. ACMPress.
Enrique Amigo, Julio Gonzalo, Victor Peinado, AnselmoPen˜as, and Felisa Verdejo. 2004. An Empirical Studyof Information Synthesis Task. In Proceedings of the
42nd Meeting of the Association for ComputationalLinguistics (ACL’04), Main Volume, pages 207–214,Barcelona, Spain, July.
Sergey Brin and Lawrence Page. 1998. The anatomy ofa large-scale hypertextual Web search engine. Com-puter Networks and ISDN Systems, 30(1–7):107–117.
Jean Carletta. 1996. Assessing Agreement on Classifica-tion Tasks: The Kappa Statistic. CL, 22(2):249–254.
Gunes Erkan and Dragomir Radev. 2004. LexRank:Graph-based Lexical Centrality as Salience in Text.JAIR, 22:457–479.
Robert Gaizauskas, Mark Hepple, and Mark Greenwood.2004. Information Retrieval for Question Answering:a SIGIR 2004Workshop. In SIGIR 2004 Workshop onInformation Retrieval for Question Answering.
Oren Kurland and Lillian Lee. 2005. PageRank withouthyperlinks: Structural re-ranking using links inducedby language models. In SIGIR 2005, Salvador, Brazil,August.
L. Page, S. Brin, R. Motwani, and T. Winograd. 1998.The pagerank citation ranking: Bringing order to theweb. Technical report, Stanford University, Stanford,CA.
Bo Pang and Lillian Lee. 2004. A Sentimental Educa-tion: Sentiment Analysis Using Subjectivity Summa-rization Based on Minimum Cuts. In Association forComputational Linguistics.
Dragomir Radev, Weiguo Fan, Hong Qi, Harris Wu, andAmardeep Grewal. 2005. Probabilistic Question An-swering on the Web. Journal of the American So-ciety for Information Science and Technology, 56(3),March.
Stephen E. Robertson, Steve Walker, MichelineHancock-Beaulieu, Aarron Gull, and Marianna Lau.1992. Okapi at TREC. In Text REtrieval Conference,pages 21–30.
G. Salton, J. Allan, and C. Buckley. 1993. Approachesto Passage REtrieval in Full Text Information Systems.In Proceedings of the 16th Annual International ACMSIGIR Conference on Research and Development inInformation Retrieval, pages 49–58.
E. Seneta. 1981. Non-negative matrices and markovchains. Springer-Verlag, New York.
Ellen Voorhees and Dawn Tice. 2000. The TREC-8Question Answering Track Evaluation. In Text Re-trieval Conference TREC-8, Gaithersburg, MD.
Harris Wu, Dragomir R. Radev, and Weiguo Fan.2004. Towards Answer-focused Summarization UsingSearch Engines. New Directions in Question Answer-ing.
922
