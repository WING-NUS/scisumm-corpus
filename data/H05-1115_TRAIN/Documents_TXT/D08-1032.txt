Selecting Sentences for Answering Complex Questions
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 304–313,Honolulu, October 2008. c©2008 Association for Computational Linguistics
Selecting Sentences for Answering Complex Questions
Yllias ChaliUniversity of Lethbridge4401 University Drive
Lethbridge, Alberta, Canada, T1K 3M4chali@cs.uleth.ca
Shafiq R. JotyUniversity of British Columbia
2366 Main MallVancouver, B.C. Canada V6T 1Z4
rjoty@cs.ubc.ca
Abstract
Complex questions that require inferencingand synthesizing information from multipledocuments can be seen as a kind of topic-oriented, informative multi-document summa-rization. In this paper, we have experimentedwith one empirical and two unsupervisedstatistical machine learning techniques: k-means and Expectation Maximization (EM),for computing relative importance of the sen-tences. However, the performance of these ap-proaches depends entirely on the feature setused and the weighting of these features. Weextracted different kinds of features (i.e. lex-ical, lexical semantic, cosine similarity, ba-sic element, tree kernel based syntactic andshallow-semantic) for each of the documentsentences in order to measure its importanceand relevancy to the user query. We used alocal search technique to learn the weights ofthe features. For all our methods of generatingsummaries, we have shown the effects of syn-tactic and shallow-semantic features over thebag of words (BOW) features.
1 Introduction
After having made substantial headway in factoidand list questions, researchers have turned their at-tention to more complex information needs that can-not be answered by simply extracting named enti-ties (persons, organizations, locations, dates, etc.)from documents. For example, the question: “De-scribe the after-effects of cyclone Sidr-Nov 2007 inBangladesh” requires inferencing and synthesizinginformation from multiple documents. This infor-mation synthesis in NLP can be seen as a kind of
topic-oriented, informative multi-document summa-rization, where the goal is to produce a single text asa compressed version of a set of documents with aminimum loss of relevant information.
In this paper, we experimented with one em-pirical and two well-known unsupervised statisti-cal machine learning techniques: k-means and EMand evaluated their performance in generating topic-oriented summaries. However, the performance ofthese approaches depends entirely on the feature setused and the weighting of these features. We ex-tracted different kinds of features (i.e. lexical, lexi-cal semantic, cosine similarity, basic element, treekernel based syntactic and shallow-semantic) foreach of the document sentences in order to measureits importance and relevancy to the user query. Wehave used a gradient descent local search techniqueto learn the weights of the features. Traditionally,information extraction techniques are based on theBOW approach augmented by language modeling.But when the task requires the use of more com-plex semantics, the approaches based on only BOWare often inadequate to perform fine-level textualanalysis. Some improvements on BOW are givenby the use of dependency trees and syntactic parsetrees (Hirao et al., 2004), (Punyakanok et al., 2004),(Zhang and Lee, 2003), but these, too are not ade-quate when dealing with complex questions whoseanswers are expressed by long and articulated sen-tences or even paragraphs. Shallow semantic rep-resentations, bearing a more compact information,could prevent the sparseness of deep structural ap-proaches and the weakness of BOW models (Mos-chitti et al., 2007). Attempting an application of
304
syntactic and semantic information to complex QAhence seems natural, as pinpointing the answer to aquestion relies on a deep understanding of the se-mantics of both. In more complex tasks such ascomputing the relatedness between the query sen-tences and the document sentences in order to gen-erate query-focused summaries (or answers to com-plex questions), to our knowledge no study uses treekernel functions to encode syntactic/semantic infor-mation. For all our methods of generating sum-maries (i.e. empirical, k-means and EM), we haveshown the effects of syntactic and shallow-semanticfeatures over the BOW features.
This paper is organized as follows: Section 2 fo-cuses on the related work, Section 3 describes howthe features are extracted, Section 4 discusses thescoring approaches, Section 5 discusses how we re-move the redundant sentences before adding themto the summary, Section 6 describes our experimen-tal study. We conclude and give future directions inSection 7.
2 Related Work
Researchers all over the world working on query-based summarization are trying different direc-tions to see which methods provide the best re-sults. The LexRank method addressed in (Erkanand Radev, 2004) was very successful in genericmulti-document summarization. A topic-sensitiveLexRank is proposed in (Otterbacher et al., 2005).As in LexRank, the set of sentences in a documentcluster is represented as a graph, where nodes aresentences and links between the nodes are inducedby a similarity relation between the sentences. Thenthe system ranked the sentences according to a ran-dom walk model defined in terms of both the inter-sentence similarities and the similarities of the sen-tences to the topic description or question.
The summarization methods based on lexicalchain first extract the nouns, compound nouns andnamed entities as candidate words (Li et al., 2007).Then using WordNet, the systems find the semanticsimilarity between the nouns and compound nouns.After that, lexical chains are built in two steps: 1)Building single document strong chains while dis-ambiguating the senses of the words and, 2) build-ing multi-chain by merging the strongest chains of
the single documents into one chain. The systemsrank sentences using a formula that involves a) thelexical chain, b) keywords from query and c) namedentities.
(Harabagiu et al., 2006) introduce a new paradigmfor processing complex questions that relies on acombination of (a) question decompositions; (b) fac-toid QA techniques; and (c) Multi-Document Sum-marization (MDS) techniques. The question decom-position procedure operates on a Marcov chain, byfollowing a random walk with mixture model on abipartite graph of relations established between con-cepts related to the topic of a complex question andsubquestions derived from topic-relevant passagesthat manifest these relations. Decomposed questionsare then submitted to a state-of-the-art QA systemin order to retrieve a set of passages that can later bemerged into a comprehensive answer by a MDS sys-tem. They show that question decompositions usingthis method can significantly enhance the relevanceand comprehensiveness of summary-length answersto complex questions.
There are approaches that are based on probabilis-tic models (Pingali et al., 2007) (Toutanova et al.,2007). (Pingali et al., 2007) rank the sentences basedon a mixture model where each component of themodel is a statistical model:
Score(s) = a×QIScore(s)+(1-a)×QFocus(s,Q)
Where, Score(s) is the score for sentence s. Query-independent score (QIScore) and query-dependent score(QFocus) are calculated based on probabilistic models.(Toutanova et al., 2007) learns a log-linear sentence rank-ing model by maximizing three metrics of sentence good-ness: (a) ROUGE oracle, (b) Pyramid-derived, and (c)Model Frequency. The scoring function is learned by fit-ting weights for a set of feature functions of sentencesin the document set and is trained to optimize a sentencepair-wise ranking criterion. The scoring function is fur-ther adapted to apply to summaries rather than sentencesand to take into account redundancy among sentences.
There are approaches in “Recognizing Textual Entail-ment”, “Sentence Alignment” and “Question Answering”that use syntactic and/or semantic information in order tomeasure the similarity between two textual units. (Mac-Cartney et al., 2006) use typed dependency graphs (sameas dependency trees) to represent the text and the hypoth-esis. Then they try to find a good partial alignment be-tween the typed dependency graphs representing the hy-pothesis and the text in a search space of O((m + 1)n)
305
where hypothesis graph contains n nodes and a text graphcontains m nodes. (Hirao et al., 2004) represent the sen-tences using Dependency Tree Path (DTP) to incorporatesyntactic information. They apply String SubsequenceKernel (SSK) to measure the similarity between the DTPsof two sentences. They also introduce Extended StringSubsequence Kernel (ESK) to incorporate semantics inDTPs. (Kouylekov and Magnini, 2005) use the tree editdistance algorithms on the dependency trees of the textand the hypothesis to recognize the textual entailment.According to this approach, a text T entails a hypothesisH if there exists a sequence of transformations (i.e. dele-tion, insertion and substitution) applied to T such thatwe can obtain H with an overall cost below a certainthreshold. (Punyakanok et al., 2004) represent the ques-tion and the sentence containing answer with their depen-dency trees. They add semantic information (i.e. namedentity, synonyms and other related words) in the depen-dency trees. They apply the approximate tree matchingin order to decide how similar any given pair of trees are.They also use the edit distance as the matching criteria inthe approximate tree matching. All these methods showthe improvement over the BOW scoring methods.
Our Basic Element (BE)-based feature used the depen-dency tree to extract the BEs (i.e. head-modifier-relation)and ranked the BEs based on their log-likelihood ratios.For syntactic feature, we extracted the syntactic trees forthe sentence as well as for the query using the Charniakparser and measured the similarity between the two treesusing the tree kernel function. We used the ASSERT se-mantic role labeler system to parse the sentence as wellas the query semantically and used the shallow seman-tic tree kernel to measure the similarity between the twoshallow-semantic trees.
3 Feature ExtractionThe sentences in the document collection are analyzedin various levels and each of the document-sentences isrepresented as a vector of feature-values. The featurescan be divided into several categories:
3.1 Lexical Features3.1.1 N-gram Overlap
N-gram overlap measures the overlapping word se-quences between the candidate sentence and the querysentence. With the view to measure the N-gram(N=1,2,3,4) overlap scores, a query pool and a sentencepool are created. In order to create the query (or sentence)pool, we took the query (or document) sentence and cre-ated a set of related sentences by replacing its importantwords1 by their first-sense synonyms. For example given
1hence forth important words are the nouns, verbs, adverbsand adjectives
a stemmed document-sentence: “John write a poem”, thesentence pool contains: “John compose a poem”, “Johnwrite a verse form” along with the given sentence. Wemeasured the recall based n-gram scores for a sentence Pusing the following formula:
n-gramScore(P) = maxi(maxj N-gram(si, qj))
N-gram(S,Q) =?
gramn?SCountmatch(gramn)?
gramn?SCount(gramn)
Where, n stands for the length of the n-gram (n =1, 2, 3, 4) and Countmatch (gramn) is the numberof n-grams co-occurring in the query and the candi-date sentence, qj is the jth sentence in the querypool and si is the ith sentence in the sentence poolof sentence P .
3.1.2 LCS, WLCS and Skip-Bigram
A sequence W = [w1, w2, ..., wn] is a subse-quence of another sequence X = [x1, x2, ..., xm], ifthere exists a strict increasing sequence [i1, i2, ..., ik]of indices of X such that for all j =1, 2, ..., k we have xij = wj . Given two sequences,S1 and S2, the Longest Common Subsequence(LCS) of S1 and S2 is a common subsequence withmaximum length. The longer the LCS of two sen-tences is, the more similar the two sentences are.
The basic LCS has a problem that it does not dif-ferentiate LCSes of different spatial relations withintheir embedding sequences (Lin, 2004). To improvethe basic LCS method, we can remember the lengthof consecutive matches encountered so far to a reg-ular two dimensional dynamic program table com-puting LCS. We call this weighted LCS (WLCS)and use k to indicate the length of the current con-secutive matches ending at words xi and yj . Giventwo sentences X and Y, the WLCS score of X andY can be computed using the similar dynamic pro-gramming procedure as stated in (Lin, 2004). Wecomputed the LCS and WLCS-based F-measure fol-lowing (Lin, 2004) using both the query pool and thesentence pool as in the previous section.
Skip-bigram is any pair of words in their sentenceorder, allowing for arbitrary gaps. Skip-bigram mea-sures the overlap of skip-bigrams between a candi-date sentence and a query sentence. Following (Lin,2004), we computed the skip bi-gram score usingboth the sentence pool and the query pool.
306
3.1.3 Head and Head Related-words Overlap
The number of head words common in betweentwo sentences can indicate how much they are rel-evant to each other. In order to extract the headsfrom the sentence (or query), the sentence (or query)is parsed by Minipar 2 and from the dependencytree we extract the heads which we call exact headwords. For example, the head word of the sentence:“John eats rice” is “eat”.
We take the synonyms, hyponyms and hyper-nyms3 of both the query-head words and thesentence-head words and form a set of words whichwe call head-related words. We measured the exacthead score and the head-related score as follows:
ExactHeadScore =
?w1?HeadSet
Countmatch(w1)?w1?HeadSet
Count(w1)
HeadRelatedScore =
?w1?HeadRelSet
Countmatch(w1)?w1?HeadRelSet
Count(w1)
Where HeadSet is the set of head words in the sen-tence and Countmatch is the number of matchesbetween the HeadSet of the query and the sen-tence. HeadRelSet is the set of synonyms, hy-ponyms and hypernyms of head words in the sen-tence and Countmatch is the number of matchesbetween the head-related words of the query and thesentence.
3.2 Lexical Semantic Features
We form a set of words which we call QueryRe-latedWords by taking the important words from thequery, their first-sense synonyms, the nouns’ hy-pernyms/hyponyms and important words from thenouns’ gloss definitions.
Synonym overlap measure is the overlap be-tween the list of synonyms of the important wordsextracted from the candidate sentence and theQueryRelatedWords. Hypernym/hyponym overlapmeasure is the overlap between the list of hypernymsand hyponyms of the nouns extracted from the sen-tence and the QueryRelatedWords, and gloss overlapmeasure is the overlap between the list of importantwords that are extracted from the gloss definitionsof the nouns of the sentence and the QueryRelated-Words.
2http://www.cs.ualberta.ca/ lindek/minipar.htm3hypernym and hyponym levels are restricted to 2 and 3 re-
spectively
3.3 Statistical Similarity Measures
Statistical similarity measures are based on theco-occurance of similar words in a corpus. Wehave used two statistical similarity measures:1. Dependency-based similarity measure and 2.Proximity-based similarity measure.
Dependency-based similarity measure uses thedependency relations among words in order to mea-sure the similarity. It extracts the dependency triplesthen uses statistical approach to measure the similar-ity. Proximity-based similarity measure is computedbased on the linear proximity relationship betweenwords only. It uses the information theoretic defini-tion of similarity to measure the similarity.
We used the data provided by Dr. Dekang Lin4.Using the data, one can retrieve most similar wordsfor a given word. The similar words are grouped intoclusters. Note that, for a word there can be more thanone cluster. Each cluster represents the sense of theword and its similar words for that sense.
For each query word, we extract all of its clus-ters from the data. Now, in order to determine theright cluster for a query word, we measure the over-lap score between the QueryRelatedWords and theclusters of words. The hypothesis is that, the clusterthat has more words common with the QueryRelat-edWords is the right cluster. We chose the cluster fora word which has the highest overlap score.
Once we get the clusters for the query words, wemeasured the overlap between the cluster words andthe sentence words as follows:
Measure =
?w1?SenWords
Countmatch(w1)?w1?SenWords
Count(w1)
Where, SenWords is the set of important words ex-tracted from the sentence and Countmatch is the numberof matches between the sentence words and the clustersof similar words of the query words.
3.4 Graph-based Similarity MeasureIn LexRank (Erkan and Radev, 2004), the concept ofgraph-based centrality is used to rank a set of sentences,in producing generic multi-document summaries. A sim-ilarity graph is produced for the sentences in the docu-ment collection. In the graph, each node represents asentence. The edges between the nodes measure the co-sine similarity between the respective pair of sentences.The degree of a given node is an indication of how muchimportant the sentence is. Once the similarity graph is
4http://www.cs.ualberta.ca/ lindek/downloads.htm
307
constructed, the sentences are then ranked according totheir eigenvector centrality. To apply LexRank to query-focused context, a topic-sensitive version of LexRank isproposed in (Otterbacher et al., 2005). We followed asimilar approach in order to calculate this feature. Thescore of a sentence is determined by a mixture model ofthe relevance of the sentence to the query and the similar-ity of the sentence to other high-scoring sentences.
3.5 Syntactic and Semantic Features:So far, we have included the features of type Bag ofWords (BOW). The task like query-based summarizationthat requires the use of more complex syntactic and se-mantics, the approaches with only BOW are often inade-quate to perform fine-level textual analysis. We extractedthree features that incorporate syntactic/semantic infor-mation.
3.5.1 Basic Element (BE) Overlap MeasureThe “head-modifier-relation” triples, extracted from
the dependency trees are considered as BEs in our exper-iment. The triples encode some syntactic/semantic infor-mation and one can quite easily decide whether any twounits match or not- considerably more easily than withlonger units (Zhou et al., 2005). We used the BE packagedistributed by ISI5 to extract the BEs for the sentences.
Once we get the BEs for a sentence, we computed theLikelihood Ratio (LR) for each BE following (Zhou etal., 2005). Sorting BEs according to their LR scores pro-duced a BE-ranked list. Our goal is to generate a sum-mary that will answer the user questions. The rankedlist of BEs in this way contains important BEs at the topwhich may or may not be relevant to the user questions.We filter those BEs by checking whether they contain anyword which is a query word or a QueryRelatedWords (de-fined in Section 3.2). The score of a sentence is the sumof its BE scores divided by the number of BEs in the sen-tence.
3.5.2 Syntactic FeatureEncoding syntactic structure is easier and straight for-
ward. Given a sentence (or query), we first parse it intoa syntactic tree using a syntactic parser (i.e. Charniakparser) and then we calculate the similarity between thetwo trees using the tree kernel defined in (Collins andDuffy, 2001).
3.5.3 Shallow-semantic FeatureThough introducing BE and syntactic information
gives an improvement on BOW by the use of depen-dency/syntactic parses, but these, too are not adequatewhen dealing with complex questions whose answersare expressed by long and articulated sentences or even
5BE website:http://www.isi.edu/ cyl/BE
Figure 1: Example of semantic trees
paragraphs. Shallow semantic representations, bearing amore compact information, could prevent the sparsenessof deep structural approaches and the weakness of BOWmodels (Moschitti et al., 2007).
Initiatives such as PropBank (PB) (Kingsbury andPalmer, 2002) have made possible the design of accurateautomatic Semantic Role Labeling (SRL) systems likeASSERT (Hacioglu et al., 2003). For example, considerthe PB annotation:
[ARG0 all][TARGET use][ARG1 the frenchfranc][ARG2 as their currency]
Such annotation can be used to design a shallow se-mantic representation that can be matched against othersemantically similar sentences, e.g.
[ARG0 the Vatican][TARGET use][ARG1 the Italianlira][ARG2 as their currency]
In order to calculate the semantic similarity betweenthe sentences, we first represent the annotated sentenceusing the tree structures like Figure 1 which we call Se-mantic Tree (ST). In the semantic tree, arguments are re-placed with the most important word-often referred to asthe semantic head.
The sentences may contain one or more subordinateclauses. For example the sentence, “the Vatican, locatedwholly within Italy uses the Italian lira as their currency.”gives the STs as in Figure 2. As we can see in Fig-ure 2(A), when an argument node corresponds to an en-tire subordinate clause, we label its leaf with ST, e.g.the leaf of ARG0. Such ST node is actually the root ofthe subordinate clause in Figure 2(B). If taken separately,such STs do not express the whole meaning of the sen-tence, hence it is more accurate to define a single struc-ture encoding the dependency between the two predicatesas in Figure 2(C). We refer to this kind of nested STs asSTNs.
Note that, the tree kernel (TK) function defined in(Collins and Duffy, 2001) computes the number of com-mon subtrees between two trees. Such subtrees are sub-ject to the constraint that their nodes are taken with allor none of the children they have in the original tree.
308
Figure 2: Two STs composing a STN
Though, this definition of subtrees makes the TK func-tion appropriate for syntactic trees but at the same timemakes it not well suited for the semantic trees (ST) de-fined above. For instance, although the two STs of Fig-ure 1 share most of the subtrees rooted in the ST node,the kernel defined above computes only one match (STARG0 TARGET ARG1 ARG2) which is not useful.
The critical aspect of the TK function is that the pro-ductions of two evaluated nodes have to be identical toallow the match of further descendants. This means thatcommon substructures cannot be composed by a nodewith only some of its children as an effective ST represen-tation would require. (Moschitti et al., 2007) solve thisproblem by designing the Shallow Semantic Tree Kernel(SSTK) which allows to match portions of a ST. We fol-lowed the similar approach to compute the SSTK.
4 Ranking SentencesIn this section, we describe the scoring techniques in de-tail.
4.1 Learning Feature-weights: A Local SearchStrategy
In order to fine-tune the weights of the features, we useda local search technique with simulated annealing to findthe global maximum. Initially, we set all the feature-weights, w1, · · · , wn, as equal values (i.e. 0.5) (see Al-gorithm 1). Based on the current weights we score thesentences and generate summaries accordingly. We eval-uate the summaries using the automatic evaluation toolROUGE (Lin, 2004) (described in Section 6) and theROUGE value works as the feedback to our learningloop. Our learning system tries to maximize the ROUGEscore in every step by changing the weights individuallyby a specific step size (i.e. 0.01). That means, to learnweight wi, we change the value of wi keeping all otherweight values (wj?j 6=i) stagnant. For each weight wi,the algorithm achieves the local maximum of ROUGEvalue. In order to find the global maximum we ran this
algorithm multiple times with different random choicesof initial values (i.e. simulated annealing).
Input: Stepsize l, Weight Initial Value vOutput: A vector ~w of learned weightsInitialize the weight values wi to v.for i? 1 to n do
rg1 = rg2 = prev = 0while (true) do
scoreSentences(~w)generateSummaries()rg2 = evaluateROUGE()if rg1 = rg2 then
prev = wiwi+ = lrg1 = rg2
elsebreak
endend
endreturn ~w
Algorithm 1: Tuning weights using Local Searchtechnique
Once we have learned the feature-weights, our empir-ical method computes the final scores for the sentencesusing the formula:
scorei = ~xi. ~w (1)
Where, ~xi is the feature vector for i-th sentence, ~w isthe weight vector and scorei is the score of i-th sentence.
4.2 K-means LearningWe start with a set of initial cluster centers and go throughseveral iterations of assigning each object to the clusterwhose center is closest. After all objects have been as-signed, we recompute the center of each cluster as thecentroid or mean (µ) of its members.
Once we have learned the means of the clusters usingthe k-means algorithm, our next task is to rank the sen-tences according to a probability model. We have usedBayesian model in order to do so. Bayes’ law says:
P (qk|~x,T) = p(~x|qk,T)P (qk|T)?Kk=1 p(~x|qk,T)p(qk|T)
(2)
where qk is a class, ~x is a feature vector repre-senting a sentence and T is the parameter set of allclass models. We set the weights of the clusters asequiprobable (i.e. P (qk|T) = 1/K). We calculated
309
p(x|qk,T) using the gaussian probability distribu-tion. The gaussian probability density function (pdf)for the d-dimensional random variable ~x is given by:
p(µ,S)(~x) =e-12 (~x-µ)TS-1(~x-µ)v
2pidv
det(S)(3)
where µ, the mean vector and S, the covariancematrix are the parameters of the gaussian distribu-tion. We get the means (µ) from the k-means algo-rithm and we calculate the covariance matrix usingthe unbiased covariance estimation:
Sˆ =1
N - 1N?i=1
(xj - µj)(xi - µi)T (4)
4.3 EM LearningEM is an iterative two step procedure:1. Expectation-step and 2. Maximization-step.In the expectation step, we compute expected valuesfor the hidden variables hi,j which are cluster mem-bership probabilities. Given the current parameters,we compute how likely an object belongs to anyof the clusters. The maximization step computesthe most likely parameters of the model given thecluster membership probabilities. The data-pointsare considered to be generated by a mixture modelof k-gaussians of the form:
P (~x) =k?i=1
P (C = i)P (~x|µi,Si) (5)
Where the total likelihood of model T with kcomponents given the observed data points, X =x1, · · · , xn is:
L(T|X) =n?i=1
k?j=1
P (C = j)P (xi|Tj)
=n?i=1
k?j=1
wjP (xi|µj ,Sj)
?n?i=1
logk?j=1
wjP (xi|µj ,Sj)
where P is the probability density function (i.e.eq 3). µj and Sj are the mean and covariance ma-trix of component j, respectively. Each component
contributes a proportion, wj , of the total population,such that:
?Kj=1wj = 1.
However, a significant problem with the EM al-gorithm is that it converges to a local maximumof the likelihood function and hence the quality ofthe result depends on the initialization. In orderto get good results from using random starting val-ues, we can run the EM algorithm several timesand choose the initial configuration for which weget the maximum log likelihood among all con-figurations. Choosing the best one among severalruns is very computer intensive process. So, to im-prove the outcome of the EM algorithm on gaus-sian mixture models it is necessary to find a bettermethod of estimating initial means for the compo-nents. To achieve this aim we explored the widelyused “k-means” algorithm as a cluster (means) find-ing method. That means, the means found by k-means clustering above will be utilized as the initialmeans for EM and we calculate the initial covari-ance matrices using the unbiased covariance estima-tion procedure (eq:4).
Once the sentences are clustered by EM al-gorithm, we filter out the sentences which arenot query-relevant by checking their probabilities,P (qr|xi,T) where, qr denotes the cluster “query-relevant”. If for a sentence xi, P (qr|xi,T) > 0.5then xi is considered to be query-relevant.
Our next task is to rank the query-relevant sen-tences in order to include them in the summary. Thiscan be done easily by multiplying the feature vector~xi with the weight vector ~w that we learned by thelocal search technique (eq:1).
5 Redundancy Checking
When many of the competing sentences are includedin the summary, the issue of information overlap be-tween parts of the output comes up, and a mecha-nism for addressing redundancy is needed. There-fore, our summarization systems employ a final levelof analysis: before being added to the final output,the sentences deemed to be important are comparedto each other and only those that are not too simi-lar to other candidates are included in the final an-swer or summary. Following (Zhou et al., 2005), wemodeled this by BE overlap between an intermedi-ate summary and a to-be-added candidate summary
310
sentence. We call this overlap ratio R, where R isbetween 0 and 1 inclusively. Setting R = 0.7 meansthat a candidate summary sentence, s, can be addedto an intermediate summary, S, if the sentence has aBE overlap ratio less than or equal to 0.7.
6 Experimental Evaluation
6.1 Evaluation Setup
We used the main task of Document UnderstandingConference (DUC) 2007 for evaluation. The taskwas: “Given a complex question (topic description)and a collection of relevant documents, the task is tosynthesize a fluent, well-organized 250-word sum-mary of the documents that answers the question(s)in the topic.”
NIST assessors developed topics of interest tothem and choose a set of 25 documents relevant(document cluster) to each topic. Each topic and itsdocument cluster were given to 4 different NIST as-sessors. The assessor created a 250-word summaryof the document cluster that satisfies the informationneed expressed in the topic statement. These multi-ple “reference summaries” are used in the evaluationof summary content.
We carried out automatic evaluation of our sum-maries using ROUGE (Lin, 2004) toolkit, whichhas been widely adopted by DUC for automaticsummarization evaluation. It measures summaryquality by counting overlapping units such as then-grams (ROUGE-N), word sequences (ROUGE-Land ROUGE-W) and word pairs (ROUGE-S andROUGE-SU) between the candidate summary andthe reference summary. ROUGE parameters wereset as the same as DUC 2007 evaluation setup.
One purpose of our experiments is to study theimpact of different features for complex questionanswering task. To accomplish this, we generatedsummaries for the topics of DUC 2007 by each ofour seven systems defined as below:
The LEX system generates summaries based ononly lexical features: n-gram (n=1,2,3,4), LCS,WLCS, skip bi-gram, head, head synonym. TheLSEM system considers only lexical semanticfeatures: synonym, hypernym/hyponym, gloss,dependency-based and proximity-based similarity.The COS system generates summary based on thegraph-based method. The SYS1 system considers
all the features except the BE, syntactic and seman-tic features. The SYS2 system considers all the fea-tures except the syntactic and semantic features. TheSYS3 considers all the features except the semanticand the ALL6 system generates summaries takingall the features into account.
6.2 Evaluation Results
Table 17 to Table 3, Table 4 to Table 6 and Table 7 toTable 9 show the evaluation measures for k-means,EM and empirical approaches respectively. As Ta-ble 1 shows, in k-means, SYS2 gets 0-21%, SYS3gets 4-32% and ALL gets 3-36% improvement inROUGE-2 scores over the SYS1 system. We get bestROUGE-W (Table 2) scores for SYS2 (i.e. includ-ing BE) but SYS3 and ALL do not perform well inthis case. SYS2 improves the ROUGE-W F-score by1% over SYS1. We do not get any improvement inROUGE-SU (Table 3) scores when we include anykind of syntactic/semantic structures.
The case is different for EM and empirical ap-proaches. Here, in every case we get a significantamount of improvement when we include the syn-tactic and/or semantic features. For EM (Table 4 toTable 6), the ratio of improvement in F-scores overSYS1 is: 1-3% for SYS2, 3-15% for SYS3 and 2-24% for ALL. In our empirical approach (Table 7to Table 9), SYS2, SYS3 and ALL improve the F-scores by 3-11%, 7-15% and 8-19% over SYS1 re-spectively. These results clearly indicate the positiveimpact of the syntactic/semantic features for com-plex question answering task.
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.074 0.077 0.086 0.075 0.075 0.078 0.077
P 0.081 0.084 0.093 0.081 0.098 0.107 0.110
F 0.078 0.080 0.089 0.078 0.085 0.090 0.090
Table 1: ROUGE-2 measures in k-means learning
Table 10 shows the F-scores of the ROUGE mea-sures for one baseline system, the best system inDUC 2007 and our three scoring techniques con-sidering all features. The baseline system gener-
6SYS2, SYS3 and ALL systems show the impact of BE,syntactic and semantic features respectively
7R stands for Recall, P stands for Precision and F stands forF-score
311
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.098 0.097 0.101 0.099 0.101 0.097 0.097P 0.195 0.194 0.200 0.237 0.233 0.241 0.237F 0.130 0.129 0.134 0.140 0.141 0.139 0.138
Table 2: ROUGE-W measures in k-means learning
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.131 0.127 0.139 0.136 0.135 0.135 0.135P 0.155 0.152 0.162 0.176 0.171 0.174 0.174F 0.142 0.139 0.150 0.153 0.151 0.152 0.152
Table 3: ROUGE-SU in k-means learning
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.089 0.080 0.087 0.085 0.085 0.089 0.091P 0.096 0.087 0.094 0.092 0.095 0.116 0.138F 0.092 0.083 0.090 0.088 0.090 0.101 0.109
Table 4: ROUGE-2 measures in EM learning
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.103 0.096 0.101 0.102 0.101 0.102 0.101P 0.205 0.193 0.200 0.203 0.218 0.222 0.223F 0.137 0.128 0.134 0.136 0.138 0.139 0.139
Table 5: ROUGE-W measures in EM learning
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.146 0.128 0.138 0.143 0.144 0.145 0.144P 0.171 0.153 0.162 0.168 0.177 0.186 0.185F 0.157 0.140 0.149 0.154 0.159 0.163 0.162
Table 6: ROUGE-SU measures in EM learning
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.086 0.080 0.087 0.087 0.090 0.095 0.099P 0.093 0.087 0.094 0.094 0.112 0.115 0.116F 0.089 0.083 0.090 0.090 0.100 0.104 0.107
Table 7: ROUGE-2 in empirical approach
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.102 0.096 0.101 0.102 0.102 0.104 0.105P 0.203 0.193 0.200 0.204 0.239 0.246 0.247F 0.135 0.128 0.134 0.137 0.143 0.147 0.148
Table 8: ROUGE-W in empirical approach
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.144 0.129 0.138 0.145 0.146 0.149 0.150P 0.169 0.153 0.162 0.171 0.182 0.195 0.197F 0.155 0.140 0.150 0.157 0.162 0.169 0.170
Table 9: ROUGE-SU in empirical approach
ates summaries by returning all the leading sen-tences (up to 250 words) in the <TEXT > field ofthe most recent document(s). It shows that the em-pirical approach outperforms the other two learningtechniques and EM performs better than k-means al-gorithm. EM improves the F-scores over k-meansby 0.7-22.5%. Empirical approach improves the F-scores over k-means and EM by 5.9-20.2% and 3.5-6.5% respectively. Comparing with the DUC 2007participants our systems achieve top scores and forsome ROUGE measures there is no statistically sig-nificant difference between our system and the bestDUC 2007 system.
System ROUGE-1
ROUGE-2
ROUGE-W
ROUGE-SU
Baseline 0.335 0.065 0.114 0.113
Best 0.438 0.122 0.153 0.174
k-means 0.390 0.090 0.138 0.152
EM 0.399 0.109 0.139 0.162
Empirical 0.413 0.107 0.148 0.170
Table 10: F-measures for different systems
7 Conclusion and Future Work
Our experiments show the following: (a) our ap-proaches achieve promising results, (b) empiricalapproach outperforms the other two learning andEM performs better than the k-means algorithm forthis particular task, and (c) our systems achieve bet-ter results when we include BE, syntactic and se-mantic features.
In future, we have the plan to decompose the com-plex questions into several simple questions beforemeasuring the similarity between the document sen-tence and the query sentence. We expect that by de-composing complex questions into the sets of sub-questions that they entail, systems can improve theaverage quality of answers returned and achieve bet-ter coverage for the question as a whole.
312
References
M. Collins and N. Duffy. 2001. Convolution Kernels forNatural Language. In Proceedings of Neural Informa-tion Processing Systems, pages 625–632, Vancouver,Canada.
G. Erkan and D. R. Radev. 2004. LexRank: Graph-based Lexical Centrality as Salience in Text Summa-rization. Journal of Artificial Intelligence Research,22:457–479.
K. Hacioglu, S. Pradhan, W. Ward, J. H. Martin, andD. Jurafsky. 2003. Shallow Semantic Parsing UsingSupport Vector Machines. In Technical Report TR-CSLR-2003-03, University of Colorado.
S. Harabagiu, F. Lacatusu, and A. Hickl. 2006. Answer-ing complex questions with random walk models. InProceedings of the 29th annual international ACM SI-GIR conference on Research and development in in-formation retrieval, pages 220 – 227. ACM.
T. Hirao, , J. Suzuki, H. Isozaki, and E. Maeda. 2004.Dependency-based sentence alignment for multipledocument summarization. In Proceedings of Coling2004, pages 446–452, Geneva, Switzerland. COLING.
P. Kingsbury and M. Palmer. 2002. From Treebank toPropBank. In Proceedings of the international con-ference on Language Resources and Evaluation, LasPalmas, Spain.
M. Kouylekov and B. Magnini. 2005. Recognizingtextual entailment with tree edit distance algorithms.In Proceedings of the PASCAL Challenges Workshop:Recognising Textual Entailment Challenge.
J. Li, L. Sun, C. Kit, and J. Webster. 2007. A Query-Focused Multi-Document Summarizer Based on Lex-ical Chains. In Proceedings of the Document Under-standing Conference, Rochester. NIST.
C. Y. Lin. 2004. ROUGE: A Package for Auto-matic Evaluation of Summaries. In Proceedings ofWorkshop on Text Summarization Branches Out, Post-Conference Workshop of Association for Computa-tional Linguistics, pages 74–81, Barcelona, Spain.
B. MacCartney, T. Grenager, M.C. de Marneffe, D. Cer,and C. D. Manning. 2006. Learning to recognize fea-tures of valid textual entailments. In Proceedings ofthe Human Language Technology Conference of theNorth American Chapter of the ACL, page 4148, NewYork, USA.
A. Moschitti, S. Quarteroni, R. Basili, and S. Manand-har. 2007. Exploiting Syntactic and Shallow Seman-tic Kernels for Question/Answer Classificaion. In Pro-ceedings of the 45th Annual Meeting of the Associationof Computational Linguistics, pages 776–783, Prague,Czech Republic. ACL.
J. Otterbacher, G. Erkan, and D. R. Radev. 2005. Us-ing Random Walks for Question-focused Sentence Re-trieval. In Proceedings of Human Language Technol-ogy Conference and Conference on Empirical Meth-ods in Natural Language Processing, pages 915–922,Vancouver, Canada.
P. Pingali, Rahul K., and V. Varma. 2007. IIIT Hyder-abad at DUC 2007. In Proceedings of the DocumentUnderstanding Conference, Rochester. NIST.
V. Punyakanok, D. Roth, and W. Yih. 2004. Mapping de-pendencies trees: An application to question answer-ing. In Proceedings of AI & Math, Florida, USA.
K. Toutanova, C. Brockett, M. Gamon, J. Jagarlamudi,H. Suzuki, and L. Vanderwende. 2007. The PYTHYSummarization System: Microsoft Research at DUC2007 . In proceedings of the Document UnderstandingConference, Rochester. NIST.
D. Zhang and W. S. Lee. 2003. A Language Mod-eling Approach to Passage Question Answering. InProceedings of the Twelfth Text REtreival Conference,pages 489–495, Gaithersburg, Maryland.
L. Zhou, C. Y. Lin, and E. Hovy. 2005. A BE-basedMulti-dccument Summarizer with Query Interpreta-tion. In Proceedings of Document UnderstandingConference, Vancouver, B.C., Canada.
313
