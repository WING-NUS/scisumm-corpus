Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 305–312,Sydney, July 2006. c©2006 Association for Computational Linguistics
Bayesian Query-Focused Summarization
Hal Daume´ III and Daniel MarcuInformation Sciences Institute
4676 Admiralty Way, Suite 1001Marina del Rey, CA 90292
me@hal3.name,marcu@isi.edu
AbstractWe present BAYESUM (for “Bayesiansummarization”), a model for sentence ex-traction in query-focused summarization.BAYESUM leverages the common case inwhich multiple documents are relevant to asingle query. Using these documents as re-inforcement for query terms, BAYESUM isnot afflicted by the paucity of informationin short queries. We show that approxi-mate inference in BAYESUM is possibleon large data sets and results in a state-of-the-art summarization system. Further-more, we show how BAYESUM can beunderstood as a justified query expansiontechnique in the language modeling for IRframework.
1 IntroductionWe describe BAYESUM, an algorithm for perform-ing query-focused summarization in the commoncase that there are many relevant documents for agiven query. Given a query and a collection of rel-evant documents, our algorithm functions by ask-ing itself the following question: what is it aboutthese relevant documents that differentiates themfrom the non-relevant documents? BAYESUM canbe seen as providing a statistical formulation ofthis exact question.
The key requirement of BAYESUM is that mul-tiple relevant documents are known for the queryin question. This is not a severe limitation. In twowell-studied problems, it is the de-facto standard.In standard multidocument summarization (withor without a query), we have access to known rel-evant documents for some user need. Similarly, inthe case of a web-search application, an underly-ing IR engine will retrieve multiple (presumably)
relevant documents for a given query. For both ofthese tasks, BAYESUM performs well, even whenthe underlying retrieval model is noisy.
The idea of leveraging known relevant docu-ments is known as query expansion in the informa-tion retrieval community, where it has been shownto be successful in ad hoc retrieval tasks. Viewedfrom the perspective of IR, our work can be inter-preted in two ways. First, it can be seen as an ap-plication of query expansion to the summarizationtask (or, in IR terminology, passage retrieval); see(Liu and Croft, 2002; Murdock and Croft, 2005).Second, and more importantly, it can be seen as amethod for query expansion in a non-ad-hoc man-ner. That is, BAYESUM is a statistically justifiedquery expansion method in the language modelingfor IR framework (Ponte and Croft, 1998).
2 Bayesian Query-FocusedSummarization
In this section, we describe our Bayesian query-focused summarization model (BAYESUM). Thistask is very similar to the standard ad-hoc IR task,with the important distinction that we are compar-ing query models against sentence models, ratherthan against document models. The shortness ofsentences means that one must do a good job ofcreating the query models.
To maintain generality, so that our model is ap-plicable to any problem for which multiple rele-vant documents are known for a query, we formu-late our model in terms of relevance judgments.For a collection of D documents and Q queries,we assume we have a D × Q binary matrix r,where rdq = 1 if an only if document d is rele-vant to query q. In multidocument summarization,rdq will be 1 exactly when d is in the document setcorresponding to query q; in search-engine sum-
305
marization, it will be 1 exactly when d is returnedby the search engine for query q.
2.1 Language Modeling for IRBAYESUM is built on the concept of languagemodels for information retrieval. The idea behindthe language modeling techniques used in IR isto represent either queries or documents (or both)as probability distributions, and then use stan-dard probabilistic techniques for comparing them.These probability distributions are almost always“bag of words” distributions that assign a proba-bility to words from a fixed vocabulary V .
One approach is to build a probability distri-bution for a given document, pd(·), and to lookat the probability of a query under that distribu-tion: pd(q). Documents are ranked according tohow likely they make the query (Ponte and Croft,1998). Other researchers have built probabilitydistributions over queries pq(·) and ranked doc-uments according to how likely they look underthe query model: pq(d) (Lafferty and Zhai, 2001).A third approach builds a probability distributionpq(·) for the query, a probability distribution pd(·)for the document and then measures the similaritybetween these two distributions using KL diver-gence (Lavrenko et al., 2002):
KL (pq || pd) =?w?V
pq(w) logpq(w)
pd(w)(1)
The KL divergence between two probabilitydistributions is zero when they are identical andotherwise strictly positive. It implicitly assumesthat both distributions pq and pd have the samesupport: they assign non-zero probability to ex-actly the same subset of V; in order to accountfor this, the distributions pq and pd are smoothedagainst a background general English model. Thisfinal mode—the KL model—is the one on whichBAYESUM is based.
2.2 Bayesian Statistical ModelIn the language of information retrieval, the query-focused sentence extraction task boils down to es-timating a good query model, pq(·). Once we havesuch a model, we could estimate sentence modelsfor each sentence in a relevant document, and rankthe sentences according to Eq (1).
The BAYESUM system is based on the follow-ing model: we hypothesize that a sentence ap-pears in a document because it is relevant to some
query, because it provides background informa-tion about the document (but is not relevant to aknown query) or simply because it contains use-less, general English filler. Similarly, we modeleach word as appearing for one of those purposes.More specifically, our model assumes that eachword can be assigned a discrete, exact source, suchas “this word is relevant to query q1” or “this wordis general English.” At the sentence level, how-ever, sentences are assigned degrees: “this sen-tence is 60% about query q1, 30% backgrounddocument information, and 10% general English.”
To model this, we define a general Englishlanguage model, pG(·) to capture the Englishfiller. Furthermore, for each document dk, wedefine a background document language model,pdk(·); similarly, for each query qj , we definea query-specific language model pqj (·). Everyword in a document dk is modeled as being gen-erated from a mixture of pG, pdk and {pqj :query qj is relevant to document dk}. Supposingthere are J total queries and K total documents,we say that the nth word from the sth sentencein document d, wdsn, has a corresponding hiddenvariable, zdsn that specifies exactly which of thesedistributions is used to generate that one word. Inparticular, zdsn is a vector of length 1 + J + K,where exactly one element is 1 and the rest are 0.
At the sentence level, we introduce a secondlayer of hidden variables. For the sth sentence indocument d, we let pids be a vector also of length1 + J + K that represents our degree of beliefthat this sentence came from any of the models.The pidss lie in the J + K-dimensional simplex?J+K = {? = <?1, . . . , ?J+K+1> : (?i) ?i =0,
?i ?i = 1}. The interpretation of the pi vari-
ables is that if the “general English” component ofpi is 0.9, then 90% of the words in this sentencewill be general English. The pi and z variables areconstrained so that a sentence cannot be generatedby a document language model other than its owndocument and cannot be generated by a query lan-guage model for a query to which it is not relevant.
Since the pis are unknown, and it is unlikely thatthere is a “true” correct value, we place a corpus-level prior on them. Since pi is a multinomial dis-tribution over its corresponding zs, it is natural touse a Dirichlet distribution as a prior over pi. ADirichlet distribution is parameterized by a vectora of equal length to the corresponding multino-mial parameter, again with the positivity restric-
306
tion, but no longer required to sum to one. Ithas continuous density over a variable ?1, . . . , ?Igiven by: Dir(? | a) = G(
Pi ai)Q
i G(ai)
?i ?
ai-1i . The
first term is a normalization term that ensures that??I d? Dir(? | a) = 1.
2.3 Generative StoryThe generative story for our model defines a distri-bution over a corpus of queries, {qj}1:J , and doc-uments, {dk}1:K , as follows:
1. For each query j = 1 . . . J : Generate eachword qjn in qj by pqj (qjn)
2. For each document k = 1 . . .K and eachsentence s in document k:(a) Select the current sentence degree piks
by Dir(piks | a)rk(piks)(b) For each word wksn in sentence s:
• Select the word source zksn accord-ing to Mult(z | piks)
• Generate the word wksn by???
pG(wksn) if zksn = 0pdk(wksn) if zksn = k + 1pqj (wksn) if zksn = j + K + 1
We used r to denote relevance judgments:rk(pi) = 0 if any document component of pi ex-cept the one corresponding to k is non-zero, or ifany query component of pi except those queries towhich document k is deemed relevant is non-zero(this prevents a document using the “wrong” doc-ument or query components). We have further as-sumed that the z vector is laid out so that z0 cor-responds to general English, zk+1 corresponds todocument dk for 0 = j < J and that zj+K+1 cor-responds to query qj for 0 = k < K.
2.4 Graphical ModelThe graphical model corresponding to this gener-ative story is in Figure 1. This model depicts thefour known parameters in square boxes (a, pQ, pDand pG) with the three observed random variablesin shaded circles (the queries q, the relevance judg-ments r and the words w) and two unobserved ran-dom variables in empty circles (the word-level in-dicator variables z and the sentence level degreespi). The rounded plates denote replication: thereare J queries and K documents, containing S sen-tences in a given document and N words in a givensentence. The joint probability over the observedrandom variables is given in Eq (2):
w
z
rq
pQ
pG
pD
K
J
N
pi
a
S
Figure 1: Graphical model for the BayesianQuery-Focused Summarization Model.
p (q1:J , r, d1:K) =
[ ?j
?n
pqj (qjn)
]× (2)
[ ?k
?s
??
dpiks p (piks | a, r)
?n
?zksn
p (zksn | piks) p (wksn | zksn)
]
This expression computes the probability of thedata by integrating out the unknown variables. Inthe case of the pi variables, this is accomplishedby integrating over ?, the multinomial simplex,according to the prior distribution given by a. Inthe case of the z variables, this is accomplished bysumming over all possible (discrete) values. Thefinal word probability is conditioned on the z valueby selecting the appropriate distribution from pG,pD and pQ. Computing this expression and findingoptimal model parameters is intractable due to thecoupling of the variables under the integral.
3 Statistical Inference in BAYESUMBayesian inference problems often give rise to in-tractable integrals, and a large variety of tech-niques have been proposed to deal with this. Themost popular are Markov Chain Monte Carlo(MCMC), the Laplace (or saddle-point) approxi-mation and the variational approximation. A third,less common, but very effective technique, espe-cially for dealing with mixture models, is expec-tation propagation (Minka, 2001). In this paper,we will focus on expectation propagation; exper-iments not reported here have shown variational
307
EM to perform comparably but take roughly 50%longer to converge.
Expectation propagation (EP) is an inferencetechnique introduced by Minka (2001) as a gener-alization of both belief propagation and assumeddensity filtering. In his thesis, Minka showedthat EP is very effective in mixture modelingproblems, and later demonstrated its superiorityto variational techniques in the Generative As-pect Model (Minka and Lafferty, 2003). The keyidea is to compute an integral of a product ofterms by iteratively applying a sequence of “dele-tion/inclusion” steps. Given an integral of theform:
?? dpi p(pi)
?n tn(pi), EP approximates
each term tn by a simpler term t˜n, giving Eq (3).?
?dpi q(pi) q(pi) = p(pi)
?n
t˜n(pi) (3)
In each deletion/inclusion step, one of the ap-proximate terms is deleted from q(·), leavingq-n(·) = q(·)/t˜n(·). A new approximation fortn(·) is computed so that tn(·)q-n(·) has the sameintegral, mean and variance as t˜n(·)q-n(·). Thisnew approximation, t˜n(·) is then included backinto the full expression for q(·) and the process re-peats. This algorithm always has a fixed point andthere are methods for ensuring that the approxi-mation remains in a location where the integral iswell-defined. Unlike variational EM, the approx-imation given by EP is global, and often leads tomuch more reliable estimates of the true integral.
In the case of our model, we follow Minka andLafferty (2003), who adapts latent Dirichlet allo-cation of Blei et al. (2003) to EP. Due to spaceconstraints, we omit the inference algorithms andinstead direct the interested reader to the descrip-tion given by Minka and Lafferty (2003).
4 Search-Engine Experiments
The first experiments we run are for query-focusedsingle document summarization, where relevantdocuments are returned from a search engine, anda short summary is desired of each document.
4.1 DataThe data we use to train and test BAYESUMis drawn from the Text REtrieval Conference(TREC) competitions. This data set consists ofqueries, documents and relevance judgments, ex-actly as required by our model. The queries are
typically broken down into four fields of increas-ing length: the title (3-4 words), the summary (1sentence), the narrative (2-4 sentences) and theconcepts (a list of keywords). Obviously, onewould expect that the longer the query, the bettera model would be able to do, and this is borne outexperimentally (Section 4.5).
Of the TREC data, we have trained our modelon 350 queries (queries numbered 51-350 and401-450) and all corresponding relevant docu-ments. This amounts to roughly 43k documents,2.1m sentences and 65.8m words. The meannumber of relevant documents per query is 137and the median is 81 (the most prolific query has968 relevant documents). On the other hand, eachdocument is relevant to, on average, 1.11 queries(the median is 5.5 and the most generally relevantdocument is relevant to 20 different queries). In allcases, we apply stemming using the Porter stem-mer; for all other models, we remove stop words.
In order to evaluate our model, we hadseven human judges manually perform the query-focused sentence extraction task. The judges weresupplied with the full TREC query and a singledocument relevant to that query, and were asked toselect up to four sentences from the document thatbest met the needs given by the query. Each judgeannotated 25 queries with some overlap to allowfor an evaluation of inter-annotator agreement,yielding annotations for a total of 166 uniquequery/document pairs. On the doubly annotateddata, we computed the inter-annotator agreementusing the kappa measure. The kappa value foundwas 0.58, which is low, but not abysmal (also,keep in mind that this is computed over only 25of the 166 examples).
4.2 Evaluation Criteria
Since there are differing numbers of sentences se-lected per document by the human judges, onecannot compute precision and recall; instead, weopt for other standard IR performance measures.We consider three related criteria: mean averageprecision (MAP), mean reciprocal rank (MRR)and precision at 2 (P@2). MAP is computed bycalculating precision at every sentence as orderedby the system up until all relevant sentences are se-lected and averaged. MRR is the reciprocal of therank of the first relevant sentence. P@2 is the pre-cision computed at the first point that two relevantsentences have been selected (in the rare case that
308
humans selected only one sentence, we use P@1).
4.3 Baseline Models
As baselines, we consider four strawman modelsand two state-of-the-art information retrieval mod-els. The first strawman, RANDOM ranks sentencesrandomly. The second strawman, POSITION,ranks sentences according to their absolute posi-tion (in the context of non-query-focused summa-rization, this is an incredibly powerful baseline).The third and fourth models are based on the vec-tor space interpretation of IR. The third model,JACCARD, uses standard Jaccard distance score(intersection over union) between each sentenceand the query to rank sentences. The fourth, CO-SINE, uses TF-IDF weighted cosine similarity.
The two state-of-the-art IR models used as com-parative systems are based on the language mod-eling framework described in Section 2.1. Thesesystems compute a language model for each queryand for each sentence in a document. Sentencesare then ranked according to the KL divergencebetween the query model and the sentence model,smoothed against a general model estimated fromthe entire collection, as described in the case ofdocument retrieval by Lavrenko et al. (2002). Thisis the first system we compare against, called KL.
The second true system, KL+REL is based onaugmenting the KL system with blind relevancefeedback (query expansion). Specifically, we firstrun each query against the document set returnedby the relevance judgments and retrieve the top nsentences. We then expand the query by interpo-lating the original query model with a query modelestimated on these sentences. This serves as amethod of query expansion. We ran experimentsranging n in {5, 10, 25, 50, 100} and the interpo-lation parameter ? in {0.2, 0.4, 0.6, 0.8} and usedoracle selection (on MRR) to choose the valuesthat performed best (the results are thus overly op-timistic). These values were n = 25 and ? = 0.4.
Of all the systems compared, only BAYESUMand the KL+REL model use the relevance judg-ments; however, they both have access to exactlythe same information. The other models only runon the subset of the data used for evaluation (thecorpus language model for the KL system and theIDF values for the COSINE model are computedon the full data set). EP ran for 2.5 hours.
MAP MRR P@2RANDOM 19.9 37.3 16.6POSITION 24.8 41.6 19.9JACCARD 17.9 29.3 16.7COSINE 29.6 50.3 23.7KL 36.6 64.1 27.6KL+REL 36.3 62.9 29.2BAYESUM 44.1 70.8 33.6
Table 1: Empirical results for the baseline modelsas well as BAYESUM, when all query fields areused.
4.4 Performance on all Query FieldsOur first evaluation compares results when allquery fields are used (title, summary, descriptionand concepts1). These results are shown in Ta-ble 1. As we can see from these results, the JAC-CARD system alone is not sufficient to beat theposition-based baseline. The COSINE does beatthe position baseline by a bit of a margin (5 pointsbetter in MAP, 9 points in MRR and 4 points inP@2), and is in turn beaten by the KL system(which is 7 points, 14 points and 4 points betterin MAP, MRR and P@2, respectively). Blind rel-evance feedback (parameters of which were cho-sen by an oracle to maximize the P@2 metric) ac-tually hurts MAP and MRR performance by 0.3and 1.2, respectively, and increases P@2 by 1.5.Over the best performing baseline system (eitherKL or KL+REL), BAYESUM wins by a margin of7.5 points in MAP, 6.7 for MRR and 4.4 for P@2.
4.5 Varying Query FieldsOur next experimental comparison has to do withreducing the amount of information given in thequery. In Table 2, we show the performanceof the KL, KL-REL and BAYESUM systems, aswe use different query fields. There are severalthings to notice in these results. First, the stan-dard KL model without blind relevance feedbackperforms worse than the position-based modelwhen only the 3-4 word title is available. Sec-ond, BAYESUM using only the title outperformthe KL model with relevance feedback using allfields. In fact, one can apply BAYESUM withoutusing any of the query fields; in this case, only therelevance judgments are available to make sense
1A reviewer pointed out that concepts were later removedfrom TREC because they were “too good.” Section 4.5 con-siders the case without the concepts field.
309
MAP MRR P@2POSITION 24.8 41.6 19.9Title KL 19.9 32.6 17.8
KL-Rel 31.9 53.8 26.1BAYESUM 41.1 65.7 31.6
+Description KL 31.5 58.3 24.1KL-Rel 32.6 55.0 26.2BAYESUM 40.9 66.9 31.0
+Summary KL 31.6 56.9 23.8KL-Rel 34.2 48.5 27.0BAYESUM 42.0 67.8 31.8
+Concepts KL 36.7 64.2 27.6KL-Rel 36.3 62.9 29.2BAYESUM 44.1 70.8 33.6
No Query BAYESUM 39.4 64.7 30.4
Table 2: Empirical results for the position-basedmodel, the KL-based models and BAYESUM, withdifferent inputs.
of what the query might be. Even in this cir-cumstance, BAYESUM achieves a MAP of 39.4,an MRR of 64.7 and a P@2 of 30.4, still bet-ter across the board than KL-REL with all queryfields. While initially this seems counterintuitive,it is actually not so unreasonable: there is signifi-cantly more information available in several hun-dred positive relevance judgments than in a fewsentences. However, the simple blind relevancefeedback mechanism so popular in IR is unable toadequately model this.
With the exception of the KL model without rel-evance feedback, adding the description on top ofthe title does not seem to make any difference forany of the models (and, in fact, occasionally hurtsaccording to some metrics). Adding the summaryimproves performance in most cases, but not sig-nificantly. Adding concepts tends to improve re-sults slightly more substantially than any other.
4.6 Noisy Relevance JudgmentsOur model hinges on the assumption that, for agiven query, we have access to a collection ofknown relevant documents. In most real-worldcases, this assumption is violated. Even in multi-document summarization as run in the DUC com-petitions, the assumption of access to a collectionof documents all relevant to a user need is unreal-istic. In the real world, we will have to deal withdocument collections that “accidentally” containirrelevant documents. The experiments in this sec-tion show that BAYESUM is comparatively robust.
For this experiment, we use the IR engine thatperformed best in the TREC 1 evaluation: In-query (Callan et al., 1992). We used the offi-
0.4 0.5 0.6 0.7 0.8 0.9 128
30
32
34
36
38
40
42
44
R-precision of IR Engine
Mea
n Av
erag
e Pr
ecisi
on o
f Sen
tenc
e Ex
tract
ion
KL-Rel (title only)BayeSum (title only)KL-Rel (title+desc+sum)BayeSum (title+desc+sum)KL-Rel (all fields)BayeSum (all fields)
Figure 2: Performance with noisy relevance judg-ments. The X-axis is the R-precision of the IRengine and the Y-axis is the summarization per-formance in MAP. Solid lines are BAYESUM, dot-ted lines are KL-Rel. Blue/stars indicate title only,red/circles indicated title+description+summaryand black/pluses indicate all fields.
cial TREC results of Inquery on the subset ofthe TREC corpus we consider. The Inquery R-precision on this task is 0.39 using title only, and0.51 using all fields. In order to obtain curvesas the IR engine improves, we have linearly in-terpolated the Inquery rankings with the true rel-evance judgments. By tweaking the interpolationparameter, we obtain an IR engine with improv-ing performance, but with a reasonable bias. Wehave run both BAYESUM and KL-Rel on the rel-evance judgments obtained by this method for sixvalues of the interpolation parameter. The resultsare shown in Figure 2.
As we can observe from the figure, the solidlines (BAYESUM) are always above the dottedlines (KL-Rel). Considering the KL-Rel resultsalone, we can see that for a non-perfect IR engine,it makes little difference what query fields we usefor the summarization task: they all obtain roughlyequal scores. This is because the performance inKL-Rel is dominated by the performance of the IRengine. Looking only at the BAYESUM results, wecan see a much stronger, and perhaps surprisingdifference. For an imperfect IR system, it is betterto use only the title than to use the title, descriptionand summary for the summarization component.We believe this is because the title is more on topicthan the other fields, which contain terms like “Arelevant document should describe . . . .” Never-
310
theless, BAYESUM has a more upward trend thanKL-Rel, which indicates that improved IR will re-sult in improved summarization for BAYESUM butnot for KL-Rel.
5 Multidocument Experiments
We present two results using BAYESUM in themultidocument summarization settings, based onthe official results from the Multilingual Summa-rization Evaluation (MSE) and Document Under-standing Conference (DUC) competitions in 2005.
5.1 Performance at MSE 2005We participated in the Multilingual Summariza-tion Evaluation (MSE) workshop with a systembased on BAYESUM. The task for this competi-tion was generic (no query) multidocument sum-marization. Fortunately, not having a query isnot a hindrance to our model. To account for theredundancy present in document collections, weapplied a greedy selection technique that selectssentences central to the document cluster but farfrom previously selected sentences (Daume´ III andMarcu, 2005a). In MSE, our system performedvery well. According to the human “pyramid”evaluation, our system came first with a score of0.529; the next best score was 0.489. In the au-tomatic “Basic Element” evaluation, our systemscored 0.0704 (with a 95% confidence interval of[0.0429, 0.1057]), which was the third best scoreon a site basis (out of 10 sites), and was not statis-tically significantly different from the best system,which scored 0.0981.
5.2 Performance at DUC 2005We also participated in the Document Understand-ing Conference (DUC) competition. The chosentask for DUC was query-focused multidocumentsummarization. We entered a nearly identical sys-tem to DUC as to MSE, with an additional rule-based sentence compression component (Daume´III and Marcu, 2005b). Human evaluators consid-ered both responsiveness (how well did the sum-mary answer the query) and linguistic quality. Oursystem achieved the highest responsiveness scorein the competition. We scored more poorly on thelinguistic quality evaluation, which (only 5 out ofabout 30 systems performed worse); this is likelydue to the sentence compression we performed ontop of BAYESUM. On the automatic Rouge-basedevaluations, our system performed between third
and sixth (depending on the Rouge parameters),but was never statistically significantly worse thanthe best performing systems.
6 Discussion and Future Work
In this paper we have described a model for au-tomatically generating a query-focused summary,when one has access to multiple relevance judg-ments. Our Bayesian Query-Focused Summariza-tion model (BAYESUM) consistently outperformscontending, state of the art information retrievalmodels, even when it is forced to work with sig-nificantly less information (either in the complex-ity of the query terms or the quality of relevancejudgments documents). When we applied our sys-tem as a stand-alone summarization model in the2005 MSE and DUC tasks, we achieved amongthe highest scores in the evaluation metrics. Theprimary weakness of the model is that it currentlyonly operates in a purely extractive setting.
One question that arises is: why doesBAYESUM so strongly outperform KL-Rel, giventhat BAYESUM can be seen as Bayesian formalismfor relevance feedback (query expansion)? Bothmodels have access to exactly the same informa-tion: the queries and the true relevance judgments.This is especially interesting due to the fact thatthe two relevance feedback parameters for KL-Rel were chosen optimally in our experiments, yetBAYESUM consistently won out. One explanationfor this performance win is that BAYESUM pro-vides a separate weight for each word, for eachquery. This gives it significantly more flexibility.Doing something similar with ad-hoc query ex-pansion techniques is difficult due to the enormousnumber of parameters; see, for instance, (Buckleyand Salton, 1995).
One significant advantage of working in theBayesian statistical framework is that it gives usa straightforward way to integrate other sources ofknowledge into our model in a coherent manner.One could consider, for instance, to extend thismodel to the multi-document setting, where onewould need to explicitly model redundancy acrossdocuments. Alternatively, one could include usermodels to account for novelty or user preferencesalong the lines of Zhang et al. (2002).
Our model is similar in spirit to the random-walk summarization model (Otterbacher et al.,2005). However, our model has several advan-tages over this technique. First, our model has
311
no tunable parameters: the random-walk methodhas many (graph connectivity, various thresholds,choice of similarity metrics, etc.). Moreover, sinceour model is properly Bayesian, it is straightfor-ward to extend it to model other aspects of theproblem, or to related problems. Doing so in a nonad-hoc manner in the random-walk model wouldbe nearly impossible.
Another interesting avenue of future work is torelax the bag-of-words assumption. Recent workhas shown, in related models, how this can be donefor moving from bag-of-words models to bag-of-ngram models (Wallach, 2006); more interestingthan moving to ngrams would be to move to de-pendency parse trees, which could likely be ac-counted for in a similar fashion. One could alsopotentially relax the assumption that the relevancejudgments are known, and attempt to integratethem out as well, essentially simultaneously per-forming IR and summarization.
Acknowledgments. We thank Dave Blei and TomMinka for discussions related to topic models, and to theanonymous reviewers, whose comments have been of greatbenefit. This work was partially supported by the NationalScience Foundation, Grant IIS-0326276.
ReferencesDavid Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet allocation. Journal of MachineLearning Research (JMLR), 3:993–1022, January.
Chris Buckley and Gerard Salton. 1995. Optimiza-tion of relevance feedback weights. In Proceedingsof the Conference on Research and Developments inInformation Retrieval (SIGIR).
Jamie Callan, Bruce Croft, and Stephen Harding.1992. The INQUERY retrieval system. In Pro-ceedings of the 3rd International Conference onDatabase and Expert Systems Applications.
Hal Daume´ III and Daniel Marcu. 2005a. Bayesianmulti-document summarization at MSE. In ACL2005 Workshop on Intrinsic and Extrinsic Evalua-tion Measures.
Hal Daume´ III and Daniel Marcu. 2005b. Bayesiansummarization at DUC and a suggestion for extrin-sic evaluation. In Document Understanding Confer-ence.
John Lafferty and ChengXiang Zhai. 2001. Documentlanguage models, query models, and risk minimiza-tion for information retrieval. In Proceedings of theConference on Research and Developments in Infor-mation Retrieval (SIGIR).
Victor Lavrenko, M. Choquette, and Bruce Croft.2002. Crosslingual relevance models. In Proceed-ings of the Conference on Research and Develop-ments in Information Retrieval (SIGIR).
Xiaoyong Liu and Bruce Croft. 2002. Passage re-trieval based on language models. In Processingof the Conference on Information and KnowledgeManagement (CIKM).
Thomas Minka and John Lafferty. 2003. Expectation-propagation for the generative aspect model. In Pro-ceedings of the Converence on Uncertainty in Artifi-cial Intelligence (UAI).
Thomas Minka. 2001. A family of algorithms for ap-proximate Bayesian inference. Ph.D. thesis, Mas-sachusetts Institute of Technology, Cambridge, MA.
Vanessa Murdock and Bruce Croft. 2005. A transla-tion model for sentence retrieval. In Proceedings ofthe Joint Conference on Human Language Technol-ogy Conference and Empirical Methods in NaturalLanguage Processing (HLT/EMNLP), pages 684–691.
Jahna Otterbacher, Gunes Erkan, and Dragomir R.Radev. 2005. Using random walks for question-focused sentence retrieval. In Proceedings of theJoint Conference on Human Language TechnologyConference and Empirical Methods in Natural Lan-guage Processing (HLT/EMNLP).
Jay M. Ponte and Bruce Croft. 1998. A language mod-eling approach to information retrieval. In Proceed-ings of the Conference on Research and Develop-ments in Information Retrieval (SIGIR).
Hanna Wallach. 2006. Topic modeling: beyond bag-of-words. In Proceedings of the International Con-ference on Machine Learning (ICML).
Yi Zhang, Jamie Callan, and Thomas Minka. 2002.Novelty and redundancy detection in adaptive filter-ing. In Proceedings of the Conference on Researchand Developments in Information Retrieval (SIGIR).
312
