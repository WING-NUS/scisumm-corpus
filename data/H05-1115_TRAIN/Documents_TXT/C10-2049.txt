Towards Automated Related Work Summarization
Coling 2010: Poster Volume, pages 427–435,Beijing, August 2010
Towards Automated Related Work Summarization
Cong Duy Vu Hoang and Min-Yen KanDepartment of Computer Science
School of ComputingNational University of Singapore
{hcdvu,kanmy}@comp.nus.edu.sg
Abstract
We introduce the novel problem of auto-matic related work summarization. Givenmultiple articles (e.g., conference/journalpapers) as input, a related work sum-marization system creates a topic-biasedsummary of related work specific to thetarget paper. Our prototype Related WorkSummarization system, ReWoS, takes inset of keywords arranged in a hierarchicalfashion that describes a target paper’s top-ics, to drive the creation of an extractivesummary using two different strategies forlocating appropriate sentences for generaltopics as well as detailed ones. Our initialresults show an improvement over genericmulti-document summarization baselinesin a human evaluation.
1 Introduction
In many fields, a scholar needs to show an under-standing of the context of his problem and relatehis work to prior community knowledge. A re-lated work section is often the vehicle for this pur-pose; it contextualizes the scholar’s contributionsand helps readers understand the critical aspectsof the previous works that current work addresses.Creating such a summary requires the author toposition his own work within the contextual re-search to showcase the advantages of his method.
We envision an NLP application that assists increating a related work summary. We propose thisrelated work summarization task as a challengeto the automatic summarization community. Inits full form, it is a topic-biased, multi-document
summarization problem that takes as input a tar-get scientific document for which a related worksection needs to be drafted. The output goal is tocreate a related work section that finds the relevantrelated works and contextually describes them inrelationship to the scientific document at hand.
We dissect the full challenge as bringing to-gether work of disparate interests; 1) in findingrelevant documents; 2) in identifying the salientaspects of these documents in relation to the cur-rent work worth summarizing; and 3) in generat-ing the final topic-biased summary. While it isclear that current NLP technology does not let usbuild a complete solution for this task, we believethat tackling the individual components will helpbring us towards an eventual solution.
In fact, existing works in the NLP and rec-ommendation systems communities have alreadybegun work that fits towards the completion ofthe first two tasks. Citation prediction (Nallapatiet al., 2008) is a growing research area that hasaimed both at predicting citation growth over timewithin a community and at individual paper cita-tion patterns. This year, an automatic keyphraseextraction task from scientific articles was firstfielded in SemEval-2, partially addressing Task11. Also, automatic survey generation (Moham-mad et al., 2009) is becoming a growing fieldwithin the summarization community. However,to date, we have not yet seen any work that exam-ines topic-biased summarization of multiple sci-entific articles. For these reasons, we focus onTask 3 here – the creation of a related work sec-tion, given a structured input of the topics for sum-mary.. The remaining contributions of our paper
1http://semeval2.fbk.eu/semeval2.php
427
consists of work towards this goal:• We conduct a study of the argumentative pat-terns used in related work sections, to describe theplausible summarization tactics for their creationin Section 3.• We describe our approach to generate an extrac-tive related work summary given an input topic hi-erarchy tree, using two separate strategies to dif-ferentiate between summarizing shallow internalnodes from deep detailed leaf nodes of the topictree in Section 4.
2 Related Work
Fully automated related work summarization issignificantly different from traditional summa-rization. While there are no existing studies onthis specific problem, there are closely related en-deavors. The iOPENER2 project works towardsautomated creation of technical surveys, given aresearch topic (Mohammad et al., 2009). Stan-dard generic multi-document summarization al-gorithms were applied to generate technical sur-veys. They showed that citation information waseffective in the generation process. This was alsovalidated earlier in (Nakov et al., 2004), whichshowed that the citing sentences in other paperscan give a useful description of a target work.
Other studies focus mainly on single-documentscientific article summarization. The pioneers ofautomated summarization (Luhn, 1958; Baxen-dale, 1958; Edmundson, 1969) had envisionedtheir approaches being used for the automatic cre-ation of scientific summaries. They examinedvarious features specific to scientific texts (e.g.,frequency-based, sentence position, or rhetoricalclues features) which were proven effective fordomain-specific summarization tasks.
Further, Mei and Zhai (2008) and Qazvinianand Radev (2008) utilized citation information increating summaries for a single scientific article incomputational linguistics domain. Also, Schwartzand Hearst (2006) also utilized the citation sen-tences to summarize the key concepts and entitiesin bioscience texts, and argued that citation sen-tences may contain informative contributions of apaper that complement its original abstract.
2http://clair.si.umich.edu/clair/iopener/
These works all center on the role of citationsand their contexts in creating a summary, using ci-tation information to rank content for extraction.However, they did not study the rhetorical struc-ture of the intended summaries, targeting more onderiving useful content. For working along thisvein, we turn to studies on the rhetorical structureof scientific articles. Perhaps the most relevant iswork by (Teufel, 1999; Teufel and Moens, 2002)who defined and studied argumentative zoning oftexts, especially ones in computational linguistics.While they studied the structure of an entire arti-cle, it is clear from their studies that a related worksection would contain general background knowl-edge (BACKGROUND zone) as well as specific in-formation credited to others (OTHER and BASISzones). This vein of work has been followed bymany, including Teufel et al. (2009; Angrosh etal. (2010).
3 Structure of Related Work SectionWe first extend the work on rhetorical analysis,concentrating on related work sections. By study-ing examples in detail, we gain insight on how toapproach related work summarization. We focuson a concrete related work example for illustra-tion, an excerpt of which is shown in Figure 1a.Focusing on the argumentative progression of thetext, we note the flow through different topics ishierarchical and can be represented as a topic treeas in Figure 1b.
This summary provides background knowledgefor a paper on text classification, which is the rootof the topic tree (node 1; lines 1–5). Two top-ics (“feature selection” and “machine learning”)are then presented in parallel (nodes 2 & 3; lines5–8 & 9–15), where specific details on relevantworks are selected to describe two topics. Thesetwo topics are implicitly understood as subtopicsof a more general topic, namely “mono-lingualtext classification” (node 4; lines 16–17). The au-thors use the monolingual topic to contrast it withthe subsequent subtopic “multi-lingual text classi-fication” (node 5; lines 18–21). This topic is de-scribed by elaborating its details through two sub-topics: “bilingual text classification” and “cross-lingual text classification” (nodes 6 & 7; lines 22–25 & 25–39) where again, various example works
428
12
9
16
21
2223
32
40
42
35
line line
(a)
1
54
6 732
contrast
parallel parallel
Text classification (lines 1-5)Feature selection (lines 5-8)Machine learning (lines 9-15)Mono-lingual text classification (lines 16-17)Multi-lingual text classification (lines 18-21)Bilingual text classification (lines 22-25)Cross-lingual text classification (lines 25-39)
1
2
3
4
5
6
7
(b)
1
2 3
4 5
6 7
monolingual;language
text;classification
multi-language;multi-lingual;language
features;selection learning;probabilistic bilingual cross-lingual
(c)
Figure 1: a) A related work section extracted from (Wu and Oard, 2008); b) An associated topic hierar-chy tree of a); c) An associated topic tree, annotated with key words/phrases.
are described and cited. The authors then con-clude by contrasting their proposed approach withthe introduced relevant approaches (lines 40–42).
This summary illustrates three importantpoints. First, the topic tree is an essential inputto the summarization process. The topic tree canbe thought of as a high-level rhetorical structurefor which a process then attaches content. Whileit is certainly non-trivial to build such a tree, mod-ifications to hierarchical topic modeling (M. et al.,2004) or keyphrase extraction algorithms (Wittenet al., 1999) we believe can be used to induce asuitable form. A resulting topic hierarchy fromsuch a process would provide an associated setof key words or phrases that would describe thenode, as shown in Figure 1c.
Second, while summaries can be structured inmany ways, they can be viewed as moves alongthe topic hierarchy tree. In the example, nodes2 and 3 are discussed before their parent, as theparent node (node 4) serves as a useful contrastto introduce its sibling (node 5). We find variantsof depth-first traversal common, but breadth-firsttraversals of nodes with multiple descendants aremore rare. They may be structured this way toease the reader’s burden on memory and atten-tion. This is in line with other summary genreswhere information is ordered by high-level logicalconsiderations that place macro level constraints(Barzilay et al., 2002).
Third, there is a clear distinction between sen-tences that describe a general topic and those that
describe work in detail. Generic topics are oftenrepresented by background information, which isnot tied to a particular prior work. These includedefinitions or descriptions of a topic’s purpose.In contrast, detailed information forms the bulkof the summary and often describes key relatedwork that is attributable to specific authors. Re-cently, Jaidka et al. (2010) also present the begin-nings of a corpus study of related work sections,where they differentiate integrative and descrip-tive strategies in presenting discourse work. Wesee our differentiation between general and de-tailed topics as a natural parallel to their notionof integrative and descriptive strategies.
To introspect on these findings further, we cre-ated a related work data set (called RWSData3),which includes 20 articles from well-respectedvenues in NLP and IR, namely SIGIR, ACL,NAACL, EMNLP and COLING. We extractedthe related work sections directly from those re-search articles as well as references the sectionscited. References to books and Ph.D. theses wereremoved, as their verbosity would change theproblem drastically (Mihalcea and Ceylan, 2007).Since we view each related work summary as atopic-biased summary originating from a topic hi-erarchy tree, annotation of such topical informa-tion for our data set is necessary. Each article’sdata consists of the reference related work sum-mary, the collection of the input research articles
3To be made available at http://wing.comp.nus.edu.sg/downloads/rwsdata.
429
SbL-RW WbL-RW No-RAs SbL-RA WbL-RA TS TDaverage 17.9 522.4 10.9 2386.0 51739.6 3.3 1.8stdev 7.9 216.5 5.6 1306.7 26682.3 1.7 0.6min 6 179 2 348 8580 1 1max 40 922 26 5549 112267 7 3
Table 1: The demographics of RWSData. No, RW, RA, SbL, WbL, TS, and TD are labeled as(N)umber (o)f, (R)elated (W)orks, (R)eferenced (A)rticles, (S)entence-(b)ased (L)ength of, (W)ord-(b)ased (L)ength of, (T)ree (S)ize, and (T)ree (D)epth, respectively.
that were referenced and a manually-constructedtopic descriptions in a hierarchical fashion (topictree). More details on the demographics of RWS-Data are shown in Table 1. RWSData summariesaverage 17.9 sentences, 522 words in length, cit-ing an average of 10.9 articles. While hierarchi-cal, the topic trees are simple, averaging 3.3 topicnodes in size and average depth of 1.8. Their sim-plicity furthers our claim that automated methodswould be able to create such trees.
4 ReWoS: Paired General and SpecificSummarization
sentences
Pre-processor
Agent-based rule
Subject-based rule
OR
Verb-based rule Citation-based rule
Topic relevance computationGCSum
Topic relevance computationSCSum
Weighting
Context modeling
Ranking
Re-ranking
Post-processor General content sentences
Specific content sentences GeneratorRelated work 
summary
T F
R
R
T
T T
Specific Content Summarization General Content Summarization
Ranking
sentences
Pre-processor
Agent-based rule
Subject-based rule
OR
Verb-based rule Citation-based rule
Topic relevance computationGCSum
Topic relevance computationSCSum
Weighting
Context modeling
Ranking
Re-ranking
Post-processor General content sentences
Specific content sentences GeneratorRelated work 
summary
T F
R
R
T
T T
Specific Content Summarization General Content Summarization
Ranking
Figure 2: The ReWoS architecture. Decisionedges labeled as True, False and Relevant.
Inspired by the above observations, we proposea novel strategy for related work summarizationwith respect to a given topic tree. Note that whilethe construction of the topic tree is central to theprocess, we consider this outside the scope of thecurrent work (see §1); our investigation focuses
on how such input could be utilized to construct areasonable topic-biased related work summary.
We posit that sentences within a related worksection come about by means of two separate pro-cesses – a process that gives general backgroundinformation and another that describes specific au-thor contributions. A key realization in our workis that these processes are easily mapped to thetopic tree topologically: general content is de-scribed in tree-internal nodes, whereas leaf nodescontribute detailed specifics. In our approach,these two processes are independent, and com-bined to construct the final summary.
We have implemented our idea in ReWoS(Related Work Summarizer), whose general ar-chitecture is shown in Figure 2. ReWoS is alargely heuristic system, featuring both a GeneralContent Summarization (GCSum) and a SpecificContent Summarization (SCSum) modules, pre-fixed by preprocessing. A natural language tem-plate generation system fills out the end of thesummary.
ReWoS first applies a set of preprocessing steps(shown in the top of Figure 2). Input sentences(i.e., the set of sentences from each related/citedarticle) first removes sentences that are too short(< 7 tokens) or too long (> 80 tokens), ones thatuse future tense (possibly future work), and exam-ple and navigation sentences. This last categoryis filtered out by checking for the presence of acue phrase among a lexical pattern database: e.g.,“in the section”, “figure x shows”, “for instance”.Lowercasing and stemming are also performed.
We then direct sentences to either GCSum orSCSum based on whether it describes the author’sown work or not, similar in spirit and execution to(Teufel et al., 2009). If sentence contains indica-tive pronouns or cue phrases (e.g., “we”, “this ap-
430
proach”), the sentence is deemed to describe ownwork and is directed to SCSum; otherwise the sen-tence is directed to the GCSum workflow.
4.1 (G)eneral (C)ontent (Sum)marizationGCSum extracts sentences containing usefulbackground information on the topics of the inter-nal node in focus. Since general content sentencesdo not specifically describe work done by the au-thors, we only take sentences that do not have theauthor-as-agent as input for GCSum.
We divide such general content sentences intotwo groups: indicative and informative. Infor-mative sentences give detail on a specific aspectof the problem. They often give definitions, pur-pose or application of the topic (“Text classifica-tion is a task that assigns a certain number of pre-defined labels for a given text.”). In contrast, in-dicative sentences are simpler, inserted to makethe topic transition explicit and rhetorically sound(“Many previous studies have studied monolin-gual text classification.”).
Indicative sentences can be easily generated bytemplates, as the primary information that is trans-mitted is the identity of the topic itself. Informa-tive sentences, on the other hand, are better ex-tracted from the source articles themselves, re-quiring a specific strategy. As informative sen-tences contain more content, our strategy withGCSum is to attempt to locate informative sen-tences to describe the internal nodes, failing whichGCSum falls back to using predefined templatesto generate an indicative placeholder.
To implement GCSum’s informative extractor,we use a set of heuristics in a decision cascadeto first filter inappropriate sentences (as shown onthe RHS of Figure 2). Remaining candidates (ifany) are then ranked by relevance and the top nare selected for the summary.
The heuristic cascade’s purpose is to ensuresentences fit the syntactic structure of commonly-observed informative sentences. A useful sen-tence should discuss the topic directly, so GCSumfirst checks the subject of each candidate sentence,filtering sentences whose subject do not contain atleast one topic keyword. We observed that back-ground sentences often feature specific verbs orcitations. GCSum thus also checks whether stock
verb phrases (i.e., “based on”, “make use of” and23 other patterns) are used as the main verb. Oth-erwise, GCSum checks for the presence of at leastone citation – general sentences may list a set ofcitations as examples. If both the cue verb andcitation checks fail, the sentence is dropped.
GCSum’s topic relevance computation ranksremaining sentences based on keyword content.We state that the topic of an internal node is af-fected by its surrounding nodes – ancestor, de-scendants and siblings. Based on this idea, thescore of a sentence is computed in a discrimina-tive way using the following linear combination:
scoreS ? scoreQAS + scoreQS - scoreQRS (1)where scoreS is the final relevance score, andscoreQAS , score
QS , and score
QRS are the compo-
nent scores of the sentence S with respect to theancestor, current or other remaining nodes. Wegive positive credit to a sentence that contains key-words from an ancestor node, but penalize sen-tences with keywords from other topics (as suchsentences would be better descriptors for thoseother topics). Component relevance scores arecalculated using Term Frequency × Inverse Sen-tence Frequency (TF×ISF) (Otterbacher et al.,2005):
scoreQS
=rel(S,Q)?Q' rel(S,Q')
(2)
=
?w?Q log(tf
Sw + 1)× log(tfQw + 1)× isfw
Norm
where rel(S,Q) is the relevance of S with respectto topic Q, Norm is a normalization factor ofrel(S,Q) over all input sentences, tfSw and tf
Qw
are the term frequencies of token w within S orsentences that discuss topic Q, respectively. isfwis the inverse sentence frequency of w.
4.2 (S)pecific (C)ontent (Sum)marizationSCSum aims to extract sentences that contain de-tailed information about a specific author’s workthat is relevant to the input leaf node’s topic fromthe set of sentences that exhibit the author-as-agent. SCSum starts by computing the topic rel-evance of each candidate sentence as shown inEquation (3). This process is identical to the stepin GCSum, except that the term scoreQRS in Equa-tion (1) is replaced by scoreQSS , which is the rel-evance of S with respect to its sibling nodes. We
431
hypothesize that given a leaf node, sibling nodetopics may have an even more pronounced nega-tive effect than other remaining nodes in the topictree.
scoreS ? scoreQAS + scoreQS - scoreQSS (3)Context Modeling. We note that single sen-
tences occasionally do not contain enough con-texts to clearly express the idea mentioned in orig-inal articles. In fact, an agent-based sentence oftenintroduces a concept but pertinent details are of-ten described later. Extracting just the agent-basedsentence may incompletely describe a concept andlead to false inferences. Consider the examplein Figure 3. Here Sentences 0-5 are an contigu-ous extract of a source article being summarized,where Sentence 0 is an identified agent-based sen-tence. Sentence 6 shows a related work sectionsentence from a citing article that describes theoriginal article. It is clear that the citing descrip-tion is composed of information taken not onlyfrom the agent-based sentence but its context inthe following sentences as well. This observation
Figure 3: A context modeling example.
motivates us to choose nearby sentences within acontextual window after the agent-based sentenceto represent the topic. We set the contextual win-dow to 5 and extract a maximum of 2 additionalsentences. These additions are chosen based ontheir relevance scores to the topic, using Equa-tion (3). Sentences with non-zero scores are thenadded as contexts of the anchor agent-based sen-tence, otherwise they are excluded. As a result,some topics may contain only a single sentence,but others may be described by additional contex-tual sentences.
Weighting. The score of a candidate contentsentence is computed from topic relevance com-putation (SCSum) that includes contributions forkeywords present in the current, ancestor and sib-ling nodes. We observe that the presence of one ormore of current, ancestor and sibling nodes mayaffect the final score from the computation. Thus,to partially address this, we add a new weightingcoefficient for the score computed from the topicrelevance computation (SCSum) (Equation (3)) asfollows:
score*S = wQA,Q,QSS × scoreS (4)
where: wQA,Q,QSS is a weighting coefficient thattakes on differing values based on the presenceof keywords in the sentence. Q, QA, and QS de-note keywords from current, ancestor and siblingnodes. If the sentence contains keywords fromother sibling nodes, we assign a penalty of 0.1.Otherwise, we assign a weight of 1.0, 0.5, or 0.25,based on whether keywords are present from boththe ancestor node and current node, just the cur-rent node or just the ancestor node.
To build the final summary, ReWoS selects thetop scoring sentence and iteratively adds the nextmost highly ranked sentence, until the n sentencebudget is reached. We use SimRank (Li et al.,2008) to remove the next sentence to be added,if it is too similar to the sentences already in thesummary.
4.3 Generation
ReWoS generates its summaries by using depth-first traversal to order the topic nodes, as in RWS-Data we observed this to be the most prevalentdiscourse pattern. It calls GCSum and SCSum tosummarize individual nodes, distributing the totalsentence budget equally among nodes.
ReWoS post-processes sentences to improvefluency where possible. We first replace agentiveforms with a citation to the articles (e.g., “we” ?“(Wu and Oard, 2008)”). ReWoS also replacesfound abbreviations with their corresponding longforms, by connecting abbreviation with their ex-pansions by utilizing dependency relation outputfrom the Stanford dependency parser.
432
System ROUGE Recall Scores Human Evaluation ScoresROUGE-1 ROUGE-2 ROUGE-S4 ROUGE-SU4 Correctness Novelty Fluency Usefulness
LEAD 0.501 0.096 0.116 0.181 3.027 2.764 3.082 2.745MEAD 0.663 0.178 0.211 0.287 3.009 3.109 2.591 2.700ReWoS-WCM 0.584 0.127 0.154 0.227 3.618 3.391 3.391 3.609ReWoS-CM 0.698 0.183 0.218 0.298 3.691 3.618 2.955 3.573
Table 2: Evaluation results for ReWoS variants and baselines.
5 Evaluation
We wish to assess the quality of ReWoS, compar-ing to state-of-the-art generic summarization sys-tems. We first detail our baseline systems used forperformance comparison, and defined evaluationmeasures specific to related work summary eval-uation. In our evaluation, we use our manually-compiled RWSData data set.
We benchmark ReWoS against two baselinesystems: LEAD and MEAD. The LEAD baselinerepresents each of the cited article with an equalnumber of sentences. The first n sentences aredrawn from the article, meaning that the title andabstract are usually extracted. The order of the ar-ticle leads used in the resulting summary was de-termined by the order of articles to be processed.MEAD is a well-documented baseline extractivemulti-document summarizer, developed in (Radevet al., 2004). MEAD offers a set of different fea-tures that can be parameterized to create result-ing summaries. We conducted an internal tun-ing of MEAD to maximize its performance onthe RWSData. The optimal configuation uses justtwo tuned features of centroid and cosine similar-ity. Note that the MEAD baseline does use thetopic tree keywords in computing cosine similar-ity score. Our ReWoS system is the only sys-tem that leverages the topic tree structure whichis central to our approach. In our experiments, weused MEAD toolkit4 to produce the summaries forLEAD and MEAD baseline systems.
Automatic evaluation was performed withROUGE (Lin, 2004), a widely used and rec-ognized automated summarization evaluationmethod. We employed a number of ROUGE vari-ants, which have been proven to correlate with hu-man judgments in multi-document summarization(Lin, 2004). However, given the small size of oursummarization dataset, we can only draw notional
4http://www.summarization.com/mead/
evidence from such an evaluation; it is not possi-ble to find statistically significant conclusion fromour evaluation.
To partially address this, we also conducteda human evaluation to assess more fine-grainedqualities of our system. We asked 11 humanjudges to follow an evaluation guideline that weprepared, to evaluate the summary quality, con-sisting of the following evaluation measures:• Correctness: Is the summary content actuallyrelevant to the hierarchical topics given?• Novelty: Does the summary introduce novel in-formation that is significant in comparison withthe human created summary?• Fluency: Does the summary’s exposition flowwell, in terms of syntax as well as discourse?•Usefulness: Is the summary useful in supportingthe researchers to quickly grasp the related worksgiven hierarchical topics?
Each judge was asked to grade the four sum-maries according to the measures on a 5-pointscale of 1 (very poor) to 5 (very good). Sum-maries 1 and 2 come from LEAD-based andMEAD systems, respectively. Summaries 3 and4 come from our proposed ReWoS systems, with-out (ReWoS-WCM) and with (ReWoS-CM) thecontext modeling in SCSum. All summarizerswere set to yield a summary with the same length(1% of the original relevant articles, measured insentences). Due to limited time, only 10 out of 20evaluation sets were assessed by the evaluators.Each set was graded at least 3 times by 3 differentevaluators; evaluators did not know the identitiesof the systems, which were randomized for eachset examined.
6 Results
ROUGE results are summarized in Table 2. Sur-prisingly, the MEAD baseline system outperformsboth LEAD baseline and ReWoS–WCM (with-out context modeling). Only ReWoS–CM (with
433
context modeling) is significantly better than oth-ers, in terms of all ROUGE variants. Here aresome possible reasons to explain this. First,ROUGE evaluation seems to work unreasonablywhen dealing with verbose summaries, often pro-duced by MEAD. Second, related work sum-maries are multi-topic summaries of multi-articlereferences. This may cause miscalculation fromoverlapping n-grams that occur across multipletopics or references.
Since automatic evaluation with ROUGE doesnot allow much introspection, we turn to our hu-man evaluation. Results are also summarized inTable 2. They show that both ReWoS–WCMand ReWoS–CM perform significantly better thanbaselines in terms of correctness, novelty, and use-fulness. This is because our system utilized fea-tures developed specifically for related work sum-marization. Also, our proposed systems comparefavorably with LEAD, showing that necessary in-formation is not only located in titles or abstracts,but also in relevant portions of the research articlebody.
ReWoS–CM (with context modeling) per-formed equivalent to ReWoS–WCM (without it)in terms of correctness and usefulness. For nov-elty, ReWoS–CM is better than ReWoS–WCM.It proved that the proposed component of con-text moding is useful in providing new informa-tion that is necessary for the related work sum-maries. For fluency, only ReWoS–CM is bet-ter than baseline systems. This is a negative re-sult, but is not surprising because the summariesfrom the ReWoS–CM which uses context model-ing seems to be longer than others. It makes thesummaries quite hard to digest; some evaluatorstold us that they preferred the shorter summaries.A future extension is that using information fu-sion techniques to fuse the contextual sentenceswith its anchor agentive sentence.
A detailed error analysis of the results revealedthat there are three main types of errors producedby our systems. The first issue is in calculat-ing topic relevance. In the context of relatedwork summarization, our heuristics-based strate-gies for sentence extraction cannot capture fullythis issue. Some sentences that have high relevantscores to topics are not actually semantically rele-
vant to the topics. The second issue of anaphoricexpression is more addressable. Some extractedsentences still contain anaphoric expression (e.g.,“they”, “these”, “such”, . . . ), making final gen-erated summaries incoherent. The third issue isparaphrasing, where substituted paraphrases re-place the original words and phrases in the sourcearticles.
7 Conclusion and Future Work
According to the best of our knowledge, auto-mated related work summarization has not beenstudied before. In this paper, we have taken theinitial steps towards solving this problem, by di-viding the task into general and specific summa-rization processes. Our initial results show an im-provement over generic multi-document summa-rization baselines in human evaluation. However,our work shows that there is much room for addi-tional improvement, for which we have outlined afew challenges.
A shortcoming of our current work is that weassume that a topic hierarchy tree is given as in-put. We feel that this is an acceptable limitationbecause we feel existing techniques will be able tocreate such input, and that the topic trees used inthis study were quite simple. We plan to validatethis by generating these topic trees automaticallyin our future work.
Exploring related work summarization comesat a timely moment, as scholars now have accessto a preponderous amount of scholarly literature.Automated assistance in interpreting and organiz-ing scholarly work will help build future applica-tions for integration with digital libraries and ref-erence management tools.
ReferencesAngrosh, M. A., Stephen Cranefield, and Nigel
Stanger. 2010. Context identification of sentencesin related work sections using a conditional randomfield: towards intelligent digital libraries. In JCDL’10: Proceedings of the 10th annual joint confer-ence on Digital libraries, pages 293–302. ACM.
Barzilay, Regina, Noemie Elhadad, and Kathleen R.McKeown. 2002. Inferring strategies for sen-tence ordering in multidocument news summariza-
434
tion. In Journal of Artificial Intelligence Research,volume 17, pages 35–55.
Baxendale, P. B. 1958. Machine-made index for tech-nical literature - an experiment. IBM Journal of Re-search Development, 2(4):354–361.
Edmundson, H. P. 1969. New methods in automaticextracting. Journal of the ACM, 16(2):264–285.
Jaidka, Kokil, Christopher S. G. Khoo, and Jin-CheonNa. 2010. Imitating human literature review writ-ing: An approach to multi-document summariza-tion. In ICADL, pages 116–119.
Li, Wenjie, Furu Wei, Qin Lu, and Yanxiang He. 2008.PNR2: Ranking sentences with positive and nega-tive reinforcement for query-oriented update sum-marization. In Proceedings of 22nd InternationalConference on Computational Linguistics, pages489–496, Manchester, UK, August.
Lin, Chin-Yew. 2004. Rouge: A package for au-tomatic evaluation of summaries. In Proceed-ings of the ACL-04 Workshop Text SummarizationBranches Out, pages 74–81, Spain, July.
Luhn, H. P. 1958. The automatic creation of literatureabstracts. IBM Journal of Research Development,2(2):159–165.
M., Blei D., Griffiths T. L., Jordan M. I., and Tenen-baum J. B. 2004. Hierarchical topic models and thenested chinese restaurant process. In Advances inNeural Information Processing Systems (NIPS).
Mei, Qiaozhu and ChengXiang Zhai. 2008. Gen-erating impact-based summaries for scientific lit-erature. In Proceedings of the 46th Annual Meet-ing of the Association of Computational Linguistics(ACL), pages 816–824, Columbus, Ohio, June.
Mihalcea, Rada and Hakan Ceylan. 2007. Explo-rations in automatic book summarization. In Pro-ceedings of Empirical Methods in Natural Lan-guage Processing - Conference on Natural Lan-guage Learning (EMNLP-CoNLL), pages 380–389,Prague, Czech Republic, June.
Mohammad, S., B. Dorr, M. Egan, A. Hassan,P. Muthukrishan, V. Qazvinian, D. Radev, andD. Zajic. 2009. Using citations to generate surveysof scientific paradigms. In Proceedings of HumanLanguage Technologies - North American Associa-tion for Computational Linguistics (HLT-NAACL),pages 584–592, Boulder, Colorado, June.
Nakov, Preslav I., Ariel S. Schwartz, and Marti A.Hearst. 2004. Citances: Citation sentences for se-mantic analysis of bioscience text. In Workshop onSearch and Discovery in Bioinformatics.
Nallapati, R. M., A. Ahmed, E. P. Xing, and W. W.Cohen. 2008. Joint latent topic models for text andcitations. In Proceeding of the 14th ACM SIGKDDInternational Conference on Knowledge Discoveryin Data and Data Mining, pages 542–550.
Otterbacher, Jahna, Gu¨nes¸ Erkan, and Dragomir R.Radev. 2005. Using random walks for question-focused sentence retrieval. In Proceedings of Hu-man Language Technologies - Empirical Methods inNatural Language Processing (HLT-EMNLP ’05),pages 915–922. ACL.
Qazvinian, Vahed and Dragomir R. Radev. 2008. Sci-entific paper summarization using citation summarynetworks. In Proceedings of International Con-ference on Computational Linguistics (COLING),pages 689–696, Manchester, UK, August.
Radev, Dragomir R., Hongyan Jing, Malgorzata Sty,and Daniel Tam. 2004. Centroid-based summariza-tion of multiple documents. Information Processing& Management (IPM), 40(6):919–938.
Schwartz, Ariel S. and Marti Hearst. 2006. Summa-rizing key concepts using citation sentences. In Pro-ceedings of Natural language processing of biologytext (BioNLP ’06), pages 134–135. ACL.
Teufel, Simone and Marc Moens. 2002. Summariz-ing scientific articles: experiments with relevanceand rhetorical status. Computational Linguistics,28(4):409–445.
Teufel, Simone, Advaith Siddharthan, and ColinBatchelor. 2009. Towards domain-independent ar-gumentative zoning: Evidence from chemistry andcomputational linguistics. In Proceedings of the2009 Conference on Empirical Methods in Natu-ral Language Processing, pages 1493–1502, Singa-pore, August. Association for Computational Lin-guistics.
Teufel, Simone. 1999. Argumentative Zoning: Infor-mation Extraction from Scientific Text. Ph.D. thesis,University of Edinburgh.
Witten, Ian H., Gordon Paynter, Eibe Frank, CarlGutwin, and Craig G. Nevill-Manning. 1999. Kea:Practical automatic keyphrase extraction. In Pro-ceedings of Digital Libraries 99 (DL’99), pages254–255. ACM Press.
Wu, Yejun and Douglas W. Oard. 2008. Bilingualtopic aspect classification with a few training exam-ples. In SIGIR ’08: Proceedings of the 31st an-nual international ACM SIGIR conference on Re-search and development in information retrieval,pages 203–210, New York, NY, USA. ACM.
435
