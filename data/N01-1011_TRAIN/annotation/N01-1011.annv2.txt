Citance Number: 1 | Reference Article:  N01-1011.txt | Citing Article:  W02-0812.txt | Citation Marker Offset:  23623-23638 | Citation Marker:  Pedersen, 2001a | Citation Offset:  23490-23997 | Citation Text:  The frustration with models that lack an intuitive interpretation led to the development of decision trees based on bigram features (Pedersen, 2001a). This is quite similar to the bagged decision trees of bigrams (B) presented here, except that the earlier work learns a single decision tree where training examples are represented by the top 100 ranked bigrams, according to the loglikelihood ratio. This earlier approach was evaluated on the SENSEVAL -1 data and achieved an overall accuracy of 64% | Reference Offset:  ['84', '101', '38,'161', '168'] | Reference Text:  Our empirical study utilizes the training and test data from the 1998 SENSEVAL evaluation of word sense disambiguation systems ... Two feature sets are selected from the training data based on the top 100 ranked bigrams according to the power divergence statistic and the Dice Coefficient ... Table 1: Experimental Results ... win-tie-loss (j48-pow vs. X) 23-7-6 19-0-17 30-0-6 28-9-3 14-15-7 28-9-3 24-1-11 ... While the accuracy of this approach was as good as any previously published results, the learned models were complex and difficult to interpret, in effect acting as very accurate black boxes ... Most important, their structure is easy to interpret and may provide insights into the relationships that exist among features and more general rules of disambiguation | Discourse Facet:  Method_Facet | Annotator:  Muthu Kumar Chandrasekaran, NUS |


Citance Number: 2 | Reference Article:  N01-1011.txt | Citing Article:  W04-0813.txt | Citation Marker Offset:  6792-6806 | Citation Marker:  Pedersen, 2001 | Citation Offset:  6693-6807 | Citation Text:  We also obtain salient bigrams in the context, with the methods and the software described in (Pedersen, 2001) | Reference Offset:  ['18', '19', '61', '62'] | Reference Text:  The approach in this paper relies upon a feature set made up of bigrams, two word sequences that occur in a text. The context in which an ambiguous word occurs is represented by some number of binary features that indicate whether or not a particular bigram has occurred within approximately 50 words to the left or right of the word being disambiguated. ... We have developed the Bigram Statistics Package to produce ranked lists of bigrams using a range of tests. This software is written in Perl and is freely available from www.d.umn.edu/~tpederse | Discourse Facet:  Method_Facet | Annotator:  Muthu Kumar Chandrasekaran, NUS |


Citance Number: 3 | Reference Article:  N01-1011.txt | Citing Article:  W02-1011.txt | Citation Marker Offset:  27383-27392 | Citation Marker:  2001 | Citation Offset:  27374-27478 | Citation Text:  in fact, Pedersen (2001) found that bigrams alone can be effective features for word sense disambiguation | Reference Offset:  ['173', '189'] | Reference Text:  We believe that the approach in this paper is the first time that decision trees based strictly on bigram features have been employed ... This paper shows that the combination of a simple feature set made up of bigrams and a standard decision tree learning algorithm results in accurate word sense disambiguation | Discourse Facet:  Results_Facet | Annotator:  Muthu Kumar Chandrasekaran, NUS |


Citance Number: 4 | Reference Article:  N01-1011.txt | Citing Article:  N03-3004-parscit.txt | Citation Marker Offset:  19932-19946 | Citation Marker:  Pedersen, 2001 | Citation Offset:  19827-19947 | Citation Text:  Bigrams have recently been shown to be very successful features in supervised word sense disambiguation (Pedersen, 2001) | Reference Offset:  ['189'] | Reference Text:  This paper shows that the combination of a simple feature set made up of bigrams and a standard decision tree learning algorithm results in accurate word sense disambiguation | Discourse Facet:  Results_Facet | Annotator:  Muthu Kumar Chandrasekaran, NUS |


Citance Number: 5 | Reference Article:  N01-1011.txt | Citing Article:  C02-1039.txt | Citation Marker Offset:  3131-3362 | Citation Marker: Pedersen, 2001 | Citation Offset:  3287-3301 | Citation Text:  Commonly used features include surrounding words and their part of speech(Bruce and Wiebe, 1999), context keywords (Ng and Lee, 1996) or context bigrams (Pedersen, 2001), various syntactic properties (Fellbaum et al., 2001) etc | Reference Offset:  ['18', '19', '61', '62'] | Reference Text:  The approach in this paper relies upon a feature set made up of bigrams, two word sequences that occur in a text. The context in which an ambiguous word occurs is represented by some number of binary features that indicate whether or not a particular bigram has occurred within approximately 50 words to the left or right of the word being disambiguated. ... We have developed the Bigram Statistics Package to produce ranked lists of bigrams using a range of tests. This software is written in Perl and is freely available from www.d.umn.edu/~tpederse | Discourse Facet:  Method_Facet | Annotator:  Muthu Kumar Chandrasekaran, NUS |


Citance Number: 6 | Reference Article:  N01-1011.txt | Citing Article:  C02-1039.txt | Citation Marker Offset:  3513-3527 | Citation Marker:  Pedersen, 2001 | Citation Offset:  3364-3528 | Citation Text:  As for the learning methodology, a large range of algorithms have been employed, including neural networks (Leacock et al., 1998), decision trees (Pedersen, 2001) | Reference Offset:  ['1'] | Reference Text:  This paper presents a corpus-based approach to word sense disambiguation where a decision tree assigns a sense to an ambiguous word based on the bigrams that occur nearby | Discourse Facet:  Method_Facet | Annotator:  Muthu Kumar Chandrasekaran, NUS |


Citance Number: 7 | Reference Article:  N01-1011.txt | Citing Article:  J02-2003.txt | Citation Marker Offset:  29088-29096 | Citation Marker:  2001 | Citation Offset:  29075-29352 | Citation Text:  In addition, Pedersen (2001) questions whether one statistic should be preferred over the other for the bigram acquisition task and cites Cressie and Read (1984), who argue that there are some cases where the Pearson statistic is more reliable than the log-likelihood statistic | Reference Offset:  ['39', '40', '42'] | Reference Text:  (Dunning, 1993) argues in favor of G2 over X2, especially when dealing with very sparse and skewed data distributions. However, (Cressie and Read, 1984) suggest that there are cases where Pearson's statistic is more reliable than the likelihood ratio and that one test should not always be preferred over the other ... Unfortunately it is usually not clear which test is most appropriate for a particular sample of data | Discourse Facet:  Implication_Facet | Annotator:  Muthu Kumar Chandrasekaran, NUS |


Citance Number: 8 | Reference Article:  N01-1011.txt | Citing Article:  W08-0611.txt | Citation Marker Offset:  16883-16898 | Citation Marker:  2001 | Citation Offset:  16777-16898 | Citation Text:  Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores, as described by Pedersen (2001) | Reference Offset:  ['18', '29', '30'] | Reference Text:  The approach in this paper relies upon a feature set made up of bigrams, two word sequences that occur in a text ... Given the sparse and skewed nature of this data, the statistical methods used to select interesting bigrams must be carefully chosen. We explore two alternatives, the power divergence family of goodness of fit statistics and the Dice Coefficient, an information theoretic measure related to point-wise Mutual Information | Discourse Facet:  Method_Facet | Annotator:  Muthu Kumar Chandrasekaran, NUS |


Citance Number: 9 | Reference Article:  N01-1011.txt | Citing Article:  W09-1309.txt | Citation Marker Offset:  12373-12381 | Citation Marker:  2001 | Citation Offset:  12267-12388 | Citation Text:  Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores, as described by Pedersen (2001) | Reference Offset:  ['181', '29', '30'] | Reference Text:  The approach in this paper relies upon a feature set made up of bigrams, two word sequences that occur in a text ... Given the sparse and skewed nature of this data, the statistical methods used to select interesting bigrams must be carefully chosen. We explore two alternatives, the power divergence family of goodness of fit statistics and the Dice Coefficient, an information theoretic measure related to point-wise Mutual Information | Discourse Facet:  Method_Facet | Annotator:  Muthu Kumar Chandrasekaran, NUS |


