<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">We automatically classify verbs into lexical se­ rnantic classes, based on distributions of indie&lt;t­ tors or verb alternations, extracted from a very large a.nnota.ted corpus.</S>
		<S sid ="2" ssid = "2">\Ve address a problem which is particularly difficult because the verb classes, although semantically difl&apos;err nt, ::;how sim­ ilar surface syntactic behavior.</S>
		<S sid ="3" ssid = "3">Five p;rantmatica.l fea.ttJres arc suflicient to reduce error rate by more th&lt;w 50% over dance: we achieve almost 70% accuracy in a task whose baseline performance is :11%, and whos&lt;expert-based upptr bound we cal­ culated a.t 8G.51/L. vVe conclude that corpu::;-driven extraction of grannua.tical f&quot;ea.turcs is a promi::;ing methodology for fin&lt; -grained verb classification.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="4" ssid = "4">Deta.ilcd information about verbs is critical to a broad range of NLI&gt; and IR tasks, yet its Jnau­ ual determination for large numbers of verbs is difficult and resourc&lt;&apos; intensive.</S>
			<S sid ="5" ssid = "5">H.mearch 011 tit&lt;&apos; automatic acquisition of&apos; verb-bas&lt; d krtowledg&lt;&apos; has succeded in p;leaning syntactic prop&lt;&apos;rti&lt;s of v&lt; rbs such as subcategorization frames from on­ line resources (lh&lt;mt, 199:3; Briscoe and Carroll, 1997; Dorr, 1997; Manning, 1993).</S>
			<S sid ="6" ssid = "6">Recently, researchers have investigated statistical corpus­ based methods for lexical semantic classification from syntactic properties of verb usage (;\one and Ivlcl(cc, 199G; La.pata aud Brew, 1999; Schulte i111 Walde, 199H; Stevenson and Ivlerlo, 1999; Steven­ son ct al., 1999; McCarthy, 2000).</S>
			<S sid ="7" ssid = "7">Corpus-based approaches to lexical semantic classification in particular have drawn on Levin&apos;s hypothesis (Levin, 1993) that verbs can be classi­ fied according to the diathesis a.lternations (alter­ nations in !.he syntactic expressions or arguments) in which they participate--for example, whether a • &apos;fhis research was partly sponsored hy US N ·W p;rauts#0702:1:{1 aml # li5J8:J22, Swiss NSF fellowship 8210 16.5()0, Information Sciences Cmmcil of Hutgers University and !HCS, U _ of Pennsylvania.</S>
			<S sid ="8" ssid = "8">This research was cml­ clncted while the first author was at Hutgers University.</S>
			<S sid ="9" ssid = "9">Paola Merlo LATL - Department of Linguistics University of Geneva.</S>
			<S sid ="10" ssid = "10">2 rue de Caudollc 1211 Gcncvc tlSuisse.</S>
			<S sid ="11" ssid = "11">merlo©lettres.unige.ch verb occurs in the dative/prepositional phrase al­ ternation in English.</S>
			<S sid ="12" ssid = "12">One diagnostic for diathesis alternations is the subcategoriza.tion a.lternatives of a. verb.</S>
			<S sid ="13" ssid = "13">However, some classes exhibit the same subca!.egorization possibilities but difl&apos;er in their argument structures, i.e. the content of the the­ tnatic roles assigned to the arguments of the verb.</S>
			<S sid ="14" ssid = "14">This type of situation constitutes a particularly difficult case for corpus-based classiltcation meth­ ods.</S>
			<S sid ="15" ssid = "15">Jn this paper, we apply corpus-based lexica.!</S>
			<S sid ="16" ssid = "16">acquisition tucthodology to distinguish classes of verbs which allow the sante su bcatcgorizations, but difrer in thematic roles.</S>
			<S sid ="17" ssid = "17">\Ve first assume that one can a.tttontatically restrict the choice of classes t.o those that participate in the relevant su bcai.c­ gorizations (d.</S>
			<S sid ="18" ssid = "18">(La.pata.</S>
			<S sid ="19" ssid = "19">and Brew, ID99)).</S>
			<S sid ="20" ssid = "20">Our proposal is then to use statistics over diathesis alternants as a. way to f&apos;urthcr distinguish those verbs w h iclt allow tlw sante su bca.t&lt; goriza.tions, achi&lt;&apos;vinp; fin&lt; -gra.incd classification within that S&lt;&apos;t.</S>
			<S sid ="21" ssid = "21">Our work focuses 011 dekrtnining the bestS&lt; ­ llla.ntic class for a verb lypr-- the set of usages of a verb across a document or corpus--rather than for a single verb lokr:n. in a single loca.l context.</S>
			<S sid ="22" ssid = "22">In this way, we can exploit the broad behavior of the verb across the corpus to determine its most likely class overall.</S>
			<S sid ="23" ssid = "23">VVe investigate the proposed approach in an in­ depth case study of the three major classes of op­ tionally intransitive verbs in English: utwrgat.ive, una.ccusativc, and object-drop.</S>
			<S sid ="24" ssid = "24">Nlore specifically, according to Levin&apos;s classification ( lA vin, 1993), the unergatives arc manner of motion verbs, such as .iump and march; the unaccusatives arc verbs of change of state, such as open and explode; the object-drop verbs are unexpressed object a.lterna­ tion verbs, such as played and painlcd.</S>
			<S sid ="25" ssid = "25">These classes all support both transitive and intransi­ tive subcategoriza.tions, but arc distinguished by the pattern of thematic role assignments to sub­ jed and object position.</S>
			<S sid ="26" ssid = "26">\Ve au tom atica.lly clas­ sify these verbs on the basis of statistical ap proximations to syntactic indicators of the under­ lying argument structures, using numerical fea­ tures collected from a large syntactically anno­ tated (tagged or parsed) corpus.</S>
			<S sid ="27" ssid = "27">We apply ma­ chine learning techniques to determine whether the frequency distributions of the features, in­ dividually or in combination, support automatic classification of the verbs.</S>
			<S sid ="28" ssid = "28">To preview our re­ sults, we demonstrate that combining only fivc numerical indicators is sufficient to reduce the er­ ror rate in this classification task by more than 50% over chance.</S>
			<S sid ="29" ssid = "29">Specifically, we achieve almost 70% accuracy in a task whose baseline (chance) performance is 31%, and whose expert-based up­ per bound is calculated at 86.5%.</S>
			<S sid ="30" ssid = "30">We conclude that a distribution-based method for lexical se­ mantic verb classification is a promising avenue of research.</S>
	</SECTION>
	<SECTION title="The Argument Structures. " number = "2">
			<S sid ="31" ssid = "1">Our approach rests on the hypothesis that, even in cases where verb classes cannot be distinguished by subcategorizations, the frequency distributions of syntactic indicators can hold clues to the under­ lying thematic role differences.</S>
			<S sid ="32" ssid = "2">We start here then with a description of the subcatcgorizations and thematic role assignments for each of the three verb classes under investigation.</S>
			<S sid ="33" ssid = "3">As optionally intransitive verbs, each of the th rce classes participates in the transi­ tive/intransitive alternation: U ner?;ativc (la) The horse raced past the barn.</S>
			<S sid ="34" ssid = "4">(lb) The jockey raced the horse past the barn.</S>
			<S sid ="35" ssid = "5">U naccusative (2a) The butter melted in the pan.</S>
			<S sid ="36" ssid = "6">(2b) The cook melted the butter in the pan.</S>
			<S sid ="37" ssid = "7">Object-drop (3a) The boy washed the hall.</S>
			<S sid ="38" ssid = "8">(3b) The boy washed.</S>
			<S sid ="39" ssid = "9">U nergativcs are intransitive action verbs, as in (l), whose transitive form can be the causative coun­ terpart of the intransitive form.</S>
			<S sid ="40" ssid = "10">In the causative use, the semantic argument that appears as the subject of the intransitive, as in (la), surfaces as the object of the transitive, as in (lb) (Hale and Keyser, 1993).</S>
			<S sid ="41" ssid = "11">Unaccusatives are intransitive change of state verbs, as in (2a); the transitive counterpart for these verbs exhibits the causative alternation, as in (2b).</S>
			<S sid ="42" ssid = "12">Object-drop verbs, as in (3), have a non-causative transitive/intransitive alternation, in which the object is simply optionaL Table 1: Summary of Thematic Alternations.</S>
			<S sid ="43" ssid = "13">Each class is distinguished by the content of the thematic roles assigned by the verb.</S>
			<S sid ="44" ssid = "14">For object­ drop verbs, the subject is an Agent and the op­ tional object is a Theme, yielding the thematic assignments (Agent, Theme) and (Agent) for the transitive and intransitive alternants respectively.</S>
			<S sid ="45" ssid = "15">Unergatives and unaccusatives differ from object­ drop verbs in participating in the causative alter­ nation, and also differ from each other in their core thematic argument.</S>
			<S sid ="46" ssid = "16">In an intransitive uncrga­ tivc, the subject is an Agent, and in an intran­ sitive unaccusativc, the subject is a Theme.</S>
			<S sid ="47" ssid = "17">In the causative transitive form of each, this core se­ mantic argument is expressed as the direct object, with the addition of a Causa.!</S>
			<S sid ="48" ssid = "18">Agent (the causer of the action) as subject in both cases.</S>
			<S sid ="49" ssid = "19">The thematic roles assigned, and their mapping to syntactic po­ sition, arc summarized in Table l.</S>
	</SECTION>
	<SECTION title="The Features for  Classification. " number = "3">
			<S sid ="50" ssid = "1">The key to any automatic classification task is to determine a set of useful features for discriminat­ ing the items to be classified.</S>
			<S sid ="51" ssid = "2">ln what follows, we refer to the columns of Table 1 to explain how we expect the thematic distinctions to yield distri­ butional features whose frequencies discriminate among the classes at hand.</S>
			<S sid ="52" ssid = "3">Considering column one of Table 1, only unergative and unaccusa.tive verbs assign a Causal Agent to the subject of the transitive.</S>
			<S sid ="53" ssid = "4">\1\Tc hy­ pothesize that the causative construction is lin­ guistically more complex than the simple argu­ ment optionality of object-drop verbs (Stevenson and Merlo, 1997).</S>
			<S sid ="54" ssid = "5">We expect then that object­ drop verbs will be more frequent in the tr&lt;wsi­ tivc than the other two classes.</S>
			<S sid ="55" ssid = "6">Furthermore, the object of an unergative verb receives the Agent role (see the second column of Table 1), a linguis­ tically marked transitive construction (Stevenson and Merlo, 1997).</S>
			<S sid ="56" ssid = "7">We therefore expect uncrga­ tives to be quite rare in the transitive, leading to a three-way distinction in transitive usage among the three classes.</S>
			<S sid ="57" ssid = "8">Second, due to the causative alternation of &apos;fable 2: &apos;J&apos;he 1•&apos;catures and Their Expected Behavior Transitivity Unaccwmtivcs and unergativcs have a causative transitive, hence lower transitive usc.</S>
			<S sid ="58" ssid = "9">Fmthennore, unergat.ive:c; have all agent.ive object, hence very low transitive usc.</S>
			<S sid ="59" ssid = "10">Passive Voice Passive implies transitive usc, hence corrclatccl with transitive feature.</S>
			<S sid ="60" ssid = "11">VBN Tag Passive implies past participle usc (VBN), hence correlated with transitive (and passive).</S>
			<S sid ="61" ssid = "12">Cansativity Object-drop verbs do not have a causal age.nt, hence low &quot;causative&quot; use.</S>
			<S sid ="62" ssid = "13">Unergat.ives arc rare in the transitive, hence low causative usc.</S>
			<S sid ="63" ssid = "14">An&apos;i_n_1_ac-·y--- U n-a_c_c_u_sati vcs have a Them c su b_fcc-c·t-to-.n--c-.!.1lc-o,-i1--ct1-r-;u-1s-· i-:-to-\i-,e-.,h_e_n_c_e lo_w_e_r-u-sc--o f_a_n..,..it_n_act-c_s_u b 7je-,c-:-&apos;L·-s-.</S>
			<S sid ="64" ssid = "15">-H unergativcs and unaccusativec;, the thematic role of the su{ jed of the intransitive is identical to that of the object of the transitive, as shown in columns two and three of &apos;fable I. C:ivcn the identity of thematic role mapped to subject and object positions, we expect to observe the same noun occurring a.t times as subject of the verb, and a.t other times as object of the vcr b. In con­ trast, for object-drop verbs, the thematic role of the ;ubject o[ the intransitive is identical to that of the subject of the transitive, not the object of the transitive.</S>
			<S sid ="65" ssid = "16">&apos;J&apos;hus, we expect that it will he less common for the same noun to occur in subject and object position of the same object-drop verb.</S>
			<S sid ="66" ssid = "17">We hypothesize that this pattern of thematic role as­ signments will be reflected in diff&apos;erential amount of w;ae across the classes of the same nouns as subjects and objects for a. iwn verb.</S>
			<S sid ="67" ssid = "18">1&lt;&apos;11 rther­ mon, since the causative is a. transitive use, and the transitive use of uner a.tives is ( xpec:Led to be rare, this overlap of subjects and objects should prim.arily distinguish una.ccusatives (predicted to have high overlap of subjects and objects) from the other two classes.</S>
			<S sid ="68" ssid = "19">Finally, considering columns one and three of Table 1, we note that unerga.tivc and object-drop verbs assign an agentive role to their subject in botl1 the transitive and intransitive, while unac­ cusatives assign an agcntive role to their subject only in the transitive.</S>
			<S sid ="69" ssid = "20">Under the assumption that the intransitive usc of unaccusativcs is not rare, 1 we then expect that unaccusativcs will occur less often overall with an agentivc subject than the other two verb classes.</S>
			<S sid ="70" ssid = "21">On the further assump­ tion that Agents tend to be animate entities more so than Themes, we expect that unaccusatives will occur less frequently with an animate subject compared to uncrgative and object-drop verbs.</S>
			<S sid ="71" ssid = "22">be animate, but rather that nouns that receive an Agent role will more often be animate than nouns that receive a Theme role.</S>
			<S sid ="72" ssid = "23">The above interactions between thematic roles and the syntactic expressions of arguments thus lead to three features whose distributional proper­ tics appear promising for distinguishing the verb classes: transitivity, causativity, and animacy of subject.</S>
			<S sid ="73" ssid = "24">V•.,Te also investigate two additional syn­ tactic features, the passive voice and the past par­ ticiple POS tag (V13N).</S>
			<S sid ="74" ssid = "25">These features are related to the transitive/intransitive alternation, since a passive usc implies a transitive usc of the verb, and the usc of passive in turn implies the usc of the past participle.</S>
			<S sid ="75" ssid = "26">Our hypothesis is that these five features will exhibit clistribu tiona!</S>
			<S sid ="76" ssid = "27">differences in the observed usages of the verbs, which can be used for classification.</S>
			<S sid ="77" ssid = "28">The features and their ex­ pected relevance arc summarized in Table 2.</S>
			<S sid ="78" ssid = "29">1 Data Collection and Analysis.</S>
			<S sid ="79" ssid = "30">\Vc chose a set of 20 verbs from each of&apos; three classes.</S>
			<S sid ="80" ssid = "31">The complete list of verbs is reported in Appendix A. Recall that our goal is to achieve a fine-gra.incd classification of verbs that exhibit the same su bcategori:;,ation frames; thus, the verbs were chosen because they do not generally show massive departures from the intended verb sense (and usage) in the corpus.2 In order to simplify the counting procedure, we included only the reg­ ular (&quot;-eel&quot;) simple past/ past participle form of the verb, assuming that this would approximate the distribution of the features across all forms of the verb.</S>
			<S sid ="81" ssid = "32">FinaJly, as far as we were able given the preceding constraints, we selected verbs that could occur in the transitive and in the passive.</S>
			<S sid ="82" ssid = "33">We counted the occurrences of each verb token in a transitive or intransitive usc (TitANs), in a Note the importance of our usc of frequency dis­ tributions: the claim is not that only Agents can 1 This assumption is based on t.hc linguistic complexi Ly of the causative, and borne ont in our corpus analysis.</S>
			<S sid ="83" ssid = "34">2 Though note that there arc only 19 nnaccusatives be­.</S>
			<S sid ="84" ssid = "35">cause ripped was excluded from the analysis as it occurred mostly in a very different nse ( 1&apos;ippcd off) in Lhe corpus from the intended change of state usage.</S>
			<S sid ="85" ssid = "36">passive or active use (PAss), in a past participle or simple past usc (vHN), in a causative or non­ causative use (cAus), and with an animate subject or not (ANIM), as described below.</S>
			<S sid ="86" ssid = "37">The first three counts (TRANS, PASS, V13N) were performed on the LDC&apos;s G5-million wore!</S>
			<S sid ="87" ssid = "38">tagged ACL/DCI cor­ pus (l3rown, and Wall Street Journal 19871989).</S>
			<S sid ="88" ssid = "39">The last two counts (cAUS and ANIM) were per­ formed on a 29-million word parsed corpus (Wall Street Journal 1988, provided by Michael Collins (Collins, 1997)).</S>
			<S sid ="89" ssid = "40">The features were counted as follows: TRANS: The closest noun following a verb was considered a potential object.</S>
			<S sid ="90" ssid = "41">A verb immedi­ ately followed by a potential object was counted as transitive, otherwise as intransitive.</S>
			<S sid ="91" ssid = "42">PASS: A token tagged VBD (the tag for simple past) was counted a.s active.</S>
			<S sid ="92" ssid = "43">A token tagged VBN (the tag for past participle) was counted as active if the closest preceding auxiliary was have, and as passive if the closest preceding auxiliary was be.</S>
			<S sid ="93" ssid = "44">VBN: The counts for VDN/VBD were based on the POS label in the tagged corpus.</S>
			<S sid ="94" ssid = "45">Each of the above counts was normalized over a.ll occurrences of the &quot;-cd&quot; form of the verb, yield­ ing a single relative frequency measure for each verb for that feature.</S>
			<S sid ="95" ssid = "46">CAUS: For each verb token, the subject and ob­ ject (if there was one) were extracted from the parsed corpus, and the proportion of overlap be­ tween subject and object nouns across all tokens of a verb was calculated.</S>
			<S sid ="96" ssid = "47">ANIM: To approximate animacy without refer­ ence to a resource external to the corpus (such as VVordNct), we count pronouns (other than it) in subject position (cf&apos;.</S>
			<S sid ="97" ssid = "48">(Aonc and McKee, 1996)).</S>
			<S sid ="98" ssid = "49">The assumption is that the words I, we, you, she, he, and they most often refer to animate entities.</S>
			<S sid ="99" ssid = "50">VVc automatically extracted all subject/verb tu­ ples, and computed the ratio of occurrences of pronoun subjects to all subjects for each verb.</S>
			<S sid ="100" ssid = "51">The aggregate means by class resulting from the counts above arc shown in Table 3.</S>
			<S sid ="101" ssid = "52">The distri­ butions of each feature are indeed roughly as ex­ pected according to the description in Section 3.</S>
			<S sid ="102" ssid = "53">Uncrgatives show a very low relative frequency of the TRANS feature, followed by unaccusatives, then object-drop verbs.</S>
			<S sid ="103" ssid = "54">U naccusative verbs show a high frequency of the CAUS feature and a low frequency of the ANIM feature compared to the other classes.</S>
			<S sid ="104" ssid = "55">Although expected to be a redun­ dant indicator of transitivity, PASS and VBN do &apos;fable 3: Aggregated Relative Frequency Data for the Five Features.</S>
			<S sid ="105" ssid = "56">E = unergatives, A = unac­ cusatives, 0 = object-drops.</S>
			<S sid ="106" ssid = "57">Cl as s N MEAN RELATIVE FREQUENCY TR AN S PA SS VB N CA US AN IM E A 0 20 19 20 0 . 2 3 0 . 4 0 0 . 6 2 0.</S>
			<S sid ="107" ssid = "58">0 7 0.</S>
			<S sid ="108" ssid = "59">3 3 0.</S>
			<S sid ="109" ssid = "60">3 1 0.</S>
			<S sid ="110" ssid = "61">21 0.</S>
			<S sid ="111" ssid = "62">65 0.</S>
			<S sid ="112" ssid = "63">65 0.</S>
			<S sid ="113" ssid = "64">0 0 0.</S>
			<S sid ="114" ssid = "65">1 2 0.</S>
			<S sid ="115" ssid = "66">0 4 0.</S>
			<S sid ="116" ssid = "67">2 5 0.</S>
			<S sid ="117" ssid = "68">0 7 0.</S>
			<S sid ="118" ssid = "69">1 5 not distinguish between unaccusative and object­ drop verbs, indicating that their distributions are sensitive to factors we have not yet investigated.:1 5 Experiments in Classification.</S>
			<S sid ="119" ssid = "70">The frequency distributions of our features yield a vector for each verb that represents the relative frequency values for the verb on each dimension: [verb, TRANS, PASS, VBN, CAUS, ANIM, class) Example: [opened, .69, .09, .21, .16, .36, unacc] \t\Tc use the resulting 59 vectors to train an au­ tomatic classifier to determine, given a verb that exhibits transitive/intransitive subcatcgori?:ation frames, which of the three major lexical semantic classes of English optionally intransitive verbs it belongs to.</S>
			<S sid ="120" ssid = "71">Note that the baseline (chance) per­ formance in this task is 33.9%, since there arc .59 vectors and 3 possible classes, with the most com­ mon class having 20 verbs.</S>
			<S sid ="121" ssid = "72">\t\Tc used the C5.0 machine learning system (http:/ jwww.rulequcst.com), a newer version of C4.5 (Quinlan, ]992), which generates decision trees and corresponding rule sets from a. training set of known classifications.</S>
			<S sid ="122" ssid = "73">\Nc found little to no difference in performance between the trees and rule sets, and report only the rule set results.</S>
			<S sid ="123" ssid = "74">\1\Te report here on experimcn ts using a single hold­ out training and testing methodology.</S>
			<S sid ="124" ssid = "75">In this ap­ proach, we hold out a single verb vector as the test case, and train the system on the remaining 58 cases.</S>
			<S sid ="125" ssid = "76">We then test the resulting classifier on the single holdout case, recording the assigned class for that verb.</S>
			<S sid ="126" ssid = "77">This is then repeated for each of the 59 verbs.</S>
			<S sid ="127" ssid = "78">This technique has the benefit of yielding both an overall accuracy rate (when the results are averaged across all 59 trials), as well as providing the data necessary for determin­ ing accuracy for each verb class (because \Ve have the classification of each verb when it is the test case).</S>
			<S sid ="128" ssid = "79">This allows us to evaluate the contribution 3 These observations have been confirmed by t-tests be­.</S>
			<S sid ="129" ssid = "80">tween feature values for each pair of classes.</S>
			<S sid ="130" ssid = "81">Table 1]: Percent Accuracy of Verb Clas­ sification Task Using Featnres in Combina­ tion.</S>
			<S sid ="131" ssid = "82">T=THANS; P=PAss; V=VBN; C=cAus; An=ANIM.</S>
			<S sid ="132" ssid = "83">E=unergatives, A=unaccusatives, O==object-drops Percent.</S>
			<S sid ="133" ssid = "84">Accuracy hy Class Fe at ur es All E A () 1..</S>
			<S sid ="134" ssid = "85">&apos;J&apos; P V C An 69 .5 85 .0 63 .2 6 0.</S>
			<S sid ="135" ssid = "86">0 2.</S>
			<S sid ="136" ssid = "87">PVC An :3.</S>
			<S sid ="137" ssid = "88">TV CAn 1.</S>
			<S sid ="138" ssid = "89">T l&apos; CAn 5.</S>
			<S sid ="139" ssid = "90">T P VAn 6.</S>
			<S sid ="140" ssid = "91">T PVC 61 .1 71 .2 G l. O G 2.</S>
			<S sid ="141" ssid = "92">7 61 .0 80 .0 80.</S>
			<S sid ="142" ssid = "93">0 65 .0 70.</S>
			<S sid ="143" ssid = "94">0 80 .0 17 .1 73 .7 G8 .1 G3 .2 12 .1 6 5.</S>
			<S sid ="144" ssid = "95">0 G O. O 5 0.</S>
			<S sid ="145" ssid = "96">0 5 5.</S>
			<S sid ="146" ssid = "97">0 6 0.</S>
			<S sid ="147" ssid = "98">0 of individual features with respect to tlwir effect on the perfonnance of individua.l classes.</S>
			<S sid ="148" ssid = "99">the classes&apos; frequency.</S>
			<S sid ="149" ssid = "100">U nergatives have the low­ est average (log) frequency (1.3), but arc the best classified, while unaccusatives and object-drops arc comparable (average log frequency= 2).</S>
			<S sid ="150" ssid = "101">If we group verbs by frequency, the proportion of errors to the total number of verbs remains fairly simi­ lar (frcq 1: 7 crrors/2verbs; frcq.</S>
			<S sid ="151" ssid = "102">2: 6 errors/24 verbs; frcq.</S>
			<S sid ="152" ssid = "103">:3: 1 errors/10 verbs).</S>
			<S sid ="153" ssid = "104">The only verb offrequcncy 0 is correctly classified, while the only one with lop; frequency 11 is not . In sum, we do not find th&lt; .t more frequent classes or verbs arc more accurately classified.</S>
			<S sid ="154" ssid = "105">Importantly, the experiments also enable us to sec whether the features indeed contribute to dis­ criminating the classes in the manner predicted in Section :3.</S>
			<S sid ="155" ssid = "106">The single holdout results allow us to do this, by comparing the individual class labels assigned using the full set of five features (THANS, \Ve performed experiments on the full set of fea­ tures, as well as each subset of features with a sin­ PASS, VHN 1 CAUS, AN!lvl) to the class labels as­ gle feature removed, as reported in Table 11.</S>
			<S sid ="156" ssid = "107">Con­ sider the first column in the table.</S>
			<S sid ="157" ssid = "108">The first line shows that the overall accuracy for all five features is G9.5%, a reduction in the error rate of more th&lt;w 50% above the baseline.</S>
			<S sid ="158" ssid = "109">The removal of&apos; the PASS feature appears to improve performance (rovv ;) of Ta.ble !J).</S>
			<S sid ="159" ssid = "110">However, it should he noted that this increase in performance results fro Ill a. single additional verb being classified correctly.</S>
			<S sid ="160" ssid = "111">The re­ maining rows show that no feature is superflous or harmful as the removal of any feature has a 5 0&apos;1(, negative effect on pcrforma.nu . Co1npa.ra.hle a.ccunt ics have been demonstrated using a. lllOrc thorough cross-validation methodology and using mcLhods tha.t a.rc, in principle, better at ta.king a.d vantage of correlated features (Stevenson and Merlo, 1999; Stevenson eta.!., 1999).</S>
			<S sid ="161" ssid = "112">The single holdout protocol provides new data for analysing the performance on individual verbs and classes.</S>
			<S sid ="162" ssid = "113">The class-by-class accuracies arc shown in the rema.ining columns of Table 1L Vvc ca.n sec clea,rly that, using all five features, the unergativcs arc classified with much greater ac­ curacy (85%) than the u naccusativcs a.ncl object­ drop verbs (G3.2% and GO.O% respectively), as shown in the first row.</S>
			<S sid ="163" ssid = "114">The remaining rows show that this pattern generally holds for the su bscts of f&apos;ea.tures as well, with the exception of line 1.</S>
			<S sid ="164" ssid = "115">\Vhile future work on our verb classification task will need to focus on determining features that better discriminate unaccusative a.nd object­ drop verbs, we can alrca.cly exclude an explana­ tion of the results based simply on the verbs&apos; or signed using each siY-c four subset offeatures.</S>
			<S sid ="165" ssid = "116">This comparison indicates the changes in class labels that we can attribute to the added fea.ture in going from a size four subset to the full set of features.</S>
			<S sid ="166" ssid = "117">(The individual class labels supporting our analy­ sis below a.rc a.va.ilablc from the a.u thors.)</S>
			<S sid ="167" ssid = "118">\Vc cou­ ccnLratc on the three main features: CAUS, ANIM, TRANS.</S>
			<S sid ="168" ssid = "119">\&apos;Vc find Lha.t the behaviour of these fea­ tures generally docs conform to our predictions.</S>
			<S sid ="169" ssid = "120">\Vc expected that THANS wmtlclhelp make a. three­ way distinction among the verb classes.</S>
			<S sid ="170" ssid = "121">\Vhile unergaLivcs arc already accurately classified with­ out cj&apos;HANS, inspection of the change in class la­ bels reveals that the addition of THANS to the set improves performance on unaccusativcs by help­ ing to distinguish them from object-drops.</S>
			<S sid ="171" ssid = "122">lJow­ ever, in this case, we also observe a. loss in pre­ cision of uncrga.tives, since some object-drops arc now classified a.s uncrga.tives.</S>
			<S sid ="172" ssid = "123">Moreover, we ex­ pected CAUS a.nd ANIM to be particularly helpful in identifying unaccuscttivcs, and this is also borne out in our analysis of individual la.bds.</S>
			<S sid ="173" ssid = "124">We note that the increased accuracy from CAUS is primar­ ily due to better distinguishing uncrga.tivcs from u.naccusativcs, and the increased accuracy from ANIM is primarily d uc to better distinguishing u n­ a.ccusativcs from object-drops.</S>
			<S sid ="174" ssid = "125">Vvc conclude tha.t the features we have devised arc successful in clas­ sifing optionally transitive verbs because they cap­ ture -predicted differences in underlying argument structure.</S>
			<S sid ="175" ssid = "126">1 1Mat ten; arc more complex with the other two features and we arc still interpreting the results.</S>
			<S sid ="176" ssid = "127">Our prcdidion Table 5: Pairwise Agreement (Calculated by the Kappa Statistics) of Three Experts (E1, E2, E3) Com pared to a Gold Standard (Levin) and to the Classifier (Prog).</S>
			<S sid ="177" ssid = "128">Numbers in parentheses arc per­ centage of verbs on which judges agree.</S>
			<S sid ="178" ssid = "129">mal pa.irs&quot; with respect to argument structure.</S>
			<S sid ="179" ssid = "130">By classifying verbs that show the same subcatego­ rizations into different classes, we a.re able to dim­ in ate one of the confounds in classification work created by the fact that subcategorization and ar­ gument structure are largely co-variant.</S>
			<S sid ="180" ssid = "131">\Tile can n===========o=;=o=========== ===n infer that the accuracy in our classification is clue PROG - .36 (59) E2 .50 (68) .59 (75) E3 .49 (66) .53 (70) .66 (77) LEVIN .54 (69.5) .56 (71) .80 (86.5) .74 (83) to argument structure information, as subcatego­ rization is the same for all verbs, confirming that the content of thematic roles is crucial for clas­ sification.</S>
			<S sid ="181" ssid = "132">Secondly, our results further support ============== ======== =========&quot;&quot; the assumption that thematic diiierences such as 6 Comparison to Experts.</S>
			<S sid ="182" ssid = "133">In order to evaluate the performance of the al­ gorithm in practice, we need to compare it to the accuracy of classification performed by an expert, which gives a realistic upper bound for the task.</S>
			<S sid ="183" ssid = "134">In (Merlo and Stevenson, 2000) we report the re­ sults of an experiment that measures experts per­ formance and agreement on a classification task very similar to the program we have described here.</S>
			<S sid ="184" ssid = "135">The results summarised in Table 5 illus­ trate the performance of the program.</S>
			<S sid ="185" ssid = "136">On the one hand, the algorithm does not perform at ex­ pert level, as indicated by the fact tha.t, for all ex­ perts, the lowest agreement score is with the pro­ gram.</S>
			<S sid ="186" ssid = "137">On the other hand, the accuracy a.chicved by the program of 69.5% is only 1.5% less than one of the human experts in comparison to the gold standard.</S>
			<S sid ="187" ssid = "138">ln fact, if we take the best per­ formance achieved by an expert in this task 86.5%-as the maximum achievable accuracy in classification, our algorithm then reduces the er­ ror rate over chance by approximately G8%, a very respectable result.</S>
			<S sid ="188" ssid = "139">7 Discussion.</S>
			<S sid ="189" ssid = "140">The work here contributes both to general and technical issues in automatic lexical acquisition.</S>
			<S sid ="190" ssid = "141">Firstly, our results confirm the primary role of argument structure in verb classification.</S>
			<S sid ="191" ssid = "142">Our ex­ perimental focus is particularly clear in this re­ gard because we deal with verbs that are &quot;mini was that VI3N and PASS would behave similarly to TllANS.</S>
			<S sid ="192" ssid = "143">In fact, PASS is at best unhelpful in classification.</S>
			<S sid ="193" ssid = "144">VBN does appear to make the expected three-way distinction.</S>
			<S sid ="194" ssid = "145">The change in class labels shows that the improvement in performance with VBN results from better distinguishing unergatives from object-drops, and object-dmps from un­ accusatives.</S>
			<S sid ="195" ssid = "146">The latter is surprising, since analysis of the data found that the VBN feature values are statistically in­ distinct for the object-drop and unaccusative classes as a whole.</S>
			<S sid ="196" ssid = "147">these are apparent not only in differences in sub­ categorization frames, but also in differences in their frequencies.</S>
			<S sid ="197" ssid = "148">\Tile thus join the many recent results that all seem to converge in supporting the view that the relation between lexical syntax and semantics can be usefully exploited (Aone and McKee, 1996; Dorr, 1997; Dorr and Jones, 1996; Lapata and Brew, 1999; Schulte im Walde, 1998; Siegel, 1998), especially in a statistical framework.</S>
			<S sid ="198" ssid = "149">Finally, we observe that this information is de­ tectable in a corpus and can be learned automat­ ically.</S>
			<S sid ="199" ssid = "150">Thus we view corpora, especially if an­ notated with currently available tools, as useful repositories of implicit grammars.</S>
			<S sid ="200" ssid = "151">Technically, our approach extends existing corpus-based learning techniques to a more com­ plex learning problem, in scvera.l dimensions.</S>
			<S sid ="201" ssid = "152">Our statistical approach, which docs not require ex­ plicit negative examples, extends approaches that encode Levin&apos;s alternations directly, as symbolic properties of a verb (Dorr et al., 1995; Dorr and Jones, 1996; Dorr, 1997).</S>
			<S sid ="202" ssid = "153">We also extend work using surface indicators to a.pproximate underly­ ing properties.</S>
			<S sid ="203" ssid = "154">(Oishi and Matsumoto, 1997) usc case marking particles to approximate grammat­ ical functions, such as subject and object.</S>
			<S sid ="204" ssid = "155">We improve on this approach by learning argument structure properties, which, unlike grammatical functions, are not marked morphologically.</S>
			<S sid ="205" ssid = "156">Oth­ ers have tackled the problem of lexical semantic classification, as we have, but using only subcate­ gorization frequencies as input data (Lapata and I3rew, 1999; Schulte im Walde, 1998).</S>
			<S sid ="206" ssid = "157">I3y con­ trast, we explicitly address the definition of fea­ tures that can tap directly into thematic role dif­ ferences that are not reflected in &quot;subcategoriza­ tion distinctions.</S>
			<S sid ="207" ssid = "158">Finally, when learning of the­ matic role assignment has been the explicit goal, the text has been semantically annotated (Web­ ster and Marcus, 1989), or external semantic re sources have been consulted (1\one and McKee, 1996).</S>
			<S sid ="208" ssid = "159">\Ve extend these results by showing that thematic information can be inc! uced from corpus counts.</S>
			<S sid ="209" ssid = "160">The experimenta.l results show that our rncthod is powerful, and suited to the classification of lex­ ical items.</S>
			<S sid ="210" ssid = "161">However, we have not yet addressed the problem of verbs that can have tnultiple clas­ sifications.</S>
			<S sid ="211" ssid = "162">\tVe think that many cases of am­ biguous classification of verb types can be ad­ dressed with the notion of intersedive sets in­ troduced by (Dang ct a!., 1998).</S>
			<S sid ="212" ssid = "163">This is a.n im­ portant concept that proposes that &quot;regular&quot; a.m­ biguity in classification--i.e., sets of verbs that ha.ve the same multi-way classifications according to (Levin, J gg;n--can be captured with a. finer­ gra.i ned notion of l&lt; xical semantic class&lt; s. l x­ tencling our work to exploit this idea requires only to define the classes appropriately; the ba­ sic approach will remain the c;amc.</S>
			<S sid ="213" ssid = "164">\Vhen we tmn to consider ambiguity, we must also address the problem that individua.l instances of verbs may come from different classes.</S>
			<S sid ="214" ssid = "165">In future research we plan to extend our method to the classification of ambiguous tokens, by experimenting with a. func­ tion that combines severa.l sources of information: a bias for tire verb type (using tire cross-corpus static;tics we collect), as well a.c; featn n c; of the us­ age of&apos; the inc;ta.nce being classified (cf.</S>
			<S sid ="215" ssid = "166">(Lapata.</S>
			<S sid ="216" ssid = "167">and Brew, 1999; Si&lt;&apos;gel, 199g)).</S>
	</SECTION>
</PAPER>
