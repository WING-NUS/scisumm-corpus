Autotnatic Lexical Acquisition  Based on  Statistical Distributions*



Suzanne Stevenson Department  
of  Computer  Science University o!' 
Toronto
6 l(ing's College  l .oacl
'J'oronto, ON Canada M5S Jl15
suzanne@cs.toronto.edu

Abstract

We  automatically  classify   verbs   into   lexical   se­ 
rnantic classes,  based   on  distributions of  indie<t­ 
tors  or  verb   alternations, extracted from  a  very 
large   a.nnota.ted   corpus.   \Ve address a  problem 
which   is  particularly  difficult    because  the   verb 
classes, although semantically difl'err nt, ::;how sim­ 
ilar  surface syntactic behavior. Five  p;rantmatica.l 
fea.ttJres arc suflicient to  reduce  error rate  by more 
th<w 50% over dance: we achieve almost 70% 
accuracy in  a  task  whose  baseline   performance  is
:11%, and  whos<expert-based upptr  bound  we cal­
culated a.t 8G.51/L. vVe conclude that corpu::;-driven 
extraction of grannua.tical f"ea.turcs is a  promi::;ing
methodology for  fin< -grained  verb  classification.

1   Introduction

Deta.ilcd   information about  verbs  is critical to  a 
broad   range  of  NLI>  and   IR  tasks, yet  its  Jnau­ 
ual  determination  for  large   numbers of  verbs   is 
difficult   and   resourc<'  intensive.  H.mearch  011   tit<' 
automatic  acquisition  of'  verb-bas< d   krtowledg<' 
has  succeded  in  p;leaning  syntactic prop<'rti<s of 
v< rbs such   as  subcategorization  frames from  on­ 
line  resources  (lh<mt, 199:3;  Briscoe  and  Carroll,
1997; Dorr,  1997;   Manning,   1993).    Recently,
researchers have investigated statistical corpus­ 
based   methods for  lexical  semantic  classification 
from  syntactic properties of verb  usage  (;\one and 
Ivlcl(cc,  199G; La.pata aud  Brew,  1999; Schulte i111
Walde, 199H; Stevenson and  Ivlerlo,  1999; Steven­
son  ct  al.,  1999;  McCarthy, 2000).
  Corpus-based approaches to lexical  semantic 
classification in particular have drawn  on Levin's 
hypothesis (Levin,  1993)  that verbs  can  be classi­ 
fied according to  the  diathesis a.lternations (alter­ 
nations in !.he syntactic expressions or arguments) 
in which  they  participate--for example, whether a

•  'fhis  research was  partly  sponsored  hy  US  N ·W p;rauts
#0702:1:{1   aml   # li5J8:J22,    Swiss   NSF   fellowship    8210-
16.5()0,  Information Sciences Cmmcil  of Hutgers University 
and   !HCS,  U _         of  Pennsylvania.   This  research  was  cml­ 
clncted  while  the  first  author was at  Hutgers  University.


Paola Merlo
LATL -  Department of Linguistics
University of Geneva.
2 rue de Caudollc
1211 Gcncvc  tl  -    Suisse
merlo©lettres.unige.ch

verb  occurs in the  dative/prepositional  phrase al­ 
ternation in  English. One  diagnostic for diathesis 
alternations is  the  subcategoriza.tion a.lternatives 
of a. verb.   However, some classes exhibit the  same 
subca!.egorization possibilities but difl'er in their 
argument structures, i.e.   the  content of  the  the­ 
tnatic roles assigned to the  arguments of the  verb. 
This type  of situation constitutes a particularly 
difficult  case  for corpus-based classiltcation meth­ 
ods.
  Jn this  paper, we apply corpus-based lexica.! 
acquisition tucthodology to distinguish  classes of 
verbs   which   allow   the   sante su bcatcgorizations, 
but  difrer  in thematic roles.  \Ve first  assume that 
one can  a.tttontatically restrict the  choice of classes 
t.o those that  participate in  the  relevant su bcai.c­ 
gorizations (d. (La.pata.  and   Brew,   ID99)).  Our 
proposal is then  to  use statistics over diathesis 
alternants as  a. way  to  f'urthcr   distinguish those 
verbs   w h iclt  allow   tlw  sante su bca.t< goriza.tions, 
achi<'vinp;  fin< -gra.incd  classification  within    that 
S<'t. Our  work  focuses 011  dekrtnining the  bestS< ­ 
llla.ntic class  for  a  verb  lypr-- the  set  of  usages  of a  
verb  across a document or  corpus--rather  than
for  a  single   verb   lokr:n. in  a  single  loca.l context.
In this  way, we can  exploit the  broad   behavior  of 
the  verb  across the  corpus to  determine its  most 
likely  class  overall.
   VVe investigate the  proposed approach in an  in­ 
depth case  study of the  three major classes of op­ 
tionally intransitive  verbs in  English:  utwrgat.ive, 
una.ccusativc, and object-drop. Nlore specifically, 
according to  Levin's classification  ( lA vin,  1993), 
the  unergatives arc  manner of motion verbs, such 
as .iump  and   march;  the   unaccusatives arc  verbs 
of change of state, such  as  open  and  explode;  the 
object-drop verbs  are  unexpressed object a.lterna­ 
tion   verbs,   such   as   played  and    painlcd.    These 
classes all  support  both   transitive and   intransi­ 
tive  subcategoriza.tions,  but  arc  distinguished   by 
the  pattern of thematic role  assignments to  sub­ 
jed and  object  position.  \Ve au tom atica.lly  clas­ 
sify   these  verbs   on   the   basis   of  statistical  ap-


proximations to syntactic indicators of the  under­ 
lying  argument structures,  using  numerical   fea­ 
tures  collected   from  a  large  syntactically  anno­ 
tated  (tagged  or  parsed)  corpus.    We apply  ma­ 
chine  learning   techniques  to  determine  whether 
the frequency  distributions of the features,  in­ 
dividually   or  in  combination, support  automatic 
classification  of  the   verbs.    To  preview   our  re­ 
sults,  we demonstrate that  combining only fivc 
numerical  indicators is sufficient  to reduce  the er­ 
ror  rate  in  this  classification   task   by  more  than
50% over chance.   Specifically,  we achieve  almost
70% accuracy in a task whose baseline  (chance) 
performance is 31%, and  whose expert-based  up­ 
per  bound  is calculated  at  86.5%.   We conclude 
that a distribution-based method  for lexical se­ 
mantic  verb classification  is a promising  avenue of 
research.

2    The Argument Structures

Our approach rests on the hypothesis that, even in 
cases  where  verb  classes  cannot   be distinguished 
by subcategorizations, the frequency  distributions 
of syntactic indicators can hold clues to the under­ 
lying thematic role differences.  We start here then 
with a description of the subcatcgorizations and 
thematic role  assignments for  each  of  the  three 
verb classes  under  investigation.
  As    optionally   intransitive   verbs,     each    of 
the    th rce   classes    participates   in   the    transi­ 
tive/intransitive alternation:
U ner?;ativc
(la) The  horse  raced  past  the  barn.
(lb)  The  jockey raced  the  horse  past  the  barn. 
U naccusative
(2a)  The  butter melted  in the  pan.
(2b)  The  cook  melted  the  butter in the  pan. 
Object-drop
(3a) The  boy washed  the  hall. 
(3b)  The  boy washed.
U nergativcs are intransitive action  verbs, as in (l),
whose  transitive form  can  be the  causative coun­ 
terpart of the  intransitive form.   In  the  causative 
use,  the  semantic argument that  appears as  the 
subject of  the  intransitive,  as  in  (la),  surfaces 
as  the  object   of  the  transitive, as  in  (lb)  (Hale 
and  Keyser,  1993).  Unaccusatives are intransitive 
change  of state verbs,  as  in  (2a);  the  transitive 
counterpart for these  verbs exhibits the causative 
alternation, as in  (2b).   Object-drop verbs,  as  in 
(3),   have  a  non-causative transitive/intransitive 
alternation, in which the object  is simply  optionaL









Table  1: Summary of Thematic Alternations. 
Each class is distinguished by the content of the
thematic roles  assigned  by the  verb.   For object­ 
drop  verbs,  the  subject is an  Agent  and  the  op­ 
tional  object is a Theme, yielding  the thematic 
assignments (Agent,  Theme) and  (Agent)  for  the 
transitive and  intransitive alternants respectively. 
Unergatives and  unaccusatives differ from object­ 
drop  verbs  in participating in the causative alter­ 
nation, and also differ from each other  in their core 
thematic  argument.   In  an  intransitive  uncrga­ 
tivc,  the  subject is an  Agent,   and  in  an  intran­ 
sitive  unaccusativc,  the  subject is a  Theme.   In 
the  causative transitive form of each,  this core se­ 
mantic  argument is expressed  as the direct  object, 
with  the addition of a Causa.! Agent  (the causer of 
the action)  as subject in both  cases.  The  thematic 
roles assigned,  and  their  mapping to syntactic  po­ 
sition,  arc summarized in Table  l.

3   The Features for  Classification
The  key to any  automatic classification   task  is to 
determine a set  of useful features for discriminat­ 
ing the items  to  be classified.  ln what  follows, we 
refer to the columns  of Table  1 to explain  how we 
expect  the thematic distinctions to yield distri­ 
butional   features whose  frequencies discriminate 
among  the  classes  at  hand.
  Considering column  one  of Table  1,  only 
unergative and  unaccusa.tive  verbs assign a Causal 
Agent  to the subject of the  transitive.  \1\Tc  hy­ 
pothesize  that  the causative construction is lin­ 
guistically   more  complex   than   the  simple  argu­ 
ment  optionality of object-drop verbs  (Stevenson 
and  Merlo,  1997).   We expect then  that   object­ 
drop   verbs  will  be  more  frequent  in  the  tr<wsi­ 
tivc  than  the  other  two classes.  Furthermore, the 
object   of an  unergative verb  receives  the  Agent 
role (see the second  column  of Table 1), a linguis­ 
tically  marked  transitive construction (Stevenson 
and  Merlo,  1997).   We therefore expect   uncrga­ 
tives  to  be quite  rare  in the  transitive, leading  to 
a three-way distinction in transitive usage among 
the  three  classes.
Second,   due   to   the   causative  alternation  of



'fable 2: 'J'he 1•'catures and  Their  Expected   Behavior

Transitivity 	Unaccwmtivcs and  unergativcs have  a causative transitive, hence  lower  transitive usc.   Fm- 
thennore, unergat.ive:c; have  all  agent.ive  object, hence  very  low  transitive usc.
Passive Voice 	Passive implies transitive  usc,  hence  corrclatccl with  transitive feature.
VBN  Tag 	Passive implies past   participle usc  (VBN), hence  correlated with  transitive (and   passive). 
Cansativity 	Object-drop verbs  do  not  have  a causal age.nt,  hence  low  "causative" use.   Unergat.ives arc
rare  in  the  transitive, hence  low causative  usc.
An'i_n_1_ac-·y--- U n-a_c_c_u_sati vcs have  a Them c su b_f-cc-c·t-to-.n--c-.!.1-lc-o,-i1--ct1-r-;u-1s-· i-:-to-\i-,e-.,h_e_n_c_e lo_w_e_r-u-sc--o  f_a_n..,..it_n_a-ct-c_s_u b   7je-,c-:-'L·-s-.  -H




unergativcs and  unaccusativec;,  the  thematic  role 
of  the   su{ jed   of  the  intransitive is  identical   to 
that of the  object  of the  transitive, as shown  in 
columns   two  and   three   of  'fable I. 	C:ivcn  the 
identity of thematic role  mapped   to subject  and 
object positions,   we expect   to  observe  the  same 
noun  occurring a.t times  as  subject of  the  verb, 
and  a.t other  times  as object of the  vcr b.  In con­ 
trast, for  object-drop verbs,  the  thematic role of 
the  ;ubject o[ the  intransitive is identical  to that 
of the  subject of the  transitive, not  the  object  of 
the  transitive. 'J'hus,  we expect  that it will he less 
common  for the same  noun to occur in subject and 
object position  of the same  object-drop verb.  We 
hypothesize that this  pattern of thematic role as­ 
signments will be reflected  in diff'erential  amount 
of w;ae  across  the  classes  of the  same  nouns  as 
subjects and  objects for  a.   iwn verb.    1<'11 rther­ 
mon,  since  the  causative is a. transitive use, and 
the  transitive use of uner a.tives  is ( xpec:Led  to be 
rare,  this  overlap  of subjects and  objects should 
prim.arily distinguish  una.ccusatives  (predicted to 
have  high  overlap  of subjects and  objects)  from 
the  other  two classes.
Finally,  considering columns  one  and  three  of
Table  1, we note  that unerga.tivc and  object-drop 
verbs  assign  an  agentive role  to  their  subject in 
botl1 the transitive and intransitive, while unac­ 
cusatives assign  an  agcntive  role to  their  subject 
only in the  transitive. Under  the assumption that 
the  intransitive usc of unaccusativcs is not  rare, 1 
we then  expect  that unaccusativcs will occur  less 
often  overall  with  an  agentivc subject  than   the 
other   two  verb  classes.   On  the  further  assump­ 
tion  that Agents  tend  to be animate entities more 
so  than   Themes,  we  expect   that  unaccusatives 
will occur  less frequently with  an animate subject 
compared  to  uncrgative and   object-drop  verbs.


be animate, but  rather that nouns  that  receive an 
Agent  role will more often  be animate than  nouns 
that   receive a Theme role.
  The  above  interactions between  thematic  roles 
and  the  syntactic expressions of arguments thus 
lead to three features whose distributional  proper­ 
tics appear promising  for distinguishing the verb 
classes:  transitivity, causativity, and  animacy of 
subject. V•.,Te also investigate two additional syn­ 
tactic features, the  passive voice and the  past  par­ 
ticiple  POS  tag  (V13N). These  features are  related 
to the transitive/intransitive alternation, since a 
passive  usc  implies  a  transitive  usc of  the  verb, 
and  the  usc of passive  in  turn  implies  the  usc of 
the  past  participle.  Our  hypothesis is that these 
five features will exhibit  clistribu tiona! differences 
in the  observed  usages of the  verbs,  which can  be 
used for classification. The  features and  their  ex­ 
pected  relevance  arc summarized in Table  2.

1 Data Collection and Analysis
\Vc chose  a  set  of  20  verbs  from  each  of' three 
classes.  The  complete list  of verbs  is reported in 
Appendix A. Recall  that  our  goal  is to achieve  a 
fine-gra.incd classification of verbs that  exhibit  the 
same  su bcategori:;,ation   frames;   thus,   the   verbs 
were chosen because they do not generally  show 
massive  departures from  the  intended  verb  sense 
(and  usage)  in  the  corpus.2    In order  to  simplify 
the counting procedure, we included  only the  reg­ 
ular  ("-eel")  simple   past/ past  participle form  of 
the  verb,  assuming that  this  would  approximate 
the  distribution of the  features across  all forms of 
the  verb.    FinaJly,  as  far  as  we were  able  given 
the  preceding  constraints, we selected  verbs  that 
could  occur  in the  transitive and  in the  passive.
  We counted  the  occurrences of each  verb  token 
in  a  transitive or  intransitive usc  (TitANs), in  a


Note  the importance  of  our  usc  of  frequency  dis­	 	


tributions: the  claim  is not  that only  Agents  can

  1 This  assumption is  based  on  t.hc linguistic complexi Ly 
of the  causative, and  borne ont  in  our  corpus analysis.
  

2 Though note that  there arc  only  19 nnaccusatives  be­ 
cause  ripped   was excluded from  the  analysis as it  occurred 
mostly   in  a  very  different  nse  ( 1'ippcd  off) in  Lhe corpus 
from  the  intended change of state usage.





passive  or  active use  (PAss), in  a  past   participle 
or  simple   past   usc  (vHN), in  a  causative or  non­ 
causative use (cAus), and  with  an animate subject 
or  not  (ANIM), as described below.  The  first  three 
counts  (TRANS,   PASS,  V13N)   were   performed  on 
the  LDC's G5-million  wore! tagged ACL/DCI cor­ 
pus  (l3rown,  and  Wall  Street Journal 1987-1989).
The  last  two  counts (cAUS  and   ANIM)  were  per­ 
formed on  a  29-million word  parsed corpus (Wall 
Street Journal 1988,  provided  by Michael   Collins 
(Collins, 1997)).  The  features were counted as 
follows:
  TRANS: The  closest noun  following a verb was 
considered  a  potential object.   A  verb   immedi­ 
ately  followed   by  a  potential object was  counted 
as  transitive, otherwise as  intransitive.
   PASS:  A token   tagged VBD  (the tag  for  simple 
past)  was counted a.s active. A token  tagged VBN 
(the  tag  for  past  participle) was counted as active 
if the  closest  preceding auxiliary was have, and  as 
passive  if the  closest preceding auxiliary was  be.
  VBN:  The  counts for  VDN/VBD were  based  on 
the  POS  label  in  the  tagged corpus.
  Each  of  the  above counts was  normalized  over 
a.ll occurrences of the  "-cd"  form  of the  verb,  yield­ 
ing  a  single   relative frequency  measure for  each 
verb  for  that feature.
   CAUS:  For each  verb  token, the  subject and  ob­ 
ject  (if there was  one)   were  extracted from   the 
parsed  corpus, and  the  proportion of overlap  be­ 
tween  subject and  object  nouns across all  tokens 
of a verb  was calculated.
  ANIM:  To  approximate  animacy without refer­ 
ence   to  a  resource external  to  the  corpus  (such 
as  VVordNct), we count pronouns (other than  it) 
in subject  position (cf'. (Aonc and  McKee,  1996)). 
The  assumption is that the  words I, we,  you,  she, 
he,  and  they  most  often   refer  to  animate entities.
VVc automatically extracted all subject/verb tu­ 
ples,   and   computed  the   ratio of  occurrences   of 
pronoun subjects to  all subjects for  each  verb.
  The  aggregate means by class  resulting from  the 
counts above arc shown in Table 3.  The  distri­ 
butions of each  feature are  indeed   roughly as ex­ 
pected   according to  the  description in  Section 3. 
Uncrgatives show   a  very  low  relative  frequency 
of the  TRANS feature, followed   by  unaccusatives, 
then  object-drop verbs. U naccusative verbs  show 
a high frequency of the  CAUS feature and  a low 
frequency  of  the   ANIM  feature  compared  to  the 
other classes.  Although expected  to  be  a  redun­ 
dant  indicator of  transitivity,  PASS   and   VBN   do



'fable 3:  Aggregated Relative Frequency Data for 
the  Five  Features.  E  =  unergatives, A  = unac­ 
cusatives, 0 = object-drops.

Cl
as
s
N
MEAN  
RELATIVE  
FREQUENCY


TR
AN
S
PA
SS
VB
N
CA
US
AN
IM
E
A
0
20
19
20
0
.
2
3
0
.
4
0
0
.
6
2
0.
0
7
0.
3
3
0.
3
1
0.
21
0.
65
0.
65
0.
0
0
0.
1
2
0.
0
4
0.
2
5
0.
0
7
0.
1
5


not  distinguish  between unaccusative  and  object­ 
drop verbs, indicating that their distributions are 
sensitive to  factors we have  not  yet  investigated.:1

5    Experiments in  Classification
The  frequency distributions  of our  features  yield 
a vector for each  verb  that represents the  relative 
frequency values  for  the  verb on  each  dimension:
[verb, TRANS, PASS,   VBN,   CAUS,  ANIM, class)
Example: [opened,  .69,  .09,  .21,  .16,  .36,  unacc]
\t\Tc  use  the   resulting 59  vectors to  train an  au­ 
tomatic classifier   to  determine, given  a verb  that 
exhibits transitive/intransitive subcatcgori?:ation 
frames, which  of the  three major lexical  semantic 
classes of English  optionally intransitive verbs it 
belongs to.  Note that the  baseline (chance) per­ 
formance in  this  task  is 33.9%, since  there arc  .59 
vectors and  3 possible  classes, with  the  most  com­
mon  class  having 20 verbs.
   \t\Tc   used   the   C5.0   machine  learning  system 
(http:/ jwww.rulequcst.com), a  newer   version   of 
C4.5   (Quinlan,  ]992),  which   generates  decision 
trees and  corresponding rule  sets  from  a. training 
set  of known  classifications. \Nc found  little to  no 
difference in  performance  between  the  trees and 
rule  sets, and  report only  the  rule  set  results. \1\Te 
report  here  on  experimcn ts  using   a  single   hold­ 
out  training and  testing methodology. In  this  ap­ 
proach,  we  hold  out   a  single verb   vector as  the 
test  case,  and  train the  system on  the  remaining
58 cases.  We  then   test   the   resulting classifier  on 
the  single   hold-out case,   recording the   assigned 
class  for  that verb.  This is then repeated for each 
of  the  59  verbs.  This  technique  has  the   benefit 
of  yielding both  an  overall accuracy rate  (when 
the   results are  averaged across all  59  trials),  as 
well as  providing the  data necessary for determin­ 
ing  accuracy for each  verb  class  (because \Ve have 
the  classification of each  verb when  it  is the  test 
case). This allows  us to evaluate the  contribution

3 These observations have  been  confirmed by t-tests be­
tween  feature values  for  each  pair  of classes.






Table   1]:       Percent    Accuracy   of    Verb    Clas­ 
sification   Task     Using     Featnres   in    Combina­ 
tion.   T=THANS;  P=PAss; V=VBN;  C=cAus; 
An=ANIM.     E=unergatives, A=unaccusatives, 
O==object-drops



Percent.  
Accuracy  hy 
Class

Fe
at
ur
es
All 	E
A	()
1..  'J'  
P V 
C An
69
.5
85
.0
63
.2
6
0.
0
2.  
PVC 
An
:3.  
TV 
CAn
1. T 
l' 
CAn
5.  T 
P 
VAn
6.  T 
PVC
61
.1
71
.2
G
l.
O 
G
2.
7
61
.0
80
.0
80.
0
65
.0
70.
0
80
.0
17
.1
73
.7
G8
.1
G3
.2
12
.1
6
5.
0
G
O.
O
5
0.
0
5
5.
0
6
0.
0


of individual features with   respect to  tlwir  effect 
on  the  perfonnance of individua.l  classes.


the  classes' frequency.  U nergatives have  the  low­ 
est  average (log)  frequency (1.3), but  arc  the  best 
classified,  while   unaccusatives  and   object-drops 
arc  comparable (average log frequency= 2).  If we 
group verbs  by frequency, the  proportion of errors 
to  the  total number of verbs  remains fairly  simi­ 
lar  (frcq  1:  7 crrors/2verbs; frcq.  2:  6 errors/24 
verbs;  frcq.  :3:  1 errors/10 verbs). The  only  verb 
offrequcncy 0 is correctly classified, while  the only 
one  with  lop; frequency 11  is  not  .  In  sum, we do 
not  find  th< .t  more  frequent classes   or  verbs arc 
more  accurately classified.
  Importantly, the  experiments also  enable us to 
sec  whether the  features indeed  contribute to dis­ 
criminating the  classes in the  manner  predicted  in 
Section :3.  The  single  hold-out results allow  us to 
do  this,   by comparing the  individual class  labels
assigned   using  the  full set  of five features (THANS,


\Ve performed experiments on the  full set  of fea­
tures, as well as each  subset of features with  a sin­


PASS, VHN 1


CAUS, AN!lvl) to  the  class   labels  as­



gle feature removed, as  reported in Table 11.  Con­ 
sider   the  first  column in  the  table.  The   first  line 
shows  that the  overall accuracy for all five features 
is  G9.5%,  a  reduction  in  the  error rate of  more 
th<w 50%  above the  baseline. The  removal   of' the 
PASS feature appears to improve performance (rovv
;) of  Ta.ble !J). However, it  should   he  noted   that
this  increase in  performance results fro Ill a. single 
additional verb  being classified  correctly. The  re­ 
maining  rows  show   that no  feature is superflous 
or  harmful as  the  removal of any  feature has  a 5
0'1(,  negative effect  on  pcrforma.nu .  Co1npa.ra.hle
a.ccunt ics  have  been  demonstrated using  a. lllOrc 
thorough cross-validation methodology and  using 
mcLhods   tha.t  a.rc,  in  principle, better  at  ta.king 
a.d vantage of  correlated  features  (Stevenson and 
Merlo,  1999;  Stevenson eta.!., 1999).
  The  single  hold-out protocol provides new data 
for  analysing the  performance on  individual verbs 
and   classes.    The   class-by-class accuracies arc 
shown in  the  rema.ining columns of  Table 1L  Vvc 
ca.n  sec  clea,rly  that, using   all  five  features, the
unergativcs arc classified with  much  greater ac­ 
curacy (85%)  than the  u naccusativcs a.ncl object­ 
drop  verbs   (G3.2%  and   GO.O%  respectively),  as 
shown in  the  first  row.  The  remaining rows show 
that this   pattern  generally holds  for  the  su bscts 
of f'ea.tures as  well,  with  the  exception of line 1.
  \Vhile   future  work   on   our   verb   classification 
task   will  need   to  focus  on  determining features 
that  better discriminate unaccusative a.nd object­ 
drop  verbs, we  can   alrca.cly exclude an  explana­ 
tion  of  the  results based  simply on  the  verbs' or


signed  using each  siY-c four subset offeatures. This
comparison indicates  the   changes in  class  labels 
that we can  attribute to the  added fea.ture  in going 
from  a size  four  subset to  the  full  set  of features. 
(The  individual class  labels supporting our  analy­ 
sis below a.rc a.va.ilablc from  the  a.u thors.)  \Vc cou­ 
ccnLratc  on  the  three main  features: CAUS, ANIM, 
TRANS. \'Vc find  Lha.t the   behaviour of these fea­ 
tures generally docs  conform  to  our   predictions.
\Vc expected that THANS wmtlclhelp make a. three­
way  distinction  among  the   verb   classes. 	\Vhile 
unergaLivcs  arc  already accurately classified  with­ 
out  cj'HANS,  inspection  of  the  change in  class  la­ 
bels  reveals  that the  addition of THANS to  the  set 
improves performance  on  unaccusativcs by  help­ 
ing  to  distinguish them   from  object-drops. lJow­ 
ever,   in  this  case,   we  also  observe a.  loss  in  pre­ 
cision  of uncrga.tives, since  some  object-drops arc 
now  classified a.s uncrga.tives.   Moreover,  we ex­ 
pected   CAUS a.nd  ANIM to  be  particularly helpful 
in identifying unaccuscttivcs, and  this  is also borne 
out  in our  analysis of individual  la.bds.  We note 
that the  increased accuracy from  CAUS is primar­ 
ily  due  to  better  distinguishing  uncrga.tivcs from 
u.naccusativcs,  and   the   increased  accuracy  from 
ANIM  is primarily d uc to better distinguishing u n­ 
a.ccusativcs from  object-drops.  Vvc conclude  tha.t 
the  features we have devised arc successful in clas­ 
sifing optionally transitive verbs  because they  cap­ 
ture -predicted differences in  underlying argument
structure. 1

  1Mat ten;  arc  more  complex with  the  other two features 
and   we arc  still   interpreting the  results.   Our   prcdidion






Table 5: Pair-wise  Agreement (Calculated by the 
Kappa  Statistics) of Three Experts (E1,  E2,  E3) 
Com pared  to a Gold Standard (Levin)  and  to the 
Classifier  (Prog). Numbers  in parentheses arc per­ 
centage of verbs on which  judges  agree.


mal pa.irs" with respect  to argument structure.  By 
classifying verbs that show the same subcatego­ 
rizations into different  classes,  we a.re able to dim­ 
in ate  one  of the  confounds in  classification   work 
created by the fact  that  subcategorization and  ar­ 
gument structure are  largely  co-variant.  \Tile can


n===========o=;=o===========  ===n  infer  that the  accuracy  in our  classification  is clue
 	PROG 	


-	.36 (59)
E2 	.50 (68) 	.59 (75)
E3 	.49 (66) 	.53 (70) 	.66 (77)
LEVIN	.54 (69.5) 	.56 (71) 	.80  (86.5) 	.74 (83)


to argument structure information, as subcatego­
rization   is the  same  for all verbs,  confirming  
that the  content  of thematic roles  is crucial  for  
clas­
sification.   Secondly,  our  results  further support


============== ======== ========="" the  assumption that  thematic diiierences such  as



6     Comparison to Experts
  In order  to evaluate the  performance of the  al­ 
gorithm in practice, we need  to compare it to the 
accuracy  of classification performed   by an expert, 
which gives a realistic  upper  bound  for  the  task. 
In  (Merlo  and  Stevenson, 2000)  we report  the  re­ 
sults  of an experiment that measures experts per­ 
formance and  agreement on  a  classification   task 
very  similar   to  the  program  we  have  described 
here.    The  results  summarised  in  Table  5  illus­ 
trate the  performance of  the  program.   On  the 
one  hand,  the  algorithm does  not  perform  at  ex­ 
pert  level, as indicated   by the fact  tha.t, for all ex­ 
perts,  the  lowest agreement score  is with  the  pro­ 
gram. On  the  other   hand,  the  accuracy  a.chicved 
by the  program   of 69.5%  is only  1.5%  less  than 
one  of the  human  experts in  comparison   to  the 
gold standard. ln fact,  if we take the best per­ 
formance  achieved   by  an  expert  in  this   task-
86.5%-as the  maximum   achievable  accuracy   in 
classification, our  algorithm then  reduces  the  er­ 
ror rate over chance  by approximately G8%, a very 
respectable result.

7    Discussion
The work here contributes both to general and 
technical  issues in automatic lexical  acquisition.
  Firstly,  our  results  confirm  the  primary  role of 
argument structure in verb classification. Our ex­ 
perimental focus  is  particularly clear  in  this  re­ 
gard  because  we deal  with  verbs  that are  "mini-

was  that VI3N and  PASS  would  behave similarly to  TllANS. 
In  fact,  PASS   is  at best   unhelpful  in  classification.    VBN 
does  appear to  make  the  expected  three-way distinction. 
The change in  class  labels shows  that the  improvement in 
performance with  VBN  results from  better distinguishing 
unergatives from  object-drops, and  object-dmps from  un­ 
accusatives.  The  latter is surprising, since  analysis of the 
data found that the  VBN  feature values  are  statistically  in­ 
distinct for  the  object-drop and   unaccusative classes  as  a 
whole.


these are apparent not only in differences in sub­ 
categorization  frames,   but   also  in  differences  in 
their  frequencies.  \Tile  thus  join  the  many  recent 
results   that all  seem  to  converge   in  supporting 
the  view that the  relation   between  lexical syntax 
and semantics can  be usefully exploited  (Aone and 
McKee,  1996;  Dorr,  1997;  Dorr  and  Jones,  1996; 
Lapata and  Brew,  1999; Schulte im Walde,  1998; 
Siegel, 1998),  especially  in a statistical 
framework. Finally,   we observe  that this  
information  is de­ tectable in a corpus  and  can  
be learned  automat­ ically.    Thus   we  view  
corpora,  especially   if an­ notated with currently 
available  tools, as useful repositories of implicit  
grammars.

  Technically,  our  approach extends existing 
corpus-based  learning  techniques to  a more com­ 
plex learning  problem,  in scvera.l dimensions. Our 
statistical  approach, which  docs  not  require  ex­ 
plicit  negative examples, extends approaches that 
encode  Levin's  alternations directly,   as symbolic 
properties of a verb  (Dorr  et  al.,  1995;  Dorr  and 
Jones,   1996;  Dorr,  1997).    We also  extend   work 
using  surface indicators to  a.pproximate underly­ 
ing  properties.  (Oishi  and  Matsumoto, 1997)  usc 
case  marking particles to  approximate grammat­ 
ical functions, such as subject and object.   We 
improve on this approach  by learning  argument 
structure  properties, which,  unlike grammatical 
functions, are  not  marked  morphologically.  Oth­ 
ers  have  tackled   the  problem  of lexical  semantic 
classif-ication,  as we have,  but  using only subcate­ 
gorization frequencies as input data (Lapata and 
I3rew,  1999;  Schulte  im  Walde,   1998).    I3y con­ 
trast, we explicitly   address the  definition  of fea­ 
tures  that can  tap  directly into  thematic role dif­ 
ferences  that are  not  reflected  in "subcategoriza­ 
tion  distinctions.  Finally,   when  learning   of the­ 
matic  role assignment has  been  the  explicit  goal, 
the  text   has  been semantically annotated  (Web­ 
ster  and  Marcus, 1989),  or  external semantic re-



sources have   been  consulted  (1\one  and   McKee,
1996). \Ve extend these results by showing that 
thematic information can  be inc! uced  from  corpus 
counts.
  The  experimenta.l results show  that our  rncthod 
is powerful, and  suited to  the  classification of lex­ 
ical  items.   However, we  have  not  yet  addressed 
the  problem   of verbs  that can  have  tnultiple clas­ 
sifications.    \tVe think  that  many   cases   of  am­ 
biguous classification  of  verb   types can   be  ad­ 
dressed with  the  notion of intersedive sets in­ 
troduced  by  (Dang ct  a!.,  1998).   This is a.n im­ 
portant concept that proposes that  "regular" a.m­ 
biguity  in  classification--i.e.,  sets   of  verbs   that 
ha.ve the  same multi-way classifications according 
to  (Levin,  J gg;n--can be  captured  with   a.  finer­ 
gra.i ned   notion   of  l< xical semantic class< s.    l x­ 
tencling   our   work   to   exploit  this   idea   requires
only   to  define   the  classes  appropriately;  the  ba­ 
sic approach will remain the  c;amc.  \Vhen  we tmn 
to consider ambiguity, we must  also address the 
problem that  individua.l   instances of  verbs   may 
come  from  different classes. In future  research we 
plan to extend our method to the classification of 
ambiguous tokens, by experimenting with  a. func­ 
tion  that combines severa.l sources of information: 
a bias  for  tire  verb   type   (using   tire  cross-corpus 
static;tics we collect), as well a.c;  featn n c; of the  us­ 
age  of' the  inc;ta.nce being  classified   (cf.   (Lapata. 
and  Brew,  1999;  Si<'gel, 199g)).

References
Chinatsu  Anne   and  Douglas    lVIcl(ee.    I ) )G.   Acquiring 
predicatc-argtuneJJt. mapping information in nmltilingnal 
texts. lu Branimir Boguraev and  .James l'ns!.cjovsky, ed­ 
itors,  Corpus l't·occssinq for  J,e1:ica/  ;\cquisit.ion, pages
191-202. MIT Press.
Michael  Brent. 19D3.  From  graHunar to lexicon:   lJnsuper­ 
vised  lcaming of lexical  syntax.   Compul.al.ional Unquis­ 
l.ics, 19(2):2'1:2G2.
Ted  Briscoe  and  .John  Carroll.  1997.  Automatic extraction 
of subcat.egori:- ation  from  corpora.  In  !'roes  of !.he Fifth
;lNl,J'  Confet·encc, pages  356-363.
M.ichad   John  Collins.  1997.  Three generative:,   kxicalised 
models  for statistical parsing. In l'mes of A CL '97,  pages 
lG-23. Madrid, Spain.
I-Ioa  Trang  Dang,  Karin  Kipper,  Martha  Palmer,  and
.Joseph   llosen:- weig.    1998.   Investigating 1·egular  sense 
extensions   based   on   intersec!.ive    Levin    classes. 	In
}Jrocs of COJ,JNG-A CJ,  '98, pages  2D:{- 299,  !Vlontreal, 
Canada.
Bonnie   Dorr  and   Doug   Jones.   199G.  Hole  of  word  sense
disambiguation ill lexical  acquisition: PredidiHg seman­
tics  from  syntacl.ic cues.   In  ]'roc. of CO Ll N G '.<Jri, pages
3:22-327,  Copenhagen, Denmark.
Bonnie    DolT,   .Joe  Garman,  and   A my   \1\Teinberg.  F)95.
From   syntactic  encodiugs to  thematic roles:    Building



lexical  entries for  iutcrlingual  MT.   Journal of Machine
'lhmslalion, 9(3):71·  JOO.
Bonnie   DolT.    19D7.   Large-scale  dict.iomu·y  construdion 
for foreign  language tutoring and  interlingual machine 
translation.  Machine Jhmslalion, 12:1-55.
Ken  Hale  and   Jay   Keyser.  J 993.   On  m·gumcnl  structure 
and   t.he lexical  represc:ntaliou of syntactic relations.  I11
I\.I!ale and  J. T\cyser,  editors,  The  View  from  /Jui/ding
20,  pages  53· 110..MIT  Press.
Maria  Lapata aud  Chris Brew.  1999.  Using  subcategori a­ 
tion   to  resolve   verb  class  ambiguity.  In  Fmcs   of  Joint 
S'IGJJAT   Conference on  Rmpi1·ical  Methods in  Natm·a1
Lanquage, College  Park,  MD.
Beth   Levin.   1993.   I1'nqlish Verb  Classes and  Allenwtions.
University of Chicago Press, Chicago, !L.
Christopher D. Manning.  199a.   Automatic acquisition of 
a  large  subcategori:- ation   dictionary  from   corpora.   In 
I' roes  of A CJ,'9.'1, pages  235 -212. Ohio  State University.
Diana  McCarthy. 2000.  Using semantic preference Lo iden­ 
tify  verb  p;uticipation in role  switching allenmtions.   In 
l'mcs of NAA CJ,-2000, Seattle,  \•Vashiugton.
Paola   Merlo   and   Su:- amw  Stevenson.  2000.-Establi,;hiug 
the upper-bound aud  inter-judge agrentent in a verb 
classi[ication task.  lu  l'rocs  of J,Rl\C-2000, pages   J(i59- 
l(jfi.·t.  Athens, Greece.
Akira  Oishi  aud  Yuji  Matsumoto.  1997.   Detecting them·­ 
gani ation of semantic subclasses of Japanese verbs.   ln­ 
lernational Journal of Corpus Linquislics, 2(1 ):65--89.
.J.  Hoss   Quinlan.   1992.    C.f.5  :   l'mym111.s  for  Machine
/,earning. Morgan ](aufmanu,  San  Mateo, CA.
Sabine Schulte im  vValcle. j 998.  Automatic  seutantic clas­ 
sification  of  verbs   according  to   their   alternation   be­ 
haviour. AlMS  Heport  4(  ), lMS,  Univcrsit.iit. St.uUgarL.
!•:ric V.  Siegd.  J D')il.    J,inyaislic  lnclicolors  for  
/,allfJUO!JC Undcrslandinq Ph.D.  tltesi  ,  Dept..  of  
Computer  Sci­ ence,  Columbia  University.
sll i\lll!C  Ste\'etJ Oll  and   Paola  Merlo.   1997.   Lexical  st.ruc­
Lllrc and  processing complexity. Languayc and  Cor;nilivc
l'rocxsscs, 12( J -2):3,19<!99.
Su:- amte  Stevenson  and   Paola   Merlo.   1999.  Verb  classifi­ 
cation   using  distrilmtions of  grammatical   features.   In 
l'mcs of K4 CJ,'.99. Bergen,  Norway.
Su atllie  Stevenson,  Paola  Merlo,   Natalia 1\ariaeva, and
1\amin  vVhitehouse.  1999.  Supervised learning of lexical 
semantic verb  classes   using  frequency  disLrilmtions.  In 
l'mcs of Siql,c:r  '.<J.9,  College  Park, Maryland.
Mort  \>\l'ebst.eJ· and  Mitch  Marcus.  1.989. Automatic ac­ 
quisition of the  lexical  >Jemantics of  verbs  fmm  sentence 
frames. In !'roes of ;\(}[,'8.9, pages  177-181,  
Vancouvet·, Canada.

Appendix A
Uncrgatives:  floated, galloped, glided, hiked, hopped, lwr­ 
riccl,  jogged,   jumped,   leaped,   marclwd, pamdcd,  meed, 
mshed, scooted, scutTied, skipped, tiptoed, t.rot.t.cd, vcmlted, 
wandered.	Unaccusatives: 	boiled,   changed,   clca1'Cd, 
collapsed,  cooled,   cracked,  dissolved,  divided,  c.7:p/odcd, 
flooded,   folded,   frac/.zll'ed, lwnlened,  mcllcd, opened, silll­ 
mct·cd,   solidified,  stabilized, widened.  Objec!.-drops:  bor­ 
rowed,   called,   carved, cleaned, dcmced,   inherited, kicked, 
knitled, 01TJaniscd, packed,   painted, played, reaped,  t·cut.ed, 
sl,ctchcd, studied, swallowed, typed, washed, yelled.




