Multi-Task Transfer Learning for Weakly-Supervised Relation Extraction


Jing Jiang
School of Information Systems
Singapore Management University
80 Stamford Road, Singapore 178902
jingjiang@smu.edu.sg





Abstract

Creating labeled  training data for rela- 
tion extraction is expensive.   In this pa- 
per, we study relation extraction in a spe- 
cial weakly-supervised  setting when we 
have only a few seed instances of the tar- 
get relation type we want to extract but 
we also have  a large amount of labeled 
instances  of other relation types.  Ob- 
serving that different relation types can 
share certain common structures, we pro- 
pose to use a multi-task  learning method 
coupled with human guidance to address 
this weakly-supervised relation extraction 
problem. The proposed framework  mod- 
els the commonality  among different re- 
lation types through  a shared weight  vec- 
tor, enables knowledge  learned from the 
auxiliary relation  types to be transferred 
to the target relation type, and allows easy 
control of the tradeoff between precision 
and recall.  Empirical evaluation on the 
ACE 2004 data set shows  that the pro- 
posed method substantially  improves over 
two baseline methods.

1   Introduction

Relation extraction is the task of detecting and 
characterizing semantic relations between entities 
from free text. Recent work on relation extraction 
has shown that supervised machine learning cou- 
pled with intelligent feature engineering or ker- 
nel design provides state-of-the-art solutions to the 
problem (Culotta and Sorensen, 2004; Zhou et al.,
2005; Bunescu and Mooney, 2005; Qian et al.,
2008). However, supervised learning heavily re- 
lies on a sufficient amount of labeled data for train- 
ing, which is not always available in practice due 
to the labor-intensive nature of human annotation. 
This problem is especially serious for relation ex-


traction  because the types of relations to be ex- 
tracted are highly  dependent on the application  do- 
main. For example, when working in the financial 
domain we may be interested in the employment 
relation, but when moving to the terrorism domain 
we now may be interested in the ethnic and ide- 
ology affiliation relation,  and thus have to create 
training data for the new relation type.
  However, is the old training data really useless? 
Inspired by recent work on transfer learning and 
domain adaptation, in this paper, we study how 
we can leverage labeled data of some old relation 
types to help the extraction of a new relation type 
in a weakly-supervised  setting, where only a few 
seed instances of the new relation type are avail- 
able. While transfer learning was proposed more 
than a decade ago (Thrun,  1996; Caruana, 1997), 
its application in natural language processing is 
still a relatively new territory (Blitzer et al., 2006; 
Daume III, 2007; Jiang and Zhai, 2007a; Arnold  et 
al., 2008; Dredze and Crammer, 2008), and its ap- 
plication in relation extraction is still unexplored.
  Our idea of performing transfer learning is mo- 
tivated by the observation that different relation 
types share certain common  syntactic  structures, 
which can possibly be transferred from the old 
types to the new type. We therefore propose to use 
a general multi-task  learning framework  in which 
classification models for a number of related tasks 
are forced to share a common model component 
and trained together. By treating classification 
of different relation types as related  tasks,  the 
learning framework can naturally model the com- 
mon syntactic structures among different  relation 
types in a principled  manner. It also allows us 
to introduce human guidance in separating the 
common model component from the type-specific 
components.   The framework  naturally  transfers 
the knowledge learned from the old relation types 
to the new relation type and helps improve the re- 
call of the relation extractor.  We also exploit ad-




1012

Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP,  pages 1012–1020, 
Suntec, Singapore, 2-7 August 2009. Qc 2009 ACL and AFNLP


ditional human knowledge  about the entity type 
constraints on the relation arguments, which can 
usually be derived from the definition of a relation 
type. Imposing these constraints further improves 
the precision of the final relation extractor.  Em- 
pirical  evaluation on the ACE 2004 data set shows 
that our proposed method largely outperforms two 
baseline methods, improving  the average F1 mea- 
sure from 0.1532 to 0.4132 when only 10 seed in- 
stances of the new relation type are used.

2   Related work

Recent work on relation extraction has been dom- 
inated by feature-based and kernel-based super- 
vised learning methods. Zhou et al. (2005) and 
Zhao and Grishman (2005) studied various fea- 
tures and feature combinations for relation extrac- 
tion. We systematically explored the feature space 
for relation extraction (Jiang and Zhai, 2007b) . 
Kernel methods allow a large set of features to be 
used without being explicitly extracted.  A num- 
ber of relation  extraction  kernels have been pro- 
posed, including  dependency tree kernels (Culotta 
and Sorensen,  2004), shortest dependency  path 
kernels (Bunescu and Mooney, 2005) and more re- 
cently convolution  tree kernels (Zhang et al., 2006; 
Qian et al., 2008). However, in both feature-based 
and kernel-based studies, availability of sufficient 
labeled training data is always assumed.
  Chen et al. (2006) explored  semi-supervised 
learning for relation extraction using label prop- 
agation, which makes  use  of  unlabeled data. 
Zhou et al. (2008) proposed a hierarchical learning 
strategy to address the data sparseness problem  in 
relation extraction. They also considered the com- 
monality among different relation types, but com- 
pared with our work, they had a different problem 
setting and a different  way of modeling the com- 
monality.  Banko and Etzioni (2008) studied open 
domain relation extraction, for which they man- 
ually identified several common relation patterns. 
In contrast, our method obtains common patterns 
through statistical learning. Xu et al. (2008) stud- 
ied the problem of adapting  a rule-based relation 
extraction system to new domains, but the types 
of relations to be extracted remain the same.
  Transfer learning aims at transferring knowl- 
edge learned from one or a number  of old tasks 
to a  new task.   Domain adaptation  is a  spe-


of data changes. There  has been an increasing 
amount of work on transfer learning and domain 
adaptation in natural language processing recently. 
Blitzer et al. (2006) proposed  a structural   cor- 
respondence learning  method  for domain  adap- 
tation and applied it to part-of-speech tagging. 
Daume III (2007) proposed a simple feature aug- 
mentation method to achieve domain adaptation. 
Arnold et al. (2008) used a hierarchical prior struc- 
ture to help transfer learning and domain adap- 
tation for named entity recognition. Dredze and 
Crammer (2008) proposed an online method for 
multi-domain  learning and adaptation.
  Multi-task learning is  another learning 
paradigm  in  which multiple related tasks are 
learned simultaneously in order to achieve better 
performance for each  individual task (Caruana,
1997; Evgeniou  and Pontil, 2004). Although it 
was not originally  proposed to transfer knowledge 
to a particular  new task, it can be naturally  used to 
achieve this goal because it models the common- 
ality among  tasks, which is the knowledge that 
should be transferred to a new task. In our work, 
transfer learning is done through a multi-task 
learning framework similar  to  Evgeniou and 
Pontil (2004).

3   Task definition

Our study is conducted using data from the Au- 
tomatic Content Extraction (ACE) program1.  We 
focus on extracting binary relation instances be- 
tween two relation arguments  occurring in the 
same sentence. Some example relation  instances 
and their corresponding relation types as defined 
by ACE can be found in Table 1.
  We consider the following weakly-supervised 
problem setting. We are interested in extracting
instances of a target relation  type T , but this re-
lation type is only specified by a small set of seed
instances. We may possibly have some additional 
knowledge about the target type not in the form of 
labeled instances. For example, we may be given 
the entity type restrictions on the two relation ar- 
guments.  In addition to such limited information 
about the target relation type, we also have a large 
amount of labeled instances for K auxiliary rela-
tion types A1, . . . , AK . Our goal is to learn a re-
lation extractor for T , leveraging all the data and
information we have.


cial  case  of  transfer  learning  where  the  learn-	 	


ing task remains  the same  but the distribution


1 http://projects.ldc.upenn.edu/ace/















leNNaPdNBer 1



NPIoNf3





PPgoveN2NrPnNBment2


Table 1: Examples of similar  syntactic structures across different  relation types. The head words of the
first and the second arguments are shown in italic and bold, respectively.





  Before introducing our transfer learning solu- 
tion, let us first briefly explain our basic classifi- 
cation approach and the features we use, as well 
as two baseline solutions.



PER 1	0



ORG 2


3.1   Feature configuration

We  treat relation extraction  as a classification 
problem. Each pair of entities within a single sen- 
tence is considered a candidate relation  instance, 
and the task becomes predicting  whether or not
each candidate  is a true instance of T .  We use
feature-based logistic regression classifiers.   Fol-
lowing  our previous work (Jiang and Zhai, 2007b), 
we extract  features from a sequence representa- 
tion and a parse tree representation  of each rela- 
tion instance.  Each node in the sequence or the 
parse tree is augmented by an argument tag that 
indicates whether the node subsumes arg-1, arg-
2, both or neither.  Nodes that represent the argu- 
ments are also labeled with the entity type, subtype 
and mention  type as defined by ACE. Based on the 
findings of Qian et al. (2008), we trim the parse 
tree of a relation  instance so that it contains only 
the most essential components.  We extract uni- 
gram features (consisting of a single node) and bi- 
gram features (consisting of two connected nodes) 
from the graphic representations. An example of 
the graphic representation of a relation  instance 
is shown in Figure 1 and some features extracted 
from this instance  are shown  in Table 2.  This 
feature configuration gives state-of-the-art perfor- 
mance (F1 = 0.7223) on the ACE 2004 data set in 
a standard setting with sufficient data for training.

3.2   Baseline solutions

We consider two baseline solutions to the weakly- 
supervised relation extraction problem. In the first






Figure 1: The combined  sequence and parse 
tree representation of the relation instance 
“leader of a minority government.”  The non-
essential nodes for “a” and for “minority” are 
removed  based on the algorithm from Qian et 
al. (2008).


F
e
a
t
u
r
e
E
x
p
l
a
n
a
t
i
o
n
O
R
G
2
of
0 
g
o
v
er
n
m
e
nt
2

N
P
3
 
→
 
P
P
2
ar
g-
2 
is 
an 
O
R
G 
en
tit
y.
a
r
g
-
2 
is 
“g
o
v
er
n
m
e
nt
”  
a
n
d 
fo
ll
o
w
s 
th
e 
w
or
d 
“
o
f.
” 
T
h
e
r
e 
is  
a   
n
o
u
n 
p
hr
a
s
e 
c
o
nt
ai
ni
n
g 
b
ot
h 
ar
g
u
m
e
nt
s, 
w
it
h 
a
r
g
-
2 
c
o
nt
ai
n
e
d 
in 
a 
pr
e
p
o
si
ti
o
n
al 
p
hr
a
s
e 
in
si
d
e 
th
e 
n
o
u
n 
p
h
r
a
s
e
.

Table 2: Examples of unigram and bigram 
features extracted from Figure 1.


baseline, we use only  the few seed instances of 
the target relation type together with labeled 
negative relation instances (i.e. pairs of 
entities within the same sentence but having 
no relation) to train a binary classifier. In the 
second baseline, we take the union of the 
positive instances of both the tar- get relation 
type and the auxiliary  relation types as our 
positive training  set, and together with the neg- 
ative instances we train a binary  classifier. 
Note that the second baseline method 
essentially learns


a classifier  for any relation type.
  Another existing solution to weakly-supervised 
learning problems  is semi-supervised learning, 
e.g. bootstrapping.   However,  because our pro- 
posed transfer learning method can be combined 
with semi-supervised learning, here we do not in- 
clude semi-supervised learning as a baseline.

4   A multi-task transfer learning solution

We now present a multi-task  transfer learning so- 
lution to the weakly-supervised relation extraction 
problem,  which  makes use of the labeled data from 
the auxiliary relation types.

4.1	Syntactic similarity between relation 
types

To see why the auxiliary relation types may help 
the identification  of the target relation type, let us 
first look at how different relation types may be re- 
lated and even similar to each other. Based on our 
inspection of a sample  of the ACE data, we find 
that instances of different  relation types can share 
certain common syntactic structures. For example, 
the syntactic pattern “arg-1 of arg-2” strongly in- 
dicates that there exists some relation between the 
two arguments, although the nature of the relation 
may be well dependent on the semantic meanings 
of the two arguments.  More examples are shown 
in Table 1.  This observation  suggests that some 
of the syntactic patterns learned from the auxiliary 
relation types may be transferable to the target re- 
lation type, making it easier to learn the target rela- 
tion type and thus alleviating the insufficient train-


to the target relation type.

4.2   Statistical learning model

As we have discussed, we want to force the linear 
classifiers for different  relation types to share their 
model weights for those features that are related 
to the common syntactic patterns.  Formally,  we 
consider the following statistical learning model.
  Let ωk  denote the weight vector of the linear 
classifier  that separates positive  instances of aux-
iliary type Ak  from negative instances, and let ωT
denote a similar weight vector for the target type
T . If different relation types are totally unrelated,
these weight vectors should also be independent of
each other. But because we observe similar syn- 
tactic structures across different  relation types, we 
now assume that these weight vectors are related 
through a common component ν:


ωT 	=  µT  + ν,
ωk	=   µk + ν   for  k = 1, . . . , K.

  If we assume that only weights of certain gen- 
eral features can be shared between different  rela- 
tion types, we can force certain dimensions of ν to 
be 0. We express this constraint by introducing  a 
matrix F and setting F ν = 0. Here F is a square 
matrix with all entries set to 0 except that Fi,i = 1 
if we want to force νi  = 0.
Now we can learn these weight vectors in a
multi-task learning framework. Let x represent 
the feature vector of a candidate relation instance,
and y  ∈ {+1, −1} represent  a class label. Let
NT


ing data problem with the target type.


DT   = {(xT , yT )}


denote the set of labeled


i	i	i=1


How can we incorporate this desired knowledge
transfer process into our learning method? While


instances for the target type T .  (Note that the
number of positive instances in DT  is very small.)
Nk


one can make explicit use of these general syntac-


And let Dk   = {(xk , yk )}


denote the labeled


i	i	i=1


tic patterns in a rule-based relation extraction sys-
tem, here we restrict our attention to feature-based


instances for the auxiliary type Ak .
We learn the optimal weight vectors {µˆk }K    ,


linear classifiers. We note that in feature-based lin-


µˆT


and νˆ by optimizing the following objective


ear classifiers,  a useful syntactic  pattern is trans- 
lated into large weights for features related to the 
syntactic pattern. For example, if “arg-1 of arg-2”


function:





k  K 	T 	\


is a useful pattern, in the learned linear classifier
we should have relatively large weights for fea- 
tures such as “the word of occurs before arg-2” or


{µˆ  }k=1 , µˆ


=	arg min


, νˆ
r
L(DT , µT  + ν)


k 	T


“a preposition occurs before arg-2,” or even more


{µ  },µ
K


,ν,F ν=0


complex features such as “there is a prepositional
phrase containing  arg-2 attached to arg-1.”  It is 
the weights of these generally useful features that


+ ) L(Dk , µk + ν)
k=1
K	l


are transferable  from the auxiliary relation types


+λµ  µT


2 	)
µ
k=1


.   (1)


  The objective function follows standard empir- 
ical risk minimization with regularization.   Here
L(D, ω) is the aggregated loss of labeling x with
y for all (x, y) in D, using weight vector  ω.  In
logistic regression models, the loss function is the
negative log likelihood, that is,


Human guidance
Another way to select the general features is to fol- 
low some guidance from human knowledge.  Re- 
call that in Section 4.1 we find that the common- 
ality among different relation types usually lies 
in the syntactic  structures between the two ar- 
guments. This observation  gives some intuition


L(D, ω)    =  −  )
(x,y)∈D


log p(y|x, ω),


about how to separate 
general features from type- 
specific features.  In 
particular,  here we consider


p(y|x, ω)    =

λT 	k


exp(ωy  · x) 	.
y! ∈{+1,−1} exp(ωy!  · 
x)


two hypotheses 
regarding the 
generality of 
differ- ent kinds 
of features.


µ , λµ  and λν  are regularization  parameters.
By adjusting their values, we can control the de-
gree of weight sharing among the relation types. 
The larger the ratio λT /λν  (or λk /λν ) is, the more
  

Argument word features: We hypothesize that 
the head words of the relation arguments are more 
likely to be strong indicators of specific relation


µ 	µ 	types rather than any relation type. For example, if


we believe that the model for T (or Ak ) should
conform to the common model, and the smaller
the type-specific weight vector µT  (or µk ) will be.
  The model presented above is based on our pre- 
vious work (Jiang and Zhai, 2007c), which bears 
the same spirit of some other recent work on multi- 
task learning (Ando and Zhang, 2005; Evgeniou 
and Pontil, 2004; Daume III, 2007). It is general 
for any transfer learning problem with auxiliary la- 
beled data from similar tasks. Here we are mostly 
interested in the model’s applicability  and effec- 
tiveness on the relation extraction problem.

4.3   Feature separation
Recall that we impose a constraint F ν = 0 when 
optimizing the objective function. This constraint 
gives us the freedom to force only the weights of a 
subset of the features to be shared among different 
relation types. A remaining question is how to set 
this matrix F , that is, how to determine the set of 
general features to use. We propose two ways of 
setting this matrix F .

Automatically  setting F
One way is to fix the number of non-zero entries 
in ν to be a pre-defined number H of general fea- 
tures, and allow F to change during the optimiza- 
tion process.  This can be done by repeating the 
following two steps until F converges:

1. Fix F , and optimize the objective function as 
in Equation (1).

2. Fix  µT  + ν  and  µk + ν , and search for
µT , {µk } and ν that minimizes  λT  µT    +


an argument has the head word “sister,” it strongly 
indicates a family  relation. We refer to the set of 
features that contain any head word of an argu- 
ment as “arg-word” features.
  Entity type features:  We hypothesize  that the 
entity types and subtypes of the relation arguments 
are also more likely to be associated with specific 
relation types. For example, arguments that are 
location entities may be strongly correlated with 
physical proximity relations. We refer to the set of 
features that contain the entity type or subtype of 
an argument  as “arg-NE” features.
  We hypothesize that the arg-word and arg-NE 
features are type-specific  and therefore should be 
excluded from the set of general features. We 
can force the weights of these hypothesized type- 
specific features to be 0 in the shared weight vec- 
tor ν, i.e. we can set the matrix  F to achieve this 
feature separation.

Combined method

We can also combine the automatic way of setting 
F with human guidance. Specifically, we still fol- 
low the first automatic procedure to choose gen- 
eral features, but we then filter out any hypothe- 
sized type-specific feature from the set of general 
features chosen by the automatic procedure.

4.4   Imposing entity type constraints

Finally,  we consider how we can exploit additional 
human knowledge about the target relation type T
to further improve the classifier. We note that usu- 
ally when a relation type is defined, we often have


k=1 λk 	k


2  + λν  ν


2  , subject  to the


strong preferences 
or even hard 
constraints on the


constraint that at most H
zero.


entries of ν


are non-


types of entities that can 
possibly be the two rela-
tion arguments. These type 
constraints can help us



Targ
et 
Type 
T
B
L
B
L-
A
T
L-
a
ut
o
T
L-
g
ui
de
T
L-
co
m
b
T
L-
N
E

P
h
y
s
i
c
a
l
P 
R 
F
0.
0
0
0
0
0.
0
0
0
0
0.
0
0
0
0
0.
16
92
0.
08
48
0.
11
30
0
.
2
9
2
0
0
.
1
6
9
6
0
.
2
1
4
6
0
.
2
9
3
4
0
.
1
7
2
2
0
.
2
1
7
0
0
.
3
3
2
5
0
.
2
3
8
3
0
.
2
7
7
7
0.
5
0
5
6
0.
2
3
1
6
0.
3
1
7
6
P
e
r
s
o
n
a
l
/
S
o
c
i
a
l
P
R 
F
1.
0
0
0
0
0.
0
3
8
6
0.
0
7
4
3
0.
08
04
0.
17
08
0.
10
93
0
.
1
0
0
5
0
.
1
5
9
8
0
.
1
2
3
4
0
.
3
0
6
9
0
.
7
2
4
5
0
.
4
3
1
1
0
.
3
2
1
4
0
.
7
6
8
6
0
.
4
5
3
3
0.
6
4
1
2
0.
7
6
3
1
0.
6
9
6
9
E
m
pl
o
y
m
e
nt
/M
e
m
be
rs
hi
p
/
S
u
b
si
di
a
r
y
P
R 
F
0.
9
2
3
1
0.
0
0
7
5
0.
0
1
4
8
0.
35
61
0.
18
50
0.
24
35
0
.
5
2
3
0
0
.
2
6
1
7
0
.
3
4
8
8
0
.
5
4
2
8
0
.
2
6
4
8
0
.
3
5
5
9
0
.
5
9
7
3
0
.
3
6
3
2
0
.
4
5
1
8
0.
7
1
4
5
0.
3
6
0
1
0.
4
7
8
9
A
g
e
n
t
-
A
r
t
i
f
a
c
t
P
R 
F
0.
8
7
5
0
0.
0
3
4
3
0.
0
6
6
0
0.
06
03
0.
23
53
0.
09
60
0
.
1
8
1
3
0
.
6
4
7
1
0
.
2
8
3
3
0
.
1
8
2
5
0
.
6
2
2
5
0
.
2
8
2
2
0
.
1
8
3
5
0
.
6
4
2
2
0
.
2
8
5
4
0.
1
9
6
7
0.
6
3
7
3
0.
3
0
0
6
P
E
R
/
O
R
G
A
f
fi
li
a
ti
o
n
P
R 
F
0.
8
8
8
9
0.
0
5
6
7
0.
1
0
6
7
0.
08
38
0.
49
65
0.
14
34
0
.
1
5
1
0
0
.
6
9
5
0
0
.
2
4
8
1
0
.
1
5
9
2
0
.
8
3
6
9
0
.
2
6
7
6
0
.
1
6
6
7
0
.
8
7
9
4
0
.
2
8
0
2
0.
1
8
4
4
0.
8
7
2
3
0.
3
0
4
5
G
P
E
A
f
f
i
l
i
a
t
i
o
n
P
R 
F
1.
0
0
0
0
0.
0
0
7
7
0.
0
1
5
3
0.
25
30
0.
45
09
0.
32
41
0
.
3
9
0
4
0
.
6
4
1
6
0
.
4
8
5
4
0
.
3
6
0
4
0
.
5
9
9
2
0
.
4
5
0
1
0
.
3
5
6
0
0
.
6
1
6
6
0
.
4
5
1
3
0.
5
8
2
4
0.
6
1
2
7
0.
5
9
7
2

D
i
s
c
o
u
r
s
e
P
R 
F
1.
0
0
0
0
0.
0
0
3
6
0.
0
0
7
1
0.
02
98
0.
07
89
0.
04
33
0
.
0
5
0
3
0
.
1
0
7
5
0
.
0
6
8
5
0
.
0
4
7
1
0
.
1
1
4
7
0
.
0
6
6
8
0
.
1
3
7
0
0
.
3
4
7
7
0
.
1
9
6
6
0.
1
3
7
0
0.
3
4
7
7
0.
1
9
6
6

A
v
e
r
a
g
e
P 
R 
F
0.
8
1
2
4
0.
0
2
1
2
0.
0
4
0
6
0.
14
75
0.
24
32
0.
15
32
0
.
2
4
1
2
0
.
3
8
3
2
0
.
2
5
3
2
0
.
2
7
0
3
0
.
4
7
6
4
0
.
2
9
5
8
0
.
2
9
9
2
0
.
5
5
0
9
0
.
3
4
2
3
0.
4
2
3
1
0.
5
4
6
4
0.
4
1
3
2

Table 3: Comparison of different methods on ACE 2004 data set. P, R and F stand for precision, recall 
and F1, respectively.




remove some false positive instances. We there- 
fore manually identify the entity type constraints 
for each target relation  type based on the defini- 
tion of the relation type given in the ACE annota- 
tion guidelines, and impose these type constraints 
as a final refinement step on top of the predicted 
positive instances.

5   Experiments

5.1   Data set and experiment setup

We used the ACE 2004  data set to evaluate our 
proposed methods. There are seven relation types 
defined in ACE 2004. After data cleaning, we ob- 
tained 4290 positive instances among 48614 can- 
didate relation instances.  We took each relation 
type as  the target type and used the remaining 
types as auxiliary types. This gave us seven sets 
of experiments.  In each set of experiments for a 
single target relation type, we randomly divided 
all the data into five subsets, and used each subset 
for testing while using the other four subsets for


training, i.e.  each experiment  was repeated five 
times with different training and test sets.  Each 
time, we removed most of the positive instances 
of the target type from the training set except only 
a small number S of seed instances.   This gave 
us the weakly-supervised setting. We kept all the 
positive instances of the target type in the test set. 
In order to concentrate on the classification accu- 
racy for the target relation type, we removed the 
positive  instances of the auxiliary relation types 
from the test set, although  in practice we need 
to extract these auxiliary relation instances using 
learned classifiers for these relation types.

5.2   Comparison of different methods

We first show the comparison of our proposed 
multi-task transfer learning methods with the two 
baseline methods described in Section 3.2. The 
performance on each target relation  type and the 
average performance across seven types are shown 
in Table 3. BL refers to the first baseline and BL-
A refers to the second baseline which  uses auxil-



TL-comb
TL-auto   	 
BL-A   	





Table 4:  The average performance  of TL-comb 
with different λT . (λk  = 104 and λν  = 1.)


0.25
0.2


µ 	µ 	0.15
0.1
100	1000	10000


iary relation instances. The four TL methods are
all based on the multi-task transfer learning frame- 
work.  TL-auto sets F  automatically within the 
optimization problem itself. TL-guide  chooses all 
features except arg-word  and arg-NE  features as 
general features and sets F accordingly. TL-comb 
combines TL-auto  and TL-guide,  as described in 
Section 4.3. Finally, TL-NE builds on top of TL- 
comb  and uses the entity type constraints to re- 
fine the predictions. In this set of experiments, 
the number of seed instances  for each target re- 
lation type was set to 10.  The parameters were


H

Figure 2: Performance of TL-comb  and TL-auto 
as H changes.


higher recall. These results make sense because 
the larger λT    is, the more we penalize  large 
weights of µT . As a result, the model for the tar- 
get type is forced to conform to the shared model 
ν and prevented from overfitting the few seed tar- 
get instances.  λT   is therefore a useful parameter


set to their optimal values (λT
λν  = 1, and H = 500).


= 104, λk  = 104,


to help us control  the tradeoff 
between precision
and recall for the target type.
While varying λk



As we can see from the table, first of all, BL
generally  has high precision but very low recall. 
BL-A performs better than BL in terms of F1 be- 
cause it gives better recall. However, BL-A still
cannot achieve as high recall  as the TL methods. 
This is probably  because the model learned by BL-
A still focuses more on type-specific features for
each relation  type rather than on the commonly
useful general  features,  and therefore  does not 
help much in classifying the target relation type.
  The four TL methods all outperform the two 
baseline methods. TL-comb  performs better than 
both TL-auto and TL-guide, which shows  that 
while we can either  choose general features au- 
tomatically by the learning algorithm or manu- 
ally with human knowledge, it is more effective 
to combine human knowledge with the multi-task 
learning framework. Not surprisingly, TL-NE im- 
proves the precision over TL-comb without hurt- 
ing the recall much. Ideally, TL-NE should not 
decrease recall if the type constraints are strictly 
observed in the data. We find that it is not always 
the case with the ACE data, leading to the small 
decrease of recall from TL-comb to TL-NE.

5.3   The effect of λT

Let us now take a look at the effect of using dif- 
ferent λT .  As we can see from Table 4, smaller
µ   gives higher precision while larger λµ   gives
               

µ  also gives similar effect for 
type Ak , we found that setting λk  to smaller values 
would not help T because in this case the auxiliary
relation instances would be used more for train- 
ing the type-specific component µk rather than the 
common component ν.

5.4   Sensitivity of H

Another parameter in the multi-task transfer learn- 
ing framework is the number of general features 
H , i.e.  the number of non-zero  entries in the 
shared weight vector ν.  To see how the perfor- 
mance may vary as H changes, we plot the per- 
formance of TL-comb and TL-auto in terms of the 
average F1 across the seven target relation types, 
with H ranging from 100 to 50000. As we can see 
in Figure 2, the performance is relatively  stable,
and always above BL-A.  This suggests that the
performance of TL-comb and TL-auto is not very
sensitive to the value of H .

5.5   Hypothesized type-specific features

In Section 4.3, we showed two sets of hypoth- 
esized type-specific  features, namely,  arg-word 
features and arg-NE features. We also experi- 
mented with each set separately  to see whether 
both sets are useful.  The comparison is shown in 
Table 5. As we can see, using either set of type- 
specific features in either TL-guide or TL-comb
can improve the performance over BL-A, but the


λT 	T




ar
g-
w
or
d
ar
g-
N
E
u
ni
o
n
TL
-
gu
id
e
TL
-
co
m
b
0
.
2
0
9
5
0
.
2
2
1
5
0.
2
9
8
3
0.
3
3
3
1
0.
29
58
0.
34
23
B
L
-
A
0
.
1
5
3
2

Table 5: Average F1 using different hypothesized 
type-specific features.


0.6

0.5

0.4

0.3

0.2


6   Conclusions and future work

In this paper, we applied multi-task  transfer learn- 
ing to solve a weakly-supervised  relation  extrac- 
tion problem, leveraging both labeled instances of 
auxiliary  relation types and human knowledge in- 
cluding  hypotheses on feature generality and en- 
tity type constraints. In the multi-task learning 
framework that we introduced, different relation 
types are treated as different  but related tasks that 
are learned together, with the common structures 
among the relation types modeled by a shared 
weight vector. The shared weight vector corre-



0.1

0


T
L
-
N
E
 
(
1
0
4
)
T
L-
N
E 
(1
02
)   
	
B
L
B
L
-
A
   
	

10	100	1000
S


sponds to the 
general 
features 
across 
different  re-
lation types. 
We proposed 
to choose the 
general 
features either 
automatically 
inside the 
learning al- 
gorithm or 
guided by 
human 
knowledge. 
We also 
leveraged 
additional 
human 
knowledge 
about the


Figure 3: Performance of TL-NE, BL and BL-A
as the number  of seed instances S of the target type


target relation type in the form of entity type con- 
straints. Experiment results on the ACE 2004 data


increases. (H = 500. λT


was set to 104 and 102).


show that the multi-task  
transfer learning method
achieves the best 
performance  when we 
combine human 
guidance with automatic  
general feature


arg-NE features are probably more type-specific 
than arg-word  features because they give better 
performance.  Using the union of the two sets is 
still the best for TL-comb.


5.6   Changing  the number  of seed instances
Finally, we compare TL-NE with BL and BL-A 
when the number of seed instances increases. We 
set S from 5 up to 1000. When S is large, the 
problem becomes more like traditional supervised


selection, followed by imposing the entity type 
constraints. The final method substantially outper- 
forms two baseline methods, improving  the aver- 
age F1 measure from 0.1532 to 0.4132 when only
10 seed target instances are used.
  Our work is the first to explore transfer learning 
for relation extraction, and we have achieved very 
promising  results. Because of the practical impor- 
tance of transfer learning and adaptation for rela- 
tion extraction due to lack of training data in new 
domains, we hope our study and findings will lead


learning, and our setting of λT


= 104 is no longer


to further investigation into 
this problem. There


optimal  because we are now not afraid of overfit- 
ting the large set of seed target instances. There- 
fore we also included another TL-NE experiment


are still many issues that remain unsolved. For ex- 
ample, we have not looked at the degrees of re- 
latedness between different  pairs of relation types.


with λT


set to 
102. The 
comparis
on of the 
perfor-


Presuma
bly, 
when 
adapting 
to a 
specific 
target 
re-


mance is shown in Figure 3. We see that as S in- 
creases, both BL and BL-A catch up, and BL over- 
takes BL-A when S is sufficiently  large because
BL uses positive  training  examples only from the 
target type. Overall, TL-NE still outperforms the 
two baselines in most of the cases over the wide 
range of values of S, but the optimal value for λT 
decreases  as S increases,  as we have suspected.


lation type, we want to choose the most similar 
auxiliary relation types to use.  Our current study 
is based on ACE relation types. It would also be 
interesting to study similar problems in other do- 
mains, for example, the protein-protein interaction 
extraction problem in biomedical text mining.


The results show that if λT


is set appropriately,


References


our multi-task  transfer learning method is robust 
and advantageous over the baselines under both 
the weakly-supervised  setting and the traditional 
supervised setting.



Rie Kubota Ando and Tong Zhang. 2005. A frame- 
work for learning predictive structures from multi- 
ple tasks and unlabeled  data. Journal of Machine 
Learning Research, 6:1817–1853, November.


Andrew Arnold, Ramesh Nallapati, and William W.
Cohen.  2008.   Exploiting feature hierarchy  for 
transfer learning in named entity recognition. In 
Proceedings of the 46th Annual Meeting of the As- 
sociation for Computational Linguistics, pages 245–
253.

Michele Banko and Oren Etzioni. 2008. The tradeoffs 
between open and traditional relation extraction.  In 
Proceedings of the 46th Annual Meeting of the Asso- 
ciation for Computational Linguistics, pages 28–36.

John Blitzer, Ryan McDonald,  and Fernando Pereira.
2006. Domain adaptation with structural correspon- 
dence learning.  In Proceedings of the Conference on 
Empirical Methods in Natural Language Process- 
ing, pages 120–128.

Razvan Bunescu and Raymond Mooney.  2005.  A 
shortest path dependency kernel for relation extrac- 
tion. In Proceedings of the Conference on Empiri- 
cal Methods in Natural Language Processing, pages
724–731.

Rich Caruana. 1997.  Multitask learning. Machine
Learning, 28:41–75.

Jinxiu Chen, Donghong Ji,  Chew Lim  Tan, and 
Zhengyu Niu.  2006. Relation extraction using la- 
bel propagation based semi-supervised learning.  In 
Proceedings of the 21st International  Conference on 
Computational Linguistics and 44th Annual Meet- 
ing of the Association for Computational Linguis- 
tics, pages 129–136.

Aron Culotta and Jeffrey Sorensen. 2004. Dependency 
tree kernels for relation extraction.  In Proceedings 
of the 42nd Meeting of the Association for Compu- 
tational Linguistics, pages 423–429.

Hal Daume III. 2007. Frustratingly easy domain adap- 
tation.  In Proceedings of the 45th Annual Meet- 
ing of the Association for Computational Linguis- 
tics, pages 256–263.

Mark Dredze  and Koby Crammer.  2008.   Online 
methods for multi-domain  learning and adaptation. 
In Proceedings of the 2008 Conference on Empiri- 
cal Methods in Natural Language Processing, pages
689–697.

Theodoros Evgeniou and Massimiliano  Pontil. 2004.
Regularized multi-task learning. In Proceedings of 
the 10th ACM SIGKDD International  Conference on 
Knowledge Discovery and Data Mining, pages 109–
117.

Jing Jiang and ChengXiang Zhai.  2007a. Instance 
weighting for domain adaptation in nlp. In Proceed- 
ings of the 45th Annual Meeting of the Association 
for Computational Linguistics, pages 264–271.

Jing Jiang and ChengXiang Zhai. 2007b. A systematic 
exploration of the feature space for relation extrac- 
tion. In Proceedings of the Human Language Tech- 
nologies Conference, pages 113–120.


Jing Jiang and ChengXiang Zhai. 2007c. A two-stage 
approach to domain adaptation for statistical classi- 
fiers. In Proceedings of the 16th ACM Conference 
on Information  and Knowledge Management, pages
401–410.

Longhua Qian, Guodong Zhou, Fang Kong, Qiaom- 
ing Zhu, and Peide Qian.  2008. Exploiting con- 
stituent dependencies for tree kernel-based semantic 
relation extraction.  In Proceedings of the 22nd In- 
ternational Conference on Computational Linguis- 
tics, pages 697–704.

Sebastian Thrun. 1996. Is learning the n-th thing any 
easier than learning the first? In Advances in Neural 
Information Processing  Systems 8, pages 640–646.

Feiyu Xu, Hans Uszkoreit, Hong Li, and Niko Felger.
2008. Adaptation of relation extraction rules to new 
domains. In Proceedings of the 6th International 
Conference on Language Resources and Evaluation, 
pages 2446–2450.

Min Zhang, Jie Zhang, and Jian Su. 2006. Exploring 
syntactic features for relation extraction using a con- 
volution tree kernel. In Proceedings of the Human 
Language Technology Conference, pages 288–295.

Shubin Zhao and Ralph Grishman.  2005. Extracting 
relations with integrated information using kernel 
methods.  In Proceedings of the 43rd Annual Meet- 
ing of the Association for Computational Linguis- 
tics, pages 419–426.

GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation ex- 
traction. In Proceedings of the 43rd Annual Meet- 
ing of the Association for Computational Linguis- 
tics, pages 427–434.

GuoDong Zhou, Min  Zhang, DongHong Ji,  and 
QiaoMing Zhu. 2008. Hierarchical learning strat- 
egy in semantic relation extraction.    Information 
Processing and Management, 44(3):1008–1021.

