Exploiting Syntactico-Semantic Structures for Relation Extraction




Yee Seng Chan and Dan Roth
University of Illinois at Urbana-Champaign
{chanys,danr}@illinois.edu










Abstract

In this paper, we observe that there exists  a 
second dimension  to the relation extraction 
(RE) problem that is orthogonal to the relation 
type dimension.  We show that most of these 
second dimensional  structures are relatively 
constrained and not difficult to identify. We 
propose  a novel algorithmic  approach to RE 
that starts by first identifying these structures 
and then, within these, identifying  the seman- 
tic type of the relation. In the real RE problem 
where relation  arguments need to be identi- 
fied, exploiting these structures also allows re- 
ducing pipelined propagated errors. We show 
that this RE framework provides significant 
improvement in RE performance.

1   Introduction

Relation extraction  (RE) has been defined as the task 
of identifying a given set of semantic binary rela- 
tions in text. For instance, given the span of text 
“. . . the Seattle zoo . . . ”, one would like to extract the 
relation that “the Seattle zoo” is located-at “Seattle”. 
RE has been frequently  studied  over the last few 
years as a supervised  learning  task, learning  from 
spans of text that are annotated with a set of seman- 
tic relations of interest.  However, most approaches 
to RE have assumed that the relations’ arguments 
are given as input (Chan and Roth, 2010; Jiang and 
Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and 
therefore offer only a partial solution to the problem.
  Conceptually, this is a rather simple  approach as 
all spans of texts are treated uniformly and are be- 
ing mapped to one of several relation  types of in- 
terest. However,  these approaches to RE require  a


large amount of manually annotated training  data to 
achieve good performance, making it difficult to ex- 
pand the set of target relations. Moreover,   as we 
show, these approaches become brittle when the re- 
lations’ arguments are not given but rather need to 
be identified in the data too.
  In this paper we build on the observation that there 
exists a second dimension  to the relation extraction 
problem that is orthogonal to the relation type di- 
mension: all relation  types are expressed in one of 
several constrained syntactico-semantic structures. 
As we show, identifying  where the text span is on the 
syntactico-semantic structure dimension first, can be 
leveraged in the RE process to yield improved per- 
formance. Moreover, working in the second dimen- 
sion provides robustness to the real RE problem, that 
of identifying arguments along with the relations be- 
tween them.
  For example, in “the Seattle zoo”, the entity men- 
tion “Seattle” modifies the noun “zoo”.  Thus, the 
two mentions “Seattle” and “the Seattle zoo”, are 
involved in what we later call a premodifier   rela- 
tion, one of several syntactico-semantic  structures 
we identify in Section 3.
  We highlight that all relation types can be ex- 
pressed in one of several syntactico-semantic struc- 
tures – Premodifiers,  Possessive, Preposition,  For- 
mulaic and Verbal. As it turns out, most of these 
structures are relatively  constrained and are not dif- 
ficult to identify.  This suggests  a novel algorith- 
mic approach to RE that starts by first identifying 
these structures and then, within these, identifying 
the semantic type of the relation. Not only does this 
approach provide significantly  improved RE perfor-



551


Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 551–560, 
Portland, Oregon, June 19-24, 2011. Qc 2011 Association for Computational Linguistics


mance, it carries with it two additional advantages.
  First, leveraging  the syntactico-semantic  struc- 
ture is especially beneficial in the presence of small 
amounts of data. Second, and more important, is the 
fact that exploiting the syntactico-semantic dimen- 
sion provides several new options for dealing with 
the full RE problem – incorporating the argument 
identification into the problem. We explore one of 
these possibilities,   making use of the constrained 
structures as a way to aid in the identification of the 
relations’ arguments. We show that this already pro- 
vides significant  gain, and discuss other possibilities 
that can be explored. The contributions of this paper 
are summarized below:
• We  highlight that all relation types are  ex- 
pressed as one of several syntactico-semantic 
structures and show that most of these are rela- 
tively constrained and not difficult to identify.
Consequently,  working first in this structural 
dimension  can be leveraged in the RE process 
to improve performance.
• We show that when one does not have a large 
number of training examples, exploiting  the 
syntactico-semantic structures is crucial for RE 
performance.
• We show how to leverage these constrained 
structures to improve RE when the relations’ 
arguments are not given. The constrained struc- 
tures allow us to jointly entertain argument can-
didates and relations built with them  as argu- 
ments. Specifically,  we show that considering 
argument  candidates which otherwise would 
have  been discarded  (provided   they exist  in 
syntactico-semantic structures), we reduce er- 
ror propagation  along a standard pipeline  RE 
architecture,  and that this joint inference pro- 
cess leads to improved RE performance.

  In the next section, we describe our relation ex- 
traction framework that leverages  the syntactico- 
semantic structures. We  then present these struc- 
tures in Section 3. We describe our mention entity 
typing system in Section 4 and features for the RE 
system in Section 5. We present our RE experiments 
in Section 6 and perform analysis in Section 7, be- 
fore concluding in Section 8.


S = {premodifier, possessive, preposition,  formulaic}
gold mentions in training data Mtrain
Dg = {(mi , mj ) ∈ Mtrain × Mtrain |
mi  in same sentence as mj  ∧ i /= j ∧ i < j}
REbase  = RE classifier trained on Dg
Ds  = ∅
for each (mi , mj ) ∈ Dg
do
p = structure inference on (mi , mj ) using patterns
if p ∈ S ∨ (mi , mj ) was annotated with a S structure
Ds = Ds ∪ (mi , mj )
done
REs  = RE classifier trained on Ds

Output: REbase  and REs

Figure 1:    Training a   regular baseline RE  classi- 
fier REbase   and a RE classifier leveraging syntactico- 
semantic structures REs .


2   Relation Extraction Framework

In Figure 1, we show the algorithm for training 
a typical baseline RE classifier (REbase), and for 
training  a RE classifier that leverages the syntactico- 
semantic structures (REs).
During evaluation and when the gold mentions are
already annotated, we apply REs as follows. When 
given  a test example mention  pair (xi,xj ), we per- 
form structure inference on it using the patterns de- 
scribed in Section 3. If (xi,xj ) is identified  as hav-
ing any of the four syntactico-semantic structures S,
apply REs  to predict the relation label, else apply
REbase.
Next, we show in Figure 2 our joint inference al-
gorithmic  framework  that leverages the syntactico- 
semantic structures for RE, when mentions need to 
be predicted. Since the structures  are fairly con- 
strained, we can use them to consider mention can- 
didates that are originally predicted   as non men- 
tions. As shown in Figure 2, we conservatively in- 
clude such mentions when forming mention pairs, 
provided their null labels are predicted  with a low 
probability t1.

  1 In this work, we arbitrary set t=0.2. After the experiments, 
and in our own analysis, we observe that t=0.25 achieves better 
performance. Besides using the probability  of the 1-best predic- 
tion, one could also for instance, use the probability  difference 
between the first and second best predictions.  However, select- 
ing an optimal t value is not the main focus of this work.


S = {premodifier, possessive, preposition,  formulaic}
candidate mentions Mcand
Let Lm = argmaxPM ET (y|m, θ), m ∈ Mcand
y
selected mentions Msel = {m ∈ Mcand | 
Lm /= null ∨ PM ET (null|m, θ) ≤ t} 
QhasN ull = {(mi , mj ) ∈ Msel × Msel |
mi  in same sentence as mj  ∧ i /= j ∧ i < j ∧
(Lmi   /= null ∨ Lmj   /= null)}
Let pool of relation predictions R = ∅
for each (mi , mj ) ∈ QhasN ull
do

p = structure inference on (mi , mj ) using patterns 
if p ∈ S
r = relation prediction for (mi , mj ) using REs
R = R ∪ r
else if Lmi   /= null ∧ Lmj   /= null
r = relation prediction for (mi , mj ) using REbase
R = R ∪ r
done

O  utput: R

Figure 2: RE using predicted mentions and patterns. Ab- 
breviations: Lm : predicted entity label for mention m us- 
ing the mention entity typing (MET) classifier described 
in Section 4; PM ET : prediction probability according to 
the MET classifier; t: used for thresholding.


  There is a large  body of work in using patterns 
to extract relations (Fundel et al., 2007; Greenwood 
and Stevenson, 2006; Zhu et al., 2009). However, 
these works operate along the first dimension, that 
of using patterns to mine for relation type examples. 
In contrast, in our RE framework, we apply patterns 
to identify the syntactico-semantic structure dimen- 
sion first, and leverage this in the RE process.  In 
(Roth and Yih, 2007), the authors used entity types 
to constrain the (first dimensional) relation types al- 
lowed among them. In our work, although  a few of 
our patterns involve semantic type comparison, most 
of the patterns are syntactic in nature.
  In this work, we performed RE evaluation on the 
NIST Automatic Content Extraction (ACE) corpus. 
Most prior RE evaluation on ACE data assumed that 
mentions are already pre-annotated and given as in- 
put (Chan and Roth, 2010; Jiang and Zhai, 2007; 
Zhou et al., 2005). An exception is the work of 
(Kambhatla, 2004), where the author evaluated on 
the ACE-2003 corpus. In that work, the author did


not address the pipelined  errors propagated from  the 
mention identification process.

3   Syntactico-Semantic Structures

In this paper, we performed RE on the ACE-2004 
corpus. In ACE-2004  when the annotators tagged a 
pair of mentions with a relation, they also specified 
the type of syntactico-semantic structure2. ACE-
2004 identified five  types of structures: premodi- 
fier, possessive, preposition,  formulaic,  and verbal. 
We are unaware of any previous computational ap- 
proaches that recognize these structures automati- 
cally in text, as we do, and use it in the context of 
RE (or any other problem). In (Qian et al., 2008), the 
authors reported the recall scores of their RE system 
on the various syntactico-semantic structures.  But 
they do not attempt to recognize nor leverage these 
structures.
  In this work, we focus on detecting the first four 
structures.  These four structures cover 80% of the 
mention pairs having valid semantic relations (we 
give the detailed breakdown in Section 7) and we 
show that they are relatively  easy to identify using 
simple rules or patterns. In this section, we indicate 
mentions using square bracket pairs, and use mi and 
mj to represent a mention pair. We now describe the 
four structures.

Premodifier relations specify the proper adjective 
or proper noun premodifier  and the following noun 
it modifies, e.g.: [the [Seattle] zoo]

Possessive   indicates  that the first mention is in a 
possessive case, e.g.: [[California] ’s Governor]

Preposition indicates that the two mentions  are 
semantically related via the existence of a preposi- 
tion, e.g.: [officials] in [California]

Formulaic  The ACE04 annotation guideline3  in- 
dicates the annotation of several formulaic relations, 
including for example address: [Medford] , [Mas- 
sachusetts]

  2 ACE-2004 termed it as lexical  condition.  We use the term 
syntactico-semantic structure in this paper as the mention  pair 
exists in specific syntactic structures, and we use rules or pat- 
terns that are syntactically  and semantically motivated to detect 
these structures.
3 http://projects.ldc.upenn.edu/ace/docs/EnglishRDCV4-3-
2.PDF



Str
uct
ur
e 
ty
pe
Pat
ter
n
Pr
e
m
od
ifie
r
Ba
sic 
pat
ter
n: 
[u
* 
[v
+] 
w
+] 
, 
wh
ere 
u, 
v, 
w 
rep
res
ent 
wo
rd
s
Ea
ch 
w 
is 
a 
no
un 
or 
adj
ect
ive
If 
u* 
is 
not 
em
pty
, 
the
n 
u*: 
JJ+ 
∨ 
JJ 
“a
nd
” 
JJ? 
∨ 
C
D 
JJ* 
∨ 
R
B 
D
T 
JJ? 
∨ 
R
B 
C
D 
JJ 
∨
D
T
 
(
R
B
|
J
J
|
V
B
G
|
V
B
D
|
V
B
N
|
C
D
)
?
Le
t 
w1 
= 
fir
st 
wo
rd 
in 
w
+. 
w1 
/= 
“’s
” 
an
d 
PO
S 
tag 
of 
w1 
/= 
P
O
S
Le
t vl  
= 
las
t 
wo
rd 
in 
v+
. 
PO
S 
tag 
of 
vl  
/= 
PR
P$ 
nor 
W
P$
Po
ss
es
siv
e
Ba
sic 
pat
ter
n: 
[u
? 
[v
+] 
w
+] 
, 
wh
ere 
u, 
v, 
w 
rep
res
ent 
wo
rd
s
Le
t 
w1 
= 
fir
st 
wo
rd 
in 
w
+. 
If 
w1 
= 
“’s
” 
∨ 
PO
S 
tag 
of 
w1 
= 
PO
S, 
ac
ce
pt 
me
nti
on 
pa
ir
Le
t vl  
= 
las
t 
wo
rd 
in 
v+
. If 
PO
S 
tag 
of 
vl  
= 
PR
P$ 
or 
W
P$
, 
ac
ce
pt 
me
nti
on 
pa
ir
Pr
ep
osi
tio
n
Ba
sic 
pat
ter
n: 
[m
i ] 
v* 
[m
j ], 
wh
ere 
v 
rep
res
ent 
wo
rd
s
a
n
d
 
n
u
m
b
e
r
 
o
f
 
p
r
e
p
o
s
i
t
i
o
n
s
 
i
n
 
t
h
e
 
t
e
x
t
 
s
p
a
n
 
v
*
 
b
e
t
w
e
e
n
 
t
h
e
m
 
=
 
0
,
 
1
,
 
o
r
 
2
If 
sat
isf
y 
pat
ter
n: 
IN 
[m
i 
][
mj 
], 
ac
ce
pt 
me
nti
on 
pa
ir
If 
sat
isf
y 
pat
ter
n: 
[m
i ] 
(I
N|
T
O) 
[m
j ], 
ac
ce
pt 
me
nti
on 
pa
ir
If 
all 
lab
els 
in 
Ld  
sta
rt 
wit
h 
“pr
ep
”, 
ac
ce
pt 
me
nti
on 
pa
ir
Fo
rm
ul
aic
If 
sat
isf
y 
pat
ter
n: 
[m
i ] / 
[m
j ] 
∧ 
Ec 
(m
i ) 
= 
PE
R 
∧ 
Ec 
(m
j ) 
= 
O
R
G, 
ac
ce
pt 
me
nti
on 
pa
ir
If 
sat
isf
y 
pat
ter
n: 
[m
i 
][
mj 
]
I
f
 
E
c
 
(
m
i
 
)
 
=
 
P
E
R
 
∧
 
E
c
 
(
m
j
 
)
 
=
 
O
R
G
 
∨
 
G
P
E
,
 
a
c
c
e
p
t
 
m
e
n
t
i
o
n
 
p
a
i
r

Table 1: Rules and patterns for the four syntactico-semantic  structures.  Regular expression notations:  ‘*’ matches 
the preceding element zero or more times; ‘+’ matches the preceding element one or more times; ‘?’  indicates that
the preceding element is optional; ‘|’ indicates or. Abbreviations: Ec (m): coarse-grained entity type of mention m;
Ld : labels in dependency path between the headword of two mentions.  We use square brackets ‘[’ and ‘]’ to denote
mention boundaries. The ‘/’ in the Formulaic row denotes the occurrence of a lexical  ‘/’ in text.




  In  this rest of  this section, we present  the 
rules/patterns for   detecting  the   above    four 
syntactico-semantic structure,  giving an overview 
of them in Table 1.  We plan to release all of the 
rules/patterns along with associated code4. Notice 
that the patterns are intuitive and mostly syntactic in 
nature.

3.1	Premodifier Structures
• We require that one of the mentions completely 
include the other mention. Thus, the basic pat- 
tern is [u* [v+] w+].


• If u* is not empty, we require that it satisfies 
any of the following POS tag sequences: JJ+ ∨ 
JJ and JJ? ∨ CD JJ*, etc. These are (optional)
POS tag sequences that normally  start a valid 
noun phrase.

• We  use two patterns to differentiate  between 
premodifier  relations and possessive relations, 
by checking  for the existence  of POS tags 
PRP$, WP$, POS, and the word “’s”.

4 http://cogcomp.cs.illinois.edu/page/publications


3.2	Possessive Structures
• The basic pattern for possessive is similar to
that for premodifier: [u? [v+] w+]
• If the word immediately following v+ is “’s” or 
its POS tag is “POS”, we accept the mention 
pair. If the POS tag of the last word in v+ is ei- 
ther PRP$ or WP$, we accept the mention pair.

3.3	Preposition Structures
• We first require the two mentions to be non-
overlapping,  and check for the existence  of

patterns  such as  “IN  [mi]  [mj ]” and “[mi] 
(IN|TO) [mj ]”.
• If  the only dependency labels  in the depen- 
dency path between the head words of mi  and 
mj  are “prep” (prepositional modifier), accept 
the mention pair.

3.4	Formulaic Structures
• The ACE-2004 annotator  guidelines  specify
that several relations such as reporter signing
off, addresses, etc. are often specified in stan- 
dard structures. We check for the existence of 
patterns  such as “[mi] / [mj ]”, “[mi] [mj ]”,


Category	Feature
For every 	POS of wk and offset from lw
word wk 	wk and offset from lw
in	POS of wk , wk , and offset from lw
mention mi	POS of wk , offset from lw, and lw
Bc(wk ) and offset from lw
POS of wk , Bc(wk ), and offset from lw
POS of wk , offset from lw, and Bc(lw)
Contextual    C−1,−1  of mi 
C+1,+1 of mi 
P−1,−1  of mi 
P+1,+1  of mi
NE tags	tag of NE, if lw of NE coincides 
with lw of mi  in the sentence
Syntactic	parse-label of parse tree constituent
parse	that exactly covers mi
parse-labels of parse tree constituents
                         covering mi

Table 2:  Features  used  in our mention entity typing 
(MET) system. The abbreviations  are as follows.  lw: 
last word in the mention;  Bc(w):  the brown cluster bit 
string representing w; NE: named entity


and whether they satisfy certain semantic entity 
type constraints.

4   Mention Extraction System

As part of our experiments, we perform RE using 
predicted mentions. We first describe the features 
(an overview is given in Table 2) and then describe 
how we extract candidate mentions from sentences 
during evaluation.

4.1   Mention Extraction Features

Features for every word in the mention  For ev- 
ery word wk in a mention  mi, we extract seven fea- 
tures. These are a combination  of wk itself, its POS 
tag, and its integer offset from the last word (lw) in 
the mention. For instance, given the mention “the 
operation room”, the offsets for the three words in 
the mention  are -2, -1, and 0 respectively. These 
features are meant to capture the word and POS tag 
sequences in mentions.
  We also use  word clusters which are automat- 
ically generated  from unlabeled  texts, using the 
Brown clustering (Bc) algorithm of (Brown et al.,
1992). This algorithm  outputs a binary tree where 
words are leaves in the tree. Each word (leaf) in the 
tree can be represented by its unique path from the


Category	Feature
POS	POS of single word between m1 , m2 
hw of mi , mj  and P−1,−1  of mi , mj 
hw of mi , mj  and P−1,−1  of mi , mj 
hw of mi , mj  and P+1,+1  of mi , mj 
hw of mi , mj  and P−2,−1  of mi , mj 
hw of mi , mj  and P−1,+1  of mi , mj 
hw of mi , mj  and P+1,+2  of mi , mj
Base chunk	any base phrase chunk between mi , mj

Table 3: Additional RE features.


root and this path can be represented as a simple bit 
string. As part of our features, we use the cluster bit 
string representation of wk and lw.

Contextual We extract the word C−1,−1  immedi- 
ately before mi, the word C+1,+1 immediately after 
mi, and their associated POS tags P .

NE tags   We automatically  annotate the sentences 
with named entity (NE) tags using the named en- 
tity tagger of (Ratinov and Roth, 2009). This tagger 
annotates proper nouns with the tags PER (person), 
ORG (organization), LOC (location), or MISC (mis- 
cellaneous). If the lw of mi  coincides (actual token 
offset) with the lw of any NE annotated by the NE 
tagger, we extract the NE tag as a feature.

Syntactic parse  We parse the sentences using the 
syntactic parser of (Klein and Manning, 2003). We 
extract the label of the parse tree constituent  (if it ex- 
ists) that exactly covers the mention, and also labels 
of all constituents that covers the mention.


4.2   Extracting Candidate Mentions

From a sentence, we gather the following as candi- 
date mentions:  all nouns and possessive pronouns, 
all named entities annotated by the the NE tagger 
(Ratinov and Roth, 2009), all base noun phrase (NP) 
chunks, all chunks satisfying the pattern: NP (PP 
NP)+, all NP constituents in the syntactic parse tree, 
and from each of these constituents,  all substrings 
consisting of two or more words, provided the sub- 
strings do not start nor end on punctuation marks. 
These mention  candidates are then fed to our men- 
tion entity typing (MET) classifier for type predic- 
tion (more details in Section 6.3).


5   Relation Extraction System

We build a supervised  RE system using sentences 
annotated with entity mentions and predefined target 
relations. During evaluation,  when given a pair of 
mentions mi, mj , the system predicts whether any 
of the predefined target relation holds between the 
mention pair.
  Most of our features  are based on the work of 
(Zhou et al., 2005; Chan and Roth, 2010). Due to 
space limitations, we refer the reader to our prior 
work (Chan and Roth, 2010) for the lexical, struc- 
tural, mention-level,  entity type, and dependency 
features. Here, we only describe the features that 
were not used in that work.
  As part of our RE system, we need to extract the 
head word (hw) of a mention (m), which we heuris- 
tically determine  as follows: if m contains a prepo- 
sition and a noun preceding the preposition,  we use 
the noun as the hw. If there is no preposition in m, 
we use the last noun in m as the hw.

POS features   If there is a single word between the 
two mentions, we extract its POS tag. Given the hw 
of m, Pi,j  refers to the sequence of POS tags in the 
immediate context of hw (we exclude the POS tag 
of hw). The offsets i and j denote the position (rela- 
tive to hw) of the first and last POS tag respectively. 
For instance, P−2,−1  denotes the sequence of two 
POS tags on the immediate left of hw, and P−1,+1 
denotes the POS tag on the immediate left of hw and 
the POS tag on the immediate right of hw.

Base phrase chunk  We add a boolean feature to 
detect whether there is any base phrase chunk in the 
text span between the two mentions.

6   Experiments

We  use  the ACE-2004 dataset (catalog 
LDC2005T09 from  the  Linguistic  Data Con- 
sortium) to conduct our experiments.    Following 
prior work, we use  the news wire (nwire) and 
broadcast news (bnews)  corpora of ACE-2004 for


tive, we compare against the state-of-the-art feature- 
based RE systems of (Jiang and Zhai, 2007) and 
(Chan and Roth, 2010).  In these works, the au- 
thors reported performance  on undirected  coarse- 
grained RE. Performing 5-fold cross validation  on 
the nwire and bnews corpora, (Jiang and Zhai, 2007) 
and (Chan and Roth, 2010) reported F-measures of
71.5 and 71.2, respectively.  Using the same evalua- 
tion setting, our baseline RE system achieves a com- 
petitive 71.4 F-measure.
  We build three RE classifiers: binary, coarse, fine. 
Lumping all the predefined target relations  into a 
single label, we build a binary classifier to predict 
whether any of the predefined relations exists be- 
tween a given mention pair.
  In this work, we model the argument order of the 
mentions when performing  RE, since relations  are 
usually asymmetric in nature. For instance, we con- 
sider mi:EMP-ORG:mj  and mj :EMP-ORG:mi  to 
be distinct relation types. In our experiments, we ex- 
tracted a total of 55,520 examples or mention pairs. 
Out of these, 4,011 are positive  relation  examples 
annotated with 6 coarse-grained relation types and
22 fine-grained relation types5.
  We build a coarse-grained   classifier  to disam- 
biguate between 13 relation labels (two asymmetric 
labels for each of the 6 coarse-grained relation types 
and a null label). We similarly build a fine-grained 
classifier to disambiguate between 45 relation labels.

6.1   Evaluation Method

For our experiments, we adopt the experimental set- 
ting in our prior work (Chan and Roth, 2010) of en- 
suring that all examples from a single document are 
either all used for training, or all used for evaluation.
  In that work, we also highlight that ACE anno- 
tators rarely duplicate a relation link for coreferent 
mentions.  For instance, assume mentions mi, mj , 
and mk  are in the same sentence, mentions mi  and 
mj  are coreferent,  and the annotators tag the men- 
tion pair mj , mk  with a particular  relation  r.  The 
annotators will rarely duplicate the same (implicit)


our experiments, which consists of 345 documents.	 	


  To build our RE system, we use the LIBLINEAR 
(Fan et al., 2008) package, with its default settings 
of L2-loss SVM (dual) as the solver, and we use an 
epsilon of 0.1. To ensure that this baseline RE sys- 
tem based on the features in Section 5 is competi-


5 We omit a single  relation: Discourse (DISC). The ACE-
2004 annotation guidelines states that the DISC relation is es- 
tablished only for the purposes of the discourse and does not 
reference an official entity relevant to world knowledge. In this 
work, we focus on semantically meaningful relations. Further- 
more, the DISC relation is dropped in ACE-2005.




RE 
m
od
el
1
0
 
d
o
c
u
m
e
n
t
s
Re
c
%	Pre%	F1%
5
%
 
o
f
 
d
a
t
a
R
ec
%	Pre%	F1%
8
0
%
 
o
f
 
d
a
t
a
R
ec
%	Pre%	F1%
Bi
na
ry
Bi
na
ry
+P
att
ern
s
5
8.
0	80.3	67.4
7
3.
1	78.5	75.7 (+8.3)
6
4.
4	80.6	71.6
7
5.
3	80.6	77.9
7
3.
2	84.0	78.2
8
0.
1	84.2	82.1
Co
ar
se
Co
ars
e+
Pat
ter
ns
3
3.
5	62.5	43.6
4
4.
2	59.6	50.8 (+7.2)
4
2.
4	66.2	51.7
5
1.
2	64.2	56.9
6
2.
1	75.5	68.1
6
8.
0	75.4	71.5
Fi
ne
Fi
ne
+P
att
er
ns
1
8.
1	47.0	26.1
2
4.
8	43.5	31.6 (+5.5)
2
6.
3	51.6	34.9
3
2.
2	48.9	38.9
5
1.
6	68.4	58.8
5
6.
4	67.5	61.5

Table 4: Micro-averaged (across the 5 folds) RE results using gold mentions.


RE 
m
od
el
1
0
 
d
o
c
u
m
e
n
t
s
Re
c
%	Pre%	F1%
5
%
 
o
f
 
d
a
t
a
R
ec
%	Pre%	F1%
8
0
%
 
o
f
 
d
a
t
a
R
ec
%	Pre%	F1%
Bi
na
ry
Bi
na
ry
+P
att
ern
s
3
2.
2	46.6	38.1
4
6.
7	45.9	46.3 (+8.2)
3
5.
5	48.9	41.1
4
7.
6	47.8	47.2
4
0.
1	52.7	45.5
5
0.
2	50.4	50.3
Co
ar
se
Co
ars
e+
Pat
ter
ns
1
8.
6	41.1	25.6
2
6.
8	34.7	30.2 (+4.6)
2
2.
4	40.9	28.9
3
0.
3	37.0	33.3
3
2.
3	47.5	38.5
3
8.
9	42.9	40.8
Fi
ne
Fi
ne
+P
att
er
ns
1
0.
7	32.2	16.1
1
5.
7	26.3	19.7 (+3.6)
1
4.
6	33.4	20.3
1
9.
4	29.2	23.3
2
6.
9	44.3	33.5
3
1.
7	38.3	34.7

Table 5: Micro-averaged (across the 5 folds) RE results using predicted mentions.




relation r between mi and mk , thus leaving the gold 
relation label as null. Whether this is correct or not is 
debatable. However, to avoid being penalized when 
our RE system actually correctly predicts the label 
of an implicit relation, we take the following ap- 
proach.
  During evaluation,  if our system correctly pre- 
dicts an implicit label, we simply switch its predic- 
tion to the null label.  Since the RE recall scores 
only take into account non-null relation labels, this 
scoring method does not change the recall, but could 
marginally increase the precision scores by decreas- 
ing the count of RE predictions. In our experi- 
ments, we observe that both the usual and our scor- 
ing method give very similar RE results and the ex- 
perimental trends remain the same. Of course, us- 
ing this scoring method requires coreference infor- 
mation, which is available in the ACE data.

6.2   RE Evaluation Using Gold Mentions

To perform our experiments, we split the 345 docu- 
ments into 5 equal sets. In each of the 5 folds, 4 sets 
(276 documents) are reserved for drawing training 
examples, while the remaining  set (69 documents) 
is used as evaluation   data. In the experiments de- 
scribed in this section, we use the gold mentions 
available in the data.
When one only has  a  small amount  of train-


ing data, it is crucial to take advantage of external 
knowledge   such  as the syntactico-semantic struc- 
tures. To simulate this setting, in each fold, we ran- 
domly selected 10 documents from the fold’s avail- 
able training documents (about 3% of the total 345 
documents)  as training data. We built one binary, 
one coarse-grained, and one fine-grained classifier 
for each fold.
  In Section 2, we described how we trained a base- 
line RE classifier (REbase) and a RE classifier using 
the syntactico-semantic patterns (REs).
  We first apply REbase on each test example men- 
tion pair (mi,mj ) to obtain the RE baseline results, 
showing these in Table 4 under the column “10 doc- 
uments”,  and in the rows “Binary”, “Coarse”, and 
“Fine”.   We then applied REs  on the test exam- 
ples as described  in Section 2, showing the results 
in the rows “Binary+Patterns”, “Coarse+Patterns”, 
and “Fine+Patterns”.   The results show that by us- 
ing syntactico-semantic structures, we obtain signif- 
icant F-measure improvements of 8.3, 7.2, and 5.5 
for binary, coarse-grained, and fine-grained relation 
predictions respectively.

6.3   RE Evaluation Using Predicted Mentions

Next, we perform our experiments using predicted 
mentions. ACE-2004 defines 7 coarse-grained entity 
types, each of which are then refined into 43 fine-


Improvement  in (gold mentions) RE by using patterns
8
Binary+Pattern
7 	Coarse+Pattern   	 
Fine+Pattern

6


Improvement  in (predicted mentions) RE by using patterns
8
Binary+Pattern
7 	Coarse+Pattern
Fine+Pattern
6



5 	5

4 	4

3 	3

2 	2

1 	1



0
5    10   15   20   25   30   35   40   45   50   55   60   65   70   75   80
Proportion (%) of data used for training



0
5    10   15   20   25   30   35   40   45   50   55   60   65   70   75   80
Proportion (%) of data used for training






Figure 3: Improvement in (gold mention) RE.


grained entity types. Using the ACE data annotated 
with mentions and predefined entity types, we build 
a fine-grained  mention  entity typing (MET) clas- 
sifier to disambiguate between 44 labels (43 fine- 
grained  and a null label to indicate not a mention). 
To obtain the coarse-grained entity type predictions 
from the classifier, we simply check which coarse- 
grained type the fine-grained prediction belongs to. 
We use the LIBLINEAR package with the same set- 
tings as earlier specified for the RE system. In each 
fold, we build a MET classifier using all the (276) 
training documents in that fold.
  We apply REbase on all mention pairs (mi,mj ) 
where both mi and mj have non null entity type pre- 
dictions.  We show these baseline results in the Rows 
“Binary”, “Coarse”, and “Fine” of Table 5.
  In Section 2, we described our algorithmic ap- 
proach (Figure 2) that takes advantage of the struc- 
tures with predicted mentions. We show the results 
of this approach  in the Rows “Binary+Patterns”, 
“Coarse+Patterns”,  and “Fine+Patterns”  of Table
5. The results show that by leveraging syntactico- 
semantic structures, we obtain significant F-measure 
improvements of 8.2, 4.6, and 3.6 for binary, coarse- 
grained, and fine-grained  relation predictions  re- 
spectively.

7   Analysis

We first show statistics regarding  the syntactico- 
semantic structures. In Section 3, we mentioned 
that ACE-2004 identified five  types of structures: 
premodifier,  possessive, preposition, formulaic, and


Figure 4: Improvement in (predicted mention) RE.

Pat
ter
n 
ty
pe
Re
c
%
Pr
e
%
Pr
e
M
od
8
6.
8
7
9.
7
Po
ss
9
4.
3
8
8.
3
Pr
ep
9
4.
6
2
0.
0
Fo
rm
ul
a
8
5.
5
6
2.
2

Table 6: Recall and precision of the patterns.


verbal. On the 4,011 examples that we experimented 
on, premodifiers  are the most frequent,  account- 
ing for 30.5% of the examples (or about 1,224 ex- 
amples).  The occurrence distributions  of the other 
structures are 18.9% (possessive), 23.9% (preposi- 
tion), 7.2% (formulaic),  and 19.5% (verbal). Hence, 
the four syntactico-semantic structures that we fo- 
cused on in this paper account for a large majority 
(80%) of the relations.
  In Section 6, we note that out of 55,520 men- 
tion pairs, only 4,011 exhibit valid relations.  Thus, 
the proportion of positive relation examples is very 
sparse at 7.2%. If we can effectively  identify and 
discard most of the negative relation  examples, it 
should improve RE performance, including yielding 
training  data with a more balanced label distribution.
  We now analyze the utility of the patterns. As 
shown in Table 6, the patterns are effective in infer- 
ring the structure of mention pairs. For instance, ap- 
plying the premodifier  patterns on the 55,520 men- 
tion pairs, we correctly identified 86.8% of the 1,224 
premodifier  occurrences as premodifiers,  while in- 
curring a false-positive rate of only about 20%6. We

  6 Random  selection  will  give  a precision  of about 2.2% 
(1,224 out of 55,520) and thus a false-positive rate of 97.8%


note that preposition structures are relatively  harder 
to identify. Some of the reasons are due to possi- 
bly multiple prepositions in between a mention pair, 
preposition sense ambiguity, pp-attachment ambigu- 
ity, etc. However, in general, we observe that infer- 
ring the structures allows us to discard a large por- 
tion of the mention pairs which have no valid re- 
lation between them. The intuition behind this is 
the following: if we infer that there is a syntactico- 
semantic structure  between a mention  pair, then it 
is likely that the mention pair exhibits  a valid rela- 
tion. Conversely, if there is a valid relation between 
a mention  pair, then it is likely that there exists  a 
syntactico-semantic structure between the mentions.
  Next, we repeat the experiments in Section 6.2 
and Section 6.3, while gradually increasing  the 
amount of training data  used for training the RE 
classifiers. The detailed results of using 5% and 80% 
of all available data are shown in Table 4 and Table
5. Note that these settings are with respect to all 345 
documents and thus the 80% setting represents us- 
ing all 276 training documents in each fold. We plot 
the intermediate results in Figure 3 and Figure 4. We 
note that leveraging the structures provides improve- 
ments on all experimental settings. Also, intuitively, 
the binary predictions benefit the most from lever- 
aging the structures. How to further exploit this is a 
possible future work.

8   Conclusion

In this paper, we propose  a novel algorithmic ap- 
proach to RE by exploiting  syntactico-semantic 
structures. We show that this approach provides 
several advantages and improves  RE performance. 
There are several interesting directions for future 
work. There are probably  many near misses when 
we apply our structure patterns on predicted men- 
tions. For instance, for both premodifier and posses- 
sive structures, we require that one mention  com- 
pletely includes the other.   Relaxing this might 
potentially recover additional  valid mention pairs 
and improve performance. We could also try to 
learn classifiers to automatically identify  and disam- 
biguate  between the different syntactico-semantic 
structures. It will also be interesting to feedback the 
predictions of the structure patterns to the mention 
entity typing classifier and possibly retrain to obtain


a better classifier.

Acknowledgements     This research is supported by 
the Defense Advanced  Research Projects  Agency 
(DARPA)  Machine Reading  Program under Air 
Force Research Laboratory  (AFRL) prime contract 
no.  FA8750-09-C-0181.   Any opinions, findings, 
and conclusion or recommendations  expressed in 
this material  are those of the author(s) and do not 
necessarily reflect the view of the DARPA,  AFRL, 
or the US government.
  We thank Ming-Wei Chang and Quang Do for 
building the mention extraction system.


References

Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, 
Jenifer C. Lai, and Robert L. Mercer. 1992. Class- 
based n-gram models of natural language. Computa- 
tional Linguistics, 18(4):467–479.
Yee Seng Chan and Dan Roth. 2010. Exploiting back- 
ground knowledge for relation extraction. In Proceed- 
ings of the International Conference on Computational 
Linguistics (COLING), pages 152–160.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang- 
Rui Wang, and Chih-Jen Lin.   2008.  Liblinear: A 
library for large linear classification.  Journal of Ma- 
chine Learning Research, 9:1871–1874.
Katrin Fundel, Robert Ku¨ ffner, and Ralf Zimmer. 2007.
Relex – Relation  extraction  using dependency parse 
trees. Bioinformatics,  23(3):365–371.
Mark A. Greenwood  and Mark Stevenson.   2006. Im- 
proving semi-supervised acquisition of relation extrac- 
tion patterns. In Proceedings of the COLING-ACL 
Workshop on Information Extraction Beyond The Doc- 
ument, pages 29–35.
Jing Jiang and ChengXiang  Zhai. 2007. A systematic 
exploration of the feature space for relation extraction. 
In Proceedings of Human  Language Technologies - 
North American Chapter of the Association for Com- 
putational Linguistics (HLT-NAACL), pages 113–120.
Jing Jiang.   2009.   Multi-task transfer learning for 
weakly-supervised relation extraction. In Proceedings 
of the Annual Meeting of the Association for Com- 
putational Linguistics and International Joint Confer- 
ence on Natural Language Processing (ACL-IJCNLP), 
pages 1012–1020.
Nanda Kambhatla.  2004. Combining lexical, syntactic, 
and semantic features with maximum entropy mod- 
els for information extraction.  In Proceedings of the 
Annual Meeting of the Association for Computational 
Linguistics (ACL), pages 178–181.


Dan Klein and Christoper D. Manning.  2003. Fast exact 
inference with a factored model for natural language 
parsing. In The Conference on Advances in Neural 
Information Processing  Systems (NIPS),  pages 3–10.
Longhua Qian, Guodong Zhou, Qiaomin Zhu, and Peide 
Qian.  2008.  Relation extraction using convolution 
tree kernel expanded with entity features. In Pacific 
Asia Conference on Language, Information  and Com- 
putation,  pages 415–421.
Lev Ratinov and Dan Roth.  2009. Design challenges and 
misconceptions in named entity recognition. In Pro- 
ceedings of the Annual Conference on Computational 
Natural Language Learning (CoNLL), pages 147–155. 
Dan Roth and Wen Tau Yih. 2007. Global inference for 
entity and relation identification via a linear program- 
ming formulation. In Lise Getoor and Ben Taskar, ed- 
itors, Introduction to Statistical Relational Learning.
MIT Press.
Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation extrac- 
tion. In Proceedings of the Annual Meeting of the As- 
sociation for Computational Linguistics (ACL), pages
427–434.
Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, and Ji- 
Rong Wen. 2009. Statsnowball:  a statistical approach 
to extracting entity relationships. In The International 
World  Wide Web Conference, pages 101–110.













