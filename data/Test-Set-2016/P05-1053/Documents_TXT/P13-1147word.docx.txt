Embedding Semantic Similarity in Tree Kernels for Domain Adaptation 
of Relation Extraction





Barbara Plank∗
Center for Language Technology 
University of Copenhagen, Denmark 
bplank@gmail.com


Alessandro Moschitti
QCRI - Qatar Foundation & DISI - 
University of Trento, Italy 
amoschitti@qf.org.qa










Abstract

Relation Extraction (RE) is the task of 
extracting semantic relationships between 
entities in text.  Recent studies on rela- 
tion extraction are mostly supervised. The 
clear drawback of supervised methods is 
the need of training data: labeled data is 
expensive to obtain,  and there is often a 
mismatch  between the training data and 
the data the system  will  be applied to. 
This is the problem of domain adapta- 
tion. In this paper, we propose to combine 
(i) term generalization  approaches such as 
word clustering and latent semantic anal- 
ysis (LSA) and (ii) structured kernels to 
improve the adaptability of relation ex- 
tractors to new text genres/domains.  The 
empirical evaluation  on ACE 2005 do- 
mains shows that a suitable combination 
of syntax and lexical generalization is very 
promising for domain adaptation.

1   Introduction

Relation extraction is  the task of extracting  se- 
mantic relationships between entities in text, e.g. 
to detect an employment  relationship between the 
person Larry Page and the company Google in 
the following text snippet: Google CEO Larry 
Page holds a press announcement at its headquar- 
ters in New York on May 21, 2012. Recent stud- 
ies on relation extraction have shown that super- 
vised approaches based on either feature or ker- 
nel methods achieve state-of-the-art accuracy (Ze- 
lenko et al., 2002; Culotta and Sorensen, 2004;
   ∗ The first author was affiliated  with the Department of 
Computer Science and Information  Engineering of the Uni- 
versity of Trento (Povo, Italy) during the design of the mod- 
els, experiments and writing of the paper.


Zhang et al., 2005; Zhou et al., 2005; Zhang et 
al., 2006; Bunescu, 2007; Nguyen et al., 2009; 
Chan and Roth, 2010; Sun et al., 2011). How- 
ever, the clear drawback of supervised methods is 
the need of training data, which can slow down 
the delivery of commercial  applications in new 
domains: labeled data is expensive to obtain, and 
there is often a mismatch between the training  data 
and the data the system will be applied to.  Ap- 
proaches that can cope with domain  changes are 
essential.  This is the problem of domain adapta- 
tion (DA) or transfer learning (TL). Technically, 
domain adaptation addresses the problem  of learn- 
ing when the assumption of independent and iden- 
tically distributed (i.i.d.) samples is violated. Do- 
main adaptation has been studied extensively dur- 
ing the last couple of years for various NLP tasks, 
e.g. two shared tasks have been organized on do- 
main adaptation for dependency parsing (Nivre et 
al., 2007; Petrov and McDonald,  2012). Results 
were mixed, thus it is still a very active research 
area.
  However, to the best of our knowledge,  there 
is almost no work on adapting relation extraction 
(RE) systems to new domains.1 There are some 
prior studies  on the related  tasks of multi-task 
transfer learning (Xu et al., 2008; Jiang, 2009) 
and distant supervision (Mintz et al., 2009), which 
are clearly related but different: the former is the 
problem of how to transfer knowledge from old 
to new relation types, while distant supervision 
tries to learn new relations from unlabeled text 
by exploiting weak-supervision in the form of a 
knowledge  resource (e.g. Freebase).  We assume 
the same relation  types but a shift in the underlying

  1 Besides an unpublished manuscript of a student project, 
but it is not clear what data was used. http://tinyurl.com/ 
bn2hdwk




1498

Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1498–1507, 
Sofia, Bulgaria, August 4-9 2013. Qc 2013 Association for Computational Linguistics


data distribution. Weak supervision is a promis- 
ing approach to improve a relation extraction sys- 
tem, especially to increase its coverage in terms of 
types of relations covered. In this paper we ex- 
amine the related issue of changes in the underly- 
ing data distribution,  while keeping the relations 
fixed.  Even a weakly supervised system is ex- 
pected to perform well when applied to any kind of 
text (other domain/genre), thus ideally, we believe 
that combining domain adaptation with weak su- 
pervision is the way to go in the future. This study 
is a first step towards this.

  We focus on unsupervised domain adaptation, 
i.e. no labeled target data. Moreover, we consider 
a  particular domain adaptation  setting: single- 
system DA, i.e. learning  a single  system able to 
cope with different but related domains. Most 
studies on DA so far have focused  on building 
a specialized  system for every specific target do- 
main, e.g. Blitzer et al. (2006). In contrast, the 
goal here is to build a single system that can ro- 
bustly handle several domains,  which is in line 
with the setup of the recent shared task on pars- 
ing the web (Petrov and McDonald,  2012). Par- 
ticipants were asked to build a single system that 
can robustly  parse all domains (reviews, weblogs, 
answers, emails, newsgroups), rather than to build 
several domain-specific systems. We consider this 
as a shift in what was considered domain adapta- 
tion in the past (adapt from source to a specific tar- 
get) and what can be considered a somewhat dif- 
ferent recent view of DA, that became widespread 
since 2011/2012.  The latter assumes that the tar- 
get domain(s) is/are not really known in advance. 
In this setup, the domain adaptation problem boils


“US”, are terms indicating  an employment rela- 
tion between a person and a location.  Rather than 
only matching the surface string of words, lexi- 
cal similarity enables soft matches between similar 
words in convolution  tree kernels. In the empir- 
ical evaluation on Automatic Content Extraction 
(ACE) data, we evaluate the impact of convolu- 
tion tree kernels embedding lexical semantic sim- 
ilarities. The latter is derived in two ways with: 
(a) Brown word clustering (Brown et al., 1992); 
and (b) Latent Semantic Analysis (LSA). We first 
show that our system aligns well with the state of 
the art on the ACE 2004 benchmark.  Then, we 
test our RE system on the ACE 2005 data, which 
exploits kernels, structures and similarities  for do- 
main adaptation. The results show that combining 
the huge space of tree fragments generalized at the 
lexical level provides an effective model for adapt- 
ing RE systems to new domains.

2   Semantic Syntactic Tree Kernels

In kernel-based methods, both learning and classi- 
fication only depend on the inner product between 
instances. Kernel functions can be efficiently  and 
implicitly computed by exploiting the dual formu-
lation: 'L.i=1..l yiαiφ(oi)φ(o) + b  = 0, where oi
and o are two objects, φ is a mapping  from an ob-
ject to a feature vector x-+i and φ(oi)φ(o) = K (oi, o) 
is a kernel function implicitly defining  such a map- 
ping. In case of structural kernels, K determines 
the shape of the substructures describing the ob- 
jects. Commonly  used kernels in NLP are string 
kernels (Lodhi et al., 2002) and tree kernels (Mos- 
chitti, 2006; Moschitti, 2008).

NP


down to finding a more  robust  system (Søgaard
and Johannsen, 2012),  i.e. one wants to build a 
system that can robustly handle any kind of data.

We propose to combine (i) term generalization



NP E1
NNP



PP

IN 	NP
→
from	E2



NP NP   PP NP



NP

NP	PP

E1	E1




NNP Texas


approaches and (ii) structured kernels to improve 
the performance of a  relation extractor on new 
domains.  Previous studies have shown that lexi-


governor


NNP Texas


NP  PP

E1


NNP

governor


NNP

governor


. . .


cal and syntactic features are both very important 
(Zhang et al., 2006). We combine structural fea- 
tures with lexical information generalized by clus- 
ters or similarity. Given the complexity of feature 
engineering, we exploit kernel methods (Shawe- 
Taylor and Cristianini, 2004). We encode word 
clusters or similarity in tree kernels,  which, in 
turn, produce  spaces of tree fragments. For ex- 
ample, “president”, “vice-president”  and “Texas”,


Figure 1: Syntactic tree kernel (STK).

  Syntactic tree kernels (Collins and Duffy, 2001) 
compute the similarity between  two trees T1 
and T2 by counting  common  sub-trees (cf. Fig- 
ure 1), without enumerating the whole fragment 
space.  However,  if two trees have similar sub- 
structures that employ different though related ter- 
minal nodes, they will not be matched. This is


clearly a limitation. For instance, the fragments 
corresponding to governor from Texas and 
head  of  Maryland are  intuitively semanti- 
cally related and should obtain a  higher match 
when compared to mother of  them.
  Semantic syntactic tree kernels (Bloehdorn 
and Moschitti, 2007a; Bloehdorn and Moschitti,
2007b; Croce et al., 2011) provide one way to ad- 
dress this problem by introducing similarity σ that 
allows soft matches between words and, conse- 
quently, between fragments containing them. Let 
N1  and N2  be the set of nodes in T1 and T2, re- 
spectively. Moreover,  let Ii(n)  be an indicator 
variable that is 1 if subtree i is rooted at n and
0 otherwise.  The syntactic semantic convolution 
kernel T Kσ  (Bloehdorn and Moschitti, 2007b) 
over T 1 and T 2 is computed  as T Kσ (T 1, T 2) =
'L.n1 ∈N1 ,n2 ∈N2 ∆σ (n1, n2) where ∆σ (n1, n2)  =
'L.n1 ∈N1 'L.n2 ∈N2 'L.i Ii(n1)Ii(n2) is computed ef-
ficiently using the following  recursive  defini-
tion:		i)  If   the  nodes n1    and n2    are  ei- 
ther different or have  different number of chil- 
dren  then  ∆σ (n1, n2) 	=	0;	else ii)   If 
n1   and n2   are  pre-terminals   then ∆σ (n1, n2) 
j=1	∆σ (ch(n1, j), ch(n2, j)),   where σ
measures the similarity between the correspond-
ing children of n1 and n2; iii) If n1 and n2 have 
identical children: ∆σ (n1, n2) = λ    nc(n1 ) (1 +


weighting  (Jiang and Zhai, 2007) to approaches 
that change the feature representation (Daume´ III,
2007) or try  to exploit  pivot features  to find 
a generalized shared representation between do- 
mains (Blitzer et al., 2006). The easy-adapt ap- 
proach presented in Daume´ III (2007) assumes the 
supervised adaptation setting and is thus not ap- 
plicable here.  Structural  correspondence learn- 
ing (Blitzer et al., 2006) exploits  unlabeled data 
from both source and target domain to find cor- 
respondences among features from different do- 
mains. These correspondences are then integrated 
as new features in the labeled data of the source 
domain. The key to SCL is to exploit pivot fea- 
tures to automatically identify feature correspon- 
dences, and as such is applicable to feature-based 
approaches but not in our case since we do not as- 
sume availability of target domain data. Instead, 
we apply  a similar idea where we exploit an en- 
tire unlabeled  corpus  as pivot, and compare our 
approach to instance weighting  (Jiang and Zhai,
2007).
  Instance  weighting is a  method for domain 
adaptation in which instance-dependent weights 
are  assigned  to the loss function that is mini- 
mized during the training  process. Let l(x, y, θ) 
be some loss function. Then,  as shown in Jiang 
and Zhai (2007), the loss function can be weighted
Pt (xi )


∆σ (ch(n1, j)), ch(n2, j)); else ∆σ (n1, n2) = 0.


by βil(x, y, θ), such that βi   =


Ps (xi ) , where Ps


T Kσ combines generalized lexical with structural 
information: it allows matching tree fragments 
that have the same syntactic structure but differ in
their terminals. After introducing related work, we


and Pt  are the source and target distributions,  re-
spectively.  Huang et al. (2007)  present an appli- 
cation of instance weighting to support vector ma- 
chines by minimizing the following re-weighted


will discuss computational  structures for RE and


function: minθ,ξ  1 ||θ||2


+ C 'L.m    βiξi.  Finding


their extension with semantic similarity.

3   Related Work

Semantic syntactic  tree kernels have been previ- 
ously used for question classification (Bloehdorn 
and Moschitti, 2007a; Bloehdorn  and Moschitti,
2007b; Croce et al., 2011). These kernels have 
not yet been studied for either domain adaptation 
or RE. Brown clusters were studied previously for 
feature-based approaches to RE (Sun et al., 2011; 
Chan and Roth, 2010), but they were not yet eval- 
uated in kernels. Thus, we present a novel applica- 
tion of semantic syntactic tree kernels and Brown 
clusters for domain adaptation of tree-kernel based 
relation extraction.
  Regarding  domain adaptation,  several meth- 
ods have been proposed,  ranging  from instance


a good  weight function is non-trivial (Jiang and
Zhai, 2007) and several approximations  have been 
evaluated in the past, e.g. Søgaard and Haulrich 
(2011)  use a bigram-based  text classifier to dis- 
criminate between domains. We will use a binary 
classifier trained on RE instance representations.

4   Computational Structures for RE

A common way to represent a constituency-based 
relation instance is the PET (path-enclosed-tree), 
the smallest subtree including  the two target enti- 
ties (Zhang et al., 2006). This is basically the for- 
mer structure PAF2  (predicate argument feature) 
defined in Moschitti (2004) for the extraction of 
predicate argument relations. The syntactic rep-

  2 It is the smallest subtree enclosing the predicate and one 
of its argument node.


resentation used by Zhang et al. (2006) (we will 
refer to it as PET Zhang) is the PET with enriched 
entity information:  e.g. E1-NAM-PER,  including 
entity type (PER, GPE, LOC, ORG) and mention 
type (NAM, NOM, PRO, PRE: name, nominal, 
pronominal or premodifier). An alternative ker- 
nel that does not use syntactic information  is the 
Bag-of-Words (BOW) kernel, where a single root 
node is added above the terminals. Note that in 
this BOW kernel we actually mark target entities 
with E1/E2. Therefore, our BOW kernel can be 
considered an enriched BOW model. If we do not 
mark target entities, performance drops consider- 
ably, as discussed later.
  As shown by Zhang et al. (2006), includ- 
ing gold-standard information on entity and men- 
tion type substantially improves relation extrac- 
tion performance. We will  use this gold infor- 
mation also in Section 6.1 to show that our sys- 
tem aligns well to the state of the art on the ACE
2004 benchmark.  However, in a realistic setting 
this information is not available or noisy. In fact, 
as we discuss later, excluding  gold entity informa- 
tion decreases system performance  considerably. 
In the case of porting a system  to new domains 
entity information will be unreliable or missing. 
Therefore, in our domain adaptation experiments 
on the ACE 2005 data (Section 6.3) we will not 
rely on this gold information but rather train a sys- 
tem using PET (target mentions only marked with 
E1/E2 and no gold entity label).3

4.1   Syntactic Semantic Structures

Combining syntax with semantics has a clear ad-


allow the system to perform better in the target 
domain. The question remains how to establish a 
link between the semantic similarity  in the source 
and target domain. We propose to use an entire 
unlabeled  corpus  as pivot:  this corpus must be 
general enough to encapsulate the source and tar- 
get domains of interest. The idea is to (i) learn 
semantic similarity between words on the pivot 
corpus and (ii) use tree kernels embedding  such 
a similarity to learn  a RE system on the source, 
which allows to generalize to the new target do- 
main. This reasoning is related to Structural Cor- 
respondence Learning (SCL) (Blitzer et al., 2006). 
In SCL, a representation shared across domains is 
learned by exploiting pivot features,  where  a set 
of pivot features has to be selected (usually  a few 
thousands).  In our case pivots are words that co- 
occur with the target words in a large unlabeled 
corpus and are thus implicitly represented in the 
similarity matrix. Thus, in contrast to SCL, we do 
not need to select a set of pivot features but rather 
rely on the distributional  hypothesis to infer a se- 
mantic similarity from a large unlabeled corpus. 
Then, this similarity is incorporated into the tree 
kernel that provides the necessary restriction  for 
an effective  semantic similarity calculation.  One 
peculiarity of our work is that we exploit a large 
amount of general data, i.e. data gathered from the 
web, which is a different  but also more challeng- 
ing scenario than the general unsupervised DA set- 
ting where domain specific data is available.  We 
study two ways for term generalization in tree ker- 
nels: Brown words clusters and Latent Semantic 
Analysis (LSA), both briefly described next.


vantage: it generalizes lexical information  encap- 
sulated in syntactic  parse trees, while at the same 
time syntax guides semantics in order to obtain an



a) replace pos

NP



b) replace word c) above pos

..
..


effective semantic similarity.  In fact, lexical infor- 
mation is highly affected by data-sparseness, thus 
tree kernels combined with semantic information 
created from additional  resources should provide 
a way to obtain a more robust system.
We exploit this idea here for domain adaptation


NP E1
1101100011 officials


PP

10001110	NP

from	E2

1111100110

Seoul


NP E2
NNP

1111100110


NP

E2

1111100110

NNP Seoul


(DA): if words are generalized by semantic simi-
larity LS, then in a hypothetical  world changing
LS such that it reflects the target domain would

  3 In a setup where gold label info is included, the impact 
of similarity-based  methods is limited – gold information 
seems to predominate.  We argue that whenever gold data is 
not available, distributional semantics paired with kernels can 
be useful to improve generalization and complement missing 
gold info.



Figure 2: Integrating Brown cluster information

  The Brown algorithm (Brown et al., 1992) is 
a hierarchical agglomerative hard-clustering algo- 
rithm. The path from the root of the tree down to 
a leaf node is represented compactly as a bitstring. 
By cutting the hierarchy at different levels one can 
obtain different granularities of word clusters. We


evaluate different  ways to integrate cluster infor- 
mation into tree kernels, some of which are illus- 
trated in Figure 2.
  For LSA, we compute term similarity functions 
following the distributional hypothesis  (Harris,
1964), i.e. the meaning of a word can be described 
by the set of textual contexts in which it appears. 
The original word-by-word  context matrix M  is 
decomposed through Singular Value Decomposi- 
tion (SVD) (Golub and Kahan, 1965), where M 
is approximated by Ul Sl V T .   This approxima- 
tion supplies  a way to project a generic term wi
into the l-dimensional space using W = Ul S1/2,


sentence,  which are separated  by no more than 
three other mentions (Zhang et al., 2006; Sun et 
al., 2011). After data preprocessing, we obtained
4,327 positive and 39,120 negative instances.

A
C
E 
20
05
do
cs	sents	ASL	relations
n
w
+
b
n
b
c
 
c
t
s
 
w
l
2
9
8	5029	18.8	3562
5
2
	2267	16.3	1297
3
4
	2696	15.3	603
1
1
4	1697	22.6	677

Table 1: Overview of the ACE 2005 data.


For the domain adaptation experiments we use


where  each row corresponds to the vectors


w-+i.


Given two words w1 and w2, the term similarity 
function σ is estimated as the cosine similarity be- 
tween the corresponding projections w-+1, w-+2  and 
used in the kernel as described in Section 2.

5   Experimental Setup

We treat relation  extraction  as a multi-class classi- 
fication  problem and use SVM-light-TK4 to train 
the binary classifiers. The output of the classifiers 
is combined using the one-vs-all approach. We 
modified the SVM-light-TK package to include 
the semantic  tree kernels  and instance  weight- 
ing. The entire software package is publicly avail- 
able.5  For the SVMs,  we use the same parameters 
as Zhang et al. (2006):  λ = 0.4, c = 2.4 using the 
Collins Kernel (Collins  and Duffy, 2001). The pre- 
cision/recall trade-off parameter for the none class 
was found on held-out data: j = 0.2.  Evalua- 
tion metrics are standard micro average Precision, 
Recall and balanced Fscore (F1). To compute sta- 
tistical significance, we use the approximate  ran- 
domization test (Noreen, 1989).6  In all our exper- 
iments, we model argument order of the relations 
explicitly. Thus, for instance for the 7 coarse ACE
2004 relations, we build 14 coarse-grained classi- 
fiers (two for each coarse ACE 2004 relation type 
except for PER-SOC, which is symmetric, and one 
classifier for the none relation).

Data  We use two datasets.   To compare our 
model against the state of the art we use the ACE
2004 data. It contains 348 documents and 4,374 
positive relation instances. To generate the train-


the ACE 2005 corpus. An overview of the data
is given in Table 1.  Note that this data is dif- 
ferent from ACE 2004: it covers different  years 
(ACE 2004: texts from 2001-2002; ACE 2005:
2003-2005). Moreover, the annotation guidelines 
have changed (for example, ACE 2005 contains no 
discourse relation,  some relation (sub)types have 
changed/moved, and care must be taken for differ- 
ences in SGM markup, etc.).
  More importantly, the ACE 2005 corpus cov- 
ers additional  domains:  weblogs, telephone con- 
versation,  usenet and broadcast conversation.   In 
the experiments, we use news (the union of nw 
and bn) as source domain, and weblogs (wl), tele- 
phone conversations (cts) and broadcast conversa- 
tion (bc) as target domains.7    We take half of bc 
as only target development  set, and leave the re- 
maining  data and domains for final testing (since 
they are already small, cf. Table 1). To get a feel- 
ing of how these domains differ, Figure 3 depicts 
the distribution  of relations in each domain and Ta- 
ble 2 provides the most frequent out-of-vocabulary 
words together with their percentage.

Lexical Similarity and Clustering We applied 
LSA to ukWaC (Baroni et al., 2009),  a 2 billion 
word corpus constructed from the Web8  using the 
s-space toolkit.9 Dimensionality reduction was 
performed using SVD with 250 dimensions, fol- 
lowing (Croce et al., 2011). The co-occurrence 
matrix was transformed by tfidf.  For the Brown 
word clusters, we used Percy Liang’s implemen- 
tation10 of the Brown clustering algorithm (Liang,
2005). We incorporate cluster information by us-


ing data,  we follow prior studies and extract an	 	


instance for every pair of mentions in the same

4 http://disi.unitn.it/moschitti/Tree- Kernel.htm
5 http://disi.unitn.it/ikernels/RelationExtraction
6 http://www.nlpado.de/˜sebastian/software/sigf.shtml
  

7 We did not consider the usenet subpart, since it is among 
the smaller domains and data-preprocessing was difficult.
8 http://wacky.sslmit.unibo.it/
9 http://code.google.com/p/airhead- research/
10 https://github.com/percyliang/brown- cluster



Distribution of relations across domains (normalized)

ART  GEN−AFF 
ORG−AFF 
PART−WHOLE 
PER−SOC 
PHYS











nw_bn	bc 	cts 	wl

Domain


Figure 3: Distribution of relations in ACE 2005.


D
o
m
M
os
t 
fr
eq
ue
nt 
O
O
V 
w
or
d
s
bc
(2
4
%
)
ins
ur
an
ce
,  
un
int
ell
igi
bl
e, 
m
al
pr
ac
-
tic
e, 
ph
, 
cli
p, 
co
lo
ne
l, 
cr
os
st
al
k
ct
s
(3
4
%
)
uh
, 
Ye
ah
, 
u
m, 
eh
, 
m
h
m, 
uh
-
hu
h, 
˜,
ah
, 
m
m, 
th, 
pl
o, 
to
pi
c, 
y, 
w
or
kp
la
ce
wl
(4
9
%
)
titl
e,   
St
ar
bu
ck
s, 
W
ell
,   
bl
og
,  
!!,
w
er
kh
ei
se
r, 
un
de
fe
at
ed
, 
po
or, 
sh
it

Table 2: For each domain the percentage of target 
domain words (types) that are unseen in the source 
together with the most frequent OOV words.


ing the 10-bit cluster prefix (Sun et al., 2011; Chan 
and Roth, 2010). For the domain adaptation exper- 
iments, we use ukWaC corpus-induced clusters as 
bridge between domains. We limited the vocabu- 
lary to that in ACE 2005, which are approximately
16k words. Following previous work, we left case 
intact in the corpus and induced 1,000 word clus- 
ters from words appearing at least 100 times.11

DA baseline We compare our approach to in- 
stance weighting (Jiang and Zhai, 2007). We mod- 
ified SVM-light-TK such that it takes a parameter 
vector βi, .., βm as input,  where each βi represents 
the relative importance of example i with respect 
to the target domain (Huang et al., 2007; Wid- 
mer, 2008). To estimate the importance weights, 
we train a binary classifier that distinguishes be- 
tween source and target domain  instances. We 
consider the union of the three target domains as 
target data. To train the classifier, the source in- 
stances are marked  as negative  and the target in- 
stances are marked as positive.   Then, this classi-

  11 Clusters are available at http://disi.unitn.it/ikernels/ 
RelationExtraction


Table 3: Comparison to previous work on the 7 re- 
lations of ACE 2004. K: kernel-based; F: feature- 
based; yes/no: models argument order explicitly.


fier is applied to the source data. To obtain the 
weights βi, we convert the SVM scores into pos- 
terior probabilities by training a sigmoid using the 
modified Platt algorithm (Lin et al., 2007).12

6   Results

6.1   Alignment to Prior Work

Although most prior studies  performed  5-fold 
cross-validation on ACE 2004, it is often not clear 
whether the partitioning  has been done on the in- 
stance or on the document level. Moreover, it is 
often not stated whether argument order is mod- 
eled explicitly, making it difficult to compare sys- 
tem performance.  Citing Wang (2008), “We feel 
that there is a sense of increasing confusion down 
this line of research”. To ease comparison  for fu- 
ture research we use the same 5-fold split on the 
document level as Sun et al. (2011)13   and make 
our system publicly  available (see Section 5).
  Table 3 shows that our system (bottom) aligns 
well with the state  of the art.   Our best sys- 
tem (composite  kernel with polynomial expan- 
sion) reaches an F1 of 70.1, which aligns well to 
the 70.4 of Sun et al. (2011) that use the same data- 
split. This is slightly behind that of Zhang (2006); 
the reason might be threefold:  i) different  data par- 
titioning; ii) different pre-processing; iii) they in- 
corporate features from additional  sources, i.e. a 
phrase chunker, dependency parser and semantic 
resources (Zhou et al., 2005) (we have on aver- 
age 9 features/instance, they use 40).  Since we 
focus on evaluating the impact of semantic simi- 
larity in tree kernels, we think our system is very 
competitive.  Removing gold entity and mention

  12 Other weightings/normalizations (like LDA) didn’t im- 
prove the results; best was to take the posteriors and add c.
13 http://cs.nyu.edu/˜asun/pub/ACL11_CVFileList.txt


information  results in a significant  F1 drop from
66.3% to 54.2%. However, in a realistic setting 
we do not have gold entity info available,  espe- 
cially not in the case when  we apply the system 
to any kind of text. Thus, in the domain adapta- 
tion setup we assume entity  boundaries given but 
not their label. Clearly, evaluating the approach on 
predicted mentions, e.g. Giuliano et al. (2007), is 
another important dimension, however, out of the 
scope of the current paper.

6.2   Tree Kernels with Brown Word Clusters

To evaluate the effectiveness of Brown word clus- 
ters in tree kernels, we evaluated different instance 
representations (cf. Figure 2) on the ACE 2005 de- 
velopment set. Table 4 shows the results.

bc
-
de
v
P
R
F
1
ba
se
lin
e
52
.2
41
.7
46
.4
re
pla
ce 
w
or
d
49
.7
38
.6
43
.4
re
pla
ce 
po
s
56
.3
41
.9
48
.0
re
pla
ce 
po
s 
on
ly 
m
en
tio
ns
55
.3
41
.6
47
.5
ab
ov
e 
w
or
d
54
.5
42
.2
47
.6
ab
ov
e 
po
s
55
.8
41
.1
47
.3

Table 4: Brown clusters in tree kernels (cf. Fig 2).


  To summarize, we found: i) it is generally a bad 
idea to dismiss  lexical information completely, 
i.e. replacing or ignoring terminals harms perfor- 
mance; ii) the best way to incorporate Brown clus- 
ters is to replace the Pos tag with the cluster bit- 
string; iii) marking all words is generally better 
than only mentions; this is in contrast to Sun et 
al. (2011) who found that in their feature-based 
system it was better  to add cluster information 
to entity mentions only. As we will discuss, the 
combination of syntax and semantics exploited in 
this novel kernel avoids the necessity of restricting 
cluster information to mentions only.

6.3   Semantic Tree Kernels for DA

To evaluate the effectiveness of the proposed ker- 
nels across domains, we use the ACE 2005 data 
as testbed. Following  standard practices on ACE
2004, the newswire (nw) and broadcast news (bn) 
data from ACE 2005 are considered training  data 
(labeled source domain). The test data consists 
of three targets: broadcast conversation, telephone 
conversation, weblogs. As we want to build a sin- 
gle system that is able to handle  heterogeneous 
data, we do not assume that there is further unla-


beled domain-specific  data, but we assume to have 
a large unlabeled corpus (ukWaC) at our disposal 
to improve the generalizability of our models.
  Table 5 presents the results. In the first three 
rows we see  the performance  of  the baseline 
models (PET, BOW and BOW without mark- 
ing).  In-domain (col 1): when evaluated on the 
same domain the system was trained on (nw+bn,
5-fold cross-validation). Out-of-domain perfor- 
mance (cols 2-4):  the system evaluated  on the 
targets, namely broadcast conversation (bc), tele- 
phone conversation (cts) and weblogs (wl). While 
the system  achieves  a performance   of 46.0 F1 
within its own domain, the performance drops to
45.3, 43.4 and 34.0 F1 on the target domains, re- 
spectively.  The BOW kernel that disregards syn- 
tax is often less effective  (row 2).  We see also 
the effect of target entity marking:  the BOW ker- 
nel without entity marking performs substantially 
worse (row 3). For the remaining experiments we 
use the BOW  kernel with entity marking.
  Rows 4 and 5 of Table 5 show the effect of 
using instance  weighting for the PET baseline. 
Two models  are shown: they differ in whether 
PET or BOW was used as instance  representa- 
tion for training the discriminative classifier. In- 
stance weighting  shows mixed results: it helps 
slightly on the weblogs domain, but does not help 
on broadcast conversation and telephone conversa- 
tions. Interestingly, the two models used to obtain 
the weights perform similarly,  despite the fact that 
their performance differs (F1: 70.5 BOW, 73.5
PET); it turns out that the correlation between the 
weights is high (+0.82).
  The next part (rows 6-9) shows the effect of en- 
riching the syntactic structures with either Brown 
word clusters  or LSA. The Brown cluster ker- 
nel applied to PET (P WC) improves performance 
over the baseline over all target domains. The 
same holds also for the lexical semantic kernel 
based on LSA (P LSA), however, to only two out 
of three domains.  This suggests that the two ker- 
nels capture different information  and a combined 
kernel might be effective.   More importantly, the 
table shows the effect of adding Brown clusters or 
LSA semantics to the BOW kernel: it can actually 
hurt performance, sometimes to a small but other 
times to a considerably  degree. For instance, WC 
applied to PET achieves an F1 of 47.0 (baseline:
45.3) on the bc domain, while applied to BOW it 
hurts performance significantly,  i.e. it drops from



n
w
+
b
n
 
(
i
n
-
d
o
m
.
)
Baseli
ne:	P:	R:	F1:
b
c
P
:
	R:	F1:
c
t
s
P
:	R:	F1:
w
l
P
:	R:	F1:
P
E
T
B
O
W
B
O
W 
no 
m
ar
ki
ng
50
.6	42.1	46.0
55
.1	37.3	44.5
49
.6	34.6	40.7
51
.2	40.6	45.3
57
.2	37.1	45.0
51
.5	34.7	41.4
51
.0	37.8	43.4
57
.5	31.8	41.0
54
.6	30.7	39.3
35
.4	32.8	34.0
41
.1	27.2	32.7
37
.6	25.7	30.6
P
E
T 
a
d
a
p
te
d
: 
I
W
1 
(u
si
n
g 
P
E
T
) 
I
W
2 
(u
si
n
g 
B
O
W
)
P:	R:	F:
51
.4	44.1	47.4
51
.2	43.6	47.1
P:	R:	F:
49
.1	41.1	44.7
49
.1	41.3	44.9
P:	R:	F:
50
.8	37.5	43.1
51
.2	37.8	43.5
P:	R:	F:
35
.5	33.9	34.7
35
.6	33.8	34.7
W
i
t
h
 
S
i
m
i
l
a
r
i
t
y
:
P
 
W
C
 
B
 
W
C
 
P
 
L
S
A
 
B
 
L
S
A
P:	R:	F1:
55
.4	44.6	49.4
47
.9	36.4	41.4
52
.3	44.1	47.9
53
.7	37.8	44.4
P:	R:	F1:
54
.3	41.4	47.0
49
.5	35.2	41.2
51
.4	41.7	46.0
55
.1	33.8	41.9
P:	R:	F1:
55
.9	37.1	44.6
53
.3	33.2	40.9
49
.7	36.5	42.1
54
.9	32.3	40.7
P:	R:	F1:
40
.0	32.7	36.0
31
.7	24.1	27.4
38
.1	36.5	37.3
39
.2	28.6	33.0
P+
P 
W
C
P+
P 
LS
A
55
.0	46.5	50.4
52
.7	46.6	49.5
54
.4	43.4	48.3
53
.9	45.2	49.2
54
.1	38.1	44.7
49
.9	37.6	42.9
38
.4	34.5	36.3
37
.9	38.3	38.1
P+
P 
W
C+
P 
LS
A
55
.1	45.9	50.1
55
.3	43.1	48.5†
53
.1	37.0	43.6
39
.9	35.8	37.8†

Table 5: In-domain (first column) and out-of-domain performance (columns two to four) on ACE 2005. 
PET and BOW are abbreviated by P and B, respectively. If not specified BOW is marked.




45.0 to 41.2. This is also the case for LSA ap- 
plied to the BOW kernel, which drops to 41.9. On 
the cts domain this is less pronounced.   Only on 
the weblogs domain B LSA achieves a minor im- 
provement (from 32.7 to 33.0). In general, dis- 
tributional semantics constrained by syntax (i.e. 
combined with PET) can be effectively  exploited, 
while if applied ‘blindly’ – without the guide of 
syntax (i.e. BOW) – performance might drop, of- 
ten considerably. We believe that the semantic in- 
formation does not help the BOW  kernel as there is 
no syntactic information  that constrains the appli- 
cation of the noisy source, as opposed to the case 
with the PET kernel.

  As  the two  semantically enriched kernels, 
PET LSA and PET WC, seem to capture different 
information we use composite  kernels (rows 10-
11): the baseline kernel (PET) summed with the 
lexical semantic kernels. As we can see, results 
improve further: for instance on the bc test set, 
PET WC reaches an F1 of 47.0, while combined 
with PET (PET+PET WC) this improves to 48.3. 
Adding also PET LSA results in the best perfor- 
mance and our final system (last row): the com- 
posite kernel (PET+PET WC+PET LSA) reaches 
an F1 of 48.5, 43.6 and 37.8 on the target domains, 
respectively, i.e. with an absolute improvement of:
+3.2%, +0.2% and +3.8%, respectively.  Two out 
of three improvements are significant  at p < 0.05 
(indicated by † in Table 5). Moreover, the system 
also improved  in its own domain (first column),


therefore having achieved robustness.
  By performing an error analysis we found that, 
for instance, the Brown clusters help to general- 
ize locations  and professions.   For example, the 
baseline incorrectly  considered ‘Dutch  filmmaker’ 
in a PART-WHOLE   relation, while our system 
correctly predicted GEN-AFF(filmmaker,Dutch).
‘Filmmaker’ does not appear in the source, how- 
ever ‘Dutch citizen’ does. Both ‘citizen’ and ‘film- 
maker’ appear in the same cluster, thereby helping 
the system to recover the correct relation.


Rel
ati
on:
b
c
BL 
	SYS
c
t
s
BL 
	SYS
w
l
BL 
	SYS
PA
RT-
W
HO
LE
O
R
G
-
A
F
F
 
P
H
Y
S
 
A
R
T
G
E
N
-
A
F
F
 
P
E
R
-
S
O
C
µ 
ave
rag
e
37.
8   
43.
1
60.
7   
62.
9
35.
3   
37.
6
20.
8   
37.
9
30.
1   
33.
0
74.
1   
74.
2
45.
3   
48.
5
59.
3   
52.
3
35.
5   
42.
3
25.
4   
28.
7
34.
5   
43.
5
16.
8   
18.
6
66.
3   
63.
1
43.
4   
43.
6
30.
5   
36.
3
41.
0   
42.
0
25.
2   
26.
9
26.
5   
40.
3
21.
6   
28.
1
42.
6   
48.
0
34.
0   
37.
8

Table 6:   F1 per coarse relation type (ACE
2005).   SYS is the final model, i.e. last row
(PET+PET WC+PET LSA) of Table 5.

  Furthermore, Table 6 provides the performance 
breakdown per relation for the baseline (BL) and 
our best system (SYS).  The table shows that our 
system is able to improve F1 on all relations for 
the broadcast and weblogs  data. On most rela- 
tions, this is also the case for the telephone (cts) 
data, although the overall improvement is not sig- 
nificant.  Most errors were made on the PER-SOC


relation, which constitutes the largest portion of 
cts (cf. Figure 3). As shown in the same figure, 
the relation distribution of the cts domain is also 
rather different from the source. This conversation 
data is a very hard domain, with a lot of disflu- 
encies and spoken language patterns.  We believe 
it is more distant from the other domains,  espe- 
cially from the unlabeled collection, thus other ap- 
proaches might be more appropriate, e.g. domain 
identification  (Dredze et al., 2010).

7   Conclusions and Future  Work

We proposed syntactic tree kernels enriched by 
lexical semantic similarity to tackle the portabil- 
ity of a  relation extractor to different domains. 
The results of diverse kernels exploiting (i) Brown 
clustering and (ii) LSA show that a suitable com- 
bination of syntax and lexical generalization is 
very promising for domain adaptation.  The pro- 
posed system is able to improve performance sig- 
nificantly on two out of three target domains (up 
to 8% relative improvement).  We compared it to 
instance weighting, which gave only modest or 
no improvements.  Brown clusters remained un- 
explored for kernel-based approaches.  We saw 
that adding cluster information  blindly might ac- 
tually hurt performance.  In contrast, adding lex- 
ical information combined with syntax can help 
to improve performance:  the syntactic structure 
enriched with lexical information  provides a fea- 
ture space where syntax constrains lexical  similar- 
ity obtained from unlabeled data. Thus, seman- 
tic syntactic  tree kernels appear to be a suitable 
mechanism to adequately trade off the two kinds 
of information. In future we plan to extend the 
evaluation to predicted mentions, which necessar- 
ily includes a careful evaluation of pre-processing 
components, as well as evaluating the approach on 
other semantic tasks.

Acknowledgments

We would like  to thank Min Zhang for discus- 
sions on his prior work as  well as  the anony- 
mous reviewers for their valuable feedback.  The 
research described  in this paper  has been  sup- 
ported by the European  Community’s  Seventh 
Framework Programme (FP7/2007-2013) under 
the grant #288024:  LIMOSINE  – Linguistically 
Motivated Semantic aggregation engiNes.


References

Marco Baroni, Silvia Bernardini, Adriano Ferraresi, 
and Eros Zanchetta.  2009. The WaCky Wide Web: 
A Collection of Very Large Linguistically Processed 
Web-Crawled Corpora. Language Resources and 
Evaluation, pages 209–226.

John Blitzer, Ryan McDonald,  and Fernando Pereira.
2006.  Domain Adaptation with Structural Corre- 
spondence Learning. In Conference on Empirical 
Methods in Natural Language Processing, Sydney, 
Australia.

Stephan Bloehdorn and Alessandro Moschitti. 2007a.
Combined syntactic and semantic kernels for text 
classification. In ECIR, pages 307–318.

Stephan Bloehdorn and Alessandro Moschitti. 2007b.
Exploiting Structure  and Semantics for Expressive 
Text Kernels. In Conference on Information Knowl- 
edge and Management, Lisbon, Portugal.

Peter F. Brown, Peter V. deSouza, Robert L. Mercer, 
Vincent J. Della Pietra, and Jenifer C. Lai.  1992. 
Class-Based n-gram Models of Natural Language. 
Computational Linguistics, 18:467–479.

Razvan C. Bunescu. 2007. Learning to extract rela- 
tions from the web using minimal supervision.  In 
Proceedings of ACL.

Yee  Seng Chan  and Dan Roth.   2010.  Exploiting 
background knowledge for relation extraction.   In 
Proceedings of the 23rd International Conference 
on Computational Linguistics (Coling 2010), pages
152–160, Beijing, China, August. Coling 2010 Or- 
ganizing Committee.

Michael Collins and Nigel Duffy.   2001.  Convolu- 
tion Kernels for Natural Language. In Proceedings 
of Neural Information Processing  Systems (NIPS
2001).

Danilo Croce, Alessandro  Moschitti, and Roberto 
Basili.  2011.  Semantic convolution  kernels over 
dependency trees: smoothed partial tree kernel. In 
CIKM, pages 2013–2016.

Aron Culotta and Jeffrey Sorensen. 2004. Dependency 
tree kernels for relation extraction.  In Proceedings 
of the 42nd Annual Meeting on ACL, Barcelona, 
Spain.

Hal Daume´ III. 2007. Frustratingly easy domain adap- 
tation. In Proceedings of the 45th Annual Meeting of 
ACL, pages 256–263, Prague, Czech Republic,  June.

Mark Dredze, Tim Oates, and Christine  Piatko. 2010.
We’re not in kansas  anymore: Detecting domain 
changes in streams. In Proceedings of EMNLP, 
pages 585–595, Cambridge,  MA.

Claudio Giuliano, Alberto Lavelli, and Lorenza Ro- 
mano. 2007. Relation extraction and the influence 
of automatic named-entity recognition. ACM Trans. 
Speech Lang. Process., 5(1):2:1–2:26, December.


G. Golub and W. Kahan. 1965. Calculating the singu- 
lar values and pseudo-inverse of a matrix. Journal of 
the Society for Industrial and Applied Mathematics: 
Series B, Numerical  Analysis, 2(2):pp. 205–224.

Zellig Harris. 1964. Distributional structure. In Jer- 
rold J. Katz and Jerry A. Fodor, editors, The Philos- 
ophy of Linguistics. Oxford University Press.

Jiayuan Huang, Arthur Gretton, Bernhard Scho¨ lkopf, 
Alexander   J. Smola, and Karsten M. Borgwardt.
2007. Correcting sample selection bias by unlabeled 
data. In In NIPS. MIT Press.

Jing Jiang and Chengxiang Zhai.   2007.   Instance 
weighting for domain adaptation in NLP. In In ACL
2007, pages 264–271.

Jing Jiang.  2009.  Multi-task transfer learning  for 
weakly-supervised relation extraction.  In Proceed- 
ings of the Joint Conference  of the 47th Annual 
Meeting of the ACL and the 4th IJCNLP, pages
1012–1020, Suntec, Singapore.

Percy Liang.  2005.  Semi-Supervised Learning  for 
Natural Language. Master’s  thesis, Massachusetts 
Institute of Technology.

Hsuan-Tien Lin, Chih-Jen Lin, and Ruby C. Weng.
2007.  A note on platt’s  probabilistic  outputs for 
support vector machines. Mach. Learn., 68(3):267–
276.

Huma Lodhi, Craig Saunders,  John Shawe-Taylor, 
Nello Cristianini,  and Chris Watkins. 2002. Text 
classification using string kernels. Journal of Ma- 
chine Learning  Research, pages 419–444.

Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju- 
rafsky. 2009. Distant supervision for relation ex- 
traction without labeled data.  In Proceedings of 
ACL-IJCNLP, pages 1003–1011,  Suntec, Singapore, 
August.

Alessandro Moschitti. 2004. A study on convolution 
kernels for shallow semantic parsing. In Proceed- 
ings of the 42nd Meeting of the ACL, Barcelona, 
Spain.

Alessandro Moschitti. 2006. Efficient convolution ker- 
nels for dependency and constituent syntactic trees. 
In Proceedings of the 17th ECML, Berlin, Germany.

Alessandro Moschitti. 2008. Kernel methods, syntax 
and semantics for relational text categorization.  In 
CIKM, pages 253–262.

Truc-Vien T. Nguyen, Alessandro  Moschitti, and 
Giuseppe Riccardi. 2009. Convolution  kernels on 
constituent,  dependency and sequential  structures 
for relation extraction.  In Proceedings of EMNLP
’09, pages 1378–1387, Stroudsburg, PA, USA.

J. Nivre, J. Hall, S. Ku¨ bler, R. McDonald,  J. Nilsson, 
S. Riedel, and D. Yuret. 2007. The CoNLL 2007 
shared task on dependency parsing.  In Proceed- 
ings of the CoNLL  Shared Task Session of EMNLP- 
CoNLL,  pages 915–932.


Eric W. Noreen. 1989. Computer-Intensive Methods 
for Testing Hypotheses:  An Introduction.  Wiley- 
Interscience.

Slav Petrov and Ryan McDonald. 2012. Overview of 
the 2012 shared task on parsing the web. Notes of 
the First Workshop on Syntactic Analysis of Non- 
Canonical Language (SANCL).

John Shawe-Taylor  and Nello Cristianini. 2004. Ker- 
nel Methods for Pattern Analysis.  Cambridge Uni- 
versity Press.

Anders Søgaard and  Martin  Haulrich.	2011.
Sentence-level instance-weighting for graph-based 
and transition-based  dependency parsing. In Pro- 
ceedings of the 12th International Conference on 
Parsing Technologies,  IWPT ’11,  pages 43–47, 
Stroudsburg, PA, USA.

Anders Søgaard and Anders Johannsen. 2012. Robust 
learning in random subspaces: equipping  NLP for 
OOV effects. In Proceedings of Coling.

Ang Sun, Ralph Grishman, and Satoshi Sekine. 2011.
Semi-supervised relation extraction with large-scale 
word clustering. In Proceedings of ACL-HLT, pages
521–529, Portland, Oregon, USA.

Mengqiu Wang. 2008. A re-examination of depen- 
dency path kernels for relation extraction.  In Pro- 
ceedings of the 3rd International Joint Conference 
on Natural Language Processing-IJCNLP.

Christian Widmer.   2008.   Domain adaptation  in 
sequence  analysis.   Diplomarbeit, University of 
Tu¨ bingen.

Feiyu Xu, Hans Uszkoreit, Hond Li, and Niko Felger.
2008. Adaptation of relation extraction rules to new 
domains. In Proceedings of LREC’08, Marrakech, 
Morocco.

Dmitry  Zelenko, Chinatsu Aone,  and Anthony 
Richardella.	2002.  Kernel  methods for relation 
extraction.  In Proceedings of EMNLP-ACL,  pages
181–201.

Min Zhang, Jian Su, Danmei Wang, Guodong Zhou, 
and Chew Lim Tan.  2005. Discovering relations 
between named entities from a large raw corpus us- 
ing tree similarity-based clustering.  In Proceedings 
of IJCNLP’2005, pages 378–389, Jeju Island, South 
Korea.

Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou.
2006. A composite kernel to extract relations be- 
tween entities with both flat and structured features. 
In Proceedings of COLING-ACL  2006, pages 825–
832.

GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation ex- 
traction. In Proceedings of the 43rd Annual Meeting 
of ACL), pages 427–434, Ann Arbor, Michigan.

