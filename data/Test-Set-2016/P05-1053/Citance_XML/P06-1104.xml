<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">This paper proposes a novel composite kernel for relation extraction.</S>
		<S sid ="2" ssid = "2">The composite kernel consists of two individual kernels: an entity kernel that allows for entity-related features and a convolution parse tree kernel that models syntactic information of relation examples.</S>
		<S sid ="3" ssid = "3">The motivation of our method is to fully utilize the nice properties of kernel methods to explore diverse knowledge for relation extraction.</S>
		<S sid ="4" ssid = "4">Our study illustrates that the composite kernel can effectively capture both flat and structured features without the need for extensive feature engineering, and can also easily scale to include more features.</S>
		<S sid ="5" ssid = "5">Evaluation on the ACE corpus shows that our method outperforms the previous best-reported methods and significantly outperforms previous two dependency tree kernels for relation extraction.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="6" ssid = "6">The goal of relation extraction is to find various predefined semantic relations between pairs of entities in text.</S>
			<S sid ="7" ssid = "7">The research on relation extraction has been promoted by the Message Understanding Conferences (MUCs) (MUC, 1987 1998) and Automatic Content Extraction (ACE) program (ACE, 20022005).</S>
			<S sid ="8" ssid = "8">According to the ACE Program, an entity is an object or set of objects in the world and a relation is an explicitly or implicitly stated relationship among entities.</S>
			<S sid ="9" ssid = "9">For example, the sentence “Bill Gates is chairman and chief software architect of Microsoft Corporation.” conveys the ACE-style relation “EMPLOYMENT.exec” between the entities “Bill Gates” (PERSON.Name) and “Microsoft Corporation” (ORGANIZATION.</S>
			<S sid ="10" ssid = "10">Commercial).</S>
			<S sid ="11" ssid = "11">In this paper, we address the problem of relation extraction using kernel methods (Schölkopf and Smola, 2001).</S>
			<S sid ="12" ssid = "12">Many feature-based learning algorithms involve only the dot-product between feature vectors.</S>
			<S sid ="13" ssid = "13">Kernel methods can be regarded by replacing the dot-product with a kernel function between two vectors, or even between two objects.</S>
			<S sid ="14" ssid = "14">A kernel function is a similarity function satisfying the properties of being symmetric and positive-definite.</S>
			<S sid ="15" ssid = "15">Recently, kernel methods are attracting more interests in the NLP study due to their ability of implicitly exploring huge amounts of structured features using the original representation of objects.</S>
			<S sid ="16" ssid = "16">For example, the kernels for structured natural language data, such as parse tree kernel (Collins and Duffy, 2001), string kernel (Lodhi et al., 2002) and graph kernel (Suzuki et al., 2003) are example instances of the well- known convolution kernels1 in NLP.</S>
			<S sid ="17" ssid = "17">In relation extraction, typical work on kernel methods includes: Zelenko et al.</S>
			<S sid ="18" ssid = "18">(2003), Culotta and Sorensen (2004) and Bunescu and Mooney (2005).</S>
			<S sid ="19" ssid = "19">This paper presents a novel composite kernel to explore diverse knowledge for relation extraction.</S>
			<S sid ="20" ssid = "20">The composite kernel consists of an entity kernel and a convolution parse tree kernel.</S>
			<S sid ="21" ssid = "21">Our study demonstrates that the composite kernel is very effective for relation extraction.</S>
			<S sid ="22" ssid = "22">It alsoshows without the need for extensive feature en gineering the composite kernel can not only capture most of the flat features used in the previous work but also exploit the useful syntactic structure features effectively.</S>
			<S sid ="23" ssid = "23">An advantage of our method is that the composite kernel can easily cover more knowledge by introducing more kernels.</S>
			<S sid ="24" ssid = "24">Evaluation on the ACE corpus shows that our method outperforms the previous best- reported methods and significantly outperforms the previous kernel methods due to its effective exploration of various syntactic features.</S>
			<S sid ="25" ssid = "25">The rest of the paper is organized as follows.</S>
			<S sid ="26" ssid = "26">In Section 2, we review the previous work.</S>
			<S sid ="27" ssid = "27">Section 3 discusses our composite kernel.</S>
			<S sid ="28" ssid = "28">Section 4 reports the experimental results and our observations.</S>
			<S sid ="29" ssid = "29">Section 5 compares our method with the 1 Convolution kernels were proposed for a discrete structure.</S>
			<S sid ="30" ssid = "30">by Haussler (1999) in the machine learning field.</S>
			<S sid ="31" ssid = "31">This framework defines a kernel between input objects by applying convolution “sub-kernels” that are the kernels for the decompositions (parts) of the objects.</S>
			<S sid ="32" ssid = "32">825 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 825–832, Sydney, July 2006.</S>
			<S sid ="33" ssid = "33">Qc 2006 Association for Computational Linguistics previous work from the viewpoint of feature exploration.</S>
			<S sid ="34" ssid = "34">We conclude our work and indicate the future work in Section 6.</S>
	</SECTION>
	<SECTION title="Related Work. " number = "2">
			<S sid ="35" ssid = "1">Many techniques on relation extraction, such as rule-based (MUC, 19871998; Miller et al., 2000), feature-based (Kambhatla 2004; Zhou et al., 2005) and kernel-based (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), have been proposed in the literature.</S>
			<S sid ="36" ssid = "2">Rule-based methods for this task employ a number of linguistic rules to capture various relation patterns.</S>
			<S sid ="37" ssid = "3">Miller et al.</S>
			<S sid ="38" ssid = "4">(2000) addressed the task from the syntactic parsing viewpoint and integrated various tasks such as POS tagging, NE tagging, syntactic parsing, template extraction and relation extraction using a generative model.</S>
			<S sid ="39" ssid = "5">Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052 ) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.</S>
			<S sid ="40" ssid = "6">These methods are very effective for relation extraction and show the best- reported performance on the ACE corpus.</S>
			<S sid ="41" ssid = "7">However, the problems are that these diverse features have to be manually calibrated and the hierarchical structured information in a parse tree is not well preserved in their parse tree-related features, which only represent simple flat path information connecting two entities in the parse tree through a path of non-terminals and a list of base phrase chunks.</S>
			<S sid ="42" ssid = "8">Prior kernel-based methods for this task focus on using individual tree kernels to exploit tree structure-related features.</S>
			<S sid ="43" ssid = "9">Zelenko et al.</S>
			<S sid ="44" ssid = "10">(2003) developed a kernel over parse trees for relation extraction.</S>
			<S sid ="45" ssid = "11">The kernel matches nodes from roots to leaf nodes recursively layer by layer in a top- down manner.</S>
			<S sid ="46" ssid = "12">Culotta and Sorensen (2004) generalized it to estimate similarity between dependency trees.</S>
			<S sid ="47" ssid = "13">Their tree kernels require the match- able nodes to be at the same layer counting from the root and to have an identical path of ascending nodes from the roots to the current nodes.</S>
			<S sid ="48" ssid = "14">The two constraints make their kernel high precision but very low recall on the ACE 2003 corpus.</S>
			<S sid ="49" ssid = "15">Bunescu and Mooney (2005) proposed another dependency tree kernel for relation extraction.</S>
			<S sid ="50" ssid = "16">Their kernel simply counts the number of common word classes at each position in the shortest paths between two entities in dependency trees.</S>
			<S sid ="51" ssid = "17">The kernel requires the two paths to have the same length; otherwise the kernel value is zero.</S>
			<S sid ="52" ssid = "18">Therefore, although this kernel shows performance improvement over the previous one (Culotta and Sorensen, 2004), the constraint makes the two dependency kernels share the similar behavior: good precision but much lower recall on the ACE corpus.</S>
			<S sid ="53" ssid = "19">The above discussion shows that, although kernel methods can explore the huge amounts of implicit (structured) features, until now the feature-based methods enjoy more success.</S>
			<S sid ="54" ssid = "20">One may ask: how can we make full use of the nice properties of kernel methods and define an effective kernel for relation extraction?</S>
			<S sid ="55" ssid = "21">In this paper, we study how relation extraction can benefit from the elegant properties of kernel methods: 1) implicitly exploring (structured) features in a high dimensional space; and 2) the nice mathematical properties, for example, the sum,product, normalization and polynomial expan sion of existing kernels is a valid kernel (Schölkopf and Smola, 2001).</S>
			<S sid ="56" ssid = "22">We also demonstrate how our composite kernel effectively captures the diverse knowledge for relation extraction.</S>
	</SECTION>
	<SECTION title="Composite  Kernel  for  Relation  Ex-. " number = "3">
			<S sid ="57" ssid = "1">traction In this section, we define the composite kernel and study the effective representation of a relation instance.</S>
			<S sid ="58" ssid = "2">3.1 Composite Kernel.</S>
			<S sid ="59" ssid = "3">Our composite kernel consists of an entity kernel and a convolution parse tree kernel.</S>
			<S sid ="60" ssid = "4">To our knowledge, convolution kernels have not been explored for relation extraction.</S>
			<S sid ="61" ssid = "5">(1) Entity Kernel: The ACE 2003 data defines four entity features: entity headword, entity type and subtype (only for GPE), and mention type while the ACE 2004 data makes some modifications and introduces a new feature “LDC mention type”.</S>
			<S sid ="62" ssid = "6">Our statistics on the ACE data reveals that the entity features impose a strong constraint on relation types.</S>
			<S sid ="63" ssid = "7">Therefore, we design a linear kernel to explicitly capture such features: 2 We classify the feature-based kernel defined in (Zhao and.</S>
			<S sid ="64" ssid = "8">K L (R1 , R2 ) = ∑i =1,2 K E (R1.Ei , R2 .Ei ) (1) Grishman, 2005) into the feature-based methods since their kernels can be easily represented by the dot-products between explicit feature vectors.</S>
			<S sid ="65" ssid = "9">where R1 and R2 stands for two relation instances, Ei means the ith entity of a relation instance, and K E (•, •) is a simple kernel function over the features of entities: where nc(n1 ) is the child number of n1 , ch(n,j) is the jth child of node n and λ (0&lt; λ &lt;1) is the de K E (E1 , E2 ) = ∑i C (E1.</S>
			<S sid ="66" ssid = "10">fi , E2 . fi ) th (2) cay factor in order to make the kernel value less variable with respect to the subtree sizes.</S>
			<S sid ="67" ssid = "11">In ad where fi represe nts the i entity feature, and the dition, the recursi ve rule (3) holds becaus e givenfunction C(•, •) returns 1 if the two feature val two nodes with the same children, one can construct common sub-trees using these children and ues are identical and 0 otherwise.</S>
			<S sid ="68" ssid = "12">K E (•, •)re common sub-trees of further offspring.</S>
			<S sid ="69" ssid = "13">turns the number of feature values in common of two entities.</S>
			<S sid ="70" ssid = "14">(2) Convolution Parse Tree Kernel: A convolution kernel aims to capture structured information in terms of substructures.</S>
			<S sid ="71" ssid = "15">Here we use the same convolution parse tree kernel as described in Collins and Duffy (2001) for syntactic parsing and Moschitti (2004) for semantic role labeling.</S>
			<S sid ="72" ssid = "16">Generally, we can represent a parse tree T by a vector of integer counts of each sub-tree type The parse tree kernel counts the number of common sub-trees as the syntactic similarity measure between two relation instances.</S>
			<S sid ="73" ssid = "17">The time complexity for computing this kernel is O(| N1 | ⋅| N 2 |) .In this paper, two composite kernels are de fined by combing the above two individual kernels in the following ways: 1) Linear combination: (regardless of its ancestors): K (R , R ) = α • Kˆ (R , R ) + (1−α ) • Kˆ (T ,T ) (4) φ(T ) = (# subtree1(T), …, # subtreei(T), …, # Here, Kˆ (•, •) is the normalized 3 K (•, •) and α subtreen(T) ) where # subtreei(T) is the occurrence number of the ith sub-tree type (subtreei) in T. Since the number of different sub-trees is exponential with the parse tree size, it is computationally infeasi is the coefficient.</S>
			<S sid ="74" ssid = "18">Evaluation on the development set shows that this composite kernel yields the best performance when α is set to 0.4.</S>
			<S sid ="75" ssid = "19">2) Polynomial expansion: ble to directly use the feature vector φ(T ) . To solve this computational issue, Collins and Duffy K2 (R1, R2 ) = α • Kˆ L (R1, R2 ) + (1−α) • Kˆ (T1,T2 ) (5) p (2001) proposed the following parse tree kernel Here, Kˆ (•, •) is the normalized K (•, •) , K (•, •) to calculate the dot product between the above is the polynomial expansion ofK (•, •) with de high dimensional vectors implicitly.</S>
			<S sid ="76" ssid = "20">K (T1 ,T2 ) =&lt; φ (T1 ),φ (T2 ) &gt; = ∑i # subtreei (T1 ) ⋅ # subtreei (T2 ) (3) gree d=2, i.e. K p (•, •) = (K(•,•) +1)2 , and α is the coefficient.</S>
			<S sid ="77" ssid = "21">Evaluation on the development set shows that this composite kernel yields the best performance when α is set to 0.23.</S>
			<S sid ="78" ssid = "22">= ∑i (∑ n ∈N I subtreei ( n1 )) ⋅(∑ n2 ∈N 2 I subtree ( n2 )) The polynomial expansion aims to explore the entity bi-gram features, esp. the combined fea = ∑ n ∈N ∑ n ∈N ∆( n1 , n2 ) where N1 and N2 are the sets of nodes in trees T1 tures from the first and second entities, respectively.</S>
			<S sid ="79" ssid = "23">In addition, due to the different scales of and T2, respectively, and I subtreei (n) is a function the values of the two individual kernels, they are that is 1 iff the subtreei occurs with root at node n and zero otherwise, and ∆(n1 , n2 ) is the number of the common subtrees rooted at n1 and n2, i.e. normalized before combination.</S>
			<S sid ="80" ssid = "24">This can avoid one kernel value being overwhelmed by that of another one.</S>
			<S sid ="81" ssid = "25">The entity kernel formulated by eqn.</S>
			<S sid ="82" ssid = "26">(1) is a ∆(n1 , n2 ) = ∑i I subtreei (n1 ) ⋅ I subtreei (n2 ) proper kernel since it simply calculates the dot product of the entity feature vectors.</S>
			<S sid ="83" ssid = "27">The tree∆(n1 , n2 ) can be computed by the following recur sive rules: (1) if the productions (CFP rules) at n1 and n2 are different, ∆(n1, n2 ) = 0 ; (2) else if both n1 and n2 are pre-terminals (POS tags), ∆(n1, n2 ) =1× λ ; kernel formulated by eqn.</S>
			<S sid ="84" ssid = "28">(3) is proven to be a proper kernel (Collins and Duffy, 2001).</S>
			<S sid ="85" ssid = "29">Since kernel function set is closed under normalization, polynomial expansion and linear combination (Schölkopf and Smola, 2001), the two composite kernels are also proper kernels.</S>
			<S sid ="86" ssid = "30">3 A kernel K ( x, y) can be normalized by dividing it by (3) else, ∆ = λ∏ 1 + ∆ , (n1, n2 ) j =1 (1 (ch(n1, j), ch(n2 , j))) K ( x, x) • K ( y, y) . 3.2 Relation Instance Spaces.</S>
			<S sid ="87" ssid = "31">A relation instance is encapsulated by a parse tree.</S>
			<S sid ="88" ssid = "32">Thus, it is critical to understand which portion of a parse tree is important in the kernel calculation.</S>
			<S sid ="89" ssid = "33">We study five cases as shown in Fig.1.</S>
			<S sid ="90" ssid = "34">(1) Minimum Complete Tree (MCT): the complete sub-tree rooted by the nearest common ancestor of the two entities under consideration.</S>
			<S sid ="91" ssid = "35">(2) Path-enclosed Tree (PT): the smallest common sub-tree including the two entities.</S>
			<S sid ="92" ssid = "36">In other words, the sub-tree is enclosed by the shortest path linking the two entities in the parse tree (this path is also commonly-used as the path tree feature in the feature-based methods).</S>
			<S sid ="93" ssid = "37">(3) Context-Sensitive Path Tree (CPT): the PT extended with the 1st left word of entity 1 and the 1st right word of entity 2.</S>
			<S sid ="94" ssid = "38">(4) Flattened Path-enclosed Tree (FPT): the PT with the single in and out arcs of non- terminal nodes (except POS nodes) removed.</S>
			<S sid ="95" ssid = "39">(5) Flattened CPT (FCPT): the CPT with the single in and out arcs of non-terminal nodes (except POS nodes) removed.</S>
			<S sid ="96" ssid = "40">Fig.</S>
			<S sid ="97" ssid = "41">1 illustrates different representations of an example relation instance.</S>
			<S sid ="98" ssid = "42">T1 is MCT for the relation instance, where the sub-tree circled by a dashed line is PT, which is also shown in T2 for clarity.</S>
			<S sid ="99" ssid = "43">The only difference between MCT and PT lies in that MCT does not allow partial production rules (for example, NPÆPP is a partial production rule while NPÆNP+PP is an entire production rule in the top of T2).</S>
			<S sid ="100" ssid = "44">For instance, only the most-right child in the most-left sub-tree [NP [CD 200] [JJ domestic] [E1-PER …]] of T1 is kept in T2.</S>
			<S sid ="101" ssid = "45">By comparing the performance of T1 and T2, we can evaluate the effect of sub-trees with partial production rules as shown in T2 and the necessity of keeping the whole left and right context sub-trees as shown in T1 in relation extraction.</S>
			<S sid ="102" ssid = "46">T3 is CPT, where the two sub-trees circled by dashed lines are included as the context to T2 and make T3 context-sensitive.</S>
			<S sid ="103" ssid = "47">This is to evaluate whether the limited context information in CPT can boost performance.</S>
			<S sid ="104" ssid = "48">FPT in T4 is formed by removing the two circled nodes in T2.</S>
			<S sid ="105" ssid = "49">This is to study whether and how the elimination of single non-terminal nodes affects the performance of relation extraction.</S>
			<S sid ="106" ssid = "50">T1): MCT T ): PT T3):CPT T4): FPT Figure 1.</S>
			<S sid ="107" ssid = "51">Different representations of a relation instance in the example sentence “…provide benefits to 200 domestic partners of their own workers in New York”, where the phrase type “E1-PER” denotes that the current node is the 1st entity with type “PERSON”, and likewise for the others.</S>
			<S sid ="108" ssid = "52">The relation instance is excerpted from the ACE 2003 corpus, where a relation “SOCIAL.Other-Personal” exists between entities “partners” (PER) and “workers” (PER).</S>
			<S sid ="109" ssid = "53">We use Charniak’s parser (Charniak, 2001) to parse the example sentence.</S>
			<S sid ="110" ssid = "54">To save space, the FCPT is not shown here.</S>
	</SECTION>
	<SECTION title="Experiments. " number = "4">
			<S sid ="111" ssid = "1">4.1 Experimental Setting.</S>
			<S sid ="112" ssid = "2">Data: We use the English portion of both the ACE 2003 and 2004 corpora from LDC in our experiments.</S>
			<S sid ="113" ssid = "3">In the ACE 2003 data, the training set consists of 674 documents and 9683 relation instances while the test set consists of 97 documents and 1386 relation instances.</S>
			<S sid ="114" ssid = "4">The ACE 2003 data defines 5 entity types, 5 major relation types and 24 relation subtypes.</S>
			<S sid ="115" ssid = "5">The ACE 2004 data contains 451 documents and 5702 relation instances.</S>
			<S sid ="116" ssid = "6">It redefines 7 entity types, 7 major relation types and 23 subtypes.</S>
			<S sid ="117" ssid = "7">Since Zhao and Grishman (2005) use a 5-fold cross-validation on a subset of the 2004 data (newswire and broadcast news domains, containing 348 documents and 4400 relation instances), for comparison, we use the same setting (5-fold cross-validation on the same subset of the 2004 data, but the 5 partitions may not be the same) for the ACE 2004 data.</S>
			<S sid ="118" ssid = "8">Both corpora are parsed using Charniak’s parser (Charniak, 2001).</S>
			<S sid ="119" ssid = "9">We iterate over all pairs of entity mentions occurring in the same sentence to generate potential relation instances.</S>
			<S sid ="120" ssid = "10">In this paper, we only measure the performance of relation extraction models on “true” mentions with “true” chaining of coreference (i.e. as annotated by LDC annotators).</S>
			<S sid ="121" ssid = "11">Implementation: We formalize relation extraction as a multi-class classification problem.</S>
			<S sid ="122" ssid = "12">SVM is selected as our classifier.</S>
			<S sid ="123" ssid = "13">We adopt the one vs. others strategy and select the one with the largest margin as the final answer.</S>
			<S sid ="124" ssid = "14">The training parameters are chosen using cross-validation (C=2.4(SVM); λ =0.4(tree kernel)).</S>
			<S sid ="125" ssid = "15">In our implementa tion, we use the binary SVMLight (Joachims, 1998) and Tree Kernel Tools (Moschitti, 2004).</S>
			<S sid ="126" ssid = "16">Precision (P), Recall (R) and F-measure (F) are adopted to measure the performance.</S>
			<S sid ="127" ssid = "17">4.2 Experimental Results.</S>
			<S sid ="128" ssid = "18">In this subsection, we report the experiments of different kernel setups for different purposes.</S>
			<S sid ="129" ssid = "19">(1) Tree Kernel only over Different Relation Instance Spaces: In order to better study the impact of the syntactic structure information in a parse tree on relation extraction, we remove the entity-related information from parse trees by replacing the entity-related phrase types (“E1- PER” and so on as shown in Fig.</S>
			<S sid ="130" ssid = "20">1) with “NP”.</S>
			<S sid ="131" ssid = "21">Table 1 compares the performance of 5 tree kernel setups on the ACE 2003 data using the tree structure information only.</S>
			<S sid ="132" ssid = "22">It shows that: • Overall the five different relation instance spaces are all somewhat effective for relation extraction.</S>
			<S sid ="133" ssid = "23">This suggests that structured syntactic information has good predication power for relation extraction and the structured syntactic information can be well captured by the tree kernel.</S>
			<S sid ="134" ssid = "24">• MCT performs much worse than the others.</S>
			<S sid ="135" ssid = "25">The reasons may be that MCT includes too much left and right context information, which may introduce many noisy features and cause over-fitting (high precision and very low recall as shown in Table 1).</S>
			<S sid ="136" ssid = "26">This suggests that only keeping the complete (not partial) production rules in MCT does harm performance.</S>
			<S sid ="137" ssid = "27">• PT achieves the best performance.</S>
			<S sid ="138" ssid = "28">This meansthat only keeping the portion of a parse tree en closed by the shortest path between entities can model relations better than all others.</S>
			<S sid ="139" ssid = "29">This may be due to that most significant information is with PT and including context information may introduce too much noise.</S>
			<S sid ="140" ssid = "30">Although context may include some useful information, it is still a problem to correctly utilize such useful information in the tree kernel for relation extraction.</S>
			<S sid ="141" ssid = "31">• CPT performs a bit worse than PT.</S>
			<S sid ="142" ssid = "32">In some cases (e.g. in sentence “the merge of company A and company B….”, “merge” is a critical context word), the context information is helpful.</S>
			<S sid ="143" ssid = "33">However, the effective scope of context is hard to determine given the complexity and variability of natural languages.</S>
			<S sid ="144" ssid = "34">• The two flattened trees perform worse than theoriginal trees.</S>
			<S sid ="145" ssid = "35">This suggests that the single non terminal nodes are useful for relation extraction.</S>
			<S sid ="146" ssid = "36">Evaluation on the ACE 2004 data also shows that PT achieves the best performance (72.5/56.7 /63.6 in P/R/F).</S>
			<S sid ="147" ssid = "37">More evaluations with the entity type and order information incorporated into tree nodes (“E1-PER”, “E2-PER” and “E-GPE” as shown in Fig.</S>
			<S sid ="148" ssid = "38">1) also show that PT performs best with 76.1/62.6/68.7 in P/R/F on the 2003 data and 74.1/62.4/67.7 in P/R/F on the 2004 data.</S>
			<S sid ="149" ssid = "39">Instance Spaces P(%) R(%) F Minimum Complete Tree (MCT) 77.5 38.4 51.3 Path-enclosed Tree (PT) 72.8 53.8 61.9 Context-Sensitive PT(CPT) 75.9 48.6 59.2 Flattened PT 72.7 51.7 60.4 Flattened CPT 76.1 47.2 58.2 Table 1.</S>
			<S sid ="150" ssid = "40">five different tree kernel setups on the ACE 2003 five major types using the parse tree structure information only (regardless of any entity-related information)PTs (with Tree Struc P(%) R(%) F Methods (2002/2003 data) P(%) R(%) F ture Information only) Entity kernel only 75.1 42.7 54.4 Ours: composite kernel 2 (polynomial expansion) 77.3 (64.9) 65.6 (51.2) 70.9 (57.2) (79.5) (34.6) (48.2) Tree kernel only 72.5 (72.8) 56.7 (53.8) 63.6 (61.9) Zhou et al.</S>
			<S sid ="151" ssid = "41">(2005): feature-based SVM 77.2 (63.1) 60.7 (49.5) 68.0 (55.5) Composite kernel 1 (linear combination) 73.5 (76.3) 67.0 (63.0) 70.1 (69.1) Kambhatla (2004): feature-based ME (-) (63.5) (-) (45.2) (-) (52.8) Composite kernel 2 76.1 68.4 72.1 (polynomial expansion) (77.3) (65.6) (70.9) Ours: tree kernel with entity information at node 76.1 (62.4) 62.6 (48.5) 68.7 (54.6) Table 2.</S>
			<S sid ="152" ssid = "42">Performance comparison of different kernel setups over the ACE major types of both the 2003 data (the numbers in parentheses) and the 2004 data (the numbers outside parentheses) (2) Composite Kernels: Table 2 compares the Bunescu and Mooney (2005): shortest path dependency kernel Culotta and Sorensen (2004): dependency kernel 65.5 (-) 67.1 (-) 43.8 (-) 35.0 (-) 52.5 (-) 45.8 (-) performance of different kernel setups on the ACE major types.</S>
			<S sid ="153" ssid = "43">It clearly shows that:• The composite kernels achieve significant per formance improvement over the two individual kernels.</S>
			<S sid ="154" ssid = "44">This indicates that the flat and the structured features are complementary and the com Table 3.</S>
			<S sid ="155" ssid = "45">Performance comparison on the ACE 2003/2003 data over both 5 major types (the numbers outside parentheses) and 24 subtypes (the numbers in parentheses) Methods (2004 data) P(%) R(%) F posite kernels can well integrate them: 1) the flat entity information captured by the entity kernel; 2) the structured syntactic connection information between the two entities captured by the tree kernel.</S>
			<S sid ="156" ssid = "46">Ours: composite kernel 2 (polynomial expansion) Zhao and Grishman (2005): feature-based kernel 76.1 (68.6) 69.2 (-) 68.4 (59.3) 70.5 (-) 72.1 (63.6) 70.4 (-)• The composite kernel via the polynomial expansion outperforms the one via the linear com bination by ~2 in F-measure.</S>
			<S sid ="157" ssid = "47">It suggests that the bi-gram entity features are very useful.</S>
			<S sid ="158" ssid = "48">• The entity features are quite useful, which can achieve F-measures of 54.4/48.2 alone and canboost the performance largely by ~7 (70.1 63.2/69.161.9) in F-measure when combining with the tree kernel.</S>
			<S sid ="159" ssid = "49">• It is interesting that the ACE 2004 data shows consistent better performance on all setups than the 2003 data although the ACE 2003 data is two times larger than the ACE 2004 data.</S>
			<S sid ="160" ssid = "50">This may be due to two reasons: 1) The ACE 2004 data defines two new entity types and redefines the relation types and subtypes in order to reduce the inconsistency between LDC annota- tors.</S>
			<S sid ="161" ssid = "51">2) More importantly, the ACE 2004 data defines 43 entity subtypes while there are only 3 subtypes in the 2003 data.</S>
			<S sid ="162" ssid = "52">The detailed classification in the 2004 data leads to significant performance improvement of 6.2 (54.448.2) in F- measure over that on the 2003 data.</S>
			<S sid ="163" ssid = "53">Our composite kernel can achieve 77.3/65.6/70.9 and 76.1/68.4/72.1 in P/R/F over the ACE 2003/2004 major types, respectively.</S>
			<S sid ="164" ssid = "54">Table 4.</S>
			<S sid ="165" ssid = "55">Performance comparison on the ACE 2004 data over both 7 major types (the numbers outside parentheses) and 23 subtypes (the numbers in parentheses) (3) Performance Comparison: Tables 3 and 4 compare our method with previous work on the ACE 2002/2003/2004 data, respectively.</S>
			<S sid ="166" ssid = "56">They show that our method outperforms the previous methods and significantly outperforms the previous two dependency kernels4.</S>
			<S sid ="167" ssid = "57">This may be due to two reasons: 1) the dependency tree (Culotta and Sorensen, 2004) and the shortest path (Bunescu and Mooney, 2005) lack the internal hierarchical phrase structure information, so their corresponding kernels can only carry out node-matching directly over the nodes with word tokens; 2) the parse tree kernel has less constraints.</S>
			<S sid ="168" ssid = "58">That is, it is 4 Bunescu and Mooney (2005) used the ACE 2002.</S>
			<S sid ="169" ssid = "59">corpus, including 422 documents, which is known to have many inconsistencies than the 2003 version.</S>
			<S sid ="170" ssid = "60">Culotta and Sorensen (2004) used a generic ACE corpus including about 800 documents (no corpus version is specified).</S>
			<S sid ="171" ssid = "61">Since the testing corpora are in different sizes and versions, strictly speaking, it is not ready to compare these methods exactly and fairly.</S>
			<S sid ="172" ssid = "62">Therefore Table 3 is only for reference purpose.</S>
			<S sid ="173" ssid = "63">We just hope that we can get a few clues from this table.</S>
			<S sid ="174" ssid = "64">not restricted by the two constraints of the two dependency kernels (identical layer and ancestors for the matchable nodes and identical length of two shortest paths, as discussed in Section 2).</S>
			<S sid ="175" ssid = "65">The above experiments verify the effectiveness of our composite kernels for relation extraction.</S>
			<S sid ="176" ssid = "66">They suggest that the parse tree kernel can effectively explore the syntactic features which are critical for relation extraction.</S>
			<S sid ="177" ssid = "67">and the chunking features) used in the feature- based methods are embedded (as a subset) in our feature space.</S>
			<S sid ="178" ssid = "68">Moreover, the in-between word features and the entity-related features used in the feature-based methods are also captured by the tree kernel and the entity kernel, respectively.</S>
			<S sid ="179" ssid = "69">Therefore our method has the potential of effectively capturing not only most of the previous flat features but also the useful syntactic structure features.</S>
			<S sid ="180" ssid = "70">Error Type # of error instances 2004 data 2003 data (2) Compared with Previous Kernels: Since our method only counts the occurrence of each False Negative 198 416 False Positive 115 171 Cross Type 62 96 Table 5.</S>
			<S sid ="181" ssid = "71">Error distribution of major types on both the 2003 and 2004 data for the composite kernel by polynomial expansion (4) Error Analysis: Table 5 reports the error distribution of the polynomial composite kernel over the major types on the ACE data.</S>
			<S sid ="182" ssid = "72">It shows that 83.5%(198+115/198+115+62) / 85.8%(416 +171/416+171+96) of the errors result from relation detection and only 16.5%/14.2% of the errors result from relation characterization.</S>
			<S sid ="183" ssid = "73">This may be due to data imbalance and sparseness issues since we find that the negative samples are 8 times more than the positive samples in the training set.</S>
			<S sid ="184" ssid = "74">Nevertheless, it clearly directs our future work.</S>
	</SECTION>
	<SECTION title="Discussion. " number = "5">
			<S sid ="185" ssid = "1">In this section, we compare our method with the previous work from the feature engineering viewpoint and report some other observations and issues in our experiments.</S>
			<S sid ="186" ssid = "2">5.1 Comparison with Previous Work.</S>
			<S sid ="187" ssid = "3">This is to explain more about why our method performs better and significantly outperforms the previous two dependency tree kernels from the theoretical viewpoint.</S>
			<S sid ="188" ssid = "4">(1) Compared with Feature-based Methods: The basic difference lies in the relation instance representation (parse tree vs. feature vector) and the similarity calculation mechanism (kernel function vs. dot-product).</S>
			<S sid ="189" ssid = "5">The main difference is the different feature spaces.</S>
			<S sid ="190" ssid = "6">Regarding the parse tree features, our method implicitly represents a parse tree by a vector of integer counts of each sub-tree type, i.e., we consider the entire sub-tree types and their occurring frequencies.</S>
			<S sid ="191" ssid = "7">In this way, the parse tree-related features (the path features sub-tree without considering the layer and the ancestors of the root node of the sub-tree, our method is not limited by the constraints (identical layer and ancestors for the matchable nodes, as discussed in Section 2) in Culotta and Sorensen (2004).</S>
			<S sid ="192" ssid = "8">Moreover, the difference between our method and Bunescu and Mooney (2005) is that their kernel is defined on the shortest path between two entities instead of the entire sub- trees.</S>
			<S sid ="193" ssid = "9">However, the path does not maintain the tree structure information.</S>
			<S sid ="194" ssid = "10">In addition, their kernel requires the two paths to have the same length.</S>
			<S sid ="195" ssid = "11">Such constraint is too strict.</S>
			<S sid ="196" ssid = "12">5.2 Other Issues.</S>
			<S sid ="197" ssid = "13">(1) Speed Issue: The recursively-defined convolution kernel is much slower compared to feature-based classifiers.</S>
			<S sid ="198" ssid = "14">In this paper, the speed issue is solved in three ways.</S>
			<S sid ="199" ssid = "15">First, the inclusion of the entity kernel makes the composite kernel converge fast.</S>
			<S sid ="200" ssid = "16">Furthermore, we find that the small portion (PT) of a full parse tree can effectively represent a relation instance.</S>
			<S sid ="201" ssid = "17">This significantly improves the speed.</S>
			<S sid ="202" ssid = "18">Finally, the parse tree kernel requires exact match between two sub- trees, which normally does not occur very frequently.</S>
			<S sid ="203" ssid = "19">Collins and Duffy (2001) report that in practice, running time for the parse tree kernel is more close to linear (O(|N1|+|N2|), rather than O(|N1|*|N2| ).</S>
			<S sid ="204" ssid = "20">As a result, using the PC with Intel P4 3.0G CPU and 2G RAM, our system only takes about 110 minutes and 30 minutes to do training on the ACE 2003 (~77k training instances) and 2004 (~33k training instances) data, respectively.</S>
			<S sid ="205" ssid = "21">(2) Further Improvement: One of the potential problems in the parse tree kernel is that it carries out exact matches between sub-trees, so that this kernel fails to handle sparse phrases (i.e. “a car” vs. “a red car”) and near-synonymic grammar tags (for example, the variations of a verb (i.e. go, went, gone)).</S>
			<S sid ="206" ssid = "22">To some degree, it could possibly lead to over-fitting and compromise the per formance.</S>
			<S sid ="207" ssid = "23">However, the above issues can be handled by allowing grammar-driven partial rule matching and other approximate matching mechanisms in the parse tree kernel calculation.</S>
			<S sid ="208" ssid = "24">Finally, it is worth noting that by introducing more individual kernels our method can easily scale to cover more features from a multitude of sources (e.g. Wordnet, gazetteers, etc) that can be brought to bear on the task of relation extraction.</S>
			<S sid ="209" ssid = "25">In addition, we can also easily implement the feature weighting scheme by adjusting the eqn.(2) and the rule (2) in calculating ∆(n1 , n2 ) (see subsection 3.1).</S>
	</SECTION>
	<SECTION title="Conclusion and Future Work. " number = "6">
			<S sid ="210" ssid = "1">Kernel functions have nice properties.</S>
			<S sid ="211" ssid = "2">In this paper, we have designed a composite kernel for relation extraction.</S>
			<S sid ="212" ssid = "3">Benefiting from the nice properties of the kernel methods, the composite kernel could well explore and combine the flat entity features and the structured syntactic features, and therefore outperforms previous best- reported feature-based methods on the ACE corpus.</S>
			<S sid ="213" ssid = "4">To our knowledge, this is the first research to demonstrate that, without the need for extensive feature engineering, an individual tree kernel achieves comparable performance with the feature-based methods.</S>
			<S sid ="214" ssid = "5">This shows that the syntactic features embedded in a parse tree are particularly useful for relation extraction and which can be well captured by the parse tree kernel.</S>
			<S sid ="215" ssid = "6">In addition, we find that the relation instance representation (selecting effective portions of parse trees for kernel calculations) is very important for relation extraction.</S>
			<S sid ="216" ssid = "7">The most immediate extension of our work is to improve the accuracy of relation detection.</S>
			<S sid ="217" ssid = "8">This can be done by capturing more features by including more individual kernels, such as the WordNet-based semantic kernel (Basili et al., 2005) and other feature-based kernels.</S>
			<S sid ="218" ssid = "9">We can also benefit from machine learning algorithms to study how to solve the data imbalance and sparseness issues from the learning algorithm viewpoint.</S>
			<S sid ="219" ssid = "10">In the future work, we will design a more flexible tree kernel for more accurate similarity measure.</S>
			<S sid ="220" ssid = "11">Acknowledgements: We would like to thank Dr. Alessandro Moschitti for his great help in using his Tree Kernel Toolkits and fine-tuning the system.</S>
			<S sid ="221" ssid = "12">We also would like to thank the three anonymous reviewers for their invaluable suggestions.</S>
	</SECTION>
</PAPER>
