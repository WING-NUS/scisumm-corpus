<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">This paper presents a method of automatically constructing information extraction patterns on predicate-argument structures (PASs) obtained by full parsing from a smaller training corpus.</S>
		<S sid ="2" ssid = "2">Because PASs represent generalized structures for syntactical variants, patterns on PASs are expected to be more generalized than those on surface words.</S>
		<S sid ="3" ssid = "3">In addition, patterns are divided into components to improve recall and we introduce a Support Vector Machine to learn a prediction model using pattern matching results.</S>
		<S sid ="4" ssid = "4">In this paper, we present experimental results and analyze them on how well protein-protein interactions were extracted from MEDLINE abstracts.</S>
		<S sid ="5" ssid = "5">The results demonstrated that our method improved accuracy compared to a machine learning approach using surface word/part-of-speech patterns.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="6" ssid = "6">One primitive approach to Information Extraction (IE) is to manually craft numerous extraction patterns for particular applications and this is presently one of the main streams of biomedical IE (Blaschke and Valencia, 2002; Koike et al., 2003).</S>
			<S sid ="7" ssid = "7">Although such IE attempts have demonstrated near-practical performance, the same sets of patterns cannot be applied to different kinds of information.</S>
			<S sid ="8" ssid = "8">A real-world task requires several kinds of IE, thus manually engineering extraction Current Afﬁliation: † FUJITSU LABORATORIES LTD. ‡ Faculty of Informatics, Kogakuin University patterns, which is tedious and time-consuming process, is not really practical.</S>
			<S sid ="9" ssid = "9">Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE.</S>
			<S sid ="10" ssid = "10">However, in most cases, the cost of manually crafting patterns is simply transferred to that for constructing a large amount of training data, which requires tedious amount of manual labor to annotate text.</S>
			<S sid ="11" ssid = "11">To systematically reduce the necessary amount of training data, we divided the task of constructing extraction patterns into a subtask that general natural language processing techniques can solve and a subtask that has speciﬁc properties according to the information to be extracted.</S>
			<S sid ="12" ssid = "12">The former subtask is of full parsing (i.e. recognizing syntactic structures of sentences), and the latter subtask is of constructing speciﬁc extraction patterns (i.e. ﬁnding clue words to extract information) based on the obtained syntactic structures.</S>
			<S sid ="13" ssid = "13">We adopted full parsing from various levels of parsing, because we believe that it offers the best utility to generalize sentences into normalized syntactic relations.</S>
			<S sid ="14" ssid = "14">We also divided patterns into components to improve recall and we introduced machine learning with a Support Vector Machine (SVM) to learn a prediction model using the matching results of extraction patterns.</S>
			<S sid ="15" ssid = "15">As an actual IE task, we extracted pairs of interacting protein names from biomedical text.</S>
	</SECTION>
	<SECTION title="Full Parsing. " number = "2">
			<S sid ="16" ssid = "1">2.1 Necessity for Full Parsing.</S>
			<S sid ="17" ssid = "2">A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005).</S>
			<S sid ="18" ssid = "3">Their assertion is 284 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 284–292, Sydney, July 2006.</S>
			<S sid ="19" ssid = "4">Qc 2006 Association for Computational Linguistics Distance Count (%) Sum (%) −1 0 1 2–5 6–10 11– 54 8 170 337 267 248 5.0 0.7 15.7 31.1 24.6 22.9 5.0 5.7 21.4 52.5 77.1 100.0 Distance −1 means protein word has been annotated as interacting with itself (e.g. “actin polymerization”).</S>
			<S sid ="20" ssid = "5">Distance 0 means words of the interacting proteins are directly next to one another.</S>
			<S sid ="21" ssid = "6">Multi-word protein names are concatenated as long as they do not cross tags to annotate proteins.</S>
			<S sid ="22" ssid = "7">Table 1: Distance between Interacting Proteins that shallow parsers are more robust and would be sufﬁcient for IE.</S>
			<S sid ="23" ssid = "8">However, their claims that shallow parsers are sufﬁcient, or that full parsers do not contribute to application tasks, have not been fully proved by experimental results.</S>
			<S sid ="24" ssid = "9">Zhou et al.</S>
			<S sid ="25" ssid = "10">(2005) argued that most information useful for IE derived from full parsing was shallow.</S>
			<S sid ="26" ssid = "11">However, they only used dependency trees and paths on full parse trees in their experiment.</S>
			<S sid ="27" ssid = "12">Such structures did not include information of semantic subjects/objects, which full parsing can recognize.</S>
			<S sid ="28" ssid = "13">Additionally, most relations they extracted from the ACE corpus (Linguistic Data Consortium, 2005) on broadcasts and newswires were within very short word-distance (70% where two entities are embedded in each other or separated by at most one word), and therefore shallow information was beneﬁcial.</S>
			<S sid ="29" ssid = "14">However, Table 1 shows that the word distance is long between interacting protein names annotated on the AImed corpus (Bunescu and Mooney, 2004), and we have to treat long-distance relations for information like protein-protein interactions.</S>
			<S sid ="30" ssid = "15">Full parsing is more effective for acquiring generalized data from long-length words than shallow parsing.</S>
			<S sid ="31" ssid = "16">The sentences at left in Figure 1 exemplify the advantages of full parsing.</S>
			<S sid ="32" ssid = "17">The gerund “activating” in the last sentence takes a non-local semantic subject “ENTITY1”, and shallow parsing cannot recognize this relation because “ENTITY1” and “activating” are in different phrases.</S>
			<S sid ="33" ssid = "18">Full parsing, on the other hand, can identify both the subject of the whole sentence and the semantic subject of “activating” have been shared.</S>
			<S sid ="34" ssid = "19">2.2 Predicate-argument Structures.</S>
			<S sid ="35" ssid = "20">We applied Enju (Tsujii Laboratory, 2005a) as a full parser which outputs predicate-argument structures (PASs).</S>
			<S sid ="36" ssid = "21">PASs are well normalized forms that represent syntactic relations.</S>
			<S sid ="37" ssid = "22">Enju is based on Head-driven Phrase Structure Grammar (Sag and Wasow, 1999), and it has been trained on the Penn Treebank (PTB) (Marcus et al., 1994) and a biomedical corpus, the GENIA Treebank (GTB) (Tsujii Laboratory, 2005b).</S>
			<S sid ="38" ssid = "23">We used a part-of-speech (POS) tagger trained on the GENIA corpus (Tsujii Laboratory, 2005b) as a preprocessor for Enju.</S>
			<S sid ="39" ssid = "24">On predicate-argument relations, Enju achieved 88.0% precision and 87.2% recall on PTB, and 87.1% precision and 85.4% recall on GTB.</S>
			<S sid ="40" ssid = "25">The illustration at right in Figure 1 is a PAS example, which represents the relation between “activate”, “ENTITY1” and “ENTITY2” of all sentences to the left.</S>
			<S sid ="41" ssid = "26">The predicate and its arguments are words converted to their base forms, augmented by their POSs.</S>
			<S sid ="42" ssid = "27">The arrows denote the connections from predicates to their arguments and the types of arguments are indicated as arrow labels, i.e., ARGn (n = 1, 2, . . .), MOD.</S>
			<S sid ="43" ssid = "28">For example, the semantic subject of a transitive verb is ARG1 and the semantic object is ARG2.</S>
			<S sid ="44" ssid = "29">What is important here is, thanks to the strong normalization of syntactic variations, that we can expect that the construction algorithm for extracting patterns that works on PASs will need a much smaller training corpus than those working on surface-word sequences.</S>
			<S sid ="45" ssid = "30">Furthermore, because of the reduced diversity of surface-word sequences at the PAS level, any IE system at this level should demonstrate improved recall.</S>
	</SECTION>
	<SECTION title="Related Work. " number = "3">
			<S sid ="46" ssid = "1">Sudo et al.</S>
			<S sid ="47" ssid = "2">(2003), Culotta and Sorensen (2004) and Bunescu and Mooney (2005) acquired substructures derived from dependency trees as extraction patterns for IE in general domains.</S>
			<S sid ="48" ssid = "3">Their approaches were similar to our approach using PASs derived from full parsing.</S>
			<S sid ="49" ssid = "4">However, one problem with their systems is that they could not treat non-local dependencies such as semantic subjects of gerund constructions (discussed in Section 2), and thus rules acquired from the constructions were partial.</S>
			<S sid ="50" ssid = "5">Bunescu and Mooney (2006) also learned extraction patterns for protein-protein interactions by SVM with a generalized subsequence kernel.</S>
			<S sid ="51" ssid = "6">Their patterns are sequences of words, POSs, entity types, etc., and they heuristically restricted length and word positions of the patterns.</S>
			<S sid ="52" ssid = "7">Al ENTITY1 recognizes and activates ENTITY2.</S>
			<S sid ="53" ssid = "8">ENTITY2 activated by ENTITY1 are not well characterized.</S>
			<S sid ="54" ssid = "9">The herpesvirus encodes a functional ENTITY1 that activates human ENTITY2.</S>
			<S sid ="55" ssid = "10">ENTITY1 can functionally cooperate to synergistically activate ENTITY2.</S>
			<S sid ="56" ssid = "11">The ENTITY1 plays key roles by activating ENTITY2.</S>
			<S sid ="57" ssid = "12">ENTITY1/NANRGac1tivate/VABRGEN2TITY2/NN though they achieved about 60% precision and SentepTnrITcYtee1inininTtrearaincitnsgwCithorppoulsymorphicregionsofMEHNCTIIITY.2 Figure 1: Syntactical Variations of “activate” about 40% recall, these heuristic restrictions could for protein-protein interactions as sequences of CPDaE4rNsingResult CD4p0NNprotepin/NNinterpact/VBwitph/INpolymorphic/JJregiopn/NNof/pINMHCpII/NN not be guaranteed to be applied to other IE tasks.</S>
			<S sid ="58" ssid = "13">1 2 3</S>
	</SECTION>
	<SECTION title="5	6" number = "4">
			<S sid ="59" ssid = "1">Hao et al.</S>
			<S sid ="60" ssid = "2">(2005) learned extraction patterns words, POSs, entity tags and gaps by dynamic ENTITY1 ARG1 ENTITY2 RawPMaOttDern ARG1 ARG1 ARG2 ARG1 ARG2 programming, and reduced/merged them using a minimum description length-based algorithm.</S>
			<S sid ="61" ssid = "3">Although they achieved 79.8% precision and 59.5% recall, sentences in their test corpus have too many positive instances and some of the patterns they claimed to have been successfully constructed went against linguistic or biomedical intuition.</S>
			<S sid ="62" ssid = "4">(e.g. “ENTITY1 and interacts with ENTITY2” should be replaced by a more general pattern because they aimed to reduce the number of patterns.)</S>
			<S sid ="63" ssid = "5">4 Method.</S>
			<S sid ="64" ssid = "6">We automatically construct patterns to extract protein-protein interactions from an annotated training corpus.</S>
			<S sid ="65" ssid = "7">The corpus needs to be tagged to denote which protein words are interacting pairs.</S>
			<S sid ="66" ssid = "8">We follow ﬁve steps in constructing extraction patterns from the training corpus.</S>
			<S sid ="67" ssid = "9">(1) Sentences in the training corpus are parsed into PASs and we extract raw patterns from the PASs.</S>
			<S sid ="68" ssid = "10">(2) We divide the raw patterns to generate both combination and fragmental patterns.</S>
			<S sid ="69" ssid = "11">Because obtained patterns include inappropriate ones (wrongly generated or too general), (3) we apply both kinds of patterns to PASs of sentences in the training corpus, (4) calculate the scores for matching results of combination patterns, and (5) make a prediction model with SVM using these matching results and scores.</S>
			<S sid ="70" ssid = "12">We extract pairs of interacting proteins from a target text in the actual IE phase, in three steps.</S>
			<S sid ="71" ssid = "13">(1) Sentences in the target corpus are parsed into PASs.</S>
			<S sid ="72" ssid = "14">(2) We apply both kinds of extraction patterns to these PASs and (3) calculate scores for combination pattern matching.</S>
			<S sid ="73" ssid = "15">(4) We use the prediction model to predict interacting pairs.</S>
			<S sid ="74" ssid = "16">ENTITY1/NNprotein/NNinteract/VBwith/IN region/NNof/INENTITY2/NN Figure 2: Extraction of Raw Pattern 4.1 Full Parsing and Extraction of.</S>
			<S sid ="75" ssid = "17">Raw Patterns As the ﬁrst step in both the construction phase and application phase of extraction patterns, we parse sentences into PASs using Enju.1 We label all PASs of the protein names as protein PASs.</S>
			<S sid ="76" ssid = "18">After parsing, we extract the smallest set of PASs, which connect words that denote interacting proteins, and make it a raw pattern.</S>
			<S sid ="77" ssid = "19">We take the same method to extract and reﬁne raw patterns as Yakushiji et al.</S>
			<S sid ="78" ssid = "20">(2005).</S>
			<S sid ="79" ssid = "21">Connecting means we can trace predicate argument relations from one protein word to the other in an interacting pair.</S>
			<S sid ="80" ssid = "22">The procedure to obtain a raw pattern (p0 , . . .</S>
			<S sid ="81" ssid = "23">, pn) is as follows: predicate(p): PASs that have p as their argument argument(p): PASs that p has as its arguments 1.</S>
			<S sid ="82" ssid = "24">pi = p0 is the PAS of a word correspondent to one of interacting proteins, and we obtain candidates of the raw pattern as follows: 11.</S>
			<S sid ="83" ssid = "25">If pi is of the word of the other interacting protein, (p0, . . .</S>
			<S sid ="84" ssid = "26">, pi) is a candidate of the raw pattern.</S>
			<S sid ="85" ssid = "27">12.</S>
			<S sid ="86" ssid = "28">If not, make pattern candidates for each pi+1 ∈ predicate(pi) ∪ argument(pi) − {p0, . . .</S>
			<S sid ="87" ssid = "29">, pi } by returning to 11.</S>
			<S sid ="88" ssid = "30">2.</S>
			<S sid ="89" ssid = "31">Select the pattern candidate of the.</S>
			<S sid ="90" ssid = "32">smallest set as the raw pattern.</S>
			<S sid ="91" ssid = "33">1 Before parsing, we concatenate each multi-word protein name into the one word as long as the concatenation does not cross name boundaries.</S>
			<S sid ="92" ssid = "34">3.</S>
			<S sid ="93" ssid = "35">Substitute variables (ENTITY1, ENTITY2) for.</S>
			<S sid ="94" ssid = "36">the predicates of PASs correspondent to the interacting proteins.</S>
			<S sid ="95" ssid = "37">RawPattern ENTITY1/NNprotein/NN$Xinteract/VBwith/INregion/NNof/INENTITY2/NN CombinationPattern The lower part of Figure 2 shows an example of the extraction of a raw pattern.</S>
			<S sid ="96" ssid = "38">“CD4” and“MHCII” are words representing interacting pro */NNARinGte1ract/VMBain Main teins.</S>
			<S sid ="97" ssid = "39">First, we set the PAS of “CD4” as p0.</S>
			<S sid ="98" ssid = "40">= ARG1 ARG1ARG2 Prep argument(p0) includes the PAS of “protein”, and MObD*/NN $X*/VBE*/VBwith/IN*/NNANRNGo1f/INARENGT2ITY/NN ENTITY/NN Entity Main Mntaitiyn Entity ion/ we set it as p1 (in other words, tracing the arrow (1)).</S>
			<S sid ="99" ssid = "41">Next, predicate(p1) includes the PAS of “interact” (tracing the arrow (2) back), so we set it protein/ENnNtity Entity as p2.</S>
			<S sid ="100" ssid = "42">We continue similarly until we reach the PAS of “MHCII” (p6 ).</S>
			<S sid ="101" ssid = "43">The result of the extracted raw pattern is the set of p0 , . . .</S>
			<S sid ="102" ssid = "44">, p6 with substituting variables ENTITY1 and ENTITY2 for “CD4” and “MHCII”.</S>
			<S sid ="103" ssid = "45">There are some cases where an extracted raw pattern is not appropriate and we need to re- ﬁne it.</S>
			<S sid ="104" ssid = "46">One case is when unnecessary coordi- nations/parentheses are included in the pattern, e.g. two interactions are described in a combined representation (“ENTITY1 binds this protein and ENTITY2”).</S>
			<S sid ="105" ssid = "47">Another is when two interacting proteins are connected directly by a conjunction or only one protein participates in an interaction.</S>
			<S sid ="106" ssid = "48">In such cases, we reﬁne patterns by unfolding of coordinations/parentheses and extension of patterns, respectively.</S>
			<S sid ="107" ssid = "49">We have omitted detailed explanations because of space limitations.</S>
			<S sid ="108" ssid = "50">The details are described in the work of Yakushiji et al.</S>
			<S sid ="109" ssid = "51">(2005).</S>
			<S sid ="110" ssid = "52">4.2 Division of Patterns.</S>
			<S sid ="111" ssid = "53">Division for generating combination patterns is based on observation of Yakushiji et al.</S>
			<S sid ="112" ssid = "54">(2005) that there are many cases where combinations of verbs and certain nouns form IE patterns.</S>
			<S sid ="113" ssid = "55">In the work of Yakushiji et al.</S>
			<S sid ="114" ssid = "56">(2005), we divided only patterns that include only one verb.</S>
			<S sid ="115" ssid = "57">We have extended the division process to also treat nominal patterns or patterns that include more than one verb.</S>
			<S sid ="116" ssid = "58">Combination patterns are not appropriate for utilizing individual word information because they are always used in rather strictly combined ways.</S>
			<S sid ="117" ssid = "59">Therefore we have newly introduced fragmental patterns which consist of independent PASs from raw patterns, in order to use individual word information for higher recall.</S>
			<S sid ="118" ssid = "60">4.2.1 Division for Generating Combination Patterns Raw patterns are divided into some components and the components are combined to conFigure 3: Division of Raw Pattern into Combina tion Pattern Components (Entity-Main-Entity) struct combination patterns according to types of the division.</S>
			<S sid ="119" ssid = "61">There are three types of division of raw patterns for generating combination patterns.</S>
			<S sid ="120" ssid = "62">These are: (a) Two-entity Division (a-1) Entity-Main-Entity Division (a-2) Main-Entity-Entity Division (b) Single-entity Division, and (c) No Division (Naive Patterns).</S>
			<S sid ="121" ssid = "63">Most raw patterns, where entities are at both ends of the patterns, are divided into Entity-Main- Entity.</S>
			<S sid ="122" ssid = "64">Main-Entity-Entity are for the cases where there are PASs other than entities at the ends of the patterns (e.g. “interaction between ENTITY1 and ENTITY2”).</S>
			<S sid ="123" ssid = "65">Single-entity is a special Main- Entity-Entity for interactions with only one participant (e.g. “ENTITY1 dimerization”).</S>
			<S sid ="124" ssid = "66">There is an example of Entity-Main-Entity division in Figure 3.</S>
			<S sid ="125" ssid = "67">First, the main component from the raw pattern is the syntactic head PAS of the raw pattern.</S>
			<S sid ="126" ssid = "68">If the raw pattern corresponds to a sentence, the syntactic head PAS is the PAS of the main verb.</S>
			<S sid ="127" ssid = "69">We underspecify the arguments of the main component, to enable them to unify with the PASs of any words with the same POSs.</S>
			<S sid ="128" ssid = "70">Next, if there are PASs of prepositions connecting to the main component, they become prep components.</S>
			<S sid ="129" ssid = "71">If there is no PAS of a preposition next to the main component on the connecting link from the main component to an entity, we make the pseudo PAS of a null preposition the prep component.</S>
			<S sid ="130" ssid = "72">The left prep component ($X) in Figure 3 is a pseudo PAS of a null preposition.</S>
			<S sid ="131" ssid = "73">We also underspecify the arguments of prep components.</S>
			<S sid ="132" ssid = "74">Finally, the remaining two parts, which are typically noun phrases, of the raw pattern become entity components.</S>
			<S sid ="133" ssid = "75">PASs corresponding to the entities of the original pair are labeled as only uniﬁable with the entities of other pairs.</S>
			<S sid ="134" ssid = "76">Main-Entity-Entity division is similar, except we distinguish only one prep component as a double-prep component and the PAS of the coordinate conjunction between entities becomes the coord component.</S>
			<S sid ="135" ssid = "77">Single-entity division is similar to Main-Entity-Entity division and the difference is that single-entity division produces no coord and one entity component.</S>
			<S sid ="136" ssid = "78">Naive patterns are patterns without division, where no division can be applied (e.g. “ENTITY1/NN in/IN complexes/NN with/IN ENTITY2/NN ”).</S>
			<S sid ="137" ssid = "79">All PASs on boundaries of components are labeled to determine which PAS on a boundary of another component can be uniﬁed.</S>
			<S sid ="138" ssid = "80">Labels are represented by subscriptions in Figure 3.</S>
			<S sid ="139" ssid = "81">These restrictions on component connection are used in the step of constructing combination patterns.</S>
			<S sid ="140" ssid = "82">Constructing combination patterns by combining components is equal to reconstructing original raw patterns with the original combination of components, or constructing new raw patterns with new combinations of components.</S>
			<S sid ="141" ssid = "83">For example, an Entity-Main-Entity pattern is constructed by combination of any main, any two prep and any two entity components.</S>
			<S sid ="142" ssid = "84">Actually, this construction process by combination is executed in the pattern matching step.</S>
			<S sid ="143" ssid = "85">That is, we do not off-line construct all possible combination patterns from the components and only construct the combination patterns that are able to match the target.</S>
			<S sid ="144" ssid = "86">4.2.2 Division for Generating Fragmental Patterns A raw pattern is splitted into individual PASs and each PAS becomes a fragmental pattern.</S>
			<S sid ="145" ssid = "87">We also prepare underspeciﬁed patterns where one or more of the arguments of the original are under- speciﬁed, i.e., are able to match any words of the same POSs and the same label of protein/not- protein.</S>
			<S sid ="146" ssid = "88">We underspecify the PASs of entities in fragmental patterns to enable them to unify with 4.3 Pattern Matching.</S>
			<S sid ="147" ssid = "89">Matching of combination patterns is executed as a process to match and combine combination pattern components according to their division types (Entity-Main-Entity, Main-Entity-Entity, Single- entity and No Division).</S>
			<S sid ="148" ssid = "90">Fragmental matching is matching all fragmental patterns to PASs derived from sentences.</S>
			<S sid ="149" ssid = "91">4.4 Scoring for Combination Matching.</S>
			<S sid ="150" ssid = "92">We next calculate the score of each combination matching to estimate the adequacy of the combination of components.</S>
			<S sid ="151" ssid = "93">This is because new combination of components may form inadequate patterns.</S>
			<S sid ="152" ssid = "94">(e.g. “ENTITY1 be ENTITY2” can be formed of components from “ENTITY1 be ENTITY2 receptor”.)</S>
			<S sid ="153" ssid = "95">Scores are derived from the results of combination matching to the source training corpus.</S>
			<S sid ="154" ssid = "96">We apply the combination patterns to the training corpus, and count pairs of True Positives (TP) and False Positives (FP).</S>
			<S sid ="155" ssid = "97">The scores are calculated basically by the following formula: Score = T P /(T P + F P ) + α × T P This formula is based on the precision of the pattern on the training corpus, i.e., an estimated precision on a test corpus.</S>
			<S sid ="156" ssid = "98">α works for smoothing, that is, to accept only patterns of large T P when F P = 0.</S>
			<S sid ="157" ssid = "99">α is set as 0.01 empirically.</S>
			<S sid ="158" ssid = "100">The formula is similar to the Apriori algorithm (Agrawal and Srikant, 1995) that learns association rules from a database.</S>
			<S sid ="159" ssid = "101">The ﬁrst term corresponds to the conﬁ- dence of the algorithm, and the second term corresponds to the support.</S>
			<S sid ="160" ssid = "102">For patterns where T P = F P = 0, which are not matched to PASs in the training corpus (i.e., newly produced by combinations of components), we estimates T P ′ and F P ′ by using the conﬁdence of the main and entity components.</S>
			<S sid ="161" ssid = "103">This is because main and entity components tend to contain pattern meanings, whereas prep, double-prep and coord components are rather functional.</S>
			<S sid ="162" ssid = "104">The formulas to calculate the scores for all cases are: any PASs with the same POSs and a protein label, although in combination patterns we retain the PASs of entities as only uniﬁable with entities of 8T P /(T P + F P ) + α &gt; (T P + F P ̸= 0) Score = &lt; P ′ /(T P ′ + F P ′ ) × T Ppairs.</S>
			<S sid ="163" ssid = "105">This is because fragmental patterns are de signed to be less strict than combination patterns.</S>
			<S sid ="164" ssid = "106">&gt; (T P = F P = 0, T P ′ + F P ′ ̸= 0) :0 (T P = F P = T P ′ = F P ′ = 0) Co mb ina tio n Pat ter n (1) Combination of components in combination m a t c h i n g ( 2 ) M a i n c o m p o n e n t i n c o m b i n a t i o n m a t c h i n g ( 3 ) E n t i t y c o m p o n e n t s i n c o m b i n a t i o n m a t c h i n g ( 4 ) S c o r e f o r c o m b i n a t i o n m a t c h i n g ( S C O R E ) Fra gm ent al Pat ter n (5) Matched fragmental patterns (6 ) N u m b e r o f P A S s o f e x a m p l e t h a t a r e n o t m a t c h e d i n f r a g m e n t a l m a t c h i n g Ra w Pat ter n (7) Length of raw pattern derived from example Table 2: Features for SVM Learning of Prediction Model 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 ALL SCORE ERK 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Recall Figure 4: Results of IE Experiment FGF2/NN bindwelltoFGFR1butinteract/VBpoorly/RB with/INKGFR/NN 8T P ′ + T P ′ (+T P ′ ) ENTITY1 ENTITY2 &lt;&gt; main entity1 entity2 T P ′ =(for Two entity, Single entity) &gt;:0 (for Naive) F P ′ = (similar to T P ′ but T P ′ is replaced by F P ′ ) x x 8T Pmain:two /(T Pmain:two + F Pmain:two ) &gt; T Pmain:two + F Pmain:two ̸= 0, ! &gt; for Two-entity &gt; Figure 5: Example Demonstrating Advantages of Full Parsing ′ main &lt; Pmain:s ingle &gt; /(T P main:singl e + F P main:singl e ! )5 Re sults and Discus sion &gt; T Pmain:single + F Pmain:single ̸= 0, &gt; for Single-entity &gt;:0 (other cases) 8T Pentityi /(T Pentityi + F Pentityi ) 5.1 Experimental Results on the AImed.</S>
			<S sid ="165" ssid = "107">Corpus To evaluate extraction patterns automatically con &gt; entityi = “ T P entityi + F P entityi ” ̸= 0 structed with our method, we used the AImed cor &gt;:0 (other cases) „ similar to T P ′ but T P ′ in the « pus, which consists of 225 MEDLINE (U.S. Na tional Library of Medicine, 2006) abstracts (1969 sentences) annotated with protein names and F P ′ = x y x numerators is replaced by F P ′ • T P : number of TPs by the combination of components • T Pmain:two : sum of TPs by two-entity combinations that include the same main component • T Pmain:single : sum of TPs by single-entity combinations that include the same main component • T Pentityi : sum of TPs by combinations that include the same entity component which is not the straight entity component • F Px : similar to T Px but TP is replaced by FP The entity component “ENTITY/NN ”, which only consists of the PAS of an entity, adds no information to combinations of components.</S>
			<S sid ="166" ssid = "108">We call this component a straight entity component and exclude its effect from the scores.</S>
			<S sid ="167" ssid = "109">4.5 Construction of Prediction Model.</S>
			<S sid ="168" ssid = "110">We use an SVM to learn a prediction model to determine whether a new protein pair is interacting.</S>
			<S sid ="169" ssid = "111">We used SV M light (Joachims, 1999) with an rbf kernel, which is known as the best kernel for most tasks.</S>
			<S sid ="170" ssid = "112">The prediction model is based on the features of Table 2.</S>
			<S sid ="171" ssid = "113">protein-protein interactions, for the training/test corpora.</S>
			<S sid ="172" ssid = "114">We used tags for the protein names given.</S>
			<S sid ="173" ssid = "115">We measured the accuracy of the IE task using the same criterion as Bunescu and Mooney (2006), who used an SVM to construct extraction patterns on word/POS/type sequences from the AImed corpus.</S>
			<S sid ="174" ssid = "116">That is, an extracted interaction from an abstract is correct if the proteins are tagged as interacting with each other somewhere in that abstract (document-level measure).</S>
			<S sid ="175" ssid = "117">Figure 4 plots our 10-fold cross validation and the results of Bunescu and Mooney (2006).</S>
			<S sid ="176" ssid = "118">The line ALL represents results when we used all features for SVM learning.</S>
			<S sid ="177" ssid = "119">The line SCORE represents results when we extracted pairs with higher combination matching scores than various threshold values.</S>
			<S sid ="178" ssid = "120">And the line ERK represents results by Bunescu and Mooney (2006).</S>
			<S sid ="179" ssid = "121">The line ALL obtained our best overall F- measure 57.3%, with 71.8% precision and 48.4% recall.</S>
			<S sid ="180" ssid = "122">Our method was signiﬁcantly better than Bunescu and Mooney (2006) for precision be tween 50% and 80%.</S>
			<S sid ="181" ssid = "123">It also needs to be noted that SCORE, which did not use SVM learning and only used the combination patterns, achieved performance comparable to that by Bunescu and Mooney (2006) for the precision range from 50% to 80%.</S>
			<S sid ="182" ssid = "124">And for this range, introducing the frag- mental patterns with SVM learning raised the recall.</S>
			<S sid ="183" ssid = "125">This range of precision is practical for the IE task, because precision is more important than recall for signiﬁcant interactions that tend to be described in many abstracts (as shown by the next experiment), and too-low recall accompanying too-high precision requires an excessively large source text.</S>
			<S sid ="184" ssid = "126">Figure 5 shows the advantage of introducing full parsing.</S>
			<S sid ="185" ssid = "127">“FGF2” and “KGFR” is an interacting protein pair.</S>
			<S sid ="186" ssid = "128">The pattern “ENTITY1 interact with ENTITY2” based on PASs successfully extracts this pair.</S>
			<S sid ="187" ssid = "129">However, it is difﬁcult to extract this pair with patterns based on surface words, because there are 5 words between “FGF2” and “interact”.</S>
			<SUBSECTION>5.2 Experimental Results on Abstracts of.</SUBSECTION>
			<S sid ="188" ssid = "130">MEDLINE We also conducted an experiment to extract interacting protein pairs from a large amount of biomedical text, i.e. about 14 million titles and 8 million abstracts in MEDLINE.</S>
			<S sid ="189" ssid = "131">We constructed combination patterns from all 225 abstracts of the AImed corpus, and calculated a threshold value of combination scores that produced about 70% precision and 30% recall on the training corpus.</S>
			<S sid ="190" ssid = "132">We extracted protein pairs with higher combination scores than the threshold value.</S>
			<S sid ="191" ssid = "133">We excluded single-protein interactions to reduce time consumption and we used a protein name recognizer in this experiment2.</S>
			<S sid ="192" ssid = "134">We compared the extracted pairs with a manually curated database, Reactome (JoshiTope et al., 2005), which published 16,564 human protein interaction pairs as pairs of Entrez Gene IDs (U.S. National Library of Medicine, 2006).</S>
			<S sid ="193" ssid = "135">We converted our extracted protein pairs into pairs of Entrez Gene IDs by the protein name recognizer.3 Because there may be pairs missed by Re 2 Because protein names were recognized after the parsing, multi-word protein names were not concatenated.</S>
			<S sid ="194" ssid = "136">3 Although the same protein names are used for humans and other species, these are considered to be human proteins without checking the context.</S>
			<S sid ="195" ssid = "137">This is a fair assumption because Reactome itself infers human interaction events from experiments on model organisms such as mice.</S>
			<S sid ="196" ssid = "138">Tot al 8 9 Par sin g Err or/ Fai lur e ( R e l a t e d t o c o o r d i n a t i o n s ) 3 5 (1 4) La ck of Co mb ina tio n Pat ter n Co mp on ent 3 3 Re qui rin g An ap hor a Re sol uti on 9 Err or in Pre dic tio n Mo del 8 Re qui rin g Att rib uti ve Ad jec tiv es 5 Ot her s 1 0 More than one cause can occur in one error, thus the sum of all causes is larger than the total number of False Negatives.</S>
			<S sid ="197" ssid = "139">Table 3: Causes of Error for FNs actome or pairs that our processed text did not include, we excluded extracted pairs of IDs that are not included in Reactome and excluded Reactome pairs of IDs that do not co-occur in the sentences of our processed text.</S>
			<S sid ="198" ssid = "140">After this postprocessing, we found that we had extracted 7775 human protein pairs.</S>
			<S sid ="199" ssid = "141">Of them, 155 pairs were also included in Reactome ([a] pseudo TPs) and 7620 pairs were not included in Reac- tome ([b] pseudo FPs).</S>
			<S sid ="200" ssid = "142">947 pairs of Reactome were not extracted by our system ([c] pseudo False Negatives (FNs)).</S>
			<S sid ="201" ssid = "143">However, these results included pairs that Reactome missed or those that only co- occurred and were not interacting pairs in the text.</S>
			<S sid ="202" ssid = "144">There may also have been errors with ID assignment.</S>
			<S sid ="203" ssid = "145">To determine such cases, a biologist investigated 100 pairs randomly selected from pairs of pseudo TPs, FPs and FNs retaining their ratio of numbers.</S>
			<S sid ="204" ssid = "146">She also checked correctness of the assigned IDs.</S>
			<S sid ="205" ssid = "147">2 pairs were selected from pseudo TPs, 88 pairs were from pseudo FPs and 10 pairs were from pseudo FNs.</S>
			<S sid ="206" ssid = "148">The biologist found that 57 pairs were actual TPs (2 pairs of pseudo TPs and 55 pairs of pseudo FPs) and 32 pairs were actual FPs of the pseudo FPs.</S>
			<S sid ="207" ssid = "149">Thus, the precision was 64.0% in this sample set.</S>
			<S sid ="208" ssid = "150">Furthermore, even if we assume that all pseudo FNs are actual FNs, the recall can be estimated by actual TPs / (actual TPs + pseudo FNs) × 100 = 83.8%.These results mean that the recall of an IE sys tem for interacting proteins is improved for a large amount of text even if it is low for a small corpus.</S>
			<S sid ="209" ssid = "151">Thus, this justiﬁes our assertion that a high degree of precision in the low-recall range is important.</S>
			<SUBSECTION>5.3 Error Analysis.</SUBSECTION>
			<S sid ="210" ssid = "152">Tables 3 and 4 list causes of error for FNs/FPs on a test set of the AImed corpus using the prediction model with the best F-measure with all the 0.55 0.5 Table 4: Causes of Error for FPsfeatures.</S>
			<S sid ="211" ssid = "153">Different to Subsection 5.1, we individ 0.45 0.4 0.35 0 50 100 150 200 Training Corpus Size (Number of Abstracts) ually checked each occurring pair of interacting proteins.</S>
			<S sid ="212" ssid = "154">The biggest problems were parsing error/failure, lack of necessary patterns and learning of inappropriate patterns.</S>
			<SUBSECTION>5.3.1 Parsing Error As listed in Table 3, 14 (40%) of the 35 parsing errors/failures were related to coordinations.</SUBSECTION>
			<S sid ="213" ssid = "155">Many of these were caused by differences in the characteristics of the PTB/GTB, the training corpora for Enju, and the AImed Corpus.</S>
			<S sid ="214" ssid = "156">For example, Enju failed to obtain the correct structure for “the ENTITY1 / ENTITY1 complex” because words in the PTB/GTB are not segmented with “/” and Enju could not be trained on such a case.</S>
			<S sid ="215" ssid = "157">One method to solve this problem is to avoid segmenting words with “/” and introducing extraction patterns based on surface characters, such as “ENTITY1/ENTITY2 complex”.</S>
			<S sid ="216" ssid = "158">Parsing errors are intrinsic problems to IE methods using parsing.</S>
			<S sid ="217" ssid = "159">However, from Table 3, we can conclude that the key to gaining better accuracy is reﬁning of the method with which the PAS patterns are constructed (there were 46 related FNs) rather than improving parsing (there were 35 FNs).</S>
			<SUBSECTION>5.3.2 Lack of Necessary Patterns and Learning of Inappropriate Patterns There are two different reasons causing the problems with the lack of necessary patterns and the learning of inappropriate patterns: (1) the training corpus was not sufﬁciently large to saturate IE accuracy and (2) our method of pattern construction was too limited.</SUBSECTION>
			<S sid ="218" ssid = "160">Effect of Training Corpus Size To investigate whether the training corpus was large enough to maximize IE accuracy, we conducted experiments on training corpora of various sizes.</S>
			<S sid ="219" ssid = "161">Figure 6 plots Figure 6: Effect of Training Corpus Size (1) 600 500 400 300 200 100 0 0 50 100 150 200 Training Corpus Size (Number of Abstracts) Raw Patterns (before division) Main Component Entity Component Other Component Naive Pattern Figure 7: Effect of Training Corpus Size (2) is not large enough.</S>
			<S sid ="220" ssid = "162">Thus increasing corpus size will further improve IE accuracy.</S>
			<S sid ="221" ssid = "163">Limitation of the Present Pattern Construction The limitations with our pattern construction method are revealed by the fact that we could not achieve a high precision like Bunescu and Mooney (2006) within the high-recall range.</S>
			<S sid ="222" ssid = "164">Compared to theirs, one of our problems is that our method could not handle attributives.</S>
			<S sid ="223" ssid = "165">One example is “binding property of ENTITY1 to ENTITY2”.</S>
			<S sid ="224" ssid = "166">We could not obtain “binding” because the smallest set of PASs connecting “ENTITY1” and “ENTITY2” includes only the PASs of “property”, “of” and “to”.</S>
			<S sid ="225" ssid = "167">To handle these attributives, we need distinguish necessary attributives from those that are general4 by semantic analysis or bootstrapping.</S>
			<S sid ="226" ssid = "168">Another approach to improve our method is to include local information in sentences, such as surface words between protein names.</S>
			<S sid ="227" ssid = "169">Zhao and Grishman (2005) reported that adding local information to deep syntactic information improved IE results.</S>
			<S sid ="228" ssid = "170">This approach is also applicable to IE in other domains, where related entities are in a short graphs of F-measures by SCORE and Figure 7 4 Consider the case where a source sentence for a pattern is. plots the number of combination patterns on train ing corpora of various sizes.</S>
			<S sid ="229" ssid = "171">From Figures 6 and 7, the training corpus (207 abstracts at a maximum) “ENTITY1 is an important homodimeric protein.” (“homodimeric” represents that two molecules of “ENTITY1” interact with each other.)</S>
			<S sid ="230" ssid = "172">distance like the work of Zhou et al.</S>
			<S sid ="231" ssid = "173">(2005).</S>
			<S sid ="232" ssid = "174">6 Conclusion.</S>
			<S sid ="233" ssid = "175">We proposed the use of PASs to construct patterns as extraction rules, utilizing their ability to abstract syntactical variants with the same relation.</S>
			<S sid ="234" ssid = "176">In addition, we divided the patterns for generalization, and used matching results for SVM learning.</S>
			<S sid ="235" ssid = "177">In experiments on extracting of protein- protein interactions, we obtained 71.8% precision and 48.4% recall on a small corpus and 64.0% precision and 83.8% recall estimated on a large text, which demonstrated the obvious advantages of our method.</S>
	</SECTION>
	<SECTION title="Acknowledgement">
			<S sid ="236" ssid = "178">This work was partially supported by Grant-in-Aid for Scientiﬁc Research on Priority Areas “Systems Genomics” (MEXT, Japan) and Solution- Oriented Research for Science and Technology (JST, Japan).</S>
	</SECTION>
</PAPER>
