<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Seed sampling is critical in semi-supervised learning.</S>
		<S sid ="2" ssid = "2">This paper proposes a clustering- based stratified seed sampling approach to semi-supervised learning.</S>
		<S sid ="3" ssid = "3">First, various clustering algorithms are explored to partition the unlabeled instances into different strata with each stratum represented by a center.</S>
		<S sid ="4" ssid = "4">Then, diversity-motivated intra-stratum sampling is adopted to choose the center and additional instances from each stratum to form the unlabeled seed set for an oracle to annotate.</S>
		<S sid ="5" ssid = "5">Finally, the labeled seed set is fed into a bootstrapping procedure as the initial labeled data.</S>
		<S sid ="6" ssid = "6">We systematically evaluate our stratified bootstrapping approach in the semantic relation classification subtask of the ACE RDC (Relation Detection and Classification) task.</S>
		<S sid ="7" ssid = "7">In particular, we compare various clustering algorithms on the stratified bootstrapping performance.</S>
		<S sid ="8" ssid = "8">Experimental results on the ACE RDC 2004 corpus show that our clustering- based stratified bootstrapping approach achieves the best F1-score of 75.9 on the sub- task of semantic relation classification, approaching the one with golden clustering.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="9" ssid = "9">Semantic relation extraction aims to detect and classify semantic relationships between a pair of named entities occurring in a natural language text.</S>
			<S sid ="10" ssid = "10">Many machine learning approaches have been proposed to attack this problem, including supervised (Miller et al., 2000; Zelenko et al., 2003; Culotta and Soresen, 2004; Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005; Zhang et al., 2006; Zhou and Zhang, 2007; Zhou et al., 2007; Qian et al., 2008; Zhou et al., 2010), semi- supervised (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004; Chen et al., 2006; Qian et al., 2009; Zhou et al., 2009), and unsupervised methods (Hasegawa et al., 2004; Zhang et al., 2005; Chen et al., 2005).</S>
			<S sid ="11" ssid = "11">Current work on relation extraction mainly adopts supervised learning methods, since they achieve much better performance.</S>
			<S sid ="12" ssid = "12">However, theynormally require a large number of manually la beled relation instances, whose acquisition is both time consuming and labor intensive.</S>
			<S sid ="13" ssid = "13">In contrast, unsupervised methods do not need any manually labeled instances.</S>
			<S sid ="14" ssid = "14">Nevertheless, it is difficult to assess their performance due to the lack of evaluation criteria.</S>
			<S sid ="15" ssid = "15">As something between them, semi- supervised learning has received more and more attention recently.</S>
			<S sid ="16" ssid = "16">With the plenitude of unlabeled natural language text at hand, semi-supervised learning can significantly reduce the need for labeled data with only limited sacrifice in performance.</S>
			<S sid ="17" ssid = "17">For example, Abney (2002) proposes a bootstrapping algorithm which chooses the unlabeled instances with the highest probability of being correctly labeled and add them in turn into the labeled training data iteratively.</S>
			<S sid ="18" ssid = "18">This paper focuses on bootstrapping-based semi- supervised learning in relation extraction.</S>
			<S sid ="19" ssid = "19">Since the performance of bootstrapping depends much on the quality and quantity of the seed set and researchers tend to employ as few seeds as possible (e.g. 100 instances) to save time and labor, the quality of the seed set plays a critical role in bootstrapping.</S>
			<S sid ="20" ssid = "20">Furthermore, the imbalance of different classes and 346 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 346–355, MIT, Massachusetts, USA, 911 October 2010.</S>
			<S sid ="21" ssid = "21">Qc 2010 Association for Computational Linguistics the inherent structural complexity of instances will severely weaken the strength of bootstrapping and semi-supervised learning as well.</S>
			<S sid ="22" ssid = "22">Therefore, it is critical for a bootstrapping procedure to select an appropriate seed set, which should be representative and diverse.</S>
			<S sid ="23" ssid = "23">However, most of current semi- supervised relation extraction systems (Zhang, 2004; Chen et al., 2006) use a random seed sampling strategy, which fails to fully exploit the affinity nature in the training data to derive the seed set.</S>
			<S sid ="24" ssid = "24">Alternatively, Zhou et al.</S>
			<S sid ="25" ssid = "25">(2009) bootstrap a set of weighted support vectors from both labeled and unlabeled data using SVM and feed these instances into semi-supervised relation extraction.</S>
			<S sid ="26" ssid = "26">However, their seed set is sequentially generated only to ensure that there are at least 5 instances for each relation class.</S>
			<S sid ="27" ssid = "27">Our previous work (Qian et al., 2009) attempts to solve this problem via a simple stratified sampling strategy for selecting the seed set.</S>
			<S sid ="28" ssid = "28">Experimentation on the ACE RDC 2004 corpus shows that the stratified sampling strategy achieves promising results for semi-supervised learning.</S>
			<S sid ="29" ssid = "29">Nevertheless, the success of the strategy relies on the assumption that the true distribution of all relation types is already known, which is impractical for real NLP applications.</S>
			<S sid ="30" ssid = "30">This paper presents a clustering-based stratified seed sampling approach for semi-supervised relation extraction, without the assumption on the true distribution of different relation types.</S>
			<S sid ="31" ssid = "31">The motivations behind our approach are that the unlabeled data can be partitioned into a number of strata using a clustering algorithm and that representative and diverse seeds can be derived from such strata in the framework of stratified sampling (Neyman, 1934) for an oracle to annotate.</S>
			<S sid ="32" ssid = "32">Particularly, we employ a diversity-motivated intra-stratum sampling scheme to pick a center and additional instances as seeds from each stratum.</S>
			<S sid ="33" ssid = "33">Experimental results show the effectiveness of the clustering- based stratified seed sampling for semi-supervised relation classification.</S>
			<S sid ="34" ssid = "34">The rest of this paper is organized as follows.</S>
			<S sid ="35" ssid = "35">First an overview of the related work is given in Section 2.</S>
			<S sid ="36" ssid = "36">Then, Section 3 introduces the stratifiedbootstrapping framework including an intra stratum sampling scheme while Section 4 describes various clustering algorithms.</S>
			<S sid ="37" ssid = "37">The experimental results on the ACE RDC 2004 corpus are reported in Section 5.</S>
			<S sid ="38" ssid = "38">Finally we conclude our work and indicate some future directions in Section 6.</S>
	</SECTION>
	<SECTION title="Related Work. " number = "2">
			<S sid ="39" ssid = "1">In semi-supervised learning for relation extraction, most of previous work construct the seed set either randomly (Zhang, 2004; Chen et al., 2006) or sequentially (Zhou et al., 2009).</S>
			<S sid ="40" ssid = "2">Qian et al.</S>
			<S sid ="41" ssid = "3">(2009) adopt a stratified sampling strategy to select the seed set.</S>
			<S sid ="42" ssid = "4">However, their method needs a stratification variable such as the known distribution of the relation types, while our method uses clustering to divide relation instances into different strata.</S>
			<S sid ="43" ssid = "5">In the literature, clustering techniques have been employed in active learning to sample representative seeds in a certain extent (Nguyen and Smeulders, 2004; Tang et al., 2002; Shen et al., 2004).</S>
			<S sid ="44" ssid = "6">Our work is similar to the formal framework, as proposed in Nguyen and Smeulders (2004), in which K-medoids clustering is incorporated into active learning.</S>
			<S sid ="45" ssid = "7">The cluster centers are used to construct a classifier and which in turn propagates classification decision to other examples via a local noise model.</S>
			<S sid ="46" ssid = "8">Unlike their probabilistic models, we apply various clustering algorithms together with intra-stratum sampling to select a seed set in discriminative models like SVMs.</S>
			<S sid ="47" ssid = "9">In active learning for syntactic parsing, Tang et al.</S>
			<S sid ="48" ssid = "10">(2002) employ a sampling strategy of “most uncertain per cluster” to select representative examples and weight them using their cluster density, while we pick a few seeds (the number of the sampled seeds is proportional to the cluster density) from a cluster in addition to its center.</S>
			<S sid ="49" ssid = "11">Shen et al.</S>
			<S sid ="50" ssid = "12">(2004) combine multiple criteria to measure the informativeness, representativeness, and diversity of examples in active learning for named entity recognition.</S>
			<S sid ="51" ssid = "13">Unlike our sampling strategy of clustering for representativeness and stratified sampling for diversity, they either select cluster centroids or diverse examples from a pre- chosen set in terms of some combined metrics.</S>
			<S sid ="52" ssid = "14">To the best of our knowledge, this is the first work to address the issue of seed selection using clustering techniques for semi-supervised learning with dis- criminative models.</S>
	</SECTION>
	<SECTION title="Stratified Bootstrapping Framework. " number = "3">
			<S sid ="53" ssid = "1">The stratified bootstrapping framework consists of three major components: an underlying supervised learner and a bootstrapping algorithm on top of it as usual, plus a clustering-based stratified seed sampler.</S>
			<S sid ="54" ssid = "2">3.1 Underlying Supervised Learner.</S>
			<S sid ="55" ssid = "3">Due to recent success in tree kernel-based relation extraction, this paper adopts a tree kernel-based method in the underlying supervised learner.</S>
			<S sid ="56" ssid = "4">Following the previous work in relation extraction (Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008), we use the standard convolution tree kernel (Collins and Duffy, 2001) to count the number of common sub-trees as the structural similarity be Algorithm self-bootstrapping Require: labeled seed set L Require: unlabeled data set U Require: batch size S Repeat Train a single classifier on L Run the classifier on U Find at most S instances in U that the classifier has the highest prediction confidence Add them into L Until: no data points available or the stoppage condition is reached tween two parse trees.</S>
			<S sid ="57" ssid = "5">Besides, to properly represent a relation instance, this paper adopts the Unified Parse and Semantic Tree (UPST), as proposed in Qian et al.</S>
			<S sid ="58" ssid = "6">(2008).</S>
			<S sid ="59" ssid = "7">To our knowledge, the USPT has achieved the best performance in relation extraction so far on the ACE RDC 2004 corpus.</S>
			<S sid ="60" ssid = "8">In particular, we use the SVMlightTK1 package as our classifier.</S>
			<S sid ="61" ssid = "9">Since the package is a binary clas sifier, we adapt it to the multi-class tasks of relation extraction by applying the one vs. others strategy, which builds K binary classifiers so as to separate one class from all others.</S>
			<S sid ="62" ssid = "10">The final classification decision of an instance is determined by the class that has the maximal SVM output margin.</S>
			<S sid ="63" ssid = "11">3.2 Bootstrapping Algorithm.</S>
			<S sid ="64" ssid = "12">Following Zhang (2004), we have developed a baseline self-bootstrapping procedure, which keeps augmenting the labeled data by employing the models trained from previously available labeled data, as shown in Figure 1.</S>
			<S sid ="65" ssid = "13">Since the SVMlightTK package doesn’t output any probability that it assigns to the class label on an instance, we devise a metric to measure the confidence with regard to the classifier’s prediction.</S>
			<S sid ="66" ssid = "14">Given a sequence of output margins of all K binary classifiers at some iteration, denoted as {m1,m2,…mK} with mi the margin for the i-th classifier, we compute the margin gap between the largest and the mean of the others, i.e. K K K Figure 1: Self-bootstrapping algorithm th classifier.</S>
			<S sid ="67" ssid = "15">Intuitively, the bigger the H, the greater the difference between the maximal margin and all others, and thus the more reliably the classifier makes the prediction on the instance.</S>
			<S sid ="68" ssid = "16">3.3 Clustering-based Stratified Seed Sampler.</S>
			<S sid ="69" ssid = "17">Stratified sampling is a method of sampling in statistics, in which the members of a population are grouped into relatively homogeneous subgroups (i.e. strata) according to one certain property, and then a sample is selected from each stratum.</S>
			<S sid ="70" ssid = "18">This process of grouping is called stratification, and the property on which the stratification is performed is called the stratification variable.</S>
			<S sid ="71" ssid = "19">Previous work justifies theoretically and practically that stratified sampling is more appropriate than random sampling for general use (Neyman, 1934) as well as for relation extraction (Qian et al., 2009).</S>
			<S sid ="72" ssid = "20">However, the difficulty lies in how to find the appropriate stratification variable for complicated tasks, such as relation extraction.</S>
			<S sid ="73" ssid = "21">The idea of clustering-based stratification circumvents this problem by clustering the unlabeled data into a number of strata without the need to explicitly specify a stratification variable.</S>
			<S sid ="74" ssid = "22">Figure 2 illustrates the clustering-based stratified seed sampling strategy employed in the bootstrapping procedure, where RSET denotes the whole unlabeled data, SeedSET the seed set to be labeled and H = max mi − (∑mi − max mi ) /(K −1) (1) |RSETi| the number of instances in the i-th cluster2 i=1 i=1 i=1 RSETi.</S>
			<S sid ="75" ssid = "23">Here, a relation instance is represented us Where K denotes the total number of relation classes, and mi denotes the output margin of the i ing USPT and the similarity between two instances is computed using the standard convolution tree 1 http://ainlp.info.uniroma2.it/moschitti/ 2 Hereafter, when we refer to clusters.</S>
			<S sid ="76" ssid = "24">from the viewpoint of stratified sampling, they are often called “strata”.</S>
			<S sid ="77" ssid = "25">kernel, as described in Section 3.1 (i.e., both the clustering and the classification adopt the same structural representation, since we want the representative seeds in the clustering space to be also representative in the classification space).</S>
			<S sid ="78" ssid = "26">After clustering, a certain number of instances from every stratum are sampled using an intra-stratum scheme (c.f. Subsection 3.4).</S>
			<S sid ="79" ssid = "27">Normally, this number is proportional to the size of that stratum in the whole data set.</S>
			<S sid ="80" ssid = "28">However, in case this number is 0 Require: RSET ={R1,R2,…,RN}, the set of unlabeled relation instances and K, the number of strata being clustered Output: SeedSET with the size of NS (100) Procedure Initialize SeedSET = NULL Cluster RSET into K strata using a clustering algorithm and perform stratum pruning if necessary.</S>
			<S sid ="81" ssid = "29">Calculate the number of instances being sampled for each stratum i={1,2,…,K} due to the rounding of real numbers, it is set to 1 to ensure the existence of at least one seed from that N = | RSETi | ∗ N i N S (2)stratum.</S>
			<S sid ="82" ssid = "30">Furthermore, to ensure that the total num ber of instances being sampled equals the prescribed NS, the number of seeds from dominant strata may be slightly adjusted accordingly.</S>
			<S sid ="83" ssid = "31">Finally, these instances form the unlabeled seed set for an oracle to annotate as the input to the underlying supervised learner in the bootstrapping procedure.</S>
			<S sid ="84" ssid = "32">3.4 Intra-stratum sampling.</S>
			<S sid ="85" ssid = "33">Given the distribution of clusters, a simple way to select the most representative instances is to choose the center of each cluster with the cluster prior as the weight of the center (Tang et al., 2002; Nguyen and Smeulders, 2004).</S>
			<S sid ="86" ssid = "34">Nevertheless, for the complicated task of relation extraction on the ACE RDC corpora, which is highly skewed across different relation classes, only considering the center of each cluster would severely under-represent the high-density data.</S>
			<S sid ="87" ssid = "35">To overcome this problem, we adopt a sampling approach, in particular stratified sampling, which takes the size of each stratum into consideration.</S>
			<S sid ="88" ssid = "36">Given the size of the seed set NS and the number of strata K, a natural question will arise as how to select the remaining (NS-K) seeds after we have extracted the K centers from the K strata.</S>
			<S sid ="89" ssid = "37">We view this problem as intra-stratum sampling, which is required to choose the remaining number of seeds from inside individual stratum (excluding the centers themselves).</S>
			<S sid ="90" ssid = "38">At the first glance, sampling a certain number of seeds from one particular stratum (e.g., RSETi), seems to be the same sampling problem as we have encountered before, which aims to select the most representative and diverse seeds.</S>
			<S sid ="91" ssid = "39">This will naturally lead to another application of a clustering algorithm to the stratification of the stratum RSETi.</S>
			<S sid ="92" ssid = "40">and adjust this number if necessary.</S>
			<S sid ="93" ssid = "41">Perform intra-strata sampling to form SeedSETi from each stratum RSETi, by selecting the center Ci and (Ni1) additional instances Generate SeedSET by summating RSETi from each stratum Figure 2: Clustering-based stratified seed sampling Nevertheless, remember the fact that, this time for the stratum RSETi, the center Ci has been chosen, so it may not be reasonable to extract additional centers in this way.</S>
			<S sid ="94" ssid = "42">Therefore, in order to avoid recursion and over-complexity, we employ a diversity-motivated intra-stratum sampling scheme (Shen et al., 2004), called KDN (K-diverse neighbors), which aims to maximize the training utility of all seeds from a stratum.</S>
			<S sid ="95" ssid = "43">The motivation is that we prefer the seeds with high variance to each other, thus avoiding repetitious seeds from a single stratum.</S>
			<S sid ="96" ssid = "44">The basic idea is to add a candidate instance to the seed set only if it is sufficiently different from any previously selected seeds, i.e., the similarity between the candidate instance and any of the current seeds is less than a threshold β.</S>
			<S sid ="97" ssid = "45">In this paper, the threshold β is set to the average pairwise similarity between any two instances in a stratum.</S>
	</SECTION>
	<SECTION title="Clustering Algorithms. " number = "4">
			<S sid ="98" ssid = "1">This section describes several typical clustering algorithms in the literature, such as K-means, HAC, spectral clustering and affinity propagation, as well as their application in this paper.</S>
			<S sid ="99" ssid = "2">4.1 K-medoids (KM) As a simple yet effective clustering method, the K- means algorithm assigns each instance to the cluster whose center (also called centroid) is nearest.</S>
			<S sid ="100" ssid = "3">In particular, the center is the average of all the instances in the cluster, i.e., with its coordinates the arithmetic means for each dimension separately over all the instances in the cluster.</S>
			<S sid ="101" ssid = "4">One problem with K-means is that it does not yield the same result with each run while the other problem is the requirement for the concept of a mean to be definable, which is unfortunately not available in our setting (we employ a parse tree representation for a relation instance).</S>
			<S sid ="102" ssid = "5">Hence, we adopt a variant of K-means, namely, K-medoids, where a medoid, rather than a centroid, is defined as a representative of a cluster.</S>
			<S sid ="103" ssid = "6">Besides, K- medoids has proved to be more robust to noise and outliers in comparison with K-means.</S>
			<S sid ="104" ssid = "7">4.2 Hierarchical Agglomerative Clustering.</S>
			<S sid ="105" ssid = "8">(HAC) Different from K-medoids, hierarchical clustering creates a hierarchy of clusters which can be represented in a tree structure called a dendrogram.</S>
			<S sid ="106" ssid = "9">The root of the tree consists of a single cluster containing all objects, and the leaves correspond to individual object.</S>
			<S sid ="107" ssid = "10">Typically, hierarchical agglomerative clustering (HAC) starts at the leaves and successively merges two clusters together as long as they have the shortest distance among all the pairwise distances between any two clusters.</S>
			<S sid ="108" ssid = "11">Given a specified number of clusters, the key problem is to determine where to cut the hierarchi cal tree into clusters.</S>
			<S sid ="109" ssid = "12">In this paper, we generate the final flat cluster structures greedily by maximizing the equal distribution of instances among different clusters.</S>
			<S sid ="110" ssid = "13">4.3 Spectral Clustering (SC).</S>
			<S sid ="111" ssid = "14">Spectral clustering has become more and more popular recently.</S>
			<S sid ="112" ssid = "15">Taking as input a similarity matrix between any two instances, spectral clustering makes use of the spectrum of the similarity matrix of the data to perform dimensionality reduction for clustering in fewer dimensions.</S>
			<S sid ="113" ssid = "16">Compared to the “traditional algorithms” such as K-means or HAC, spectral clustering has many fundamental advantages.</S>
			<S sid ="114" ssid = "17">Results obtained by spectral clustering often outperform the traditional approaches.</S>
			<S sid ="115" ssid = "18">Furthermore, spectral clustering is very simple to implement and can be solved efficiently using standard linear algebra methods (von Luxburg, 2006).</S>
			<S sid ="116" ssid = "19">4.4 Affinity Propagation (AP).</S>
			<S sid ="117" ssid = "20">As a new emerging clustering algorithm, affinity propagation (AP) (Frey and Dueck, 2007) is basically an iterative message-passing procedure in which the instances being clustered compete to serve as cluster exemplars by exchanging two types of messages, namely, “responsibility” and “availability”.</S>
			<S sid ="118" ssid = "21">After the procedure converges or has repeated a finite number of iterations, each cluster is represented by an exemplar.</S>
			<S sid ="119" ssid = "22">AP was reported to find clusters with much lower error than those found by other methods.</S>
			<S sid ="120" ssid = "23">For our application, affinity propagation takes as input a similarity matrix, whose elements represent either the similarity between two different instances or the preference (a real number p) for an instance when two instances are the same.</S>
			<S sid ="121" ssid = "24">One problem with AP is that the number of clusters cannot be predefined, which is indirectly determined by the preference as well as the convergence procedure itself.</S>
	</SECTION>
	<SECTION title="Experimentation. " number = "5">
			<S sid ="122" ssid = "1">This section systematically evaluates the boot- strapping approach using clustering-based stratified seed sampling, in the relation classification (i.e., given the relationship already detected) sub- task of relation extraction on the ACE RDC 2004 corpus.</S>
			<S sid ="123" ssid = "2">5.1 Experimental Setting.</S>
			<S sid ="124" ssid = "3">The ACE RDC 2004 corpus 3 is gathered from various newspapers, newswire and broadcasts.</S>
			<S sid ="125" ssid = "4">It contains 451 documents and 5702 positive relation instances of 7 relation types and 23 subtypes between 7 entity types.</S>
			<S sid ="126" ssid = "5">For easy reference with related work in the literature, evaluation is done on 347 documents (from nwire and bnews domains), which include 4305 relation instances.</S>
			<S sid ="127" ssid = "6">Table 1 lists the major relation types and subtypes, including their corresponding instance numbers and ratios in our evaluation set.</S>
			<S sid ="128" ssid = "7">One obvious observation from the table is that the numbers of different relation types is highly imbalanced.</S>
			<S sid ="129" ssid = "8">These 347 documents are then divided into 3 disjoint sets randomly, with 3 http//www.ldc.upenn.edu/ Projects/ACE/ Ty pes Su bty pes # % PH YS Lo cat ed 7 3 8 1 7 . 1 Ne ar 8 7 2 . 0 Part W hol e 3 7 8 8 . 8 PER SO C Bu sin ess 1 7 3 4 . 0 Fa mil y 1 2 1 2 . 8 Ot her 5 5 1 . 3 E M P O R G E mp loy Ex ec uti ve 4 8 9 1 1 . 4 E mp loy Sta ff 5 3 9 1 2 . 5 E mp loy Un det er.</S>
			<S sid ="130" ssid = "9">7 8 1 . 8 Me mberof Gr ou p 1 9 1 4 . 4 Su bsi dia ry 2 0 6 4 . 8 Par tne r 1 2 0 . 3 Ot her 8 0 1 . 9 A RT Useror O wn er 2 0 0 4 . 6 Inv entoror Ma n. 9 0 . 2 Ot her 2 0 . 0 O T H E R A F F Et hni c 3 9 0 . 9 Ide olo gy 4 8 1 . 1 Ot her 5 4 1 . 3 GPE AF F Cit izenor Re sid . 2 7 3 6 . 3 Ba sed -In 2 1 5 5 . 0 Ot her 3 9 0 . 9 DI SC 2 7 9 6 . 5 To tal 4 3 0 5 10 0.</S>
			<S sid ="131" ssid = "10">0 Table 1: Relation types and their corresponding instance numbers and ratios in the ACE RDC 2004 corpus 10% of them (35 files, around 400 instances) held out as the test data set, 10% of them (35 files, around 400 instances) used as the development data set to fine-tune various settings and parameters, while the remaining 277 files (over 3400 instances) used as the training data set, from which the seed set will be sampled.</S>
			<S sid ="132" ssid = "11">The corpus is parsed using Charniak’s parser (Charniak, 2001) and relation instances are generated by extracting all pairs of entity mentions oc curring in the same sentence with positive relationships.</S>
			<S sid ="133" ssid = "12">For easy comparison with related work, we only evaluate the relation classification task on the 7 major relation types of the ACE RDC 2004 corpus.</S>
			<S sid ="134" ssid = "13">For the SVMlightTK classifier, the training parameters C (SVM) and λ (tree kernel) are fine-tuned to 2.4 and 0.4 respectively.</S>
			<S sid ="135" ssid = "14">The performance is measured using the standard P/R/F1 (Precision/Recall/F1-measure).</S>
			<S sid ="136" ssid = "15">For each relation type, P is the ratio of the true relation instances in all the relation instances being identified, R is the ratio of the true relation instances being identified in all the true relation instances in the corpus, and F1 is the harmonic mean of P and R. The overall performance P/R/F1 is then calculated using the micro-average measure over all major class types.</S>
			<S sid ="137" ssid = "16">5.2 Experimental Results.</S>
			<S sid ="138" ssid = "17">Comparison of various seed sampling strategies without intra-stratum sampling on the development data Table 2 compares the performance of bootstrap- ping-based relation classification using various seed sampling strategies without intra-stratum sampling on the development data.</S>
			<S sid ="139" ssid = "18">Here, the size of the seed set L is set to 100, and the top 100 instances with the highest confidence (c.f. Formula 1) are augmented at each iteration.</S>
			<S sid ="140" ssid = "19">For sampling strategies marked with an asterisk, we performed 10 trials and calculated their averages.</S>
			<S sid ="141" ssid = "20">Since for these strategies the seed sets sampled from different trials may be quite different, their performance scores vary in a great degree accordingly.</S>
			<S sid ="142" ssid = "21">This experimental setting and notation are also used in all the subsequent experiments unless specified.</S>
			<S sid ="143" ssid = "22">Besides, two additional baseline sampling strategies are included for comparison: sequential sampling (SEQ), which selects a sequentially-occurring L instances as the seed set, and random sampling (RAND), which randomly selects L instances as the seed set.</S>
			<S sid ="144" ssid = "23">Table 2 shows that 1) RAND outperforms SEQ by 1.2 units in F1- score.</S>
			<S sid ="145" ssid = "24">This is due to the fact that the seed set via RAND may better reflect the distribution ofthe whole training data than that via SEQ, nev ertheless at the expense of collecting the whole training data in advance.</S>
			<S sid ="146" ssid = "25">2) While HAC performs moderately better than RAND, it is surprising that both KM and AP perform even worse than SEQ, and that SC per forms worse than RAND.</S>
			<S sid ="147" ssid = "26">Furthermore, all the four clustering-based seed sampling strategies achieve much smaller performance improvement in F1-score than RAND, among which KM performs worst with performance improvement of only 0.1 in F1-score.</S>
			<S sid ="148" ssid = "27">S a m p l i n g s t r a t e g i e s P ( Δ P ) R ( Δ R ) F 1 ( Δ F 1 ) R A N D* 6 9.</S>
			<S sid ="149" ssid = "28">1( 3.</S>
			<S sid ="150" ssid = "29">1) 6 6.</S>
			<S sid ="151" ssid = "30">4( 0.</S>
			<S sid ="152" ssid = "31">2) 6 7.</S>
			<S sid ="153" ssid = "32">8( 2.</S>
			<S sid ="154" ssid = "33">0) SE Q* 6 5.</S>
			<S sid ="155" ssid = "34">8( 2.</S>
			<S sid ="156" ssid = "35">6) 6 8.</S>
			<S sid ="157" ssid = "36">0( 0.</S>
			<S sid ="158" ssid = "37">1) 6 6.</S>
			<S sid ="159" ssid = "38">6( 1.</S>
			<S sid ="160" ssid = "39">3) K M * 6 2.</S>
			<S sid ="161" ssid = "40">0( 0.</S>
			<S sid ="162" ssid = "41">9) 61 .0( 0.</S>
			<S sid ="163" ssid = "42">5) 6 1.</S>
			<S sid ="164" ssid = "43">3( 0.</S>
			<S sid ="165" ssid = "44">1) H A C 6 9.</S>
			<S sid ="166" ssid = "45">9( 1.</S>
			<S sid ="167" ssid = "46">3) 7 0.</S>
			<S sid ="168" ssid = "47">4( 0.</S>
			<S sid ="169" ssid = "48">4) 7 0.</S>
			<S sid ="170" ssid = "49">1( 0.</S>
			<S sid ="171" ssid = "50">8) SC * 6 7.</S>
			<S sid ="172" ssid = "51">1( 1.</S>
			<S sid ="173" ssid = "52">5) 6 8.</S>
			<S sid ="174" ssid = "53">1( 0.</S>
			<S sid ="175" ssid = "54">0) 6 7.</S>
			<S sid ="176" ssid = "55">5( 0.</S>
			<S sid ="177" ssid = "56">8) AP 6 6.</S>
			<S sid ="178" ssid = "57">6( 2.</S>
			<S sid ="179" ssid = "58">0) 6 6.</S>
			<S sid ="180" ssid = "59">2( 0.</S>
			<S sid ="181" ssid = "60">1) 6 6.</S>
			<S sid ="182" ssid = "61">4( 1.</S>
			<S sid ="183" ssid = "62">1) Table 2: Comparison of various seed sampling strategies without intra-stratum sampling on the development data 3) All the performance improvements from boot- strapping largely come from the improvements in precision.</S>
			<S sid ="184" ssid = "63">While the bootstrapping procedure makes the SVM classifier more accurate, it lacks enough generalization ability.</S>
			<S sid ="185" ssid = "64">To explain above special phenomena, we have a look at the clustering results.</S>
			<S sid ="186" ssid = "65">Our inspection reveals that most of them are severely imbalanced, i.e., some clusters are highly dense while others are extremely sparse.</S>
			<S sid ="187" ssid = "66">This indicates that merely selecting the centers from each cluster cannot properly represent the overall distribution.</S>
			<S sid ="188" ssid = "67">Moreover, the centers with high density lack the generalization ability due to its solitude in the cluster, leading to less performance enhancement than expected.</S>
			<S sid ="189" ssid = "68">The only exception is HAC, which much outperforms RAND by 2.3 in F1-score, although HAC is usually not considered as an effective clustering algorithm.</S>
			<S sid ="190" ssid = "69">The reason may be that HAC creates a hierarchy of clusters in the top-down manner by cutting a cluster into two.</S>
			<S sid ="191" ssid = "70">Therefore, the centers in the two sibling clusters will be closer to each other than they are to the centers in other clusters.</S>
			<S sid ="192" ssid = "71">Besides, the final flat cluster structures given a special number of clusters are generated greedily from the cluster hierarchy by maximizing the equal distribution of instances among different clusters.</S>
			<S sid ="193" ssid = "72">In other words, when the cluster number reaches a certain threshold, the dense area will get more seeds represented in the seed set.</S>
			<S sid ="194" ssid = "73">As a consequence, the distribution of all the seeds sampled by HAC will approximate the distribution of the whole training data in some degree, while the seeds sampled by other clustering algorithm are kept as far as possible due to the objective of clustering and the lack of intra-stratum sampling.</S>
			<S sid ="195" ssid = "74">These observations also justify the application of the stratified seed sampling to the bootstrapping procedure, which enforces the number of seeds sampled from a cluster to be proportional to its density, presumably approximated by its size in this paper.</S>
			<S sid ="196" ssid = "75">Comparison of different cluster numbers with intra-stratum sampling on the development data In order to fine-tune the optimal cluster numbers for seed sampling, we compare the performance of different numbers of clusters for each clustering algorithm on the development data set and report their F-scores in Table 3.</S>
			<S sid ="197" ssid = "76">For reference, we also list the F-score for golden clustering (GOLD), in which all instances are grouped in terms of their annotated ground relation major types (7), major types considering relation direction (13), subtypes (23), and subtypes considering direction (38).</S>
			<S sid ="198" ssid = "77">Besides, the performance of clustering-based semi- supervised relation classification is also measured over other typical cluster numbers (i.e., 1, 50, 60, 80, 100).</S>
			<S sid ="199" ssid = "78">Particularly, when the cluster number equals 1, it means that only diversity other than representativeness is considered in the seed sampling.</S>
			<S sid ="200" ssid = "79">Among these clustering algorithms, one of the distinct characteristics with the AP algorithm is that the number of clusters cannot be specified in advance, rather, it is determined by the predefined preference parameter (c.f. Subsection 4.4).</S>
			<S sid ="201" ssid = "80">Therefore, we should tune the preference parameter so as to get the predefined cluster number.</S>
			<S sid ="202" ssid = "81">However, sometimes we still couldn’t get the exact number of clusters as we expected.</S>
			<S sid ="203" ssid = "82">In these cases, we use the approximate cluster numbers for AP instead.</S>
			<S sid ="204" ssid = "83">Table 3 shows that 1) The performance for all the clustering algorithms varies in some degree with the number of clusters being grouped.</S>
			<S sid ="205" ssid = "84">Interestingly, the performance with only one cluster is better than those of clustering-based strategies with 100 clusters, at most cases.</S>
			<S sid ="206" ssid = "85">This implies that the diversity of the seeds is at least as important as their representativeness.</S>
			<S sid ="207" ssid = "86">And this could be further explained by our observation that, with the increase of cluster numbers, the clusters get smaller and denser while their centers also come closer to each other.</S>
			<S sid ="208" ssid = "87">Therefore, the representativeness and diversity as well as the distribution of the seeds sampled from them may vary accordingly, leading to different performance.</S>
			<S sid ="209" ssid = "88">S a m pl in g st ra te gi es P ( Δ P ) R ( Δ R ) F 1 ( Δ F 1 ) GO LD 7 9 . 5 ( 7 . 8 ) 7 2 . 7 ( 2 . 1 ) 7 6 . 0 ( 4 . 8 ) RA ND * 7 1 . 9 ( 3 . 7 ) 6 9 . 7 ( 0 . 1 ) 7 0 . 8 ( 1 . 8 ) SE Q* 7 1 . 9 ( 2 . 6 ) 6 5 . 2 ( 0 . 1 ) 6 9 . 3 ( 1 . 3 ) K M* 7 3 . 6 ( 2 . 1 ) 7 2 . 3 ( 0 . 3 ) 7 2 . 9 ( 1 . 2 ) HA C 7 9.</S>
			<S sid ="210" ssid = "89">0( 1 0.</S>
			<S sid ="211" ssid = "90">2) 7 3 . 0 ( 1 . 1 ) 7 5 . 9 ( 5 . 6 ) SC * 7 2 . 3 ( 2 . 1 ) 7 2 . 1 ( 0 . 4 ) 7 2 . 2 ( 1 . 2 ) AP 7 5 . 7 ( 2 . 5 ) 7 2 . 0 ( 0 . 4 ) 7 3 . 7 ( 1 . 4 ) Table 3: Performance in F1-score over different cluster numbers with intra-stratum sampling on the development data 2) Golden clustering achieves the best performance of 73.9 in F1-score when the cluster number is set to 7, significantly higher than the performance using other cluster numbers.</S>
			<S sid ="212" ssid = "91">Interestingly, this number coincides with the number of major relation types needed to be classified in our task.</S>
			<S sid ="213" ssid = "92">This is reasonable since the instances with the same relation type should be much more similar than those with different relation types and it is easy to discriminate the seed set of one relation type from that of other relation types.</S>
			<S sid ="214" ssid = "93">3) Among the four clustering algorithms, HAC achieves best performance over most of cluster numbers.</S>
			<S sid ="215" ssid = "94">This further verifies the aforementioned analysis.</S>
			<S sid ="216" ssid = "95">That is, as a hierarchical clustering algorithm, HAC can sample seeds that better capture the distribution of the training data.</S>
			<S sid ="217" ssid = "96">4) For KM, the best performance is achieved around the number of 23 while for both HAC and SC, the optimal cluster number is consis tent with GOLD clustering, namely, 7.</S>
			<S sid ="218" ssid = "97">For AP, the optimal cluster number for AP is 38.</S>
			<S sid ="219" ssid = "98">This is largely due to that we fail to cluster the training data into about 7 and 13 groups no matter how we vary the preference parameter.</S>
			<S sid ="220" ssid = "99">Final comparison of different clustering algorithms on the held-out test data After the optimal cluster numbers are determined for each clustering algorithm, we apply these numbers on the held-out test data and report the performance results (P/R/F1 and their respective improvements) in Table 4.</S>
			<S sid ="221" ssid = "100">For easy reference, we also include the performance for GOLD, RAND, and SEQ sampling strategies.</S>
			<S sid ="222" ssid = "101">Table 4: Performance of various clustering-based seed sampling strategies on the held-out test data with the optimal cluster number for each clustering algorithm Table 4 shows that 1) Among all the clustering algorithms, HAC achieves the best F1-score of 75.9, significantly higher than RAND and SEQ by 5.1 and 6.6 respectively.</S>
			<S sid ="223" ssid = "102">The improvement comes not only from significant precision boost, but also from moderate recall increase.</S>
			<S sid ="224" ssid = "103">This further justifies the merits of HAC as a clustering algorithm for stratified seed sampling in semi-supervised relation classification.</S>
			<S sid ="225" ssid = "104">2) HAC approaches the best F1-score of 76.0 for golden clustering.</S>
			<S sid ="226" ssid = "105">Obviously, this doesn’t mean HAC performs as well as golden clustering in terms of clustering quality measures, rather it does imply that HAC achieves the performance improvement by making the seed set better represent the overall distribution over inherent structure of relation instances, while golden clustering accomplishes this using the distribution over relation types.</S>
			<S sid ="227" ssid = "106">Since the distribution over relation types doesn’t always conform to that over instance structures, and for a statistical discriminative classifier, often the latter is more important than the former, it will be no surprise if HAC outperforms golden clustering in some real applications, e.g. clustering-based stratified sampling.</S>
	</SECTION>
	<SECTION title="Conclusion and Future Work. " number = "6">
			<S sid ="228" ssid = "1">This paper presents a stratified seed sampling strategy based on clustering algorithms for semi- supervised learning.</S>
			<S sid ="229" ssid = "2">Our strategy does not rely on any stratification variable to divide the training instances into a number of strata.</S>
			<S sid ="230" ssid = "3">Instead, the strata are formed via clustering, given a metric measuring the similarity between any two instances.</S>
			<S sid ="231" ssid = "4">Further, diversity-motivated intra-strata sampling is employed to sample additional instances from within each stratum besides its center.</S>
			<S sid ="232" ssid = "5">We compare the effect of various clustering algorithms on the performance of semi-supervised learning and find that HAC achieves the best performance since the distribution of its seed set better approximates that of the whole training data.</S>
			<S sid ="233" ssid = "6">Extensive evaluation on the ACE RDC 2004 benchmark corpus shows that our clustering-based stratified seed sampling strategy significantly improves the performance of semi-supervised relation classification.</S>
			<S sid ="234" ssid = "7">We believe that our clustering-based stratified seed sampling strategy can not only be applied to other semi-supervised learning tasks, but also can be incorporated into active learning, where the instances to be labeled at each iteration as well as the seed set could be selected using clustering techniques, thus further reducing the amount of instances needed to be annotated.</S>
			<S sid ="235" ssid = "8">For the future work, it is possible to adapt our one-level clustering-based sampling to the multi- level one, where for every stratum it is still possi ble to divide it into lower substrata for further stratified sampling in order to make the seeds better represent the true distribution of the data.</S>
	</SECTION>
	<SECTION title="Acknowledgments">
			<S sid ="236" ssid = "9">This research is supported by Projects 60873150, 60970056, and 90920004 under the National Natural Science Foundation of China.</S>
	</SECTION>
</PAPER>
