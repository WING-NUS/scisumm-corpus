<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">In this paper, we observe that there exists a second dimension to the relation extraction (RE) problem that is orthogonal to the relation type dimension.</S>
		<S sid ="2" ssid = "2">We show that most of these second dimensional structures are relatively constrained and not difficult to identify.</S>
		<S sid ="3" ssid = "3">We propose a novel algorithmic approach to RE that starts by first identifying these structures and then, within these, identifying the semantic type of the relation.</S>
		<S sid ="4" ssid = "4">In the real RE problem where relation arguments need to be identified, exploiting these structures also allows reducing pipelined propagated errors.</S>
		<S sid ="5" ssid = "5">We show that this RE framework provides significant improvement in RE performance.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="6" ssid = "6">Relation extraction (RE) has been defined as the task of identifying a given set of semantic binary relations in text.</S>
			<S sid ="7" ssid = "7">For instance, given the span of text “.</S>
			<S sid ="8" ssid = "8">the Seattle zoo . . .</S>
			<S sid ="9" ssid = "9">”, one would like to extract the relation that “the Seattle zoo” is located-at “Seattle”.</S>
			<S sid ="10" ssid = "10">RE has been frequently studied over the last few years as a supervised learning task, learning from spans of text that are annotated with a set of semantic relations of interest.</S>
			<S sid ="11" ssid = "11">However, most approaches to RE have assumed that the relations’ arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem.</S>
			<S sid ="12" ssid = "12">Conceptually, this is a rather simple approach as all spans of texts are treated uniformly and are being mapped to one of several relation types of interest.</S>
			<S sid ="13" ssid = "13">However, these approaches to RE require a large amount of manually annotated training data to achieve good performance, making it difficult to expand the set of target relations.</S>
			<S sid ="14" ssid = "14">Moreover, as we show, these approaches become brittle when the relations’ arguments are not given but rather need to be identified in the data too.</S>
			<S sid ="15" ssid = "15">In this paper we build on the observation that there exists a second dimension to the relation extraction problem that is orthogonal to the relation type dimension: all relation types are expressed in one of several constrained syntactico-semantic structures.</S>
			<S sid ="16" ssid = "16">As we show, identifying where the text span is on the syntactico-semantic structure dimension first, can be leveraged in the RE process to yield improved performance.</S>
			<S sid ="17" ssid = "17">Moreover, working in the second dimension provides robustness to the real RE problem, that of identifying arguments along with the relations between them.</S>
			<S sid ="18" ssid = "18">For example, in “the Seattle zoo”, the entity mention “Seattle” modifies the noun “zoo”.</S>
			<S sid ="19" ssid = "19">Thus, the two mentions “Seattle” and “the Seattle zoo”, are involved in what we later call a premodifier relation, one of several syntactico-semantic structures we identify in Section 3.</S>
			<S sid ="20" ssid = "20">We highlight that all relation types can be expressed in one of several syntactico-semantic structures – Premodifiers, Possessive, Preposition, Formulaic and Verbal.</S>
			<S sid ="21" ssid = "21">As it turns out, most of these structures are relatively constrained and are not difficult to identify.</S>
			<S sid ="22" ssid = "22">This suggests a novel algorithmic approach to RE that starts by first identifying these structures and then, within these, identifying the semantic type of the relation.</S>
			<S sid ="23" ssid = "23">Not only does this approach provide significantly improved RE perfor 551 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 551–560, Portland, Oregon, June 1924, 2011.</S>
			<S sid ="24" ssid = "24">Qc 2011 Association for Computational Linguistics mance, it carries with it two additional advantages.</S>
			<S sid ="25" ssid = "25">First, leveraging the syntactico-semantic structure is especially beneficial in the presence of small amounts of data.</S>
			<S sid ="26" ssid = "26">Second, and more important, is the fact that exploiting the syntactico-semantic dimension provides several new options for dealing with the full RE problem – incorporating the argument identification into the problem.</S>
			<S sid ="27" ssid = "27">We explore one of these possibilities, making use of the constrained structures as a way to aid in the identification of the relations’ arguments.</S>
			<S sid ="28" ssid = "28">We show that this already provides significant gain, and discuss other possibilities that can be explored.</S>
			<S sid ="29" ssid = "29">The contributions of this paper are summarized below: • We highlight that all relation types are expressed as one of several syntactico-semantic structures and show that most of these are relatively constrained and not difficult to identify.</S>
			<S sid ="30" ssid = "30">Consequently, working first in this structural dimension can be leveraged in the RE process to improve performance.</S>
			<S sid ="31" ssid = "31">• We show that when one does not have a large number of training examples, exploiting the syntactico-semantic structures is crucial for RE performance.</S>
			<S sid ="32" ssid = "32">• We show how to leverage these constrained structures to improve RE when the relations’ arguments are not given.</S>
			<S sid ="33" ssid = "33">The constrained structures allow us to jointly entertain argument can didates and relations built with them as arguments.</S>
			<S sid ="34" ssid = "34">Specifically, we show that considering argument candidates which otherwise would have been discarded (provided they exist in syntactico-semantic structures), we reduce error propagation along a standard pipeline RE architecture, and that this joint inference process leads to improved RE performance.</S>
			<S sid ="35" ssid = "35">In the next section, we describe our relation extraction framework that leverages the syntactico- semantic structures.</S>
			<S sid ="36" ssid = "36">We then present these structures in Section 3.</S>
			<S sid ="37" ssid = "37">We describe our mention entity typing system in Section 4 and features for the RE system in Section 5.</S>
			<S sid ="38" ssid = "38">We present our RE experiments in Section 6 and perform analysis in Section 7, before concluding in Section 8.</S>
			<S sid ="39" ssid = "39">S = {premodifier, possessive, preposition, formulaic} gold mentions in training data Mtrain Dg = {(mi , mj ) ∈ Mtrain × Mtrain | mi in same sentence as mj ∧ i /= j ∧ i &lt; j} REbase = RE classifier trained on Dg Ds = ∅ for each (mi , mj ) ∈ Dg do p = structure inference on (mi , mj ) using patterns if p ∈ S ∨ (mi , mj ) was annotated with a S structure Ds = Ds ∪ (mi , mj ) done REs = RE classifier trained on Ds Output: REbase and REs Figure 1: Training a regular baseline RE classifier REbase and a RE classifier leveraging syntactico- semantic structures REs .</S>
	</SECTION>
	<SECTION title="Relation Extraction Framework. " number = "2">
			<S sid ="40" ssid = "1">In Figure 1, we show the algorithm for training a typical baseline RE classifier (REbase), and for training a RE classifier that leverages the syntactico- semantic structures (REs).</S>
			<S sid ="41" ssid = "2">During evaluation and when the gold mentions are already annotated, we apply REs as follows.</S>
			<S sid ="42" ssid = "3">When given a test example mention pair (xi,xj ), we perform structure inference on it using the patterns described in Section 3.</S>
			<S sid ="43" ssid = "4">If (xi,xj ) is identified as hav ing any of the four syntactico-semantic structures S, apply REs to predict the relation label, else apply REbase.Next, we show in Figure 2 our joint inference al gorithmic framework that leverages the syntactico- semantic structures for RE, when mentions need to be predicted.</S>
			<S sid ="44" ssid = "5">Since the structures are fairly constrained, we can use them to consider mention candidates that are originally predicted as non mentions.</S>
			<S sid ="45" ssid = "6">As shown in Figure 2, we conservatively include such mentions when forming mention pairs, provided their null labels are predicted with a low probability t1.</S>
			<S sid ="46" ssid = "7">1 In this work, we arbitrary set t=0.2.</S>
			<S sid ="47" ssid = "8">After the experiments, and in our own analysis, we observe that t=0.25 achieves better performance.</S>
			<S sid ="48" ssid = "9">Besides using the probability of the 1-best prediction, one could also for instance, use the probability difference between the first and second best predictions.</S>
			<S sid ="49" ssid = "10">However, selecting an optimal t value is not the main focus of this work.</S>
			<S sid ="50" ssid = "11">S = {premodifier, possessive, preposition, formulaic} candidate mentions Mcand Let Lm = argmaxPM ET (y|m, θ), m ∈ Mcand y selected mentions Msel = {m ∈ Mcand | Lm /= null ∨ PM ET (null|m, θ) ≤ t} QhasN ull = {(mi , mj ) ∈ Msel × Msel | mi in same sentence as mj ∧ i /= j ∧ i &lt; j ∧ (Lmi /= null ∨ Lmj /= null)} Let pool of relation predictions R = ∅ for each (mi , mj ) ∈ QhasN ull do p = structure inference on (mi , mj ) using patterns if p ∈ S r = relation prediction for (mi , mj ) using REs R = R ∪ r else if Lmi /= null ∧ Lmj /= null r = relation prediction for (mi , mj ) using REbase R = R ∪ r done O utput: R Figure 2: RE using predicted mentions and patterns.</S>
			<S sid ="51" ssid = "12">Abbreviations: Lm : predicted entity label for mention m using the mention entity typing (MET) classifier described in Section 4; PM ET : prediction probability according to the MET classifier; t: used for thresholding.</S>
			<S sid ="52" ssid = "13">There is a large body of work in using patterns to extract relations (Fundel et al., 2007; Greenwood and Stevenson, 2006; Zhu et al., 2009).</S>
			<S sid ="53" ssid = "14">However, these works operate along the first dimension, that of using patterns to mine for relation type examples.</S>
			<S sid ="54" ssid = "15">In contrast, in our RE framework, we apply patterns to identify the syntactico-semantic structure dimension first, and leverage this in the RE process.</S>
			<S sid ="55" ssid = "16">In (Roth and Yih, 2007), the authors used entity types to constrain the (first dimensional) relation types allowed among them.</S>
			<S sid ="56" ssid = "17">In our work, although a few of our patterns involve semantic type comparison, most of the patterns are syntactic in nature.</S>
			<S sid ="57" ssid = "18">In this work, we performed RE evaluation on the NIST Automatic Content Extraction (ACE) corpus.</S>
			<S sid ="58" ssid = "19">Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005).</S>
			<S sid ="59" ssid = "20">An exception is the work of (Kambhatla, 2004), where the author evaluated on the ACE-2003 corpus.</S>
			<S sid ="60" ssid = "21">In that work, the author did not address the pipelined errors propagated from the mention identification process.</S>
	</SECTION>
	<SECTION title="Syntactico-Semantic Structures. " number = "3">
			<S sid ="61" ssid = "1">In this paper, we performed RE on the ACE-2004 corpus.</S>
			<S sid ="62" ssid = "2">In ACE-2004 when the annotators tagged a pair of mentions with a relation, they also specified the type of syntactico-semantic structure2.</S>
			<S sid ="63" ssid = "3">ACE 2004 identified five types of structures: premodifier, possessive, preposition, formulaic, and verbal.</S>
			<S sid ="64" ssid = "4">We are unaware of any previous computational approaches that recognize these structures automatically in text, as we do, and use it in the context of RE (or any other problem).</S>
			<S sid ="65" ssid = "5">In (Qian et al., 2008), the authors reported the recall scores of their RE system on the various syntactico-semantic structures.</S>
			<S sid ="66" ssid = "6">But they do not attempt to recognize nor leverage these structures.</S>
			<S sid ="67" ssid = "7">In this work, we focus on detecting the first four structures.</S>
			<S sid ="68" ssid = "8">These four structures cover 80% of the mention pairs having valid semantic relations (we give the detailed breakdown in Section 7) and we show that they are relatively easy to identify using simple rules or patterns.</S>
			<S sid ="69" ssid = "9">In this section, we indicate mentions using square bracket pairs, and use mi and mj to represent a mention pair.</S>
			<S sid ="70" ssid = "10">We now describe the four structures.</S>
			<S sid ="71" ssid = "11">Premodifier relations specify the proper adjective or proper noun premodifier and the following noun it modifies, e.g.: [the [Seattle] zoo] Possessive indicates that the first mention is in a possessive case, e.g.: [[California] ’s Governor] Preposition indicates that the two mentions are semantically related via the existence of a preposition, e.g.: [officials] in [California] Formulaic The ACE04 annotation guideline3 indicates the annotation of several formulaic relations, including for example address: [Medford] , [Massachusetts] 2 ACE-2004 termed it as lexical condition.</S>
			<S sid ="72" ssid = "12">We use the term syntactico-semantic structure in this paper as the mention pair exists in specific syntactic structures, and we use rules or patterns that are syntactically and semantically motivated to detect these structures.3 http://projects.ldc.upenn.edu/ace/docs/EnglishRDCV43 2.PDF Str uct ur e ty pe Pat ter n Pr e m od ifie r Ba sic pat ter n: [u * [v +] w +] , wh ere u, v, w rep res ent wo rd s Ea ch w is a no un or adj ect ive If u* is not em pty , the n u*: JJ+ ∨ JJ “a nd ” JJ?</S>
			<S sid ="73" ssid = "13">∨ C D JJ* ∨ R B D T JJ?</S>
			<S sid ="74" ssid = "14">∨ R B C D JJ ∨ D T ( R B | J J | V B G | V B D | V B N | C D ) ? Le t w1 = fir st wo rd in w +.</S>
			<S sid ="75" ssid = "15">w1 /= “’s ” an d PO S tag of w1 /= P O S Le t vl = las t wo rd in v+ . PO S tag of vl /= PR P$ nor W P$ Po ss es siv e Ba sic pat ter n: [u ? [v +] w +] , wh ere u, v, w rep res ent wo rd s Le t w1 = fir st wo rd in w +.</S>
			<S sid ="76" ssid = "16">If w1 = “’s ” ∨ PO S tag of w1 = PO S, ac ce pt me nti on pa ir Le t vl = las t wo rd in v+ . If.</S>
			<S sid ="77" ssid = "17">PO S tag of vl = PR P$ or W P$ , ac ce pt me nti on pa ir Pr ep osi tio n Ba sic pat ter n: [m i ] v* [m j ], wh ere v rep res ent wo rd s a n d n u m b e r o f p r e p o s i t i o n s i n t h e t e x t s p a n v * b e t w e e n t h e m = 0 , 1 , o r 2 If sat isf y pat ter n: IN [m i ][ mj ], ac ce pt me nti on pa ir If sat isf y pat ter n: [m i ] (I N| T O) [m j ], ac ce pt me nti on pa ir If all lab els in Ld sta rt wit h “pr ep ”, ac ce pt me nti on pa ir Fo rm ul aic If sat isf y pat ter n: [m i ] / [m j ] ∧ Ec (m i ) = PE R ∧ Ec (m j ) = O R G, ac ce pt me nti on pa ir If sat isf y pat ter n: [m i ][ mj ] I f E c ( m i ) = P E R ∧ E c ( m j ) = O R G ∨ G P E , a c c e p t m e n t i o n p a i r Table 1: Rules and patterns for the four syntactico-semantic structures.</S>
			<S sid ="78" ssid = "18">Regular expression notations: ‘*’ matches the preceding element zero or more times; ‘+’ matches the preceding element one or more times; ‘?</S>
			<S sid ="79" ssid = "19">’ indicates that the preceding element is optional; ‘|’ indicates or.</S>
			<S sid ="80" ssid = "20">Abbreviations: Ec (m): coarse-grained entity type of mention m; Ld : labels in dependency path between the headword of two mentions.</S>
			<S sid ="81" ssid = "21">We use square brackets ‘[’ and ‘]’ to denote mention boundaries.</S>
			<S sid ="82" ssid = "22">The ‘/’ in the Formulaic row denotes the occurrence of a lexical ‘/’ in text.</S>
			<S sid ="83" ssid = "23">In this rest of this section, we present the rules/patterns for detecting the above four syntactico-semantic structure, giving an overview of them in Table 1.</S>
			<S sid ="84" ssid = "24">We plan to release all of the rules/patterns along with associated code4.</S>
			<S sid ="85" ssid = "25">Notice that the patterns are intuitive and mostly syntactic in nature.</S>
			<S sid ="86" ssid = "26">3.1 Premodifier Structures.</S>
			<S sid ="87" ssid = "27">• We require that one of the mentions completely include the other mention.</S>
			<S sid ="88" ssid = "28">Thus, the basic pattern is [u* [v+] w+].</S>
			<S sid ="89" ssid = "29">• If u* is not empty, we require that it satisfies any of the following POS tag sequences: JJ+ ∨ JJ and JJ?</S>
			<S sid ="90" ssid = "30">∨ CD JJ*, etc. These are (optional) POS tag sequences that normally start a valid noun phrase.</S>
			<S sid ="91" ssid = "31">• We use two patterns to differentiate between premodifier relations and possessive relations, by checking for the existence of POS tags PRP$, WP$, POS, and the word “’s”.</S>
			<S sid ="92" ssid = "32">4 http://cogcomp.cs.illinois.edu/page/publications 3.2 Possessive Structures.</S>
			<S sid ="93" ssid = "33">• The basic pattern for possessive is similar to that for premodifier: [u? [v+] w+] • If the word immediately following v+ is “’s” or its POS tag is “POS”, we accept the mention pair.</S>
			<S sid ="94" ssid = "34">If the POS tag of the last word in v+ is either PRP$ or WP$, we accept the mention pair.</S>
			<S sid ="95" ssid = "35">3.3 Preposition Structures.</S>
			<S sid ="96" ssid = "36">• We first require the two mentions to be non overlapping, and check for the existence of patterns such as “IN [mi] [mj ]” and “[mi] (IN|TO) [mj ]”.</S>
			<S sid ="97" ssid = "37">• If the only dependency labels in the dependency path between the head words of mi and mj are “prep” (prepositional modifier), accept the mention pair.</S>
			<S sid ="98" ssid = "38">3.4 Formulaic Structures.</S>
			<S sid ="99" ssid = "39">• The ACE-2004 annotator guidelines specify that several relations such as reporter signing off, addresses, etc. are often specified in standard structures.</S>
			<S sid ="100" ssid = "40">We check for the existence of patterns such as “[mi] / [mj ]”, “[mi] [mj ]”, Category Feature For every POS of wk and offset from lw word wk wk and offset from lw in POS of wk , wk , and offset from lw mention mi POS of wk , offset from lw, and lw Bc(wk ) and offset from lw POS of wk , Bc(wk ), and offset from lw POS of wk , offset from lw, and Bc(lw) Contextual C−1,−1 of mi C+1,+1 of mi P−1,−1 of mi P+1,+1 of mi NE tags tag of NE, if lw of NE coincides with lw of mi in the sentence Syntactic parse-label of parse tree constituent parse that exactly covers mi parse-labels of parse tree constituents covering mi Table 2: Features used in our mention entity typing (MET) system.</S>
			<S sid ="101" ssid = "41">The abbreviations are as follows.</S>
			<S sid ="102" ssid = "42">lw: last word in the mention; Bc(w): the brown cluster bit string representing w; NE: named entity and whether they satisfy certain semantic entity type constraints.</S>
	</SECTION>
	<SECTION title="Mention Extraction System. " number = "4">
			<S sid ="103" ssid = "1">As part of our experiments, we perform RE using predicted mentions.</S>
			<S sid ="104" ssid = "2">We first describe the features (an overview is given in Table 2) and then describe how we extract candidate mentions from sentences during evaluation.</S>
			<S sid ="105" ssid = "3">4.1 Mention Extraction Features.</S>
			<S sid ="106" ssid = "4">Features for every word in the mention For every word wk in a mention mi, we extract seven features.</S>
			<S sid ="107" ssid = "5">These are a combination of wk itself, its POS tag, and its integer offset from the last word (lw) in the mention.</S>
			<S sid ="108" ssid = "6">For instance, given the mention “the operation room”, the offsets for the three words in the mention are -2, -1, and 0 respectively.</S>
			<S sid ="109" ssid = "7">These features are meant to capture the word and POS tag sequences in mentions.</S>
			<S sid ="110" ssid = "8">We also use word clusters which are automatically generated from unlabeled texts, using the Brown clustering (Bc) algorithm of (Brown et al., 1992).</S>
			<S sid ="111" ssid = "9">This algorithm outputs a binary tree where words are leaves in the tree.</S>
			<S sid ="112" ssid = "10">Each word (leaf) in the tree can be represented by its unique path from the Category Feature POS POS of single word between m1 , m2 hw of mi , mj and P−1,−1 of mi , mj hw of mi , mj and P−1,−1 of mi , mj hw of mi , mj and P+1,+1 of mi , mj hw of mi , mj and P−2,−1 of mi , mj hw of mi , mj and P−1,+1 of mi , mj hw of mi , mj and P+1,+2 of mi , mj Base chunk any base phrase chunk between mi , mj Table 3: Additional RE features.</S>
			<S sid ="113" ssid = "11">root and this path can be represented as a simple bit string.</S>
			<S sid ="114" ssid = "12">As part of our features, we use the cluster bit string representation of wk and lw.</S>
			<S sid ="115" ssid = "13">Contextual We extract the word C−1,−1 immediately before mi, the word C+1,+1 immediately after mi, and their associated POS tags P . NE tags We automatically annotate the sentences with named entity (NE) tags using the named entity tagger of (Ratinov and Roth, 2009).</S>
			<S sid ="116" ssid = "14">This tagger annotates proper nouns with the tags PER (person), ORG (organization), LOC (location), or MISC (miscellaneous).</S>
			<S sid ="117" ssid = "15">If the lw of mi coincides (actual token offset) with the lw of any NE annotated by the NE tagger, we extract the NE tag as a feature.</S>
			<S sid ="118" ssid = "16">Syntactic parse We parse the sentences using the syntactic parser of (Klein and Manning, 2003).</S>
			<S sid ="119" ssid = "17">We extract the label of the parse tree constituent (if it exists) that exactly covers the mention, and also labels of all constituents that covers the mention.</S>
			<S sid ="120" ssid = "18">4.2 Extracting Candidate Mentions.</S>
			<S sid ="121" ssid = "19">From a sentence, we gather the following as candidate mentions: all nouns and possessive pronouns, all named entities annotated by the the NE tagger (Ratinov and Roth, 2009), all base noun phrase (NP) chunks, all chunks satisfying the pattern: NP (PP NP)+, all NP constituents in the syntactic parse tree, and from each of these constituents, all substrings consisting of two or more words, provided the sub- strings do not start nor end on punctuation marks.</S>
			<S sid ="122" ssid = "20">These mention candidates are then fed to our mention entity typing (MET) classifier for type prediction (more details in Section 6.3).</S>
	</SECTION>
	<SECTION title="Relation Extraction System. " number = "5">
			<S sid ="123" ssid = "1">We build a supervised RE system using sentences annotated with entity mentions and predefined target relations.</S>
			<S sid ="124" ssid = "2">During evaluation, when given a pair of mentions mi, mj , the system predicts whether any of the predefined target relation holds between the mention pair.</S>
			<S sid ="125" ssid = "3">Most of our features are based on the work of (Zhou et al., 2005; Chan and Roth, 2010).</S>
			<S sid ="126" ssid = "4">Due to space limitations, we refer the reader to our prior work (Chan and Roth, 2010) for the lexical, structural, mention-level, entity type, and dependency features.</S>
			<S sid ="127" ssid = "5">Here, we only describe the features that were not used in that work.</S>
			<S sid ="128" ssid = "6">As part of our RE system, we need to extract the head word (hw) of a mention (m), which we heuristically determine as follows: if m contains a preposition and a noun preceding the preposition, we use the noun as the hw.</S>
			<S sid ="129" ssid = "7">If there is no preposition in m, we use the last noun in m as the hw.</S>
			<S sid ="130" ssid = "8">POS features If there is a single word between the two mentions, we extract its POS tag.</S>
			<S sid ="131" ssid = "9">Given the hw of m, Pi,j refers to the sequence of POS tags in the immediate context of hw (we exclude the POS tag of hw).</S>
			<S sid ="132" ssid = "10">The offsets i and j denote the position (relative to hw) of the first and last POS tag respectively.</S>
			<S sid ="133" ssid = "11">For instance, P−2,−1 denotes the sequence of two POS tags on the immediate left of hw, and P−1,+1 denotes the POS tag on the immediate left of hw and the POS tag on the immediate right of hw.</S>
			<S sid ="134" ssid = "12">Base phrase chunk We add a boolean feature to detect whether there is any base phrase chunk in the text span between the two mentions.</S>
	</SECTION>
	<SECTION title="Experiments. " number = "6">
			<S sid ="135" ssid = "1">We use the ACE-2004 dataset (catalog LDC2005T09 from the Linguistic Data Consortium) to conduct our experiments.</S>
			<S sid ="136" ssid = "2">Following prior work, we use the news wire (nwire) and broadcast news (bnews) corpora of ACE-2004 for tive, we compare against the state-of-the-art feature- based RE systems of (Jiang and Zhai, 2007) and (Chan and Roth, 2010).</S>
			<S sid ="137" ssid = "3">In these works, the authors reported performance on undirected coarse- grained RE.</S>
			<S sid ="138" ssid = "4">Performing 5-fold cross validation on the nwire and bnews corpora, (Jiang and Zhai, 2007) and (Chan and Roth, 2010) reported F-measures of 71.5 and 71.2, respectively.</S>
			<S sid ="139" ssid = "5">Using the same evaluation setting, our baseline RE system achieves a competitive 71.4 F-measure.</S>
			<S sid ="140" ssid = "6">We build three RE classifiers: binary, coarse, fine.</S>
			<S sid ="141" ssid = "7">Lumping all the predefined target relations into a single label, we build a binary classifier to predict whether any of the predefined relations exists between a given mention pair.</S>
			<S sid ="142" ssid = "8">In this work, we model the argument order of the mentions when performing RE, since relations are usually asymmetric in nature.</S>
			<S sid ="143" ssid = "9">For instance, we consider mi:EMPORG:mj and mj :EMPORG:mi to be distinct relation types.</S>
			<S sid ="144" ssid = "10">In our experiments, we extracted a total of 55,520 examples or mention pairs.</S>
			<S sid ="145" ssid = "11">Out of these, 4,011 are positive relation examples annotated with 6 coarse-grained relation types and 22 fine-grained relation types5.</S>
			<S sid ="146" ssid = "12">We build a coarse-grained classifier to disambiguate between 13 relation labels (two asymmetric labels for each of the 6 coarse-grained relation types and a null label).</S>
			<S sid ="147" ssid = "13">We similarly build a fine-grained classifier to disambiguate between 45 relation labels.</S>
			<S sid ="148" ssid = "14">6.1 Evaluation Method.</S>
			<S sid ="149" ssid = "15">For our experiments, we adopt the experimental setting in our prior work (Chan and Roth, 2010) of ensuring that all examples from a single document are either all used for training, or all used for evaluation.</S>
			<S sid ="150" ssid = "16">In that work, we also highlight that ACE annotators rarely duplicate a relation link for coreferent mentions.</S>
			<S sid ="151" ssid = "17">For instance, assume mentions mi, mj , and mk are in the same sentence, mentions mi and mj are coreferent, and the annotators tag the mention pair mj , mk with a particular relation r. The annotators will rarely duplicate the same (implicit) our experiments, which consists of 345 documents.</S>
			<S sid ="152" ssid = "18">To build our RE system, we use the LIBLINEAR (Fan et al., 2008) package, with its default settings of L2-loss SVM (dual) as the solver, and we use an epsilon of 0.1.</S>
			<S sid ="153" ssid = "19">To ensure that this baseline RE system based on the features in Section 5 is competi 5 We omit a single relation: Discourse (DISC).</S>
			<S sid ="154" ssid = "20">The ACE-.</S>
			<S sid ="155" ssid = "21">2004 annotation guidelines states that the DISC relation is established only for the purposes of the discourse and does not reference an official entity relevant to world knowledge.</S>
			<S sid ="156" ssid = "22">In this work, we focus on semantically meaningful relations.</S>
			<S sid ="157" ssid = "23">Furthermore, the DISC relation is dropped in ACE-2005.</S>
			<S sid ="158" ssid = "24">RE m od el 1 0 d o c u m e n t s Re c % Pre% F1% 5 % o f d a t a R ec % Pre% F1% 8 0 % o f d a t a R ec % Pre% F1% Bi na ry Bi na ry +P att ern s 5 8.</S>
			<S sid ="159" ssid = "25">0 80.3 67.4 7 3.</S>
			<S sid ="160" ssid = "26">1 78.5 75.7 (+8.3) 6 4.</S>
			<S sid ="161" ssid = "27">4 80.6 71.6 7 5.</S>
			<S sid ="162" ssid = "28">3 80.6 77.9 7 3.</S>
			<S sid ="163" ssid = "29">2 84.0 78.2 8 0.</S>
			<S sid ="164" ssid = "30">1 84.2 82.1 Co ar se Co ars e+ Pat ter ns 3 3.</S>
			<S sid ="165" ssid = "31">5 62.5 43.6 4 4.</S>
			<S sid ="166" ssid = "32">2 59.6 50.8 (+7.2) 4 2.</S>
			<S sid ="167" ssid = "33">4 66.2 51.7 5 1.</S>
			<S sid ="168" ssid = "34">2 64.2 56.9 6 2.</S>
			<S sid ="169" ssid = "35">1 75.5 68.1 6 8.</S>
			<S sid ="170" ssid = "36">0 75.4 71.5 Fi ne Fi ne +P att er ns 1 8.</S>
			<S sid ="171" ssid = "37">1 47.0 26.1 2 4.</S>
			<S sid ="172" ssid = "38">8 43.5 31.6 (+5.5) 2 6.</S>
			<S sid ="173" ssid = "39">3 51.6 34.9 3 2.</S>
			<S sid ="174" ssid = "40">2 48.9 38.9 5 1.</S>
			<S sid ="175" ssid = "41">6 68.4 58.8 5 6.</S>
			<S sid ="176" ssid = "42">4 67.5 61.5 Table 4: Micro-averaged (across the 5 folds) RE results using gold mentions.</S>
			<S sid ="177" ssid = "43">RE m od el 1 0 d o c u m e n t s Re c % Pre% F1% 5 % o f d a t a R ec % Pre% F1% 8 0 % o f d a t a R ec % Pre% F1% Bi na ry Bi na ry +P att ern s 3 2.</S>
			<S sid ="178" ssid = "44">2 46.6 38.1 4 6.</S>
			<S sid ="179" ssid = "45">3 5.</S>
			<S sid ="180" ssid = "46">5 48.9 41.1 4 7.</S>
			<S sid ="181" ssid = "47">6 47.8 47.2 4 0.</S>
			<S sid ="182" ssid = "48">1 52.7 45.5 5 0.</S>
			<S sid ="183" ssid = "49">2 50.4 50.3 Co ar se Co ars e+ Pat ter ns 1 8.</S>
			<S sid ="184" ssid = "50">6 41.1 25.6 2 6.</S>
			<S sid ="185" ssid = "51">8 34.7 30.2 (+4.6) 2 2.</S>
			<S sid ="186" ssid = "52">4 40.9 28.9 3 0.</S>
			<S sid ="187" ssid = "53">3 37.0 33.3 3 2.</S>
			<S sid ="188" ssid = "54">3 47.5 38.5 3 8.</S>
			<S sid ="189" ssid = "55">9 42.9 40.8 Fi ne Fi ne +P att er ns 1 0.</S>
	</SECTION>
	<SECTION title="32.2	16.1" number = "7">
			<S sid ="190" ssid = "1">1 5.</S>
			<S sid ="191" ssid = "2">7 26.3 19.7 (+3.6) 1 4.</S>
			<S sid ="192" ssid = "3">6 33.4 20.3 1 9.</S>
			<S sid ="193" ssid = "4">4 29.2 23.3 2 6.</S>
			<S sid ="194" ssid = "5">9 44.3 33.5 3 1.</S>
			<S sid ="195" ssid = "6">7 38.3 34.7 Table 5: Micro-averaged (across the 5 folds) RE results using predicted mentions.</S>
			<S sid ="196" ssid = "7">relation r between mi and mk , thus leaving the gold relation label as null.</S>
			<S sid ="197" ssid = "8">Whether this is correct or not is debatable.</S>
			<S sid ="198" ssid = "9">However, to avoid being penalized when our RE system actually correctly predicts the label of an implicit relation, we take the following approach.</S>
			<S sid ="199" ssid = "10">During evaluation, if our system correctly predicts an implicit label, we simply switch its prediction to the null label.</S>
			<S sid ="200" ssid = "11">Since the RE recall scores only take into account non-null relation labels, this scoring method does not change the recall, but could marginally increase the precision scores by decreasing the count of RE predictions.</S>
			<S sid ="201" ssid = "12">In our experiments, we observe that both the usual and our scoring method give very similar RE results and the experimental trends remain the same.</S>
			<S sid ="202" ssid = "13">Of course, using this scoring method requires coreference information, which is available in the ACE data.</S>
			<S sid ="203" ssid = "14">6.2 RE Evaluation Using Gold Mentions.</S>
			<S sid ="204" ssid = "15">To perform our experiments, we split the 345 documents into 5 equal sets.</S>
			<S sid ="205" ssid = "16">In each of the 5 folds, 4 sets (276 documents) are reserved for drawing training examples, while the remaining set (69 documents) is used as evaluation data.</S>
			<S sid ="206" ssid = "17">In the experiments described in this section, we use the gold mentions available in the data.When one only has a small amount of train ing data, it is crucial to take advantage of external knowledge such as the syntactico-semantic structures.</S>
			<S sid ="207" ssid = "18">To simulate this setting, in each fold, we randomly selected 10 documents from the fold’s available training documents (about 3% of the total 345 documents) as training data.</S>
			<S sid ="208" ssid = "19">We built one binary, one coarse-grained, and one fine-grained classifier for each fold.</S>
			<S sid ="209" ssid = "20">In Section 2, we described how we trained a baseline RE classifier (REbase) and a RE classifier using the syntactico-semantic patterns (REs).</S>
			<S sid ="210" ssid = "21">We first apply REbase on each test example mention pair (mi,mj ) to obtain the RE baseline results, showing these in Table 4 under the column “10 documents”, and in the rows “Binary”, “Coarse”, and “Fine”.</S>
			<S sid ="211" ssid = "22">We then applied REs on the test examples as described in Section 2, showing the results in the rows “Binary+Patterns”, “Coarse+Patterns”, and “Fine+Patterns”.</S>
			<S sid ="212" ssid = "23">The results show that by using syntactico-semantic structures, we obtain significant F-measure improvements of 8.3, 7.2, and 5.5 for binary, coarse-grained, and fine-grained relation predictions respectively.</S>
			<S sid ="213" ssid = "24">6.3 RE Evaluation Using Predicted Mentions.</S>
			<S sid ="214" ssid = "25">Next, we perform our experiments using predicted mentions.</S>
			<S sid ="215" ssid = "26">ACE-2004 defines 7 coarse-grained entity types, each of which are then refined into 43 fine Improvement in (gold mentions) RE by using patterns 8 Binary+Pattern 7 Coarse+Pattern.</S>
			<S sid ="216" ssid = "27">Fine+Pattern 6 Improvement in (predicted mentions) RE by using patterns 8 Binary+Pattern 7 Coarse+Pattern.</S>
			<S sid ="217" ssid = "28">Fine+Pattern 6 5 5 4 4 3 3 2 2 1 1 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 Proportion (%) of data used for training 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 Proportion (%) of data used for training Figure 3: Improvement in (gold mention) RE.</S>
			<S sid ="218" ssid = "29">grained entity types.</S>
			<S sid ="219" ssid = "30">Using the ACE data annotated with mentions and predefined entity types, we build a fine-grained mention entity typing (MET) classifier to disambiguate between 44 labels (43 fine- grained and a null label to indicate not a mention).</S>
			<S sid ="220" ssid = "31">To obtain the coarse-grained entity type predictions from the classifier, we simply check which coarse- grained type the fine-grained prediction belongs to.</S>
			<S sid ="221" ssid = "32">We use the LIBLINEAR package with the same settings as earlier specified for the RE system.</S>
			<S sid ="222" ssid = "33">In each fold, we build a MET classifier using all the (276) training documents in that fold.</S>
			<S sid ="223" ssid = "34">We apply REbase on all mention pairs (mi,mj ) where both mi and mj have non null entity type predictions.</S>
			<S sid ="224" ssid = "35">We show these baseline results in the Rows “Binary”, “Coarse”, and “Fine” of Table 5.</S>
			<S sid ="225" ssid = "36">In Section 2, we described our algorithmic approach (Figure 2) that takes advantage of the structures with predicted mentions.</S>
			<S sid ="226" ssid = "37">We show the results of this approach in the Rows “Binary+Patterns”, “Coarse+Patterns”, and “Fine+Patterns” of Table 5.</S>
			<S sid ="227" ssid = "38">The results show that by leveraging syntactico-.</S>
			<S sid ="228" ssid = "39">semantic structures, we obtain significant F-measure improvements of 8.2, 4.6, and 3.6 for binary, coarse- grained, and fine-grained relation predictions respectively.</S>
			<S sid ="229" ssid = "40">7 Analysis.</S>
			<S sid ="230" ssid = "41">We first show statistics regarding the syntactico- semantic structures.</S>
			<S sid ="231" ssid = "42">In Section 3, we mentioned that ACE-2004 identified five types of structures: premodifier, possessive, preposition, formulaic, and Figure 4: Improvement in (predicted mention) RE.</S>
			<S sid ="232" ssid = "43">Pat ter n ty pe Re c % Pr e % Pr e M od 8 6.</S>
			<S sid ="233" ssid = "44">8 7 9.</S>
			<S sid ="234" ssid = "45">7 Po ss 9 4.</S>
			<S sid ="235" ssid = "46">3 8 8.</S>
			<S sid ="236" ssid = "47">3 Pr ep 9 4.</S>
			<S sid ="237" ssid = "48">6 2 0.</S>
			<S sid ="238" ssid = "49">0 Fo rm ul a 8 5.</S>
			<S sid ="239" ssid = "50">5 6 2.</S>
			<S sid ="240" ssid = "51">2 Table 6: Recall and precision of the patterns.</S>
			<S sid ="241" ssid = "52">verbal.</S>
			<S sid ="242" ssid = "53">On the 4,011 examples that we experimented on, premodifiers are the most frequent, accounting for 30.5% of the examples (or about 1,224 examples).</S>
			<S sid ="243" ssid = "54">The occurrence distributions of the other structures are 18.9% (possessive), 23.9% (preposition), 7.2% (formulaic), and 19.5% (verbal).</S>
			<S sid ="244" ssid = "55">Hence, the four syntactico-semantic structures that we focused on in this paper account for a large majority (80%) of the relations.</S>
			<S sid ="245" ssid = "56">In Section 6, we note that out of 55,520 mention pairs, only 4,011 exhibit valid relations.</S>
			<S sid ="246" ssid = "57">Thus, the proportion of positive relation examples is very sparse at 7.2%.</S>
			<S sid ="247" ssid = "58">If we can effectively identify and discard most of the negative relation examples, it should improve RE performance, including yielding training data with a more balanced label distribution.</S>
			<S sid ="248" ssid = "59">We now analyze the utility of the patterns.</S>
			<S sid ="249" ssid = "60">As shown in Table 6, the patterns are effective in inferring the structure of mention pairs.</S>
			<S sid ="250" ssid = "61">For instance, applying the premodifier patterns on the 55,520 mention pairs, we correctly identified 86.8% of the 1,224 premodifier occurrences as premodifiers, while incurring a false-positive rate of only about 20%6.</S>
			<S sid ="251" ssid = "62">We 6 Random selection will give a precision of about 2.2% (1,224 out of 55,520) and thus a false-positive rate of 97.8% note that preposition structures are relatively harder to identify.</S>
			<S sid ="252" ssid = "63">Some of the reasons are due to possibly multiple prepositions in between a mention pair, preposition sense ambiguity, pp-attachment ambiguity, etc. However, in general, we observe that inferring the structures allows us to discard a large portion of the mention pairs which have no valid relation between them.</S>
			<S sid ="253" ssid = "64">The intuition behind this is the following: if we infer that there is a syntactico- semantic structure between a mention pair, then it is likely that the mention pair exhibits a valid relation.</S>
			<S sid ="254" ssid = "65">Conversely, if there is a valid relation between a mention pair, then it is likely that there exists a syntactico-semantic structure between the mentions.</S>
			<S sid ="255" ssid = "66">Next, we repeat the experiments in Section 6.2 and Section 6.3, while gradually increasing the amount of training data used for training the RE classifiers.</S>
			<S sid ="256" ssid = "67">The detailed results of using 5% and 80% of all available data are shown in Table 4 and Table 5.</S>
			<S sid ="257" ssid = "68">Note that these settings are with respect to all 345.</S>
			<S sid ="258" ssid = "69">documents and thus the 80% setting represents using all 276 training documents in each fold.</S>
			<S sid ="259" ssid = "70">We plot the intermediate results in Figure 3 and Figure 4.</S>
			<S sid ="260" ssid = "71">We note that leveraging the structures provides improvements on all experimental settings.</S>
			<S sid ="261" ssid = "72">Also, intuitively, the binary predictions benefit the most from leveraging the structures.</S>
			<S sid ="262" ssid = "73">How to further exploit this is a possible future work.</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "8">
			<S sid ="263" ssid = "1">In this paper, we propose a novel algorithmic approach to RE by exploiting syntactico-semantic structures.</S>
			<S sid ="264" ssid = "2">We show that this approach provides several advantages and improves RE performance.</S>
			<S sid ="265" ssid = "3">There are several interesting directions for future work.</S>
			<S sid ="266" ssid = "4">There are probably many near misses when we apply our structure patterns on predicted mentions.</S>
			<S sid ="267" ssid = "5">For instance, for both premodifier and possessive structures, we require that one mention completely includes the other.</S>
			<S sid ="268" ssid = "6">Relaxing this might potentially recover additional valid mention pairs and improve performance.</S>
			<S sid ="269" ssid = "7">We could also try to learn classifiers to automatically identify and disambiguate between the different syntactico-semantic structures.</S>
			<S sid ="270" ssid = "8">It will also be interesting to feedback the predictions of the structure patterns to the mention entity typing classifier and possibly retrain to obtain a better classifier.</S>
			<S sid ="271" ssid = "9">Acknowledgements This research is supported by the Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract no.</S>
			<S sid ="272" ssid = "10">FA875009-C-0181.</S>
			<S sid ="273" ssid = "11">Any opinions, findings, and conclusion or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the view of the DARPA, AFRL, or the US government.</S>
			<S sid ="274" ssid = "12">We thank MingWei Chang and Quang Do for building the mention extraction system.</S>
	</SECTION>
</PAPER>
