High OOV-Recall Chinese Word Segmenter
Xiaoming Xu, Muhua Zhu, Xiaoxu Fei, and Jingbo Zhu
School of
Information Science and Engineering
Northeastern University
{xuxm, zhumh, feixx}@ics.neu.edu.cn
zhujingbo@mail.neu.edu.cn
Abstract
For the competition of Chinese word seg-
mentation held in the first CIPS-SIGHNA
joint conference. We applied a subword-
based word segmenter using CRFs and ex-
tended the segmenter with OOV words
recognized by Accessor Variety. More-
over, we proposed several post-processing
rules to improve the performance. Our
system achieved promising OOV recall
among all the participants.
1 Introduction
Chinese word segmentation is deemed to be a pre-
requisite for Chinese language processing. The
competition in the first CIPS-SIGHAN joint con-
ference put the task of Chinese word segmenta-
tion in a more challengeable setting, where train-
ing and test data are obtained from different do-
mains. This setting is widely known as domain
adaptation.
For domain adaptation, either a large-scale un-
labeled target domain data or a small size of la-
beled target domain data is required to adapt a
system built on source domain data to the tar-
get domain. In this word segmentation competi-
tion, unfortunately, only a small size of unlabeled
target domain data is available. Thus we focus
on handling out-of-vocabulary (OOV) words. For
this purpose, our system is based on a combina-
tion of subword-based tagging method (Zhang et
al., 2006) and accessor variety-based new word
recognition method (Feng et al, 2004). In more
detail, we adopted and extended subword-based
method. Subword list is augmented with new-
word list recognized by accessor variety method.
Feature Template Description
a) cn(?2,?1, 0, 1, 2) unigram of characters
b) cncn+1(?2,?1, 0, 1) bigram of characters
c) cn?1cncn+1(?1, 0, 1) trigram of characters
d) Pu(C0) whether punctuation
e) T (C?1)T (C0)T (C+1) type of characters
Table 1: Basic Features for CRF-based Segmenter
We participated in the close track of the word
segmentation competition, on all the four test
datasets, in two of which our system is ranked at
the 1st position with respect to the metric of OOV
recall.
2 System Description
2.1 Subword-based Tagging with CRFs
The backbone of our system is a character-based
segmenter with the application of Conditional
Random Fields (CRFs) (Zhao and Kit, 2008). In
detail, we apply a six-tag tagging scheme, as in
(Zhao et al, 2006). That is , each Chinese char-
acter can be assigned to one of the tags in {B,
B2, B3, M , E, S }. Refer to (Zhao et al, 2006)
for detailed meaning of the tags. Table 1 shows
basic feature templates used in our system, where
feature templates a, b, d, e are also used in (Zhu et
al., 2006) for SVM-based word segmentation.
In order to extend basic CRF-based segmenter,
we first collect 2k most frequent words from train-
ing data. Hereafter, the list of such words is
referred to as subword list. Moreover, single-
character words 1, if they are not contained in
the subword list, are also added. Such proce-
1By single-character word, we refer to words that consist
solely of a Chinese character.
Feature Template Description
f) in(str, subword-list) is str in subword list
g) in(str, confident-word-list) is str in confident-word
list
Table 2: Subword Features for CRF-based Seg-
menter
dure for constructing a subword list is similar to
the one used in (Zhang et al, 2006). To en-
hance the effect of subwords, we go one step
further to build a list, named confident-word list
here and below, which contains words that are
not a portion of other words and are never seg-
mented in the training data. In the competition,
400 most frequent words in the confident-word list
are used. With subword list and confident-word
list, both training and test data are segmented
with forward maximum match method by using
the union of subword list and confident-word list.
Each segmentation unit (single-character or multi-
character unit) in the segmentation results are re-
garded as ?pseudo character? and thus can be rep-
resented with the basic features in Table 1 and
two additional features as shown in Table 2. See
the details of subword-based Chinese word seg-
mentation in (Zhang et al, 2006)
2.2 OOV Recognition with Accessor Variety
Accessor variety (AV) (Feng et al, 2004) is a sim-
ple and effective unsupervised method for extrac-
tion of new Chinese words. Given a unsegmented
text, each substring (candidate word) in the text
can be assigned a value according to the follow-
ing equation:
AV (s) = min{Lav(s), Rav(s)} (1)
where the left and right AV values, Lav(s) and
Rav(s) are defined to be the number of distinct
character types appearing on the left and right,
respectively. Candidate words are sorted in the
descending order of AV values and most highly
ranked ones can be chosen as new words. In
practical applications, heuristic filtering rules are
generally needed (Feng et al, 2004). We re-
implemented the AV method and filtering rules,
as in (Feng et al, 2004). Moreover, we filter out
candidate words that have AV values less than 3.
Unfortunately, candidate word list generated this
way still contains many noisy words (substrings
that are not words). One possible reason is that
unlabeled data (test data) used in the competition
is extremely small in size. In order to refine the
results derived from the AV method, we make use
of the training data to filter the results from two
different perspectives.
? Segment test data with the CRF-based seg-
menter described above. Then we collect
(candidate) words that are in the CRF-based
segmentation results, but not appear in the
training data. Such words are called CRF-
OOV words hereafter. We retain the intersec-
tion of CRF-OOV words and AV-based re-
sults as the set of candidate words to be pro-
cessed by the following step.
? Any candidate word in the intersection of
CRF-based and AV-based results will be fil-
tered out if they satisfy one of the following
conditions: 1) the candidate word is a part of
some word in the training data; 2) the candi-
date word is formed by connection of consec-
utive words in the training data; 3) the candi-
date word contains position words, such as
? (up), ? (down),? (left),? (right), etc.
Moreover, we take all English words in test data
as OOV words. A simple heuristic rule is defined
for the purpose of English word recognition: an
English word is a consecutive sequence of English
characters and punctuations between two English
characters (including these two characters).
We finally add all the OOV words into subword
list and confident-word list.
3 Post-Processing Rules
In the results of subword-based word segmenta-
tion with CRFs, we found some errors could be
corrected with heuristic rules. For this purpose,
we propose following post-processing rules, for
handling OOV and in-vocabulary (IV) words, re-
spectively.
3.1 OOV Rules
3.1.1 Annotation-Standard Independent
Rules
We assume the phenomena discussed in the fol-
lowing are general across all kinds of annotation
standards. Thus corresponding rules can be ap-
plied without considering annotation standards of
training data.
? A punctuation tends to be a single-character
word. If a punctation?s previous character
and next character are both Chinese charac-
ters, i.e. not punctuation, digit, or English
character, we always regard the punctuation
as a word.
? Consecutive and identical punctuations tend
to be joined together as a word. For exam-
ple, ??? represents a Chinese hyphen which
consists of three ?-?, and ?!!!? is used to
show emphasizing. Inspired by this obser-
vations, we would like to unite consecutive
and identical punctuations as a single word.
? When the character ??? appears in the train-
ing data, it is generally used as a connec-
tions symbol in a foreign person name, such
as ????? (Saint John)?. Taking this ob-
servation into consideration, we always unite
the character ??? and its previous and next
segment units into a single word. A similar
rule is designed to unite consecutive digits on
the sides of the symbol ?.?, ex. ?1.11?.
? We notice that four consecutive characters
which are in the pattern of AABB generally
form a single word in Chinese, for example
????? (dull)?. Taking this observation
into account, we always unite consecutive
characters in the AABB into a single word.
3.1.2 Templates with Generalized Digits
Words containing digits generally belong to a
open class, for example, the word ?2012? (AD
2012?? means a date. Thus CRF-based seg-
menter has difficulties in recognizing such words
since they are frequently OOV words. To attack
this challenge, we first generalize digits in the
training data. In detail, we replaced consecutive
digits with ?*?. For example, the word ?2012??
will be transformed into ?*??. Second, we col-
lect word templates which consist of three con-
secutive words on condition that at least one of
the words in a template contains the character ?*?
and that the template appears in the training data
more than 4 times. For example, we can get a
template like ?*?(month) *?(day)?(publish)?.
With such templates, we are able to correct errors,
say ?10? 17??? into ?10? 17???.
3.2 IV Rules
We notice that long words have less ambiguity
than short words in the sense of being words.
For example, characters in ????? ?full
of talents)? always form a word in the training
data, whereas ???? have two plausible split-
ting forms, as ??? (talent)? or ?? (people) ?
(only)?. In our system, we collect words that have
at least four characters and filter out words which
belong to one of following cases: 1) the word is
a part of other words; 2) the word consists solely
of punctation and/or digit. For example, ???
?? (materialism)? and ????? (120)? are
discarded, since the former is a substring of the
word ?????? (materialist)? and the latter is
a word of digits. Finally we get a list containing
about 6k words. If a character sequence in the test
data is a member in the list, it is retained as a word
in the final segmentation results.
Another group of IV rules concern character
sequences that have unique splitting in the train-
ing data. For example, ???? (women)? is al-
ways split as ??? (woman) ? (s)?. Hereafter,
we refer to such character sequences as unique-
split-sequence (USS). In our system, we are con-
cerned with UUSs which are composed of less
than 5 words. In order to apply UUSs for post-
processing, we first collect word sequence of vari-
able length (word number) from training data. In
detail, we collect word sequences of two words,
three words, and four words. Second, word se-
quences that have more than one splitting cases
in the training data are filtered out. Third, spaces
between words are removed to form USSs. For
example, the words ??? (woman) ? (s)? will
form the USS ???? ?. Finally, we search the
test data for each USS. If the searching succeeds,
the USS will be replaced with the corresponding
word sequence.
4 Evaluation Results
We evaluated our Chinese word segmenter in the
close track, in four domain: literature (Lit), com-
Domain Basic +OOV +OOV+IV
ROV RIV F ROV RIV F ROV RIV F
Lit .643 .946 .927 .652 .947 .929 .648 .952 .934
Com .839 .961 .938 .850 .961 .941 .852 .965 .947
Med .725 .938 .912 .754 .939 .917 .756 .944 .923
Fin .761 .956 .932 .854 .958 .950 .871 .961 .955
Table 3: Effectiveness of post-processing rules
puter (Com), medicine (Med) and finance (Fin).
The results are depicted in Table 4, where R,
P and F refer to Recall, Precision, F measure
respectively, and ROOV and RIV refer to recall
of OOV and IV words respectively. Since OOV
words are the obstacle for practical Chinese word
segmenters to achieve high accuracy, we have spe-
cial interest in the metric of OOV recall. We
found that our system achieved high OOV recall
2
. Actually, OOV recall of our system in the do-
mains of computer and finance are both ranked at
the 1st position among all the participants. Com-
pared with the systems ranked second in these
two domains, our system achieved OOV recall
.853 vs. .827 and .871 vs. .857 respectively.
We also examined the effectiveness of post-
processing rules, as shown in Table 3, where
Basic represents the performance achieved be-
fore post-processing, +OOV represents the results
achieved after applying OOV post-processing
rules, and +OOV+IV denotes the results achieved
after using all the post-processing rules, including
both OOV and IV rules. As the table shows, de-
signed post-processing rules can improve both IV
and OOV recall significantly.
Domain R P F ROOV RIV
Lit .931 .936 .934 .648 .952
Com .948 .945 .947 .853 .965
Med .924 .922 .923 .756 .944
Fin .953 .956 .955 .871 .961
Table 4: Performance of our system in the compe-
tition
2For the test data from the domain of literature, we actu-
ally use combination of our system and forward maximum
match, so we will omit the results on this test dataset in our
discussion.
5 Conclusions and Future Work
We proposed an approach to refine new words rec-
ognized with the accessor variety method, and in-
corporated such words into a subword-based word
segmenter. We found that such method could
achieve high OOV recall. Moreover, we designed
effective post-processing rules to further enhance
the performance of our systems. Our system fi-
nally achieved satisfactory results in the competi-
tion.
Acknowledgments
This work was supported in part by the National
Science Foundation of China (60873091).
References
Feng, Haodi, Kang Chen, Xiaotie Deng, and Weimin
zhang. 2004. Accessor Variety Criteriafor Chinese
Word Extraction. Computational Linguistics 2004,
30(1), pages 75-93.
Zhang, Ruiqiang, Genichiro Kikui, and Eiichiro
Sumita. 2006. Subword-based Tagging by Condi-
tional Random Fileds for Chinese Word Segmenta-
tion. In Proceedings of HLT-NAACL 2006, pages
193-196.
Zhao, Hai, Chang-Ning Huang, and Mu Li. 2006.
Improved Chinese Word Segmentation System with
Conditional Random Field. In Proceedings of
SIGHAN-5 2006, pages 162-165.
Zhao, Hai and Chunyu Kit. 2008. Unsupervised Seg-
mentation Helps Supervised Learning of Character
Tagging for Word Segmentation and Named Entity
Recognition. In Proceedings of SIGHAN-6 2008,
pages 106-111.
Zhu, Muhua, Yiling Wang, Zhenxing Wang, Huizhen
Wang, and Jingbo Zhu. 2006. Designing Spe-
cial Post-Processing Rules for SVM-based Chinese
Word Segmentation. In Proceedigns of SIGHAN-5
2006, pages 217-220.
