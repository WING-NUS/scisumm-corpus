The Character-based CRF Segmenter of MSRA&NEU 
 for the 4th Bakeoff 
Zhenxing Wang1,2, Changning Huang2 and Jingbo Zhu1 
1 Institute of Computer Software and Theory, Northeastern University,  
Shenyang, China, 110004 
2 Microsoft Research Asia, 49, Zhichun Road,  
Haidian District, Beijing, China, 100080 
zxwang@ics.neu.edu.cn 
v-cnh@microsoft.com 
zhujingbo@mail.neu.edu.cn 
 
Abstract 
This paper describes the Chinese Word 
Segmenter for the fourth International 
Chinese Language Processing Bakeoff. 
Base on Conditional Random Field (CRF) 
model, a basic segmenter is designed as a 
problem of character-based tagging.  To 
further improve the performance of our 
segmenter, we employ a word-based ap-
proach to increase the in-vocabulary (IV) 
word recall and a post-processing to in-
crease the out-of-vocabulary (OOV) word 
recall. We participate in the word segmen-
tation closed test on all five corpora and 
our system achieved four second best and 
one the fifth in all the five corpora.  
1 Introduction 
Since Chinese Word Segmentation was firstly 
treated as a character-based tagging task in (Xue 
and Converse, 2002), this method has been widely 
accepted and further developed by researchers 
(Peng et al, 2004), (Tseng et al, 2005), (Low et 
al., 2005), (Zhao et al, 2006). Thus, as a powerful 
sequence tagging model, CRF became the domi-
nant method in the Bakeoff 2006 (Levow, 2006).  
      In this paper, we improve basic segmenter un-
der the CRF work frame in two aspects, namely 
IV and OOV identification respectively. We use 
the result from word-based segmentation to revise 
the CRF output so that we gain a higher IV word 
recall. For the OOV part a post-processing rule is 
proposed to find those OOV words which are 
wrongly segmented into several fractions. Our 
system performs well in the Fourth Bakeoff, 
achieving four second best and on the fifth in all 
the five corpora. In the following of this paper, we 
describe our method in more detail.    
      The rest of this paper is organized as follows. 
In Section 2, we first give a brief review to the 
basic CRF tagging approach and then we propose 
our methods to improve IV and OOV performance 
respectively. In Section 3 we give the experiment 
results on the fourth Bakeoff corpora to show that 
our method is effective to improve the perfor-
mance of the segmenter. In Section 4, we con-
clude our work. 
2 Our Word Segmentation System 
In this section, we describe our system in more 
detail. Our system includes three modules: a basic 
CRF tagger, a word-base segmenter to improve 
the IV recall and a post-processing rule to 
improve the OOV recall. In the following of this 
section, we introduce these three modules 
respectively. 
2.1 Basic CRF tagger 
Sequence tagging approach treat Word Segmenta-
tion task as a labeling problem. Every character in 
input sentences will be given a label which indi-
cates whether this character is a word boundary. 
Our basic CRF1 tagger is almost the same as the 
system described in (Zhao et al, 2006) except we 
add a feature to incorporate word information, 
which is learned from training corpus.  
                                               
1 CRF tagger in this paper  is implemented by CRF++ 
which is downloaded from http://crfpp.sourceforge.net/ 
98
Sixth SIGHAN Workshop on Chinese Language Processing
Type Feature Function 
Unigram C-1, C0, C1 Previous, current and next character 
Bigram C-1 C0, C0 C1 Two adjacent character  
Jump C-1 C1 Previous character and next character 
Word Flag F0 F1 Whether adjacent characters form an IV word 
  Table 1 Feature templates used for CRF in our system 
 Under the CRF tagging scheme, each character 
in one sentence will be given a label by CRF 
model to indicate which position this character 
occupies in a word. In our system, CRF tag set is 
proposed to distinguish different positions in the 
multi-character words when the word length is 
less than 6, namely 6-tag set {B, B2, B3, M, E, 
O}. Here, Tag B and E stand for the first and the 
last position in a multi-character word, respective-
ly. S stands up a single-character word. B2 and B3 
stand for the second and the third position in a 
multi-character word, whose length is larger than 
two-character or three-character. M stands for the 
fourth or more rear position in a multi-character 
word, whose length is larger than four-character. 
      We add a new feature, which also used in 
maximum entropy model for word segmentation 
task by (Low et al, 2005), to the feature templates 
for CRF model while keep the other features same 
as (Zhao et al, 2006). The feature templates are 
defined in table 1. In the feature template, only the 
Word Flag feature needs an explanation. The bi-
nary function F0 = 1 if and only if C-1 C0  form a IV 
word, else F0 = 0 and F1 = 1 if and only if C0 C1 
form a IV word, else F1 = 0.   
2.2 Word based segmenter and revise rules 
For the word-based word segmentation, we collect 
dictionary from training corpus first. Instead of 
Maximum Match, trigram language model 2 
trained on training corpus is employed for disam-
biguation. During the disambiguation procedure, a 
beam search decoder is used to seek the most 
possible segmentation. For detail, the decoder 
reads characters from the input sentence one at a 
time, and generates candidate segmentations in-
crementally. At each stage, the next incoming cha-
racter is combined with an existing candidate in 
two different ways to generate new candidates: it 
is either appended to the last word in the candidate, 
or taken as the start of a new word. This method 
guarantees exhaustive generation of possible seg-
                                               
2 Language model used in this paper is SLRIM down-
loaded from http://www.speech.sri.com/projects/srilm/ 
mentations for any input sentence. However, the 
exponential time and space of the length of the 
input sentence are needed for such a search and it 
is always intractable in practice. Thus, we use the 
trigram language model to select top B (B is a 
constant predefined before search and in our expe-
riment 3 is used) best candidates with highest 
probability at each stage so that the search algo-
rithm can work in practice. Finally, when the 
whole sentence has been read, the best candidate 
with the highest probability will be selected as the 
segmentation result.  
      After we get word-based segmentation result, 
we use it to revise the CRF tagging result similar 
to (Zhang et al, 2006). Since word-based segmen-
tation result also corresponds to a tag sequence 
according to the 6-tag set, we now have two tags 
for each character, word-based tag (WT) and CRF 
tag (CT). Which tag will be kept as the final result 
depends on Marginal Probability (MP) of the CT. 
      Here, we give a short explanation about what 
is the MP of the CT. Suppose there is a sentence 
McccC ...10? , where ic  is the character this sen-
tence containing. CRF model gives this sentence a 
optimal tag sequence 
MtttT ...10? , where it is the 
tag for 
ic . If tti ? and },,,,,{ 32 SEMBBBt ? , 
the MP of 
it is defined as: 
?
? ???
T
ttT
i CTP
CTP
tt i )|(
)|(
)(MP ,
 
Here, )|( CTP is the conditional probability giv-
en by CRF model. For more detail about how to 
calculate this conditional probability, please refer 
to (Lafferty et al, 2001). 
      Assume that the tag assigned to the current 
character is CT by CRF and WT by word-based 
segmenter respectively. The rules under which we 
revise CRF result with word-based result is that if 
MP(CT) of a character is less than a predefined 
threshold and WT is not ?S?, the WT of this cha-
racter will be kept as the final result, else the CT 
of the character will be kept as the final result.  
99
Sixth SIGHAN Workshop on Chinese Language Processing
      The restriction that WT should not be ?S? is 
reasonable because word-based segmentation is 
incapable to recognize the OOV word and always 
segments OOV word into single characters. Be-
sides CRF model is better at dealing with OOV 
word than our word-based segmentation. When 
WT is ?S? it is possible that current word is an 
OOV word and segmented into single character 
wrongly by the word-based segmenter, so the CT 
of the character should be kept under such situa-
tion. For more detail about this analysis please 
refer to (Wang et al, 2008). 
2.3 Post-processing rule  
The rules we described in last subsection is help-
ful to improve the IV word recall and now we in-
troduce our post-processing rule to improve the 
OOV recall.  
      Our post-processing rule is designed to deal 
with one typical type of OOV errors, namely an 
OOV word wrongly segmented into several parts.  
In practice many OOV errors belong to such type. 
      The rule is quite simple. When we read a sen-
tence from the result we get by the last step, we 
also kept the last N sentences in memory, in our 
system we set N equals to 20. We do this because 
adjacent sentences are always relevant and some 
named entity likely occurs repeatedly in these sen-
tences. Then, we scan these sentences to find all 
n-grams (n from 2 to 7) and count their occur-
rence. If certain n-gram appears more than a thre-
shold and this n-gram never appears in training 
corpus, the n-gram will be selected as a word can-
didate. Then, we filter these word candidates ac-
cording to the context entropy (Luo and Song, 
2004). Assume w  is a word candidate appears 
n times in the current sentence and last N sen-
tences and },...,,{ 10 laaa?? is the set of left side 
characters of w . Left Context Entropy (LCE) can 
be defined as: 
?
?
?
?ia i
i waC
nwaCnwLCE ),(log),(
1)(
 
Here, ),( waC i is the count of concurrence of 
ia and w . For the Right Context Entropy, the de-
finition is the same except change left into right. 
Now, we define Context Entropy (CE) of a word 
candidate w as ))(),(min( wRCEwLCE . The 
word candidates with CE larger than a predefined 
threshold will be bind as a whole word in test cor-
pus no matter what tag sequence the segmenter 
giving it. If a shorter n-gram is contained in a 
longer n-gram and both of them satisfy the above 
condition, the shorter n-gram will be overlooked 
and the longer n-gram is bind as a whole word. 
3 Evaluation of Our System 
On the corpora of the Fourth Bakeoff, we evaluate 
our system.  We carry out our evaluation on the 
closed tracks. It means that we do not use any ad-
ditional knowledge beyond the training corpus. 
The thresholds set for MP and CE on each corpus 
are tuned on left-out data of training corpus by 
cross validation. To analyze our methods on IV 
and OOV words, we use a detailed evaluation me-
tric than Bakeoff 2006 (Levow, 2006) which in-
cludes Foov and Fiv. Our results are shown in Ta-
ble 2. In Table 2, the row ?Basic Model? means 
the results produced by our basic CRF tagger, the 
row ?+IV? means the results produced by the 
combination of CRF tagger and word-based seg-
menter and the row ?+IV+OOV? means the result 
we get by executing post-processing rule on the 
combination results. The F measure of the basic 
CRF tagger alone in the Table 2 is within the top 
three in the closed tests except Cityu. Performance 
on Cityu corpus is not so good because the incon-
sistencies existing in Cityu training and test corpo-
ra. In the training corpus the quotation marks are
??while in test corpus quotation marks are??, 
which never apper in the training corpus. As a 
reult, a lot of errors were caused by quotation 
marks. For example, the following four character
????were combined as a one word in our 
result and fragment????was tagged as two 
words?? and ??. Because CRF tagger never 
met ? and ? in training corpus so the tagger 
gave the most common tags, namely B and E to 
the quotation marks, which cause segmentation 
errors not only on quotation marks themselves but 
also on the characters adjacent to them. We 
remove these inconsistencies munually and got 
the F measure 0.5 percentage higer than the rusult 
in table 2. This result is within the top three in the 
closed tests. On all the five corpora, our ?+IV? 
module can increase the Fiv and our ?+OOV? 
module can increase Foov respectively. However, 
these improvements are not significant.  
100
Sixth SIGHAN Workshop on Chinese Language Processing
Corpus Method R P F ROOV POOV FOOV RIV PIV FIV 
CKIP 
Basic Model 0.946 0.923 0.940 0.651 0.719 0.683 0.969 0.948 0.958 
+ IV 0.949 0.935 0.942 0.647 0.741 0.691 0.973 0.948 0.960 
+ IV + OOV 0.950 0.936 0.943 0.656 0.748 0.699 0.973 0.949 0.961 
CityU 
Basic Model 0.944 0.934 0.939 0.654 0.721 0.686 0.970 0.951 0.960 
+ IV 0.946 0.936 0.941 0.655 0.738 0.694 0.972 0.951 0.962 
+ IV + OOV 0.949 0.937 0.943 0.678 0.759 0.716 0.973 0.951 0.962 
CTB 
Basic Model 0.953 0.951 0.952 0.703 0.727 0.715 0.967 0.964 0.965 
+ IV 0.954 0.952 0.953 0.697 0.747 0.721 0.969 0.963 0.966 
+ IV + OOV 0.954 0.953 0.953 0.703 0.749 0.725 0.969 0.964 0.966 
NCC 
Basic Model 0.940 0.928 0.934 0.438 0.580 0.499 0.965 0.940 0.952 
+ IV 0.944 0.930 0.936 0.434 0.603 0.504 0.969 0.941 0.955 
+ IV + OOV 0.945 0.932 0.939 0.450 0.620 0.522 0.970 0.943 0.956 
SXU 
Basic Model 0.960 0.953 0.956 0.636 0.674 0.654 0.977 0.967 0.972 
+ IV 0.962 0.955 0.958 0.637 0.696 0.665 0.980 0.967 0.973 
+ IV + OOV 0.962 0.955 0.959 0.645 0.702 0.673 0.979 0.968 0.974 
Table 2 performance each step of our system achieves 
4 Conclusions and Future Work 
In this paper, we propose a three-stage strategy in 
Chinese Word Segmentation. Based on the results 
produced by basic CRF, our word-based segmen-
tation module and post-processing module are 
designed to improve IV and OOV performance 
respectively. The results above show that our sys-
tem achieves the state-of-the-art performance. 
Since only the CRF tagger is good enough as we 
shown in our experiment, in the future work we 
will pay effort on the semi-supervised learning for 
CRF model in order to mining more useful infor-
mation from training and test corpus for CRF tag-
ger. 
References 
John Lafferty, Andrew McCallum, and Fernando Perei-
ra. 2001. Conditional random fields: probabilistic 
models for segmenting and labeling sequence data. 
In Proceedings of ICML-2001, pages 591?598. 
Gina-Anne Levow. 2006. The Third International Chi-
nese Language Processing Bakeoff: Word Segmen-
tation and Named Entity Recognition. In  Proceed-
ings of the Fifth SIGHAN Workshop on Chinese 
Language Processing , pages 108-117, Sydney: Ju-
ly. 
Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005. 
A Maximum Entropy Approach to Chinese Word 
Segmentation. In Proceedings of the Fourth SIGHAN 
Workshop on Chinese Language Processing, pages 161-
164, Jeju Island, Korea. 
Zhiyong Luo, Rou Song, 2004. ?An integrated method 
for Chinese unknown word extraction?, In Proceed-
ings of Third SIGHAN Workshop on Chinese Lan-
guage Processing, pages 148-154. Barcelona, Spain. 
Fuchun Peng, Fangfang Feng, and Andrew McCallum. 
2004. Chinese segmentation and new word detection 
using conditional random fields. In COLING 2004, 
pages 562?568. Geneva, Switzerland. 
Huihsin Tseng, Pichuan Chang et al 2005. A Conditional 
Random Field Word Segmenter for SIGHAN Bakeoff 
2005. In Proceedings of the Fourth SIGHAN Workshop 
on Chinese Language Processing, pages 168-171, Jeju 
Island, Korea. 
Zhenxing Wang, Changning Huang and Jingbo Zhu. 
2008. Which Performs Better on In-Vocabulary 
Word Segmentation: Based on Word or Character? 
In Proceeding of the Sixth Sighan Workshop on 
Chinese Language Processing. To be published.  
Neinwen Xue and Susan P. Converse. 2002. Combin-
ing Classifiers for Chinese Word Segmentation. In 
Proceedings of the First SIGHAN Workshop on 
Chinese Language Processing, pages 63-70, Taipei, 
Taiwan. 
Ruiqiang Zhang, Genichiro Kikui and Eiichiro Sumita. 
2006. Subword-based Tagging by Conditional Ran-
dom Fields for Chinese Word Segmentation. In Pro-
ceedings of the Human Language Technology Con-
ference of the NAACL, Companion volume, pages 
193-196. New York, USA. 
Hai Zhao, Changning Huang et al 2006. Effective Tag 
Set Selection in Chinese Word Segmentation via 
Conditional Random Field Modeling. In Proceed-
ings of PACLIC-20. pages 87-94. Wuhan, China, 
Novemeber. 
 
101
Sixth SIGHAN Workshop on Chinese Language Processing
