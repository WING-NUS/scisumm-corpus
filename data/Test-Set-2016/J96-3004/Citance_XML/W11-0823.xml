<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">What is a multiword expression (MWE) and how many are there?</S>
		<S sid ="2" ssid = "2">What is a MWE?</S>
		<S sid ="3" ssid = "3">What is many?</S>
		<S sid ="4" ssid = "4">Mark Liberman gave a great invited talk at ACL89 titled “how many words do people know?” where he spent the entire hour questioning the question.</S>
		<S sid ="5" ssid = "5">Many of these same questions apply to multiword expressions.</S>
		<S sid ="6" ssid = "6">What is a word?</S>
		<S sid ="7" ssid = "7">What is many?</S>
		<S sid ="8" ssid = "8">What is a person?</S>
		<S sid ="9" ssid = "9">What does it mean to know?</S>
		<S sid ="10" ssid = "10">Rather than answer these questions, this paper will use these questions as Liberman did, as an excuse for surveying how such issues are addressed in a variety of fields: computer science, web search, linguistics, lexicography, educational testing, psychology, statistics, etc. 1 How many words do people know?</S>
		<S sid ="11" ssid = "11">One can find all sorts of answers on the web: • Very low: Apparently I only knew 7,000 words when I was seven and 14,000 when I was fourteen.</S>
		<S sid ="12" ssid = "12">I learned from exposure.</S>
		<S sid ="13" ssid = "13">Now things are not that easy in a second language, but it just shows that the brain can absorb information from sheer input.1 • Low: 12,000 – 20,000 words2 • Higher: 988,9683 • Even higher: 13,588,3914 1 http://thelinguist.blogs.com/how_to_learn_english_and/2009/0 2/how-many-words-do-you-know-how-many-have-you- looked-up-in-a-dictionary.html 2 http://answers.yahoo.com/question/index?qid=200611052050 54AA5YL0B 3 http://www.independent.co.uk/news/world/americas/english- language-nears-the-one-millionword-milestone-473935.html</S>
	</ABSTRACT>
	<SECTION title="Motivation" number = "1">
			<S sid ="14" ssid = "14">As mentioned in the abstract, Liberman used his ACL89 invited talk to survey how various fields approach these issues.</S>
			<S sid ="15" ssid = "15">He started his ACL89 invited talk by questioning every word in the title of his talk: How many words do people know?</S>
			<S sid ="16" ssid = "16">1.</S>
			<S sid ="17" ssid = "17">What is a word?</S>
			<S sid ="18" ssid = "18">Is a word defined in.</S>
			<S sid ="19" ssid = "19">terms of meaning?</S>
			<S sid ="20" ssid = "20">Sound?</S>
			<S sid ="21" ssid = "21">Syntax?</S>
			<S sid ="22" ssid = "22">Spelling?</S>
			<S sid ="23" ssid = "23">White space?</S>
			<S sid ="24" ssid = "24">Distribution?</S>
			<S sid ="25" ssid = "25">Etymology?</S>
			<S sid ="26" ssid = "26">Learnability?</S>
	</SECTION>
	<SECTION title="What is a person?  Child?  Adult?  Native. " number = "2">
			<S sid ="27" ssid = "1">speaker?</S>
			<S sid ="28" ssid = "2">Language Learner?</S>
	</SECTION>
	<SECTION title="What  does  it  mean  to  know  something?. " number = "3">
			<S sid ="29" ssid = "1">Active knowledge is different from passive knowledge.</S>
			<S sid ="30" ssid = "2">What is (Artificial) Intelligence?</S>
			<S sid ="31" ssid = "3">Is vocabulary size a measure of intelligence?</S>
			<S sid ="32" ssid = "4">(Terman, 1918)</S>
	</SECTION>
	<SECTION title="What  do we mean  by many?   Is there  a. " number = "4">
			<S sid ="33" ssid = "1">limit like 20,000 or 1M or 13.6M or does vocabulary size (V) keep growing with experience (larger corpora à larger V)?</S>
			<S sid ="34" ssid = "2">The original motivation for Liberman’s talk came from a very practical business concern.</S>
			<S sid ="35" ssid = "3">At the time, Liberman was running a speech synthesis effort at AT&amp;T Bell Labs.</S>
			<S sid ="36" ssid = "4">As the manager of this effort, Liberman would receive questions from the business asking how large the synthesizer’s dictionary would have to be for such and such commercial application.</S>
			<S sid ="37" ssid = "5">Vocabulary size was also a hot topic in many other engineering applications.</S>
			<S sid ="38" ssid = "6">How big does the dictionary have to be for X?</S>
			<S sid ="39" ssid = "7">X can be anything from parsing, part of speech tagging, spelling correction, machine translation, word breaking for Chinese and Japanese (and English), speech recog 4 Franz and Brants (2006).</S>
			<S sid ="40" ssid = "8">137 Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 137–144, Portland, Oregon, USA, 23 June 2011.</S>
			<S sid ="41" ssid = "9">Qc 2011 Association for Computational Linguistics nition, speech synthesis, web search or some other application.</S>
			<S sid ="42" ssid = "10">3 Dictionary Estimates.</S>
			<S sid ="43" ssid = "11">These questions reminded Liberman of similar questions that his colleagues in lexicography were receiving from their marketing departments.</S>
			<S sid ="44" ssid = "12">Many dictionaries and other reference books lead with a marketing pitch such as: “Most comprehensive: more than 330,000 words and phrases [MWEs]…” (Kipfer, 2001).</S>
			<S sid ="45" ssid = "13">The very smallest dictionaries are called “gems.” They typically contain 20,000 words.</S>
			<S sid ="46" ssid = "14">Unabridged collegiate dictionaries have about 500,000 words.5 The Oxford English Dictionary (OED) has 600,000 entries.6 All of these dictionaries limit themselves to what is known as general vocabulary, words that would be expected to be understood by a general audience.</S>
			<S sid ="47" ssid = "15">General vocabulary is typically contrasted with technical terminology, words that would only be understood by domain experts in a particular topic.</S>
			<S sid ="48" ssid = "16">There are reference books that specialize on place names (Gazetteers), surnames, technical terminology, quotations, etc., but standard dictionaries of general vocabulary tend to avoid proper nouns, complex nominals (e.g., “staff meeting”), abbreviations, acronyms, technical terminology, digit sequences, street addresses, trademarks, product numbers, etc.7 Even the largest dictionaries may not have all that much coverage because in practice, one often runs into texts that go well beyond general vocabulary.</S>
			<S sid ="49" ssid = "17">4 Broder’s Taxonomy of Web Queries.</S>
			<S sid ="50" ssid = "18">Obviously, the web goes well beyond general vocabulary.</S>
			<S sid ="51" ssid = "19">Web queries tend to be short phrases (MWEs), often a word or two such as a product number.</S>
			<S sid ="52" ssid = "20">Broder (2002) introduced a taxonomy of 5 http://www.collinslanguage.com/shop/english-dictionary- landing.aspx 6 http://www.oed.com/public/about 7 See Sproat (1994) and references therein for more on.</S>
			<S sid ="53" ssid = "21">complex nominals.</S>
			<S sid ="54" ssid = "22">See Coker et al (1990) for coverage statistics on surnames.</S>
			<S sid ="55" ssid = "23">See Liberman and Church (1991) for more on abbreviations, acronyms, digit sequences and more.</S>
			<S sid ="56" ssid = "24">See Dagan and Church (1994) for more on technical terminology.</S>
			<S sid ="57" ssid = "25">queries that has become widely accepted.</S>
			<S sid ="58" ssid = "26">His percentage estimates were estimated from AltaVista query logs and could use updating.</S>
			<S sid ="59" ssid = "27">• Naviational (20%) • Informational (48%) • Transactional (30%) Navigational queries are extremely common these days, perhaps even more common than 20%.</S>
			<S sid ="60" ssid = "28">The user intent is to navigate to a particular url: • google à www.google.com • Greyhound Bus à www.greyhound.com • American Airlines à www.aa.com Broder’s examples of informational queries are: cars, San Francisco, normocytic anemia, Scoville heat units.</S>
			<S sid ="61" ssid = "29">The user intent is to research a particular information need.</S>
			<S sid ="62" ssid = "30">The user expects to read one or more static web pages in order to address the information need.</S>
			<S sid ="63" ssid = "31">Broder italicized “static” to distinguish informational queries from transactional queries.</S>
			<S sid ="64" ssid = "32">Transactional queries are intended to reach a site where further (non-static) action will take place: shopping, directions, web- mediated services, medical advice, gaming, downloading music, pictures, videos, etc.</S>
	</SECTION>
	<SECTION title="User Intent &amp; One Sense Per Query. " number = "5">
			<S sid ="65" ssid = "1">I prefer a two-way distinction between 1.</S>
			<S sid ="66" ssid = "2">Navigational queries: user knows where.</S>
			<S sid ="67" ssid = "3">she wants to go, and 2.</S>
			<S sid ="68" ssid = "4">Non-navigational queries: user is open to.</S>
			<S sid ="69" ssid = "5">suggestions.</S>
			<S sid ="70" ssid = "6">Google, for example, offers the following “related search” suggestions for “camera:” digital camera, video camera, history of the camera, sony camera, ritz camera, Nikon camera, camera brands, camera reviews, camera store, beach camera, canon, photography, bestbuy, camara, cannon, circuit city.</S>
			<S sid ="71" ssid = "7">camero.</S>
			<S sid ="72" ssid = "8">Olympus, camcorder, b&amp;h. These kinds of suggestions can be very successful when the user is open to suggestions, but not for navigational queries.</S>
			<S sid ="73" ssid = "9">There are a number of other mechanisms for making suggestions such as ads and did-you-mean spelling suggestions.</S>
			<S sid ="74" ssid = "10">Pitler and Church (2009) used click logs to classify queries by intent (CQI).</S>
			<S sid ="75" ssid = "11">Consider five types of clicks.</S>
			<S sid ="76" ssid = "12">Some types of clicks are evidence that the user knows where she wants to go, and some are evidence that the user is open to suggestions.</S>
			<S sid ="77" ssid = "13">1.</S>
			<S sid ="78" ssid = "14">Algo: clicks on the so-called 10 blue links.</S>
			<S sid ="79" ssid = "15">2.</S>
			<S sid ="80" ssid = "16">Paid: clicks on commercial ads.</S>
			<S sid ="81" ssid = "17">3.</S>
			<S sid ="82" ssid = "18">Wikipedia: clicks on Wikipedia entries.</S>
			<S sid ="83" ssid = "19">4.</S>
			<S sid ="84" ssid = "20">Spelling Corrections: did you mean …?.</S>
			<S sid ="85" ssid = "21">5.</S>
			<S sid ="86" ssid = "22">Other suggestions from search engine.</S>
			<S sid ="87" ssid = "23">Many queries are strongly associated with one type of click (more than others).</S>
			<S sid ="88" ssid = "24">• Commercial queries à clicks on ads • Noncommercial queries à Wikipedia.</S>
			<S sid ="89" ssid = "25">There is a one-sense-per-X constraint (Gale et al, 1992; Yarowsky, 1993).</S>
			<S sid ="90" ssid = "26">It is unlikely that the same query will be ambiguous with both commercial and noncommercial senses.</S>
			<S sid ="91" ssid = "27">Indeed, the click logs show that both ads and Wikipedia are effective, but they have complementary distributions.</S>
			<S sid ="92" ssid = "28">There are few queries with both clicks on ads and clicks on Wikipedia entries.</S>
			<S sid ="93" ssid = "29">For a commercial query like, “JC Penney,” it is ok for Google to return an ad and a store locator map, but Google shouldn’t return a Wikipedia discussion of the history of the company.</S>
			<S sid ="94" ssid = "30">Although the click logs are very large, they are never large enough.</S>
			<S sid ="95" ssid = "31">How do we resolve the user intention when the click logs are too sparse to resolve the ambiguity directly?</S>
			<S sid ="96" ssid = "32">Pitler suggested using word sense disambiguation methods.</S>
			<S sid ="97" ssid = "33">For example, her method labels the ambiguous query “designer trench” as commercial because it is closer (in random walk distance) to a couple of stores than to a Wikipedia discussion of trench warfare during World War I. More generally, random walk methods (like word sense disambiguation) can be used to resolve all sorts of hidden variables such as gender, age, location, political orientation, user intent, etc. Did the user mean X?</S>
			<S sid ="98" ssid = "34">Does the user know what she wants, or is she open to suggestions?</S>
			<S sid ="99" ssid = "35">5.1 User Intent &amp; Spelling Correction.</S>
			<S sid ="100" ssid = "36">Spelling correction is an extreme case where it is often relatively easy for the system to determine user intent.</S>
			<S sid ="101" ssid = "37">On the web, spelling correction has become synonymous with did-you-mean.</S>
			<S sid ="102" ssid = "38">The synonymy makes it clear that the point of spelling correction is to get at what users mean as opposed to what they say.</S>
			<S sid ="103" ssid = "39">Then you should say what you mean,&apos; the March Hare went on.</S>
			<S sid ="104" ssid = "40">`I do,&apos; Alice hastily replied; `at least--at least I mean what I say-- that&apos;s the same thing, you know.&apos;</S>
			<S sid ="105" ssid = "41">`Not the same thing a bit!&apos; said the Hatter.</S>
			<S sid ="106" ssid = "42">(Lewis Carroll, 1865) See Kukich (1992) for a comprehensive survey on spelling correction.</S>
			<S sid ="107" ssid = "43">Boswell (2004) is a nice research exam; it is short and crisp and recent.</S>
			<S sid ="108" ssid = "44">I’ve worked on Microsoft’s spelling correction products in two different divisions: Office and Web Search.</S>
			<S sid ="109" ssid = "45">One might think that correcting documents in Microsoft Word would be similar to correcting web queries, but in fact, the two applications have remarkably little in common.</S>
			<S sid ="110" ssid = "46">A dictionary of general vocabulary is essential for correcting documents and nearly useless for correcting web queries.</S>
			<S sid ="111" ssid = "47">General vocabulary is more important in documents than web queries.</S>
			<S sid ="112" ssid = "48">The surveys mentioned above are more appropriate for correcting documents than web queries.</S>
			<S sid ="113" ssid = "49">Cucerzan and Brill (2004) propose an iterative process that is more appropriate for web queries.</S>
			<S sid ="114" ssid = "50">In Table 1, they show a number of (mis)spellings of Albert Einstein’s name from a query log, sorted by frequency: albert einstein (4834), albert einstien (525), albert einstine (149), albert einsten (27), albert einsteins (25), etc. Their method takes a web query that may or may not be misspelled and considers nearby corrections with higher frequencies.</S>
			<S sid ="115" ssid = "51">The method continues to iterate in this way until it converges at a fixed point.</S>
			<S sid ="116" ssid = "52">The iteration makes it possible to correct multiple errors.</S>
			<S sid ="117" ssid = "53">For example, anol scwartegger à arnold schwartznegger à arnold schwarznegger à arn- old schwarzenegger.</S>
			<S sid ="118" ssid = "54">They find that context is often very helpful.</S>
			<S sid ="119" ssid = "55">In general, it is easier to correct the combination of the first name and the last name together than separately.</S>
			<S sid ="120" ssid = "56">So too, it is probably easier to correct MWEs as a combination than to correct each of the parts separately.</S>
			<S sid ="121" ssid = "57">5.2 User Intent &amp; Spoken Queries.</S>
			<S sid ="122" ssid = "58">Queries often depend on context in complex and unexpected ways.</S>
			<S sid ="123" ssid = "59">It has been said that there is no there there on the web, but queries from cell phones are often looking for stuff that has a “there” (a location), and moreover the location is often near the user (e.g., restaurants, directions).</S>
			<S sid ="124" ssid = "60">Users now have the option to enter queries by voice in addition to the keyboard.</S>
			<S sid ="125" ssid = "61">Kamvar and Beeferman (2010) found voice was relatively popular on mobile devices with “compressed” (hard-to-use) keyboards.</S>
			<S sid ="126" ssid = "62">They also found some topics were relatively more likely to be spoken: • Food &amp; Drink: Starbucks, tuna fish, Mexican food • Business Listings: Starbucks Holmdel NJ, Lake George • Properties relating to places: weather Holmdel NJ, best gas prices • Shopping &amp; Travel: Rapids Water Park coupons, black Converse shoes, Costco, Walmart Other topics such as adult queries are relatively less likely to be spoken, presumably because users don’t want to be overheard.</S>
			<S sid ="127" ssid = "63">Privacy is more of a concern for some topics and less for others.</S>
	</SECTION>
	<SECTION title="What is “large”?. " number = "6">
			<S sid ="128" ssid = "1">The term “large vocabulary” has been a moving and Gale, 1989).</S>
			<S sid ="129" ssid = "2">A leading researcher pulled me aside and begged me to tell him that I had made a mistake and there was no need to go beyond 20,000 words.</S>
			<S sid ="130" ssid = "3">Similar questions came up when Google released their ngram counts over a trillion word corpus (Franz and Brants, 2006).</S>
			<S sid ="131" ssid = "4">There was considerable pushback from the community over the size of the vocabulary (13,588,391).</S>
			<S sid ="132" ssid = "5">Norvig (personal communication) called me up to ask if their estimate of 13.6 million seemed unreasonable.</S>
			<S sid ="133" ssid = "6">While I had no reason to question Google’s estimate, I was reluctant to make a strong statement, given Efron and Thisted (1976).</S>
			<S sid ="134" ssid = "7">Efron and Thisted studied a similar question: How many words did Shakespeare know (but didn’t use)?</S>
			<S sid ="135" ssid = "8">They conclude that one can extrapolate corpus size a little bit (e.g., a factor of two) but not too much (e.g., an order of magnitude).</S>
			<S sid ="136" ssid = "9">Since Google is working with corpora that are many orders of magnitude larger than what I had the opportunity to work with, it would require way too much extrapolation to answer Norvig’s question based on my relatively limited experience.</S>
	</SECTION>
	<SECTION title="Vocabulary Grows with Experience. " number = "7">
			<S sid ="137" ssid = "1">Many people share the (mistaken) intuition that there is an upper bound on the size of the vocabulary.</S>
			<S sid ="138" ssid = "2">Marketing pitches such “330,000 words” (above) suggest that there is a reasonable upper bound that a person could hope to master (something considerably more manageable than Google’s estimate of 13.6 million).</S>
			<S sid ="139" ssid = "3">In fact, the story is probably worse than that.</S>
			<S sid ="140" ssid = "4">At ACL1989, Liberman showed plots like those 8 target.</S>
			<S sid ="141" ssid = "5">Vocabulary sizes have been increasing with below.</S>
			<S sid ="142" ssid = "6">These plots make it clear that vocabulary advances in technology.</S>
			<S sid ="143" ssid = "7">Around the time of Liberman’s ACL89 talk, the speech recognition community was working really hard on a 20,000- word task.</S>
			<S sid ="144" ssid = "8">Since it was so hard at the time to scale up recognizers to such large vocabularies, some researchers were desperately hoping that 20,000 words would be sufficient to achieve broad coverage of unrestricted language.</S>
			<S sid ="145" ssid = "9">At that time, I gave a workshop talk that used a much larger vocabulary of 400,000 words (Church (V) is going up and up and up with corpus size (N).</S>
			<S sid ="146" ssid = "10">There appears to be no end in sight.</S>
			<S sid ="147" ssid = "11">It is unlikely that there is an upper bound.</S>
			<S sid ="148" ssid = "12">20k isn’t enough.</S>
			<S sid ="149" ssid = "13">Nor is 400k, or even 13.6 million… The different curves call out differences in what counts as a word.</S>
			<S sid ="150" ssid = "14">Do we consider morphologically related forms to be one word or two?</S>
			<S sid ="151" ssid = "15">How about</S>
	</SECTION>
	<SECTION title="Plots borrowed with permission from Language Log:. " number = "8">
			<S sid ="152" ssid = "1">http://itre.cis.upenn.edu/~myl/languagelog/archives/005514.ht ml upper and lower case?</S>
			<S sid ="153" ssid = "2">MWEs?</S>
			<S sid ="154" ssid = "3">The different curves correspond to different choices.</S>
			<S sid ="155" ssid = "4">No matter how we define a word, we find that vocabulary grows (rapidly) with corpus size for as far as we can see.</S>
			<S sid ="156" ssid = "5">This observation appears to hold across a broad set of conditions (languages, definitions of word/ngram, etc.) Vocabulary becomes larger and larger with experience.</S>
			<S sid ="157" ssid = "6">Similar comments apply to ngrams and MWEs.</S>
			<S sid ="158" ssid = "7">There is wide agreement that there’s no data like more data (Mercer, 1985).9 Google quoted Mercer in their announcement of ngram counts (Franz and Brants, 2006).</S>
			<S sid ="159" ssid = "8">Banko and Brill (2001) observed that performance goes up and up and up with experience (data).</S>
			<S sid ="160" ssid = "9">In the plot below, they note that the differences between lines (learners) are small compared to the gains to be had by simply collecting more data.</S>
			<S sid ="161" ssid = "10">Based on this observation, Brill has suggested (probably in jest) that we should fire everyone and spend the money on collecting data.</S>
			<S sid ="162" ssid = "11">Another interpretation is that experience improves performance on a host of tasks.</S>
			<S sid ="163" ssid = "12">This pattern might help account for the large correlation (0.91) in Terman (1918).</S>
			<S sid ="164" ssid = "13">Terman suggests that vocabulary size should not be viewed as a measure of intelligence but rather a measure of experience.</S>
			<S sid ="165" ssid = "14">He uses the term “mental age” for experience, and measures “mental age” by performance on a standardized test.</S>
			<S sid ="166" ssid = "15">After adjusting for the large correlation between vocabulary size and experience, there is little evidence of a connection between vocabulary size and intelligence (or much of anything else).</S>
			<S sid ="167" ssid = "16">Terman also considers a number of other factors such as gender and the language spoken at home (and testing errors), but ultimately concludes that experience dominates all of these alternative factors.</S>
	</SECTION>
	<SECTION title="Jelinek (2004) attributes this position to Mercer (1985). " number = "9">
			<S sid ="168" ssid = "1">http://www.lrecconf.org/lrec2004/doc/jelinek.pdf.</S>
			<S sid ="169" ssid = "2">8 What is a Word?</S>
			<S sid ="170" ssid = "3">MWE?.</S>
			<S sid ="171" ssid = "4">We tend to think that white space makes it pretty easy to tokenize English text into words.</S>
			<S sid ="172" ssid = "5">Obviously, white space makes the task much easier than it would be otherwise.</S>
			<S sid ="173" ssid = "6">There is a considerable literature on word breaking in Chinese and Japanese which is considerably more challenging than English largely because there is no white space in Chinese and Japanese.</S>
			<S sid ="174" ssid = "7">There are a number of popular dictionary-based solutions such as Cha Sen10 and Juman.11 Sproat et al (1996) proposed an alternative solution based on distributional statistics such as mutual information.</S>
			<S sid ="175" ssid = "8">The situation may not be all that different in English.</S>
			<S sid ="176" ssid = "9">English is full of multiword expressions.</S>
			<S sid ="177" ssid = "10">An obvious example involves words that look like prepositions: up, in, on, with.</S>
			<S sid ="178" ssid = "11">A great example is often attributed to Winston Churchill: This is the sort of arrant nonsense up with which I will not put.12 One could argue that “put up with” is a phrasal verb and therefore it should be treated more like a fixed expression (or a word) than a stranded preposition.</S>
			<S sid ="179" ssid = "12">8.1 Preventing Bloopers.</S>
			<S sid ="180" ssid = "13">Almost any high frequency verb (go, make, do, have, give, call) can form a phrase with almost any high frequency function word (it, up, in, on, with, out, down, around, over), often with non- compositional (taboo) semantics.</S>
			<S sid ="181" ssid = "14">This fact led to a rather entertaining failure mode with a word sense program that was trained on a combination of Roget’s Thesaurus and Grolier’s Encyclopedia (Yarowsky, 1992).</S>
			<S sid ="182" ssid = "15">Yarowsky’s program had a tendency to label high frequency words incorrectly with taboo senses due to a mismatch between Groliers and Roget’s. Groliers was written for the parents of middle-American school children and therefore avoided taboo language, whereas Roget’s was edited by Chapman, an authority on American Slang (taboo language).</S>
			<S sid ="183" ssid = "16">The mismatch was particularly nasty for high frequency words, which are very common 10 http://chasen.naist.jp/hiki/ChaSen/ in Groliers, but unlikely to be mentioned in Roget’s, except when the semantics are non- compositional (taboo).</S>
			<S sid ="184" ssid = "17">Consequently, there was an embarrassingly high probability that Yarow- sky’s program would find embarrassing interpretations of benign texts.</S>
			<S sid ="185" ssid = "18">While some of these mistakes are somewhat understandable and even somewhat amusing in a research prototype, such mistakes are no laughing matter in a commercial product.</S>
			<S sid ="186" ssid = "19">The testers at Microsoft worked really hard to make sure that their products don’t make inappropriate suggestions.</S>
			<S sid ="187" ssid = "20">Despite their best efforts, there have been a few highly publicized mistakes 13 and there will probably be more unless we find better ways to prevent bloopers.</S>
			<S sid ="188" ssid = "21">8.2 Complex Nominals and What is a Word?.</S>
			<S sid ="189" ssid = "22">Complex nominals are probably more common than phrasal verbs.</S>
			<S sid ="190" ssid = "23">Is “White House” one word or two?</S>
			<S sid ="191" ssid = "24">Is a word defined in terms of spelling?</S>
			<S sid ="192" ssid = "25">White space?</S>
			<S sid ="193" ssid = "26">These days, among computational linguists, there would be considerable sympathy for using distributional statistics such as word frequency and mutual information to find MWEs.</S>
			<S sid ="194" ssid = "27">Following Firth (1957), we know a word by the company that it keeps.</S>
			<S sid ="195" ssid = "28">In Church and Hanks (1990), we suggested using pointwise mutual information as a heuristic to look for pairs of words that have noncom- positional distributional statistics.</S>
			<S sid ="196" ssid = "29">That is, if the joint probability, P(x,y), of seeing two words together in a context (e.g., window of 5 words) is much higher than chance, P(x)P(y), then there is probably a hidden variable such as meaning that is causing the deviation from chance.</S>
			<S sid ="197" ssid = "30">In this way, we are able to discover lots of word associations (e.g., doctor…nurse), collocates, fixed expressions, etc. If the list of MWEs becomes too large and too unmanageable, one could turn to a method like Stolcke pruning to cut back the list as necessary.</S>
			<S sid ="198" ssid = "31">Stolcke pruning is designed to prune ngram models so they fit in a manageable amount of memory.</S>
			<S sid ="199" ssid = "32">Suppose we have an ngram model with too many 11 http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman-e.html 12 For a discussion of the source of this quotation, see.</S>
			<S sid ="200" ssid = "33">http://itre.cis.upenn.edu/~myl/languagelog/archives/002 670.html . 13 http://www.zdnet.co.uk/news/it- strategy/1999/07/01/microsoft-sued-for-racist-application 2072468/ ngrams and we have to drop some of them.</S>
			<S sid ="201" ssid = "34">Which ones should we drop?</S>
			<S sid ="202" ssid = "35">Stolcke pruning computes a loss in terms of relative entropy for dropping each ngram in the model.</S>
			<S sid ="203" ssid = "36">The method drops the ngram that minimizes loss.</S>
			<S sid ="204" ssid = "37">When an ngram is dropped from the model, that sequence is modeled with a backed off estimate from other ngrams.</S>
			<S sid ="205" ssid = "38">Stolcke pruning can be thought of as introducing compositionality assumptions.</S>
			<S sid ="206" ssid = "39">Suppose, for example, that “nice house” has more compositional statistics than “white house.” That is, Pr(nice house) ≈ Pr(nice) Pr(house) whereas Pr(white house) &gt;&gt; Pr(white) Pr(house).</S>
			<S sid ="207" ssid = "40">In this case, Stolcke pruning would drop “nice house” before it drops “white house.” 8.3 Linguistic Diagnostics.</S>
			<S sid ="208" ssid = "41">Linguists would feel more comfortable with defining word in terms of sound (phonology) and meaning (semantics).</S>
			<S sid ="209" ssid = "42">It is pretty clear that “White House” has noncompositional sound and meaning.</S>
			<S sid ="210" ssid = "43">The “White House” does not refer to a house that happens to be white, which is what would be expected under compositional semantics.</S>
			<S sid ="211" ssid = "44">It is accented on the left (the WHITE house) in contrast with the general pattern where adjective-noun complex nominals are typically accented on the right (a nice HOUSE), though there are many exceptions to this rule (Sproat 1994).14 Linguists would also feel comfortable with diagnostic tests based on paraphrases and transformations.</S>
			<S sid ="212" ssid = "45">Fixed expressions are fixed.</S>
			<S sid ="213" ssid = "46">One can’t paraphrase a “red herring” as “*herring that is red.” They resist regular inflection: “*two red herrings.” In Bergsma et al (2011), we use a paraphrase diagnostic to distinguish [N &amp; N] N from N &amp; [ N N]: • [dairy and meat] production o meat and dairy production o production of meat and dairy o production de produits [laitiers et de viand] (French) 14 Sproat has posted a list of 7831 English binary noun.</S>
			<S sid ="214" ssid = "47">compounds with hand assigned accent labels at: http://www.cslu.ogi.edu/~sproatr/newindex/ap90nominals.txt • asbestos and [polyvinyl chloride] o polyvinyl chloride and asbestos o asbestos and chloride o l’asbesto e il [polivinilcloruro] (Italian) The first three paraphrases make it clear that “dairy and meat” is a constituent whereas the last three paraphrases make it clear that “polyvinyl chloride” is a constituent.</S>
			<S sid ="215" ssid = "48">Comparable corpora can be viewed as a rich source of paraphrase data, as indicated by the French and Italian examples above.</S>
			<S sid ="216" ssid = "49">9 Conclusions.</S>
			<S sid ="217" ssid = "50">How many multiword expressions (MWEs) do people know?</S>
			<S sid ="218" ssid = "51">The question is related to how many words do people know.</S>
			<S sid ="219" ssid = "52">20k?</S>
			<S sid ="220" ssid = "53">400k?</S>
			<S sid ="221" ssid = "54">1M?</S>
			<S sid ="222" ssid = "55">13M?</S>
			<S sid ="223" ssid = "56">Is there a bound or does vocabulary size increase with experience (corpus size)?</S>
			<S sid ="224" ssid = "57">Is vocabulary size a measure of intelligence or just experience?</S>
			<S sid ="225" ssid = "58">Dictionary sizes are just a lower bound because they focus on general vocabulary and avoid much of what matters on the web.</S>
			<S sid ="226" ssid = "59">Spelling correction is not the same for documents of general vocabulary and web queries.</S>
			<S sid ="227" ssid = "60">One can use Stolcke pruning and other compositionality tricks to cut down on the number of the number of multiword units that people must know.</S>
			<S sid ="228" ssid = "61">But obviously, the number they must know is just a lower bound on the number they may know.</S>
			<S sid ="229" ssid = "62">There are lots of engineering motivations for wanting to know how many words and MWEs people know.</S>
			<S sid ="230" ssid = "63">How big does the dictionary have to be for X (where X is parsing, tagging, spelling correction, machine translation, word breaking for Chinese and Japanese (and English), speech recognition, speech synthesis or some other application)?</S>
			<S sid ="231" ssid = "64">Rather than answer these questions, this paper used these questions as Liberman did, as an excuse for surveying how such issues are addressed in a variety of fields: computer science, web search, linguistics, lexicography, educational testing, psychology, statistics, etc.</S>
	</SECTION>
</PAPER>
