<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">In this paper we report on our experiments on automatic Word Sense Disambiguation using a maximum entropy approach for both English and Chinese verbs.</S>
		<S sid ="2" ssid = "2">We compare the difficulty of the sense- tagging tasks in the two languages and investigate the types of contextual features that are useful for each language.</S>
		<S sid ="3" ssid = "3">Our experimental results suggest that while richer linguistic features are useful for English WSD, they may not be as beneficial for Chinese.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="4" ssid = "4">Word Sense Disambiguation (WSD) is a central open problem at the lexical level of Natural Language Processing (NLP).</S>
			<S sid ="5" ssid = "5">Highly ambiguous words pose continuing problems for NLP applications.</S>
			<S sid ="6" ssid = "6">They can lead to irrelevant document retrieval in Information Retrieval systems, and inaccurate translations in Machine Translation systems (Palmer et al., 2000).</S>
			<S sid ="7" ssid = "7">For example, the Chinese word (jian4) has many different senses, one of which can be translated into English as “see”, and another as “show”.</S>
			<S sid ="8" ssid = "8">Correctly sense-tagging the Chinese word in context can prove to be highly beneficial for lexical choice in ChineseEnglish machine translation.</S>
			<S sid ="9" ssid = "9">Several efforts have been made to develop automatic WSD systems that can provide accurate sense tagging (Ide and Veronis, 1998), with a current emphasis on creating manually sense-tagged data for supervised training of statistical WSD systems, as evidenced by SEN SEVA L-1 (Kilgarriff and Palmer, 2000) and SEN SEVA L-2 (Edmonds and Cotton, 2001).</S>
			<S sid ="10" ssid = "10">Highly polysemous verbs, which have several distinct but related senses, pose the greatest challenge for these systems (Palmer et al., 2001).</S>
			<S sid ="11" ssid = "11">Predicate-argument information and selectional restrictions are hypothesized to be particu Maximum entropy models can be used to solve any classification task and have been applied to a wide range of NLP tasks, including sentence boundary detection, part-of-speech tagging, and parsing (Ratnaparkhi, 1998).</S>
			<S sid ="12" ssid = "12">Assigning sense tags to words in context can be viewed as a classification task similar to part-of-speech tagging, except that a separate set of tags is required for each vocabulary item to be sense-tagged.</S>
			<S sid ="13" ssid = "13">Under the maximum entropy framework (Berger et al., 1996), evidence from different features can be combined with no assumptions of feature independence.</S>
			<S sid ="14" ssid = "14">The automatic tagger estimates the conditional probability that a word has sense given that it occurs in context , where is a conjunction of features.</S>
			<S sid ="15" ssid = "15">The estimated probability is derived from feature weights which are determined automatically from training data so as to produce a probability distribution that has maximum entropy, under the constraint that it is consistent with observed evidence.</S>
			<S sid ="16" ssid = "16">With existing tools for learning maximum entropy models, the bulk of our work is in defining the types of features to look for in the data.</S>
			<S sid ="17" ssid = "17">Our goal is to see if sense-tagging of verbs can be improved by combining linguistic features that capture information about predicate- arguments and selectional restrictions.</S>
			<S sid ="18" ssid = "18">In this paper we report on our experiments on automatic WSD using a maximum entropy approach for both English and Chinese verbs.</S>
			<S sid ="19" ssid = "19">We compare the difficulty of the sense-tagging tasks in the two languages and investigate the types of contextual features that are useful for each language.</S>
			<S sid ="20" ssid = "20">We find that while richer linguistic features are useful for English WSD, they do not prove to be as beneficial for Chinese.</S>
			<S sid ="21" ssid = "21">The maximum entropy system performed competitively with the best systems on the English verbs in SEN SEVA L-1 and SEN SEVA L-2 (Dang and Palmer, 2002).</S>
			<S sid ="22" ssid = "22">However, while SEN SEVA L-2 made over many different languages, data for the Chinese lexical sample task was not made available in time for any systems to compete.</S>
			<S sid ="23" ssid = "23">Instead, we report on two experiments that we ran using our own lexicon and two separate Chinese corpora that are very similar in style (news articles from the People’s Republic of China), but have different types and levels of annotation – the Penn Chinese Treebank (CTB)(Xia et al., 2000), and the People’s Daily News (PDN) corpus from Beijing University.</S>
			<S sid ="24" ssid = "24">We discuss the utility of different types of annotation for successful automatic word sense disambiguation.</S>
	</SECTION>
	<SECTION title="English Experiment. " number = "2">
			<S sid ="25" ssid = "1">Our maximum entropy WSD system was designed to combine information from many different sources, using as much linguistic knowledge as could be gathered automatically by current NLP tools.</S>
			<S sid ="26" ssid = "2">In order to extract the linguistic features necessary for the model, all sentences were first automatically part-of-speech-tagged using a maximum entropy tagger (Ratnaparkhi, 1998) and parsed using the Collins parser (Collins, 1997).</S>
			<S sid ="27" ssid = "3">In addition, an automatic named entity tagger (Bikel et al., 1997) was run on the sentences to map proper nouns to a small set of semantic classes.</S>
			<S sid ="28" ssid = "4">Chodorow, Leacock and Miller (Chodorow et al., 2000) found that different combinations of topical and local features were most effective for disam biguating different words.</S>
			<S sid ="29" ssid = "5">Following their work, we divided the possible model features into topical features and several types of local contextual features.</S>
			<S sid ="30" ssid = "6">Topical features looked for the presence of keywords occurring anywhere in the sentence and any surrounding sentences provided as context (usually one or two sentences).</S>
			<S sid ="31" ssid = "7">The set of 200300 keywords is specific to each lemma to be disambiguated, and is determined automatically from training data so as to minimize the entropy of the probability of the senses conditioned on the keyword.</S>
			<S sid ="32" ssid = "8">The local features for a verb in a particular sentence tend to look only within the smallest clause containing . They include collocational features requiring no linguistic preprocessing beyond part- of-speech tagging (1), syntactic features that capture relations between the verb and its complements (24), and semantic features that incorporate information about noun classes for subjects and objects (56): 1.</S>
			<S sid ="33" ssid = "9">the word , the part of speech of , the part tive to , and words at positions -2, -1, +1, +2, relative to 2.</S>
			<S sid ="34" ssid = "10">whether or not the sentence is passive 3.</S>
			<S sid ="35" ssid = "11">whether there is a subject, direct object, indirect object, or clausal complement (a complement whose node label is S in the parse tree) 4.</S>
			<S sid ="36" ssid = "12">the words (if any) in the positions of subject, direct object, indirect object, particle, prepositional complement (and its object) 5.</S>
			<S sid ="37" ssid = "13">a Named Entity tag (PERSON, ORGANIZATION, LOCATION) for proper nouns appearing in (4) 6.</S>
			<S sid ="38" ssid = "14">WordNet synsets and hypernyms for the nouns.</S>
			<S sid ="39" ssid = "15">appearing in (4) 2.1 English Results.</S>
			<S sid ="40" ssid = "16">The maximum entropy system’s performance on the verbs from the evaluation data for SEN SEVA L 1 (Kilgarriff and Rosenzweig, 2000) rivaled that of the best-performing systems.</S>
			<S sid ="41" ssid = "17">We looked at the effect of adding topical features to local features that either included WordNet class features or used just lexical and named entity features.</S>
			<S sid ="42" ssid = "18">In addition, we experimented to see if performance could be improved by undoing passivization transformations to recover underlying subjects and objects.</S>
			<S sid ="43" ssid = "19">This was expected to increase the accuracy with which verb arguments could be identified, helping in cases where selectional restrictions on arguments played an important role in differentiating between senses.</S>
			<S sid ="44" ssid = "20">The best overall variant of the system for verbs did not use WordNet class features, but included topical keywords and passivization transformation, giving an average verb accuracy of 72.3%.</S>
			<S sid ="45" ssid = "21">If only the best combination of feature sets for each verb is used, then the maximum entropy models achieve 73.7% accuracy.</S>
			<S sid ="46" ssid = "22">These results are not significantly different from the reported results of the best-performing systems (Yarowsky, 2000).</S>
			<S sid ="47" ssid = "23">Our system was competitive with the top performing systems even though it used only the training data provided and none of the information from the dictionary to identify multi-word constructions.</S>
			<S sid ="48" ssid = "24">Later experiments show that the ability to correctly identify multi-word constructions improves performance substantially.</S>
			<S sid ="49" ssid = "25">We also tested the WSD system on the verbs from the English lexical sample task for SEN SEVA L-2.1 Fe at ur e Ty pe (lo cal on ly) Ac cu ra cy Fe at ur e Ty pe (l oc al an d to pi ca l) Ac cu ra cy co llo cat io n + sy nt ax + sy nt ax + se m an tic s 4 8 . 3 5 3 . 9 5 9 . 0 co llo ca tio n + sy nt ax + sy nt ax + se m an tic s 5 2 . 9 5 4 . 2 6 0 . 2 Table 1: Accuracy of maximum entropy system using different subsets of features for SEN SEVA L-2 verbs.</S>
			<S sid ="50" ssid = "26">In contrast to SEN SEVA L-1, senses involving multi- word constructions could be identified directly from the sense tags themselves, and the head word and satellites of multi-word constructions were explicitly marked in the training and test data.</S>
			<S sid ="51" ssid = "27">This additional annotation made it much easier to incorporate information about the satellites, without having to look at the dictionary (whose format may vary from one task to another).</S>
			<S sid ="52" ssid = "28">All the best-performing systems on the English verb lexical sample task filtered out possible senses based on the marked satellites, and this improved performance.</S>
			<S sid ="53" ssid = "29">Table 1 shows the performance of the system using different subsets of features.</S>
			<S sid ="54" ssid = "30">In general, adding features from richer linguistic sources tended to improve accuracy.</S>
			<S sid ="55" ssid = "31">Adding syntactic features to collocational features proved most beneficial in the absence of topical keywords that could detect some of the complements and arguments that would normally be picked up by parsing (complementizers, prepositions, etc.).</S>
			<S sid ="56" ssid = "32">And while topical information did not always improve results significantly, syntactic features along with semantic class features always proved beneficial.</S>
			<S sid ="57" ssid = "33">Incorporating topical keywords as well as collocational, syntactic, and semantic local features, our system achieved 60.2% and 70.2% accuracy using fine-grained and coarse-grained scoring, respectively.</S>
			<S sid ="58" ssid = "34">This is in comparison to the next best- performing system, which had fine- and coarse- grained scores of 57.6% and 67.2% (Palmer et al., 2001).</S>
			<S sid ="59" ssid = "35">If we had not included a filter that only considered phrasal senses whenever there were satel lites of multi-word constructions marked in the test data, our fine- and coarse-grained accuracy would have been reduced to 57.5% and 67.2% (significant at ).</S>
	</SECTION>
	<SECTION title="Chinese Experiments. " number = "3">
			<S sid ="60" ssid = "1">We chose 28 Chinese words to be sense-tagged.</S>
			<S sid ="61" ssid = "2">Each word had multiple verb senses and possibly draw, dress, drift, drive, face, ferret, find, keep, leave, live, match, play, pull, replace, see, serve, strike, train, treat, turn, use, wander, wash, work.</S>
			<S sid ="62" ssid = "3">other senses for other parts of speech, with an average of 6 dictionary senses per word.</S>
			<S sid ="63" ssid = "4">The first 20 words were chosen by randomly selecting several files totaling 5000 words from the 100K-word Penn Chinese Treebank, and choosing only those words that had more than one dictionary verb sense and that occurred more than three times in these files.</S>
			<S sid ="64" ssid = "5">The remaining 8 words were chosen by selecting all words that had more than one dictionary verb sense and that occurred more than 25 times in the CTB.</S>
			<S sid ="65" ssid = "6">The definitions for the words were based on the CETA (ChineseEnglish Translation Assistance) dictionary (Group, 1982) and other hard-copy dictionaries.</S>
			<S sid ="66" ssid = "7">Figure 1 shows an example dictionary entry for the most common sense of jian4.</S>
			<S sid ="67" ssid = "8">For each word, a sense entry in the lexicon included the definition in Chinese as well as in English, the part of speech for the sense, a typical predicate-argument frame if the sense is for a verb, and an example sentence.</S>
			<S sid ="68" ssid = "9">With these definitions, each word was independently sense-tagged by two native Chinese-speaking annotators in a double- blind manner.</S>
			<S sid ="69" ssid = "10">Sense-tagging was done primarily using raw text, without segmentation, part of speech, or bracketing information.</S>
			<S sid ="70" ssid = "11">After finishing sense tagging, the annotators met to compare and to discuss their results, and to modify the definitions if necessary.</S>
			<S sid ="71" ssid = "12">The gold standard sense-tagged files were then made after all this discussion.</S>
			<S sid ="72" ssid = "13">In a manner similar to our English approach, we included topical features as well as collocational, syntactic, and semantic local features in the maximum entropy models.</S>
			<S sid ="73" ssid = "14">Collocational features could be extracted from data that had been segmented into words and tagged for part of speech: the target word the part of speech tag of the target word the words (if any) within 2 positions of the target word the part of speech of the words (if any) immediately preceding and following the target word whether the target word follows a verb &lt;entry id=&quot;00007&quot; word=&quot; &quot; pinyin=&quot;jian4&quot;&gt; &lt;wordsense id=&quot;00007001&quot;&gt; &lt;definition id=&quot;chinese&quot;&gt; , , &lt;/definition&gt; &lt;definition id=&quot;english&quot;&gt;to see, to perceive&lt;/definition&gt; &lt;pos&gt;VV&lt;/pos&gt; &lt;predarg&gt;NP0 NP1&lt;/predarg&gt; &lt;predarg&gt;NP0 NP1 IP&lt;/predarg&gt; &lt;example&gt; &lt;word&gt; &lt;/word&gt; &lt;/example&gt; &lt;/wordsense&gt; &lt;/entry&gt; Figure 1: Example sense definition for jian4.</S>
			<S sid ="74" ssid = "15">When disambiguating verbs, the following syntactic local features were extracted from data bracketed according to the Penn Chinese Treebank guidelines: whether the verb has a surface subject the head noun of the surface subject of the verb whether the verb has an object (any phrase la beled with “-OBJ”, such as NPOBJ, IPOBJ, QPOBJ) the phrase label of the object, if any the head noun of the object whether the verb has a VP complement the VP complement, if any whether the verb has an IP complement whether the verb has two NP complements whether the verb is followed by a predicate (any phrase labeled with “-PRD”) Semantic features were generated by assigning a HowNet2 noun category to each subject and object, and topical keywords were extracted as for English.</S>
			<S sid ="75" ssid = "16">Once all the features were extracted, a maximum entropy model was trained and tested for each target word.</S>
			<S sid ="76" ssid = "17">We used 5-fold cross validation to evaluate the system on each word.</S>
			<S sid ="77" ssid = "18">Two methods were used for partitioning a dataset of size into five subsets: Select consecutive occurrences for each set, or select every 5th occurrence for a set.</S>
			<S sid ="78" ssid = "19">In the end, the choice of partitioning method made little difference in overall performance, and we report accuracy as the precision using the latter (stratified) sampling method.</S>
			<S sid ="79" ssid = "20">2 http://www.keenage.com/ Fe at ur e Ty pe A cc St d De v co llo cat io n (n o pa rt of sp ee ch ) 86 .8 1 . 0 co llo cat io n + sy nt ax + sy nt ax + se m an tic s 93 .4 94 .4 94 .3 0 . 5 0 . 4 0 . 6 co llo cat io n + to pi c + sy nt ax + to pi c + sy nt ax + se m an tic s + to pi c 90 .3 92 .6 92 .8 1 . 0 0 . 9 0 . 9 Table 2: Overall accuracy of maximum entropy system using different subsets of features for Penn Chinese Treebank words (manually segmented, part-of- speech-tagged, parsed).</S>
			<S sid ="80" ssid = "21">3.1 Penn Chinese Treebank.</S>
			<S sid ="81" ssid = "22">All sentences containing any of the 28 target words were extracted from the Penn Chinese Treebank, yielding between 4 and 1143 occurrence (160 average) for each of the target words.</S>
			<S sid ="82" ssid = "23">The manual segmentation, part-of-speech tags, and bracketing of the CTB were used to extract collocational and syntactic features.</S>
			<S sid ="83" ssid = "24">The overall accuracy of the system on the 28 words in the CTB was 94.4% using local collocational and syntactic features.</S>
			<S sid ="84" ssid = "25">This is significantly better than the baseline of 76.7% obtained by tagging all instances of a word with the most frequent sense of the word in the CTB.</S>
			<S sid ="85" ssid = "26">Considering only the 23 words for which more than one sense occurred in the CTB, overall system accuracy was 93.9%, compared with a baseline of 74.7%.</S>
			<S sid ="86" ssid = "27">Figure 2 shows the results broken down by word.</S>
			<S sid ="87" ssid = "28">As with the English data, we experimented with different types of features.</S>
			<S sid ="88" ssid = "29">Table 2 shows the performance of the system using different subsets of features.</S>
			<S sid ="89" ssid = "30">While the system’s accuracy using syntactic features was higher than using only collocational features (significant at ), the improve Word pinying (translation) Events Senses Baseline Acc.</S>
			<S sid ="90" ssid = "31">Std Dev ------------------------------------------------------------------------------- biao3 shi4 (to indicate/express) 100 3 63.0 95.0 5.5 chu1 (to go out/to come out) 34 5 50.0 50.0 11.1 dao3 (to come/to arrive) 219 10 36.5 82.7 7.1 fa1 zhan3 (to develop/to grow) 437 3 65.2 97.0 1.2 hui4 (will/be able to) 86 6 58.1 91.9 6.0 jian4 (to see/to perceive) 4 2 75.0 25.0 38.7 jin4 xing2 (to be in progress) 159 3 89.3 95.6 2.5 ke3 (may/can) 57 1 100 100 0.0 lai2 (to come/to arrive) 148 6 66.2 96.6 2.1 li4 yong4 (to use/to utilize) 163 2 92.6 98.8 2.4 rang4 (to let/to allow) 9 1 100 100 0.0 shi3 (to make/to let) 89 1 100 100 0.0 shuo1 (to say in spoken words) 306 6 86.9 95.1 2.0 xiang3 (to think/ponder/suppose) 8 3 62.5 50.0 50.0 yin3 jin4 (to import/to introduce) 62 2 85.5 98.4 3.3 zai4 (to exist/to be at(in, on)) 1143 4 96.9 99.3 0.4 fa1 xian4 (to discover/to realize) 37 3 59.5 100.0 0.0 hui1 fu4 (to resume/to restore) 27 3 44.4 77.8 19.8 kai1 fang4 (to open to investors) 122 5 74.6 96.7 3.0 ke3 yi3 (may/can) 32 1 100 100 0.0 tong1 guo4 (to pass legislation) 81 5 66.7 95.1 2.5 tou2 ru4 (to input money, etc.) 44 4 40.9 84.1 11.7 yao4 (must/should/to intend to) 106 6 65.1 62.3 8.9 yong4 (to use) 41 2 58.5 100 0.0------------------------------------------------------------------------------ Overall 4497 3.5 76.7 94.4 0.4 Figure 2: Word, number of instances, number of senses in CTB, baseline accuracy, maximum entropy accuracy and standard deviation using local collocational and syntactic features.</S>
			<S sid ="91" ssid = "32">ment was not as substantial as for English, and this was despite the fact that the Chinese bracketing was done manually and should be almost error-free.</S>
			<S sid ="92" ssid = "33">Semantic class information from HowNet yielded no improvement at all.</S>
			<S sid ="93" ssid = "34">To see if using a different ontology would help, we subsequently experimented with the ROCLing conceptual structures (Mo, 1992).</S>
			<S sid ="94" ssid = "35">In this case, we also manually added unknown nouns from the corpus to the ontology and labeled proper nouns with their conceptual structures, in order to more closely parallel the named entity information used in the English experiments.</S>
			<S sid ="95" ssid = "36">This resulted in a system accuracy of 95.0% (std.</S>
			<S sid ="96" ssid = "37">dev.</S>
			<S sid ="97" ssid = "38">0.6), which again is not significantly better than omitting the noun class information.</S>
			<S sid ="98" ssid = "39">3.2 People’s Daily News.</S>
			<S sid ="99" ssid = "40">Five of the CTB words (chu1, jian4, xiang3, hui1 fu4, yao4) had system performance of less than 80%, probably due to their low frequency in the CTB corpus.</S>
			<S sid ="100" ssid = "41">These words were subsequently sense tagged in the People’s Daily News, a much larger corpus (about one million words) that has manual segmentation and part-of-speech, but no bracketing information.3 Those 5 words included all the words for which the system performed below the baseline 3 The PDN corpus can be found at http://icl.pku.edu.cn/research/corpus/dwldform1.asp.</S>
			<S sid ="101" ssid = "42">The annotation guidelines are not exactly the same as for the Penn CTB, and can be found at http://icl.pku.edu.cn/research/corpus/coprus-annotation.htm.</S>
			<S sid ="102" ssid = "43">Fe at ur e Ty pe A cc St d De v co llo cat io n (n o pa rt of sp ee ch ) 72 .3 2 . 2 co llo cat io n + sy nt ax + sy nt ax + se m an tic s 70 .3 71 .7 72 .7 2 . 9 3 . 0 3 . 1 co llo cat io n + to pi c + sy nt ax + to pi c + sy nt ax + se m an tic s + to pi c 73 .3 72 .6 72 .8 3 . 2 3 . 9 3 . 7 Table 3: Overall accuracy of maximum entropy system using different subsets of features for People’s Daily News words (automatically segmented, part- of-speech-tagged, parsed).</S>
			<S sid ="103" ssid = "44">Fe at ur e Ty pe A cc St d De v co llo cat io n (n o pa rt of sp ee ch ) 71 .4 4 . 3 co llo cat io n 74 .7 2 . 3 co llo cat io n + to pi c 72 .1 3 . 1 Table 4: Overall accuracy of maximum entropy system using different subsets of features for People’s Daily News words (manually segmented, part-of- speech-tagged).</S>
			<S sid ="104" ssid = "45">in the CTB corpus.</S>
			<S sid ="105" ssid = "46">About 200 sentences for each word were selected randomly from PDN and sense- tagged as with the CTB.</S>
			<S sid ="106" ssid = "47">We automatically annotated the PDN data to yield the same types of annotation that had been available in the CTB.</S>
			<S sid ="107" ssid = "48">We used a maximum- matching algorithm and a dictionary compiled from the CTB (Sproat et al., 1996; Xue, 2001) to do segmentation, and trained a maximum entropy part-of- speech tagger (Ratnaparkhi, 1998) and TAG-based parser (Bikel and Chiang, 2000) on the CTB to do tagging and parsing.4 Then the same feature extraction and model-training was done for the PDN corpus as for the CTB.</S>
			<S sid ="108" ssid = "49">The system performance is much lower for the PDN than for the CTB, for several reasons.</S>
			<S sid ="109" ssid = "50">First, the PDN corpus is more balanced than the CTB, which contains primarily financial articles.</S>
			<S sid ="110" ssid = "51">A wider range of usages of the words was expressed in PDN than in CTB, making the disambiguation task more difficult; the average number of senses for the PDN words was 8.2 (compared to 3.5 for CTB), and the 4 On held-out portions of the CTB, the accuracy of the segmentation and part-of-speech tagging are over 95%, and the accuracy of the parsing is 82%, which are comparable to the performance of the English preprocessors.</S>
			<S sid ="111" ssid = "52">The performance of these preprocessors is naturally expected to degrade when transferred to a different domain.</S>
			<S sid ="112" ssid = "53">baseline accuracy was 58.0% (compared to 76.7% for CTB).</S>
			<S sid ="113" ssid = "54">Also, using automatically preprocessed data for the PDN introduced noise that was not present for the manually preprocessed CTB.</S>
			<S sid ="114" ssid = "55">Despite these differences between PDN and CTB, the trends in using increasingly richer linguistic preprocessing are similar.</S>
			<S sid ="115" ssid = "56">Table 3 shows that adding more features from richer levels of linguistic annotation yielded no significant improvement over using only collocational features.</S>
			<S sid ="116" ssid = "57">In fact, using only lexical collocations from automatic segmentation was sufficient to produce close to the best results.</S>
			<S sid ="117" ssid = "58">Table 4 shows the system performance using the available manual segmentation and part-of-speech tagging.</S>
			<S sid ="118" ssid = "59">While using part-of-speech tags seems to be better than using only lexical collocations, the difference is not significant.</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "4">
			<S sid ="119" ssid = "1">We have demonstrated the high performance of maximum entropy models for word sense disambiguation in English, and have applied the same approach successfully to Chinese.</S>
			<S sid ="120" ssid = "2">While SEN SEVA L 2 showed that methods that work on English also tend to work on other languages, our experiments have revealed striking differences in the types of features that are important for English and Chinese WSD.</S>
			<S sid ="121" ssid = "3">While parse information seemed crucial for English WSD, it only played a minor role in Chinese; in fact, the improvement in Chinese performance contributed by manual parse information in the CTB disappeared altogether when automatic parsing was done for the PDN.</S>
			<S sid ="122" ssid = "4">The fact that bracketing was more important for English than Chinese WSD suggests that predicate-argument information and selectional restrictions may play a more important role in distinguishing English verb senses than Chinese senses.</S>
			<S sid ="123" ssid = "5">Or, it may be the case that Chinese verbs tend to be adjacent to their arguments, so collocational information is sufficient to capture the same information that would require parsing in English.</S>
			<S sid ="124" ssid = "6">This is a question for further study.</S>
			<S sid ="125" ssid = "7">The simpler level of linguistic processing required to achieve relatively high sense-tagging accuracy in Chinese highlights an important difference between Chinese and English.</S>
			<S sid ="126" ssid = "8">Chinese is different from English in that much of Chinese linguistic ambiguity occurs at the basic level of word segmentation.</S>
			<S sid ="127" ssid = "9">Chinese word segmentation is a major task in itself, and it seems that once this is accomplished little more needs to be done for sense dis ambiguation.</S>
			<S sid ="128" ssid = "10">Our experience in English has shown that the ability to identify multi-word constructions significantly improves sense-tagging performance.</S>
			<S sid ="129" ssid = "11">Multi-character Chinese words, which are identified by word segmentation, may be the analogy to English multi-word constructions.</S>
	</SECTION>
	<SECTION title="Acknowledgments. " number = "5">
			<S sid ="130" ssid = "1">This work has been supported by National Science Foundation Grants, NSF9800658 and NSF 9910603, and DARPA grant N6600100-18915 at the University of Pennsylvania.</S>
			<S sid ="131" ssid = "2">The authors would also like to thank the anonymous reviewers for their valuable comments.</S>
	</SECTION>
</PAPER>
