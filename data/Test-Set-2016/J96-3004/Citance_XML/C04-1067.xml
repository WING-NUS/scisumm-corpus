<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">In this paper, we present a hybrid method for Chinese and Japanese word segmentation.</S>
		<S sid ="2" ssid = "2">Word-level information is useful for analysis of known words, while character-level information is useful for analysis of unknown words, and the method utilizes both these two types of information in order to effectively handle known and unknown words.</S>
		<S sid ="3" ssid = "3">Experimental results show that this method achieves high overall accuracy in Chinese and Japanese word segmentation.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="4" ssid = "4">Word segmentation in Chinese and Japanese is an important and difficult task.</S>
			<S sid ="5" ssid = "5">In these languages, words are not separated by explicit delimiters, and word segmentation must be conducted first in most natural language processing applications.</S>
			<S sid ="6" ssid = "6">One of the problems which makes word segmentation more difficult is existence of unknown</S>
	</SECTION>
	<SECTION title="Previous Work on Word Segmentation. " number = "2">
			<S sid ="7" ssid = "1">Our method is based on two existing methods for Chinese or Japanese word segmentation, and we explain them in this section.</S>
			<S sid ="8" ssid = "2">2.1 The Markov Model-Based Method.</S>
			<S sid ="9" ssid = "3">Word-based Markov models are used in English part-of-speech (POS) tagging (Charniak et al., 1993; Brants, 2000).</S>
			<S sid ="10" ssid = "4">This method identifies POS- tags T = t1, . . .</S>
			<S sid ="11" ssid = "5">, tn, given a sentence as a word sequence W = w1, . . .</S>
			<S sid ="12" ssid = "6">, wn, where n is the number of words in the sentence.</S>
			<S sid ="13" ssid = "7">The method assumes that each word has a state which is the same as the POS of the word and the sequence of states is a Markov chain.</S>
			<S sid ="14" ssid = "8">A state t transits to another state s with prob ability P (s|t), and outputs a word w with probability P (w|t).</S>
			<S sid ="15" ssid = "9">From such assumptions, the probability that the word sequence W with parts-of-speech T is generated is n (out-of-vocabulary) words.</S>
			<S sid ="16" ssid = "10">Unknown words are defined as words that do not exist in a system’s dictionary.</S>
			<S sid ="17" ssid = "11">The word segmentation system has no knowledge about these unknown words, and determining word boundaries for such words is difficult.</S>
			<S sid ="18" ssid = "12">Accu P (W, T ) = n P (witi|w0t0 . . .</S>
			<S sid ="19" ssid = "13">wi−1ti−1), i=1 n n P (wi|ti)P (ti|ti−1), (1) i=1 racy of word segmentation for unknown words is usually much lower than that for known words.</S>
			<S sid ="20" ssid = "14">In this paper, we propose a hybrid method for Chinese and Japanese word segmentation, which utilizes both word-level and character-level information.</S>
			<S sid ="21" ssid = "15">Word-level information is useful for analysis of known words, and character-level information is useful for analysis of unknown words.</S>
			<S sid ="22" ssid = "16">We use these two types of information at the same time to obtain high overall performance.</S>
			<S sid ="23" ssid = "17">This paper is organized as follows: Section 2 describes previous work on Chinese and Japanese word segmentation on which our method is based.</S>
			<S sid ="24" ssid = "18">Section 3 introduces the hybrid method which combines word-level and character-level processing.</S>
			<S sid ="25" ssid = "19">Section 4 shows experimental results of Chinese and Japanese word segmentation.</S>
			<S sid ="26" ssid = "20">Section 5 discusses related work, and Section 6 gives the conclusion.</S>
			<S sid ="27" ssid = "21">where w0(t0) is a special word(part-of-speech) representing the beginning of the sentence.</S>
			<S sid ="28" ssid = "22">Given a word sequence W , its most likely POS sequence Tˆ can be found as follows: Tˆ = argmax P (T |W ), T = argmax P (W, T ) , T P (W ) = argmax P (W, T ), T n argmax n P (wi|ti)P (ti|ti−1).</S>
			<S sid ="29" ssid = "23">(2) T i=1 The equation above can be solved efficiently by the Viterbi algorithm (Rabiner and Juang, 1993).</S>
			<S sid ="30" ssid = "24">In Chinese and Japanese, the method is used with some modifications.</S>
			<S sid ="31" ssid = "25">Because each word in a Figure 1: Example of Lattice Used in the Markov Model-Based Method sentence is not separated explicitly in Chinese and Japanese, both segmentation of words and identification of the parts-of-speech tags of the words must be done simultaneously.</S>
			<S sid ="32" ssid = "26">Given a sentence S, its most likely word sequence Wˆ and POS sequence Tˆ can be found as follows where W ranges over the possible segments of S (w1 · · · wn = S): (Wˆ , Tˆ) = argmax P (W, T |S), W,T = argmax P (W, T , S) , Table 1: The ‘B, I, E, S’ Tag Set the lattice.</S>
			<S sid ="33" ssid = "27">The candidates of unknown words can be generated by heuristic rules(Matsumoto et al., 2001) or statistical word models which predict the proba W,T P (S) bilities for any strings to be unknown words (Sproat = argmax P (W, T , S), W,T = argmax P (W, T ), W,T n argmax n P (wi|ti)P (ti|ti−1).</S>
			<S sid ="34" ssid = "28">(3) et al., 1996; Nagata, 1999).</S>
			<S sid ="35" ssid = "29">However, such heuristic rules or word models must be carefully designed for a specific language, and it is difficult to properly process a wide variety of unknown words.</S>
			<S sid ="36" ssid = "30">2.2 The Character Tagging Method.</S>
			<S sid ="37" ssid = "31">W,T i=1 This method carries out word segmentation by tag The equation above can be solved using the Viterbi algorithm as well.</S>
			<S sid ="38" ssid = "32">The possible segments of a given sentence are represented by a lattice, and Figure 1 shows an example.</S>
			<S sid ="39" ssid = "33">Given a sentence, this method first constructs such a lattice using a word dictionary, then chooses the best path which maximizes Equation (3).</S>
			<S sid ="40" ssid = "34">This Markov model-based method achieves high accuracy with low computational cost, and many Japanese word segmentation systems adopt it (Kurohashi and Nagao, 1998; Matsumoto et al., 2001).</S>
			<S sid ="41" ssid = "35">However, the Markov model-based method has a difficulty in handling unknown words.</S>
			<S sid ="42" ssid = "36">In the constructing process of a lattice, only known words are dealt with and unknown words must be handled with other methods.</S>
			<S sid ="43" ssid = "37">Many practical word segmentation systems add candidates of unknown words to ging each character in a given sentence, and in this method, the tags indicate word-internal positions of the characters.</S>
			<S sid ="44" ssid = "38">We call such tags position- of-character (POC) tags (Xue, 2003) in this paper.</S>
			<S sid ="45" ssid = "39">Several POC-tag sets have been studied (Sang and Veenstra, 1999; Sekine et al., 1998), and we use the ‘B, I, E, S’ tag set shown in Table 1 1.</S>
			<S sid ="46" ssid = "40">Figure 2 shows an example of POC-tagging.</S>
			<S sid ="47" ssid = "41">The POC-tags can represent word boundaries for any sentences, and the word segmentation task can be reformulated as the POC-tagging task.</S>
			<S sid ="48" ssid = "42">The tagging task can be solved by using general machine learning techniques such as maximum entropy (ME) models (Xue, 2003) and support vector machines (Yoshida et al., 2003; Asahara et al., 2003).</S>
			<S sid ="49" ssid = "43">1 The ‘B, I, E, S’ tags are also called ‘OPCN, CNCN, CNCL, OPCL’ tags (Sekine et al., 1998) or ‘LL, MM, RR, LR’ tags (Xue, 2003).</S>
			<S sid ="50" ssid = "44">Figure 2: Example of the Character Tagging Method: Word boundaries are indicated by vertical lines (‘|’).</S>
			<S sid ="51" ssid = "45">This character tagging method can easily handle unknown words, because known words and unknown words are treated equally and no other exceptional processing is necessary.</S>
			<S sid ="52" ssid = "46">This approach is also used in base-NP chunking (Ramshaw and Marcus, 1995) and named entity recognition (Sekine et al., 1998) as well as word segmentation.</S>
	</SECTION>
	<SECTION title="Word Segmentation Using Word-Level. " number = "3">
			<S sid ="53" ssid = "1">and Character-Level Information We saw the two methods for word segmentation in the previous section.</S>
			<S sid ="54" ssid = "2">It is observed that the for each character) are made.</S>
			<S sid ="55" ssid = "3">Then, the most likely path is searched (the thick line indicates the correct path in the example).</S>
			<S sid ="56" ssid = "4">Unknown words are identified by the nodes with POC-tags.</S>
			<S sid ="57" ssid = "5">Note that some transitions of states are not allowed (e.g. from I to B, or from any POS-tags to E), and such transitions are ignored.</S>
			<S sid ="58" ssid = "6">Because the basic Markov models in Equation (1) are not expressive enough, we use the following equation instead to estimate probability of a path in a lattice more precisely: n Markov model-based method has high overall accuracy, however, the accuracy drops for unknown words, and the character tagging method has high accuracy for unknown words but lower accuracy for known words (Yoshida et al., 2003; Xue, 2003; Sproat and Emerson, 2003).</S>
			<S sid ="59" ssid = "7">This seems natural because words are used as a processing unit in the Markov model-based method, and therefore much information about known words (e.g., POS or word bigram probability) can be used.</S>
			<S sid ="60" ssid = "8">However, un P (W, T ) = n P (witi|w0t0 . . .</S>
			<S sid ="61" ssid = "9">wi−1ti−1), i=1 n {λ1P (wi|ti)P (ti) i=1 +λ2P (wi|ti)P (ti|ti−1) +λ3P (wi|ti)P (ti|ti−2ti−1) +λ4P (witi|wi−1ti−1)}, (λ1 + λ2 + λ3 + λ4 = 1).</S>
			<S sid ="62" ssid = "10">(4) known words cannot be handled directly by this method itself.</S>
			<S sid ="63" ssid = "11">On the other hand, characters are used as a unit in the character tagging method.</S>
			<S sid ="64" ssid = "12">In general, the number of characters is finite and far The probabilities in the equation above are estimated from a word segmented and POS-tagged corpus using the maximum-likelihood method, for example,fewer than that of words which continuously in creases.</S>
			<S sid ="65" ssid = "13">Thus the character tagging method may be P (wi|ti) =  f (wi ,ti )  w f (w,ti ) (f (wi, ti) &gt; 0), (5) robust for unknown words, but cannot use more detailed information than character-level information.</S>
			<S sid ="66" ssid = "14">0.5  f (w,ti ) (f (wi, ti) = 0), Then, we propose a hybrid method which combines the Markov model-based method and the character tagging method to make the most of word- level and character-level information, in order to achieve high overall accuracy.</S>
			<S sid ="67" ssid = "15">3.1 A Hybrid Method The hybrid method is mainly based on word-level Markov models, but both POC-tags and POS-tags are used in the same time and word segmentation for known words and unknown words are conducted simultaneously.</S>
			<S sid ="68" ssid = "16">Figure 3 shows an example of the method given a Japanese sentence “ ”, where the word “ ”(person’s name) is an un known word.</S>
			<S sid ="69" ssid = "17">First, given a sentence, nodes of lattice for known words are made as in the usual Markov model-based method.</S>
			<S sid ="70" ssid = "18">Next, for each character in the sentence, nodes of POC-tags (four nodes where f (w, t) is a frequency that the word w with the tag t occurred in training data.</S>
			<S sid ="71" ssid = "19">Unseen events in the training data are handled as they occurred 0.5 times for smoothing.</S>
			<S sid ="72" ssid = "20">λ1, λ2, λ3, λ4 are calculated by deleted interpolation as described in (Brants, 2000).</S>
			<S sid ="73" ssid = "21">A word dictionary for a Markov model- based system is often constructed from a training corpus, and no unknown words exist in the training corpus in such a case.</S>
			<S sid ="74" ssid = "22">Therefore, when the parameters of the above probabilities are trained from a training corpus, words that appear only once in the training corpus are regarded as unknown words and decomposed to characters with POC-tags so that statistics about unknown words are obtained2.</S>
			<S sid ="75" ssid = "23">2 As described in Equation (5), we used the additive smoothing method which is simple and easy to implement.</S>
			<S sid ="76" ssid = "24">Although there are other more sophisticated methods such as Good- Turing smoothing, they may not necessarily perform well because the distribution of words is changed by this operation.</S>
			<S sid ="77" ssid = "25">Figure 3: Example of the Hybrid Method In order to handle various character-level features, we calculate word emission probabilities for POC-tags by Bayes’ theorem: P (wi|ti) = P (ti|wi, ti ∈ TP OC )P (wi, ti ∈ TP OC ) , P (ti) P (ti|wi, ti ∈ TP OC ) t∈TP OC P (wi, t) Table 2: Character Types = P (ti) , (6) cj · · · cj+k−1 is calculated as: where TP OC = {B, I, E, S}, wi is a character and ti is a POC-tag.</S>
			<S sid ="78" ssid = "26">In the above equation, P (ti) and P (wi, t) are estimated by the maximum-likelihood method, and the probability of a POC tag ti, given a character wi (P (ti|wi, ti ∈ TP OC )) is estimated using ME models (Berger et al., 1996).</S>
			<S sid ="79" ssid = "27">We use the following features for ME models, where cx is the xth character in a sentence, wi = cit and yx is the character type of cx (Table 2 shows the definition of character types we used): (1) Characters (cit −2, cit −1, cit , cit +1, cit +2) (2) Pairs of characters (cit −2cit −1, cit −1cit , cit −1cit +1, cit cit +1, cit +1cit +2) (3) Character types (yit −2, yit −1, yit , yit +1, yit +2) (4) Pairs of character types (yit −2yit −1, yit −1yit , yit −1yit +1, yit yit +1, yit +1yit +2) Parameters of ME are trained using all the words in training data.</S>
			<S sid ="80" ssid = "28">We use the Generalized Iterative Scaling algorithm (Darroch and Ratcliff, 1972) for parameter estimation, and features that appeared less than or equal to 10 times in training data are ignored in order to avoid overfitting.</S>
			<S sid ="81" ssid = "29">What our method is doing for unknown words can be interpreted as follows: The method examines all possible unknown words in a sentence, and probability for an unknown word of length k, wi = P (witi|h) ( 7)  P (cj S|h) (k = 1), =  P (cj B|h) j+k−2 P (c I|h)P (c E|h) l=j+1 l j+k−1  (k &gt; 1), where h is a history of the sequence.</S>
			<S sid ="82" ssid = "30">In other words, the probability of the unknown word is approximated by the product of the probabilities of the composing characters, and this calculation is done in the framework of the word level Markov model-based method.</S>
			<S sid ="83" ssid = "31">4 Experiments This section gives experimental results of Chinese and Japanese word segmentation with the hybrid method.</S>
			<S sid ="84" ssid = "32">The following values are used to evaluate the performance of word segmentation: R : Recall (The number of correctly segmented words in system’s output divided by the number of words in test data) P : Precision (The number of correctly segmented words in system’s output divided by the number of words in system’s output) F : F-measure (F = 2 × R × P /(R + P )) Rknown : Recall for known words Runknown : Recall for unknown words Co rp us # of Tr ai ni ng W or ds # of Te sti ng W or ds ( k n o w n / u n k n o w n ) # of W or ds in Di cti on ar y Ra te of U nk no w n W or ds A S H K P K 5 , 8 0 6 , 6 1 1 2 3 9 , 8 5 2 1 , 1 2 1 , 0 1 7 11 ,9 85 (1 1, 72 7/ 25 8) 34 ,9 55 (3 2, 46 3/ 2, 49 2) 17 ,1 94 (1 6, 00 5/ 1, 18 9) 1 4 6 , 2 1 2 2 3 , 7 4 7 5 5 , 2 2 6 0 . 0 2 1 5 0 . 0 7 1 3 0 . 0 6 9 2 R W C P 8 4 0 , 8 7 9 93 ,1 55 (9 3, 08 5/ 70) 3 1 5 , 6 0 2 0 . 0 0 0 8 Table 3: Statistical Information of Corpora 4.1 Experiments of Chinese Word.</S>
			<S sid ="85" ssid = "33">Segmentation We use three Chinese word-segmented corpora, the Academia Sinica corpus (AS), the Hong Kong City University corpus (HK) and the Beijing University corpus (PK), all of which were used in the First International Chinese Word Segmentation Bake- off (Sproat and Emerson, 2003) at ACLSIGHAN 2003.</S>
			<S sid ="86" ssid = "34">The three corpora are word-segmented corpora, but POS-tags are not attached, therefore we need to attach a POS-tag (state) which is necessary for the Markov model-based method to each word.</S>
			<S sid ="87" ssid = "35">We attached a state for each word using the Baum-Welch algorithm (Rabiner and Juang, 1993) which is used for Hidden Markov Models.</S>
			<S sid ="88" ssid = "36">The algorithm finds a locally optimal tag sequence which maximizes Equation (1) in an unsupervised way.</S>
			<S sid ="89" ssid = "37">The initial states are randomly assigned, and the number of states is set to 64.</S>
			<S sid ="90" ssid = "38">We use the following systems for comparison: Bakeoff1, 2, 3 The top three systems participated in the SIGHAN Bakeoff (Sproat and Emerson, 2003).</S>
			<S sid ="91" ssid = "39">Maximum Matching A word segmentation system using the well-known maximum matching method.</S>
			<S sid ="92" ssid = "40">Character Tagging A word segmentation system using the character tagging method.</S>
			<S sid ="93" ssid = "41">This system is almost the same as the one studied by Xue (2003).</S>
			<S sid ="94" ssid = "42">Features described in Section 3.1 (1)–(4) and the following (5) are used to estimate a POC tag of a character cit , where tx is a POC-tag of the xth character in a sentence: (5) Unigram and bigram of previous POC- tags (tit −1, tit −2tit −1) All these systems including ours do not use any other knowledge or resources than the training data.</S>
			<S sid ="95" ssid = "43">In this experiments, word dictionaries used by the hybrid method and Maximum Matching are constructed from all the words in each training corpus.</S>
			<S sid ="96" ssid = "44">Statistical information of these data is shown in Table 3.</S>
			<S sid ="97" ssid = "45">The calculated values of λi in Equation (4) are shown in Table 4.</S>
			<S sid ="98" ssid = "46">Co rp us λ 1 λ2 λ3 λ4 A S H K P K 0.</S>
			<S sid ="99" ssid = "47">03 7 0.178 0.257 0.528 0.</S>
			<S sid ="100" ssid = "48">04 8 0.251 0.313 0.388 0.</S>
			<S sid ="101" ssid = "49">05 5 0.207 0.242 0.495 R W C P 0.</S>
			<S sid ="102" ssid = "50">07 3 0.105 0.252 0.571 Table 4: Calculated Values of λi The results are shown in Table 5.</S>
			<S sid ="103" ssid = "51">Our system achieved the best F-measure values for the three corpora.</S>
			<S sid ="104" ssid = "52">Although the hybrid system’s recall values for known words are not high compared to the participants of SIGHAN Bakeoff, the recall values for known words and unknown words are relatively well-balanced.</S>
			<S sid ="105" ssid = "53">The results of Maximum Matching and Character Tagging show the trade-off between the word-based approach and the character-based approach which was discussed in Section 3.</S>
			<S sid ="106" ssid = "54">Maximum Matching is word-based and has the higher recall values for known words than Character Tagging on the HK and PK corpus.</S>
			<S sid ="107" ssid = "55">Character Tagging is character-based and has the highest recall values for unknown words on the AS, HK and PK corpus.</S>
			<SUBSECTION>4.2 Experiments of Japanese Word.</SUBSECTION>
			<S sid ="108" ssid = "56">Segmentation We use the RWCP corpus, which is a Japanese word-segmented and POS-tagged corpus.</S>
			<S sid ="109" ssid = "57">We use the following systems for comparison: ChaSen The word segmentation and POS-tagging system based on extended Markov models (Asahara and Matsumoto, 2000; Matsumoto et al., 2001).</S>
			<S sid ="110" ssid = "58">This system carries out unknown word processing using heuristic rules.</S>
			<S sid ="111" ssid = "59">Maximum Matching The same system used in the Chinese experiments.</S>
			<S sid ="112" ssid = "60">Character Tagging The same system used in the Chinese experiments.</S>
			<S sid ="113" ssid = "61">As a dictionary for ChaSen, Maximum Matching and the hybrid method, we use IPADIC (Matsumoto and Asahara, 2001) which is attached to ChaSen.</S>
			<S sid ="114" ssid = "62">Statistical information of these data is shown in Table 3.</S>
			<S sid ="115" ssid = "63">The calculated values of λi in Equation (4) are shown in Table 4.</S>
			<S sid ="116" ssid = "64">Co rp us Sy ste m R P F Rk no wn Runknown A S H yb rid m et ho d 0.</S>
			<S sid ="117" ssid = "65">97 3 0.971 0.972 0 . 9 7 9 0.717 Ba ke off -1 Ba ke off -2 Ba ke off -3 M ax im u m M atc hi ng Ch ar act er Ta gg in g 0.</S>
			<S sid ="118" ssid = "66">96 6 0.956 0.961 0.</S>
			<S sid ="119" ssid = "67">96 1 0.958 0.959 0.</S>
			<S sid ="120" ssid = "68">94</S>
	</SECTION>
	<SECTION title="0.945	0.945" number = "4">
			<S sid ="121" ssid = "1">0.</S>
			<S sid ="122" ssid = "2">91 7 0.912 0.915 0.</S>
			<S sid ="123" ssid = "3">96 2 0.959 0.960 0 . 9 8 0 0.364 0 . 9 6 6 0.729 0 . 9 5 2 0.574 0 . 9 3 8 0.000 0 . 9 6 6 0.744 H K H yb rid m et ho d 0.</S>
			<S sid ="124" ssid = "4">95 1 0.948 0.950 0 . 9 6 9 0.715 Ba ke off -1 Ba ke off -2 Ba ke off -3 M ax im u m M atc hi ng Ch ar act er Ta gg in g 0.</S>
			<S sid ="125" ssid = "5">94 7 0.934 0.940 0.</S>
			<S sid ="126" ssid = "6">94 0 0.908 0.924 0.</S>
			<S sid ="127" ssid = "7">91 7 0.915 0.916 0.</S>
			<S sid ="128" ssid = "8">90 8 0.830 0.867 0.</S>
			<S sid ="129" ssid = "9">91 7 0.917 0.917 0 . 9 7 2 0.625 0 . 9 8 0 0.415 0 . 9 3 6 0.670 0 . 9 7</S>
	</SECTION>
	<SECTION title="0.037" number = "5">
			<S sid ="130" ssid = "1">0 . 9 3 2 0.728 P K H yb rid m et ho d 0.</S>
			<S sid ="131" ssid = "2">95 7 0.952 0.954 0 . 9 7 0 0.774 Ba ke off -1 Ba ke off -2 Ba ke off -3 M ax im u m M atc hi ng Ch ar act er Ta gg in g 0.</S>
			<S sid ="132" ssid = "3">96 2 0.940 0.951 0.</S>
			<S sid ="133" ssid = "4">95 5 0.938 0.947 0.</S>
			<S sid ="134" ssid = "5">95 5 0.938 0.946 0.</S>
			<S sid ="135" ssid = "6">93 0 0.883 0.906 0.</S>
			<S sid ="136" ssid = "7">93 2 0.931 0.931 0 . 9 7 9 0.724 0 . 9 7</S>
	</SECTION>
	<SECTION title="0.680" number = "6">
			<S sid ="137" ssid = "1">0 . 9 7</S>
	</SECTION>
	<SECTION title="0.647" number = "7">
			<S sid ="138" ssid = "1">0 . 9 7 4 0.020 0 . 9 4 3 0.786 Table 5: Performance of Chinese Word Segmentation Co rp us Sy ste m R P F Rk no wn Runknown R W C P H yb rid m et ho d 0.</S>
			<S sid ="139" ssid = "2">99 3 0.994 0.993 0 . 9 9 3 0.586 Ch aS en M ax im u m M atc hi ng Ch ar act er Ta gg in g 0.</S>
			<S sid ="140" ssid = "3">99 1 0.992 0.991 0.</S>
			<S sid ="141" ssid = "4">88 0 0.918 0.898 0.</S>
			<S sid ="142" ssid = "5">97 2 0.968 0.970 0 . 9 9 1 0.243 0 . 8 8 0 0.100 0 . 9 7 2 0.629 Table 6: Performance of Japanese Word Segmentation The results are shown in Table 63.</S>
			<S sid ="143" ssid = "6">Compared to ChaSen, the hybrid method has the comparable F- measure value and the higher recall value for unknown words (the difference is statistically significant at 95% confidence level).</S>
			<S sid ="144" ssid = "7">Character Tagging has the highest recall value for unknown words as in the Chinese experiments.</S>
			<S sid ="145" ssid = "8">5 Discussion.</S>
			<S sid ="146" ssid = "9">Several studies have been conducted on word segmentation and unknown word processing.</S>
			<S sid ="147" ssid = "10">Xue (2003) studied Chinese word segmentation using the character tagging method.</S>
			<S sid ="148" ssid = "11">As seen in the previous section, this method handles known and unknown words in the same way basing on character- level information.</S>
			<S sid ="149" ssid = "12">Our experiments showed that the method has quite high accuracy for unknown words, but accuracy for known words tends to be lower than other methods.</S>
			<S sid ="150" ssid = "13">3 In this evaluation, Rknown and Runknown are calculated considering words in the dictionary as known words.</S>
			<S sid ="151" ssid = "14">Words which are in the training corpus but not in the dictionary are handled as unknown words in the calculations.</S>
			<S sid ="152" ssid = "15">The number of known/unknown words of the RWCP corpus shown in Table 3 is also calculated in the same way.</S>
			<S sid ="153" ssid = "16">Uchimoto et al.</S>
			<S sid ="154" ssid = "17">(2001) studied Japanese word segmentation using ME models.</S>
			<S sid ="155" ssid = "18">Although their method is word-based, no word dictionaries are used directly and known and unknown words are handled in a same way.</S>
			<S sid ="156" ssid = "19">The method estimates how likely a string is to be a word using ME. Given a sentence, the method estimates the probabilities for every substrings in the sentence.</S>
			<S sid ="157" ssid = "20">Word segmentation is conducted by finding a division of the sentence which maximizes the product of probabilities that each divided substring is a word.</S>
			<S sid ="158" ssid = "21">Compared to our method, their method can handle some types of features for unknown words such as “the word starts with an alphabet and ends with a numeral” or “the word consists of four characters”.</S>
			<S sid ="159" ssid = "22">Our method cannot handle such word-level features because unknown words are handled by using a character as a unit.</S>
			<S sid ="160" ssid = "23">On the other hand, their method seems to have a computational cost problem.</S>
			<S sid ="161" ssid = "24">In their method, unknown words are processed by using a word as a unit, and the number of candidates for unknown words in a sentence which consists of n characters is equal to n(n + 1)/2.</S>
			<S sid ="162" ssid = "25">Actually, they did not consider every substrings in a sentence, and limited the length of substrings to be less than or equal to five characters.</S>
			<S sid ="163" ssid = "26">In our method, the number of POC- tagged characters which is necessary for unknown word processing is equal to 4n, and there is no limitation for the length of unknown words.</S>
			<S sid ="164" ssid = "27">Asahara et al.</S>
			<S sid ="165" ssid = "28">(2003) studied Chinese word segmentation based on a character tagging method with support vector machines.</S>
			<S sid ="166" ssid = "29">They preprocessed a given sentence using a word segmenter based on Markov models, and the output is used as features for character tagging.</S>
			<S sid ="167" ssid = "30">Their method is a character- based method incorporating word-level information and that is reverse to our approach.</S>
			<S sid ="168" ssid = "31">They did not use some of the features we used like character types, and our method achieved higher accuracies compared to theirs on the AS, HK and PK corpora (Asahara et al., 2003).</S>
			<S sid ="169" ssid = "32">6 Conclusion.</S>
			<S sid ="170" ssid = "33">In this paper, we presented a hybrid method for word segmentation, which utilizes both word-level and character-level information to obtain high accuracy for known and unknown words.</S>
			<S sid ="171" ssid = "34">The method combines two existing methods, the Markov model- based method and character tagging method.</S>
			<S sid ="172" ssid = "35">Experimental results showed that the method achieves high accuracy compared to the other state-of-the-art methods in both Chinese and Japanese word segmentation.</S>
			<S sid ="173" ssid = "36">The method can conduct POS tagging for known words as well as word segmentation, but tagging identified unknown words is left as future work.</S>
	</SECTION>
	<SECTION title="Acknowledgements">
			<S sid ="174" ssid = "37">This work was supported by a grant from the National Institute of Information and Communications Technology of Japan.</S>
	</SECTION>
</PAPER>
