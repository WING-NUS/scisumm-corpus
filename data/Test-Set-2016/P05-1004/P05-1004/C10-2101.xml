<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">In this paper, we present a two-stage approach to acquire Japanese unknown morphemes from text with full POS tags assigned to them.</S>
		<S sid ="2" ssid = "2">We first acquire unknown morphemes only making a morphology- level distinction, and then apply semantic classification to acquired nouns.</S>
		<S sid ="3" ssid = "3">One advantage of this approach is that, at the second stage, we can exploit syntactic clues in addition to morphological ones because as a result of the first stage acquisition, we can rely on automatic parsing.</S>
		<S sid ="4" ssid = "4">Japanese semantic classification poses an interesting challenge: proper nouns need to be distinguished from common nouns.</S>
		<S sid ="5" ssid = "5">It is because Japanese has no orthographic distinction between common and proper nouns and no apparent morphosyntactic distinction between them.</S>
		<S sid ="6" ssid = "6">We explore lexico-syntactic clues that are extracted from automatically parsed text and investigate their effects.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="7" ssid = "7">A dictionary plays an important role in Japanese morphological analysis, or the joint task of segmentation and part-of-speech (POS) tagging (Kurohashi et al., 1994; Asahara and Mat- sumoto, 2000; Kudo et al., 2004).</S>
			<S sid ="8" ssid = "8">Like Chinese and Thai, Japanese does not delimit words by white-space.</S>
			<S sid ="9" ssid = "9">This makes the first step of natural language processing more ambiguous than simple POS tagging.</S>
			<S sid ="10" ssid = "10">Accordingly, morphemes in a predefined dictionary compactly represent our knowledge about both segmentation and POS.</S>
			<S sid ="11" ssid = "11">One obvious problem with the dictionary-based approach is caused by unknown morphemes, or morphemes not defined in the dictionary.</S>
			<S sid ="12" ssid = "12">Even though, historically, extensive human resources were used to build high-coverage dictionaries (Yokoi, 1995), texts other than newspaper articles, in particular web pages, contain a large number of unknown morphemes.</S>
			<S sid ="13" ssid = "13">These unknown morphemes often cause segmentation errors.</S>
			<S sid ="14" ssid = "14">For example, morphological analyzer JU MAN 6.01 wrongly segments the phrase “さっぽ ろ駅” (saQporo eki, “Sapporo Station”), where “ さっぽろ” (saQporo) is an unknown morpheme, as follows: “さ” (sa, noun-common, “difference”), “っ” (Q, UNK), “ぽ” (po, UNK), “ろ” (ro, noun-common, “sumac”) and “駅” (eki, noun-common, “station”), where UNK refers to unknown morphemes automatically identified by the analyzer.</S>
			<S sid ="15" ssid = "15">Such an erroneous sequence has disastrous effects on applications of morphological analysis.</S>
			<S sid ="16" ssid = "16">For example, it can hardly be identified as a LOCATION in named entity recognition.</S>
			<S sid ="17" ssid = "17">One solution to the unknown morpheme problem is unknown morpheme acquisition (Mori and Nagao, 1996; Murawaki and Kurohashi, 2008).</S>
			<S sid ="18" ssid = "18">It is the task of automatically augmenting the dictionary by acquiring unknown morphemes from text.</S>
			<S sid ="19" ssid = "19">In the above example, the goal is to acquire the morpheme “さっぽろ” (saQporo) with the POS tag “noun-location name.” However, unknown morpheme acquisition usually adopts a coarser POS tagset that only represents the morphology level distinction among noun, verb and adjective.</S>
			<S sid ="20" ssid = "20">This means that “さっぽろ” (saQporo) is acquired as just a noun and that the semantic label “location name” remains to be assigned.</S>
			<S sid ="21" ssid = "21">The reason only the morphology level distinction is made is 1 http://nlp.kuee.kyoto-u.ac.jp/ nl-resource/juman-e.html 876 Coling 2010: Poster Volume, pages 876–884, Beijing, August 2010 that the semantic level distinction cannot easily be captured with morphological clues that are exploited in unknown morpheme acquisition.</S>
			<S sid ="22" ssid = "22">In this paper, we investigate the remaining problem and introduce the new task of semantic classification that is to be applied to automatically acquired nouns.</S>
			<S sid ="23" ssid = "23">In this task, we can exploit syntactic clues in addition to morphological ones because, as a result of acquisition, we can now rely on automatic parsing.</S>
			<S sid ="24" ssid = "24">For exam ple, since text containing “さっぽろ” (saQporo, noun-unclassified) is correctly segmented, we can extract not only the phrase “saQporo station,” but the tree fragment “ϕ go to saQporo,” and we can determine its semantic label.</S>
			<S sid ="25" ssid = "25">Japanese semantic classification poses an interesting challenge: proper nouns need to be distinguished from common nouns.</S>
			<S sid ="26" ssid = "26">Like Chinese and Thai, Japanese has no orthographic distinction between common and proper nouns as there is no such thing as capitalization.</S>
			<S sid ="27" ssid = "27">In addition, there seems no morphosyntactic (i.e. grammatical) distinction between them.</S>
			<S sid ="28" ssid = "28">In this paper, we explore lexico-syntactic clues that can be extracted from automatically parsed text.</S>
			<S sid ="29" ssid = "29">We train a classification model on manually registered nouns and apply it to automatically acquired nouns.</S>
			<S sid ="30" ssid = "30">We then investigate the effects of lexico-syntactic clues.</S>
	</SECTION>
	<SECTION title="Semantic Classification Task. " number = "2">
			<S sid ="31" ssid = "1">2.1 Two-Stage Approach to Unknown.</S>
			<S sid ="32" ssid = "2">Morpheme Acquisition Our goal is to identify unknown morphemes in un- segmented text and assign POS tags to them.</S>
			<S sid ="33" ssid = "3">In this section, we omit the details of boundary identification (segmentation) and review the Japanese POS tagset to see why we propose a two-stage approach to assign full POS tags.</S>
			<S sid ="34" ssid = "4">The Japanese POS tagset derives from traditional grammar.</S>
			<S sid ="35" ssid = "5">It is a mixture of several linguistic levels: morphology, syntax and semantics.</S>
			<S sid ="36" ssid = "6">In other words, information encoded in a POS tag is more than how the morpheme behaves in a sequence of morphemes.</S>
			<S sid ="37" ssid = "7">In fact, POS tags given to predefined morphemes are useful for applications of morphological analysis, such as dependency parsing (Kudo and Matsumoto, 2002), named entity recognition (Asahara and Matsumoto, 2003; Sasano and Kurohashi, 2008) and anaphora resolution (Iida et al., 2009; Sasano and Kurohashi, 2009).</S>
			<S sid ="38" ssid = "8">In these applications, POS tags are incorporated as features for models.</S>
			<S sid ="39" ssid = "9">On the other hand, the mixed nature of the POS tagset poses a challenge to unknown morpheme acquisition.</S>
			<S sid ="40" ssid = "10">Previous approaches (Mori and Nagao, 1996; Murawaki and Kurohashi, 2008) directly or indirectly reply on morphology, or our knowledge on how a morpheme behaves in a sequence of morphemes.</S>
			<S sid ="41" ssid = "11">This means that semantic level distinction is difficult to make in these approaches, and in fact, is left unresolved.</S>
			<S sid ="42" ssid = "12">To be specific, nouns are only distinguished from verbs and adjectives but they have subcategories in the original tagset.</S>
			<S sid ="43" ssid = "13">These are what we try to classify acquired nouns into in this paper.</S>
			<S sid ="44" ssid = "14">2.2 Semantic Labels.</S>
			<S sid ="45" ssid = "15">The Japanese noun subcategories may require an explanation since they are different from the English ones (Marcus et al., 1993) in many respects.</S>
			<S sid ="46" ssid = "16">Singular and mass nouns are not distinguished from plural nouns because Japanese has no grammatical distinction between them.</S>
			<S sid ="47" ssid = "17">More importantly for this paper, proper nouns have sub- categories such as person name, location name and organization name in addition to the distinction from common nouns.</S>
			<S sid ="48" ssid = "18">These subcategories provide important information to named entity recognition among other applications.</S>
			<S sid ="49" ssid = "19">For proper nouns, we adopt these subcategories as semantic labels in our task.</S>
			<S sid ="50" ssid = "20">In contrast to proper nouns, common nouns have only one subcategory “common.” However, we consider that subcategories of common nouns similar to those of proper nouns are useful for, for example, anaphora resolution (Sasano and Kurohashi, 2009).</S>
			<S sid ="51" ssid = "21">We adopt the “categories” of morphological analyzer JUMAN, with which common nouns in its dictionary are annotated.</S>
			<S sid ="52" ssid = "22">There are 22 “categories” including PERSON, ORGANIZATION and CONCEPT.</S>
			<S sid ="53" ssid = "23">We collapse these “categories” into coarser semantic labels that roughly correspond to those for proper nouns.</S>
			<S sid ="54" ssid = "24">To sum up, we define 9 semantic labels as shown Table 1: List of semantic labels.</S>
			<S sid ="55" ssid = "25">lab els P / C s o u r c e s 1 m an ua lly re gi st er ed n o u n s a ut o m at ic al ly a c q ui re d n o u n s PSN P pr o p er sub PO S:p ers on na me 松 井 (m ats ui, a sur na me ) ジ ョ ー ジ (joˆ ji, “G eo rg e”) 佐 祐 理 (sa yuri , a giv en na me ) キ ョ ン (ky oN, a nic kn am e) LOC P sub PO S:p lac e na me 京 都 (ky out o, “K yot o”) ド イ ツ (do itsu , “G er ma ny” ) ア キ バ (ak iba , “A kih ab ara ”) ワ イ キ キ (w aiki ki, “W aik iki” ) O RG P su bP OS :or ga niz ati on na me 日 銀 (nic higi n, a ba nk) N HK (a br oa dc ast er) マ ツ ダ (m ats ud a, “M az da” ) ヤ フ ー (ya huˆ , “Y ah oo” ) OTH P sub PO S:p rop er no un 平 成 (hei sei, an era na me ) ス ラ ブ (su rab u, “Sl av” ) ジ プ シ ー (jip us hˆı, “G yp sy” ) PSN C co m mo n cat eg ory :P ER S O N 先 生 (se Nse i, “te ac he r”) ス タ ッ フ (su taQ fu, “st aff ”) メ ル 友 (m eru to mo , “ke yp al”) ニ ー ト (nˆıt o, “N EE T”) LOC C cat eg ory :PL ACE ∗2 職 場 (sh oku ba, “of fic e”) カ フ ェ (ka fe, “ca fe” ) 囲 炉 裏 (iro ri, “he art h”) 圃 場 (ho jou , “fa rm fiel d”) O RG C cat eg ory :O RG AN IZ AT IO N 政 府 (se ifu, “g ov er nm ent ”) チ ー ム (chˆ ım u, “te am ”) メ ー カ (me ˆka , “m an ufa ctu rer ”) 弊 所 (he ish o, “ou r offi ce” ) A NI -C cat eg ory :A NI MA L an d cat eg ory :A NI M AL PA RT 犬 (in u, “d og ”) 顔 (ka o, “fa ce” ) チ ワ ワ (chi wa wa, “C hih ua hu a”) マ ン タ (m aNt a, “m ant a”) OTH C oth er cat eg ori es 主 張 (sh uch ou, “ar gu me nt” ) 枕 (m ak ura , “pil lo w” ) 甚 平 (jiN bei , a kin d of clo thi ng) 着 メ ロ (ch aku me ro, “rin gto ne” ) 1 A subPOS refers to a subcategory of noun.</S>
			<S sid ="56" ssid = "26">For example, PSN-P corresponds to the POS tag “noun-person name”.</S>
			<S sid ="57" ssid = "27">2 category:PLACE-INSTITUTION, category:PLACE-INSTITUION PART and others.</S>
			<S sid ="58" ssid = "28">in Table 1.</S>
			<S sid ="59" ssid = "29">2.3 Related Tasks.</S>
			<S sid ="60" ssid = "30">A line of research is dedicated to identify unknown morphemes with varying degrees of identification.</S>
			<S sid ="61" ssid = "31">Asahara and Matsumoto (2004) only focus on boundary identification (segmentation) of unknown morphemes.</S>
			<S sid ="62" ssid = "32">Mori and Nagao (1996), Nagata (1999) and Murawaki and Kurohashi (2008) assign POS tags at the morphology level.</S>
			<S sid ="63" ssid = "33">Uchimoto et al.</S>
			<S sid ="64" ssid = "34">(2001) assign full POS tags but unsurprisingly the accuracy is low.</S>
			<S sid ="65" ssid = "35">Nakagawa and Matsumoto (2006) also assign full POS tags.</S>
			<S sid ="66" ssid = "36">They address the fact that local information used in previous studies is inherently insufficient and present a method that uses global information, in other words, takes into consideration all occurrences of each unknown word in a document.</S>
			<S sid ="67" ssid = "37">They report an improvement in tagging proper nouns in Japanese.</S>
			<S sid ="68" ssid = "38">A related task is named entity recognition (NER).</S>
			<S sid ="69" ssid = "39">It can handle a named entity longer than a single morpheme and is usually formalized as a chunking problem.</S>
			<S sid ="70" ssid = "40">Since Japanese does not delimit words by white-space, the unit of chunk- ing can be a character (Asahara and Matsumoto,2003; Kazama and Torisawa, 2008) or a mor errors.</S>
			<S sid ="71" ssid = "41">In fact, Saito et al.</S>
			<S sid ="72" ssid = "42">(2007) report that a majority of unknown named entities (those never appear in a training corpus) contain unknown morphemes as their constituents and that NER models perform poorly on them.</S>
			<S sid ="73" ssid = "43">A straightforward solution to this problem would be to acquire unknown morphemes and to assign semantic labels to them.</S>
			<S sid ="74" ssid = "44">Another related task is supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006).</S>
			<S sid ="75" ssid = "45">A supersense corresponds to one of the 26 broad categories defined by WordNet (Fellbaum, 1998).</S>
			<S sid ="76" ssid = "46">Each noun synset is associated with a supersense.</S>
			<S sid ="77" ssid = "47">For example, “chair” has supersenses PERSON, ARTIFACT and ACT because it belongs to several synsets.</S>
			<S sid ="78" ssid = "48">Since supersense tagging is studied in English, it differs from our task in several respects.</S>
			<S sid ="79" ssid = "49">In English, the distinction between common and proper nouns is clear.</S>
			<S sid ="80" ssid = "50">In fact, the tagging models can use POS features even for unknown nouns.</S>
			<S sid ="81" ssid = "51">In addition, the syntactic behavior of English nouns is different from that of Japanese nouns (Gil, 1987).</S>
			<S sid ="82" ssid = "52">Definiteness is not marked in Japanese as it lacks determiners (e.g. “the” and “a”), and Japanese has no obligatory plural marking.</S>
			<S sid ="83" ssid = "53">On the other hand, Japanese obligatorily uses numeral classifiers to indicate the count of nouns, as in pheme (Sasano and Kurohashi, 2008).</S>
			<S sid ="84" ssid = "54">In either (1) saN satsu no hoNcase, NER models encode the output of morpho three CL GEN book logical analysis and therefore are affected by its three volumes of books, or three books, where “satsu” is a numeral classifier for books.</S>
			<S sid ="85" ssid = "55">A number together with its numeral classifier forms a numeral quantifier.</S>
			<S sid ="86" ssid = "56">Numeral quantifiers would be informative about the semantic categories of nouns.</S>
			<S sid ="87" ssid = "57">Note that Japanese shares the above features with Chinese and Thai.</S>
			<S sid ="88" ssid = "58">Our findings in this paper may hold for these languages.</S>
	</SECTION>
	<SECTION title="Proposed Method. " number = "3">
			<S sid ="89" ssid = "1">3.1 Lexico-Syntactic Clues.</S>
			<S sid ="90" ssid = "2">In the task of semantic classification, we can exploit syntactic clues in addition to morphological ones.</S>
			<S sid ="91" ssid = "3">As a result of unknown morpheme acquisition, text containing acquired morphemes, or former unknown morphemes, is correctly segmented.</S>
			<S sid ="92" ssid = "4">Now we can treat automatic parsing as (at least partly) reliable with regard to acquired morphemes.</S>
			<S sid ="93" ssid = "5">For noun X , we use the following sets of features for classification.</S>
			<S sid ="94" ssid = "6">call: noun phrase Y that appears in a pattern like “Y called X ” and “Y such as X ,” e.g. “call:kuni” from suf: suffix or suffix-like noun that follows X , e.g. “suf:saN” from “X saN” (Mr./Ms. X ) and “suf:eki” from “X eki” (X station).</S>
			<S sid ="95" ssid = "7">Using automatically parsed text to extract syntactic features has an advantage.</S>
			<S sid ="96" ssid = "8">Since no manual annotation is necessary, we can utilize a huge raw corpus.</S>
			<S sid ="97" ssid = "9">On the other hand, parsing errors are inevitable.</S>
			<S sid ="98" ssid = "10">However, we can circumvent this problem by using the constraints of Japanese dependency structures: head-final and projective.</S>
			<S sid ="99" ssid = "11">The simplest example is the second last element of a sentence, which always depends on the last element.</S>
			<S sid ="100" ssid = "12">With these constraints, we can focus on syntactically unambiguous dependency pairs and extract syntactic features accurately.</S>
			<S sid ="101" ssid = "13">We follow Kawahara and Kurohashi (2001) to extract a pair of an argument noun and a predicate (cf), and Sasano et al.</S>
			<S sid ="102" ssid = "14">(2004) to extract a pair of nouns connected with the genitive case marker “no” (ncf1 and ncf2).</S>
			<S sid ="103" ssid = "15">Noun X can be part of a compound noun.</S>
			<S sid ="104" ssid = "16">We leave it for named entity recognition.</S>
			<S sid ="105" ssid = "17">Except for suf, we extract features only when X alone forms a word.</S>
			<S sid ="106" ssid = "18">Similarly, we extract suf features only X to iu kuni when X and a suffix alone form a noun phrase.</S>
			<S sid ="107" ssid = "19">X QT call country a country called X . cf: predicate with a case marker with which it takes X as an argument, e.g. “cf:tooru:wo” from For call, ncf1, and ncf2, we generalize numerals within noun phrases.</S>
			<S sid ="108" ssid = "20">For “hoN” (book) in example 1, we extract the feature “ncf2:&lt;NUM&gt;satsu.” X wo tooru X ACC pass ϕ pass through X . demo: demonstrative that modifies X , e.g. “demo:kono” from “kono X ” (this X ) and “demo:doNna” from “doNna X ” (what kind of X ).</S>
			<S sid ="109" ssid = "21">ncf1: noun phrase which X modifies with the genitive case marker “no,” e.g. “ncf1:heya” from 3.2 Instances for Classification.</S>
			<S sid ="110" ssid = "22">Now that features are extracted for each noun, the question is how to combine them together to make an instance for classification.</S>
			<S sid ="111" ssid = "23">One factor we need to consider is polysemy: a noun can be a person name in one context and a location name in another.</S>
			<S sid ="112" ssid = "24">If we combine features extracted from the whole corpus, they may represent several semantic labels.</S>
			<S sid ="113" ssid = "25">X no heya Modeling a mixture of semantic labels might X GEN room X ’s room.</S>
			<S sid ="114" ssid = "26">ncf2: noun phrase that modifies X with the genitive case marker “no,” e.g. “ncf2:subete” from subete no X be a solution, but we do not take this approach on the grounds that each occurrence of a noun corresponds to a single semantic label.</S>
			<S sid ="115" ssid = "27">In our strategy, we perform classification multiple times for each noun and aggregate the results at the end.</S>
			<S sid ="116" ssid = "28">The features for each classification are all all X . GEN X extracted from a relatively small subset of a corpus where the noun is supposedly consistent in terms of semantic labels.</S>
			<S sid ="117" ssid = "29">In the field of named entity recognition, it is known that label consistency holds strongly at the level of a document and less strongly across different documents (Krishnan and Manning, 2006).</S>
			<S sid ="118" ssid = "30">Thus we start with a document and gradually cluster related documents until a sufficient number of features are obtained.</S>
			<S sid ="119" ssid = "31">For the specific procedures we took in the experiments, see Section 4.1.</S>
			<S sid ="120" ssid = "32">3.3 Training Data.</S>
			<S sid ="121" ssid = "33">Following unknown morpheme acquisition (Mu- rawaki and Kurohashi, 2008), we create training data using manually registered nouns, for which we can obtain correct semantic labels.</S>
			<S sid ="122" ssid = "34">We perform the same procedure as above to make instances of registered nouns.</S>
			<S sid ="123" ssid = "35">Some registered nouns are tagged with more than one semantic label, which we call “explicit polysemy.” We drop them from the training data.</S>
			<S sid ="124" ssid = "36">The remaining problem is “implicit polysemy.” Nouns are sometimes used with an uncovered sense.</S>
			<S sid ="125" ssid = "37">In preliminary experiments, we found that a typical case of implicit polysemy was that a proper noun derived from a basic noun.</S>
			<S sid ="126" ssid = "38">To alleviate this problem, we use an NE tagger for filtering.</S>
			<S sid ="127" ssid = "39">We run an NE tagger over a small portion of the corpus and extract common nouns that are frequently tagged as named entities.</S>
			<S sid ="128" ssid = "40">Then we remove these nouns from the training data.</S>
			<S sid ="129" ssid = "41">We also drop nouns that appear extremely frequently such as “人” (hito, “person”), “事” (koto, “thing”) and “私” (watashi, “I”2).</S>
			<S sid ="130" ssid = "42">Since acquired nouns to be classified are typically low frequency morphemes, they would not behave similarly to these basic nouns.</S>
			<S sid ="131" ssid = "43">the inner product of wy and x. y = argmax⟨wy , x⟩.</S>
			<S sid ="132" ssid = "44">y Several methods have been proposed to estimate weight vector wy from training data.</S>
			<S sid ="133" ssid = "45">We use online algorithms because they are easy to implement and scale to huge instances.</S>
			<S sid ="134" ssid = "46">We try the Per- ceptron family of algorithms.</S>
	</SECTION>
	<SECTION title="Experiments. " number = "4">
			<S sid ="135" ssid = "1">4.1 Settings.</S>
			<S sid ="136" ssid = "2">We used JUMAN for morphological analysis and KNP3 for dependency parsing.</S>
			<S sid ="137" ssid = "3">The dictionary of JUMAN was augmented with automatically acquired morphemes (Murawaki and Kurohashi, 2008).</S>
			<S sid ="138" ssid = "4">The number of manually registered morphemes was 120 thousands while there were 13,071 acquired morphemes, of which 12,615 morphemes were nouns.</S>
			<S sid ="139" ssid = "5">We used a web corpus that was compiled through the procedures proposed by Kawahara and Kurohashi (2006).</S>
			<S sid ="140" ssid = "6">It consisted of 100 million pages.</S>
			<S sid ="141" ssid = "7">We first extracted features from the web corpus.</S>
			<S sid ="142" ssid = "8">To keep the model size manageable, we used 447,082 features that appeared more than 100 times in the corpus.</S>
			<S sid ="143" ssid = "9">We constructed training data from manually registered nouns and test data from automatically acquired nouns.</S>
			<S sid ="144" ssid = "10">For each noun, we combined text together until the number of features grew to more than 100.</S>
			<S sid ="145" ssid = "11">We started with a single web page, then merge pages that share a domain name and finally clustered texts across different domains.</S>
			<S sid ="146" ssid = "12">We split the web corpus into 40 subcorpora and ap 4 3.4 Classifier.</S>
			<S sid ="147" ssid = "13">To assign a semantic label to each instance, we use a multiclass discriminative classifier.</S>
			<S sid ="148" ssid = "14">The input it takes is an instance that is represented by a feature vector x ∈ Rd. The output is one semantic label y ∈ Y , where Y is the set of semantic labels.</S>
			<S sid ="149" ssid = "15">We use a linear classifier.</S>
			<S sid ="150" ssid = "16">It has a weight vector wy ∈ Rd for each y and outputs y that maximizes 2 Japanese personal pronouns are treated as common nouns because they show no special morphosyntactic behavior.</S>
			<S sid ="151" ssid = "17">plied this procedure in parallel.</S>
			<S sid ="152" ssid = "18">We used Bayon for clustering domain texts.</S>
			<S sid ="153" ssid = "19">We sequentially read texts and applied the repeated bisections clustering every time some 5,000 pages were appended.</S>
			<S sid ="154" ssid = "20">The vectors for clustering were nouns, both registered and acquired, with their tfidf scores.</S>
			<S sid ="155" ssid = "21">We obtained 4,843,085 instances for 10,613 registered nouns and 196,098 instances for 2,556 acquired nouns.</S>
			<S sid ="156" ssid = "22">3 http://nlp.kuee.kyoto-u.ac.jp/ nl-resource/knp-e.html 4 http://code.google.com/p/bayon/ Table 2: Results of semantic classification.</S>
			<S sid ="157" ssid = "23">lea rni ng alg orit hm s a c q u ir e d n o u n s r e g i s t e r e d n o u n s Av era ge d Pe rce ptr on Pas sive Ag gre ssi ve Co nfi de nce We igh ted 86.</S>
			<S sid ="158" ssid = "24">40 % (43 2 / 50 0) 87.</S>
			<S sid ="159" ssid = "25">00 % (43 5 / 50 0) 85.</S>
			<S sid ="160" ssid = "26">20 % (42 6 / 50 0) 88.</S>
			<S sid ="161" ssid = "27">59 % (12 3,1 13 / 13 8,9 71) 91.</S>
			<S sid ="162" ssid = "28">68 % (12 7,4 07 / 13 8,9 71) 89.</S>
			<S sid ="163" ssid = "29">66 % (12 4,6 04 / 13 8,9 71) ba sel ine 1 69.</S>
			<S sid ="164" ssid = "30">60 % (34 8 / 50 0) 79.</S>
			<S sid ="165" ssid = "31">14 % (10 9,9 80 / 13 8,9 71) 1 assign OTH-C to all instances.</S>
			<S sid ="166" ssid = "32">Table 3: Examples of aggregated instances.</S>
			<S sid ="167" ssid = "33">a c q u i r e d n o u n s ins tan ce s l a b e l s ヒ カ ル (hik aru , a per son na me ) チ ワ ワ (ch iwa wa, “C hih ua hu a”) か み さ ん (ka mis aN, coll oq.</S>
			<S sid ="168" ssid = "34">“wi fe” ) ラ ス ベ ガ ス (ra sub ega su, “L as Ve ga s”) ア ッ プ ル (aQ pur u, “A ppl e/a ppl e”) メ ル マ ガ (m eru ma ga, ab br.</S>
			<S sid ="169" ssid = "35">of “m ail ma ga zin e”) 84 1 2 8 1 3 1 1 3 6 1 8 7 1 , 6 2 2 PSN P:5 8.3 3% , PSN C: 41.</S>
			<S sid ="170" ssid = "36">67 % ANI C: 54.</S>
			<S sid ="171" ssid = "37">69 %, OTH C: 45.</S>
			<S sid ="172" ssid = "38">31 % PSN C: 10 0% LOC P:9 7.0 6% , LOC C: 2.9 4% ORG P:6 3.1 0% , PSN C:3 4.7 6% , OTH C: 2.1 4% OTH C:9 9.3 2% , LOC C:0 .55 %, PSN C: 0.0 6% In order to handle polysemy, we evaluated semantic classification on an instance-by-instance basis.</S>
			<S sid ="173" ssid = "39">We randomly selected 500 instances from the test data and manually assigned the correct labels to them.</S>
			<S sid ="174" ssid = "40">For comparison purposes, we also classified registered nouns.</S>
			<S sid ="175" ssid = "41">We split the training data: 829 nouns or 138,971 instances for testing and the rest for training.</S>
			<S sid ="176" ssid = "42">We trained the model with three online learning algorithms, (1) the averaged version (Collins, 2002) of Perceptron (Crammer and Singer, 2003), (2) the Passive-Aggressive algorithm (Crammer et al., 2006), and (3) the Confidence-Weighted algorithm (Crammer et al., 2009).</S>
			<S sid ="177" ssid = "43">For Passive- Aggressive algorithm, we used PA-I and set parameter C to 1.</S>
			<S sid ="178" ssid = "44">For Confidence-Weighted, we used the single-constraint updates.</S>
			<S sid ="179" ssid = "45">All algorithms iterated five times through the training data.</S>
			<S sid ="180" ssid = "46">4.2 Results.</S>
			<S sid ="181" ssid = "47">Table 2 shows the results of semantic classification.</S>
			<S sid ="182" ssid = "48">All algorithms significantly improved over the baseline.</S>
			<S sid ="183" ssid = "49">As suggested by the gap in accuracy between acquired and registered nouns in the baseline method, the label distribution of the training data differed from that of the test data, but the decrease in accuracy was smaller than expected.</S>
			<S sid ="184" ssid = "50">The Passive-Aggressive algorithm performed best on both acquired and registered nouns.</S>
			<S sid ="185" ssid = "51">For the rest of this paper, we report the results of the Passive-Aggressive algorithm.</S>
			<S sid ="186" ssid = "52">Table 3 shows aggregated instances of some acquired nouns.</S>
			<S sid ="187" ssid = "53">Although classification sometimes failed, correct labels took the majority.</S>
			<S sid ="188" ssid = "54">How ever, it is noticeable that PSN-P was frequently misidentified as PSN-C while PSN-C was correctly identified.</S>
			<S sid ="189" ssid = "55">This phenomenon is clearly seen in the confusion matrix (Table 4).</S>
			<S sid ="190" ssid = "56">Half of PSN-P instances were misidentified as PSN-C but the percentage of errors in the opposite direction was just above 9%.</S>
			<S sid ="191" ssid = "57">We will investigate this in the next section.</S>
			<S sid ="192" ssid = "58">4.3 Discussion.</S>
			<S sid ="193" ssid = "59">Our interest is in determining what kinds of features are effective in semantic classification.</S>
			<S sid ="194" ssid = "60">We first performed standard ablation experiments.</S>
			<S sid ="195" ssid = "61">We trained a series of models on the training data after removing each feature set.</S>
			<S sid ="196" ssid = "62">The training and test data were the same with those in Section 4.1.</S>
			<S sid ="197" ssid = "63">Table 5 shows the results of ablation experiments.</S>
			<S sid ="198" ssid = "64">Significant decreases in accuracy are observed in the cf dataset.</S>
			<S sid ="199" ssid = "65">This is easily explained by the fact that more than half of features belonged to cf.</S>
			<S sid ="200" ssid = "66">The ratio of ncf1 was much the same with that of ncf2, but the removal of ncf1 resulted in a worse performance in classifying registered nouns than that of ncf2.</S>
			<S sid ="201" ssid = "67">This means that a modifiee of a noun explains more about the noun than its modifier.</S>
			<S sid ="202" ssid = "68">The ablation experiments cannot capture interesting properties of features because each feature set has a great diversity within it.</S>
			<S sid ="203" ssid = "69">Next, we directly examine features instead.</S>
			<S sid ="204" ssid = "70">Since we use a simple linear classifier, a feature has |Y | corresponding weights, each of which represents how likely a noun belongs to label y. For example, features whose weights for PSN-C are the largest Table 4: Confusion matrix of acquired nouns.</S>
			<S sid ="205" ssid = "71">A c t u a l PSN P LOC-P ORG-P OTH-P PSN C LOC-C ORG-C ANI-C OTH-C P r e d i c t e d P S N P L O C P O R G -P O T H P 1 6 1 4 4 1 1 P S N C L O C C O R G -C A NI C O T H C 1 6 2 2 1 3 1 1 39 1 2 10 4 2 2 8 1 13 9 338 Table 5: Results of ablation experiments.</S>
			<S sid ="206" ssid = "72">feat ure set r a ti o 1 a c q u ir e d n o u n s r e g i s t e r e d n o u n s cal l -cf de mo ncf 1 ncf 2 suf 0.</S>
			<S sid ="207" ssid = "73">2 3 % 54.</S>
			<S sid ="208" ssid = "74">84 % 2.</S>
			<S sid ="209" ssid = "75">4 0 % 19.</S>
			<S sid ="210" ssid = "76">03 % 18.</S>
			<S sid ="211" ssid = "77">40 % 5.</S>
			<S sid ="212" ssid = "78">1 0 % 87.</S>
			<S sid ="213" ssid = "79">60 % (43 8 / 50 0) 84.</S>
			<S sid ="214" ssid = "80">80 % (42 4 / 50 0) 88.</S>
			<S sid ="215" ssid = "81">00 % (44 0 / 50 0) 87.</S>
			<S sid ="216" ssid = "82">20 % (43 6 / 50 0) 85.</S>
			<S sid ="217" ssid = "83">60 % (42 8 / 50 0) 87.</S>
			<S sid ="218" ssid = "84">40 % (43 7 / 50 0) 91.</S>
			<S sid ="219" ssid = "85">58 % (12 7,2 76 / 13 8,9 71) 88.</S>
			<S sid ="220" ssid = "86">96 % (12 3,6 30 / 13 8,9 71) 91.</S>
			<S sid ="221" ssid = "87">38 % (12 6,9 96 / 13 8,9 71) 89.</S>
			<S sid ="222" ssid = "88">23 % (12 4,0 08 / 13 8,9 71) 91.</S>
			<S sid ="223" ssid = "89">54 % (12 7,2 20 / 13 8,9 71) 91.</S>
			<S sid ="224" ssid = "90">30 % (12 6,8 89 / 13 8,9 71) all 87.</S>
			<S sid ="225" ssid = "91">00 % (43 5 / 50 0) 91.</S>
			<S sid ="226" ssid = "92">68 % (12 7,4 07 / 13 8,9 71) 1 The proportion of each feature set that appears in the instances of the test.</S>
			<S sid ="227" ssid = "93">data.</S>
			<S sid ="228" ssid = "94">include: • cf:nakusu:wo (“ϕ lose X to the disease”), The modifiee of a numeral expression is not always the noun to be counted, as demonstrated by the following example: • cf:oshieru:ni (“ϕ1 teach X ϕ2”), (2) saN niN no moNdai • ncf2:ooku (“many/much X ”), and three CL GEN problem • nc f2: &lt;N UM &gt;ni N (X is m od ifi ed by &lt; N U M &gt; p l u s a nu m er al cla ssi fie r for persons).</S>
			<S sid ="229" ssid = "95">As briefly mentioned in Section 2.3, Japanese numeral quantifiers received scholarly attention in the fields of linguistic philosophy and linguistics in relation to the count/mass distinction (Quine, 1969; Gil, 1987).</S>
			<S sid ="230" ssid = "96">In our feature sets, numeral quantifiers typically appear as ncf2, e.g. “ncf2:&lt;NUM&gt;niN.” The weights given to matters among the three persons.</S>
			<S sid ="231" ssid = "97">From the above, the feature “ncf2:&lt;NUM&gt;niN” is extracted although “moNdai” is OTH-C.</S>
			<S sid ="232" ssid = "98">Theis “noise” is attributed to the genitive case marker “no” because it can denote a wide range of relations between two nouns.</S>
			<S sid ="233" ssid = "99">We might be able to avoid this problem if we focus on “floating” numeral quantifiers.</S>
			<S sid ="234" ssid = "100">A floating numeral quantifier has no direct dependency relation to the noun to be counted, as in them demonstrate their effectiveness in semantic (3) seito ga saN niN keQseki shita classification.</S>
			<S sid ="235" ssid = "101">They discriminate common nouns student NOM three CL absence do from proper nouns as the weights given to common nouns are larger with wide margins.</S>
			<S sid ="236" ssid = "102">It is not surprising because, say, the phrase “two Johns” is semantically acceptable but extremely rare in reality.</S>
			<S sid ="237" ssid = "103">They are also informative about the distinction among PSN, LOC and others.</S>
			<S sid ="238" ssid = "104">For example, the classifier “niN” for persons suggest the noun in question is a person while “keN” for houses would modify a location-like noun.</S>
			<S sid ="239" ssid = "105">However, we found quite a few “noises” about these features in data.</S>
			<S sid ="240" ssid = "106">three students were absent, where the numeral quantifier modifies the verb phrase instead of the noun.</S>
			<S sid ="241" ssid = "107">Further work is needed to anchor floating numeral quantifiers since they bring a different kind of ambiguity themselves (Bond et al., 1998).</S>
			<S sid ="242" ssid = "108">Closely related to numeral quantifiers are quantificational nouns that appear as “ncf2:ooku” (“many/much”), “ncf2:subete” (“all”) and others.</S>
			<S sid ="243" ssid = "109">They distinguish common nouns from proper nouns but does not make a further classification.</S>
			<S sid ="244" ssid = "110">The same is true of other numeral expressions such as “cf:hueru:ga” (“X increase in number”) and “cf:nai:ga” (“there is no X ” or “X do not exist”).</S>
			<S sid ="245" ssid = "111">We found that, other than numeral expressions, some features distinguished common nouns from proper nouns because they indicated the noun denoted an attribute.</S>
			<S sid ="246" ssid = "112">Such features include “cf:naru:ni” (“ϕ become X ”) and “cf:kaneru:wo” (“ϕ double as X ”).</S>
			<S sid ="247" ssid = "113">We expected that demonstratives (demo) served similar functions to quantificational expressions, but it turned out to be more complex.</S>
			<S sid ="248" ssid = "114">The distal demonstrative “ano” (“that”) often modifies proper nouns to give emphasis.</S>
			<S sid ="249" ssid = "115">In fact, the model gave larger weights to proper nouns.</S>
			<S sid ="250" ssid = "116">On the other hand, interrogative demonstratives such as “dono” (“which”) and “doNna” (“what kind of”) are rarely used with proper nouns although semantically acceptable.</S>
			<S sid ="251" ssid = "117">As seen above, there is an abundant variety of features that distinguish common nouns from proper nouns.</S>
			<S sid ="252" ssid = "118">Also, it is not difficult to make a distinction among PSN, LOC and others although the far largest cluster OTH-C sometimes absorbs other instances.</S>
			<S sid ="253" ssid = "119">The remaining question is how to distinguish proper nouns from common nouns, or specifically PSN-P from PSN-C.</S>
			<S sid ="254" ssid = "120">We examined features that gave larger weights to PSN-P than to PSN-C.</S>
			<S sid ="255" ssid = "121">They generally had smaller margins in weights than those which distinguish PSN-C from PSN-P.</S>
			<S sid ="256" ssid = "122">Among them, features such as “cf:utau:ga” (“X sing”) and “cf:hanasu:ni” (“ϕ talk to X ”) have no problem with being used for common nouns in terms of both semantics and pragmatics.</S>
			<S sid ="257" ssid = "123">They seem to have resulted from over-training.</S>
			<S sid ="258" ssid = "124">There were seemingly appropriate features such as “suf:saNchi” (“X ’s house”) and “suf:seNshu” (honorific suffix for players), but they were not ubiquitous in the corpus.</S>
			<S sid ="259" ssid = "125">PSN-P instances suffered from lack of distinctive features.</S>
			<S sid ="260" ssid = "126">One solution to this problem is to combine additional knowledge about person names.</S>
			<S sid ="261" ssid = "127">For example, a Japanese family name is followed by a given name, and most Chinese names consist of three Chinese characters.</S>
			<S sid ="262" ssid = "128">However, quite a few person names in the web corpus do not follow the usual patterns of person names because they are handles (or nicknames) and names for fictional characters.</S>
			<S sid ="263" ssid = "129">Thus it would be desirable to be able to classify person names without additional knowledge.</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "5">
			<S sid ="264" ssid = "1">In this paper, we presented the new task of semantic classification of Japanese nouns and applied it to nouns automatically acquired from text.</S>
			<S sid ="265" ssid = "2">Unlike in unknown morpheme identification in previous studies, we can exploit automatically parsed text.</S>
			<S sid ="266" ssid = "3">We explored lexico-syntactic clues and investigated their effects.</S>
			<S sid ="267" ssid = "4">We found plenty of features that distinguished common nouns from proper nouns, but few features worked in the opposite direction.</S>
			<S sid ="268" ssid = "5">Further work is needed to overcome this bias.</S>
	</SECTION>
</PAPER>
