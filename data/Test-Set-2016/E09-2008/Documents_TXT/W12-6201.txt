Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing, pages 1?9,
Donostia?San Sebastia?n, July 23?25, 2012. c?2012 Association for Computational Linguistics
Effect of Language and Error Models on Efficiency of Finite-State
Spell-Checking and Correction
Tommi A Pirinen
University of Helsinki
Department of Modern Languages
FI-00014 Univ. of Helsinki, PO box 24
tommi.pirinen@helsinki.fi
Sam Hardwick
University of Helsinki
Department of Modern Languages
FI-00014 Univ. of Helsinki, PO box 24
sam.hardwick@helsinki.fi
Abstract
We inspect the viability of finite-state spell-
checking and contextless correction of non-
word errors in three languages with a large de-
gree of morphological variety. Overviewing
previous work, we conduct large-scale tests
involving three languages ? English, Finnish
and Greenlandic ? and a variety of error mod-
els and algorithms, including proposed im-
provements of our own. Special reference
is made to on-line three-way composition of
the input, the error model and the language
model. Tests are run on real-world text ac-
quired from freely available sources. We show
that the finite-state approaches discussed are
sufficiently fast for high-quality correction,
even for Greenlandic which, due to its mor-
phological complexity, is a difficult task for
non-finite-state approaches.
1 Introduction
In most implementations of spell-checking, effi-
ciency is a limiting factor for selecting or discard-
ing spell-checking solutions. In the case of finite-
state spell-checking it is known that finite-state lan-
guage models can efficiently encode dictionaries of
natural languages (Beesley and Karttunen, 2003),
even for polysynthetic languages. Most contem-
porary spell-checking and correction systems are
still based on programmatic solutions (e.g. hun-
spell1, and its *spell relatives), or at most specialised
algorithms for implementing error-tolerant traver-
sal of the finite-state dictionaries (Oflazer, 1996;
Hulde?n, 2009a). There have also been few fully
1http://hunspell.sf.net
finite-state implementations that both detect and cor-
rect errors (Schulz and Mihov, 2002; Pirinen and
Linde?n, 2010). In this paper we further evaluate the
use of finite-state dictionaries with two-tape finite-
state automatons as a mechanism for correcting mis-
spellings, and optimisations to the finite-state error
models, intending to demonstrate that purely finite-
state algorithms can be made sufficiently efficient.
To evaluate the general usability and efficiency
of finite-state spell-checking we test a number of
possible implementations of such a system with
three languages of typologically different morpho-
logical features2 and reference implementations for
contemporary spell-checking applications: English
as a morphologically more isolating language with
essentially a word-list approach to spell-checking;
Finnish, whose computational complexity has been
just beyond the edge of being too hard to implement
nicely in eg. hunspell (Pitka?nen, 2006); and Green-
landic, a polysynthetic language which is imple-
mented as a finite-state system using Xerox?s orig-
inal finite-state morphology formalism (Beesley and
Karttunen, 2003). As a general purpose finite-state
library we use HFST3, which also contains our spell-
2We will not go into details regarding the morphological fea-
tures of these languages. We thank the anonymous reviewer for
guiding us to make a rough comparison using a piece of trans-
lated text. We observe from the translations of the Universal
Declaration of Human Rights (with pre-amble included) as fol-
lows: the number of word-like tokens for English is 1,746, for
Finnish 1,275 and for Greenlandic 1,063. The count of the 15
most frequent tokens are for English 120?28, for Finnish 85?
10 and for Greenlandic 38?7.
The average word length is 5.0 characters for English, 7.8
for Finnish and 14.9 for Greenlandic. For the complexity of
computational models refer to Table 2 in this article.
3http://hfst.sf.net
1
checking code.
As neither Finnish nor Greenlandic have been
successfully implemented in the hunspell formal-
ism, we mainly use them to evaluate how the com-
plexity of a language model affects the efficiency of
finite-state spell-checking. For a full-scale survey on
the state-of-the-art non-finite-state spell-checking,
refer to Mitton (2009).
The efficiency results are contrasted with the ex-
isting research on finite-state spell-checking in Has-
san et al (2008) and the theoretical results on finite-
state error-models in Mitankin (2005). Our contri-
bution primarily comprises the addition of morpho-
logically complex languages with actual cyclic dic-
tionary automata (i.e. infinite dictionaries formed
by compounding and recurring derivation) and more
complex structure in general, compared to those of
English and Arabic. Our goal is to demonstrate that
finite-state spelling is tractable for these complex
languages, to document their implications for per-
formance and to present an algorithm for the task.
We also point out that previous approaches have ne-
glected to simultaneously constrain the error model
and the dictionary with each other in on-line com-
position, which affords a significant speed benefit
compared to generating the two component compo-
sitions.
The rest of the paper is organised as follows. In
Section 2 we discuss the spell-checking task, current
non-finite-state spell-checkers and previously used
finite-state methods for spell-checking and correc-
tion and propose some possible speed optimisations
for the error models. We also investigate algorith-
mic limitations of finite-state approaches and ways
to remedy them. In Section 3 we present the lan-
guage models, error models and the testing corpora.
In Section 4 we present the comparisons of speed
and quality with combinations of different language
and error models and corpora for spell-checking. In
Section 5 we summarise our findings and results,
and outline future goals.
2 Methods
A finite-state spell-checker is typically (Pirinen and
Linde?n, 2010) composed of at least two finite-state
automata; one for the dictionary of the language, or
the language model, which contains valid strings of
the language, and one automaton to map misspelt
words into correct strings, or the error model. Both
the language model and the error model are usu-
ally (Pirinen and Linde?n, 2010) weighted finite-state
automata, where the weights represent the prob-
abilities are of a word being correctly spelled in
the language model and of specific misspellings,
respectively. We evaluate here the effect of both
the language and error model automatons? structure
and complexity on the efficiency of the finite-state
spelling task.4
2.1 Language Models
The most basic language model for a spell-checking
dictionary is a list of correctly spelled word forms.
One of the easiest ways of creating such a spell-
checker is to collect the word forms from a reason-
ably large corpus of (mostly) correctly spelt texts.
Additionally we can count the frequency of words
and use that as the likelihood, P (w) = c(w)?
w?D c(w)where c(w) is the count of the word w and D is the
set of corpus word forms. For morphologically more
isolating languages such as English, this is often a
sufficient approach (Norvig, 2010), and we use it to
create a dictionary for our English spell-checker as
well. As a non-finite-state reference point we use
hunspell.
For agglutinative languages like Finnish, for
which the word-list approach is likely to miss a
much greater number of words, one of the most
common approaches is to use right-linear gram-
mars, possibly combined with finite-state rule lan-
guages to implement morphophonological alter-
ations (Koskenniemi, 1983). This approach also ap-
plies to the newest available free / open source and
full-fledged finite-state Finnish morphological dic-
tionary we found (Pirinen, 2011). This language
model features productive derivations, compound-
ing and rudimentary probabilistic models. We take,
as a reference non-finite state language model for
Finnish, Voikko?s implementation in Malaga, which
is currently used as a spell-checking component
in open source software. It is implemented in a
4The methods introduced in this research as well as all ma-
terials are free/libre open source. Please see our svn repos-
itory https://hfst.svn.sf.net/svnroot/trunk/
fsmnlp-2012-spellers/ for detailed implementation
and scripts to reproduce all the results.
2
left-associative grammar formalism, which is a po-
tentially less efficient system with more expressive
power. It?s similar to finite-state formulations in
terms of linguistic coverage.
For polysynthetic languages it will be obvious
that the coverage of any word-list-based approach
will be even lower. Furthermore, most simple ex-
tensions to it such as affix stripping (as in hun-
spell) are not adequate for describing word forms.
To our knowledge, the only approaches that have
been widely used for spell-checking and morpho-
logical analysis of Greenlandic have been based
on traditional finite-state solutions, such as the Xe-
rox formalisms. In our case we have obtained
a freely available finite-state morphology imple-
mentation from the Internet5. For further de-
tails we refer to the authors? website http://
oqaaserpassualeriffik.org/.
2.2 Error Models
The ubiquitous formula for modeling typing er-
rors since computer-assisted spelling correction be-
gan has been the edit distance metric sometimes
attributed to Levenshtein (1966) and/or Damerau
(1964). It maps four typical slips of the fingers on
a keyboard to events in the fuzzy matching of mis-
spelt word forms to correct ones, that is, the deletion
of a character (i.e. failing to press a key), addition
of a character (i.e. hitting an extra key accidentally),
changing a character (i.e. hitting the wrong key) and
transposing adjacent characters (i.e. hitting two keys
in the wrong order).
When modeling edit distance as a finite-state au-
tomaton, a relatively simple two-tape automaton is
sufficient to implement the algorithm (Hassan et al,
2008). The automaton will consist of one arc for
each type of error, and additionally one state for
each transposition pair. This means that the trivial
nondetermistic finite-state automaton implementing
the algorithm is of space complexity S(V,E,?) =
O(|?|2|V | + |?|2|E|), where ? is the alphabet of
language, V is the set vertices in automaton and E is
the set of edges in automaton. This edit distance for-
mulation is roughly feature equivalent to hunspell?s
TRY mechanism.
5https://victorio.uit.no/langtech/trunk/
st/kal
To further fine-tune this finite-state formulation
of the edit distance algorithm, it is possible to at-
tach a probability to each of the error events as a
weight in a weighted finite-state automaton, corre-
sponding to the likelihood of an error, or a con-
fusion factor. This can be used to implement fea-
tures like keyboard adjacency or an OCR confusion
factor to the error correction model. This will not
modify the structure of the finite-state error mod-
els or the search space?which is why we did not
test their effects in this article?, but introduction of
non-homogenous weights to the resulting finite-state
network may have an effect on search time. This ad-
dition is equivalent to hunspell?s KEY mechanism.
For English language spelling correction there
is also an additional type of error model to deal
with competence-related misspellings?as opposed
to models that mainly deal with mistypings?
implemented in the form of phonemic folding and
unfolding. This type of error is very specific to cer-
tain types of English text and is not in the scope of
this experiment. This is the PHON part of the hun-
spell?s correction mechanism.
After fine-tuning the error models to reimplement
hunspell?s feature set, we propose variations of this
edit distance scheme to optimise the speed of er-
ror correction with little or no negative effect to the
quality of the correction suggestions. The time re-
quirement of the algorithm is determined by the size
of the search space, i.e. the complexity of the result-
ing network when the error model is applied to the
misspelt string and intersected with the dictionary6.
To optimise the application of edit distance by
limiting the search space, many traditional spell
checkers will not attempt to correct the very first let-
ter of the word form. We investigated whether this
decision is a particularly effective way to limit the
search space, but it does not appear to significantly
differ from restricting edits at any other position in
the input.
Dividing the states of a dictionary automaton into
6For non-finite-state solutions, the search space is simply
the number of possible strings given the error corrections made
in the algorithm. For finite-state systems the amount of gener-
ated strings with cyclic language and error models is infinite, so
complexity calculation are theoretically slightly more complex,
however for basic edit distance implementations used in this ar-
ticle the search space complexities are always the same and the
amount of suggestions generated finite
3
classes corresponding to the minimum number of
input symbols consumed by that state, we found
that the average ambiguity in a particular class is
somewhat higher for the first input symbols, but
then stabilises quickly at a lower level. This was
accomplished by performing the following state-
categorisation procedure:
1. The start state is assigned to class 0, and all
other states are assigned to a candidate pool.
2. All states to which there is an (input) epsilon
transition from the start state are assigned to
class 0 and removed from the candidate pool.
3. This is repeated for each state in class 0 until
no more states are added to class 0. This com-
pletes class 0 as the set of states in which the
automaton can be before consuming any input.
4. For each state in class 0, states in the candidate
pool to which there is a non-epsilon transition
are assigned to class 1 and removed from the
candidate pool.
5. Class 1 is epsilon-completed as in (2-3).
6. After the completion of class n, class n + 1
is constructed. This continues until the candi-
date pool is empty, which will happen as long
as there are no unreachable states.
With this categorisation, we tallied the total num-
ber of arcs from states in each class and divided the
total by the number of states in the class. This is
intended as an approximate measure of the ambigu-
ity present at a particular point in the input. Some
results are summarized in Table 1.
Class Transitions States Average
0 156 3 52
1 1,015 109 9.3
2 6,439 1,029 6.3
3 22,436 5,780 3.9
4 38,899 12,785 3.0
5 44,973 15,481 2.9
6 47,808 17,014 2.8
7 47,495 18,866 2.5
8 39,835 17,000 2.3
9 36,786 14,304 2.6
10 45,092 14,633 3.1
11 66,598 22,007 3.0
12 86,206 30,017 2.9
Table 1: State classification by minimum input consumed
for the Finnish dictionary
Further, the size of a dictionary automaton that is
restricted to have a particular symbol in a particular
position does not apparently depend on the choice
of position. This result was acquired by intersecting
eg. the automaton e.+ with the dictionary to restrict
the first position to have the symbol e, the automa-
ton .e.+ to restrict the second position, and so on.
The transducers acquired by this intersection vary in
size of the language, number of states and number of
transitions, but without any trend depending on the
position of the restriction. This is in line with the
rather obvious finding that the size of the restricted
dictionary in terms of number of strings is similarily
position-agnostic.
Presumably, the rationale is a belief that errors
predominately occur at other positions in the input.
As far as we know, the complete justification for this
belief remains to be made with a high-quality, hand-
checked error corpus.
On the error model side this optimisation has been
justified by findings where between 1.5 % and 15 %
of spelling errors happen in the first character of the
word, depending on the text type (Bhagat, 2007); the
1.5 % from a small corpus of academic texts (Yan-
nakoudakis and Fawthrop, 1983) and 15 % from dic-
tated corpora (Kukich, 1992). We also performed a
rudimentary classification of the errors in the small
error corpus of 333 entries from Pirinen et al (2012),
and found errors at the first position in 1.2 % of the
entries. Furthermore, we noticed that when evenly
splitting the word forms in three parts, 15 % of the
errors are in the first third of the word form, while
second has 47 % and third 38 %, which would be in
favor of discarding initial errors7.
A second form of optimisation that is used by
many traditional spell-checking systems is to apply
a lower order edit distance separately before trying
higher order ones. This is based on the assumption
that the vast majority of spelling errors will be of
lower order. In the original account of edit distance
for spell-checking, 80 % of the spelling errors were
found to be correctable with distance 1 (Pollock and
Zamora, 1984).
The third form of optimisation that we test is
omitting redundant corrections in error models of
higher order than one. Without such an optimisa-
7By crude classification we mean that all errors were forced
to one of the three classes at weight of one, e.g. a series of
three consecutive instances of the same letters was counted as
deletion at the first position.
4
tion, higher order error models will permit adding
and deleting the same character in succession at any
position, which is obviously futile work for error
correction. Performing the optimisation makes the
error model larger but reduces the search space, and
does not affect the quality of results.
2.3 Algorithms
The obvious baseline algorithm for the task of find-
ing which strings can be altered by the error model
in such a way that the alteration is present in the lan-
guage model is generating all the possible alterations
and checking which ones are present in the language
model. This was done in Hassan et al (2008) by first
calculating the composition of the input string with
the error model and then composing the result with
the language model.
If we simplify the error model to one in which
only substitutions occur, it can already be seen that
this method is quite sensitive to input length and al-
phabet size. The composition explores each combi-
nation of edit sites in the input string. If any number
of edits up to d can be made at positions in an input
string of length n, there are
d?
i=1
(n
i
)
ways to choose the edit site, and each site is subject
to a choice of |?|?1 edits (the entire alphabet except
for the actual input). This expression has no closed
form, but as d grows to n, the number of choices
has the form 2n, so the altogether complexity is ex-
ponential in input length and linear in alphabet size
(quadratic if transpositions are considered).
In practice (when d is small relative to n) it is use-
ful to observe that an increase of 1 in distance results
in an additional term to the aforementioned sum, the
ratio of which to the previously greatest term is
n!/(d! ? (n? d!))
n!/((d? 1)! ? (n? d + 1)!) =
n? d + 1
d
indicating that when d is small, increases in it pro-
duce an exponential increase in complexity. For
an English 26-letter lowercase alphabet, edit dis-
tance 2 and the 8-letter word ?spelling?, 700 strings
are stored in a transducer. With transpositions,
deletions, insertions and edit weights this grows to
100, 215 different outputs. We have implemented
this algorithm for our results by generating the
edited strings by lookup, and performing another
lookup with the language model on these strings.
Plainly, it would be desirable to improve on this.
The intuition behind our improvement is that when
editing an input string, say ?spellling?, it is a wasted
effort to explore the remainder after generating a
prefix that is not present in the lexicon. For example,
after changing the first character to ?z? and not edit-
ing the second characted, we have the prefix ?zp-?,
which does not occur in our English lexicon. So the
remaining possibilities - performing any edits on the
remaining 7-character word - can be ignored.
This is accomplished with a three-way composi-
tion in which the input, the error model and the lan-
guage model simultaneously constrain each other to
produce the legal correction set. This algorithm is
presented in some detail in Linde?n et al (2012). A
more advanced and general algorithm is due to Al-
lauzen and Mohri (2009).
3 Material
For language models we have acquired suitable free-
to-use dictionaries, readily obtainable on the Inter-
net.
We made our own implementations of the al-
gorithms to create and modify finite-state error
models. Our source repository contains a Python
script for generating error models and an extensive
Makefile for exercising it in various permuta-
tions.
To test the effect of correctness of the source
text to the speed of the spell-checker we have re-
trieved one of largest freely available open source
text materials from the Internet, i.e. Wikipedia. The
Wikipedia text is an appropriate real-world material
as it is a large body of text authored by many individ-
uals, and may be expected to contain a wide variety
of spelling errors. For material with more errors, we
have used a simple script to introduce (further, ar-
bitrary) errors at a uniform probability of 1/33 per
character; using this method we can also obtain a
corpus of errors with correct corrections along them.
Finally we have used a text corpus from a language
different than the one being spelled to ensure that the
majority of words are not in the vocabulary and (al-
5
most always) not correctable by standard error mod-
els.
The Wikipedia corpora were sourced from
wikimedia.org. For exact references, see our
previously mentioned repository. From the dumps
we extracted the contents of the articles and picked
the first 100,000 word tokens for evaluation.
In Table 2 we summarize the sizes of automata in
terms of structural elements. On the first row, we
give the size of the alphabet needed to represent the
entire dictionary. Next we give the sizes of automata
as nodes and arcs of the finite-state automaton en-
coding the dictionary. Finally we give the size of the
automaton as serialised on the hard disk. While this
is not the same amount of memory as its loaded data
structures, it gives some indication of memory usage
of the program while running the automaton in ques-
tion. As can be clearly seen from the table, the mor-
phologically less isolating languages do fairly con-
sistently have larger automata in every sense.
Automaton En Fi Kl
? set size 43 117 133
Dictionary FSM nodes 49,778 286,719 628,177
Dictionary FSM arcs 86,523 783,461 11,596,911
Dictionary FSM on disk 2.3 MiB 43 MiB 290 MiB
Table 2: The sizes of dictionaries as automata
In Table 3 we give the same figures for the sizes of
error models we?ve generated. The ? size row here
shows the number of symbols left when we have re-
moved the symbols that are usually not considered
to be a part of a spell-checking mechanism, such as
all punctuation that does not occur word-internally
and white-space characters8. Note that sizes of error
models can be directly computed from their parame-
ters; i.e., the distance, the ? set size and the optimi-
sation, so this table is provided for reference only.
4 Evaluation
We ran various combinations of language and error
models on the corpora described in section 3. We
give tabular results of the speed of the system and
the effect of the error model on recall. The latter
8The method described here does not handle run-on words
or extraneous spaces, as they introduce lot of programmatic
complexity which we believe is irrelevant to the results of this
experiment.
Automaton En Fi Kl
? set size 28 60 64
Edit distance 1 nodes 652 3,308 3,784
Edit distance 1 arcs 2,081 10,209 11,657
Edit distance 2 nodes 1,303 6,615 7,567
Edit distance 2 arcs 4136 20,360 23,252
No firsts ed 1 nodes 652 3,308 3,784
No firsts ed 1 arcs 2,107 10,267 11,719
No firsts ed 2 nodes 1,303 6,615 7,567
No firsts ed 2 arcs 4,162 20,418 23,314
No redundancy and 1st ed 2 nodes 1,303 6,615 7,567
No redundancy and 1st ed 2 arcs 4,162 20,418 23,314
Lower order first ed 1 to 2 arcs 6,217 30,569 34,909
Lower order first ed 1 to 2 nodes 1,955 9,923 11,351
Table 3: The sizes of error models as automata
is to establish that simpler error models lead to de-
graded recall?and not to more generally evaluate
the present system as a spell-checker.
The evaluations in this section are performed on
quad-core Intel Xeon E5450 running at 3 GHz with
64 GiB of RAM memory. The times are averaged
over five test runs of 10,000 words in a stable server
environment with no server processes or running
graphical interfaces or other uses. The test results
are measured using the getrusage C function on
a system that supports the maximum resident stack
size ru maxrss and user time ru utime fields.
The times are also verified with the GNU time
command. The results for hunspell, Voikkospell
and foma processes are only measured with time
and top. The respective versions of the soft-
ware are Voikkospell 3.3, hunspell 1.2.14, and Foma
0.9.16alpha. The reference systems are tested with
default settings, meaning that they will only give
some fixed number of suggestions whereas our sys-
tem will calculate all strings within the given error
model.
As a reference implementation for English we use
hunspell?s en-US dictionary9 and for a finite-state
implementation we use a weighted word-list from
Norvig (2010). As a Finnish reference implementa-
tion we use Voikko10, with a LAG-based dictionary
using Malaga11. The reference correction task for
Greenlandic is done with foma?s (Hulde?n, 2009b)
9http://wiki.services.openoffice.org/
wiki/Dictionaries
10http://voikko.sf.net
11http://home.arcor.de/bjoern-beutel/
malaga/
6
apply med function with default settings12.
The baseline feature set and the efficiency of
spell-checking we are targeting is defined by the cur-
rently de facto standard spelling suite in open source
systems, hunspell.
In Table 4 we measure the speed of the spell-
checking process on native language Wikipedia
text with real-world spelling errors and unknown
strings. The error model rows are defined as fol-
lows: on the Reference impl. row, we test the spell-
checking speed of the hunspell tool for English, and
Voikkospell tool for Finnish. On the edit distance 2
row we use the basic traditional edit distance 2 with-
out any modifications. On the No first edits row we
use the error model that does not modify the first
character of the word. On the No redundancy row
we use the edit distance 2 error model with the re-
dundant edit combinations removed. On the No re-
dundancy and firsts rows we use the combined er-
ror model of No first edits and No redundancy func-
tionalities. On the row Lower order first we apply a
lower order edit distance model first, then if no re-
sults are found, a higher order model is used. In the
tables and formulae we routinely use the language
codes to denote the languages: en for English, fi for
Finnish and kl for Greenlandic (Kalaallisut).
Error model En Fi Kl
Reference impl. 9.93 7.96 11.42
Generate all edits 2 3818.20 118775.60 36432.80
Edit distance 1 0.26 6.78 4.79
Edit distance 2 7.55 220.42 568.36
No first edits 1 0.44 3.19 3.52
No firsts ed 2 1.38 61.88 386.06
No redundancy ed 2 7.52 4230.94 6420.66
No redundancy and firsts ed 2 1.51 62.05 386.63
Lower order first ed 1 to 2 4.31 157.07 545.91
Table 4: Effect of language and error models to speed
(time in seconds per 10,000 word forms)
The results show that not editing the first posi-
tion does indeed give significant boost to the speed,
regardless of language model, which is of course
caused by the significant reduction in search space.
However, the redundancy avoidance does not seem
to make a significant difference. This is most likely
because the amount of duplicate paths in the search
space is not so proportionally large and their traver-
sal will be relatively fast. The separate application
12http://code.google.com/p/Foma/
of error models gives the expected timing result be-
tween its relevant primary and secondary error mod-
els. It should be noteworthy that, when thinking of
real world applications, the speed of the most of the
models described here is greater than 1 word per sec-
ond (i.e. 10,000 seconds per 10,000 words).
We measured memory consumption when per-
forming the same tests. Varying the error model had
little to no effect. Memory consumption was almost
entirely determined by the language model, giving
consumptions of 13-7 MiB for English, 0.2 GiB for
Finnish and 1.6 GiB for Greenlandic.
To measure the degradation of quality when us-
ing different error models we count the proportion
of suggestion sets that contain the correct correction
among the corrected strings. The suggestion sets are
the entire (unrestricted by number) results of correc-
tion, with no attempt to evaluate precision13. For
this test we use automatically generated corpus of
spelling errors to get the large-scale results.
Error model En Fi Kl
Edit distance 1 0.89 0.83 0.81
Edit distance 2 0.99 0.95 0.92
Edit distance 3 1.00 0.96 ?
No firsts ed 1 0.74 0.73 0.60
No firsts ed 2 0.81 0.82 0.69
No firsts ed 3 0.82 ? ?
Table 5: Effect of language and error models to quality
(recall, proportion of suggestion sets containing a cor-
rectly suggested word)
This test with automatically introduced errors
shows us that with uniformly distributed errors the
penalty of using an error model that ignores word-
initial corrections could be significant. This con-
trasts to our findings with real world errors, that the
distribution of errors tends towards the end of the
word, described in 2.2 and (Bhagat, 2007), but it
should be noted that degradation can be as bad as
given here.
Finally we measure how the text type used
will affect the speed of spell-checking. As the
best-case scenario we use the unmodified texts of
Wikipedia, which contain probably the most real-
istic native-language-speaker-like typing error dis-
13Which, in the absence of suitable error corpora and a more
full-fledged language model taking context into account, would
be irrelevant for the goal at hand.
7
tribution available. For text with more errors,
where the majority of errors should be recoverable,
we introduce automatically generated errors in the
Wikipedia texts. Finally to see the performance in
the worst case scenario where most of the words
have unrecoverable spelling errors we use texts
from other languages, in this case English texts for
Finnish and Greenlandic spell-checking and Finnish
texts for English spell-checking, which should bring
us close to the lower bounds on performance. The
effects of text type (i.e. frequency of non-words) on
speed of spell-checking is given in Table 6. All of
the tests in this category were performed with er-
ror models under the avoid redundancy and firsts
ed 2 row in previous tables, which gave us the best
speed/quality ratio.
Error model En Fi Kl
Native Lang. Corpus 1.38 61.88 386.06
Added automatic errors 6.91 95.01 551.81
Text in another language 22.40 148.86 783.64
Table 6: Effect of text type on error models to speed (in
seconds per 10,000 word-forms)
Here we chiefly note that the amount of non-
words in text directly reflects the speed of spell-
checking. This shows that the dominating factor of
the speed of spell-checking is indeed in the correct-
ing of misspelled words.
5 Conclusions and Future Work
In this article, we built a full-fledged finite-state
spell-checking system from existing finite-state lan-
guage models and generated error models. This
work uses the system initially described in Pirinen
and Linde?n (2010) and an algorithm described in
Linde?n et al (2012), providing an extensive quan-
titative evaluation of various combinations of con-
stituents for such a system, and applying it to the
most challenging linguistic environments available
for testing. We showed that using on-line composi-
tion of the word form, error model and dictionary is
usable for morphologically complex languages. Fur-
thermore we showed that the error models can be au-
tomatically optimised in several ways to gain some
speed at cost of recall.
We showed that the memory consumption of the
spell-checking process is mainly unaffected by the
selection of error model, apart from the need to store
a greater set of suggestions for models that generate
more suggestions. The error models may therefore
be quite freely changed in real world applications as
needed.
We verified that correcting only the first input let-
ter affords a significant speed improvement, but that
this improvement is not greatly dependent on the po-
sition of such a restriction. This practice is some-
what supported by our tentative finding that it may
cause the least drop in practical recall figures, at
least in Finnish. It is promising especially in con-
junction with a fallback model that does correct the
first letter.
We described a way to avoid having a finite-state
error model perform redundant work, such as delet-
ing and inserting the same letter in succession. The
practical improvement from doing this is extremely
modest, and it increases the size of the error model.
In this research we focused on differences in au-
tomatically generated error models and their optimi-
sations in the case of morphologically complex lan-
guages. For future research we intend to study more
realistic error models induced from actual error cor-
pora (e.g. Brill and Moore (2000)). Research into
different ways to induce weights into the language
models, as well as further use of context in finite-
state spell-checking (as in Pirinen et al (2012)), is
warranted.
Acknowledgements
We thank the anonymous reviewers for their com-
ments and the HFST research team for fruity discus-
sions on the article?s topics. The first author thanks
the people of Oqaaserpassualeriffik for introducing
the problems and possibilities of finite-state appli-
cations to the morphologically complex language of
Greenlandic.
References
Cyril Allauzen and Mehryar Mohri. 2009. N-way com-
position of weighted finite-state transducers. Interna-
tional Journal of Foundations of Computer Science,
20:613?627.
Kenneth R Beesley and Lauri Karttunen. 2003. Finite
State Morphology. CSLI publications.
8
Meenu Bhagat. 2007. Spelling error pattern analysis of
punjabi typed text. Master?s thesis, Thapar University.
Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling correction. In
ACL ?00: Proceedings of the 38th Annual Meeting
on Association for Computational Linguistics, pages
286?293, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Fred J Damerau. 1964. A technique for computer detec-
tion and correction of spelling errors. Commun. ACM,
(7).
Ahmed Hassan, Sara Noeman, and Hany Hassan. 2008.
Language independent text correction using finite state
automata. In Proceedings of the Third International
Joint Conference on Natural Language Processing,
volume 2, pages 913?918.
Ma?ns Hulde?n. 2009a. Fast approximate string match-
ing with finite automata. Procesamiento del Lenguaje
Natural, 43:57?64.
Ma?ns Hulde?n. 2009b. Foma: a finite-state compiler
and library. In Proceedings of the 12th Conference of
the European Chapter of the Association for Compu-
tational Linguistics: Demonstrations Session, EACL
?09, pages 29?32, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Kimmo Koskenniemi. 1983. Two-level Morphology: A
General Computational Model for Word-Form Recog-
nition and Production. Ph.D. thesis, University of
Helsinki.
Karen Kukich. 1992. Techniques for automatically cor-
recting words in text. ACM Comput. Surv., 24(4):377?
439.
Vladimir I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. Soviet
Physics?Doklady 10, 707710. Translated from Dok-
lady Akademii Nauk SSSR, pages 845?848.
Krister Linde?n, Erik Axelson, Senka Drobac, Sam Hard-
wick, Miikka Silfverberg, and Tommi A Pirinen.
2012. Using hfst for creating computational linguistic
applications. In Proceedings of Computational Lin-
guistics - Applications, 2012, page to appear.
Petar Nikolaev Mitankin. 2005. Universal levenshtein
automata. building and properties. Master?s thesis,
University of Sofia.
Roger Mitton. 2009. Ordering the suggestions of a
spellchecker without using context*. Nat. Lang. Eng.,
15(2):173?192.
Peter Norvig. 2010. How to write a spelling corrector.
referred 2011-01-11, available http://norvig.
com/spell-correct.html.
Kemal Oflazer. 1996. Error-tolerant finite-state recog-
nition with applications to morphological analysis and
spelling correction. Comput. Linguist., 22(1):73?89.
Tommi A Pirinen and Krister Linde?n. 2010. Finite-state
spell-checking with weighted language and error mod-
els. In Proceedings of the Seventh SaLTMiL workshop
on creation and use of basic lexical resources for less-
resourced languagages, pages 13?18, Valletta, Malta.
Tommi Pirinen, Miikka Silfverberg, and Krister Linden.
2012. Improving finite-state spell-checker suggestions
with part of speech n-grams. In Internatational Jour-
nal of Computational Linguistics and Applications IJ-
CLA (to appear).
Tommi A Pirinen. 2011. Modularisation of finnish
finite-state language descriptiontowards wide collab-
oration in open source development of morphological
analyser. In Proceedings of Nodalida, volume 18 of
NEALT proceedings.
Harri Pitka?nen. 2006. Hunspell-in kesa?koodi 2006: Fi-
nal report. Technical report.
Joseph J. Pollock and Antonio Zamora. 1984. Auto-
matic spelling correction in scientific and scholarly
text. Commun. ACM, 27(4):358?368, April.
Klaus Schulz and Stoyan Mihov. 2002. Fast string cor-
rection with levenshtein-automata. International Jour-
nal of Document Analysis and Recognition, 5:67?85.
Emmanuel J Yannakoudakis and D Fawthrop. 1983. An
intelligent spelling error corrector. Information Pro-
cessing and Management, 19(2):101?108.
9
