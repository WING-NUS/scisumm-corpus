<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">In this paper we propose a corpus-based approach to anaphora resolution combin­ ing a machine learning method and sta­ tistical information.</S>
		<S sid ="2" ssid = "2">First, a decision tree trained on an annotated corpus determines the coreference relation of a given anaphor and antecedent candidates and is utilized as a filter in order to reduce the num­ ber of potential candidates.</S>
		<S sid ="3" ssid = "3">In the sec­ ond step, preference selection is achieved by taking into account the frequency infor­ mation of coreferential and non-referential pairs tagged in the training corpus as well as distance features within the current dis­ course.</S>
		<S sid ="4" ssid = "4">Preliminary experiments concern­ ing the resolution of Japanese pronouns in spoken-language dialogs result in a success rate of 80.6%.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="5" ssid = "5">Coreference information IS relevant for numerous NLP systems.</S>
			<S sid ="6" ssid = "6">Our interest in anaphora resolu­ tion is based on the demand for machine translation systems to be able to translate (possibly omitted) anaphoric expressions in agreement with the mor­ phosyntactic characteristics of the referred object in order to prevent contextual misinterpretations.</S>
			<S sid ="7" ssid = "7">So far various approaches1 to anaphora resolution have been proposed.</S>
			<S sid ="8" ssid = "8">In this paper a machine learn­ ing approach (decision tree) is combined with a pref­ erence selection method based on the frequency in­ formation of non-/coreferential pairs tagged in the corpus as well as distance features within the cur­ rent discourse.</S>
			<S sid ="9" ssid = "9">The advantage of machine learning approaches is that they result in modular anaphora resolution sys­ tems automatically trainable from a corpus with no 1See section 4 for a more detailed comparison with related research.</S>
			<S sid ="10" ssid = "10">or only a minimal amount of human intervention.</S>
			<S sid ="11" ssid = "11">In the case of decision trees, we do have to provide in­ formation about possible antecedent indicators (syn­ tactic, semantic, and pragmatic features) contained in the corpus, but the relevance of features for the resolution task is extracted automatically from the training data.</S>
			<S sid ="12" ssid = "12">Machine learning approaches using decision trees proposed so far have focused on preference selection criteria directly derived from the decision tree re­ sults.</S>
			<S sid ="13" ssid = "13">The work described in (Conolly et al., 1994) utilized a decision tree capable of judging which one of two given anaphor-antecedent pairs is &quot;better&quot;.</S>
			<S sid ="14" ssid = "14">Due to the lack of a strong assumption on &quot;transi­ tivity&quot;, however, this sorting algorithm is more like a greedy heuristic search as it may be unable to find the &quot;best&quot; solution.</S>
			<S sid ="15" ssid = "15">The preference selection for a single antecedent in (Aone and Bennett, 1995) is based on the maximiza­ tion of confidence values returned from a pruned de­ cision tree for given anaphor-candidate pairs.</S>
			<S sid ="16" ssid = "16">How­ ever, decision trees are characterized by an indepen­ dent learning of specific features, i.e., relations be­ tween single attributes cannot be obtained automat­ ically.</S>
			<S sid ="17" ssid = "17">Accordingly, the use of dependency factors for preference selection during decision tree train­ ing requires that the artificially created attributes expressing these dependencies be defined.</S>
			<S sid ="18" ssid = "18">However, this not only extends human intervention into the automatic learning procedure (i.e., which dependen­ cies are important?), but can also result in some drawbacks on the contextual adaptation of prefer­ ence selection methods.</S>
			<S sid ="19" ssid = "19">The preference selection in our approach is based on the combination of statistical frequency informa­ tion and distance features in the discourse.</S>
			<S sid ="20" ssid = "20">There­ fore, our decision tree is not applied directly to the task of preference selection, but aims at the elimina­ tion of irrelevant candidates based on the knowledge obtained from the training data.</S>
			<S sid ="21" ssid = "21">The decision tree is trained on syntactic (lexi­ cal word attributes), semantic, and primitive dis­ course (distance, frequency) information and deter­ mines the coreferential relation between an anaphor and antecedent candidate in the given context.</S>
			<S sid ="22" ssid = "22">Irrel­ evant antecedent candidates are filtered out, achiev­ ing a noise reduction for the preference selection algorithm.</S>
			<S sid ="23" ssid = "23">A preference value is assigned to each potential anaphor-candidate pair depending on the proportion of non-/coreferential occurrences of the pair in the training corpus (frequency ratio) and the relative position of both elements in the discourse (distance).</S>
			<S sid ="24" ssid = "24">The candidate with the maximal pref­ erence value is resolved as the antecedent of the anaphoric expression.</S>
	</SECTION>
	<SECTION title="Corpus-Based Anaphora Resolution. " number = "2">
			<S sid ="25" ssid = "1">In this section we introduce a new approach to anaphora resolution based on coreferential proper­ ties automatically extracted from a training corpus.</S>
			<S sid ="26" ssid = "2">In the first step, the decison tree filter is trained on the linguistic, discourse and coreference information annotated in the training corpus which is described in section 2.1.</S>
			<S sid ="27" ssid = "3">Data Coreference Analysis Preference Selection----1 r-----------1 ---------- 1 I 1 coreffilter I I I 1 1 I I I preference I igiii (\ -·§i-iL2Ft:r i I I I N y I I I l l ------------ L----------J Figure 1: System outline The resolution system in Figure 1 applies the coreference filter (cf.</S>
			<S sid ="28" ssid = "4">section 2.2) to all anaphor­ candidate pairs (A; +C;j) found in the discourse his­ tory.</S>
			<S sid ="29" ssid = "5">The detection of anaphoric expressions is out of the scope of this paper and just reduced to tags in our annotated corpus.</S>
			<S sid ="30" ssid = "6">Antecedent candidates are identified according to noun phrase part-of-speech tags.</S>
			<S sid ="31" ssid = "7">The reduced set (A; + C;k) forms the input of the preference algorithm which selects the most salient candidate Cp as described in section 2.3.</S>
			<S sid ="32" ssid = "8">Preliminary experiments are conducted for the task of pronominal anaphora resolution and the per­ formance of our system is evaluated in section 3.</S>
			<S sid ="33" ssid = "9">2.1 Data Corpus.</S>
			<S sid ="34" ssid = "10">For our experiments we use the ATRITL Speech and Language Database (Takezawa et al., 1998) con­ sisting of 500 Japanese spoken-language dialogs an­ notated with coreferential tags.</S>
			<S sid ="35" ssid = "11">It includes nomi­ nal, pronominal, and ellipsis annotations, whereby the anaphoric expressions used in our experiments are limited to those referring to nominal antecedents (nominal: 2160, pronominal: 526, ellipsis: 3843).</S>
			<S sid ="36" ssid = "12">Besides the anaphor type, we also include mor­ phosyntactic information like stem form and inflec­ tion attributes for each surface word as well as se­ mantic codes for content words (Ohno and Haman­ ishi, 1981) in this corpus.</S>
			<S sid ="37" ssid = "13">rl: ;I;,IJ;O&lt;i::-?</S>
			<S sid ="38" ssid = "14">.:&quot;l.(••it&quot;o &apos;-&apos;71*&apos;r!v&quot;t&apos;.:&quot;l.(\,•JI:&quot;to [thank you very much] [City Hotel) &quot;Thank you for calling City Hotel.&quot; cl: Ll.., f/.ffi&lt;P&apos;ll.&apos;fi::l3••i-til&lt;, [hello) [I)[Hiroko Tanaka)[the name is) &quot;Hello, my name is Hiroko Tanaka.&quot;</S>
			<S sid ="39" ssid = "15">-t&quot; 1:, t,Q)*&apos;r !vQ)&apos;H9 l..f \,•A.,&quot;t&apos;Ti&gt;&apos;o [there) [hotel) [reservation][would like to have) &quot;I would like to make a reservation at your hotel.&quot; r2: :l5 jtQ)J5:tliiJQ);&lt;.</S>
			<S sid ="40" ssid = "16">JvalJiltit&quot;&quot;t&apos;l..J: -?i&gt;•o [yottr) [name] [spelling) [can I have] &quot;Can you spell your name for me, please?</S>
			<S sid ="41" ssid = "17">c2: li••o 7&quot;1-.:r.-.:r.y:.:r.-Jr.{.:r.-&quot;t&apos;t&quot;o [yes) [T) [A] [N] [A] [K) [A] [be] &apos;1t&apos;sTANAKA.&quot; r3: li••o +BI:::i:&gt; :&gt;I:.:&quot; J:fft••-?::c-c.&lt;.:&quot;l.(\,•;l:T o [yes] [tenth) [here] [arrival) [be) &quot;Okay, you will arrive here on the tenth, right?&quot;</S>
			<S sid ="42" ssid = "18">Figure 2: Example dialog In the example dialog between the hotel reception (r) and a customer (c) listed in Figure 2 the proper noun (r1)&quot; 71*7Jv (City Hotel]&quot; is tagged as the antecedent of the pronoun (cl) &quot;-f (there]&quot; as well as the noun (c1) &quot;*T Jv (hotel]&quot;.</S>
			<S sid ="43" ssid = "19">An exam­ ple for ellipsis is the ommitted subject (c2)&quot;0[it]&quot; referring to (r2)&quot;;l.«Jv (spelling]&quot;.</S>
			<S sid ="44" ssid = "20">According to the tagging guidelines used for our corpus an anaphoric tag refers to the most recent antecedent found in the dialog.</S>
			<S sid ="45" ssid = "21">However, this an­ tecedent might also refer to a previous one, e.g.</S>
			<S sid ="46" ssid = "22">(r3)&quot;:: (here]&quot;--&gt;(cl)&quot; -f[there]&quot;........</S>
			<S sid ="47" ssid = "23">(r1) &quot; 7 1 *7 Jv (City Hotel]&quot;.</S>
			<S sid ="48" ssid = "24">Thus, the transitive clo­ sure between the anaphora and the first mention of the antecedent in the discourse history defines the set of positive examples, e.g.</S>
			<S sid ="49" ssid = "25">( -f , 7 1 T Jv), whereas the nominal candidates outside the transi­ tive closure are considered negative examples, e.g.</S>
			<S sid ="50" ssid = "26">( -f , EB g:r), for coreferential relationships.</S>
			<S sid ="51" ssid = "27">Based on the corpus annotations we extract the frequency information of coreferential anaphor­ antecedent pairs and non-referential pairs from the training data.</S>
			<S sid ="52" ssid = "28">For each non-jcoreferential pair the occurrences of surface and stem form as well as se­ mantic code combinations are counted.</S>
			<S sid ="53" ssid = "29">Table 1: Frequency data type a.na.phor candidate freq+ freq- ratio word·word gr; 7&quot;1&apos;$7JV 6 0 1 i-J;t:, IB&lt;P 0 111 ::J;t:, -tB 0 00.1 word sem ::J;t:, lshop} 33 33 0 semsem {demonstra.hvesJ {shopJ 51 18 0.48 In Table 1 some examples are given for pronoun anaphora, whereas the expressions &quot;{...}&quot; denote semantic classes assigned to the respective words.</S>
			<S sid ="54" ssid = "30">The values freq+, freq- and ratio and their usage are described in more detailed in section 2.3.</S>
			<S sid ="55" ssid = "31">Moreover, each dialog is subdivided into utter­ ances consisting of one or more clauses.</S>
			<S sid ="56" ssid = "32">There­ fore, distance features are available on the utterance, clause, candidate, and morpheme levels.</S>
			<S sid ="57" ssid = "33">For exam­ ple, the distance values of the pronoun (r3)&quot;.::.</S>
			<S sid ="58" ssid = "34">&quot;S [here)&quot; and the antecedent (rl)&quot; &apos;;,!</S>
			<S sid ="59" ssid = "35">T 1 j- Jv [City Hotel]&quot; in our sample dialog in Figure 2 are dutter=4, dclause=1, dcand=14, dmorph=40.</S>
			<S sid ="60" ssid = "36">2.2 Coreference Analysis.</S>
			<S sid ="61" ssid = "37">To learn the coreference relations from our corpus An examination of our corpus gives rise to sus­ picion that similarities to references in our training data might be useful for the identification of those antecedents.</S>
			<S sid ="62" ssid = "38">Therefore, we propose a preference se­ lection scheme based on the combination of distance and frequency information.</S>
			<S sid ="63" ssid = "39">First, utilizing statistical information about the frequency of coreferential anaphor-antecedent pairs (freq+) and non-referential pairs (freq-) extracted from the training data, we define the ratio of a given reference pair as follows4 : . { -6 : (freq+ = freq- = 0) ratzo = 1re +q _ 1 q · otherwz.se we have chosen a C4.52 -like machine learning al­ gorithm without pruning.</S>
			<S sid ="64" ssid = "40">The training attributes consist of lexical word attributes (surface word, stem form, part-of-speech, semantic code, morphological attributes) applied to the anaphor, antecedent can­ didate, and clause predicate.</S>
			<S sid ="65" ssid = "41">In addition, features like attribute agreement, distance and frequency ra­ tio are checked for each anaphor-candidate pair.</S>
			<S sid ="66" ssid = "42">The decision tree result consists of only two classes de­ termining the coreference relation between the given anaphor-candidate pair.</S>
			<S sid ="67" ssid = "43">During anaphora resolution the decision tree is used as a module determining the coreferential prop­ erty of each anaphor-candidate pair.</S>
			<S sid ="68" ssid = "44">For each de­ tected anaphoric expression a candidate list3 is cre­ ated.</S>
			<S sid ="69" ssid = "45">The decision tree filter is then successively applied to all anaphor-candidate pairs.</S>
			<S sid ="70" ssid = "46">If the decision tree results in the non-reference class, the candidate is judged as irrelevant and elim­ inated from the list of potential antecedents forming the input of the preference selection algorithm.</S>
			<S sid ="71" ssid = "47">2.3 Preference Selection.</S>
			<S sid ="72" ssid = "48">The primary order of candidates is given by their word distance from the anaphoric expression.</S>
			<S sid ="73" ssid = "49">A straightforward preference strategy we could choose is the selection ofthe most recent candidate (MRC) as the antecedent, i.e., the first element of the can­ didate list.</S>
			<S sid ="74" ssid = "50">The success rate of this baseline test , however, is quite low as shown in section 3.</S>
			<S sid ="75" ssid = "51">But, this result does not mean that the recency factor is not important at aJl for the determination of saliency in this task.</S>
			<S sid ="76" ssid = "52">One reason for the bad per­ formance is the application of the baseline test to the unfiltered set of-candidates resulting in the frequent selection of non-referential antecedents.</S>
			<S sid ="77" ssid = "53">Addition­ ally, long-range references to candidates introduced first in the dialog are quite frequent in our data.</S>
			<S sid ="78" ssid = "54">2 cf.</S>
			<S sid ="79" ssid = "55">(Quinlan, 1993) 3A list of noun phrase candidates preceding the anaphor element in the current discourse.</S>
			<S sid ="80" ssid = "56">jreq+ + jreq · The value of ratio is in the range of [-1, +1], whereby ratio = -1 in the case of exclusive non­ referential relations and ratio = +1 in the case of exclusive coreferential relationships.</S>
			<S sid ="81" ssid = "57">In order for ref­ erential pairs occurring in the training corpus with ratio = 0 to be preferred to those without frequency information, we slightly decrease the ratio value of the latter ones by a factor 6.</S>
			<S sid ="82" ssid = "58">As mentioned above the distance plays a crucial role in our selection method, too.</S>
			<S sid ="83" ssid = "59">We define a pref­ erence value pref by normalizing the ratio value ac­ cording to the distance dist given by the primary order of the candidates in the discourse.</S>
			<S sid ="84" ssid = "60">ratio pre I = dist The pre f value is calculated for each candidate and the precedence ordered list of candidates is resorted towards the maximization of the preference factor.</S>
			<S sid ="85" ssid = "61">Similarly to the baseline test, the first element of the preferenced candidate list is chosen as the an­ tecedent.</S>
			<S sid ="86" ssid = "62">The precedence order between candidates of the same confidence continues to remain so and thus a final decision is made in the case of a draw.</S>
			<S sid ="87" ssid = "63">The robustness of our approach is ensured by the definition of a backup strategy which ultimately se­ lects one candidate occurring in the history in the case that all antecedent candidates are rejected by the decision tree filter.</S>
			<S sid ="88" ssid = "64">For our experiments reported in section 3 we adopted the selection·of the dialog­ initial candidate as the backup strategy.</S>
	</SECTION>
	<SECTION title="Evaluation. " number = "3">
			<S sid ="89" ssid = "1">For the evaluation of the experimental results de­ scribed in this section we use F-measure metrics cal­ culated by the recall and precision of the system per­ formance.</S>
			<S sid ="90" ssid = "2">Let l::t denote the total number of tagged 4 In order to keep the formula simple the frequency types are omitted (cf.</S>
			<S sid ="91" ssid = "3">Table 1) anaphor-antecedent pairs contained in the test data, Lt the number of these pairs passing the decision tree filter, and Lc the number of correctly selected antecedents.</S>
			<S sid ="92" ssid = "4">During evaluation we distinguish three classes: whether the correct antecedent is the first element of the candidate list (f), is in the candidate list (i), or is filtered out by the decision tree (o).</S>
			<S sid ="93" ssid = "5">The metrics F, recall (R) and precision (P) are defined as follows: L:=lfl ......................................................</S>
			<S sid ="94" ssid = "6">F=2xPxR P+R L:=l!l+lil J L:=l!l+lil+lol 40.0L___ [ JL_ L_ 400_, ---&apos; 100 200 300 t r a i n i n g s i z e ( d i a l o g ) In order to prove the feasibility of our approach we compare the four preference selection methods listed in Figure 3.</S>
			<S sid ="95" ssid = "7">tagged corpus Figure 3: Preference selection experiments First, the baseline test MRC selects the most re­ cent candidate as the antecedent of an anaphoric ex­ pression.</S>
			<S sid ="96" ssid = "8">The necessity of the filter and preference selection components is shown by comparing the de­ cision tree filter scheme DT (i.e., select the first el­ ement of the filtered candidate list) and preference scheme PREF (i.e., resort the complete candidate list) against our combined method DT+PREF (i.e., resort the filtered candidate list).</S>
			<S sid ="97" ssid = "9">5-way cross-validation experiments are conducted for pronominal anaphora resolution.</S>
			<S sid ="98" ssid = "10">The selected antecedents are checked against the annotated cor­ rect antecedents according to their morphosyntactic and semantic attributes.</S>
			<S sid ="99" ssid = "11">3.1 Training Size.</S>
			<S sid ="100" ssid = "12">We use varied numbers of training dialogs (50400) for the training of the decision tree and the extrac­ tion of the frequency information from the corpus.</S>
			<S sid ="101" ssid = "13">Open tests are conducted on 100 non-training dialogs whereas closed tests use the training data for evalua­ tion.</S>
			<S sid ="102" ssid = "14">The results of the different preference selection methods are shown in Figure 4.</S>
			<S sid ="103" ssid = "15">The baseline test MRC succeeds in resolving only 43.9% of the most recent candidates correctly as the antecedent.</S>
			<S sid ="104" ssid = "16">The best F-measure rate for DT is 65.0% and for PREF the best rate is 78.1% whereas Figure 4: Training size versus performance the combination of both methods achieves a success rate of 80.6%.</S>
			<S sid ="105" ssid = "17">The PREF method seems to reach a plateau at around 300 dialogs which is borne out by the closed test reaching a maximum of 81.1%.</S>
			<S sid ="106" ssid = "18">Comparing the recall rate of DT (61.2%) and DT+PREF (75.9%) with the PREF result, we might conclude that the decision tree is not much of a help due to the side­ effect of 11.8% of the correct antecedents being fil­ tered out.</S>
			<S sid ="107" ssid = "19">However, in contrast to the PREF algorithm, the DT method improves continuously according to the training size implying a lack of training data for the identification of potential candidates.</S>
			<S sid ="108" ssid = "20">Despite the sparse data the filtering method proves to be very effective.</S>
			<S sid ="109" ssid = "21">The average number of all candidates (his­ tory) for a given anaphor in our open data is 39 can­ didates which is reduced to 11 potential candidates by the decision tree filter resulting in a reduction rate of 71.8% (closed test: 81%).</S>
			<S sid ="110" ssid = "22">The number of trivial selection cases (only one candidate) increases from 2.7% (history) to 11.4% (filter; closed test: 21%).</S>
			<S sid ="111" ssid = "23">On average, two candidates are skipped in the his­ tory to select the correct antecedent.</S>
			<S sid ="112" ssid = "24">Moreover, the precision rates of DT (69.4%) and DT+PREF (86.0%) show that the utilization of the decision tree filter in combination with the statisti­ cal preference selection gains a relative improvement of 9% towards the preference and 16% towards the filter method.</S>
			<S sid ="113" ssid = "25">Additionally, the system proves to be quite robust, because the decision tree filters out all candidates in only 1% of the open test samples.</S>
			<S sid ="114" ssid = "26">Selecting the candidate first introduced in the dialog as a backup strategy shows the best performance due to the fre­ quent dialog initial references contained in our data.</S>
			<S sid ="115" ssid = "27">Table 2: Frequency and distance dependency DT DT-no-dist DT-no-freq DT+PREF DT+PREF-no-dist recall precision F-measure 61.2 69.4 65.0 60.1 68.7 64.1 53.6 64.5 58.5 75.9 86.0 80.6 73.0 82.8 77.6 (filtered-out) 11.8 12.5 16.9 11.8 11.8 3.2 Feature Dependency.</S>
			<S sid ="116" ssid = "28">In our approach frequency ratio and distance infor­ mation plays a crucial role not only for the identi­ fication of potential candidates during decision tree filtering, but also for the calculation of the prefer­ ence value for each antecedent candidate.</S>
			<S sid ="117" ssid = "29">In the first case these features are used indepen­ dently to characterize the training samples whereas the preference selection method is based on the de­ pendency between the frequency and distance values of the given anaphor-candidate pair in the context of the respective discourse.</S>
			<S sid ="118" ssid = "30">The relative importance of each factor is shown in Table 2.</S>
			<S sid ="119" ssid = "31">First, we compare our decision tree filter DT to those methods that do not use either frequency (DT­ no-freq) or distance ( D T -no-dist) information.</S>
			<S sid ="120" ssid = "32">Fre­ quency information does appear to be more relevant for the identification of potential candidates than distance features extracted from the training corpus.</S>
			<S sid ="121" ssid = "33">The recall performance of DT-no-freq decreases by 7.6% whereas DT-no-dist is only 1.1% below the re­ sult of the original DT filter5 . Moreover, the number of correct antecedents not passing the filter increases by 5.1% (DT-no-freq) and 0.7% (DT-no-dist).</S>
			<S sid ="122" ssid = "34">However, the distance factor proves to be quite important as a preference criterion.</S>
			<S sid ="123" ssid = "35">Relying only on the frequency ratio as the preference value, the re­ call performance of DT+PREF-no-dist is only 73.0%, down 2.9% of the original DT+PREF method.</S>
			<S sid ="124" ssid = "36">The effectiveness of our approach is not only based on the usage of single antecedent indicators ex­ tracted from the corpus, but also on the combination of these features for the selection of the most prefer­ able candidate in the context of the given discourse.</S>
	</SECTION>
	<SECTION title="Related Research. " number = "4">
			<S sid ="125" ssid = "1">Due to the characteristics of the underlying data used in these experiments a comparison involving absolute numbers to previous approaches gives us less evidence.</S>
			<S sid ="126" ssid = "2">However, the difficulty of our task can be verified according to the baseline experiment 5So far we have considered the decision tree filter just as a black-box tool.</S>
			<S sid ="127" ssid = "3">Further investigations on tree struc­ tures, however, should give us more evidence about the relative importance of the respective features.</S>
			<S sid ="128" ssid = "4">However, the difficulty of our task can be verified according to the baseline experiment results reported in (Mitkov, 1998).</S>
			<S sid ="129" ssid = "5">Resolving pro­ nouns in English technical manuals to the most re­ cent candidate achieved a success rate of 62.5%, whereas in our experiments only 43.9% of the most recent candidates are resolved correctly as the an­ tecedent (cf.</S>
			<S sid ="130" ssid = "6">section 3).</S>
			<S sid ="131" ssid = "7">Whereas knowledge-based systems like (Carbonell and Brown, 1988) and (Rich and LuperFoy, 1988) combining multiple resolution strategies are expen­ sive in the cost of human effort at development time and limited ability to scale to new domains, more re­ cent knowledge-poor approaches like (Kennedy and Boguraev, 1996) and (Mitkov, 1998) address the problem without sophisticated linguistic knowledge.</S>
			<S sid ="132" ssid = "8">Similarly to them we do not use any sentence parsing or structural analysis, but just rely on morphosyn­ tactic and semantic word information.</S>
			<S sid ="133" ssid = "9">Moreover, clues are used about the grammatical and pragmatic functions of expressions as in (Grosz et al., 1995), (Strube, 1998), ·or (Azzam et al., 1998) as well as rule-based empirical approaches like (Nakaiwa and Shirai, 1996) or (Murata and Nagao, 1997), to determine the most salient referent.</S>
			<S sid ="134" ssid = "10">These kinds of manually defined scoring heuristics, how­ ever, involve quite an amount of human intervention which is avoided in machine learning approaches.</S>
			<S sid ="135" ssid = "11">As briefly noted in section 1, the work described in (Conolly et al., 1994) and (Aone and Bennett, 1995) differs from our approach according to the us­ age of the decision tree in the resolution task.</S>
			<S sid ="136" ssid = "12">In (Conolly et al., 1994) a decision tree is trained on a small number of 15 features concerning anaphor type, grammatical function, recency, morphosyntac­ tic agreement and subsuming concepts.</S>
			<S sid ="137" ssid = "13">Given two anaphor-candidate pairs the system judges which is &quot;better&quot;.</S>
			<S sid ="138" ssid = "14">However, due to the lack of a strong assumption on &quot;transitivity&quot; this sorting algorithm may be unable to find the &quot;best&quot; solution.</S>
			<S sid ="139" ssid = "15">Based on discourse markers extracted from lexical, syntactic, and semantic processing, the approach of (Aone and Bennett, 1995) uses 66 unary and bi­ nary attributes (lexical, syntactic, semantic, posi­ tion, matching category, topic) during decision tree training.</S>
			<S sid ="140" ssid = "16">The confidence values returned from the pruned decision tree are utilized as a saliency mea­ sure for each anaphor-candidate pair in order to se lect a single antecedent.</S>
			<S sid ="141" ssid = "17">However, we use depen­ dency factors for preference selection which cannot be learned automatically because of the indepen­ dent learning of specific features during decision tree training.</S>
			<S sid ="142" ssid = "18">Therefore, our decision tree is not applied directly to the task of preference selection, but only used as a filter to reduce the number of potential candidates for preference selection.</S>
			<S sid ="143" ssid = "19">In addition to salience preference, a statistically modeled lexical preference is exploited in (Dagan et al., 1995) by comparing the conditional probabili­ ties of co-occurrence patterns given the occurrence of candidates.</S>
			<S sid ="144" ssid = "20">Experiments, however, are carried out on computer manual texts with mainly intra­ sentential references.</S>
			<S sid ="145" ssid = "21">This kind of data is also char­ acterized by the avoidance of disambiguities and only short discourse units, which prohibits almost any long-range references.</S>
			<S sid ="146" ssid = "22">In contrast to this re­ search, our results show that the distance factor in addition to corpus-based frequency information is quite relevant for the selection of the most salient candidate in our task.</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "5">
			<S sid ="147" ssid = "1">In this paper we proposed a corpus-based anaphora resolution method combining an automatic learning algorithm for coreferential relationships with statis­ tical preference selection in the discourse context.</S>
			<S sid ="148" ssid = "2">We proved the applicability of our approach to pro­ noun resolution achieving a resolution accuracy of 86.0% (precision) and 75.9% (recall) for Japanese pronouns despite the limitation of sparse data.</S>
			<S sid ="149" ssid = "3">Im­ provements in these results can be expected by in­ creasing the training data as well as utilizing more sophisticated linguistic knowledge (structural anal­ ysis of utterances, etc.) and discourse information (extra-sentential knowledge, etc.) which should lead to a rise of the decision tree filter performance.</S>
			<S sid ="150" ssid = "4">Preliminary experiments with nominal reference and ellipsis resolution showed promising results, too.</S>
			<S sid ="151" ssid = "5">We plan to incorporate this approach in multi­ lingual machine translation which enables us to han­ dle a variety of referential relations in order to im­ prove the translation quality.</S>
	</SECTION>
	<SECTION title="Acknowledgement">
			<S sid ="152" ssid = "6">We would like to thank Hitoshi Nishimura (ATR) for his programming support and Hideki Tanaka (ATR) for helpful personal communications.</S>
	</SECTION>
</PAPER>
