<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">In this paper, we study the impact of a group of features extracted automatically from machine-generated parse trees on coreference resolution.</S>
		<S sid ="2" ssid = "2">One focus is on designing syntactic features using the binding theory as the guideline to improve pronoun resolution, although linguistic phenomenon such as apposition is also modeled.</S>
		<S sid ="3" ssid = "3">These features are applied to the Arabic, Chinese and English coreference resolution systems and their effectiveness is evaluated on data from the Automatic Content Extraction (ACE) task.</S>
		<S sid ="4" ssid = "4">The syntactic features improve the Arabic and English systems significantly, but play a limited role in the Chinese one.</S>
		<S sid ="5" ssid = "5">Detailed analyses are done to understand the syntactic features’ impact on the three coreference systems.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="6" ssid = "6">A coreference resolution system aims to group together mentions referring to the same entity, where a mention is an instance of reference to an object, and the collection of mentions referring to the same object in a document form an entity.</S>
			<S sid ="7" ssid = "7">In the following example: (I) “John believes himself to be the best student.” mentions are underlined.</S>
			<S sid ="8" ssid = "8">The three mentions “John”, “himself ”, “the best student” are of type name, pronoun 1 , and nominal, respectively.</S>
			<S sid ="9" ssid = "9">They form an entity since they all refer to the same person.</S>
			<S sid ="10" ssid = "10">Syntactic information plays an important role in coreference resolution.</S>
			<S sid ="11" ssid = "11">For example, the binding theory (Haege- man, 1994; Beatrice and Kroch, 2000) provides a good account of the constraints on the antecedent of English pronouns.</S>
			<S sid ="12" ssid = "12">The theory relies on syntactic parse trees to determine the governing category which defines the scope 1 “Pronoun” in this paper refers to both anaphor and normal pronoun.</S>
			<S sid ="13" ssid = "13">of binding constraints.</S>
			<S sid ="14" ssid = "14">We will use the theory as a guideline to help us design features in a machine learning framework.</S>
			<S sid ="15" ssid = "15">Previous pronoun resolution work (Hobbs, 1976; Lappin and Leass, 1994; Ge et al., 1998; Stuckardt, 2001) explicitly utilized syntactic information before.</S>
			<S sid ="16" ssid = "16">But there are unique challenges in this study: (1) Syntactic information is extracted from parse trees automatically generated.</S>
			<S sid ="17" ssid = "17">This is possible because of the availability of statistical parsers, which can be trained on human-annotated tree- banks (Marcus et al., 1993; Xia et al., 2000; Maamouri and Bies, 2004) for multiple languages; (2) The binding theory is used as a guideline and syntactic structures are encoded as features in a maximum entropy coreference system; (3) The syntactic features are evaluated on three languages: Arabic, Chinese and English (one goal is to see if features motivated by the English language can help coreference resolution in other languages).</S>
			<S sid ="18" ssid = "18">All con- trastive experiments are done on publicly-available data; (4) Our coreference system resolves coreferential relationships among all the annotated mentions, not just for pronouns.</S>
			<S sid ="19" ssid = "19">Using machine-generated parse trees eliminates the need of hand-labeled trees in a coreference system.</S>
			<S sid ="20" ssid = "20">However, it is a major challenge to extract useful information from these noisy parse trees.</S>
			<S sid ="21" ssid = "21">Our approach is encoding the structures contained in a parse tree into a set of computable features, each of which is associated with a weight automatically determined by a machine learning algorithm.</S>
			<S sid ="22" ssid = "22">This contrasts with the approach of extracting rules and assigning weights to these rules by hand (Lap- pin and Leass, 1994; Stuckardt, 2001).</S>
			<S sid ="23" ssid = "23">The advantage of our approach is robustness: if a particular structure is helpful, it will be assigned a high weight; if a feature is extracted from a highly noisy parse tree and is not informative in coreference resolution, it will be assigned a small weight.</S>
			<S sid ="24" ssid = "24">By avoiding writing rules, we automatically incorporate useful information into our model and at the same time limit the potentially negative impact from noisy parsing output.</S>
			<S sid ="25" ssid = "25">660 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP), pages 660–667, Vancouver, October 2005.</S>
			<S sid ="26" ssid = "26">Qc 2005 Association for Computational Linguistics</S>
	</SECTION>
	<SECTION title="Statistical Coreference  Resolution Model. " number = "2">
			<S sid ="27" ssid = "1">Our coreference system uses a binary entity-mention NP1 S (GC) VP V NP2 NP1 S (GC) VP V NP2 model PL (·|e, m) (henceforth “link model”) to score the (Sub) (gov) (Sub) (gov) action of linking a mention m to an entity e. In our implementation, the link model is computed as John likes (1) himself.</S>
			<S sid ="28" ssid = "2">S John likes (2) him.</S>
			<S sid ="29" ssid = "3">PL (L = 1|e, m) ≈ max Pˆ mt ∈e (L = 1|e, mt, m), (1) VP NP2 (Sub) NP6 (GC) NP5 PP where mt is one mention in entity e, and the basic model NP1 V NP3 (gov) P NP4 building block Pˆ (L = 1|e, mt, m) is an exponential or John believes Miss Smith’s description of herself . maximum entropy model (Berger et al., 1996): (3) exp L (L|e, mt, m) = i λi gi(e, mt, m, L) Z (e, mt, m) , (2) Figure 1: GC examples.</S>
			<S sid ="30" ssid = "4">where Z (e, mt, m) is a normalizing factor to ensure that ˆ (·|e, mt, m) is a probability, {gi(e, mt, m, L)} are features and {λi} are feature weights.</S>
			<S sid ="31" ssid = "5">Another start model is used to score the action of creating a new entity with the current mention m. Since starting a new entity depends on all the partial entities created in 3.1 Features Inspired by Binding Theory.</S>
			<S sid ="32" ssid = "6">The binding theory (Haegeman, 1994) concerning pronouns can be summarized with the following principles: 1.</S>
			<S sid ="33" ssid = "7">A reflexive or reciprocal pronoun (e.g., “herself ” or“each other”) must be bound in its governing cate the history {ei}t , we use the following approximation: gory (GC).</S>
			<S sid ="34" ssid = "8">2.</S>
			<S sid ="35" ssid = "9">A normal pronoun must be free in its governing cat PS (S = 1|e1, e2, · · · , et, m) ≈ 1 − max PL (L = 1|ei, m) (3) 1≤i≤t In the maximum-entropy model (2), feature (typically binary) functions {gi(e, mt, m, ·)} provide us with a flex ible framework to encode useful information into the the system: it can be as simple as “gi(e, mt, m, L = 1) = 1 if mt and m have the same surface string,” or“gj (e, mt, m, L = 0) = 1 if e and m differ in num ber,” or as complex as “gl(e, mt, m, L = 1) = 1 if mt c-commands m and mt is a NAME mention and m is a pronoun mention.” These feature functions bear similarity to rules used in other coreference systems (Lappin and Leass, 1994; Mitkov, 1998; Stuckardt, 2001), except that the feature weights {λi} are automatically trained over a corpus with coreference information.</S>
			<S sid ="36" ssid = "10">Learning feature weights automatically eliminates the need of manually assigning the weights or precedence of rules, and opens the door for us to explore rich features extracted from parse trees, which is discussed in the next section.</S>
	</SECTION>
	<SECTION title="Syntactic Features. " number = "3">
			<S sid ="37" ssid = "1">In this section, we present a set of features extracted from syntactic parse trees.</S>
			<S sid ="38" ssid = "2">We discuss how we approximately compute linguistic concepts such as governing category (Haegeman, 1994), apposition and dependency relationships from noisy syntactic parse trees.</S>
			<S sid ="39" ssid = "3">While parsing and parse trees depend on the target language, the automatic nature of feature extraction from parse trees egory.</S>
			<S sid ="40" ssid = "4">The first principle states that the antecedent of a reflexive or reciprocal pronoun is within its GC, while the second principle says that the antecedent of a normal pronoun is outside its GC.</S>
			<S sid ="41" ssid = "5">While the two principles are simple, they all rely on the concept of governing category, which is defined as the minimal domain containing the pronoun in question, its governor, and an accessible subject.</S>
			<S sid ="42" ssid = "6">The concept GC can best be explained with a few examples in Figure 1, where the label of a head constituent is marked within a box, and GC, accessible subject, and governor constituents are marked in parentheses with “GC”, “Sub” and “gov.” Noun-phrases (NP) are numbered for the convenience of referencing.</S>
			<S sid ="43" ssid = "7">For example, in sub-figure (1) of Figure 1, the governor of “himself ” is “likes,” the subject is “John,” hence the GC is the entire sentence spanned by the root “S.” Since “himself ” is reflexive, its antecedent must be “John” by Principle 1.</S>
			<S sid ="44" ssid = "8">The parse tree in sub-figure (2) is the same as that.</S>
			<S sid ="45" ssid = "9">in sub-figure (1), but since “him” is a normal pronoun, its antecedent, according to Principle 2, has to be outside the GC, that is, “him” cannot be coreferenced with “John.”.</S>
			<S sid ="46" ssid = "10">Sentence in sub-figure (3) is slightly more complicated: the governor of “herself ” is “description,” and the accessible subject is “Miss Smith.” Thus, the governing category is NP6.</S>
			<S sid ="47" ssid = "11">The first principle implies that the antecedent of “herself ” must be “Miss Smith.” It is clear from these examples that GC is very useful in finding the antecedent of a pronoun.</S>
			<S sid ="48" ssid = "12">But the last example shows that determining GC is not a trivial matter.</S>
			<S sid ="49" ssid = "13">Not only is the correct parse tree required, but extra in and the minimal constituent dominating the pronoun, its governor and an accessible subject.</S>
			<S sid ="50" ssid = "14">Determining the accessible subject itself entails checking other constraints such as number and gender agreement.</S>
			<S sid ="51" ssid = "15">The complexity of computing governing category, compounded with the noisy nature of machine-generated parse tree, prompts us to compute a set of features that characterize the structural relationship between a candidate mention and a pronoun, as opposed to explicitly identify GC in a parse tree.</S>
			<S sid ="52" ssid = "16">These features are designed to implicitly model the binding constraints.</S>
			<S sid ="53" ssid = "17">Given a candidate antecedent or mention m1 and a pronoun mention m2 within a parsed sentence, we first test if they have c-command relation, and then a set of counting features are computed.</S>
			<S sid ="54" ssid = "18">The features are detailed as follows: (1) C-command ccmd(m1, m2) : A constituent X c- commands another constituent Y in a parse tree if the first branching node dominating X also dominates Y . The binary feature ccmd(m1 , m2) is true if the minimum NP dominating m1 c-commands the minimum NP dominating m2.</S>
			<S sid ="55" ssid = "19">In sub-figure (1) of Figure 1, NP1 c-commands NP2 since the first branching node dominating NP1 is S and it dominates NP2.</S>
			<S sid ="56" ssid = "20">If ccmd(m1 , m2) is true, we then define the c-command path T (m1, m2) as the path from the minimum NP dominating m2 to the first branching node that dominates the minimum NP dominating m1.</S>
			<S sid ="57" ssid = "21">In sub-figure (1) of Figure 1, the c-command path T (“John”, “himself ”) would be “NP2VP-S.” (2) N P count(m1, m2): If ccmd(m1 , m2) is true, then N P count(m1, m2 ) counts how many NPs are seen on the c-command path T (m1, m2), excluding two endpoints.</S>
			<S sid ="58" ssid = "22">In sub-figure (1) of Figure 1, N P count(“John”, “himself ”) = 0 since there is no NP on T (“John”, “himself ”).</S>
			<S sid ="59" ssid = "23">(3) V P count(m1, m2): similar to N P count(m1, m2), except that this feature counts how many verb phrases (VP) are seen on the c-command path.</S>
			<S sid ="60" ssid = "24">In sub-figure (1) of Figure 1, V P count(“John”, “himself ”) is true since there is one VP on T (“John”, “himself ”).</S>
			<S sid ="61" ssid = "25">(4) S count(m1, m2): This feature counts how many clauses are seen on the c-command path when ccmd(m1, m2) is true.</S>
			<S sid ="62" ssid = "26">In sub-figure (1) of Figure 1, S count(“John”, “himself ”) = 0 since there is no clause label on T (“John”, “himself ”).</S>
			<S sid ="63" ssid = "27">These features are designed to capture information in the concept of governing category when used in conjunction with attributes (e.g., gender, number, reflexiveness) of individual pronouns.</S>
			<S sid ="64" ssid = "28">Counting the intermediate NPs, VPs and sub-clauses implicitly characterizes the governor of a pronoun in question; the presence or absence of a sub- clause indicates whethere or not a coreferential relation is across clause boundary.</S>
			<S sid ="65" ssid = "29">3.2 Dependency Features.</S>
			<S sid ="66" ssid = "30">In addition to features inspired by the binding theory, a set of dependency features are also computed with the help of syntactic parse trees.</S>
			<S sid ="67" ssid = "31">This is motivated by examples such as “John is the president of ABC Corporation,” where “John” and “the president” refer to the same person and should be in the same entity.</S>
			<S sid ="68" ssid = "32">In scenarios like this, lexical features do not help, while the knowledge that “John” left-modifies the verb “is” and the “the president” right-modifies the same verb would be useful.</S>
			<S sid ="69" ssid = "33">Given two mentions m1 and m2 in a sentence, we compute the following dependency features: (1)same head(m1, m2): The feature compares the bi- lexical dependencies (m1, h(m1)), and (m2 , h(m2)), where h(x) is the head word which x modifies.</S>
			<S sid ="70" ssid = "34">The feature is active only if h(m1) = h(m2), in which case it returns h(m1 ).</S>
			<S sid ="71" ssid = "35">(2)same P OS(m1 , m2): To get good coverage of dependencies, we compute a feature same P OS(m1 , m2), which examines the same dependency as in (1) and returns the common head part-of-speech (POS) tag if h(m1) = h(m2).</S>
			<S sid ="72" ssid = "36">The head child nodes are marked with boxes in Figure 1.</S>
			<S sid ="73" ssid = "37">For the parse tree in sub-figure (1), same head(“John”, “him”) would return “likes” as “John” left-modifies “likes” while “him” right-modifies “likes,” and same P OS(“John”, “him”) would return “V” as the POS tag of “likes” is “V.” (3) mod(m1 , m2): the binary feature is true if m1 modifies m2.</S>
			<S sid ="74" ssid = "38">For parse tree (2) of Figure 1, mod(“John”, “him”) returns false as “John” does not modify “him” directly.</S>
			<S sid ="75" ssid = "39">A reverse order feature mod(m2 , m1) is computed too.</S>
			<S sid ="76" ssid = "40">(4) same head2(m1, m2): this set of features examine second-level dependency.</S>
			<S sid ="77" ssid = "41">It compares the head word of h(m1), or h(h(m1)), with h(m2) and returns the common head if h(h(m1 )) = h(m2).</S>
			<S sid ="78" ssid = "42">A reverse order feature same head2(m2, m1) is also computed.</S>
			<S sid ="79" ssid = "43">(5) same P OS2(m1, m2): similar to (4), except that it computes the second-level POS.</S>
			<S sid ="80" ssid = "44">A reverse order feature same P OS2(m2, m1) is computed too.</S>
			<S sid ="81" ssid = "45">(6) same head22(m1, m2): it returns the common second-level head if h(h(m1)) = h(h(m2)).</S>
			<S sid ="82" ssid = "46">3.3 Apposition and Same-Parent Features.</S>
			<S sid ="83" ssid = "47">Apposition is a phenomenon where two adjacent NPs refer to the same entity, as “Jimmy Carter” and “the former president” in the following example: (II) “Jimmy Carter, the former president of US, is visiting Europe.” Note that not all NPs separated by a comma are necessarily appositive.</S>
			<S sid ="84" ssid = "48">For example, in “John called Al, Bob, parent and are separated by comma, but they are not appositive.</S>
			<S sid ="85" ssid = "49">To compute the apposition feature appos(m1, m2) for mention-pair (m1, m2), we first determine the minimum dominating NP of m1 and m2.</S>
			<S sid ="86" ssid = "50">The minimum dominating NP of a mention is the lowest NP, with an optional modifying phrase or clause, that spans the mention.</S>
			<S sid ="87" ssid = "51">If the two minimum dominating NPs have the same parent NP, and they are the only two NP children of the parent, the value of appos(m1, m2) is true.</S>
			<S sid ="88" ssid = "52">This would exclude “Al” and “Bob” in “John called Al, Bob, and Charlie last night” from being computed as apposition.</S>
			<S sid ="89" ssid = "53">We also implement a feature same parent(m1 , m2) which tests if two mentions m1 and m2 are dominated by a common NP.</S>
			<S sid ="90" ssid = "54">The feature helps to prevent the system from linking “his” with “colleague” in the sentence “John called his colleague.” All the features described in Section 3.13.3 are computed from syntactic trees generated by a parser.</S>
			<S sid ="91" ssid = "55">While the parser is language dependent, feature computation boils down to encoding the structural relationship of two mentions, which is language independent.</S>
			<S sid ="92" ssid = "56">To test the effectiveness of the syntactic features, we integrate them into 3 coreference systems processing Arabic, Chinese and English.</S>
	</SECTION>
	<SECTION title="Experimental Results. " number = "4">
			<S sid ="93" ssid = "1">4.1 Data and System Description.</S>
			<S sid ="94" ssid = "2">All experiments are done on true mentions of the ACE (NIST, 2004) 2004 data.</S>
			<S sid ="95" ssid = "3">We reserve part of LDC- released 2004 data as the development-test set (henceforth “devtest”) as follows: documents are sorted by their date and time within each data source (e.g., broadcast news (bnews) and news wire (nwire) are two different sources) and the last 25% documents of each data source are reserved as the devtest set.</S>
			<S sid ="96" ssid = "4">Splitting data on chronological order simulates the process of a system’s development and deployment in the real world.</S>
			<S sid ="97" ssid = "5">The devtest set statistics of three languages (Arabic, Chinese and English) is summarized in Table 1, where the number of documents, mentions and entities is shown on row 2 through 4, respectively.</S>
			<S sid ="98" ssid = "6">The rest of 2004 ACE data together with earlier ACE data is used as training.</S>
			<S sid ="99" ssid = "7">Ar abi c Chinese English# do cs# me nti on s# ent itie s 1 7 8 166 114 11 35 8 8524 7008 4 4 2 8 3876 2929 Table 1: Devtest Set Statistics by Language The official 2004 evaluation test set is used as the blind test set on which we run our system once after the system development is finished.</S>
			<S sid ="100" ssid = "8">We will report summary results on this test set.</S>
			<S sid ="101" ssid = "9">As for parser, we train three off-shelf maximum-entropy parsers (Ratnaparkhi, 1999) using the Arabic, Chinese and English Penn treebank (Maamouri and Bies, 2004; Xia et al., 2000; Marcus et al., 1993).</S>
			<S sid ="102" ssid = "10">Arabic words are segmented while the Chinese parser is a character- based parser.</S>
			<S sid ="103" ssid = "11">The three parsers have a label F-measure of 77%, 80%, and 86% on their respective test sets.</S>
			<S sid ="104" ssid = "12">The three parsers are used to parse both ACE training and test data.</S>
			<S sid ="105" ssid = "13">Features described in Section 3 are computed from machine-generated parse trees.</S>
			<S sid ="106" ssid = "14">Apart from features extracted from parse trees, our coreference system also utilizes other features such as lexical features (e.g., string matching), distance features characterized as quantized word and sentence distances, mention- and entity-level attribute information (e.g, ACE distinguishes 4 types of mentions: NAM(e), NOM(inal), PRE(modifier) and PRO(noun)) found in the 2004 ACE data.</S>
			<S sid ="107" ssid = "15">Details of these features can be found in (Luo et al., 2004).</S>
			<S sid ="108" ssid = "16">4.2 Performance Metrics.</S>
			<S sid ="109" ssid = "17">The official performance metric in the ACE task is ACE- Value (NIST, 2004).</S>
			<S sid ="110" ssid = "18">The ACE-Value is an entity-based metric computed by subtracting a normalized cost from 1 (so it is unbounded below).</S>
			<S sid ="111" ssid = "19">The cost of a system is a weighted sum of costs associated with entity misses, false alarms and errors.</S>
			<S sid ="112" ssid = "20">This cost is normalized against the cost of a nominal system that outputs no entity.</S>
			<S sid ="113" ssid = "21">A perfect coreference system gets 100% ACE-Value while a system outputting many false-alarm entities could get a negative value.</S>
			<S sid ="114" ssid = "22">The default weights in ACE-Value emphasize names, and severely discount pronouns: the relative importance of a pronoun is two orders of magnitude less than that of a name.</S>
			<S sid ="115" ssid = "23">So the ACE-Value will not be able to accurately reflect a system’s improvement on pronouns2.</S>
			<S sid ="116" ssid = "24">For this reason, we compute an unweighted entity-constrained mention F-measure (Luo, 2005) and report all contrastive experiments with this metric.</S>
			<S sid ="117" ssid = "25">The F-measure is computed by first aligning system and reference entities such that the number of common mentions is maximized and each system entity is constrained to align with at most one reference entity, and vice versa.</S>
			<S sid ="118" ssid = "26">For example, suppose that a reference document contains three entities: {[m1], [m2, m3], [m4]} while a system outputs four entities: {[m1, m2], [m3], [m5 ], [m6]}, where {mi : i = 1, 2, · · · , 6} are mentions, then the best alignment from reference to system would be [m1] ⇔ [m1, m2], [m2, m3] ⇔ [m3] and other entities are not aligned.</S>
			<S sid ="119" ssid = "27">The number of common mentions of the best alignment is 2 2 Another possible choice is the MUC F-measure (Vilain et al., 1995).</S>
			<S sid ="120" ssid = "28">But the metric has a systematic bias for systems generating fewer entities (Bagga and Baldwin, 1998) – see Luo (2005).</S>
			<S sid ="121" ssid = "29">Another reason is that it cannot score single-mention entity.</S>
			<S sid ="122" ssid = "30">(i.e., m1 and m3), thus the recall is 2 and precision is 4.4 Error Analyses.</S>
	</SECTION>
	<SECTION title="Due to the one-to-one entity alignment constraint, the " number = "5">
			<S sid ="123" ssid = "1">F-measure here is more stringent than the accuracy (Ge et al., 1998; Mitkov, 1998; Kehler et al., 2004) computed on antecedent-pronoun pairs.</S>
			<S sid ="124" ssid = "2">4.3 Effect of Syntactic Features.</S>
			<S sid ="125" ssid = "3">We first present the contrastive experimental results on the devtest described in subsection 4.1.</S>
			<S sid ="126" ssid = "4">Two coreference systems are trained for each language: a baseline without syntactic features, and a system including the syntactic features.</S>
			<S sid ="127" ssid = "5">The entity-constrained F- measures with mention-type breakdown are presented in Table 2.</S>
			<S sid ="128" ssid = "6">Rows marked with Nm contain the number of mentions, while rows with “base” and “+synt” are F- measures for the baseline and the system with the syntactic features, respectively.</S>
			<S sid ="129" ssid = "7">The syntactic features improve pronoun mentions across three languages – not surprising since features inspired by the binding theory are designed to improve pronouns.</S>
			<S sid ="130" ssid = "8">The pronoun improvement on the Arabic (from 73.2% to 74.6%) and English (from 69.2% to 72.0%) system is statistically significant (at above 95% confidence level), but change on the Chinese system is not.</S>
			<S sid ="131" ssid = "9">For Arabic, the syntactic features improve Arabic NAM, NOM and PRE mentions, probably because Arabic pronouns are sometimes attached to other types of mentions.</S>
			<S sid ="132" ssid = "10">For Chinese and English, the syntactic features do not practically change the systems’ performance.</S>
			<S sid ="133" ssid = "11">As will be shown in Section 4.5, the baseline systems without syntactic features are already competitive, compared with the results on the coreference evaluation track (EDRcoref) of the ACE 2004 evaluation (NIS, 2004).</S>
			<S sid ="134" ssid = "12">So it is nice to see that syntactic features further improve a good baseline on Arabic and English.</S>
			<S sid ="135" ssid = "13">A r a b i c M e n t i o n T y p e To tal N A M NOM PRE PRO N m ba se +s ynt 28 43 3438 1291 3786 86 .8 73.2 86.7 73.2 88 .4 76.4 87.4 74.6 11 35 8 7 8.</S>
			<S sid ="136" ssid = "14">2 8 0.</S>
			<S sid ="137" ssid = "15">1 C h i n e s e N m ba se +s ynt 40 34 3696794 95 .4 77.865.9 95 .2 77.766.5 85 24 8 5.</S>
			<S sid ="138" ssid = "16">0 8 4.</S>
			<S sid ="139" ssid = "17">9 E n g l i s h N m ba se +s ynt 20 69 2173 835 1931 92 .0 73.4 88.7 69.2 92 .0 75.3 87.8 72.0 70 08 7 9.</S>
			<S sid ="140" ssid = "18">6 8 0.</S>
			<S sid ="141" ssid = "19">8 Table 2: F-measure(%) Breakdown by Mention Type: NAM(e), NOM(inal), PRE(modifier) and PRO(noun).</S>
			<S sid ="142" ssid = "20">Chinese data does not have the PRE type.</S>
			<S sid ="143" ssid = "21">From the results in Table 2, we know that the set of syntactic features are working in the Arabic and English system.</S>
			<S sid ="144" ssid = "22">But the results also raise some questions: Are there interactions among the the syntactic features and other features?</S>
			<S sid ="145" ssid = "23">Why do the syntactic features work well for Arabic and English, but not Chinese?</S>
			<S sid ="146" ssid = "24">To answer these questions, we look into each system and report our findings in the following sections.</S>
			<S sid ="147" ssid = "25">4.4.1 English System Our system uses a group of distance features.</S>
			<S sid ="148" ssid = "26">One observation is that information provided by some syntactic features (e.g., V P count(m1, m2) etc) may have overlapped with some of the distance features.</S>
			<S sid ="149" ssid = "27">To test if this is the case, we take out the distance features from the English system, and then train two systems, one with the syntactic features, one without.</S>
			<S sid ="150" ssid = "28">The results are shown in Table 3, where numbers on the row “b-dist” are F- measures after removing the distance features from the baseline, and numbers on the row “b-dist+synt” are with the syntactic features.</S>
			<S sid ="151" ssid = "29">M e n t i o n T y p e To tal N A M NOM PRE PRO b d i s tb di st +s yn t 84 .2 68.8 74.6 63.3 90 .7 74.2 87.8 69.0 72 .5 79 .3 Table 3: Impact of Syntactic Features on English System After Taking out Distance Features.</S>
			<S sid ="152" ssid = "30">Numbers are F-measures(%).</S>
			<S sid ="153" ssid = "31">As can be seen, the impact of the syntactic features is much larger when the distance features are absent in the system: performance improves across all the four mention types after adding the syntactic features, and the overall F-measure jumps from 72.5% to 79.3%.</S>
			<S sid ="154" ssid = "32">The PRE type gets the biggest improvement since features extracted from parse trees include apposition, same-parent test, and dependency features, which are designed to help mention pairs in close distance, just as in the case of PRE mentions.</S>
			<S sid ="155" ssid = "33">Comparing the numbers in Table 3 with the English baseline of Table 2, we can also conclude that distance features and syntactic features lead to about the same level of performance when the other set of features is not used.</S>
			<S sid ="156" ssid = "34">When the distance features are used, the syntactic features further help to improve the performance of the NOM and PRO mention type, albeit to a less degree because of information overlap between the two sets of features.</S>
			<S sid ="157" ssid = "35">4.4.2 Chinese System Results in Table 2 show that the syntactic features are not so effective for Chinese as for Arabic and English.</S>
			<S sid ="158" ssid = "36">The first thing we look into is if there is any idiosyncrasy in the Chinese language.</S>
			<S sid ="159" ssid = "37">In Table 4, we list the statistics collected over the training mentions.</S>
			<S sid ="160" ssid = "38">In both examples, the baseline system and the system with syntactic features give different results.</S>
			<S sid ="161" ssid = "39">Let’s consider the following sentence: sets of the three languages: the second row are the total number of mentions, the third row the number of pronoun ... its-capital ← Jerusalem ← Israel ← consider ← and ... mentions, the fourth row the number of events where the c-command feature ccmd(m1, m2) is used, and the last u _ row the average number of c-command features per pro noun (i.e., the fourth row divided by the third row).</S>
			<S sid ="162" ssid = "40">A pronouns event is defined as a tuple of training instance (e, m1, m2) where m1 is a mention in entity e, and the second mention m2 is a pronoun.</S>
			<S sid ="163" ssid = "41">of-the-city ← the-Eastern ← the-half ← the-Palestininan ← want ← while The English text shown above is a word-to-word translation of the Arabic text (read from right-to-left).</S>
			<S sid ="164" ssid = "42">In this example, the parser wrongly put the nominal mention From Table 4, it is clear that Chinese pronoun distribution ..</S>
			<S sid ="165" ssid = "43">l (Jerusalem) and the pronominal mention(the-city) under the same constituent, which acti is very different: pronoun mentions account for about 8.7% of the total mentions in Chinese, while 29.0% ofArabic mentions and 25.1% of English mentions are pro nouns (the same disparity can be observed in the devtest set in Table 2).</S>
			<S sid ="166" ssid = "44">This is because Chinese is a pro-drop language (Huang, 1984): for example, in the Chinese Penn treebank version 4, there are 4933 overt pronouns, but 5750 pro-drops!</S>
			<S sid ="167" ssid = "45">The ubiquity of pro-drops in Chinese results in signigicantly less pronoun training events.</S>
			<S sid ="168" ssid = "46">Consequently, the pronoun-related features are not trained as well as in English and Arabic.</S>
			<S sid ="169" ssid = "47">One way to quantify this vates the same parent feature.</S>
			<S sid ="170" ssid = "48">The use of the feature same parent( , ..</S>
			<S sid ="171" ssid = "49">l ) leads to the two mentions being put into different entities.</S>
			<S sid ="172" ssid = "50">This is because there are many cases in the training data where two mentions under the same parent are indeed in different entities: a similar English example is “John called his sister ”, where “his ” and “sister ” belong to two different entities.</S>
			<S sid ="173" ssid = "51">The same parent feature is a strong indicator of not putting them into the same entity.</S>
			<S sid ="174" ssid = "52">is by looking at the average number of c-command features on a per-pronoun basis: as shown in the last row of + J + + + J + Table 4, the c-command feature is seen more than twice + ) -...</S>
			<S sid ="175" ssid = "53">+ J + ..w + J-..</S>
			<S sid ="176" ssid = "54">+ J + ....</S>
			<S sid ="177" ssid = "55">-v- +often in Arabic and English as in Chinese.</S>
			<S sid ="178" ssid = "56">Since low ...</S>
			<S sid ="179" ssid = "57">+ t• + ..-.&gt; + ..w. count features are filtered out, the sparsity of pronounevents prevent many compound features (e.g., conjunc tion of syntactic and distance features) from being trained in the Chinese system, which explains why the syntactic features do not help Chinese pronouns.</S>
			<S sid ="180" ssid = "58">+ kAn + Al + zqAqywn + y + HAwl + wn + nhb + Al + mHl + At + Al + tjAry + p + b + Hjp + An + hm + ... was + the + zqAqywn + present-verb-marker y + trying + plural-verb-marker wn + to-steal + the + office + s + the + commercial + s + with + excuse + that + they + ...</S>
			<S sid ="181" ssid = "59">Table 5: An example where syntactic features help to link the PRO mention t&quot;&apos; (hm) with its antecedent, the NAM mentionu _ (AlzqAqywn): top – Arabic sen Table 4: Distribution of Pronoun Mentions and Frequency of c-command Features 4.4.3 Arabic System As stated in Table 4, 29.0% of Arabic mentions are pronouns, compared to a slightly lower number (25.1%) for English.</S>
			<S sid ="182" ssid = "60">This explains the relatively high positive impact of the syntactic features on the Arabic coreference system, compared to English and Chinese systems.</S>
			<S sid ="183" ssid = "61">To understand how syntactic features work in the Arabic system, we examine two examples extracted from the de vtest set: (1) the first example shows the negative impacttence; middle – corresponding romanized sentence; bot tom – token-to-token English translation.</S>
			<S sid ="184" ssid = "62">Table 5 shows another example in the devtest set.</S>
			<S sid ="185" ssid = "63">The top part presents the segmented Arabic text, the middle part is the corresponding romanized text, and the bottom part contains the token-to-token English translation.</S>
			<S sid ="186" ssid = "64">Note that Arabic text reads from right to left and its corresponding romanized text from left to right (i.e., the rightmost Arabic token maps to the leftmost romanized token).</S>
			<S sid ="187" ssid = "65">The parser output the correct syntactic structure: Figure 2 shows a portion of the system-generated parse tree.</S>
			<S sid ="188" ssid = "66">It can be checked that NP1 c-commands NP2 and the group of features inspired by the binding theory are active.</S>
			<S sid ="189" ssid = "67">These features help to link the PRO(onominal) mention t &quot;&apos; of syntactic features because of the noisy parsing output, (hm) with the NAM(e) mention u _ (AlzqAqywn).</S>
			<S sid ="190" ssid = "68">and (2) the second example proves the effectiveness of the syntactic features to find the dependency between two Without syntactic features theses two mentions were split into different entities.</S>
			<S sid ="191" ssid = "69">VP NP1 VP SBAR S NP2 sensitive to improvement on pronouns – the very reason we adopt the F measure in Section 4.3 and 4.4 when reporting the contrastive experiment results.</S>
			<S sid ="192" ssid = "70">5 R e l a t e d W o r k kAn Al+zqAqywn An hm Many researchers have used the syntactic information in their coreference system before.</S>
			<S sid ="193" ssid = "71">For example, Hobbs Figure 2: A Portion of the Syntactic Tree.</S>
			<S sid ="194" ssid = "72">4.5 ACE 2004 Results.</S>
			<S sid ="195" ssid = "73">To get a sense of the performance level of our system, we report the results on the ACE 2004 official test set with both the F-measure and the official ACE-Value metric.</S>
			<S sid ="196" ssid = "74">This data is used as the blind test set which we run our system only once.</S>
			<S sid ="197" ssid = "75">Results are summarized in Table 6, where the second row (i.e. “base”) contains the baseline numbers, and the third row (i.e., “+synt”) contains the numbers from systems with the syntactic features.</S>
			<S sid ="198" ssid = "76">Columns under “F” are F- measure and those under “AV” are ACE-Value.</S>
			<S sid ="199" ssid = "77">The last row Nm contains the number of mentions in the three test sets.</S>
			<S sid ="200" ssid = "78">A r a b i c C h i n e s e E n g l i s h F AV F AV F AV ba se +s ynt 80.</S>
			<S sid ="201" ssid = "79">1 88.0 81.</S>
			<S sid ="202" ssid = "80">5 88.9 84 .7 92.7 84 .7 92.8 80 .6 90.9 82 .0 91.6 N m 1 1 3 5 8 1 1 1 7 8 1 0 3 3 6 Table 6: Summary Results on the 2004 ACE Evaluation Data.</S>
			<S sid ="203" ssid = "81">The performance of three full (“+synt”) systems is remarkably close to that on the devtest set(cf.</S>
			<S sid ="204" ssid = "82">Table 2): For Arabic, F-measure is 80.1 on the devtest vs. 81.5 here; For Chinese, 84.9 vs. 84.7; And for English, 80.8 vs. 82.0.</S>
			<S sid ="205" ssid = "83">The syntactic features again help Arabic and English – statistically very significant in F-measure, but have no significant impact on Chinese.</S>
			<S sid ="206" ssid = "84">The performance consistency across the devtest and blind test set indicates that the systems are well trained.</S>
			<S sid ="207" ssid = "85">The F-measures are computed on all types of mentions.</S>
			<S sid ="208" ssid = "86">Improvement on mention-types targeted by the syntactic features is larger than the lump-sum F-measure.</S>
			<S sid ="209" ssid = "87">For example, the F-measure for English pronouns on this test set is improved from 69.5% to 73.7% (not shown in Table 6 due to space limit).</S>
			<S sid ="210" ssid = "88">The main purpose of Table 6 is to get a sense of performance level correspondence between the F-measure and ACE-Value.</S>
			<S sid ="211" ssid = "89">Also note that, for Arabic and English, the difference between the “base” and “+synt” systems, when measured by ACE-Value, is much smaller.</S>
			<S sid ="212" ssid = "90">This is not surprising since ACE-Value heavily discounts pronouns and is in (1976) uses a set of rules that are applied to parse trees to determine the antecedent of a pronoun.</S>
			<S sid ="213" ssid = "91">The rule prece dence is determined heuristically and no weight is used.</S>
			<S sid ="214" ssid = "92">Lappin and Leass (1994) extracted rules from the out put of the English Slot Grammar (ESG) (McCord, 1993).</S>
			<S sid ="215" ssid = "93">Rule weights are assigned manually and the system re solves the third person pronouns and reflexive pronouns only.</S>
			<S sid ="216" ssid = "94">Ge et al.</S>
			<S sid ="217" ssid = "95">(1998) uses a non-parametrized statistical model to find the antecedent from a list of candidates generated by applying the Hobbs algorithm to the English Penn Treebank.</S>
			<S sid ="218" ssid = "96">Kehler et al.</S>
			<S sid ="219" ssid = "97">(2004) experiments mak ing use of predicate-argument structure extracted from a large TDT-corpus.</S>
			<S sid ="220" ssid = "98">Compared with these work, our work uses machine-generated parse trees from which trainable features are extracted in a maximum-entropy coreference system, while (Ge et al., 1998) assumes that correct parse trees are given.</S>
			<S sid ="221" ssid = "99">Feature weights are automatically trained in our system while (Lappin and Leass, 1994; Stuckardt, 2001) assign weights manually.</S>
			<S sid ="222" ssid = "100">There are a large amount of published work (Morton, 2000; Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2003; Luo et al., 2004; Kehler et al., 2004) using machine-learning techniques in coreference resolution.</S>
			<S sid ="223" ssid = "101">But none of these work tried to compute complex linguistic concept such as governing category 3 . Our work demonstrates how relevant linguistic knowledge can be derived automatically from system-generated parse trees and encoded into computable and trainable features in a machine-learning framework.</S>
	</SECTION>
	<SECTION title="Conclusions. " number = "6">
			<S sid ="224" ssid = "1">In this paper, linguistic knowledge is used to guide us to design features in maximum-entropy-based coreference resolution systems.</S>
			<S sid ="225" ssid = "2">In particular, we show how to compute a set of features to approximate the linguistic notions such as governing category and apposition, and how to compute the dependency features using syntactic parse trees.</S>
			<S sid ="226" ssid = "3">While the features are motivated by examining English data, we see significant improvements on both English and Arabic systems.</S>
			<S sid ="227" ssid = "4">Due to the language idiosyncrasy (e.g., pro-drops), we do not see the syntactic features change the Chinese system significantly.</S>
			<S sid ="228" ssid = "5">3 Ng and Cardie (2002) used a BINDING feature, but it is not clear from their paper how the feature was computed and what its impact was on their system.</S>
	</SECTION>
	<SECTION title="Acknowledgments">
			<S sid ="229" ssid = "6">This work was partially supported by the Defense Advanced Research Projects Agency and monitored by SPAWAR under contract No.</S>
			<S sid ="230" ssid = "7">N6600199-28916.</S>
			<S sid ="231" ssid = "8">The views and findings contained in this material are those of the authors and do not necessarily reflect the position of policy of the Government and no official endorsement should be inferred.</S>
			<S sid ="232" ssid = "9">Suggestions for improving the paper from the anonymous reviewers are gratefully acknowledged.</S>
	</SECTION>
</PAPER>
