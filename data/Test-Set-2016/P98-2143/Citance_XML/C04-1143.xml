<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Email summarisation presents a unique set of requirements that are different from general text summarisation.</S>
		<S sid ="2" ssid = "2">This work describes the implementation of an email summarisation system for use in a voice-based Virtual Personal Assistant developed for the EU FASiL Project.</S>
		<S sid ="3" ssid = "3">Evaluation results from the first integrated version of the project are presented.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="4" ssid = "4">Email is one of the most ubiquitous applications used on a daily basis by millions of people worldwide, traditionally accessed over a fixed terminal or laptop computer.</S>
			<S sid ="5" ssid = "5">In the past years there has been an increasing demand for email access over mobile phones.</S>
			<S sid ="6" ssid = "6">Our work has focused on creating an email summarisation service that provides quality summaries adaptively and quickly enough to cater for the tight constrains imposed by a real time text-to-speech system.</S>
			<S sid ="7" ssid = "7">This work has been done as part of the European Union FASiL project, which aims to aims to construct a conversationally intelligent Virtual Personal Assistant (VPA) designed to manage the user’s personal and business information through a voice-based interface accessible over mobile phones.</S>
			<S sid ="8" ssid = "8">As the quality of life and productivity is to improved in an increasingly information dominated society, people need access to information anywhere, anytime.</S>
			<S sid ="9" ssid = "9">The Adaptive Information Management (AIM) service in the FASiL VPA seeks to automatically prioritise and present information that is most pertinent to the mobile users and adapt to different user preferences.</S>
			<S sid ="10" ssid = "10">The AIM service is comprised of three main parts: an email sum- mariser, email categoriser, calendar scheduling/PIM interaction and an adaptive prioritisation service that optimizes the sequence in which information is presented, keeping the overall duration of the voice-based dialogue to a minimum.</S>
	</SECTION>
	<SECTION title="Email Characteristics. " number = "2">
			<S sid ="11" ssid = "1">Email Summarisation techniques share many characteristics with general text summarisation techniques while catering for the unique characteristics of email: 1.</S>
			<S sid ="12" ssid = "2">short messages usually between 2 to 800 words in length (after thread-filtering)2.</S>
			<S sid ="13" ssid = "3">frequently do not obey grammatical or con ventional stylistic conventions 3.</S>
			<S sid ="14" ssid = "4">are a cross between informal mobile text or chat styles and traditional writing formats 4.</S>
			<S sid ="15" ssid = "5">display unique thread characteristics with 87% containing three previous emails or less (Fisher and Moody, 2001) All these four main characteristics combined together mean that most document summarisation techniques simply do not work well for email.</S>
			<S sid ="16" ssid = "6">The voice- based system also required that summaries be produced on demand, with only a short pause allowed for the summariser to output a result – typically a maximum of around 1 second per email.</S>
			<S sid ="17" ssid = "7">Another main constraint imposed in the FASiL VPA was the presence of two integer parameters – the preferred and maximum length of the summary.</S>
			<S sid ="18" ssid = "8">The maximum length constraint had to be obeyed strictly, while striving to fit in the summary into the preferred length.</S>
			<S sid ="19" ssid = "9">These performance and size constraints, coupled with the four characteristics of email largely determined the design of the FASiL Email Summariser.</S>
			<S sid ="20" ssid = "10">2.1 Short Messages.</S>
			<S sid ="21" ssid = "11">Email is a form of short, largely informal, written communication that excludes methods that need large amounts of words and phrases to work well.</S>
			<S sid ="22" ssid = "12">The main disadvantage is that sometimes the useful content of a whole email message is simply a one word in case of a yes/no answer to a question or request.</S>
			<S sid ="23" ssid = "13">The summariser exploits this characteristic by filtering out threads and other commonly repeated text at the bottom of the email text such as standard email text signatures.</S>
			<S sid ="24" ssid = "14">If the resulting text is very short and falls within the preferred length of the summary, the message can be output in its entirety to users.</S>
			<S sid ="25" ssid = "15">The short messages also make it easier to achieve relevancy in the summaries.</S>
			<S sid ="26" ssid = "16">Inadvertently context is sometimes lost in the summary due to replies occurring in threaded emails.</S>
			<S sid ="27" ssid = "17">Also, emails containing lots of question-answer pairs can get summarised poorly due to the fixed amount of space available for the summary.</S>
			<S sid ="28" ssid = "18">2.2 Stylistic Conventions and Grammar.</S>
			<S sid ="29" ssid = "19">Email messages often do not follow formal stylistic conventions and are may have a substantial level of spelling mistakes, abbreviations and other features that make text analysis difficult.</S>
			<S sid ="30" ssid = "20">A simple spellchecker using approximate string matching and word frequency/occurrence statistics was used to match misspelled names automatically.</S>
			<S sid ="31" ssid = "21">Another problem that was encountered was the identification of sentence boundaries, since more than 10% of the emails seen by the summariser frequently had missing punctuation and spurious line breaks inserted by various different email programs.</S>
			<S sid ="32" ssid = "22">A set of hand- coded heuristics managed to produce acceptable results, identifying sentence boundaries correctly more than 90% of the time.</S>
			<S sid ="33" ssid = "23">2.3 Informal and Formal Styles.</S>
			<S sid ="34" ssid = "24">Email can often be classified into three categories: informal short messages – often sent to people whom are directly known or with whom there has been a prolonged discussion or interaction about a subject, mixed formal/informal emails sent to strangers or when requesting information or replying to questions, and formal emails that are generally electronic versions of formal letter writing.</S>
			<S sid ="35" ssid = "25">The class of emails that cause most problems for summarisation purposes are the first two classes of emails.</S>
			<S sid ="36" ssid = "26">One of the main determining factors for the style adopted by people in replying to emails is the amount of time that lapses between replies.</S>
			<S sid ="37" ssid = "27">Generally email gets more formal as the time span between replies increases.</S>
			<S sid ="38" ssid = "28">Informal email can also be recognised by excessive use of anaphora that need to be resolved properly before summarisation can take place.</S>
			<S sid ="39" ssid = "29">The summariser thus has an anaphora resolver that is capable of resolving anaphoric references robustly.</S>
			<S sid ="40" ssid = "30">Linguistic theory indicates that as the formality of a text increases, the number of words in the deictic cate gory will decrease as the number of words in the non- deictic category increase (and vice-versa).</S>
			<S sid ="41" ssid = "31">Deictic (or anaphoric) word classes include words that have variable meaning whose meaning needs to be resolved through the surrounding (usually preceding) context.</S>
			<S sid ="42" ssid = "32">Non-deictic word classes are those words whose meaning is largely context-independent, analogous to predicates in formal logic.</S>
			<S sid ="43" ssid = "33">2.4 Threaded Emails.</S>
			<S sid ="44" ssid = "34">Many emails are composed by replying to an original email, often including part or whole of the original email together with new content, thus creating a thread or chain of emails.</S>
			<S sid ="45" ssid = "35">The first email in the thread will potentially be repeated many times over, which might mislead the summarisation process.</S>
			<S sid ="46" ssid = "36">A thread-detection filtering tool is used to eliminate unoriginal content in the email by comparing the contents of the current email with the content of previous emails.</S>
			<S sid ="47" ssid = "37">A study of over 57 user’s incoming and outgoing emails found that around 30% of all emails are threaded.</S>
			<S sid ="48" ssid = "38">Around 56% of the threaded emails contained only one previous email – i.e. a request and reply, and 87% of all emails contained only three previous emails apart from the reply (Fisher and Moody, 2001).</S>
			<S sid ="49" ssid = "39">Some reply styles also pose a problem when combined with threads.</S>
			<S sid ="50" ssid = "40">Emails containing a list of questions or requests for comments are often edited by the replying party and answers inserted directly inside the text of the original request, as illustrated in Figure 1.</S>
			<S sid ="51" ssid = "41">&gt; … now coming back to the issue &gt; of whether to include support for &gt; location names in the recogniser &gt; I think that we should include &gt; this – your opinions appreciated.</S>
			<S sid ="52" ssid = "42">I agree with this.</S>
			<S sid ="53" ssid = "43">Figure 1 Sample Embedded Answer Figure 1 illustrates the main two difficulties faced by the summariser in this situation.</S>
			<S sid ="54" ssid = "44">While the threaded content from the previous reply should be filtered out to identify the reply, the reply on its own is meaningless without any form of context.</S>
			<S sid ="55" ssid = "45">The summariser tries to overcome this by identifying this style of embedded responses when the original content is split into chunks or is only partially included in the reply.</S>
			<S sid ="56" ssid = "46">The text falling before the answer is then treated as part of the reply.</S>
			<S sid ="57" ssid = "47">Although this strategy gives acceptable results in some cases, more research is needed into finding the optimal strategy to extract the right amount of context from the thread without either destroying the context or copying too much from the original request back into the summary.</S>
	</SECTION>
	<SECTION title="Summarisation Techniques. " number = "3">
			<S sid ="58" ssid = "1">Various summarisation techniques were considered in the design of the FASiL email summariser.</S>
			<S sid ="59" ssid = "2">Few operational email-specific summarisation systems exist, so the emphasis was on extracting the best-of-breed techniques from document summarisation systems that are applicable to email summarisation.</S>
			<S sid ="60" ssid = "3">3.1 Previous Work.</S>
			<S sid ="61" ssid = "4">Many single-document summarisation systems can be split according to whether they are extractive or non- extractive systems.</S>
			<S sid ="62" ssid = "5">Extractive systems generate summaries by extracting selected segments from the original document that are deemed to be most relevant.</S>
			<S sid ="63" ssid = "6">Non- extractive systems try to build an abstract representation model and regenerate the summary using this model and words found in the original document.</S>
			<S sid ="64" ssid = "7">Previous related work on extractive systems included the use of semantic tagging and co reference/lexical chains (Saggion et al., 2003; Barzilay and Elhadad, 1997; Azzam et al., 1998), lexical occurrence/structural statistics (Mathis et al., 1973), discourse structure (Marcu, 1998), cue phrases (Luhn, 1958; Paice, 1990; Rau et al., 1994), positional indicators (Edmunson, 1964) and other extraction methods (Kuipec et al., 1995).</S>
			<S sid ="65" ssid = "8">Non-extractive systems are less common – previous related work included reformulation of extracted models (McKeown et al., 1999), gist extraction (Berger and Mittal, 2000), machine translation-like approaches (Witbrock and Mittal, 1999) and generative models (De Jong, 1982; Radev and McKeown, 1998; Fum et al., 1986; Reihmer and Hahn, 1988; Rau et al., 1989).</S>
			<S sid ="66" ssid = "9">A sentence-extraction system was decided for the FASiL summariser, with the capability to have phrase- level extraction in the future.</S>
			<S sid ="67" ssid = "10">Non-extractive systems were not likely to work as robustly and give the high quality results needed by the VPA to work as required.</S>
			<S sid ="68" ssid = "11">Another advantage that extractive systems still pose is that in general they are more applicable to a wider range of arbitrary domains and are more reliable than non- extractive systems (Teufel, 2003).</S>
			<S sid ="69" ssid = "12">The FASiL summariser uses named entities as an indication of the importance of every sentence, and performs anaphora resolution automatically.</S>
			<S sid ="70" ssid = "13">Sentences are selected according to named entity density and also according to their positional ranking.</S>
			<S sid ="71" ssid = "14">3.2 Summariser Architecture.</S>
			<S sid ="72" ssid = "15">The FASiL Summariser works in conjunction with a number of different components to present real-time voice-based summaries to users.</S>
			<S sid ="73" ssid = "16">Figure 2 shows the overall architecture of the summariser and its place in the FASiL VPA.</S>
			<S sid ="74" ssid = "17">Figure 2 Summariser and VPA Architecture An XML-based protocol is used to communicate with the Dialogue Manager enabling the system to be loosely coupled but to have high cohesion (Sommerville, 1992).</S>
			<S sid ="75" ssid = "18">3.3 Named Entity Recognition.</S>
			<S sid ="76" ssid = "19">One of the most important components in the FASiL Summariser is the Named Entity Recogniser (NER) system.</S>
			<S sid ="77" ssid = "20">The NER uses a very efficient trie-like structure to match sub-parts of every name (Gusfield, 1997; Stephen, 1994).</S>
			<S sid ="78" ssid = "21">An efficient implementation enables the NER to confirm or reject a word as being a named entity or not in O(n) time.</S>
			<S sid ="79" ssid = "22">Named entities are automatically classified according to the following list of 11 classes:  Male proper names (M)  Female proper names (F)  Places (towns, cities, etc.)</S>
			<S sid ="80" ssid = "23">(P)  Locations (upstairs, boardroom, etc.)</S>
			<S sid ="81" ssid = "24">(L)  Male titles (Mr., Esq., etc.)</S>
			<S sid ="82" ssid = "25">(Mt)  Female titles (Ms., Mrs., etc.)</S>
			<S sid ="83" ssid = "26">(Ft)  Generic titles (t)  Date and time references (TIME)  Male anaphors (Ma)  Female anaphors (Fa)  Indeterminate anaphors (a) The gazetteer list for Locations, Titles, and Anaphors were compiled manually.</S>
			<S sid ="84" ssid = "27">Date and time references were compiled from data supplied in the IBM International Components for Unicode (ICU) project (Davis, 2003).</S>
			<S sid ="85" ssid = "28">Place names were extracted from data available online from the U.S. Geological Survey Geographic Names Information System and the GEOnet Names Server (GNS) of the U.S. National Imagery and Mapping Agency (USGS, 2003; NIMA, 2003).</S>
			<S sid ="86" ssid = "29">An innovative approach to gathering names for the male and female names was adopted using a small custom-built information extraction system that crawled Internet pages to identify likely proper names in the texts.</S>
			<S sid ="87" ssid = "30">Additional hints were provided by the presence of anaphora in the same sentence or the following sentence as the suspected proper name.</S>
			<S sid ="88" ssid = "31">The gender of every title and anaphora was manually noted and this information was used to keep a count of the number of male or female titles and anaphors associated with a particular name.</S>
			<S sid ="89" ssid = "32">This information enabled the list of names to be organised by gender, enabling a rough probability to be assigned to suspect words (Azzam et al., 1998; Mitkov, 2002).</S>
			<S sid ="90" ssid = "33">An Internet-based method that verified the list and filtered out likely spelling mistakes and nonexistent names was then applied to this list, filtering out incorrectly spelt names and other features such as online chat nicknames (Dalli, 2004).</S>
			<S sid ="91" ssid = "34">A list of over 592,000 proper names was thus obtained by this method with around 284,000 names beingidentified as male and 308,000 names identified as fe male.</S>
			<S sid ="92" ssid = "35">The large size of this list contributed significantly to the NER’s resulting accuracy and compares favourably with previously compiled lists (Stevenson and Gaizauskas, 2000).</S>
			<S sid ="93" ssid = "36">3.4 Anaphora Resolution.</S>
			<S sid ="94" ssid = "37">Extracting systems suffer from the problem of dangling anaphora in summaries.</S>
			<S sid ="95" ssid = "38">Anaphora resolution is an effective way of reducing the incoherence in resulting summaries by replacing anaphors with references to the appropriate named entities (Mitkov, 2002).</S>
			<S sid ="96" ssid = "39">This substitution has the direct effect of making the text less context sensitive and implicitly increases the formality of the text.</S>
			<S sid ="97" ssid = "40">Cohesion problems due to semantic discontinuities where concepts and agents are not introduced are also partially solved by placing emphasis on named entities and performing anaphora resolution.</S>
			<S sid ="98" ssid = "41">The major cohesion problem that still has not been fully addressed is the coherence of various events mentioned in the text.</S>
			<S sid ="99" ssid = "42">The anaphora resolver is aided by the gender- categorised named entity classes, enabling it to perform better resolution over a wide variety of names.</S>
			<S sid ="100" ssid = "43">A simple linear model is adopted, where the system focuses mainly on nominal and clausal antecedents (Cristea et al., 2000).</S>
			<S sid ="101" ssid = "44">The search scope for candidate antecedents is set to the current sentence together with the three preceding sentences as suggested in (Mitkov, 1998) as empirical studies show that more than 85% of all cases are handled correctly with this window size (Mitkov, 2002).</S>
			<S sid ="102" ssid = "45">3.5 Sentence Ranking.</S>
			<S sid ="103" ssid = "46">After named entity recognition and anaphora resolution, the summariser ranks the various sentences/phrases that it identifies and selects the best sentences to extract and put in the summary.</S>
			<S sid ="104" ssid = "47">The summariser takes two parameters apart from the email text itself: a preferred length and a maximum length.</S>
			<S sid ="105" ssid = "48">Typical lengths are 160 characters preferred with 640 characters maximum, which compares to the size a mobile text message.</S>
			<S sid ="106" ssid = "49">Ranking takes into account three parameters: named entity density and importance of every class, sentence position and the preferred and maximum length parameters.</S>
			<S sid ="107" ssid = "50">Weight 8 7 6 5 4 3 2 1 0 1 3 5 7 9 11 13 15 17 19 Number of Sentences Series1 Series2 Series3 Series4 Series5 Figure 3 Positional sentence weight for varying summarisation parameters Positional importance was found to be significant in email text since relevant information was often found to be in the first few sentences of the email.</S>
			<S sid ="108" ssid = "51">Figure 3 shows how the quadratic positional weight function γ changes with position, giving less importance to sentences as they occur further from the start (although the weight is always bigger than zero).</S>
			<S sid ="109" ssid = "52">Different kinds of emails were used to calibrate the weight function.</S>
			<S sid ="110" ssid = "53">Series 1 (bottom) represents a typical mobile text message length summary with a very long message.</S>
			<S sid ="111" ssid = "54">Series 4 and 5 (middle) represent the weight function behaviour when the summary maximum length is long (approximately more than 1,000 characters), irrelevant of the email message length itself.</S>
			<S sid ="112" ssid = "55">Series 2 and 3 (top) represent email messages that fall within the maximum length constraints.</S>
			<S sid ="113" ssid = "56">The following ranking function rank(j), where j is the sentence number, is used to rank and select excerpts:  rank  j   1 −    ∑  c  j, i i   i 0Candidate antecedents being discarded after ten sen  j    − length j  −    tences have been processed without the presence of anaphora as suggested in (Kameyama, 1997).</S>
			<S sid ="114" ssid = "57">  max       j  1     where α and β are empirically determined constants, ρ is the preferred summary length, and jmax is the number of sentences in the email.</S>
			<S sid ="115" ssid = "58">The NER function τc represents the number of words of type i in sentence j and ω(i) gives the weight associated with that type.</S>
			<S sid ="116" ssid = "59">In our case τ equals 10 since there are 11 named entity classes.</S>
			<S sid ="117" ssid = "60">The NER weights ω(i) for every class have been empirically determined and optimized.</S>
			<S sid ="118" ssid = "61">A third parameter γ is used to change the values of α and β according to the maximum and preferred lengths together with the email length as shown in Figure 3.</S>
			<S sid ="119" ssid = "62">The first term handles named entity density, the second the sentence position and the third biases the ranking towards the preferred length.</S>
			<S sid ="120" ssid = "63">The sentences are then sorted in rank order and the preferred and maximum lengths used to determine which sentences to return in the summary.</S>
	</SECTION>
	<SECTION title="Experimental Results. " number = "4">
			<S sid ="121" ssid = "1">The summariser results quality was evaluated against manually produced summaries using precision and recall, together with a more useful utility-based evaluation that uses a fractional model to cater for varying degrees of importance for different sentences.</S>
			<S sid ="122" ssid = "2">4.1 Named Entity Recognition Performance.</S>
			<S sid ="123" ssid = "3">The performance of the summariser depends significantly on the performance of the NER.</S>
			<S sid ="124" ssid = "4">Speed tests show that the NER consistently processes more than 1 million wps on a 1.6 GHz machine while keeping resource usage to a manageable 300400 Mb of memory.</S>
			<S sid ="125" ssid = "5">Precision and recall curves were calculated for 100 emails chosen at random, separated into 10 random sample groups from representative subsets of the three main types of emails – short, normal and long emails as explained previously.</S>
			<S sid ="126" ssid = "6">The samples were manually marked according to the 11 different named entity classes recognised by the NER to act as a comparative standard for relevant results.</S>
			<S sid ="127" ssid = "7">Figures 4 and 5 respectively show the NER precision and recall results.</S>
			<S sid ="128" ssid = "8">Precision 1 0.8 0.6 0.4 0.2 0 1 2 3 4 5 6 7 8 9 10 Sample Group M F P L Mt Ft t TIME Ma Fa a Figure 4 Precision by Named Entity Class It is interesting to note that the NER performed worst at anaphora identification with an average precision of 77.5% for anaphora but 96.7% for the rest of the named entity classes.</S>
			<S sid ="129" ssid = "9">Recall 1 0.8 0.6 0.4 0.2 0 1 2 3 4 5 6 7 8 9 10 Sample Group Ma Figure 5 Recall by Named Entity Class Figure 6 shows the average precision and recall averaged across all the eleven types of named entity classes, for the 10 sample email groups.</S>
			<S sid ="130" ssid = "10">An average precision of 93% was achieved throughout, with 97% recall.</S>
			<S sid ="131" ssid = "11">Value 1 0.75 0.5 0.25 0 1 2 3 4 5 6 7 8 9 10 Sample Group Recall Precision Figure 6 Average Precision and Recall It is interesting to note that the precision and recall curves do not exhibit the commonly observed inverse trade-off relationship between precision and recall (Buckland and Gey, 1994; Alvarez, 2002).</S>
			<S sid ="132" ssid = "12">This result is explained by the fact that the NER, in this case, can actually identify most named entities in the text with high precision while neither over-selecting irrelevant results nor under-selecting relevant results.</S>
			<S sid ="133" ssid = "13">4.2 Summariser Results Quality.</S>
			<S sid ="134" ssid = "14">Quality evaluation was performed by selecting 150 emails at random and splitting the emails up into 15 groups of 10 emails at random to facilitate multiple person evaluation.</S>
			<S sid ="135" ssid = "15">Each sentence in every email was then manually ranked using a scale of 1 to 10.</S>
			<S sid ="136" ssid = "16">For recall and precision calculation, any sentence ranked ≥ 5 was defined as relevant.</S>
			<S sid ="137" ssid = "17">Figure 7 shows the precision and re call values with 74% average precision and 71% average recall.</S>
			<S sid ="138" ssid = "18">Value 1.5 1 0.5 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Sample Group Recall Precision Figure 7 Summaries Recall and Precision A utility-based evaluation was also used to obtain more intuitive results than those given by precision and recall using the methods reported in (Jing et al., 1998; Goldstein et al., 1999; Radev et al., 2000).</S>
			<S sid ="139" ssid = "19">The average score of each summary was compared to the average score over infinity expected to be obtained by extracting a combination of the first [1..N] sentences at random.</S>
			<S sid ="140" ssid = "20">The summary average score was also compared to the score obtained by an averaged pool of 3 human judges.</S>
			<S sid ="141" ssid = "21">Figure 8 shows a comparison between the summariser performance and human performance, with the summariser averaging at 86.5% of the human performance, ranging from 60% agreement to 100% agreement with the gold standard.</S>
			<S sid ="142" ssid = "22">Value 2 1.5 1 0.5 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Sample Group Summariser Utility Gold Standard Utility Figure 8 Utility Score Comparison In Figure 8 a random extraction system is expected to get a score of 1 averaged across an infinite amount of runs.</S>
			<S sid ="143" ssid = "23">The average sentence compression factor for the summariser was 42%, exactly the same as the human judges’ results.</S>
			<S sid ="144" ssid = "24">The selected emails had an average length of 14 sentences, varying from 7 to 27 sentences.</S>
	</SECTION>
	<SECTION title="Conclusion and Future Work. " number = "5">
			<S sid ="145" ssid = "1">The FASiL Email Summarisation system represents a compact summarisation system optimised for email summarisation in a voice-based system context.</S>
			<S sid ="146" ssid = "2">The excellent performance in both speed and accuracy of the NER component makes it ideal for reuse in projects that need high quality real-time identification and classification of named entities.</S>
			<S sid ="147" ssid = "3">A future improvement will incorporate a fast POS analyser to enable phrase-level extraction to take place while improving syntactic coherence.</S>
			<S sid ="148" ssid = "4">An additional improvement will be the incorporation of co-reference chain methods to verify email subject lines and in some cases suggest more appropriate subject lines.</S>
			<S sid ="149" ssid = "5">The FASiL summariser validates the suitability of the combined sentence position and NER-driven ap proach towards email summarisation with encouraging results obtained.</S>
	</SECTION>
	<SECTION title="Acknowledgments">
			<S sid ="150" ssid = "6">This research is funded under the EU FASiL Project, anEU grant in Human Language Technology (IST2001 38685) (Website: www.fasil.co.uk).</S>
	</SECTION>
</PAPER>
