<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">The importance of inference rules to semantic applications has long been recognized and extensive work has been carried out to automatically acquire inference-rule resources.</S>
		<S sid ="2" ssid = "2">However, evaluating such resources has turned out to be a nontrivial task, slowing progress in the field.</S>
		<S sid ="3" ssid = "3">In this paper, we suggest a framework for evaluating inference-rule resources.</S>
		<S sid ="4" ssid = "4">Our framework simplifies a previously proposed “instance-based evaluation” method that involved substantial annotator training, making it suitable for crowdsourcing.</S>
		<S sid ="5" ssid = "5">We show that our method produces a large amount of annotations with high inter-annotator agreement for a low cost at a short period of time, without requiring training expert annotators.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="6" ssid = "6">Inference rules are an important component in semantic applications, such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006), describing a directional inference relation between two text patterns with variables.</S>
			<S sid ="7" ssid = "7">For example, to answer the question ‘Where was Reagan raised?’ a QA system can use the rule ‘X brought up in Y→X raised in Y’ to extract the answer from ‘Reagan was brought up in Dixon’.</S>
			<S sid ="8" ssid = "8">Similarly, an IE system can use the rule ‘X work as Y→X hired as Y’ to extract the PERSON and ROLE entities in the “hiring” event from ‘Bob worked as an analyst for Dell’.</S>
			<S sid ="9" ssid = "9">The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.</S>
			<S sid ="10" ssid = "10">However, despite their potential, utilization of inference rule resources is currently somewhat limited.</S>
			<S sid ="11" ssid = "11">This is largely due to the fact that these algorithms often produce invalid rules.</S>
			<S sid ="12" ssid = "12">Thus, evaluation is necessary both for resource developers as well as for inference system developers who want to asses the quality of each resource.</S>
			<S sid ="13" ssid = "13">Unfortunately, as evaluating inference rules is hard and costly, there is no clear evaluation standard, and this has become a slowing factor for progress in the field.</S>
			<S sid ="14" ssid = "14">One option for evaluating inference rule resources is to measure their impact on an end task, as that is what ultimately interests an inference system developer.</S>
			<S sid ="15" ssid = "15">However, this is often problematic since inference systems have many components that address multiple phenomena, and thus it is hard to assess the effect of a single resource.</S>
			<S sid ="16" ssid = "16">An example is the Recognizing Textual Entailment (RTE) framework (Dagan et al., 2009), in which given a text T and a textual hypothesis H, a system determines whether H can be inferred from T. This type of evaluation was established in RTE challenges by ablation tests (see RTE ablation tests in ACLWiki) and showed that resources’ impact can vary considerably from one system to another.</S>
			<S sid ="17" ssid = "17">These issues have also been noted by Sammons et al.</S>
			<S sid ="18" ssid = "18">(2010) and LoBue and Yates (2011).</S>
			<S sid ="19" ssid = "19">A complementary application-independent evaluation method is hence necessary.</S>
			<S sid ="20" ssid = "20">Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).</S>
			<S sid ="21" ssid = "21">However, Szpektor et al.</S>
			<S sid ="22" ssid = "22">(2007) observed that directly judging rules out of context often results in low inter-annotator agreement.</S>
			<S sid ="23" ssid = "23">To remedy that, Szpektor et al.</S>
			<S sid ="24" ssid = "24">(2007) and 156 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 156–160, Jeju, Republic of Korea, 814 July 2012.</S>
			<S sid ="25" ssid = "25">Qc 2012 Association for Computational Linguistics Bhagat et al.</S>
			<S sid ="26" ssid = "26">(2007) proposed “instance-based evaluation”, in which annotators are presented with an application of a rule in a particular context and need to judge whether it results in a valid inference.</S>
			<S sid ="27" ssid = "27">This simulates the utility of rules in an application and yields high inter-annotator agreement.</S>
			<S sid ="28" ssid = "28">Unfortunately, their method requires lengthy guidelines and substantial annotator training effort, which are time consuming and costly.</S>
			<S sid ="29" ssid = "29">Thus, a simple, robust and replicable evaluation method is needed.</S>
			<S sid ="30" ssid = "30">Recently, crowdsourcing services such as Amazon Mechanical Turk (AMT) and CrowdFlower (CF)1 have been employed for semantic inference annotation (Snow et al., 2008; Wang and CallisonBurch, 2010; Mehdad et al., 2010; Negri et al., 2011).</S>
			<S sid ="31" ssid = "31">These works focused on generating and annotating RTE text-hypothesis pairs, but did not address annotation and evaluation of inference rules.</S>
			<S sid ="32" ssid = "32">In this paper, we propose a novel instance-based evaluation framework for inference rules that takes advantage of crowdsourcing.</S>
			<S sid ="33" ssid = "33">Our method substantially simplifies annotation of rule applications and avoids annotator training completely.</S>
			<S sid ="34" ssid = "34">The novelty in our framework is twofold: (1) We simplify instance-based evaluation from a complex decision scenario to two independent binary decisions.</S>
			<S sid ="35" ssid = "35">(2) We apply methodological principles that efficiently communicate the definition of the “inference” relation to untrained crowdsourcing workers (Turkers).</S>
			<S sid ="36" ssid = "36">As a case study, we applied our method to evaluate algorithms for learning inference rules between predicates.</S>
			<S sid ="37" ssid = "37">We show that we can produce many annotations cheaply, quickly, at good quality, while achieving high inter-annotator agreement.</S>
	</SECTION>
	<SECTION title="Evaluating Rule Applications. " number = "2">
			<S sid ="38" ssid = "1">As mentioned, in instance-based evaluation individual rule applications are judged rather than rules in isolation, and the quality of a rule-resource is then evaluated by the validity of a sample of applications of its rules.</S>
			<S sid ="39" ssid = "2">Rule application is performed by finding an instantiation of the rule left-hand-side in a corpus (termed LHS extraction) and then applying the rule on the extraction to produce an instantiation of the rule right-hand-side (termed RHS instantiation).</S>
			<S sid ="40" ssid = "3">For example, the rule ‘X observe Y→X celebrate Y’ 1 https://www.mturk.com and http://crowdflower.com can be applied on the LHS extraction ‘they observe holidays’ to produce the RHS instantiation ‘they celebrate holidays’.</S>
			<S sid ="41" ssid = "4">The target of evaluation is to judge whether each rule application is valid or not.</S>
			<S sid ="42" ssid = "5">Following the standard RTE task definition, a rule application is considered valid if a human reading the LHS extraction is highly likely to infer that the RHS instantiation is true (Dagan et al., 2009).</S>
			<S sid ="43" ssid = "6">In the aforementioned example, the annotator is expected to judge that ‘they observe holidays’ entails ‘they celebrate holidays’.</S>
			<S sid ="44" ssid = "7">In addition to this straightforward case, two more subtle situations may arise.</S>
			<S sid ="45" ssid = "8">The first is that the LHS extraction is meaningless.</S>
			<S sid ="46" ssid = "9">We regard a proposition as meaningful if a human can easily understand its meaning (despite some simple grammatical errors).</S>
			<S sid ="47" ssid = "10">A meaningless LHS extraction usually occurs due to a faulty extraction process (e.g., Table 1, Example 2) and was relatively rare in our case study (4% of output, see Section 4).</S>
			<S sid ="48" ssid = "11">Such rule applications can either be extracted from the sample so that the rule-base is not penalized (since the problem is in the extraction procedure), or can be used as examples of non-entailment, if we are interested in overall performance.</S>
			<S sid ="49" ssid = "12">A second situation is a meaningless RHS instantiation, usually caused by rule application in a wrong context.</S>
			<S sid ="50" ssid = "13">This case is tagged as non-entailment (for example, applying the rule ‘X observe Y→X celebrate Y’ in the context of the extraction ‘companies observe dress code’).</S>
			<S sid ="51" ssid = "14">Each rule application therefore requires an answer to the following three questions: 1) Is the LHS extraction meaningful?</S>
			<S sid ="52" ssid = "15">2) Is the RHS instantiation meaningful?</S>
			<S sid ="53" ssid = "16">3) If both are meaningful, does the LHS extraction entail the RHS instantiation?</S>
	</SECTION>
	<SECTION title="Crowdsourcing. " number = "3">
			<S sid ="54" ssid = "1">Previous works using crowdsourcing noted some principles to help get the most out of the service(Wang et al., 2012).</S>
			<S sid ="55" ssid = "2">In keeping with these findings we employ the following principles: (a) Simple tasks.</S>
			<S sid ="56" ssid = "3">The global task is split into simple sub-tasks, each dealing with a single aspect of the problem.</S>
			<S sid ="57" ssid = "4">(b) Do not assume linguistic knowledge by annota- tors.</S>
			<S sid ="58" ssid = "5">Task descriptions avoid linguistic terms such as “tense”, which confuse workers.</S>
			<S sid ="59" ssid = "6">(c) Gold standard validation.</S>
			<S sid ="60" ssid = "7">Using CF’s built-in methodology, Ph ras e Me ani ng ful Co m me nts 1) Do ctor s be tre at Ma ry Yes An not ato rs are inst ruct ed to ign ore sim ple infl ecti on al err ors 2) A pla yer de pos it an No Ba d ext rac tion for the rul e LH S ‘X de pos it Y’ 3) hu ma ns bri ng in be d No Wr on g con text , res ult of ap plyi ng ‘X tur n in Y → X bri ng in Y’ on ‘hu ma ns tur n in be d’ Table 1: Examples of phrase “meaningfulness” (Note that the comments are not presented to Turkers).</S>
			<S sid ="61" ssid = "8">gold standard (GS) examples are combined with actual annotations to continuously validate annotator reliability.</S>
			<S sid ="62" ssid = "9">We split the annotation process into two tasks, the first to judge phrase meaningfulness (Questions 1 and 2 above) and the second to judge entailment (Question 3 above).</S>
			<S sid ="63" ssid = "10">In Task 1, the LHS extractions and RHS instantiations of all rule applications are separated and presented to different Turkers independently of one another.</S>
			<S sid ="64" ssid = "11">This task is simple, quick and cheap and allows Turkers to focus on the single aspect of judging phrase meaningfulness.</S>
			<S sid ="65" ssid = "12">Rule applications for which both the LHS extraction and RHS instantiation are judged as meaningful are passed to Task 2, where Turkers need to decide whether a given rule application is valid.</S>
			<S sid ="66" ssid = "13">If not for Task 1, Turkers would need to distinguish in Task 2 between non-entailment due to (1) an incorrect rule (2) a meaningless RHS instantiation (3) a meaningless LHS extraction.</S>
			<S sid ="67" ssid = "14">Thanks to Task 1, Turkers are presented in Task 2 with two meaningful phrases and need to decide only whether one entails the other.</S>
			<S sid ="68" ssid = "15">To ensure high quality output, each example is evaluated by three Turkers.</S>
			<S sid ="69" ssid = "16">Similarly to Mehdad et al.</S>
			<S sid ="70" ssid = "17">(2010) we only use results for which the confidence value provided by CF is greater than 70%.</S>
			<S sid ="71" ssid = "18">We now describe the details of both tasks.</S>
			<S sid ="72" ssid = "19">Our simplification contrasts with Szpektor et al.</S>
			<S sid ="73" ssid = "20">(2007), whose judgments for each rule application are similar to ours, but had to be performed simultaneously by annotators, which required substantial training.</S>
			<S sid ="74" ssid = "21">Task 1: Is the phrase meaningful?</S>
			<S sid ="75" ssid = "22">In keeping with the second principle above, the task description is made up of a short verbal explanation followed by positive and negative examples.</S>
			<S sid ="76" ssid = "23">The definition of “meaningfulness” is conveyed via examples pointing to properties of the automatic phrase extraction process, as seen in Table 1.</S>
			<S sid ="77" ssid = "24">Task 2: Judge if one phrase is true given another.</S>
			<S sid ="78" ssid = "25">ment.</S>
			<S sid ="79" ssid = "26">The challenge is to communicate the definition of “entailment” to Turkers.</S>
			<S sid ="80" ssid = "27">To that end the task description begins with a short explanation followed by “easy” and “hard” examples with explanations, covering a variety of positive and negative entail- ment “types” (Table 2).</S>
			<S sid ="81" ssid = "28">Defining “entailment” is quite difficult when dealing with expert annotators and still more with non- experts, as was noted by Negri et al.</S>
			<S sid ="82" ssid = "29">(2011).</S>
			<S sid ="83" ssid = "30">We therefore employ several additional mechanisms to get the definition of entailment across to Turkers and increase agreement with the GS.</S>
			<S sid ="84" ssid = "31">We run an initial small test run and use its output to improve annotation in two ways: First, we take examples that were “confusing” for Turkers and add them to the GS with explanatory feedback presented when a Turker answers incorrectly.</S>
			<S sid ="85" ssid = "32">(E.g., the pair (‘The owner be happy to help drivers’, ‘The owner assist drivers’) was judged as entailing in the test run but only achieved a confidence value of 0.53).</S>
			<S sid ="86" ssid = "33">Second, we add examples that were annotated unanimously by Turkers to the GS to increase its size, allowing CF to better estimate Turker’s reliability (following CF recommendations, we aim to have around 10% GS examples in every run).</S>
			<S sid ="87" ssid = "34">In Section 4 we show that these mechanisms improved annotation quality.</S>
	</SECTION>
	<SECTION title="Case. " number = "4">
			<S sid ="88" ssid = "1">Study As a case study, we used our evaluation methodology to compare four methods for learning entailment rules between predicates: DIRT (Lin and Pantel, 2001), Cover (Weeds and Weir, 2003), BInc (Szpek- tor and Dagan, 2008) and Berant et al.</S>
			<S sid ="89" ssid = "2">(2010).</S>
			<S sid ="90" ssid = "3">To that end, we applied the methods on a set of one billion extractions (generously provided by Fader et al.</S>
			<S sid ="91" ssid = "4">(2011)) automatically extracted from the ClueWeb09 web crawl2, where each extraction comprises a predicate and two arguments.</S>
			<S sid ="92" ssid = "5">This resulted in four learned inference rule resources.</S>
			<S sid ="93" ssid = "6">As mentioned, rule applications for which both sides were judged as meaningful are evaluated for entail 2 http://lemurproject.org/clueweb09.php/ Ex am ple En tail ed Ex pla nat ion giv en to Tur ker s LH S: Th e law yer sig n the co ntr act RH S: Th e law yer rea d the co ntr act Yes Th ere is a cha nce the law yer has not rea d the con trac t, but mo st like ly that as he sig ned it, he mu st hav e rea d it.</S>
			<S sid ="94" ssid = "7">LH S: Joh n be rela ted to Jer ry RH S: Joh n be a clo se rela tive of Jer ry No Th e LH S can be und erst ood fro m the RH S, but not the oth er wa y aro und as the LH S is mo re ge ner al. LH S: Wo me n be at incr eas ed ris k of ca nc er RH S: Wo me n die of ca nc er No Alt ho ug h the RH S is cor rec t, it can not be und erst ood fro m the LH S. Table 2: Examples given in the description of Task 2.</S>
			<S sid ="95" ssid = "8">We randomly sampled 5,000 extractions, and for each one sampled four rules whose LHS matches the extraction from the union of the learned resources.</S>
			<S sid ="96" ssid = "9">We then applied the rules, which resulted in 20,000 rule applications.</S>
			<S sid ="97" ssid = "10">We annotated rule applications using our methodology and evaluated each learning method by comparing the rules learned by each method with the annotation generated by CF.</S>
			<S sid ="98" ssid = "11">In Task 1, 281 rule applications were annotated as meaningless LHS extraction, and 1,012 were annotated as meaningful LHS extraction but meaningless RHS instantiation and so automatically annotated as non-entailment.</S>
			<S sid ="99" ssid = "12">8,264 rule applications were passed on to Task 2, as both sides were judged meaningful (the remaining 10,443 discarded due to low CF confidence).</S>
			<S sid ="100" ssid = "13">In Task 2, 5,555 rule applications were judged with a high confidence and supplied as output, 2,447 of them as positive entailment and 3,108 as negative.</S>
			<S sid ="101" ssid = "14">Overall, 6,567 rule applications (dataset of this paper) were annotated for a total cost of $1000.</S>
			<S sid ="102" ssid = "15">The annotation process took about one week.</S>
			<S sid ="103" ssid = "16">In tests run during development we experimented with Task 2 wording and GS examples, seeking to make the definition of entailment as clear as possible.</S>
			<S sid ="104" ssid = "17">To do so we randomly sampled and manually annotated 200 rule applications (from the initial 20,000), and had Turkers judge them.</S>
			<S sid ="105" ssid = "18">In our initial test, Turkers tended to answer “yes” comparing to our own annotation, with 0.79 agreement between their annotation and ours, corresponding to a kappa score of 0.54.</S>
			<S sid ="106" ssid = "19">After applying the mechanisms described in Section 3, false-positive rate was reduced from 18% to 6% while false-negative rate only increased from 4% to 5%, corresponding to a high agreement of 0.9 and kappa of 0.79.</S>
			<S sid ="107" ssid = "20">In our test, 63% of the 200 rule applications were annotated unanimously by the Turkers.</S>
			<S sid ="108" ssid = "21">Importantly, all these examples were in perfect agreement with our own annotation, reflecting their high reliability.</S>
			<S sid ="109" ssid = "22">For the purpose of evaluating the resources learned by the algorithms we used annotations with CF confidence ≥ 0.7 for which kappa is 0.99.</S>
			<S sid ="110" ssid = "23">Lastly, we computed the area under the recall- precision curve (AUC) for DIRT, Cover, BInc and Berant et al.’s method, resulting in an AUC of 0.4, 0.43, 0.44, and 0.52 respectively.</S>
			<S sid ="111" ssid = "24">We used the AUC curve, with number of recall-precision points in the order of thousands, to avoid tuning a threshold parameter.</S>
			<S sid ="112" ssid = "25">Overall, we demonstrated that our evaluation framework allowed us to compare four different learning methods in low costs and within one week.</S>
	</SECTION>
	<SECTION title="Discussion. " number = "5">
			<S sid ="113" ssid = "1">In this paper we have suggested a crowdsourcing framework for evaluating inference rules.</S>
			<S sid ="114" ssid = "2">We have shown that by simplifying the previously-proposed instance-based evaluation framework we are able to take advantage of crowdsourcing services to replace trained expert annotators, resulting in good quality large scale annotations, for reasonable time and cost.</S>
			<S sid ="115" ssid = "3">We have presented the methodological principles we developed to get the entailment decision across to Turkers, achieving very high agreement both with our annotations and between the annotators themselves.</S>
			<S sid ="116" ssid = "4">Using the CrowdFlower forms we provide with this paper, the proposed methodology can be beneficial for both resource developers evaluating their output as well as inference system developers wanting to assess the quality of existing resources.</S>
	</SECTION>
	<SECTION title="Acknowledgments">
			<S sid ="117" ssid = "5">This work was partially supported by the IsraelScience Foundation grant 1112/08, the PASCAL 2 Network of Excellence of the European Com-.</S>
			<S sid ="118" ssid = "6">munity FP7ICT-20071-216886, and the European Communitys Seventh Framework Programme (FP7/20072013) under grant agreement no. 287923 (EXCITEMENT).</S>
	</SECTION>
</PAPER>
