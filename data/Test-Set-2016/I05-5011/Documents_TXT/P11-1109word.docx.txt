Extracting Paraphrases from Definition Sentences on the Web

Chikara Hashimoto∗	Kentaro Torisawa†	Stijn De Saeger‡
Jun’ichi Kazama§	Sadao Kurohashi
∗ † ‡ § National Institute of Information and Communications Technology
Kyoto, 619-0237, JAPAN
∗  Graduate School of Informatics, Kyoto University
Kyoto, 606-8501, JAPAN
{∗ ch,† torisawa, ‡  stijn,§ kazama}@nict.go.jp
 kuro@i.kyoto-u.ac.jp





Abstract

We propose an automatic method of extracting 
paraphrases from definition sentences, which 
are also automatically acquired from the Web. 
We observe that a huge number of concepts 
are defined in Web documents, and that the 
sentences that define the same concept tend 
to convey mostly the same information using 
different expressions and thus contain many 
paraphrases.   We show that a large number 
of paraphrases can be automatically extracted 
with high precision by regarding the sentences


concept.  The method is based on our 
observation that two sentences defining the 
same concept can be regarded as a parallel 
corpus since they largely convey the same 
information using different expres- sions. Such 
definition sentences abound on the Web. This 
suggests that we may be able to extract a large 
amount of phrasal paraphrase knowledge from 
the definition sentences on the Web.
  For instance, the following two sentences, 
both of which define the same concept 
“osteoporosis”, in- clude two pairs of phrasal 
paraphrases, which are


that define the same concept as parallel cor-
pora. Experimental results indicated that with


indicated by underlines 01


and 02  , respectively.


our method it was possible to extract about


(1)   a. Osteoporosis is a disease that 01


decreases the


300,000 paraphrases from 6 × 108 Web docu-


quantity of bone and 02   makes bones fragile.


ments with a precision rate of about 94%.


b. Osteoporosis is a disease that 01


reduces bone




1   Introduction

Natural language allows us to express the same in- 
formation in many ways, which makes natural lan- 
guage processing (NLP) a challenging area.  Ac- 
cordingly, many researchers have recognized that 
automatic paraphrasing is an indispensable compo- 
nent of intelligent NLP systems (Iordanskaja et al.,
1991; McKeown et al., 2002; Lin and Pantel, 2001; 
Ravichandran and Hovy, 2002; Kauchak and Barzi- 
lay, 2006; Callison-Burch et al., 2006) and have tried 
to acquire a large amount of paraphrase knowledge, 
which is a key to achieving robust automatic para- 
phrasing, from corpora (Lin and Pantel, 2001; Barzi- 
lay and McKeown, 2001; Shinyama et al., 2002; 
Barzilay and Lee, 2003).
  We propose a method to extract phrasal para- 
phrases from pairs of sentences that define the same
1087


mass and 02   increases the risk of bone fracture.

  We define paraphrase as a pair of expressions be- 
tween which entailment relations of both directions 
hold. (Androutsopoulos and Malakasiotis, 2010).
  Our objective is to extract phrasal paraphrases 
from pairs of sentences that define the same con- 
cept. We propose a supervised method that exploits 
various kinds of lexical similarity features and con- 
textual features. Sentences defining certain concepts 
are acquired automatically on a large scale from the 
Web by applying a quite simple supervised method.
  Previous  methods  most  relevant  to  our  work 
used parallel corpora such as multiple translations of 
the same source text (Barzilay and McKeown,
2001) or automatically acquired parallel news texts 
(Shinyama et al., 2002; Barzilay and Lee, 2003; 
Dolan et al., 2004).  The former requires a large 
amount of manual labor to translate the same texts




Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1087–1097, 
Portland, Oregon, June 19-24, 2011. Qc 2011 Association for Computational Linguistics


in several ways.  The latter would suffer from the 
fact that it is not easy to automatically retrieve large 
bodies of parallel news text with high accuracy. On 
the contrary,  recognizing definition sentences for 
the same concept is quite an easy task at least for 
Japanese, as we will show, and we were able to find 
a huge amount of definition sentence pairs from nor- 
mal Web texts. In our experiments, about 30 million
definition sentence pairs were extracted from 6×108
Web documents, and the estimated number of para-
phrases recognized in the definition sentences using 
our method was about 300,000, for a precision rate 
of about 94%. Also, our experimental results show 
that our method is superior to well-known compet- 
ing methods (Barzilay and McKeown, 2001; Koehn 
et al., 2007) for extracting paraphrases from defini- 
tion sentence pairs.
  Our evaluation is based on bidirectional check- 
ing of entailment relations between paraphrases that 
considers the context dependence of a paraphrase.
  Note that using definition sentences is only the 
beginning of our research on paraphrase extraction. 
We have a more general hypothesis that sentences 
fulfilling the same pragmatic function (e.g.  defini- 
tion) for the same topic (e.g.  osteoporosis) convey 
mostly the same information using different expres- 
sions. Such functions other than definition may in- 
clude the usage of the same Linux command, the 
recipe for the same cuisine, or the description of re- 
lated work on the same research issue.
  Section 2 describes related works.   Section 3 
presents our proposed method. Section 4 reports on 
evaluation results. Section 5 concludes the paper.

2   Related Work

The existing work for paraphrase extraction is cat- 
egorized into two groups.  The first involves a dis- 
tributional similarity approach pioneered by Lin and 
Pantel (2001). Basically, this approach assumes that 
two expressions that have a large distributional simi- 
larity are paraphrases. There are also variants of this 
approach that address entailment acquisition (Geffet 
and Dagan, 2005; Bhagat et al., 2007; Szpektor and 
Dagan, 2008; Hashimoto et al., 2009). These meth- 
ods can be applied to a normal monolingual corpus, 
and it has been shown that a large number of para- 
phrases or entailment rules could be extracted. How-


ever, the precision of these methods has been rela- 
tively low. This is due to the fact that the evidence, 
i.e., distributional similarity, is just indirect evidence 
of paraphrase/entailment. Accordingly, these meth- 
ods occasionally mistake antonymous pairs for para- 
phrases/entailment pairs, since an expression and its 
antonymous counterpart are also likely to have a 
large distributional similarity. Another limitation of 
these methods is that they can find only paraphrases 
consisting of frequently observed expressions since 
they must have reliable distributional similarity val- 
ues for expressions that constitute paraphrases.
The second category is a parallel corpus approach
(Barzilay and McKeown, 2001;  Shinyama et al.,
2002; Barzilay and Lee, 2003; Dolan et al., 2004). 
Our method belongs to this category. This approach 
aligns expressions between two sentences in par- 
allel corpora, based on, for example, the overlap 
of words/contexts. The aligned expressions are as- 
sumed to be paraphrases. In this approach, the ex- 
pressions do not need to appear frequently in the 
corpora. Furthermore, the approach rarely mistakes 
antonymous pairs for paraphrases/entailment pairs. 
However, its limitation is the difficulty in preparing 
a large amount of parallel corpora, as noted before. 
We avoid this by using definition sentences, which 
can be easily acquired on a large scale from the Web, 
as parallel corpora.
  Murata et al. (2004) used definition sentences in 
two manually compiled dictionaries, which are con- 
siderably fewer in the number of definition sen- 
tences than those on the Web. Thus, the coverage of 
their method should be quite limited. Furthermore, 
the precision of their method is much poorer than 
ours as we report in Section 4.
  For a more extensive survey on paraphrasing 
methods, see Androutsopoulos and Malakasiotis 
(2010) and Madnani and Dorr (2010).

3   Proposed method

Our method, targeting the Japanese language, con- 
sists of two steps:  definition sentence acquisition 
and paraphrase extraction. We describe them below.

3.1   Definition sentence acquisition

We acquire sentences that define a concept (defini- 
tion sentences) as in Example (2), which defines “骨


粗鬆症” (osteoporosis), from the 6 × 108 Web pages
(Akamine et al., 2010) and the Japanese Wikipedia.

(2) 骨粗鬆症とは、骨がもろくなってしまう病気だ。
(Osteoporosis is a disease that makes bones fragile.)

  Fujii and Ishikawa (2002) developed an unsuper- 
vised method to find definition sentences from the 
Web using 18 sentential templates and a language 
model constructed from an encyclopedia.  On the 
other hand, we developed a supervised method to 
achieve a higher precision.
  We use one sentential template and an SVM clas- 
sifier.  Specifically, we first collect definition sen- 
tence candidates by a template “ˆNP とは.*”, where
ˆ is the beginning of sentence and NP is the noun 
phrase expressing the concept to be defined followed 
by a particle sequence, “と” (comitative) and “は” 
(topic) (and optionally followed by comma), as ex- 
emplified in (2). As a result, we collected 3,027,101 
sentences.   Although the particle sequence tends 
to mark the topic of the definition sentence, it can 
also appear in interrogative sentences and normal as- 
sertive sentences in which a topic is strongly empha- 
sized. To remove such non-definition sentences, we 
classify the candidate sentences using an SVM clas- 
sifier with a polynominal kernel (d  = 2).1    Since 
Japanese is a head-final language and we can judge 
whether a sentence is interrogative or not from the 
last words in the sentence, we included morpheme 
N -grams and bag-of-words (with the window size 
of N ) at the end of sentences in the feature set. The


92.2, and 91.4, respectively.   Using the classifier, 
we acquired 1,925,052 positive sentences from all of 
the collected sentences.  After adding definition 
sentences from Wikipedia articles, which are typi- 
cally the first sentence of the body of each article 
(Kazama and Torisawa, 2007), we obtained a total 
of 2,141,878 definition sentence candidates, which 
covered 867,321 concepts ranging from weapons to 
rules of baseball.  Then, we coupled two definition 
sentences whose defined concepts were the same 
and obtained 29,661,812 definition sentence pairs.
  Obviously, our method is tailored to Japanese. For 
a language-independent method of definition acqui- 
sition, see Navigli and Velardi (2010) as an example.

3.2   Paraphrase extraction
Paraphrase extraction proceeds as follows.  First, 
each sentence in a pair is parsed by the depen- 
dency parser KNP2  and dependency tree frag- 
ments that constitute linguistically well-formed con- 
stituents are extracted.  The extracted dependency 
tree fragments are called candidate phrases here- 
after.  We restricted candidate phrases to predicate 
phrases that consist of at least one dependency re- 
lation, do not contain demonstratives, and in which 
all the leaf nodes are nominal and all of the con- 
stituents are consecutive in the sentence. KNP indi- 
cates whether each candidate phrase is a predicate 
based on the POS of the head morpheme.  Then, 
we check all the pairs of candidate phrases between 
two definition sentences to find paraphrase pairs.3
In (1), repeated in (3), candidate phrase pairs to be


features are also useful for confirming that the head


checked include (01


decreases the quantity of bone,


verb is in the present tense, which definition sen-


01     reduces bone mass), (01


decreases the quantity


tences should be.   Also, we added the morpheme


of bone, 02


increases the risk of bone fracture), (02


N -grams and bag-of-words right after the particle
sequence in the feature set since we observe that 
non-definition sentences tend to have interrogative
related words like “何” (what) or “一体” ((what) on


makes bones fragile, 01
makes bones fragile, 02
fracture).


reduces bone mass), and (02
increases the risk of bone


earth) right after the particle sequence. We chose 5


(3)   a. Osteoporosis is a disease that 01


decreases the


quantity of bone and 02   makes bones fragile.


as N from our preliminary experiments.


b. Osteoporosis is a disease that 01


reduces bone


Our training data was constructed from 2,911 sen-
tences randomly sampled from all of the collected 
sentences. 61.1% of them were labeled as positive. 
In the 10-fold cross validation, the classifier’s ac- 
curacy, precision, recall, and F1 were 89.4, 90.7,

  1 We  use  SVMlight    available  at  http://svmlight. 
joachims.org/.


mass and 02   increases the risk of bone fracture.
2 http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/knp.html.
    3 Our method discards candidate phrase pairs in which one 
subsumes the other in terms of their character string, or the dif- 
ference is only one proper noun like “toner cartridges that Ap- 
ple Inc. made” and “toner cartridges that Xerox made.” Proper 
nouns are recognized by KNP.



f1
The 
ratio 
of 
the 
num
ber 
of 
mor
phe
mes 
shar
ed 
betw
een 
two 
cand
idate 
phra
ses 
to 
the 
num
ber 
of 
all 
of 
the 
mor
phe
mes 
in 
the 
two 
phra
ses.
f2
The 
ratio 
of 
the 
num
ber 
of a 
cand
idate 
phra
se’s 
mor
phe
mes, 
for 
whic
h 
ther
e is 
a 
mor
phe
me 
with 
smal
l 
edit 
dista
nce 
(1 in 
our 
expe
rime
nt) 
in
ano
ther 
can
did
ate 
phr
ase, 
to 
the 
nu
mb
er 
of 
all 
of 
the 
mo
rph
em
es 
in 
the 
two 
phr
ase
s. 
Not
e 
that 
Jap
ane
se 
has 
ma
ny 
ort
hog
rap
hic
al 
vari
atio
ns 
and 
edit 
dist
anc
e is 
use
ful 
for 
ide
ntif
yin
g 
the
m.
f3
The 
ratio 
of 
the 
num
ber 
of a 
cand
idate 
phra
se’s 
mor
phe
mes, 
for 
whic
h 
ther
e is 
a 
mor
phe
me 
with 
the 
sam
e 
pron
unci
atio
n in 
anot
her 
cand
idate
phr
ase, 
to 
the 
nu
mb
er 
of 
all 
of 
the 
mo
rph
em
es 
in 
the 
two 
phr
ase
s.   
Pro
nun
ciat
ion 
is 
als
o 
use
ful 
for 
ide
ntif
yin
g 
ort
hog
rap
hic 
vari
atio
ns. 
Pro
nun
ciat
ion 
is 
giv
en 
by 
KN
P.
f4
The 
ratio 
of 
the 
num
ber 
of 
mor
phe
mes 
of a 
shor
ter 
cand
idate 
phra
se to 
that 
of a 
long
er 
one.
f5
The 
iden
tity 
of 
the 
infle
cted 
form 
of 
the 
head 
mor
phe
me 
betw
een 
two 
cand
idate 
phra
ses: 
1 if 
they 
are 
iden
tical, 
0 
othe
rwis
e.
f6
The 
iden
tity 
of 
the 
POS 
of 
the 
head 
mor
phe
me 
betw
een 
two 
cand
idate 
phra
ses: 
1 or 
0.
f7
The 
iden
tity 
of 
the 
infle
ctio
n 
(con
juga
tion) 
of 
the 
head 
mor
phe
me 
betw
een 
two 
cand
idate 
phra
ses: 
1 or 
0.
f8
The 
ratio 
of 
the 
num
ber 
of 
mor
phe
mes 
that 
appe
ar in 
a 
cand
idate 
phra
se 
seg
men
t of 
a 
defi
nitio
n 
sent
ence 
s1  
and 
in a 
seg
men
t 
that 
is 
NO
T a
part 
of 
the 
can
did
ate 
phr
ase 
of 
ano
ther 
defi
niti
on 
sen
ten
ce 
s2  
to 
the 
nu
mb
er 
of 
all 
of 
the 
mo
rph
em
es 
of 
s1 ’s 
can
did
ate 
phr
ase, 
i.e. 
ho
w 
ma
ny 
extr
a 
mo
rph
em
es 
are 
inc
orp
orat
ed 
into 
s1 ’s 
can
did
ate 
phr
ase.
f9
The 
reve
rsed 
(s1  
↔ 
s2 ) 
versi
on 
of 
f8.
f10
The 
ratio 
of 
the 
num
ber 
of 
pare
nt 
depe
nden
cy 
tree 
frag
men
ts 
that 
are 
shar
ed 
by 
two 
cand
idate 
phra
ses 
to 
the 
num
ber 
of 
all 
of 
the 
pare
nt 
de-
pend
ency 
tree 
frag
men
ts of 
the 
two 
phra
ses. 
Dep
ende
ncy 
tree 
frag
men
ts 
are 
repr
esen
ted 
by 
the 
pron
unci
atio
n of 
their 
com
pone
nt 
mor
phe
mes.
f11
A 
varia
tion 
of 
f10; 
tree 
frag
men
ts 
are 
repr
esen
ted 
by 
the 
base 
form 
of 
their 
com
pone
nt 
mor
phe
mes.
f12
A 
varia
tion 
of 
f10; 
tree 
frag
men
ts 
are 
repr
esen
ted 
by 
the 
POS 
of 
their 
com
pone
nt 
mor
phe
mes.
f13
The 
ratio 
of 
the 
num
ber 
of 
unig
rams 
(mor
phe
mes) 
that 
appe
ar in 
the 
chil
d 
cont
ext 
of 
both 
cand
idate 
phra
ses 
to 
the 
num
ber 
of 
all 
of 
the 
chil
d
cont
ext 
mor
phe
mes 
of 
both 
cand
idate 
phra
ses. 
Uni
gra
ms 
are 
repr
esen
ted 
by 
the 
pron
unci
atio
n of 
the 
mor
phe
me.
f14
A 
varia
tion 
of 
f13; 
unig
rams 
are 
repr
esen
ted 
by 
the 
base 
form 
of 
the 
mor
phe
me.
f15
A 
varia
tion 
of 
f14; 
the 
num
erat
or is 
the 
num
ber 
of 
chil
d 
cont
ext 
unig
rams 
that 
are 
adja
cent 
to 
both 
cand
idate 
phra
ses.
f16
The 
ratio 
of 
the 
num
ber 
of 
trigr
ams 
that 
appe
ar in 
the 
chil
d 
cont
ext 
of 
both 
cand
idate 
phra
ses 
to 
the 
num
ber 
of 
all 
of 
the 
chil
d 
cont
ext
mor
phe
mes 
of 
both 
cand
idate 
phra
ses. 
Trig
rams 
are 
repr
esen
ted 
by 
the 
pron
unci
atio
n of 
the 
mor
phe
me.
f17
Cosi
ne 
simi
larit
y 
betw
een 
two 
defi
nitio
n 
sent
ence
s 
from 
whic
h a 
cand
idate 
phra
se 
pair 
is 
extr
acte
d.

Table 1: Features used by paraphrase classifier.




  The paraphrase checking of candidate phrase 
pairs is performed by an SVM classifier with a linear 
kernel that classifies each pair of candidate phrases 
into a paraphrase or a non-paraphrase.4   Candidate 
phrase pairs are ranked by their distance from the 
SVM’s hyperplane.  Features for the classifier are 
based on our observation that two candidate phrases 
tend to be paraphrases if the candidate phrases them- 
selves are sufficiently similar and/or their surround- 
ing contexts are sufficiently similar. Table 1 lists the 
features used by the classifier.5 Basically, they rep- 
resent either the similarity of candidate phrases (f1-
9) or that of their contexts (f10-17).  We think that 
they have various degrees of discriminative power, 
and thus we use the SVM to adjust their weights. 
Figure 1 illustrates features f8-12, for which you 
may need supplemental remarks. English is used for 
ease of explanation. In the figure, f8 has a positive 
value since the candidate phrase of s1  contains mor- 
phemes “of bone”, which do not appear in the can-


  4 We  use  SVMperf    available  at  http://svmlight. 
joachims.org/svm perf.html.


 

Figure 1: Illustration of features f8-12.



didate phrase of s2  but do appear in the other part 
of s2 , i.e. they are extra morphemes for s1’s candi- 
date phrase. On the other hand, f9 is zero since there 
is no such extra morpheme in s2’s candidate phrase. 
Also, features f10-12 have positive values since the 
two candidate phrases share two parent dependency 
tree fragments, (that increases) and (of fracture).
  We have also tried the following features, which 
we do not detail due to space limitation:  the sim- 
ilarity of candidate phrases based on semantically 
similar nouns (Kazama and Torisawa, 2008), entail- 
ing/entailed verbs (Hashimoto et al., 2009), and the 
identity of the pronunciation and base form of the


5 In the table, the parent context of a candidate phrase con-
sists of expressions that appear in ancestor nodes of the candi-


head morpheme;


N -grams (N


=1,2,3) of child and


date phrase in terms of the dependency structure of the sentence. 
Child contexts are defined similarly.


parent contexts represented by either the inflected
form, base form, pronunciation, or POS of mor-


 	Original definition sentence pair (s1 , s2 ) 	Paraphrased definition sentence pair (sf    , sf  )


s1 : Osteoporosis is a disease that reduces bone mass and makes bones 
fragile.
s2 : Osteoporosis is a disease that decreases the quantity of bone and 
increases the risk of bone fracture.


sf : Osteoporosis is a disease that decreases the quantity of bone and
makes bones fragile.
sf : Osteoporosis is a disease that reduces bone mass and increases
the risk of bone fracture.



Figure 2: Bidirectional checking of entailment relation (→) of p1  → p2  and p2  → p1 . p1  is “reduces bone mass”
in s1  and p2  is “decreases the quantity of bone” in s2 .  p1  and p2  are exchanged between s1  and s2  to generate


corresponding paraphrased sentences sf


and sf . p1  → p2  (p2  → p1 ) is verified if 
s1  → sf


(s2  → sf ) holds. In this


1 	2 	1 	2
case, both of them hold. English is used for ease of explanation.




pheme; parent/child dependency tree fragments rep- 
resented by either the inflected form, base form, pro- 
nunciation, or POS; adjacent versions (cf.  f15) of 
N -gram features and parent/child dependency tree


texts. The labeling process is as follows. First, from 
each candidate phrase pair (p1, p2) and its source 
definition sentence pair (s1, s2), we create two para-
phrase sentence pairs (sf , sf ) by exchanging p1 and


1	2


features. These amount to 78 features, but we even-


p2 between s1  and s2.  Then, annotators check if s1


tually settled on the 17 features in Table 1 through


entails sf


and s2  entails sf


so that entailment rela-


ablation tests to evaluate the discriminative power 
of each feature.
  The ablation tests were conducted using training 
data that we prepared. In preparing the training data, 
we faced the problem that the completely random 
sampling of candidate paraphrase pairs provided us 
with only a small number of positive examples. 
Thus, we automatically collected candidate para- 
phrase pairs that were expected to have a high like- 
lihood of being positive as examples to be labeled. 
The likelihood was calculated by simply summing 
all of the 78 feature values that we have tried, since 
they indicate the likelihood of a given candidate 
paraphrase pair’s being a paraphrase. Note that val-
ues of the features f8 and f9 are weighted with −1,
since they indicate the unlikelihood.  Specifically,
we first randomly sampled 30,000 definition sen- 
tence pairs from the 29,661,812 pairs, and collected
3,000 candidate phrase pairs that had the highest 
likelihood from them. The manual labeling of each 
candidate phrase pair (p1 , p2) was based on bidirec-
tional checking of entailment relation, p1 → p2 and
p2 → p1, with p1 and p2 embedded in contexts.


tions of both directions p1  → p2  and p2  → p1  are
checked. Figure 2 shows an example of bidirectional
checking. In this example, both entailment relations,
s1  → s1  and s2  → s2, hold, and thus the candidate
f	f
( 	, p ) is judged as positive. We used
phrase pair
p1 	2
(p1, p2), for which entailment relations of both di-
rections held, as positive examples (1,092 pairs) and 
the others as negative ones (1,872 pairs).6
  We built the paraphrase classifier from the train- 
ing data. As mentioned, candidate phrase pairs were 
ranked by the distance from the SVM’s hyperplane.

4	Experiment

In this paper, our claims are twofold.

I. Definition sentences on the Web are a treasure 
trove of paraphrase knowledge (Section 4.2).
II. Our method of paraphrase acquisition from 
definition sentences is more accurate than well- 
known competing methods (Section 4.1).

  We first verify claim II by comparing our method 
with that of Barzilay and McKeown (2001) (BM
7



This scheme is similar to the one proposed by


method),   Moses


(Koehn  et  al.,   2007)  (SMT


Szpektor et al. (2007). We adopt this scheme since 
paraphrase judgment might be unstable between an- 
notators unless they are given a particular context


method),  and  that  of  Murata  et  al.  (2004)  (Mrt
method). The first two methods are well known for 
accurately extracting semantically equivalent phrase
8


based on which they make a judgment.	As de-


pairs from parallel corpora.


Then, we verify claim


scribed below, we use definition sentences as con- 
texts. We admit that annotators might be biased by 
this in some unexpected way, but we believe that 
this is a more stable method than that without con-


6 The remaining 36 pairs were discarded as they contained
garbled characters of Japanese.
7 http://www.statmt.org/moses/
    8 As anonymous reviewers pointed out, they are unsuper- 
vised methods and thus unable to be adapted to definition sen-


I by comparing definition sentence pairs with sen- 
tence pairs that are acquired from the Web using Ya- 
hoo!JAPAN API9 as a paraphrase knowledge source. 
In the latter data set, two sentences of each pair 
are expected to be semantically similar regardless of 
whether they are definition sentences. Both sets con- 
tain 100,000 pairs.
  Three annotators (not the authors) checked evalu- 
ation samples. Fleiss’ kappa (Fleiss, 1971) was 0.69 
(substantial agreement (Landis and Koch, 1977)).


4.1   Our method vs. competing methods

In this experiment, paraphrase pairs are extracted 
from 100,000 definition sentence pairs that are ran- 
domly sampled from the 29,661,812 pairs.  Before 
reporting the experimental results, we briefly de- 
scribe the BM, SMT, and Mrt methods.

BM method   Given parallel sentences like multi- 
ple translations of the same source text, the BM 
method works iteratively as follows. First, it collects 
from the parallel sentences identical word pairs and 
their contexts (POS N -grams with indices indicat- 
ing corresponding words between paired contexts) 
as positive examples and those of different word 
pairs as negative ones. Then, each context is ranked 
based on the frequency with which it appears in pos- 
itive (negative) examples. The most likely K posi- 
tive (negative) contexts are used to extract positive 
(negative) paraphrases from the parallel sentences. 
Extracted positive (negative) paraphrases and their 
morpho-syntactic patterns are used to collect addi- 
tional positive (negative) contexts.  All the positive 
(negative) contexts are ranked, and additional para- 
phrases and their morpho-syntactic patterns are ex- 
tracted again.  This iterative process finishes if no 
further paraphrase is extracted or the number of iter- 
ations reaches a predefined threshold T . In this ex- 
periment, following Barzilay and McKeown (2001), 
K is 10 and N is 1 to 3. The value of T is not given 
in their paper. We chose 3 as its value based on our 
preliminary experiments. Note that paraphrases ex- 
tracted by this method are not ranked.

tences. Nevertheless, we believe that comparing these methods 
with ours is very informative, since they are known to be accu- 
rate and have been influential.
9 http://developer.yahoo.co.jp/webapi/


SMT method  Our SMT method uses Moses 
(Koehn et al., 2007) and extracts a phrase table, a 
set of two phrases that are translations of each other, 
given a set of two sentences that are translations of 
each other. If you give Moses monolingual parallel 
sentence pairs, it should extract a set of two phrases 
that are paraphrases of each other.  In this experi- 
ment, default values were used for all parameters. 
To rank extracted phrase pairs, we assigned each of 
them the product of two phrase translation probabil- 
ities of both directions that were given by Moses. 
For other SMT methods, see Quirk et al. (2004) and 
Bannard and Callison-Burch (2005) among others.

Mrt method  Murata et al. (2004) proposed a 
method to extract paraphrases from two manually 
compiled dictionaries. It simply regards a difference 
between two definition sentences of the same word 
as a paraphrase candidate. Paraphrase candidates are 
ranked according to an unsupervised scoring scheme 
that implements their assumption. They assume that 
a paraphrase candidate tends to be a valid paraphrase 
if it is surrounded by infrequent strings and/or if it 
appears multiple times in the data.

  In this experiment, we evaluated the unsupervised 
version of our method in addition to the supervised 
one described in Section 3.2, in order to compare 
it fairly with the other methods.  The unsupervised 
method works in the same way as the supervised 
one, except that it ranks candidate phrase pairs by 
the sum of all 17 feature values, instead of the dis- 
tance from the SVM’s hyperplane. In other words, 
no supervised learning is used. All the feature val- 
ues are weighted with 1, except for f8 and f9, which
are weighted with −1 since they indicate the unlike-
lihood of a candidate phrase pair being paraphrases.
BM, SMT, Mrt, and the two versions of our method 
were used to extract paraphrase pairs from the same
100,000 definition sentence pairs.

Evaluation scheme  Evaluation of each para- 
phrase  pair  (p1, p2)  was  based  on  bidirectional
checking of entailment relations p1 → p2 and p2 →
p1  in a way similar to the labeling of the training
data.  The difference is that contexts for evaluation 
are two sentences that are retrieved from the Web 
and contain p1  and p2 , instead of definition sen- 
tences from which p1  and p2  are extracted.   This



is intended to check whether extracted paraphrases 
are also valid for contexts other than those from 
which they are extracted.  The evaluation proceeds 
as follows. For the top m paraphrase pairs of each 
method (in the case of the BM method, randomly 
sampled m pairs were used, since the method does 
not rank paraphrase pairs), we retrieved a sentence 
pair (s1, s2 ) for each paraphrase pair (p1, p2) from 
the Web, such that s1 contains p1 and s2 contains p2. 
In doing so, we make sure that neither s1  nor s2  are 
the definition sentences from which p1  and p2  are 
extracted.  For each method, we randomly sample 
n samples from all of the paraphrase pairs (p1, p2) 
for which both s1  and s2  are retrieved. Then, from 
each (p1, p2) and (s1, s2), we create two paraphrase
sentence pairs (sf , sf ) by exchanging p1 and p2 be-



Defi
nitio
n 
sent
ence 
pair
s
Sup
Uns
B
M
SM
T
Mr
t
with 
trivi
al
1,381,42
4
24,0
49
9,56
2
18,1
84
with
out 
trivi
al
1,377,57
3
23,4
90
7,25
6
18,1
39
Web 
sent
ence 
pair
s
Sup
Uns
B
M
SM
T
Mr
t
with 
trivi
al
277,17
2
5,1
01
4,58
6
4,9
78
with
out 
trivi
al
274,72
0
4,3
99
2,34
2
4,9
58

Table 2: Number of extracted paraphrases.


(p1, p2) is regarded as trivial if the pronunciation is 
the same between p1  and p2 ,10  or all of the con- 
tent words contained in p1  are the same as those 
of p2.  Graph (b) gives a precision curve for each 
method. Again, Sup outperforms the others too, and 
maintains a precision rate of about 90% until the top
1,000. These results support our claim II.
The upper half of Table 2 shows the number of


1 	2


tween s1  and s2.   All samples, each consisting of
(p1, p2), (s1 , s2), and (sf , sf ), are checked by three


extracted paraphrases with/without trivial pairs for
each method.11  Sup and Uns extracted many more


1 	2


human annotators to determine whether s1  entails
s1  and s2  entails s2  so that entailment relations of
f	f
both directions are verified. In advance of evaluation 
annotation, all the evaluation samples are shuffled 
so that the annotators cannot find out which sample 
is given by which method for fairness.  We regard 
each paraphrase pair as correct if at least two annota- 
tors judge that entailment relations of both directions 
hold for it. You may wonder whether only one pair 
of sentences (s1, s2 ) is enough for evaluation since a 
correct (wrong) paraphrase pair might be judged as 
wrong (correct) accidentally. Nevertheless, we sup- 
pose that the final evaluation results are reliable if 
the number of evaluation samples is sufficient.  In 
this experiment, m is 5,000 and n is 200.  We use 
Yahoo!JAPAN API to retrieve sentences.
  Graph (a) in Figure 3 shows a precision curve 
for each method.   Sup and Uns respectively indi- 
cate the supervised and unsupervised versions of our 
method.  The figure indicates that Sup outperforms 
all the others and shows a high precision rate of 
about 94% at the top 1,000.   Remember that this 
is the result of using 100,000 definition sentence 
pairs. Thus, we estimate that Sup can extract about
300,000 paraphrase pairs with a precision rate of 
about 94%, if we use all 29,661,812 definition sen- 
tence pairs that we acquired.
  Furthermore, we measured precision after trivial 
paraphrase pairs were discarded from the evaluation 
samples of each method.  A candidate phrase pair


paraphrases. It is noteworthy that Sup performed the
best in terms of both precision rate and the number 
of extracted paraphrases.
  Table 3 shows examples of correct and incorrect 
outputs of Sup. As the examples indicate, many of 
the extracted paraphrases are not specific to defini- 
tion sentences and seem very reusable.  However, 
there are few paraphrases involving metaphors or id- 
ioms in the outputs due to the nature of definition 
sentences. In this regard, we do not claim that our 
method is almighty.  We agree with Sekine (2005) 
who claims that several different methods are re- 
quired to discover a wider variety of paraphrases.
  In graphs (a) and (b), the precision of the SMT 
method goes up as rank goes down. This strange be- 
havior is due to the scoring by Moses that worked 
poorly for the data; it gave 1.0 to 82.5% of all the 
samples, 38.8% of which were incorrect. We suspect 
SMT methods are poor at monolingual alignment for 
paraphrasing or entailment tasks since, in the tasks, 
data is much noisier than that used for SMT. See 
MacCartney et al. (2008) for similar discussion.

4.2	Definition pairs vs. Web sentence pairs

To collect Web sentence pairs, first, we randomly 
sampled 1.8 million sentences from the Web corpus.

  10 There are many kinds of orthographic variants in Japanese, 
which can be identified by their pronunciation.
11 We set no threshold for candidate phrase pairs of each
method, and counted all the candidate phrase pairs in Table 2.


1


0.8



’Sup_def’
’Uns_def’   	
’SMT_def’   	
’BM_def’   	
’Mrt_def’


1


0.8



’Sup_def_n’
’Uns_def_n’   	
’SMT_def_n’   	
’BM_def_n’
’Mrt_def_n’





0.6


0.6





0.4


0.4





0.2


0.2





0
0	1000	2000	3000	4000	5000
Top-N


0
0	1000	2000	3000	4000	5000
Top-N



(a) Definition sentence pairs with trivial paraphrases 	(b) Definition sentence pairs without trivial paraphrases



1


0.8



’Sup_www’
’Uns_www’   	
’SMT_www’   	
’BM_www’
’Mrt_www’



1


0.8



’Sup_www_n’
’Uns_www_n’
’SMT_www_n’
’BM_www_n’
’Mrt_www_n’





0.6


0.6





0.4


0.4





0.2


0.2





0
0	1000	2000	3000	4000	5000
Top-N


0
0	1000	2000	3000	4000	5000
Top-N



(c) Web sentence pairs with trivial paraphrases 	(d) Web sentence pairs without trivial paraphrases

Figure 3: Precision curves of paraphrase extraction.



Ra
nk
P
a
r
a
p
h
r
a
s
e
 
p
a
i
r











Corr
ect
1
3

1
9
7
0

1
1
2
6
5
6
8
4
1

9
2
9
1,
5
5
3
2,
2
4
3

2,
8
5
5
2,
9
3
1
3,
6
6
7

4,
8
7
0
5,
5
0
1

10,
67
5
112,
819

193,
553
メー
ルア
ドレ
スに
メー
ルを
送る 
(sen
d a 
mes
sage 
to 
the 
e-
mail 
addr
ess) 
⇔ 
メー
ルア
ドレ
スに
電子
メー
ルを
送る 
(sen
d
an 
e-
mail 
mes
sage 
to 
the 
e-
mail 
addr
ess)
お
客
様
の
依
頼
に
よ
る 
(req
uest
ed 
by a 
cust
ome
r) 
⇔ 
お
客
様
の
委
託
に
よ
る 
(co
mmi
ssio
ned 
by a 
cust
ome
r)
企
業
の
財
政
状
況
を
表
す 
(des
crib
e 
the 
fisca
l 
cond
ition 
of 
com
pany
) ⇔ 
企
業
の
財
政
状
態
を
示
す 
(indi
cate 
the 
fisca
l 
state
of 
com
pan
y)
イン
フォ
メー
ショ
ンを
得る 
(get 
infor
mati
on) 
⇔ 
ニ
ュ
ー
ス
を
得
る 
(get 
new
s)
き
ま
り
の
こ
と
で
す 
(it is 
a 
conv
enti
on) 
⇔ 
ル
ー
ル
の
こ
と
で
す 
(it is 
a 
rule)
地震
のエ
ネル
ギー
規模
をあ
らわ
す 
(repr
esen
t the 
ener
gy 
scal
e of 
eart
hqua
ke) 
⇔ 
地震
の規
模を
表す 
(repr
esen
t the 
scal
e
of 
eart
hqu
ake)
細
胞
を
酸
化
さ
せ
る 
(cau
se 
the 
oxid
atio
n of 
cells
) ⇔ 
細
胞
を
老
化
さ
せ
る 
(cau
se 
cellu
lar 
agin
g)
角
質
を
取
り
除
く 
(rem
ove 
dead 
skin 
cells
) ⇔ 
角
質
を
は
が
す 
(pee
l off 
dead 
skin 
cells
)
胎児
の発
育に
必要
だ 
(req
uire
d 
for 
the 
deve
lop
men
t of 
fetus
) ⇔ 
胎児
の発
育成
長に
必要
不可
欠だ 
(indi
spen
sabl
e 
for 
the
gro
wth 
and 
deve
lop
men
t of 
fetu
s)
視
力
を
矯
正
す
る 
(corr
ect 
eyes
ight) 
⇔ 
視
力
矯
正
を
行
う 
(perf
orm 
eyes
ight 
corr
ecti
on)
チ
ャ
ラ
に
し
て
も
ら
う 
(call 
it 
even
) ⇔ 
帳
消
し
に
し
て
も
ら
う 
(call 
it 
quit
s)
ハー
ドデ
ィス
ク上
に蓄
積さ
れる 
(acc
umu
lated 
on a 
hard 
disk
) ⇔ 
ハー
ドデ
ィス
クド
ライ
ブに
保存
され
る 
(stor
ed 
on a
hard 
disk 
driv
e)
有
害
物
質
を
排
泄
す
る 
(exc
rete 
har
mful 
subs
tanc
e) 
⇔ 
有
害
毒
素
を
排
出
す
る 
(disc
harg
e 
har
mful 
toxi
n)
１つ
のＣ
ＰＵ
の内
部に
２つ
のプ
ロセ
ッサ
コア
を搭
載す
る 
(mo
unt 
two 
proc
esso
r 
core
s on 
one 
CPU
) ⇔ 
１つ
のパ
ッケ
ー
ジに
２つ
のプ
ロセ
ッサ
コア
を集
積す
る 
(buil
d 
two 
proc
esso
r 
core
s 
into 
one 
pack
age)
外
貨
を
売
買
す
る 
(trad
e 
forei
gn 
curr
enci
es) 
⇔ 
通
貨
を
交
換
す
る 
(exc
hang
e 
one 
curr
ency 
for 
anot
her)
派
遣
先
企
業
の
社
員
に
な
る 
(bec
ome 
a 
regu
lar 
staff 
me
mbe
r of 
the 
com
pany 
whe
re 
(s)h
e 
has 
wor
ked 
as a 
tem
p) 
⇔ 
派
遣
先
に
直
接
雇
用
さ
れ
る 
(em
ploy
ed 
by 
the 
com
pany 
whe
re 
(s)h
e 
has 
wor
ked 
as a 
tem
p)
Ｗｅ
ｂサ
イト
にア
クセ
スす
る 
(acc
ess 
Web 
sites
) ⇔ 
ＷＷ
Ｗサ
イト
を訪
れる 
(visi
t 
WW
W 
sites
)

Inco
rrect
9
0
3
2,
5
3
0
3,
0
0
8
ブラ
ウザ
に送
信さ
れる 
(sen
d to 
a 
Web 
bro
wser
) ⇔ 
パソ
コン
に送
信さ
れる 
(sen
d to 
a 
PC)
調
和
を
は
か
る 
(inte
nd 
to 
bala
nce) 
⇔ 
リ
フ
レ
ッ
シ
ュ
を
図
る 
(inte
nd 
to 
refre
sh)
消化
酵素
では
消化
でき
ない 
(una
ble 
to 
dige
st 
with 
dige
stive 
enzy
mes) 
⇔ 
消化
酵素
で消
化さ
れ難
い 
(har
d to 
dige
st 
with
dige
stive 
enzy
mes
)

Table 3: Examples of correct and incorrect paraphrases extracted by our supervised method with their rank.


We call them sampled sentences.  Then, using Ya- 
hoo!JAPAN API, we retrieved up to 20 snippets rele- 
vant to each sampled sentence using all of the nouns 
in each sentence as a query. After that, each snippet 
was split into sentences, which we call snippet sen- 
tences. We paired a sampled sentence and a snippet 
sentence that was the most similar to the sampled 
sentence. Similarity is the number of nouns shared 
by the two sentences. Finally, we randomly sampled
100,000 pairs from all the pairs.

  Paraphrase pairs were extracted from the Web 
sentence pairs by using BM, SMT, Mrt and the su- 
pervised and unsupervised versions of our method. 
The features used with our methods were selected 
from all of the 78 features mentioned in Section 3.2 
so that they performed well for Web sentence pairs. 
Specifically, the features were selected by ablation 
tests using training data that was tailored to Web 
sentence pairs. The training data consisted of 2,741 
sentence pairs that were collected in the same way as 
the Web sentence pairs and was labeled in the same 
way as described in Section 3.2.
  Graph (c) of Figure 3 shows precision curves. We 
also measured precision without trivial pairs in the 
same way as the previous experiment.  Graph (d) 
shows the results. The lower half of Table 2 shows 
the number of extracted paraphrases with/without 
trivial pairs for each method.
  Note that precision figures of our methods in 
graphs (c) and (d) are lower than those of our meth- 
ods in graphs (a) and (b). Additionally, none of the 
methods achieved a precision rate of 90% using Web 
sentence pairs.12   We think that a precision rate of 
at least 90% would be necessary if you apply auto- 
matically extracted paraphrases to NLP tasks with- 
out manual annotation. Only the combination of Sup 
and definition sentence pairs achieved that precision.
  Also note that, for all of the methods, the numbers 
of extracted paraphrases from Web sentence pairs 
are fewer than those from definition sentence pairs.
  From all of these results, we conclude that our 
claim I is verified.


12 Precision of SMT is unexpectedly good.  We found some


5   Conclusion

We proposed a method of extracting paraphrases 
from definition sentences on the Web. From the ex- 
perimental results, we conclude that the following 
two claims of this paper are verified.

1. Definition sentences on the Web are a treasure 
trove of paraphrase knowledge.

2. Our method extracts many paraphrases from 
the definition sentences on the Web accurately; 
it can extract about 300,000 paraphrases from
6 × 108  Web documents with a precision rate
of about 94%.

  Our future work is threefold. First, we will release 
extracted paraphrases from all of the 29,661,812 
definition sentence pairs that we acquired, after hu- 
man annotators check their validity. The result will 
be available through the ALAGIN forum.13
  Second,  we  plan  to  induce  paraphrase  rules 
from paraphrase instances.    Though our method 
can extract a  variety of  paraphrase instances on 
a large scale, their coverage might be insufficient 
for real NLP applications since some paraphrase 
phenomena are highly productive.   Therefore, we 
need paraphrase rules in addition to paraphrase in- 
stances.  Barzilay and McKeown (2001) induced 
simple POS-based paraphrase rules from paraphrase 
instances, which can be a good starting point.
  Finally, as mentioned in Section 1, the work in 
this paper is only the beginning of our research on 
paraphrase extraction.  We are trying to extract far 
more paraphrases from a set of sentences fulfilling 
the same pragmatic function (e.g. definition) for the 
same topic (e.g.  osteoporosis) on the Web.  Such 
functions other than definition may include the us- 
age of the same Linux command, the recipe for the 
same cuisine, or the description of related work on 
the same research issue.

Acknowledgments

We would like to thank Atsushi Fujita, Francis 
Bond, and all of the members of the Information 
Analysis Laboratory, Universal Communication Re- 
search Institute at NICT.


Web sentence pairs consisting of two mostly identical sentences	 	


on rare occasions. The method worked relatively well for them.


13 http://alagin.jp/


References

Susumu Akamine, Daisuke Kawahara, Yoshikiyo Kato, 
Tetsuji Nakagawa, Yutaka I. Leon-Suematsu, Takuya 
Kawada, Kentaro Inui, Sadao Kurohashi, and Yutaka 
Kidawara. 2010. Organizing information on the web 
to support user judgments on information credibil- 
ity.   In Proceedings of 2010 4th International Uni- 
versal Communication Symposium Proceedings (IUCS
2010), pages 122–129.
Ion   Androutsopoulos   and   Prodromos   Malakasiotis.
2010. A survey of paraphrasing and textual entailment 
methods.  Journal of Artificial Intelligence Research,
38:135–187.
Colin Bannard and Chris Callison-Burch.  2005.  Para- 
phrasing with bilingual parallel corpora.  In Proceed- 
ings of the 43rd Annual Meeting of the Association for 
Computational Linguistics (ACL-2005), pages 597–
604.
Regina Barzilay and Lillian Lee.   2003.   Learning to 
paraphrase: An unsupervised approach using multiple- 
sequence alignment.  In Proceedings of HLT-NAACL
2003, pages 16–23.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex- 
tracting paraphrases from a parallel corpus.  In Pro- 
ceedings of the 39th Annual Meeting of the ACL joint 
with the 10th Meeting of the European Chapter of the 
ACL (ACL/EACL 2001), pages 50–57.
Rahul Bhagat, Patrick Pantel, and Eduard Hovy.  2007.
Ledir: An unsupervised algorithm for learning direc- 
tionality of inference rules. In Proceedings of Confer- 
ence on Empirical Methods in Natural Language Pro- 
cessing (EMNLP2007), pages 161–170.
Chris Callison-Burch, Philipp Koehn, and Miles Os- 
borne. 2006. Improved statistical machine translation 
using paraphrases. In Proceedings of the 2006 Human 
Language Technology Conference of the North Ameri- 
can Chapter of the Association for Computational Lin- 
guistics (HLT-NAACL 2006), pages 17–24.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un- 
supervised construction of large paraphrase corpora: 
exploiting massively parallel news sources.   In Pro- 
ceedings of the 20th international conference on Com- 
putational Linguistics (COLING 2004), pages 350–
356.
Joseph L. Fleiss. 1971. Measuring nominal scale agree- 
ment among many raters.    Psychological Bulletin,
76(5):378–382.
Atsushi Fujii and Tetsuya Ishikawa.  2002.  Extraction 
and organization of encyclopedic knowledge informa- 
tion using the World Wide Web (written in Japanese). 
Institute of Electronics, Information, and Communica- 
tion Engineers, J85-D-II(2):300–307.


Maayan Geffet and Ido Dagan. 2005. The distributional 
inclusion hypotheses and lexical entailment.  In Pro- 
ceedings of the 43rd Annual Meeting of the Associa- 
tion for Computational Linguistics (ACL 2005), pages
107–114.
Chikara Hashimoto, Kentaro Torisawa, Kow Kuroda, 
Stijn De Saeger, Masaki Murata, and Jun’ichi Kazama.
2009.   Large-scale verb entailment acquisition from 
the web.  In Proceedings of the 2009 Conference on 
Empirical Methods in Natural Language Processing 
(EMNLP 2009), pages 1172–1181.
Lidija Iordanskaja, Richard Kittredge, and Alain 
Polgue`re.  1991.  Lexical selection and paraphrase in 
a meaning-text generation model.  In Ce´cile L. Paris, 
William R. Swartout, and William C. Mann, editors, 
Natural language generation in artificial intelligence 
and computational linguistics, pages 293–312. Kluwer 
Academic Press.
David Kauchak and Regina Barzilay.   2006.   Para- 
phrasing for automatic evaluation.  In Proceedings of 
the 2006 Human Language Technology Conference of 
the North American Chapter of the Association for 
Computational Linguistics (HLT-NAACL 2006), pages
455–462.
Jun’ichi Kazama and Kentaro Torisawa. 2007. Exploit- 
ing Wikipedia as external knowledge for named entity 
recognition. In Proceedings of the 2007 Joint Confer- 
ence on Empirical Methods in Natural Language Pro- 
cessing and Computational Natural Language Learn- 
ing (EMNLP-CoNLL 2007), pages 698–707, June.
Jun’ichi Kazama and Kentaro Torisawa. 2008. Inducing 
gazetteers for named entity recognition by large-scale 
clustering of dependency relations. In Proceedings of 
the 46th Annual Meeting of the Association for Com- 
putational Linguistics: Human Language Technolo- 
gies (ACL-08: HLT), pages 407–415.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, Richard 
Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Con- 
stantin, and Evan Herbst. 2007. Moses: Open Source 
Toolkit for Statistical Machine Translation.   In Pro- 
ceedings of the 45th Annual Meeting of the Associa- 
tion for Computational Linguistics (ACL 2007), pages
177–180.
J. Richard Landis and Gary G. Koch.  1977.  The mea- 
surement of observer agreement for categorical data. 
Biometrics, 33(1):159–174.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer- 
ence rules for question answering. Natural Language 
Engineering, 7(4):343–360.
Bill  MacCartney,  Michel  Galley,  and  Christopher D.
Manning. 2008. A phrase-based alignment model for 
natural language inference. In Proceedings of the 2008


Conference on Empirical Methods in Natural Lan- 
guage Processing (EMNLP-2008), pages 802–811.
Nitin Madnani and Bonnie Dorr.   2010.   Generating 
phrasal and sentential paraphrases: A survey of data- 
driven methods. Computational Linguistics, 36(3).
Kathleen R. McKeown, Regina Barzilay, David Evans, 
Vasileios Hatzivassiloglou, Judith L. Klavans, Ani 
Nenkova, Carl Sable, Barry Schiffman, and Sergey 
Sigelman.   2002.   Tracking and summarizing news 
on a daily basis with columbia’s newsblaster. In Pro- 
ceedings of the 2nd international conference on Hu- 
man Language Technology Research, pages 280–285.
Masaki Murata, Toshiyuki Kanemaru, and Hitoshi Isa- 
hara.  2004.  Automatic paraphrase acquisition based 
on matching of definition sentences in plural dictionar- 
ies (written in Japanese). Journal of Natural Language 
Processing, 11(5):135–149.
Roberto Navigli and Paola Velardi.   2010.   Learning 
word-class lattices for definition and hypernym extrac- 
tion.  In Proceedings of the 48th Annual Meeting of 
the Association for Computational Linguistics (ACL
2010), pages 1318–1327.
Chris Quirk, Chris Brockett, and William Dolan.  2004.
Monolingual machine translation for paraphrase gen- 
eration.   In Proceedings of the 2004 Conference on 
Empirical Methods in Natural Language Processing 
(EMNLP-2004), pages 142–149.
Deepak  Ravichandran  and  Eduard  H.  Hovy.	2002.
Learning surface text patterns for a question answer- 
ing system. In Proceedings of the 40th Annual Meet- 
ing of the Association for Computational Linguistics 
(ACL 2002), pages 41–47.
Satoshi Sekine.  2005.  Automatic paraphrase discovery 
based on context and keywords between ne pairs.  In 
Proceedings of the Third International Workshop on 
Paraphrasing (IWP-2005), pages 80–87.
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic paraphrase acquisition from news ar- 
ticles.  In Proceedings of the 2nd international Con- 
ference on Human Language Technology Research 
(HLT2002), pages 313–318.
Idan Szpektor and Ido Dagan.  2008.  Learning entail- 
ment rules for unary template.  In Proceedings of the
22nd International Conference on Computational Lin- 
guistics (COLING2008), pages 849–856.
Idan Szpektor, Eyal Shnarch, and Ido Dagan.	2007.
Instance-based evaluation of entailment rule acquisi- 
tion. In Proceedings of the 45th Annual Meeting of the 
Association of Computational Linguistics (ACL 2007), 
pages 456–463.

