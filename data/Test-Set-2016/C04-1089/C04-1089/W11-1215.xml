<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">This paper introduces a new task of crosslingual slot ﬁlling which aims to discover attributes for entity queries from crosslingual comparable corpora and then present answers in a desired language.</S>
		<S sid ="2" ssid = "2">It is a very challenging task which suﬀers from both information extraction and machine translation errors.</S>
		<S sid ="3" ssid = "3">In this paper we analyze the types of errors produced by ﬁve diﬀerent baseline approaches, and present a novel supervised rescoring based validation approach to incorporate global evidence from very large bilingual comparable corpora.</S>
		<S sid ="4" ssid = "4">Without using any additional labeled data this new approach obtained 38.5% relative improvement in Precision and 86.7% relative improvement in Recall over several state-of-the-art approaches.</S>
		<S sid ="5" ssid = "5">The ultimate system outperformed monolingual slot ﬁlling pipelines built on much larger monolingual corpora.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="6" ssid = "6">The slot ﬁlling task at NIST TAC Knowledge Base Population (KBP) track (Ji et al., 2010) is a relatively new and popular task with the goal of automatically building proﬁles of entities from large amounts of unstructured data, and using these proﬁles to populate an existing knowledge base.</S>
			<S sid ="7" ssid = "7">These proﬁles consist of numerous slots such as “title”, “parents” for persons and “top-employees” for organizations.</S>
			<S sid ="8" ssid = "8">A variety of approaches have been proposed to address both tasks with considerable success; nevertheless, all of the KBP tasks so far have been limited to monolingual processing.</S>
			<S sid ="9" ssid = "9">However, as the shrinking fraction of the world’s Web pages are written in English, many slot ﬁlls can only be discovered from comparable documents in foreign languages.</S>
			<S sid ="10" ssid = "10">By comparable corpora we mean texts that are about similar topics, but are not in general translations of each other.</S>
			<S sid ="11" ssid = "11">These corpora are naturally available, for example, many news agencies release multilingual news articles on the same day.</S>
			<S sid ="12" ssid = "12">In this paper we propose a new and more challenging crosslingual slot ﬁlling task, to ﬁnd information for any English query from crosslingual comparable corpora, and then present its proﬁle in English.</S>
			<S sid ="13" ssid = "13">We developed complementary baseline approaches which combine two diﬃcult problems: information extraction (IE) and machine translation (MT).</S>
			<S sid ="14" ssid = "14">In this paper we conduct detailed error analysis to understand how we can exploit comparable corpora to construct more complete and accurate proﬁles.</S>
			<S sid ="15" ssid = "15">Many correct answers extracted from our baselines will be reported multiple times in any external large collection of comparable documents.</S>
			<S sid ="16" ssid = "16">We can thus take advantage of such information redundancy to rescore candidate answers.</S>
			<S sid ="17" ssid = "17">To choose the best answers we consult large comparable corpora and corresponding IE results.</S>
			<S sid ="18" ssid = "18">We prefer those answers which frequently appear together with the query in certain IE contexts, including co-occurring names, coreference links, relations and events.</S>
			<S sid ="19" ssid = "19">For example, we prefer “South Korea” instead of “New York Stock Exchange” as the “per:employee of ” answer for “Roh Moo-hyun” using global evidence from employment relation extraction.</S>
			<S sid ="20" ssid = "20">Such global knowledge from comparable corpora 110 Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 110–119, 49th Annual Meeting of the Association for Computational Linguistics, Portland, Oregon, 24 June 2011.</S>
			<S sid ="21" ssid = "21">Qc 2011 Association for Computational Linguistics provides substantial improvement over each individual baseline system and even state-of-the- art monolingual slot ﬁlling systems.</S>
			<S sid ="22" ssid = "22">Compared to previous methods of exploiting comparable corpora, our approach is novel in multiple aspects because it exploits knowledge from: (1) both local and global statistics; (2) both languages; and (3) both shallow and deep analysis.</S>
	</SECTION>
	<SECTION title="Related Work. " number = "2">
			<S sid ="23" ssid = "1">Sudo et al.</S>
			<S sid ="24" ssid = "2">(2004) found that for a crosslingual single-document IE task, source language extraction and fact translation performed notably better than machine translation and target language extraction.</S>
			<S sid ="25" ssid = "3">We observed the same results.</S>
			<S sid ="26" ssid = "4">In addition we also demonstrate that these two approaches are complementary and can be used to boost each other’s results in a statistical rescoring model with global evidence from large comparable corpora.</S>
			<S sid ="27" ssid = "5">HakkaniTur et al.</S>
			<S sid ="28" ssid = "6">(2007) described a ﬁltering mechanism using two crosslingual IE systems for improving crosslingual document retrieval.</S>
			<S sid ="29" ssid = "7">Many previous validation methods for crosslingual QA, such as those organized by Cross Language Evaluation Forum (Vallin et al., 2005), focused on local information which involves only the query and answer (e.g.</S>
			<S sid ="30" ssid = "8">(Kwork and Deng, 2006)), keyword translation (e.g.</S>
			<S sid ="31" ssid = "9">(Mitamura et al., 2006)) and surface patterns (e.g.</S>
			<S sid ="32" ssid = "10">(Soubbotin and Soubbotin, 2001)).</S>
			<S sid ="33" ssid = "11">Some global validation approaches considered information redundancy based on shallow statistics including co- occurrence, density score and mutual information (Clarke et al., 2001; Magnini et al., 2001; Lee et al., 2008), deeper knowledge from dependency parsing (e.g.</S>
			<S sid ="34" ssid = "12">(Shen et al., 2006)) or logic reasoning (e.g.</S>
			<S sid ="35" ssid = "13">(Harabagiu et al., 2005)).</S>
			<S sid ="36" ssid = "14">However, all of these approaches made limited eﬀorts at disambiguating entities in queries and limited use of fact extraction in answer search and validation.</S>
			<S sid ="37" ssid = "15">Several recent IE studies have stressed the beneﬁts of using information redundancy on estimating the correctness of the IE output (Downey et al., 2005; Yangarber, 2006; Patwardhan and Riloﬀ, 2009; Ji and Grish man, 2008).</S>
			<S sid ="38" ssid = "16">Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009).</S>
			<S sid ="39" ssid = "17">To the best of our knowledge, this is the ﬁrst work on mining facts from comparable corpora for answer validation in a new crosslingual entity proﬁling task.</S>
	</SECTION>
	<SECTION title="Experimental Setup. " number = "3">
			<S sid ="40" ssid = "1">3.1 Task Deﬁnition.</S>
			<S sid ="41" ssid = "2">The goal of the KBP slot ﬁlling task is to extract facts from a large source corpus regarding certain attributes (“slots”) of an entity, which may be a person or organization, and use these facts to augment an existing knowledge base (KB).</S>
			<S sid ="42" ssid = "3">Along with each slot answer, the system must provide the ID of a document which supports the correctness of this answer.</S>
			<S sid ="43" ssid = "4">KBP 2010 (Ji et al., 2010) deﬁnes 26 types of attributes for persons (such as the age, birthplace, spouse, children, job title, and employing organization) and 16 types of attributes for organizations (such as the top employees, the founder, the year founded, the headquarters location, and the subsidiaries).</S>
			<S sid ="44" ssid = "5">The new problem we deﬁne in this paper is an extension of this task to a crosslingual paradigm.</S>
			<S sid ="45" ssid = "6">Given a query in a target language t and a collection of documents in a source language s, a system must extract slot answers about the query and present the answers in t. In this paper we examine a speciﬁc setting of s=Chinese and t=English.</S>
			<S sid ="46" ssid = "7">To score crosslingual slot ﬁlling, we pool all the system responses and group equivalent answers into equivalence classes.</S>
			<S sid ="47" ssid = "8">Each system response is rated as correct, wrong, inexact or redundant.</S>
			<S sid ="48" ssid = "9">Given these judgments, we calculate the precision, recall and F-measure of each system, crediting only correct answers.</S>
			<S sid ="49" ssid = "10">3.2 Data and Query Selection.</S>
			<S sid ="50" ssid = "11">We use the comparable corpora of English TDT5 (278,358 documents) and Chinese TDT5 (56,424 documents) as our source collection.</S>
			<S sid ="51" ssid = "12">For query selection, we collected all the entities from the entire source collection and counted their frequencies.</S>
			<S sid ="52" ssid = "13">We then selected 50 informative entities (25 persons and 25 organizations) which were located in the middle range of frequency counts.</S>
			<S sid ="53" ssid = "14">Among the 25 person queries, half are Chinesespeciﬁc names, and half are non-Chinese names.</S>
			<S sid ="54" ssid = "15">The 25 organizations follow a representative distribution according to the entity subtypes deﬁned in NIST Automatic Content Extraction (ACE) program1.</S>
			<S sid ="55" ssid = "16">3.3 Baseline Pipelines.</S>
			<S sid ="56" ssid = "17">3.3.1 Overview We employ the following two types of baseline crosslingual slot ﬁlling pipelines to process Chinese documents.</S>
			<S sid ="57" ssid = "18">Figure 1 and Table 1 shows the ﬁve system pipelines we have used to conduct our experiments.</S>
			<S sid ="58" ssid = "19">Type A Translate Chinese texts into English, and apply English slot ﬁlling systems to the translations.</S>
			<S sid ="59" ssid = "20">Type B Translate English queries into Chinese, apply Chinese slot ﬁlling systems to Chinese texts, and translate answers back to English.</S>
			<S sid ="60" ssid = "21">Pip elin e Labe l C o m p o n e n t s D a t a M o n o - li n g u a l ( 1 ) E n g li s h S u p e r v i s e d C l a s s i f i c a t i o n En gli sh T D T 5 ( 2 ) E n g l i s h P a t t e r n M a t c h i n g C r o s s- li n g u a l Ty pe A (3) M T + E n g l i s h S u p e r v i s e d C l a s s i f i c a t i o n Ch ine se T D T 5 (4) M T + E n g l i s h P a t t e r n M a t c h i n g Ty pe B (5) Q u e r y T r a n s l a t i o n +C hin ese Su per vis ed C l a s s i f i c a t i o n +A ns we r Tr an sla tio n Table 1: Monolingual and Crosslingual Baseline Slot Filling Pipelines 3.3.2 Monolingual Slot Filling We applied a state-of-the-art bilingual slot ﬁlling system (Chen et al., 2010) to process bilingual comparable corpora.</S>
			<S sid ="61" ssid = "22">This baseline system includes a supervised ACE IE pipeline and a bottom-up pattern matching pipeline.</S>
			<S sid ="62" ssid = "23">The IE pipeline includes relation extraction and event extraction based on maximum entropy models that incorporate diverse lexical, syntactic, semantic and ontological knowledge.</S>
			<S sid ="63" ssid = "24">The extracted ACE relations and events are then mapped to KBP slot ﬁlls.</S>
			<S sid ="64" ssid = "25">In pattern matching, we extract and rank patterns based on a distant supervision approach (Mintz et al., 2009) Chinese Query Chinese Texts Machine Translation Query Translation that uses entity attribut e pairs from Wikiped ia Infoboxe s and Freebase (Bollack er et al., 2008).</S>
			<S sid ="65" ssid = "26">We set a low threshol d to include more answer Chinese Slot Filling Supervised Classification English Texts English Query candidates, and then a series of ﬁltering steps to reﬁne and improve the overall pipeline results.</S>
			<S sid ="66" ssid = "27">The ﬁltering steps include removing answers which have inappropriate entity types or English Slot Filling have inappropriate dependency paths to the en Answer Translation Pattern Matching Supervised Classification tities.</S>
			<S sid ="67" ssid = "28">3.3.3 Docu ment and Name Translation English Candidate Answers Figure 1: Overview of Baseline Crosslingual Slot Filling Pipelines 1 http://www.itl.nist.gov/iad/mig/tests/ace/ We use a statistical, phrase-based MT system (Zens and Ney, 2004) to translate Chinese documents into English for Type A Approaches.</S>
			<S sid ="68" ssid = "29">The best translation is computed by using a weighted log-linear combination of various statistical models: an n-gram language model, a phrase translation model and a word-based lex icon model.</S>
			<S sid ="69" ssid = "30">The latter two models are used in source-to-target and target-to-source directions.</S>
			<S sid ="70" ssid = "31">The model scaling factors are optimized with respect to the BLEU score similar to (Och, 2003).</S>
			<S sid ="71" ssid = "32">The training data includes 200 million running words in each language.</S>
			<S sid ="72" ssid = "33">The total language model training data consists of about 600 million running words.</S>
			<S sid ="73" ssid = "34">We applied various name mining approaches from comparable corpora and parallel corpora, as described in (Ji et al., 2009) to extract and translate names in queries and answers in Type B approaches.</S>
			<S sid ="74" ssid = "35">The accuracy of name translation is about 88%.</S>
			<S sid ="75" ssid = "36">For those names not covered by these pairs, we relied on Google Translate 2 to obtain results.</S>
	</SECTION>
	<SECTION title="Analysis of Baseline Pipelines. " number = "4">
			<S sid ="76" ssid = "1">In this section we analyze the coverage (Section 4.1) and precision (Section 4.2) results of the baseline pipelines.</S>
			<S sid ="77" ssid = "2">We then illustrate the potential for global validation from comparable corpora through a series of examples.</S>
			<S sid ="78" ssid = "3">4.1 Coverage Analysis: Toward.</S>
			<S sid ="79" ssid = "4">Information Fusion Table 2 summarizes the Precision (P), Recall (R) and F-measure (F) of baseline pipelines and the union of their individual results.</S>
			<S sid ="80" ssid = "5">Table 2: Baseline Pipeline Results 2 http://translate.google.com/ Although crosslingual pipelines used a much smaller corpus than monolingual pipelines, they extracted comparable number of correct answers (66 vs. 81) with a slightly better precision.</S>
			<S sid ="81" ssid = "6">In fact, the crosslingual pipeline (5) performs even better than monolingual pipeline (2), especially on the employment slots.</S>
			<S sid ="82" ssid = "7">In particular, 96.35% of the correct answers for Chinesespeciﬁc person queries (e.g. “Tang Jiaxuan”) were extracted from Chinese data.</S>
			<S sid ="83" ssid = "8">Even for those facts discovered from English data, they are about quite general slots such as “title” and “employee of ”.</S>
			<S sid ="84" ssid = "9">In contrast, Chinese data covers more diverse biographical slots such as “family members” and “schools attended”.</S>
			<S sid ="85" ssid = "10">Compared to the union of Type A approaches (pipelines (3)+(4)), Pipeline (5) returned many more correct answers with higher precision.</S>
			<S sid ="86" ssid = "11">The main reason is that Type A approaches suﬀer from MT errors.</S>
			<S sid ="87" ssid = "12">For example, MT mistakenly translated the query name “Celine Dion” into “Clinton” and thus English slot ﬁlling components failed to identify any answers.</S>
			<S sid ="88" ssid = "13">One can hypothesize that slot ﬁlling on MT output can be improved by retraining extraction components directly from MT output.</S>
			<S sid ="89" ssid = "14">However, our experiments of learning patterns from MT output showed negative impact, mainly because MT errors were too diverse to generalize.</S>
			<S sid ="90" ssid = "15">In other cases even though slot ﬁlling produced correct results, MT still failed to translate the answer names correctly.</S>
			<S sid ="91" ssid = "16">For example, English slot ﬁlling successfully found a potential answer for “org:founded by” of the query “Microsoft” from the following MT output: “The third largest of the Microsoft common founder Alan Doss , aged 50, and net assets of US 22 bil lion.”; however, the answer string “Paul Al len” was mistakenly translated into “Alan Doss”.</S>
			<S sid ="92" ssid = "17">MT is not so crucial for “per:title” slot because it does not require translation of contexts.</S>
			<S sid ="93" ssid = "18">To summarize, 59% of the missing errors were due to text, query or answer translation errors and 20% were due to slot ﬁlling errors.</S>
			<S sid ="94" ssid = "19">Nevertheless, the union of (3)+(4)+(5) still contain more correct answers.</S>
			<S sid ="95" ssid = "20">These baseline pipelines were developed from a diverse set of algorithms, and typically showed strengths in speciﬁc slots.</S>
			<S sid ="96" ssid = "21">In general we can conclude that monolingual and crosslingual pipelines are complementary.</S>
			<S sid ="97" ssid = "22">Combining the responses from all baseline pipelines, we can get similar number of correct answers compared to one single human annota- tor.</S>
			<S sid ="98" ssid = "23">4.2 Precision Analysis: Toward Global.</S>
			<S sid ="99" ssid = "24">Validation The spurious errors from baseline crosslingual slot ﬁlling pipelines reveal both the shortcomings of the MT system and extraction across languages.</S>
			<S sid ="100" ssid = "25">Table 3 shows the distribution of spurious errors.</S>
			<S sid ="101" ssid = "26">Pi pe lin e S p u r i o u s E r r o r s Di str ib uti on T yp e A C o nt e nt T ra ns la ti o n + E x t r a c t i o n 8 5 % Q u e r y T r a n sl at i o n 1 3 % A ns w er Tr an sl ati on 2 % T y pe B W or d Se g m en tat io n 3 4 % R el ati on E xt ra cti on 3 3 % C o r e f e r e n c e 1 7 % S e m a n t i c T y p e 1 3 % S l o t T y p e 3 % Table 3: Distribution of Spurious Errors Table 3 indicates a majority (85%) of spurious errors from Type A pipelines were due to applying monolingual slot ﬁlling methods to MT output which preserves Chinese structure.</S>
			<S sid ="102" ssid = "27">As demonstrated in previous work (e.g.</S>
			<S sid ="103" ssid = "28">(Par- ton and McKeown, 2010; Ji et al., 2009)), we also found that many (14.6%) errors were caused by the low quality of name translation for queries and answers.For example, “麦克金蒂/McGinty” was mis takenly translated into the query name “Kim Jongil”, which led to many incorrect answerssuch as “The British Royal joint military re search institute” for “per:employee of ”.</S>
			<S sid ="104" ssid = "29">In contrast, the spurious errors from Type Bpipelines were more diverse.</S>
			<S sid ="105" ssid = "30">Chinese IE components severely suﬀered from word segmen tation errors (34%), which were then directly propagated into Chinese document retrieval and slot ﬁlling.</S>
			<S sid ="106" ssid = "31">Many segmentation errors occurred with out-of-vocabulary names, especially person names and nested organization names.</S>
			<S sid ="107" ssid = "32">For example, the name “姚明宝/Yao Mingbao” was mistakenly segmented into two words “姚明/Yao Ming” and “宝/bao”, and thus the document was mistakenly retrieved for the query ‘Yao Ming”.</S>
			<S sid ="108" ssid = "33">In many cases (33%) Chinese relation and event extraction components failed to capture Chinesespeciﬁc structures due to the limited size of training corpora.</S>
			<S sid ="109" ssid = "34">For example, from the context “应邀担任陈水扁经济顾问的萧万 长/Xiao Wan-chang, who were invited to become the economics consultant for Chen Shui bian”, Chinese slot ﬁlling system mistakenly extracted “consultant” as a ”per:title” answer for the query “Chen Shuibian” using a common pattern “&lt;query&gt;&lt;title&gt;”.</S>
			<S sid ="110" ssid = "35">13% of errors were caused due to invalid semantic types for certain slots.</S>
			<S sid ="111" ssid = "36">For example, many metaphoric titles such as “tough guy” don’t match the deﬁnition of “per:title” in the annotation guideline “employment or membership position”.</S>
	</SECTION>
	<SECTION title="Global Validation. " number = "5">
			<S sid ="112" ssid = "1">Based on the above motivations we propose to incorporate global evidence from a very large collection of comparable documents to reﬁne local decisions.</S>
			<S sid ="113" ssid = "2">The central idea is to over- generate candidate answers from multiple weak baselines to ensure high upper-bound of recall, and then conduct eﬀective global validation to ﬁlter spurious errors while keeping good answers in order to enhance precision.</S>
			<S sid ="114" ssid = "3">5.1 Supervised Rescoring.</S>
			<S sid ="115" ssid = "4">Ideally, we want to choose a validation model which can pick out important features in a context wider than that used by baseline pipelines.</S>
			<S sid ="116" ssid = "5">Merging individual systems to form the union of answers can be eﬀective, but Table 2 shows that simple union of all pipelines produced worse F- measure than the best pipeline.</S>
			<S sid ="117" ssid = "6">In this paper we exploit the reranking paradigm, commonly used in information retrieval, to conduct global validation.</S>
			<S sid ="118" ssid = "7">By modeling the empirical distribution of labeled training data, statistical models are used to identify the strengths and weaknesses (e.g. high and low precision slots) of individual systems, and rescore answers accordingly.</S>
			<S sid ="119" ssid = "8">Specially, we develop a supervised Maximum Entropy (MaxEnt) based model to rescore the answers from the pipelines, selecting only the highest-scoring answers.</S>
			<S sid ="120" ssid = "9">The rescorer was trained (using cross- validation) on varying subsets of the features.</S>
			<S sid ="121" ssid = "10">The threshold at which an answer is deemed to be true is chosen to maximize the F-Measure on the training set.</S>
			<S sid ="122" ssid = "11">5.2 Validation Features.</S>
			<S sid ="123" ssid = "12">Table 4 describes the validation features used for rescoring, where q is the query, q’ the Chinese translation of q, t the slot type, a the candidate answer, a’ the Chinese form of a, s the context sentence and d is the context document supporting a. The feature set beneﬁts from multiple dimensions of crosslingual slot ﬁlling.</S>
			<S sid ="124" ssid = "13">These features were applied to both languages wherever annotation resources were available.</S>
			<S sid ="125" ssid = "14">In the KBP slot ﬁlling task, slots are often dependent on each other, so we can improve the results by improving the “coherence” of the story (i.e. consistency among all generated answers - query proﬁles).</S>
			<S sid ="126" ssid = "15">We use feature f2 to check whether the same answer was generated for conﬂicting slots, such as per:parents and per:children.</S>
			<S sid ="127" ssid = "16">Compared to traditional QA tasks, slot ﬁlling is a more ﬁnegrained task in which diﬀerent slots are expected to obtain semantically diﬀerent answers.</S>
			<S sid ="128" ssid = "17">Therefore, we explored semantic constraints in both local and global contexts.</S>
			<S sid ="129" ssid = "18">For example, we utilized bilingual name gazetteers from ACE training corpora, Google n-grams (Ji and Lin, 2009) and the geonames website 3 to encode features f6, f8 and f9; The org:top members/employees slot requires a system to distinguish whether a person member/ employee is in the top position, thus we encoded f10 for this purpose.</S>
			<S sid ="130" ssid = "19">The knowledge used in our baseline pipelines is relatively static – it is not updated during the 3 http://www.geonames.org/statistics/ extraction process.</S>
			<S sid ="131" ssid = "20">Achieving high performance for cross-lingual slot ﬁlling requires that we take a broader view, one that looks outside a single document or a single language in order to exploit global knowledge.</S>
			<S sid ="132" ssid = "21">Fortunately, as more and more large crosslingual comparable corpora are available, we can take advantage of information redundancy to validate answers.</S>
			<S sid ="133" ssid = "22">The basic intuition is that if a candidate answer a is correct, it should appear together with the query q repeatedly, in diﬀerent documents, or even in certain coreference links, relations and events.</S>
			<S sid ="134" ssid = "23">For example, “David Kel ly - scientist”, and “石原慎太郎/Shintaro Ishihara - 知事/governor ” pairs appear frequently in “title” coreference links in both English and Chinese corpora; “Elizabeth II ” is very often involved in an “employment” relation with “United Kingdom” in English corpora.</S>
			<S sid ="135" ssid = "24">On the other hand, some incorrect answers with high global statistics can be ﬁltered out using these constraints.</S>
			<S sid ="136" ssid = "25">For exam ple, although the query “唐家璇/Tang Jiaxuan” appears frequently together with the candidate per:title answer “人员/personnel”, it is linked by few coreference links; in contrast, it’s coreferential with the correct title answer “国务委员/State Council member ” much more frequently.We processed cross-lingual comparable cor pora to extract coreference links, relations and events among mentions (names, nominals and time expressions etc.) and stored them in an external knowledge base.</S>
			<S sid ="137" ssid = "26">Any pair of &lt;q, a&gt;is then compared to the entries in this knowl edge base.</S>
			<S sid ="138" ssid = "27">We used 157,708 documents from Chinese TDT5 and Gigaword to count Chinese global statistics, and 7,148,446 documents from DARPA GALE MT training corpora to count English global statistics, as shown in features f12 and f13.</S>
			<S sid ="139" ssid = "28">Fact based global features f14, f15,f16 and f17, were calculated from 49,359 Chi nese and 280,513 English documents (annotated by the bilingual IE system in Section 3.3.2.</S>
	</SECTION>
	<SECTION title="Experiments. " number = "6">
			<S sid ="140" ssid = "1">In this section, we examine the overall performance of this method.</S>
			<S sid ="141" ssid = "2">We then discuss the usefulness of the individual sets of features.</S>
			<S sid ="142" ssid = "3">In C h a r a c t e r i s t i c s D e s c r i p t i o n S c o p e D e p t h La ng ua ge G l o b a l ( C r o s s s y s t e m ) S h al lo w E n g li s h f1: fre que ncy of &lt;q, a, t&gt; that app ear s in all bas elin e out put s f2: nu mb er of con flic tin g slot typ es in whi ch ans wer a app ear s in all bas elin e out put s L o c a l S h al lo w E n g li s h f3: con jun ctio n of t and wh eth er a is a yea r ans wer f4: con jun ctio n of t and wh eth er a incl ude s nu mb ers or lett ers D e e p E n g li s h f5: con jun ctio n of pla ce t and wh eth er a is a cou ntr y na me f6: con jun ctio n of per :ori gin t and wh eth er a is a nati ona lity f7: if t=p er:t itle , wh eth er a is an acc ept abl e titl e f8: if t req uir es a na me ans wer , wh eth er a is a na me f9: wh eth er a has app rop riat e se ma ntic typ e G l o b a l ( W i t h i n - D o c u m e n t ) D e e p E n g li s h f10 : con jun ctio n of org :to p_ me mb ers/ em plo yee s and wh eth er the re is a high lev el titl e in s f11 : con jun ctio n of alte rna tive na me and wh eth er a is an acr ony m of q G l o b a l ( C r o s s d o c u m e n t i n c o m p a r a b l e c o r p o r a ) S h al lo w (St atis tics ) C hi n es e f12 : con diti ona l pro bab ilit y of q/q&apos; and a/a&apos; app ear in the sa me doc um ent E n g li s h f13 : con diti ona l pro bab ilit y of q/q&apos; and a/a&apos; app ear in the sa me sen ten ce D e e p ( F a c t b a s e d ) B o t h f14 : co occ urr enc e of q/q&apos; and a/a&apos; app ear in cor efe ren ce lin ks E n g li s h f15 : co occ urr enc e of q/q&apos; and a/a&apos; app ear in rela tio n/e ven t lin ks E n g li s h f16 : con diti ona l pro bab ilit y of q/q&apos; and a/a&apos; app ear in rela tio n/e ven t lin ks E n g li s h f17 : mu tual inf or mat ion of q/q&apos; and a/a&apos; app ear in rela tio n/e ven t lin ks Table 4: Validation Features for Crosslingual Slot Filling the following results, the baseline features are always used in addition to any other features.</S>
			<S sid ="143" ssid = "4">6.1 Overall Performance.</S>
			<S sid ="144" ssid = "5">Because of the data scarcity, tenfold cross- validation, across queries, was used to train and test the system.</S>
			<S sid ="145" ssid = "6">Quantitative results after combining answers from multiple pipelines are shown in Table 5.</S>
			<S sid ="146" ssid = "7">We used two basic features, one is the slot type and the other is the entity type of the query (i.e. person or organization).</S>
			<S sid ="147" ssid = "8">This basic feature set is already successful in improving the precision of the pipelines, although this results in a number of correct answers being discarded as well.</S>
			<S sid ="148" ssid = "9">By adding the additional validation features described previously, both the f-score and precision of the models are improved.</S>
			<S sid ="149" ssid = "10">In the case of the cross-lingual pipelines (3+4+5) the number of correct answers chosen is almost doubled while increasing the precision of the output.</S>
			<S sid ="150" ssid = "11">6.2 Impact of Global Validation.</S>
			<S sid ="151" ssid = "12">A comparison of the beneﬁts of global versus local features are shown in Table 6, both of which dramatically improve scores over the baseline features.</S>
			<S sid ="152" ssid = "13">The global features are universally P i p e l i n e s F P R Basic Feat ures 1 + 2 3 + 4 + 5 1 + 2 + 3 + 4 + 5 0.</S>
			<S sid ="153" ssid = "14">31 0.</S>
			<S sid ="154" ssid = "15">26 0.</S>
			<S sid ="155" ssid = "16">27 0.</S>
			<S sid ="156" ssid = "17">31 0.</S>
			<S sid ="157" ssid = "18">39 0.</S>
			<S sid ="158" ssid = "19">29 0.</S>
			<S sid ="159" ssid = "20">30 0.</S>
			<S sid ="160" ssid = "21">20 0.</S>
			<S sid ="161" ssid = "22">25 Full Fea ture s 1 + 2 3 + 4 + 5 1 + 2 + 3 + 4 + 5 0.</S>
			<S sid ="162" ssid = "23">37 0.</S>
			<S sid ="163" ssid = "24">36 0.</S>
			<S sid ="164" ssid = "25">31 0.</S>
			<S sid ="165" ssid = "26">30 0.</S>
			<S sid ="166" ssid = "27">35 0.</S>
			<S sid ="167" ssid = "28">28 0.</S>
			<S sid ="168" ssid = "29">46 0.</S>
			<S sid ="169" ssid = "30">37 0.</S>
			<S sid ="170" ssid = "31">35 Table 5: Using Basic Features to Filter Answers more beneﬁcial than the local features, although the local features generate results with higher precision at the expense of the number of correct answers returned.</S>
			<S sid ="171" ssid = "32">The global features are especially useful for pipelines 3+4+5, where the performance using just these features reaches those of using all other features – this does not hold true for the monolingual pipelines however.</S>
			<S sid ="172" ssid = "33">6.3 Impact of Fact-driven Deep.</S>
			<S sid ="173" ssid = "34">Knowledge The varying beneﬁt of fact-driven cross- document features and statistical cross- document features are shown in Table 7.</S>
			<S sid ="174" ssid = "35">P i p e l i n e s F P R Loca l Feat ures 1 + 2 3 + 4 + 5 1 + 2 + 3 + 4 + 5 0.</S>
			<S sid ="175" ssid = "36">34 0.</S>
			<S sid ="176" ssid = "37">29 0.</S>
			<S sid ="177" ssid = "38">27 0.</S>
			<S sid ="178" ssid = "39">35 0.</S>
			<S sid ="179" ssid = "40">40 0.</S>
			<S sid ="180" ssid = "41">32 0.</S>
			<S sid ="181" ssid = "42">33 0.</S>
			<S sid ="182" ssid = "43">22 0.</S>
			<S sid ="183" ssid = "44">24 Glob al Featu res 1 + 2 3 + 4 + 5 1 + 2 + 3 + 4 + 5 0.</S>
			<S sid ="184" ssid = "45">35 0.</S>
			<S sid ="185" ssid = "46">37 0.</S>
			<S sid ="186" ssid = "47">33 0.</S>
			<S sid ="187" ssid = "48">30 0.</S>
			<S sid ="188" ssid = "49">36 0.</S>
			<S sid ="189" ssid = "50">29 0.</S>
			<S sid ="190" ssid = "51">42 0.</S>
			<S sid ="191" ssid = "52">38 0.</S>
			<S sid ="192" ssid = "53">38 Table 6: The Beneﬁt of Global versus Local Features While both feature sets are beneﬁcial, the monolingual pipelines (1+2) beneﬁt more from statistical features while the cross-lingual pipelines (3+4+7) beneﬁt slightly more from the fact-based features.</S>
			<S sid ="193" ssid = "54">Despite this bias, the overall results when the features are used in all pipelines are very close with the fact-based features being slightly more useful overall.</S>
			<S sid ="194" ssid = "55">P i p e l i n e s F P RFact Based Feature s 1 + 2 3 + 4 + 5 1 + 2 + 3 + 4 + 5 0.</S>
			<S sid ="195" ssid = "56">33 0.</S>
			<S sid ="196" ssid = "57">35 0.</S>
			<S sid ="197" ssid = "58">30 0.</S>
			<S sid ="198" ssid = "59">27 0.</S>
			<S sid ="199" ssid = "60">43 0.</S>
			<S sid ="200" ssid = "61">27 0.</S>
			<S sid ="201" ssid = "62">42 0.</S>
			<S sid ="202" ssid = "63">29 0.</S>
			<S sid ="203" ssid = "64">34 Statisti cal Featur es 1 + 2 3 + 4 + 5 1 + 2 + 3 + 4 + 5 0.</S>
			<S sid ="204" ssid = "65">37 0.</S>
			<S sid ="205" ssid = "66">34 0.</S>
			<S sid ="206" ssid = "67">29 0.</S>
			<S sid ="207" ssid = "68">34 0.</S>
			<S sid ="208" ssid = "69">35 0.</S>
			<S sid ="209" ssid = "70">25 0.</S>
			<S sid ="210" ssid = "71">40 0.</S>
			<S sid ="211" ssid = "72">33 0.</S>
			<S sid ="212" ssid = "73">34 Table 7: Fact vs. Statistical Cross-Doc Features Translation features were only beneﬁcial to pipelines 3, 4, and 5, and provided a slight increase in precision from 0.39 to 0.42, but provided no noticeable beneﬁt when used in conjunction with results from pipelines 1 and 2.</S>
			<S sid ="213" ssid = "74">This is because the answers where translation features would be most useful were already being selected by pipelines 1 and 2 using the baseline features.</S>
			<S sid ="214" ssid = "75">6.4 Discussion.</S>
			<S sid ="215" ssid = "76">The use of any re-scoring, even with baseline features, provides large gains over the union of the baseline pipelines, removing large number of incorrect answers.</S>
			<S sid ="216" ssid = "77">The use of more sophis ticated features provided substantial gains over the baseline features.</S>
			<S sid ="217" ssid = "78">In particular, global features proved very eﬀective.</S>
			<S sid ="218" ssid = "79">Further feature engineering to address the remaining errors and the dropped correct answer would likely provide increasing gains in performance.</S>
			<S sid ="219" ssid = "80">In addition, two human annotators, independently, conducted the same task on the same data, with a second pass of adjudication.</S>
			<S sid ="220" ssid = "81">The F- scores of inter-annotator agreement were 52.0% for the ﬁrst pass and 73.2% for the second pass.</S>
			<S sid ="221" ssid = "82">This indicates that slot ﬁlling remains a challenging task for both systems and human annotators—only one monolingual system exceeded 30% F-score in the KBP2010 evaluation.</S>
	</SECTION>
	<SECTION title="Conclusion and  Future Work. " number = "7">
			<S sid ="222" ssid = "1">Crosslingual slot ﬁlling is a challenging task due to limited performance in two separate areas: information extraction and machine translation.</S>
			<S sid ="223" ssid = "2">Various methods of combining techniques from these two areas provided weak yet complementary baseline pipelines.</S>
			<S sid ="224" ssid = "3">We proposed an eﬀective approach to integrate these baselines and enhance their performance using wider and deeper knowledge from comparable corpora.</S>
			<S sid ="225" ssid = "4">The ﬁnal system based on cross-lingual comparable corpora outperformed monolingual pipelines on much larger monolingual corpora.</S>
			<S sid ="226" ssid = "5">The intuition behind our approach is that over-generation of candidate answers from weak baselines provides a potentially strong recall upper-bound.</S>
			<S sid ="227" ssid = "6">The remaining enhancement becomes simpler: ﬁltering errors.</S>
			<S sid ="228" ssid = "7">Our experiments also suggest that our rescoring models tend to over-ﬁt due to small amount of training data.</S>
			<S sid ="229" ssid = "8">Manual annotation and assessment are quite costly, motivating future work in active learning and semi-supervised learning methods.</S>
			<S sid ="230" ssid = "9">In addition, we plan to apply our results as feedback to improve MT performance on facts using query and answer-driven language model adaptation.</S>
			<S sid ="231" ssid = "10">We have demonstrated our approach on EnglishChinese pair, but the framework is language- independent; ultimately we would like to extend the task to extracting information from more languages.</S>
	</SECTION>
	<SECTION title="Acknowledgments">
			<S sid ="232" ssid = "11">This work was supported by the U.S. NSF CAREER Award under Grant IIS0953149 and PSCCUNY Research Program.</S>
			<S sid ="233" ssid = "12">Any opinions, ﬁndings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reﬂect the views of the National Science Foundation.</S>
	</SECTION>
</PAPER>
