Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 870?878,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Mining Bilingual Data from the Web with Adaptively Learnt Patterns 
 
 
Long Jiang1, Shiquan Yang2, Ming Zhou1, Xiaohua Liu1, Qingsheng Zhu2 
1Microsoft Research Asia 
Beijing, 100190, P.R.China 
2Chongqing University,  
Chongqing, 400044, P.R.China 
{longj,mingzhou,xiaoliu}@microsoft.com shiquany@gmail.com,qszhu@cqu.edu.cn 
 
  
 
Abstract 
 
Mining bilingual data (including bilingual sen-
tences and terms1) from the Web can benefit 
many NLP applications, such as machine 
translation and cross language information re-
trieval. In this paper, based on the observation 
that bilingual data in many web pages appear 
collectively following similar patterns, an 
adaptive pattern-based bilingual data mining 
method is proposed. Specifically, given a web 
page, the method contains four steps: 1) pre-
processing: parse the web page into a DOM 
tree and segment the inner text of each node 
into snippets; 2) seed mining: identify poten-
tial translation pairs (seeds) using a word 
based alignment model which takes both trans-
lation and transliteration into consideration; 3) 
pattern learning: learn generalized patterns 
with the identified seeds; 4) pattern based min-
ing: extract all bilingual data in the page using 
the learned patterns. Our experiments on Chi-
nese web pages produced more than 7.5 mil-
lion pairs of bilingual sentences and more than 
5 million pairs of bilingual terms, both with 
over 80% accuracy. 
1 Introduction 
Bilingual data (including bilingual sentences and 
bilingual terms) are critical resources for build-
ing many applications, such as machine transla-
tion (Brown, 1993) and cross language informa-
tion retrieval (Nie et al, 1999). However, most 
existing bilingual data sets are (i) not adequate 
for their intended uses, (ii) not up-to-date, (iii) 
apply only to limited domains. Because it?s very 
hard and expensive to create a large scale bilin-
                                                 
1 In this paper terms refer to proper nouns, technical terms, 
movie names, and so on. And bilingual terms/sentences 
mean terms/sentences and their translations. 
gual dataset with human effort, recently many 
researchers have turned to automatically mining 
them from the Web. 
If the content of a web page is written in two 
languages, we call the page a Bilingual Web 
Page. Many such pages exist in non-English web 
sites. Most of them have a primary language 
(usually a non-English language) and a second-
ary language (usually English). The content in 
the secondary language is often the translation of 
some primary language text in the page.  
Since bilingual web pages are very common in 
non-English web sites, mining bilingual data 
from them should be an important task. However, 
as far as we know, there is no publication availa-
ble on mining bilingual sentences directly from 
bilingual web pages. Most existing methods for 
mining bilingual sentences from the Web, such 
as (Nie et al, 1999; Resnik and Smith, 2003; Shi 
et al, 2006), try to mine parallel web documents 
within bilingual web sites first and then extract 
bilingual sentences from mined parallel docu-
ments using sentence alignment methods.  
As to mining term translations from bilingual 
web pages, Cao et al (2007) and Lin et al (2008) 
proposed two different methods to extract term 
translations based on the observation that authors 
of many bilingual web pages, especially those 
whose primary language is Chinese, Japanese or 
Korean, sometimes annotate terms with their 
English translations inside a pair of parentheses, 
like ?c1c2...cn(e1 e2 ... em)? (c1c2...cn is a primary 
language term and e1 e2 ... em is its English trans-
lation).  
Actually, in addition to the parenthesis pattern, 
there is another interesting phenomenon that in 
many bilingual web pages bilingual data appear 
collectively and follow similar surface patterns. 
Figure 1 shows an excerpt of a page which intro-
duces different kinds of dogs2. The page provides 
                                                 
2 http://www.chinapet.net 
870
a list of dog names in both English and Chinese. 
Note that those bilingual names do not follow the 
parenthesis pattern. However, most of them are 
identically formatted as: ?{Number}?{English 
name}{Chinese name}{EndOfLine}?. One ex-
ceptional pair (?1.Alaskan Malamute ????
??? ?) differs only slightly. Furthermore, 
there are also many pages containing consistently 
formatted bilingual sentences (see Figure 2). The 
page3 lists the (claimed) 200 most common oral 
sentences in English and their Chinese transla-
tions to facilitate English learning. 
 
Figure 1. Consistently formatted term translation 
pairs 
 
Figure 2. Consistently formatted sentence trans-
lation pairs 
People create such web pages for various rea-
sons. Some online stores list their products in 
two languages to make them understandable to 
foreigners. Some pages aim to help readers with 
foreign language learning. And in some pages 
where foreign names or technical terms are men-
tioned, the authors provide the translations for 
disambiguation. For easy reference, from now on 
we will call pages which contain many consis-
tently formatted translation pairs Collective Bi-
lingual Pages. 
According to our estimation, at least tens of 
millions of collective bilingual pages exist in 
Chinese web sites. Most importantly, each such 
page usually contains a large amount of bilingual 
                                                 
3 http://cul.beelink.com/20060205/2021119.shtml 
data. This shows the great potential of bilingual 
data mining. However, the mining task is not 
straightforward, for the following reasons: 
1) The patterns vary in different pages, so 
it?s impossible to mine the translation 
pairs using predefined templates; 
2) Some pages contain consistently format-
ted texts in two languages but they are not 
translation pairs; 
3) Not all translations in a collective bilin-
gual page necessarily follow an exactly 
consistent format. As shown in Figure 1, 
the ten translation pairs are supposed to 
follow the same pattern, however, due to 
typos, the pattern of the first pair is 
slightly different. 
Because of these difficulties, simply using a 
classifier to extract translation pairs from adja-
cent bilingual texts in a collective bilingual page 
may not achieve satisfactory results. Therefore in 
this paper, we propose a pattern-based approach: 
learning patterns adaptively from collective bi-
lingual pages instead of using the parenthesis 
pattern, then using the learned patterns to extract 
translation pairs from corresponding web pages. 
Specifically, our approach contains four steps: 
1) Preprocessing: parse the web page into a 
DOM tree and segment the inner text of 
each node into snippets; 
2) Seed mining: identify potential translation 
pairs (seeds) using an alignment model 
which takes both translation and translite-
ration into consideration; 
3) Pattern learning: learn generalized pat-
terns with the identified seeds; 
4) Pattern based mining: extract all bilingual 
data in the page using the learnt patterns.  
Let us take mining bilingual data from the text 
shown in Figure 1 as an example. Our method 
identifies ?Boxer??? and ?Eskimo Dog??
???? as two potential translation pairs based 
on a dictionary and a transliteration model (Step 
2 above). Then we learn a generalized pattern 
that both pairs follow as ?{BulletNumb-
er}{Punctuation}{English term}{Chinese 
term}{EndOfLine}?, (Step 3 above). Finally, we 
apply it to match in the entire text and get al 
translation pairs following the pattern (Step 4 
above). 
The remainder of this paper is organized as 
follows. In Section 2, we list some related work. 
The overview of our mining approach is pre-
sented in Section 3. In Section 4, we give de-
871
tailed introduction to each of the four modules in 
our mining approach. The experimental results 
are reported in Section 5 followed by our conclu-
sion and some future work in Section 6. 
Please note that in this paper we describe our 
method using example bilingual web pages in 
English and Chinese, however, the method can 
be applied to extract bilingual data from web 
pages written in any other pair of languages, 
such as Japanese and English, Korean and Eng-
lish etc. 
2 Related Work 
Mining Bilingual Data from the Web 
As far as we know, there is no publication avail-
able on mining parallel sentences directly from 
bilingual web pages. Most existing methods of 
mining bilingual sentences from the Web, such 
as (Nie et al, 1999; Resnik and Smith, 2003; Shi 
et al, 2006), mine parallel web documents within 
bilingual web sites first and then extract bilingual 
sentences from mined parallel documents using 
sentence alignment methods. However, since the 
number of bilingual web sites is quite small, 
these methods can not yield a large number of 
bilingual sentences. (Shi et al, 2006), mined a 
total of 1,069,423 pairs of English-Chinese paral-
lel sentences. In addition to mining from parallel 
documents, (Munteanu and Marcu, 2005) pro-
posed a method for discovering bilingual sen-
tences in comparable corpora. 
As to the term translation extraction from bi-
lingual web pages, (Cao et al, 2007) and (Lin et 
al., 2008) proposed two different methods utiliz-
ing the parenthesis pattern. The primary insight 
is that authors of many bilingual web pages, es-
pecially those whose primary language is Chi-
nese, Japanese or Korean sometimes annotate 
terms with their English translations inside a pair 
of parentheses. Their methods are tested on a 
large set of web pages and achieve promising 
results. However, since not all translations in 
bilingual web pages follow the parenthesis pat-
tern, these methods may miss a lot of translations 
appearing on the Web.  
Apart from mining term translations directly 
from bilingual web pages, more approaches have 
been proposed to mine term translations from 
text snippets returned by a web search engine 
(Jiang et al, 2007; Zhang and Vines, 2004; 
Cheng et al, 2004; Huang et al, 2005). In their 
methods the source language term is usually giv-
en and the goal is to find the target language 
translations from the Web. To obtain web pages 
containing the target translations, they submit the 
source term to the web search engine and collect 
returned snippets. Various techniques have been 
proposed to extract the target translations from 
the snippets. Though these methods achieve high 
accuracy, they are not suitable for compiling a 
large-scale bilingual dictionary for the following 
reasons: 1) they need a list of predefined source 
terms which is not easy to obtain; 2) the relev-
ance ranking in web search engines is almost 
entirely orthogonal to the intent of finding the 
bilingual web pages containing the target transla-
tion, so many desired bilingual web pages may 
never be returned; 3) most such methods rely 
heavily on the frequency of the target translation 
in the collected snippets which makes mining 
low-frequency translations difficult. 
Moreover, based on the assumption that anc-
hor texts in different languages referring to the 
same web page are possibly translations of each 
other, (Lu et al, 2004) propose a novel approach 
to construct a multilingual lexicon by making use 
of web anchor texts and their linking structure. 
However, since only famous web pages may 
have inner links from other pages in multiple 
languages, the number of translations that can be 
obtained with this method is limited. 
Pattern-based Relation Extraction 
Pattern-based relation extraction has also been 
studied for years. For instance, (Hearst, 1992; 
Finkelstein-Landau and Morin, 1999) proposed 
an iterative pattern learning method for extract-
ing semantic relationships between terms. (Brin, 
1998) proposed a method called DIPRE (Dual 
Iterative Pattern Relation Expansion) to extract a 
relation of books (author, title) pairs from the 
Web. Since translation can be regarded as a kind 
of relation, those ideas can be leveraged for ex-
tracting translation pairs. 
3 Overview of the Proposed Approach 
Web 
pages
Seed mining
Pattern-based 
mining
Pattern 
learning
Preprocessing
Bilingual 
dictionary
input
output
depend
Translation 
pairs
Transliteration 
model depend
 
Figure 3. The framework of our approach 
872
As illustrated in Figure 3, our mining system 
consists of four main steps: preprocessing, seed 
mining, pattern learning and pattern based min-
ing. The input is a set of web documents and the 
output is mined bilingual data. 
In the preprocessing step, the input web doc-
uments are parsed into DOM trees and the inner 
text of each tree node is segment into snippets. 
Then we select those tree nodes whose inner 
texts are likely to contain translation pairs collec-
tively with a simple rule. 
The seed mining module receives the inner 
text of each selected tree node and uses a word-
based alignment model to identify potential 
translation pairs. The alignment model can han-
dle both translation and transliteration in a uni-
fied framework.  
The pattern learning module receives identi-
fied potential translation pairs from the seed min-
ing as input, and then extracts generalized pattern 
candidates with the PAT tree algorithm. Then a 
SVM classifier is trained to select good patterns 
from all extracted pattern candidates. 
In the pattern-based mining step, the selected 
patterns were used to match within the whole 
inner text to extract all translation pairs follow-
ing the patterns. 
4 Adaptive Pattern-based Bilingual Da-
ta Mining 
In this section, we will present the details about 
the four steps in the proposed approach. 
4.1 Preprocessing 
HTML Page Parsing 
The Document Object Model (DOM) is an appli-
cation programming interface used for parsing 
HTML documents. With DOM, an HTML doc-
ument is parsed into a tree structure, where each 
node belongs to some predefined types (e.g. DIV, 
TABLE, TEXT, COMMENT, etc.). We removed 
nodes with types of ?B?, ?FONT?, ?I? and so on, 
because they are mainly used for controlling vis-
ual effect. After removal, their child nodes will 
be directly connected to their parents. 
Text Segmentation 
After an HTML document is parsed, the inner 
text of each node in the DOM tree will be seg-
mented into a list of text snippets according to 
their languages. That means each snippet will be 
labeled as either an English snippet (E) or a Chi-
nese snippet (C).  
The text segmentation was performed based 
on the Unicode values of characters 4  first and 
then guided by the following rules to decide the 
boundary of a snippet under some special situa-
tions: 
1) Open punctuations (such as ?(?) are pad-
ded into next snippet, and close punctua-
tions (such as ?)?) are padded into pre-
vious snippet; other punctuations (such as 
?;?) are padded into previous snippet; 
2) English snippets which contains only 1 or 
2 ASCII letters are merged with previous 
and next Chinese snippets (if exist). Since 
sometimes Chinese sentences or terms al-
so contain some abbreviations in English. 
Table 1 gives some examples of how the inner 
texts are segmented. 
Inner text 
China Development Bank (??) ?
????? 
Segmentation 
China Development Bank |(?? ) 
?????? 
Inner text Windows XP ?????? XP? 
Segmentation Windows XP |?????? XP? 
Table 1. Example segmentations (?|? indicates the 
separator between adjacent snippets) 
Since a node?s inner text includes all inner 
texts of its children, the segmentation to all texts 
of a DOM tree has to be performed from the leaf 
nodes up to the root in order to avoid repetitive 
work. When segmenting a node?s inner text, we 
first segment the texts immediately dominated by 
this node and then combine those results with its 
children?s segmented inner texts in sequence. 
As a result of the segmentation, the inner text 
of every node will look like ??ECECC 5EC??. 
Two adjacent snippets in different languages (in-
dicated as ?EC? or ?CE?) are considered a Bilin-
gual Snippet Pair (BSP). 
Collective Nodes Selection 
Since our goal is to mine bilingual knowledge 
from collective bilingual pages, we have to de-
cide if a page is really a collective bilingual page. 
In this paper, the criterion is that a collective 
page must contain at least one Collective Node 
which is defined as a node whose inner text con-
tains no fewer than 10 non-overlapping bilingual 
snippet pairs and which contains less than 10 
                                                 
4 For languages with the same character zone, other tech-
niques are needed to segment the text. 
5 Adjacent snippets in the same language only appear in the 
inner texts of some non-leaf nodes. 
873
percent of other snippets which do not belong to 
any bilingual snippet pairs. 
4.2 Seed Mining 
The input of this module is a collective node 
whose inner text has been segmented into conti-
nuous text snippets, such 
as ?EkChEk+1Ch+1Ch+2?. In this step, every ad-
jacent snippet pair in different languages will be 
checked by an alignment model to see if it is a 
potential translation pair. The alignment model 
combines a translation and a transliteration mod-
el to compute the likelihood of a bilingual snip-
pet pair being a translation pair. If it is, we call 
the snippet pair as a Translation Snippet Pair 
(TSP). If both of two adjacent pairs, e.g. EkCh 
and ChEk+1, are considered as TSPs, the one with 
lower translation score will be regarded as a 
NON-TSP. 
Before computing the likelihood of a bilingual 
snippet pair being a TSP, we preprocess it via the 
following steps: 
a) Isolating the English and Chinese con-
tents from their contexts in the bilingual 
snippet pair. Here, we use a very simple 
rule: in the English snippet, we regard all 
characters within (and including) the first 
and the last English letter in the snippet as 
the English content; similarly, in the Chi-
nese snippet we regard all characters 
within (and including) the first and the 
last Chinese character in the snippet as 
the Chinese content; 
b) Word segmentation of the Chinese con-
tent. Here, the Forward Maximum Match-
ing algorithm (Chen and Liu, 1992) based 
on a dictionary is adopted; 
c) Stop words filtering. We compiled a 
small list of stop words manually (for ex-
ample, ?of?, ?to?, ???, etc.) and remove 
them from the English and Chinese con-
tent; 
d) Stemming of the English content. We use 
an in-house stemming tool to get the un-
inflected form of all English words. 
After preprocessing, all English words form a 
collection E={e1,e2,?,em } and all Chinese 
words constitute a collection C={c1,c2,?,cn}, 
where ei is an English word, and ci is a Chinese 
word. We then use a linking algorithm which 
takes both translation and transliteration into 
consideration to link words across the two col-
lections. 
In our linking algorithm, there are three situa-
tions in which two words will be linked. The first 
is that the two words are considered translations 
of each other by the translation dictionary. The 
second is that the pronunciation similarity of the 
two words is above a certain threshold so that 
one can be considered the transliteration of the 
other. The third is that the two words are identic-
al (this rule is especially designed for linking 
numbers or English abbreviations in Chinese 
snippets). The dictionary is an in-house dictio-
nary and the transliteration model is adapted 
from (Jiang et al, 2007).  
After the linking, a translation score over the 
English and Chinese content is computed by cal-
culating the percentage of words which can be 
linked in the two collections. For some pairs, 
there are many conflicting links, for example, 
some words have multiple senses in the dictio-
nary. Then we select the one with highest trans-
lation score.  
For example, given the bilingual snippet pair 
of ?Little Smoky River? and ???????, its 
English part is separated as ?Little/Smoky/River?, 
and its Chinese part is separated as ??/?/?/?/
??. According to the dictionary, ?Little? can be 
linked with ???, and ?River? can be linked with 
???. However, ?Smoky? is translated as ???
?? in the dictionary which does not match any 
Chinese characters in the Chinese snippet. How-
ever the transliteration score (pronunciation simi-
larity) between ?Smoky? (IPA: s.m.o.k.i) and 
??/?/?? (Pinyin: si mo ji) is higher than the 
threshold, so the English word ?Smoky? can be 
linked to three Chinese characters ???, ??? and 
???. The result is a translation score of 1.0 for 
the pair ?Little Smoky River? and ???????. 
4.3 Pattern Learning 
The pattern learning module is critical for mining 
bilingual data from collective pages, because 
many translation pairs whose translation scores 
are not high enough may still be extracted by 
pattern based mining methods. 
In previous modules, the inner texts of all 
nodes are segmented into continuous text snip-
pets, and translation snippet pairs (TSP) are iden-
tified in all bilingual snippet pairs. Next, in the 
pattern learning module, those translation snippet 
pairs are used to find candidate patterns and then 
a SVM classifier is built to select the most useful 
patterns shared by most translation pairs in the 
whole text. 
874
Candidate Pattern Extraction 
First, as in the seed mining module, we isolate 
the English and Chinese contents from their con-
texts in a TSP and then replace the contents with 
two placeholders ?[E]? and ?[C]? respectively. 
Second, we merge the two snippets of a TSP 
into a string and add a starting tag ?[#]? and an 
ending tag ?[#]? to its start and end. Following 
(Chang and Lui, 2001), all processed strings are 
used to build a PAT tree, and we then extract all 
substrings containing ?E? and ?C? as pattern 
candidates from the PAT tree. However, pattern 
candidates which start or end with ?[E]? (or 
?[C]?) will be removed, since they cannot speci-
fy unambiguous boundaries when being matched 
in a string.  
Web page authors commonly commit format-
ting errors when authoring the content into an 
html page, as shown in Figure 1. There, the ten 
bilingual terms should have been written in the 
same pattern, however, because of the mistaken 
use of ?.? instead of ???, the first translation 
pair follows a slightly different pattern. Some 
other typical errors may include varying length 
or types of white space, adjacent punctuation 
marks instead of one punctuation mark, and so 
on. To make the patterns robust enough to handle 
such variation, we generalized all pattern candi-
dates through the following two steps: 
1) Replace characters in a pattern with their 
classes. We define three classes of cha-
racters: Punctuation (P), Number (N), and 
White Space (S). Table 2 lists the three 
classes and the corresponding regular ex-
pressions in Microsoft .Net Framework6. 
2) Merge identical adjacent classes. 
Class Corresponding regular expression 
P [\p{P}] 
N [\d] 
S [\s] 
Table 2. Character classes 
For example, from the translation snippet pair 
of ?7. Don?t worry.? and ??????, we will 
learn the following pattern candidates: 
? ?#[N][P][S][E][P][S][C][P]#?; 
?  ?[N][P][S][E][P][S][C][P]#?; 
?  ?[N][P][S][E][P][S][C][P]?; 
? ? 
?  ?[S][E][P][S][C][P]?; 
                                                 
6 In System.Text.RegularExpressions namespace 
Pattern Selection 
After all pattern candidates are extracted, a SVM 
classifier is used to select the good ones: 
??? xwxfw ???? ,)(  
where, x?  is the feature vector of a pattern 
candidate pi, and w?  is the vector of weights. 
????,  stands for an inner product. f is the decision 
function to decide which candidates are good. 
In this SVM model, each pattern candidate pi 
has the following four features: 
1) Generality: the percentage of those bi-
lingual snippet pairs which can match pi 
in all bilingual snippet pairs. This feature 
measures if the pattern is a common pat-
tern shared by many bilingual snippet 
pairs; 
2) Average translation score: the average 
translation score of all bilingual snippet 
pairs which can match pi. This feature 
helps decide if those pairs sharing the 
same pattern are really translations; 
3) Length: the length of pi. In general, long-
er patterns are more specific and can pro-
duce more accurate translations, however, 
they are likely to produce fewer matches; 
4) Irregularity: the standard deviation of 
the numbers of noisy snippets. Here noisy 
snippets mean those snippets between any 
two adjacent translation pairs which can 
match pi. If the irregularity of a pattern is 
low, we can be confident that pairs shar-
ing this pattern have a reliably similar in-
ner relationship with each other. 
To estimate the weight vector, we extracted all 
pattern candidates from 300 bilingual web pages 
and asked 2 human annotators to label each of 
the candidates as positive or negative. The anno-
tation took each of them about 20 hours. Then 
with the labeled training examples, we use SVM 
light7 to estimate the weights. 
4.4 Pattern-based Mining 
After good patterns are selected, every two adja-
cent snippets in different languages in the inner 
text will be merged as a target string. As we 
mentioned previously, we add a starting tag ?[#]? 
and an ending tag ?[#]? to the start and end of 
every target string. Then we attempt to match 
each of the selected patterns in each of the target 
strings and extract translation pairs. If the target 
                                                 
7 http://svmlight.joachims.org/ 
875
string was matched with more than one pattern, 
the matched string with highest translation score 
will be kept. 
The matching process is actually quite simple, 
since we transform the learnt patterns into stan-
dard regular expressions and then make use of 
existing regular expression matching tools (e.g., 
Microsoft .Net Framework) to extract translation 
pairs. 
However, to make our patterns more robust, 
when transforming the selected patterns into 
standard regular expressions, we allow each cha-
racter class to match more than once. That means 
?[N]?, ?[P]? and ?[S]? will be transformed into 
?[\d]+?, ?[\p{P}]+? and ?[\s]+? respectively. And 
?[E]? and ?[C]? will be transformed into 
?[^\u4e00-\u9fa5]+? (any character except Chi-
nese character) and ?.+?, respectively. 
5 Experimental Results 
In the following subsections, first, we will report 
the results of our bilingual data mining on a large 
set of Chinese web pages and compare them with 
previous work. Second, we will report some ex-
perimental results on a manually constructed test 
data set to analyze the impact of each part of our 
method. 
5.1 Evaluation on a Large Set of Pages  
With the proposed method, we performed bilin-
gual data extraction on about 3.5 billion web 
pages crawled from Chinese web sites. Out of 
them, about 20 million were determined to con-
tain bilingual collective nodes. From the inner 
texts of those nodes, we extracted 12,610,626 
unique translation pairs. If we consider those 
pairs whose English parts contain more than 5 
words as sentence translations and all others as 
term translations, we get 7,522,803 sentence 
translations and 5,087,823 term translations. We 
evaluated the quality of these mined translations 
by sampling 200 sentence translations and 200 
term translations and presenting those to human 
judges, with a resulting precision of 83.5% for 
sentence translations and 80.5% for term transla-
tions. 
As we mentioned in Section 2, (Shi et al, 
2006) reported that in total they mined 1,069,423 
pairs of English-Chinese parallel sentences from 
bilingual web sites. However, our method yields 
about 7.5 million pairs, about seven times as 
many.  
We also re-implemented the extraction method 
using the parenthesis pattern proposed by (Lin et 
al., 2008) and were able to mine 6,538,164 bilin-
gual terms from the same web pages. A sample 
of 200 terms was submitted for human judgment, 
resulting in a precision of 78.5% which is a little 
lower than that of our original result. Further 
analysis showed that fewer than 20% of the bi-
lingual terms mined with our method overlap 
with the data mined using the re-implemented 
method proposed by (Lin et al, 2008). This indi-
cates that our method can find many translations 
which are not covered by the parenthesis pattern 
and therefore can be used together with the pa-
renthesis pattern based method to build a bilin-
gual lexicon. 
Out of the term translations we mined, we 
found many which co-occur with their source 
terms only once in the Web. We check this by 
searching in Google with a Boolean query made 
of the term and its translation and then get the 
number of pages containing the query. If one 
attempts to extract this kind of low-frequency 
translation using a search engine-based method, 
the desired bilingual page which contains the 
target translation is not likely to be returned in 
the top n results when searching with the source 
term as the query. Even if the desired page is 
returned, the translation itself may be difficult to 
extract due to its low frequency. 
5.2 Evaluation on a Human Made Test Da-
ta Set 
Besides the evaluation of our method on a huge 
set of web pages, we also carried out some expe-
riments on a human-constructed test data set. We 
randomly selected 500 collective nodes from the 
huge set of Chinese web pages and asked two 
annotators to label all bilingual data in their inner 
texts. Half of the labeled data are then used as 
the development data set and the rest as the test 
data set to evaluate our systems with different 
settings. Table 3 shows the evaluation results. 
Setting Type Recall Precision F-Score 
Without 
pattern 
Exact 52.2 75.4 61.7 
Fuzzy 56.3 79.3 65.8 
Without 
PG 
Exact 69.2 78.6 73.6 
Fuzzy 74.3 82.9 78.4 
With PG 
Exact 79.3 80.5 79.9 
Fuzzy 86.7 87.9 87.3 
Table 3. Performance of different settings 
In Table 3, ?Without pattern? means that we 
simply treat those seed pairs found by the align-
ment model as final bilingual data. ?Without PG? 
and ?With PG? mean not generalizing and gene-
ralizing the learnt patterns to class based form, 
876
respectively. Evaluation type ?Exact? means the 
mined bilingual data are considered correct only 
if they are exactly same as the data labeled by 
human, while ?Fuzzy? means the mined bilin-
gual data are considered correct if they contain 
the data labeled by the human. 
As shown in Table 3, the system without pat-
tern-based extraction yields only 52.2% recall. 
However, after adding pattern-based extraction, 
recall is improved sharply, to 69.2% for ?With-
out PG? and to 79.3% for ?With PG?. Most of 
the improvement comes from those translations 
which have very low translation scores and 
therefore are discarded by the seed mining mod-
ule, however, most of them are found with the 
help of the learnt patterns. 
From Table 3, we can also see that the system 
?With PG? outperforms ?Without PG? in terms 
of both precision and recall. The reason may be 
that web writers often make mistakes when writ-
ing on web pages, such as punctuation misuse, 
punctuation loss, and extra spaces etc., so ex-
tracting with a strict surface pattern will often 
miss those translations which follow slightly dif-
ferent patterns.  
To find out the reasons why some non-
translation pairs are extracted, we checked 20 
pairs which are not translations but extracted by 
the system. Out of them, 5 are caused by wrong 
segmentations. For example, ????????
????? Double Concerto for Violin and 
Cello D??????? Symphony No.2 in D 
Major? is segmented into ??????????
????, ?Double Concerto for Violin and Cello 
D?, ?????????, and ?Symphony No.2 in 
D Major?. However, the ending letter ?D? of the 
second segment should have been padded into 
the third segment. For 9 pairs, the Chinese parts 
are explanative texts of corresponding English 
texts, but not translations. Because they contain 
the translations of the key words in the English 
text, our seed mining module failed to identify 
them as non-translation pairs. For 3 pairs, they 
follow the same pattern with some genuine trans-
lation pairs and therefore were extracted by the 
pattern based mining module. However, they are 
not translation pairs. For the other 3 pairs, the 
errors came from the pattern generalization. 
To evaluate the contribution of each feature 
used in the pattern selection module, we elimi-
nated one feature at a time in turn from the fea-
ture set to see how the performance changed in 
the absence of any single feature. The results are 
reported below. 
Eliminated feature F-Score (Exact) 
Null 79.9 
Generality 72.3 
Avg. translation score 74.3 
Length 77.5 
Irregularity 76.6 
Table 4. Contribution of every feature 
From the table above, we can see that every 
feature contributes to the final performance and 
that Generality is the most useful feature among 
all four features. 
6 Conclusions  
Bilingual web pages have shown great potential 
as a source of up-to-date bilingual 
terms/sentences which cover many domains and 
application types. Based on the observation that 
many web pages contain bilingual data collec-
tions which follow a mostly consistent but possi-
bly somewhat variable pattern, we propose a uni-
fied approach for mining bilingual sentences and 
terms from such pages. Our approach can adap-
tively learn translation patterns according to dif-
ferent formatting styles in various web pages and 
then use the learnt patterns to extract more bilin-
gual data. The patterns are generalized to minim-
ize the impact of format variation and typos. Ac-
cording to experimental results on a large set of 
web pages as well as on a manually made test 
data set, our method is quite promising. 
In the future, we would like to integrate the 
text segmentation module with the seed mining 
and pattern learning module to improve the accu-
racy of text segmentation. We also want to eva-
luate the usefulness of our mined data for ma-
chine translation or other applications. 
 
 
References  
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra and 
R. L. Mercer. 1993. The mathematics of statistical 
machine translation: parameter estimation. Compu-
tational Linguistics, 19:2, 263-311. 
Sergey Brin. 1998. Extracting patterns and relations 
from the World Wide Web. In Proc. of the 1998 In-
ternational Workshop on the Web and Databases. 
Pp: 172-183. 
G.H. Cao, J.F. Gao and J.Y. Nie. 2007. A system to 
mine large-scale bilingual dictionaries from mono-
lingual web pages. MT summit. Pp: 57-64. 
877
Chia-Hui Chang and Shao-Chen Lui. 2001. IEPAD: 
Inform extract based on pattern discovery. In Proc. 
of the 10th ACM WWW conference.  
Keh-Jiann Chen, Shing-Huan Liu. 1992. Word Identi-
fication for Mandarin Chinese Sentences. In the 
Proceedings of COLING 1992. Pp:101-107. 
Cheng, P., Teng, J., Chen, R., Wang, J., Lu, W., and 
Cheng, L. 2004. Translating Unknown Queries 
with Web Corpora for Cross-Language Information 
Retrieval. In the Proceedings of SIGIR 2004, pp 
162-169.  
Michal Finkelstein-Landau, Emmanuel Morin. 1999. 
Extracting Semantic Relationships between Terms: 
Supervised vs. Unsupervised Methods. In Proceed-
ings of International Workshop on Ontological En-
gineering on the Global Information Infrastructure. 
Pp:71-80. 
Marti A. Hearst. 1992. Automatic Acquisition of Hy-
ponyms from Large Text Corpora. In the Proceed-
ings of COLING-92. Pp: 539-545. 
Huang, F., Zhang, Y., and Vogel, S. 2005. Mining 
Key phrase Translations from Web Corpora. In the 
Proceedings of HLT-EMNLP. 
L. Jiang, M. Zhou, L.-F. Chien, C. Niu. 2007. Named 
Entity Translation with Web Mining and Translite-
ration, Proceedings of the 20th IJCAI. Pp: 1629-
1634. 
D. Lin, S. Zhao, B. Durme and M. Pasca. 2008. Min-
ing Parenthetical Translations from the Web by 
Word Alignment. In ACL-08. pp 994-1002. 
Lu, W. and Lee, H. 2004. Anchor text mining for 
translation of Web queries: A transitive translation 
approach. ACM transactions on Information Sys-
tems, Vol.22, April 2004, pages 242-269.  
D. S. Munteanu, D. Marcu. Improving Machine 
Translation Performance by Exploiting Non-
Parallel Corpora. 2005. Computational Linguistics. 
31(4). Pp: 477-504. 
J-Y Nie, M. Simard, P. Isabelle, and R. Durand. 1999. 
Cross-Language Information Retrieval Based on 
Parallel Texts and Automatic Mining of parallel 
Text from the Web. In SIGIR 1999. Pp: 74-81. 
Philip Resnik, Noah A. Smith. 2003. The Web as a 
Parallel Corpus. Computational Linguistics. 29(3). 
Pp: 349-380. 
Li Shao and Hwee Tou Ng. 2004. Mining new word 
translations from comparable corpora. In Proc. of 
COLING 2004. Pp: 618?624. 
Lei Shi, Cheng Niu, Ming Zhou, Jianfeng Gao. 2006. 
A DOM Tree Alignment Model for Mining Paral-
lel Data from the Web. In ACL 2006. 
Jung H. Shin, Young S. Han and Key-Sun Choi. 1996. 
Bilingual knowledge acquisition from Korean-
English parallel corpus using alignment method: 
Korean-English alignment at word and phrase level. 
In Proceedings of the 16th conference on Computa-
tional linguistics, Copenhagen, Denmark. 
J.C. Wu, T. Lin and J.S. Chang. 2005. Learning 
Source-Target Surface Patterns for Web-based 
Terminology Translation. ACL Interactive Poster 
and Demonstration Sessions,. Pp 37-40, Ann Arbor. 
Zhang, Y. and Vines, P.. 2004. Using the Web for 
Automated Translation Extraction in Cross-
Language Information Retrieval. In the Proceed-
ings of SIGIR 2004. Pp: 162-169. 
878
