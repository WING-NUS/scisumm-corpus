Computational Complexity of Statistical Machine  Translation





Raghavendra Udupa U. 
IBM India Research Lab 
New Delhi
India
uraghave@in.ibm.com


Hemanta 
K. Maji
Dept. of Computer Science 
University of Illinois at Urbana-
Champaigne 
hemanta.maji@gmail.com










Abstract

In  this  paper  we  study  a  set  of  prob- 
lems that are of considerable importance 
to Statistical Machine Translation (SMT) 
but which have not been addressed satis- 
factorily by the SMT research community. 
Over the last decade, a variety of SMT 
algorithms have been built and empiri- 
cally tested whereas little is known about 
the computational complexity of some of 
the fundamental problems of SMT. Our 
work aims at providing useful insights into 
the the computational complexity of those 
problems. We prove that while IBM Mod- 
els 1-2 are conceptually and computation- 
ally simple, computations involving the 
higher (and more useful) models are hard.
Since it is unlikely that there exists a poly-


language 1  (Tillman, 2001), (Wang, 1997), 
(Ger- mann et al., 2003), (Udupa et al., 2004).  
The models are independent of the language 
pair and therefore, can be used to build a 
translation sys- tem for any language pair as 
long as a parallel corpus of texts is available 
for training.  Increas- ingly,  parallel  corpora  
are  becoming  available for many language 
pairs and SMT systems have been built for 
French-English, German-English, Arabic-
English, Chinese-English, Hindi-English and 
other language pairs (Brown et al., 1993), (Al- 
Onaizan et al., 1999), (Udupa, 2004).
  In SMT, every English sentence e is 
considered as a translation of a given French 
sentence f with
probability P r (f |e).   Therefore, the problem 
of
translating f can be viewed as a problem of 
finding
the most probable translation 
of f :
e∗ = argmax P r(e|f ) = argmax P r(f |e)P 
(e).


e
nomial time solution for any of these hard


e
(1)


problems (unless P  = NP  and P#P = 
P), our results highlight and justify the 
need for developing polynomial time ap- 
proximations for these computations. We 
also discuss some practical ways of deal- 
ing with complexity.

1   Introduction

Statistical Machine Translation is a data driven 
machine translation technique which uses proba- 
bilistic models of natural language for automatic


The   probability   distributions   P r(f |e)   and
P r(e) are known as translation  model and lan-
guage model respectively. In the classic work on 
SMT, Brown and his colleagues at IBM introduced 
the notion of alignment between a sentence f and 
its translation e and used it in the development of 
translation models (Brown et al., 1993). An align- 
ment between f  = f1 . . . fm  and e = e1 . . . el
is a many-to-one mapping a  :  {1, . . . , m}   →
{0, . . . , l}. Thus, an alignment a between f and e
associates the french word fj to the English word
2 . The number of words of f mapped to ei  by


translation (Brown et al., 1993), (Al-Onaizan et 
al., 1999).   The parameters of the models are 
estimated by iterative maximum-likelihood train- 
ing on a large parallel corpus of natural language 
texts using the EM algorithm (Brown et al., 1993). 
The models are then used to decode, i.e.  trans- 
late texts from the source language to the target


eaj

a is called the fertility of ei  and is denoted by φi . 
Since P r(f |e) = �a P r(f , a|e), equation 1 can

  1 In this paper, we use French and English as the prototyp- 
ical examples of source and target languages respectively.
2 e0 is a special word called the null word and is used to
account for those words in f that are not connected by a to 
any of the words of e.


be rewritten as follows:
e∗ = argmax ) P r(f , a|e)P r(e). 	(2)


• Relaxed Decoding
Given the model parameters and a sentence f ,


e	determine the most probable translation and
a 	alignment pair for f .


Brown and his colleagues developed a series
of 5 translation models which have become to be 
known in the field of machine translation as IBM 
models. For a detailed introduction to IBM trans- 
lation models, please see (Brown et al., 1993). In 
practice, models 3-5 are known to give good re- 
sults and models 1-2 are used to seed the EM it- 
erations of the higher models.  IBM model 3 is 
the prototypical translation model and it models
P r(f , a|e) as follows:


(e∗, a∗) = argmax P (f , a|e) P (e)
(e,a)


  Viterbi Alignment computation finds applica- 
tions not only in SMT but also in other areas 
of Natural Language Processing (Wang, 1998), 
(Marcu,  2002).    Expectation Evaluation  is the 
soul of parameter estimation (Brown et al., 1993), 
(Al-Onaizan et al., 1999).   Conditional Proba- 
bility computation is important in experimentally
studying the concentration of the probability mass


P (f , a|e) ≡ n(φ0 | �l


\ 	l
i=1


n(φi|ei ) φi!


around the Viterbi 
alignment, i.e.  in 
determining


j=1 t


 fj |eaj


× dj: aj I=0 
d(j|i, m, l)


the goodness 
of the Viterbi 
alignment in 
compar-
ison to the 
rest of the 
alignments.   
Decoding is 
an integral 
component of 
all SMT 
systems 
(Wang,


Table 1: IBM Model 3

  Here,  n(φ|e)  is the fertility  model, t(f |e)  is 
the lexicon model and d(j|i, m, l) is the distortion
model.
  The computational tasks involving IBM Models 
are the following:
• Viterbi Alignment
Given the model parameters and a sentence
pair  (f , e),   determine  the  most  probable 
alignment between f and e.
a∗ = argmax P (f , a|e)
a
• Expectation  Evaluation
This forms the core of model training via the
EM algorithm.   Please see Section 2.3 for 
a description of the computational task in- 
volved in the EM iterations.
• Conditional  Probability

Given the model parameters and a sentence 
pair (f , e), compute P (f |e).


1997), (Tillman, 2000), (Och et al., 2001), (Ger- 
mann et al., 2003), (Udupa et al., 2004).  Exact 
Decoding is the original decoding problem as de- 
fined in (Brown et al., 1993) and Relaxed Decod- 
ing is the relaxation of the decoding problem typ- 
ically used in practice.
  While several heuristics have been developed 
by practitioners of SMT for the computational 
tasks involving IBM models, not much is known 
about the computational complexity of these tasks. 
In their seminal paper on SMT, Brown and his col- 
leagues highlighted the problems we face as we go 
from IBM Models 1-2 to 3-5(Brown et al., 1993)
3:
  “As we progress from Model 1 to Model 5, eval- 
uating the expectations that gives us counts be- 
comes increasingly difficult.  In Models 3 and 4, 
we must be content with approximate EM itera- 
tions because it is not feasible to carry out sums 
over all possible alignments for these models. In 
practice, we are never sure that we have found the 
Viterbi alignment”.
However, neither their work nor the subsequent


P (f |e) = )
a
• Exact Decoding


P (f , a|e)


research in SMT studied the 
computational com- plexity 
of these fundamental 
problems with the exception 
of the Decoding problem.  
In (Knight,
1999) it was proved that the 
Exact Decoding prob-


Given the model parameters and a sentence f , 
determine the most probable translation of f .


lem is NP-Hard when the language model is a bi- 
gram model.


e∗ = argmax
e


) P (f , a|e) P (e)
a


Our results may be 
summarized as follows:

3 The emphasis is ours.



1. Viterbi Alignment computation is NP-Hard 
for IBM Models 3, 4, and 5.

2. Expectation  Evaluation  in EM Iterations is
#P-Complete for IBM Models 3, 4, and 5.

3. Conditional    Probability   computation   is
#P-Complete for IBM Models 3, 4, and 5.

4. Exact Decoding is #P-Hard for IBM Mod- 
els 3, 4, and 5.

5. Relaxed  Decoding  is  NP-Hard   for  IBM 
Models 3, 4, and 5.



lies in the class #P, where p(.) is a 
polynomial.
  Given functions f, g : Σ∗ → N, we say that 
g is polynomial-time Turing reducible to f 
(i.e. g ≤T
f ) if there is a Turing machine with an oracle 
for f that computes g in time polynomial in 
the size of the input. Similarly, we say that f 
is #P-Hard, if every function in #P can be 
polynomial time Turing reduced to f .  If f is 
#P-Hard and is in
#P, then we say that f is #P-
Complete.

2.1   Viterbi Alignment 
Computation

VITER B I-3 is defined as follows. Given the 
para- meters of IBM Model 3 and a sentence 
pair (f , e),



  Note that our results for decoding are sharper 
than that of (Knight, 1999). Firstly, we show that 
Exact Decoding is #P-Hard for IBM Models 3-5 
and not just NP-Hard.   Secondly, we show that 
Relaxed Decoding is NP-Hard  for Models 3-5


compute the most probable alignment a∗
and e:
a∗ = argmax P (f , a|e).
a


betwen f


even when the language model is a uniform dis- 
tribution.
  The rest of the paper is organized as follows. 
We formally define all the problems discussed in 
the paper (Section 2). Next, we take up each of the 
problems discussed in this section and derive the 
stated result for them (Section 3).  After this, we 
discuss the implications of our results (Section 4) 
and suggest future directions (Section 5).

2	Problem Definition

Consider the functions f, g : Σ∗  → {0, 1}.  We 
say that g ≤m  f (g is polynomial-time many-one
reducible to f ), if there exists a polynomial time 
reduction r(.) such that g(x)  = f (r(x)) for all


2.2   Conditional  Probability 
Computation

PRO BA B ILITY-3 is  defined as  follows.    
Given the  parameters  of  IBM  Model  3,  
and  a  sen- tence   pair   (f , e),    compute   
the   probability
P (f |e) = �a P (f , 
a|e).


2.3   Expectation  Evaluation  in EM 
Iterations

(f, e)-CO U N T-3,   (φ, e)-CO U N T-3,   (j, i, m, 
l)- CO U N T-3, 0-CO U N T-3, and 1-CO U N T-3 
are de- fined respectively as follows.  Given 
the parame- ters of IBM Model 3, and a 
sentence pair (f , e), compute the following 4 :
c(f |e; f , e) = ) P (a|f , e) ) δ(f, fj )δ(e, eaj 
),


input instances x ∈  Σ∗.  This means that given a	a 	j
machine to evaluate f (.) in polynomial time, there



exists a machine that can evaluate g(.) in polyno-


c(φ|e; f , e) = ) P (a|f , e) ) δ(φ, φi )δ(e, ei ),


mial time. We say a function f is NP-Hard,  if all	a 	i



functions in NP  are polynomial-time many-one 
reducible to f .  In addition, if f ∈  NP,  then we
say that f is NP-Complete.
  Also relevant to our work are counting func- 
tions that answer queries such as “how many com- 
putation paths exist for accepting a particular in- 
stance of input?”  Let w be a witness for the ac- 
ceptance of an input instance x and χ(x, w) be 
a polynomial time witness checking function (i.e.
χ(x, w) ∈ P). The function f : Σ∗ → N such that


c(j|i, m, l; f , e) = ) P (a|f , e)δ(i, 
aj ),
a
c(0; f , e) = ) P (a|f , e)(m − 2φ0 ), and
a
c(1; f , e) = ) P (a|f , 
e)φ0 .
a

2.4	Decoding

E-DEC O D IN G -3 and R-DEC O D IN G -3 are 
defined as follows. Given the parameters of 
IBM Model 3,


f (x) =	)
w∈Σ∗
|w|≤p(|x|)



χ(x, w)



    4 As the counts are 
normalized in the EM iteration, 
we can replace P (a|f , e) by P 
(f , a|e) in the Expectation 
Evaluation tasks.



and a sentence f , compute its most probable trans- 
lation according to the following equations respec- 
tively.



    Proof:  We give a polynomial time many-
one reduction from SETCOV ER to VITER B I-3. 
Given
a collection of sets C   = {S1, . . . , Sl } and a set


X	l 
i=1


Si, we create an instance of VITER B I-3


e∗	=	argmax
e


) P (f , a|e) P (e)
a


as follows:
For each set Si  ∈ C, we 
create a word ei  (1 ≤ i ≤


(e∗, a∗)   =	argmax P (f , a|e) P (e).
(e,a)

2.5	SETCOV ER
Given a collection of sets C  = {S1 , . . . , Sl } and


l). Similarly, for each element vj  ∈  X we create
a word fj (1 ≤ j ≤ |X| = m). We set the model
parameters as follows:
(1	if vj  ∈ Si


a set X ⊆  ∪l


Si , find the 
minimum 
cardinality


t(fj |ei) =



0	otherwise


subset Ct  of C  such that every element in X be-
longs to at least one member of Ct.
SETCOV ER   is  a  well-known  NP-Complete
problem.   If SETCOV ER  ≤m   f , then f is NP-



n(φ|e) =


2φ!	if φ j= 0
1	if φ = 0


p
Hard.

2.6	PER MA N EN T


d(j|i, m, l) = 1.

Now	consider	the
	sentences	e	=
e1 . . . el  and f = f1 . . . fm.


Given a matrix M = [Mj,i]n×n whose entries are
either 0 or 1, compute the following:
perm(M) = �  dn	Mj,π    where π is a per-
mutation of 1, . . . , n.



P (f , a|e) = n



l
φ0| ) φi
i=1
m


\ l
n n(φi |ei) φi!
i=1


This problem is the same as that of counting the
number of perfect matchings in a bipartite graph 
and is known to be #P-Complete (?). If PER MA -
N EN T ≤T  f , then f is #P-Hard.


× n t fj |eaj
j=1

l	1
=	1−δ(φi ,0)


n

j: aj I=0


d(j|i, m, l)



2.7	CO MPA R EPER MA N EN TS
Given two matrices A   = [Aj,i]n×n   and B   =


i=1 2

We can construct a cover for





X from the output


[Bj,i]n×n whose entries are either 0 or 1, determine


of VITER B I-3 by defining Ct
n


= {Si
1


|φi


> 0}.  We
−|C!|


which of them has a larger permanent.  PER MA -


note that P (f , a|e) = di=1


1−δ(φ ,0)  = 2	.


2	i


N EN T  is known to be Turing reducible to CO M-

PA R EPER MA N EN TS (Jerrum, 2005) and therefore, 
if CO MPA R EPER MA N EN TS ≤T  f , then f is #P-
Hard.

3   Main Results

In this section, we present the main reductions 
for the problems with Model 3 as the translation 
model. Our reductions can be easily carried over
to Models 4− 5 with minor modifications. In order
to keep the presentation of the main ideas simple,
we let the lexicon, distortion, and fertility models 
to be any non-negative functions and not just prob-


Therefore, Viterbi alignment results in the mini-
mum cover for X.

3.2	PRO BA B ILITY-3

We show that PRO BA B ILITY-3 is #P-
Complete. We begin by proving the following:
Lemma 2 PER MA N EN T ≤T  PRO BA B ILITY-3.
    Proof:	Given  a   0, 1-matrix  M 
	= [Mj, i]n×n,  we define f   
= f1 . . . fn  and e  =
e1 . . . en  where each ei  and fj is distinct and set
the Model 3 parameters as follows:
(1	if Mj,i  = 1


ability distributions in our reductions.

3.1	VITER B I-3

We show that VITER B I-3 is NP-Hard.


t(fj |ei ) =

n(φ|e) =



0	otherwise
(1	if φ = 1
0	otherwise


Lemma 1 SETCOV ER ≤m  VITER B I-3.


d(j|i, n, n) = 1.


Clearly,   with  the  above  parameter  setting,


e1 e2  . . . en eˆ. We set the translation model para-


P (f , a|e)  = dn


Mj, aj


if a is a permutation


meters as follows:


and 0 otherwise. Therefore,


P (f |e) = ) P (f , a|e)
a



t(f |e) =


1	if f = fj , e = ei and 
Mj,i  = 1
1	if f = fˆ and e = eˆ
0	otherwise.


n
=	)	n Mj, aj   = perm(M)


The rest of the parameters are set as in Lemma 2.
Let A be the set of alignments a, such that an+1  =


a is a permutation j=1


  Thus, by construction, PRO BA B ILITY-3 com- 
putes perm(M).   Besides, the construction con-
serves the number of witnesses. Hence, PER MA -
N EN T ≤T  PRO BA B ILITY-3.


n + 1 and an is a permutation of 1, 2, . . . , n. 
Now,


n+
1
c (fˆ|eˆ; f , e\ = ) P (f , a|e) ) δ(fˆ, fj )δ(eˆ, ea  )
a 
	j=
1
n+
1


We now prove that


= ) P (f , a|e) ) 
δ(fˆ, fj )δ(eˆ, eaj )


Lemma 3 PRO BA B ILITY-3 is in #P.

Proof:	Let   (f , e)  be   the   input   to
PRO BA B ILITY-3.	Let m and l be the lengths 
of f  and e respectively.	With each alignment


a∈A
= ) P (f , a|e)
a∈A
n
= ) n Mj, a


j=1


a = (a1 , a2 , . . . , am ) we associate a unique num- 
ber na   = a1a2 . . . am  in base l + 1.   Clearly,


a∈A j=1


j = perm(M) .


0 ≤  na   ≤  (l + 1)m  − 1.  Let w be the binary
encoding of na.   Conversely, with every binary

string w we can associate an alignment a if the 
value of w is in the range 0, . . . , (l + 1)m − 1. It
requires O (m log (l + 1)) bits to encode an align-
ment. Thus, given an alignment we can compute
its encoding and given the encoding we can com- 
pute the corresponding alignment in time polyno- 
mial in l and m. Similarly, given an encoding we
can compute P (f , a|e) in time polynomial in l and
m. Now, if p(.) is a polynomial, then function


Therefore, PER MA N EN T ≤T  CO U N T-3.
Lemma 5 (f, e)-CO U N T-3 is in #P.
    Proof:   The proof is essentially the same 
as that of Lemma 3. Note that given an encoding 
w,
P (f , a|e) �m    δ (fj , f ) δ  ea  , e   can be evalu-
ated in time polynomial in |(f , e)|.
Hence, from Lemma 4 and Lemma 5, it follows
that
Theorem 2 (f, e)-CO U N T-3 is #P-Complete.

3.4	(j, i, m, l)-CO U N T-3


f (f , e) =	)
∗
w∈{0,1}


P (f , a|e)


Lemma 6 PER MA N EN T  ≤T   
(j, i, m, l)-CO U N T-
3.


|w|≤p(|(f , e)|)
is in  #P.	Choose p (x)   =  \x log2 (x + 1)l. 
Clearly, all alignments can be encoded using at
most p (| (f , e) |) bits.  Therefore, if (f , e) com-
putes P (f |e) and hence,  PRO BA B ILITY-3 is in
#P.
It  follows  immediately  from  Lemma  2  and
Lemma 3 that

Theorem 1 PRO BA B ILITY-3 is #P-Complete.

3.3	(f, e)-CO U N T-3
Lemma 4 PER MA N EN T ≤T  (f, e)-CO U N T-3.

Proof:	The  proof  is  similar  to  that  of


Proof:	We	proceed	as	in	the
proof	of		Lemma	4		with	some
	modifica- tions.		Let  e	=		e1 . . . 
ei−1eˆei . . . en    and
f 	=	f1 . . . fj−1fˆfj . . . fn.	The  parameters
are set as in Lemma 4.	Let A be the set of
alignments,  a,  such that a is a permutation of
1, 2, . . . , (n + 1) and  aj    =  i.	Observe that
P (f , a|e) is non-zero only for the alignments in

A.   It follows immediately that with these 
para- meter settings,  c(j|i, n, n; f , e)  =  
perm(M) .


Lemma 7 (j, i, m, l)-CO U N T-3 is in #P.
Proof:   Similar to the proof of Lemma 
5.
Theorem 3 (j, i, m, l)-CO U N T-3	is	#P-


Lemma 2.   Let f  = f1  f2   . . .  fn


fˆ and e =


Complete.


3.5	(φ, e)-CO U N T-3
Lemma 8 PER MA N EN T ≤T  (φ, e)-CO U N T-3.


3.8	E-DEC O D IN G -3
Lemma 14 CO MPA R EPER MA N EN TS 	≤T	 
E-


Proof:	Let  e   =  e1 . . . eneˆ
k  	


and  f 	=


DEC O D IN G -3
Proof:   Let M and N  be the two 
0-1 matri-


f1 . . . fn


    '   
f . . . f .


Let A be 
the set of 
alignment
s


ces.  Let f  
= f1f2 . . 
. fn, e(1)  
= e(1)


(1)
2


. . . en


for which an  is a permutation of 1, 2, . . . , n and


and e(2)  = e(2)e(2) . . . e(2) .  Further, let e(1)  and


1	1	2	n



an+k


k
  
	
 	 '	 


e(2)  
have 
no 
words 
in 
comm
on 
and 
each 
word


n+1  =(n + 1) . . . (n + 1). We set
1	if φ = 1 and e j= eˆ



appears exactly once.  By setting the 
bigram lan-
guage model probabilities of the 
bigrams that oc- cur in e(1) and e(2) to 
1 and all other bigram prob-


n(φ|e) =


1	if φ = k and 
e = eˆ
0	otherwise.


abilities to 0, we 
can ensure that 
the only trans-
lations considered 
by E-DEC O D IN G -
3 are indeed
e(1)  and e(2)  and 
P  e(1) 	= P  e(2) 	= 1. We



The rest of the parameters are set as in Lemma 4. 
Note that P (f , a|e) is non-zero only for the align-
ments in A. It follows immediately that with these


then set



1	if f = fj , e = e(1)



and Mj,i  = 1


	i


parameter settings, c(k|eˆ; f , e) = perm(M) .


t(f |e) =


1	if f = fj , e = e(2)  and N


j,i  = 1


Lemma 9 (φ, e)-CO U N T-3 is in #P.

Proof:  Similar to the proof of Lemma 5.

Theorem 4 (φ, e)-CO U N T-3 is #P-Complete.




n(φ|e) =


0	otherwise
(1	φ = 1
0	otherwise



3.6	0-CO U N T-3
Lemma 10 PER MA N EN T ≤T  0-CO U N T-3.
    Proof:  Let e = e1 . . . en and f = f1 . . . fnfˆ. 
Let A be the set of alignments, a, such that an is 
a permutation of 1, . . . , n and an+1  = 0. We set
1	if f = fj , e = ei and Mj, i = 1



d(j|i, n, n) 
= 1.

Now, P  f |e(1)    = perm(M), and P  f |e(2)    
= perm(N ).    Therefore,  given the  output 
of  E- DEC O D IN G -3 we can find out which 
of M and N  has a larger permanent.
Hence E-DEC O D IN G -3 is #P − H 
ard.

3.9   R-DEC O D IN 
G -3


t(f |e) =


1	if f = 
fˆ and e = 
NULL
0
	other
wise.


Lemma 15 
SETCOV ER 
≤m  R-DEC 
O D IN G -3
    P
roof:   
Given 
an 
instanc
e of 
SETC
OV ER, 
we set 
the 
param
eters 
as in 
the 
proof 
of 
Lemm
a 1 
with


The rest of the parameters are set as in Lemma 4. 
It is easy to see that with these settings, c(0;f ,e)  =


the following modification:
( 1




if φ > 0


perm(M) .
Lemma 11 0-CO U N T-3 is in #P.


n(φ|e) =


2φ!
0	otherwise.



Proof:  Similar to the proof of Lemma 5.

Theorem 5 0-CO U N T-3 is #P-Complete.

3.7	1-CO U N T-3
Lemma 12 PER MA N EN T ≤T  1-CO U N T-3.
Proof:	We  set  the  parameters  as  in
Lemma   10.	It   follows   immediately   that
c(1; f , e) = perm(M) .
Lemma 13 1-CO U N T-3 is in #P.

Proof:  Similar to the proof of Lemma 5.

Theorem 6 1-CO U N T-3 is #P-Complete.


Let e be the optimal translation obtained by 
solv- ing R-DEC O D IN G -3.  As the language 
model is uniform, the exact order of the 
words in e is not important. Now, we observe 
that:
• e	contains	words	only	from	the	set
{e1 , e2 , . . . , el }.  This is because, there can-
not be any zero fertility word as n(0|e) = 0

and the only words that can have a non-
zero fertility are from {e1, e2 , . . . , el } 
due to the
way we have set the lexicon parameters.
• No word occurs more than once in e. Assume
on the contrary that the word ei occurs k > 1



times in e.  Replace these k occurrences by 
only one occurrence of ei and connect all the 
words connected to them to this word.  This 
would increase the score of e by a factor of
2k−1  > 1 contradicting the assumption on
the optimality of e.

As a result, the only candidates for e are subsets of
{e1 , e2 , . . . , el } in any order. It is now straight for-
ward to verify that a minimum set cover can be re-
covered from e as shown in the proof of Lemma 1.


3.10   IBM Models 4 and 5
The reductions are for Model 3 can be easily ex- 
tended to Models 4 and 5. Thus, we have the fol- 
lowing:
Theorem 7 Viterbi   Alignment   computation   is
NP-Hard for IBM Models 3 − 5.

Theorem 8 Expectation  Evaluation  in  the  EM 
Steps is #P-Complete for IBM Models 3 − 5.

Theorem 9 Conditional Probability computation 
is #P-Complete for IBM Models 3 − 5.

Theorem 10 Exact  Decoding  is  #P-Hard  for
IBM Models 3 − 5.
Theorem 11 Relaxed Decoding is NP-Hard for
IBM Models 3 − 5 even when the language model



lexicon and alignment models. Whereas, in Mod- 
els 3-5, the fertility model is independent of the 
lexicon and alignment models. It is precisely this 
freedom that makes computations on Models 3-5 
harder than the computations on Models 1-2.
  There are three different ways of dealing with 
the computational barrier posed by our problems. 
The first of these is to develop a restricted fertil- 
ity model that permits polynomial time computa- 
tions. It remains to be found what kind of parame- 
terized distributions are suitable for this purpose. 
The second approach is to develop provably good 
approximation algorithms for these problems as is 
done with many NP-Hard  and #P-Hard prob- 
lems.  Provably good approximation algorithms 
exist for several covering problems including Set 
Cover and Vertex Cover. Viterbi Alignment is itself 
a special type of covering problem and it remains 
to be seen whether some of the techniques devel- 
oped for covering algorithms are useful for finding 
good approximations to Viterbi Alignment.  Sim- 
ilarly, there exist several techniques for approxi- 
mating the permanent of a matrix. It needs to be 
explored if some of these ideas can be adapted for 
Expectation Evaluation.
  As the third approach to deal with complex- 
ity, we can approximate the space of all possi-
m



is a uniform distribution.


ble (l + 1)


alignments by an 
exponentially large



4   Discussion

Our results answer several open questions on the 
computation of Viterbi Alignment and Expectation 
Evaluation.   Unless P  = NP  and P#P  = P, 
there can be no polynomial time algorithms for 
either of these problems.  The evaluation of ex- 
pectations becomes increasingly difficult as we go 
from IBM Models 1-2 to Models 3-5 exactly be- 
cause the problem is #P-Complete for the latter 
models. There cannot be any trick for IBM Mod- 
els 3-5 that would help us carry out the sums over 
all possible alignments exactly. There cannot exist 
a closed form expression (whose representation is
polynomial in the size of the input) for P (f |e) and
the counts in the EM iterations for Models 3-5.
  It should be noted that the computation of 
Viterbi Alignment and Expectation Evaluation is 
easy for Models 1-2. What makes these computa- 
tions hard for Models 3-5?  To answer this ques- 
tion, we observe that Models 1-2 lack explicit fer- 
tility model unlike Models 3-5. In the former mod- 
els, fertility probabilities are determined by the


subspace.    To  be  useful  such  large  subspaces
should also admit optimal polynomial time al- 
gorithms for the problems we have discussed in 
this paper.   This is exactly the approach taken 
by (Udupa, 2005) for solving the decoding and 
Viterbi alignment problems. They show that very 
efficient polynomial time algorithms can be de- 
veloped for both Decoding and Viterbi Alignment 
problems.    Not  only  the  algorithms  are  prov- 
ably superior in a computational complexity sense, 
(Udupa, 2005) are also able to get substantial im- 
provements in BLEU and NIST scores over the 
Greedy decoder.

5   Conclusions

IBM models 3-5 are widely used in SMT. The 
computational tasks discussed in this work form 
the backbone of all SMT systems that use IBM 
models. We believe that our results on the compu- 
tational complexity of the tasks in SMT will result 
in a better understanding of these tasks from a the- 
oretical perspective.  We also believe that our re- 
sults may help in the design of effective heuristics


for some of these tasks. A theoretical analysis of 
the commonly employed heuristics will also be of 
interest.
An open question in SMT is whether there ex-

ists closed form expressions (whose representation 
is polynomial in the size of the input) for P (f |e)
and the counts in the EM iterations for models 3-5 
(Brown et al., 1993). For models 1-2, closed form
expressions exist for P (f |e) and the counts in the
EM iterations for models 3-5.  Our results show
that there cannot exist a closed form expression

(whose representation is polynomial in the size of 
the input) for P (f |e) and the counts in the EM
iterations for Models 3-5 unless P = NP.


References

K. Knight.   1999.   Decoding Complexity in Word- 
Replacement Translation Models.   Computational 
Linguistics.

Brown, P. et al:  1993.  The Mathematics of Machine 
Translation: Parameter Estimation.  Computational 
Linguistics, 2(19):263–311.

Al-Onaizan, Y. et al. 1999. Statistical Machine Trans- 
lation: Final Report. JHU Workshop Final Report.

R. Udupa, and T. Faruquie.  2004.  An English-Hindi 
Statistical Machine Translation System.   Proceed- 
ings of the 1st IJCNLP.

Y. Wang, and A. Waibel. 1998. Modeling with Struc- 
tures in Statistical Machine Translation.  Proceed- 
ings of the 36th ACL.

D. Marcu and W.  Wong. 2002. A Phrase-Based, Joint 
Probability Model for Statistical Machine Transla- 
tion. Proceedings of the EMNLP.

L. Valiant.  1979.  The complexity of computing the 
permanent.  Theoretical Computer Science, 8:189–
201.

M. Jerrum. 2005. Personal communication.

C. Tillman.   2001.   Word Re-ordering and Dynamic 
Programming based Search Algorithm for Statistical 
Machine Translation.   Ph.D. Thesis, University of 
Technology Aachen 42–45.

Y. Wang and A. Waibel. 2001. Decoding algorithm in 
statistical machine translation.  Proceedings of the
35th ACL 366–372.

C. Tillman and H. Ney.  2000.  Word reordering and 
DP-based search in statistical machine translation. 
Proceedings of the 18th COLING 850–856.

F. Och, N. Ueffing, and H. Ney. 2000. An efficient A* 
search algorithm for statistical machine translation. 
Proceedings of the ACL 2001 Workshop on Data- 
Driven Methods in Machine Translation 55–62.


U. Germann et al.  2003.  Fast Decoding and Optimal 
Decoding for Machine Translation. Artificial Intel- 
ligence.

R. Udupa, H.  Maji, and T. Faruquie.  2004.  An Al- 
gorithmic Framework for the Decoding Problem in 
Statistical Machine Translation. Proceedings of the
20th COLING.

R. Udupa and H.  Maji.  2005.  Theory of Alignment 
Generators and Applications to Statistical Machine 
Translation. Proceedings of the 19th IJCAI.

