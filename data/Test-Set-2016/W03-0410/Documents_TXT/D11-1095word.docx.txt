Hierarchical Verb Clustering Using Graph Factorization




Lin Sun and Anna Korhonen
University of Cambridge, Computer Laboratory
15 JJ Thomson Avenue, Cambridge CB3 0GD, UK
ls418,alk23@cl.cam.ac.uk









Abstract


Most previous research on verb clustering has 
focussed on acquiring flat classifications from 
corpus  data, although  many manually built 
classifications are taxonomic  in nature.  Also 
Natural Language Processing (NLP)  applica- 
tions benefit from taxonomic  classifications 
because they vary in terms of the granularity 
they require from a classification.   We intro- 
duce a new clustering method called Hierar- 
chical Graph Factorization Clustering (HGFC) 
and extend it so that it is optimal for the task. 
Our results show that HGFC outperforms the 
frequently  used agglomerative  clustering on a 
hierarchical  test set extracted  from VerbNet, 
and that it yields state-of-the-art performance 
also on a flat test set. We demonstrate how the 
method can be used to acquire novel classifi- 
cations  as well as to extend existing  ones on 
the basis of some prior knowledge about the 
classification.



1   Introduction

A variety of verb classifications  have been built to 
support NLP tasks. These include  syntactic and se- 
mantic classifications,   as well as  ones  which in- 
tegrate  aspects  of  both (Grishman  et al., 1994; 
Miller, 1995; Baker et al., 1998; Palmer et al., 2005; 
Kipper, 2005; Hovy et al., 2006). Classifications 
which integrate  a wide range of linguistic proper- 
ties can be particularly useful for tasks suffering 
from data  sparseness.   One such classification is 
the taxonomy of English  verbs proposed by Levin 
(1993) which is based on shared (morpho-)syntactic
1023


and semantic properties of verbs. Levin’s  taxon- 
omy or its extended version  in VerbNet (Kipper,
2005) has proved helpful for various NLP applica- 
tion tasks, including  e.g. parsing, word sense disam- 
biguation,  semantic role labeling, information ex- 
traction,  question-answering, and machine transla- 
tion (Swier  and Stevenson, 2004; Dang, 2004; Shi 
and Mihalcea, 2005; Zapirain et al., 2008).
  Because  verbs change  their meaning  and be- 
haviour  across domains, it is important to be able 
to tune existing  classifications  as well to build novel 
ones in a cost-effective  manner, when required.  In 
recent years, a variety of approaches have been pro- 
posed for automatic induction of Levin style classes 
from corpus data which  could be used for this pur- 
pose (Schulte  im Walde, 2006; Joanis et al., 2008; 
Sun et al.,  2008; Li  and Brew, 2008; Korhonen et 
al., 2008; O´ Se´aghdha and Copestake, 2008; Vla- 
chos et al., 2009).  The best of such  approaches 
have yielded promising results. However, they have 
mostly focussed  on acquiring  and evaluating  flat 
classifications. Levin’s classification is not flat, but 
taxonomic in nature, which is practical for NLP pur- 
poses since applications  differ in terms of the gran- 
ularity they require from a classification.
  In this paper,  we experiment  with hierarchical 
Levin-style clustering. We  adopt  as our baseline 
method  a  well-known hierarchical  method – ag- 
glomerative clustering (AGG) – which has been pre- 
viously used to acquire flat Levin-style  classifica- 
tions (Stevenson and Joanis, 2003) as well as hierar- 
chical verb classifications not based on Levin (Fer- 
rer, 2004; Schulte im Walde, 2008). The method has 
also been popular in the related task of noun clus-




Proceedings of the 2011 Conference on Empirical  Methods in Natural  Language Processing, pages 1023–1033, 
Edinburgh, Scotland, UK, July 27–31, 2011. Qc 2011 Association for Computational Linguistics


tering (Ushioda, 1996; Matsuo et al., 2006; Bassiou 
and Kotropoulos, 2011).

  We introduce  then a new method called Hierar- 
chical Graph Factorization Clustering (HGFC) (Yu 
et al., 2006). This graph-based, probabilistic cluster- 
ing algorithm  has some clear advantages over AGG 
(e.g. it delays the decision on a verb’s cluster mem- 
bership at any level until a full graph is available, 
minimising the problem of error propagation) and it 
has been shown to perform better than several other 
hierarchical  clustering  methods in recent compar- 
isons (Yu et al., 2006). The method has been applied 
to the identification  of social network communities 
(Lin et al., 2008), but has not been used (to the best 
of our knowledge) in NLP before.

  We modify HGFC with a new tree extraction  al- 
gorithm  which  ensures a more consistent result, and 
we propose two novel extensions to it. The first is a 
method for automatically determining the tree struc- 
ture (i.e. number of clusters to be produced for each 
level of the hierarchy).  This avoids the need to pre- 
determine the number of clusters manually.  The sec- 
ond is addition of soft constraints to guide the clus- 
tering performance (Vlachos et al., 2009). This is 
useful for situations where a partial (e.g. a flat) verb 
classification is available and the goal is to extend it.

  Adopting  a set of lexical and syntactic  features 
which have performed  well in previous works, we 
compare the performance of the two methods on test 
sets extracted from Levin and VerbNet.  When eval- 
uated on a flat clustering task, HGFC outperforms 
AGG and performs very similarly with the best flat 
clustering  method reported on the same test set (Sun 
and Korhonen, 2009). When evaluated on a hierar- 
chical task, HGFC performs considerably better than 
AGG at all levels of gold standard classification.  The 
constrained version of HGFC performs the best,  as 
expected, demonstrating the usefulness of soft con- 
straints for extending partial classifications.

  Our qualitative  analysis shows that HGFC is ca- 
pable of detecting novel information not included in 
our gold standards. The unconstrained version can 
be used to acquire novel classifications from scratch 
while the constrained version can be used to extend 
existing  ones with additional  class members, classes 
and levels of hierarchy.


2   Target classification and test sets

The taxonomy of Levin (1993) groups English verbs
(e.g. break, fracture,  rip) into classes  (e.g. 45.1
Break verbs) on the basis  of their shared mean- 
ing components and (morpho-)syntactic  behaviour, 
defined in terms of diathesis alternations (e.g. the 
causative/inchoative alternation, where an NP frame 
alternates with an intransitive frame: Tony broke the
window ↔ The window broke).   It classifies over
3000 verbs in 57 top level classes, some of which
divide  further  into subclasses. The extended version 
of the taxonomy in VerbNet (Kipper, 2005) classifies
5757 verbs. Its 5 level taxonomy includes 101 top 
level and 369 subclasses. We used three gold stan- 
dards (and corresponding  test sets) extracted from 
these resources in our experiments:
  T1: The first gold standard is a flat gold standard 
which  includes  13 classes appearing in Levin’s orig- 
inal taxonomy (Stevenson and Joanis, 2003). We in- 
cluded this small gold standard in our experiments 
so that we could compare the flat version of our 
method against previously published methods. Fol- 
lowing  Stevenson and Joanis (2003), we selected 20 
verbs from each class which occur at least 100 times 
in our corpus. This gave us 260 verbs in total.
  T2:  The second  gold standard  is a  large, hi- 
erarchical gold standard which we extracted from 
VerbNet  as follows: 1) We removed all the verbs 
that have  less than 1000 occurrences in our cor- 
pus. 2) In order to minimise the problem of pol- 
ysemy, we assigned each verb to the class which, 
according to VerbNet,  corresponds to its predomi- 
nant sense in WordNet (Miller, 1995). 3) In order 
to minimise  the sparse data problem  with very fine- 
grained classes, we converted the resulting classifi- 
cation into a 3-level  representation so that the classes 
at the 4th and 5th level were combined.  For exam- 
ple, the sub-classes of Declare  verbs (numbered as
29.4.1.1.{1,2,3}) were combined into 29.4.1. 4) The
classes that have fewer than 5 members were dis-
carded.  The total number of verb senses in the re- 
sulting gold standard is 1750, which is 33.2% of the 
verbs in VerbNet. T2 has 51 top level, 117 second 
level, and 133 third level classes.
  T3:  The third gold standard is a subset  of T2 
where singular  classes (top level classes which do 
not divide into subclasses) are removed.  This gold


standard was constructed  to enable proper evalua- 
tion of the constrained version of HGFC (introduced 
in the following section) where we want to com- 
pare the impact of constraints across several levels 
of classification.  T3 provides classification of 357 
verbs into 11 top level, 14 second level, and 32 third 
level classes.
  For each verb appearing in T1-T3, we extracted 
all the occurrences (up to 10,000) from the British 
National Corpus (Leech, 1992) and North American 
News Text Corpus (Graff, 1995).

3   Method

3.1   Features and feature extraction

Previous works on Levin style verb classification 
have investigated  optimal features  for this task 
(Stevenson and Joanis, 2003;  Li and Brew, 2008; 
Sun and Korhonen, 2009)). We adopt for our exper- 
iments  a set of features which have performed well 
in recent verb clustering works:
A: Subcategorization frames (SCFs) and their rela- 
tive frequencies with individual verbs.
B: A with SCFs parameterized for prepositions.
C: B with SCFs parameterized for subjects appear- 
ing in grammatical  relations associated with the 
verb in parsed data.
D: B with SCFs parameterized for objects appear- 
ing in grammatical  relations associated with the 
verb in parsed data.
  These  features  are purely syntactic.  Although 
semantic features – verb selectional  preferences – 
proved the best (when used in combination with syn- 
tactic features) in the recent work of Sun and Ko- 
rhonen (2009), we left such features for future work 
because we noticed that different levels of classifi- 
cation are likely to require semantic features at dif- 
ferent granularities.
  We extracted the syntactic features using the sys- 
tem of Preiss et al. (2007). The system tags, lemma- 
tizes and parses corpus data using the RASP (Robust 
Accurate Statistical Parsing toolkit (Briscoe et al.,
2006)), and on the basis of the resulting grammat- 
ical relations,  assigns each occurrence of a verb  as 
a member  of one of the 168 verbal SCFs.  We pa- 
rameterized the SCFs as described above using the 
information provided by the system.


3.2   Clustering

We introduce the agglomerative clustering  (AGG) 
and Hierarchical Graph Factorization  Clustering 
(HGFC)  methods in  the following  two  subsec- 
tions, respectively.  The subsequent two subsections 
present our extensions to HGFC: (i) automatically 
determining the cluster structure and (ii) adding soft 
constraints to guide clustering performance.

3.2.1  Agglomerative clustering

   AGG  is a method  which treats  each verb as  a 
singleton cluster and then successively merges two 
closest  clusters  until  all  the clusters  have  been 
merged  into one.   We  used the SciPy’s  imple- 
mentation (Oliphant, 2007) of the algorithm. The 
cluster distance is measured using linkage criteria. 
We experimented with four commonly  used link- 
age criteria: Single, Average, Complete and Ward’s 
(Ward Jr., 1963). Ward’s  criterion performed the 
best and was used in all the experiments in this pa- 
per. It measures the increase in variance after two 
clusters are merged. The output of AGG tends to 
have excessive number of levels. Cut-based meth- 
ods (Wu and Leahy, 1993; Shi and Malik, 2000) are 
frequently applied to extract a simplified  view. We 
followed previous verb clustering works and cut the 
AGG hierarchy manually.
   AGG suffers from two problems.  The first is er- 
ror propagation.  When a verb is misclassified at a 
lower level, the error propagates to all the upper lev- 
els. The second is local pairwise merging, i.e. the 
fact that only two clusters can be combined  at any 
level. For example, in order to group clusters rep- 
resenting Levin classes 9.1, 9.2 and 9.3 into a sin- 
gle cluster representing class 9, the method  has to
produce intermediate clusters, e.g. 9.{1,2} and 9.3.
Such clusters do not always  have a semantic inter-
pretation. Although they can be removed  using  a 
cut-based method, this requires a pre-defined cut-off 
value which is difficult to set (Stevenson and Joanis,
2003). In addition, a significant amount of informa- 
tion is lost in pair-wise clustering. In the above ex-
ample, only the clusters 9.{1,2} and 9.3 are consid-
ered, while alternative  clusters 9.{1,3} and 9.2 are
ignored. Ideally, information about all the possible
intermediate clusters should be aggregated, but this 
is intractable in practice.


3.2.2	Hierarchical Graph Factorization


We  use the divergence  distance: ζ (X, Y 
)   =
xij


Clustering


ij (xij log yij


−xij + yij ). Yu et 
al. (2006) showed


  Our new method HGFC derives a probabilistic  bi- 
partite graph from the similarity matrix (Yu et al.,
2006). The local and global clustering structures are


that this cost function is non-increasing under the
update rule:

wi
j


learned via the random walk properties of the graph.
The method does not suffer from the above prob-


h˜ip  ∝ hip  
j



(H ΛH T )ij


λp hjp  s.t.     h˜ip  = 1  (2)
i


lems with AGG. Firstly, there is no error propagation


λ˜p  ∝ λp  


wij
(H ΛH T )ij


hip hjp  s.t.     λ˜p  =     wij     (3)


because the decision on a verb’s membership  at any	j


p 	ij


level is delayed until the full bipartite graph is avail- 
able and until a tree structure can be extracted from 
it by aggregating probabilistic information from all 
the levels. Secondly, the bipartite graph enables the
  

wij  can be interpreted  as the probability of the 
di- rect transition between vi  and vj : wij   = p(vi, 
vj ),
when L;ij wij  = 1. bip can be interpreted as:


construction of a hierarchical structure without  any 
intermediate  classes.  For example, we can group
classes 9.{1,2,3} directly  into class 9.
We  use HGFC with the distributional similarity


bip biq
di

pq
m






(4)


measure  Jensen-Shannon  Divergence   (djs(v, v )).


D 	=  diag(d1 , ..., dn ) where di  =     bip


Given a   set   of  verbs, V	=	{vn}N


,  we


p=0


compute a  similarity matrix W  where Wij	=
exp(−djs(v1, v2)).  W can be encoded by a undi-
rected graph G (Figure 1(a)), where the verbs are
mapped to vertices and the Wij  is the edge weight 
between vertices i and j.
  The graph G and the cluster structure can be rep- 
resented by a bipartite  graph K (V, U ).  V are the



  p(up, uq ) is the similarity  between the clusters. 
It takes into account of a weighted  average of 
contri- butions from all the data. This is different 
from the linkage method where only the data 
from two clus- ters are considered.
Given the cluster similarity p(up, uq ), we can con-


vertices on G. U = {up}m


represent the hidden m


struct a new graph G1 (Figure 
1(d)) with the clusters
U as vertices.  The cluster 
algorithm  can be applied


clusters. For example, looking at Figure 1(b), V on



G can be grouped into three clusters u1, u2 and u3. 
The matrix B denotes the n × m adjacency matrix,
with bip  being the connection weight between the 
vertex vi  and the cluster up. Thus, B represents the 
connections between clusters at an upper and lower 
level of clustering. A flat clustering algorithm can 
be induced by computing B.
The bipartite graph K also induces  a similarity


again (Figure 1(e)). This process can go on itera-
tively, leading to a hierarchical  graph.

Algorithm 1 HGFC algorithm (Yu et al., 2006)
Require: N verbs V , number of clusters ml for L levels
Compute the similarity matrix W0 from V
Build the graph G0 from W0 , and m0 ← n
for l = 1, 2 to L do
Factorize Gl−1 to obtain bipartite graph Kl with the


(W  )  between  vi  and  vj :  w 


L;m
p=1


bip bjp
λp


adjacency matrix Bl  (eq. 1, 2 and 
3)
Build a  graph Gl   with 
similarity matrix Wl    =


(BΛ−1BT )ij  where Λ = diag(λ1, ..., λm).  There-
fore, B can be found by approximating the similarity 
matrix W of G using W  derived from K . Given  a 
distance function ζ between two similarity matrices, 
B approximates W by minimizing  the cost function 
ζ (W, BΛ−1BT ). The coupling between B and Λ is 
removed by setting H = BΛ−1:
n
min ζ (W, H ΛH T ), s.t.      hip = 1	(1)
H,Λ
i=1


BT Dl    Bl  according to equation 4
−
l
end for
return BL , BL−1 ...B1


  Additional steps need to be performed in order 
to extract  a tree from the hierarchical graph. Yu 
et al. (2006) performs the extraction via a 
propagation  of probabilities from the bottom 
level clusters. For a verb vi, the probability of 
assigning it to cluster v(l) at level l is given by:


v
1	u
1
2	v	v
v	v	1	2	1
2	1	v
v	v
v	v	3	7
3	7	4	v
v	u	v	8
v	8	5	2	9
9


v
1
v
2	u
1
v
u	u	3	q
1	2	v	1
4
u
5	2


v	2
6
6	6	v	3	6
v	v	5	v
5	7	4	7
4	u	3	u	2
v	3	v	3
8	8
v	v
9	9



(a)



(b)



(c)



(d)



(e)




Figure 1: (a) An undirected graph G representing the similarity  matrix; (b) The bipartite graph showing three clusters 
on G; (c) The induced clusters U ; (d) The new graph G1 over clusters U ; (e) The new bipartite graph over G1





p  |vi ) =





Vl−1




...
V1



p(v(l) |v




(l−1)




)...p(v




(1)



|vi )


3.2.3
	Aut
oma
tical
ly 
dete
rmin
ing 
the 
num
ber 
of 
clust
ers 
for 
HG
FC
HGFC 
needs the 
number of 
levels and 
clusters at


= (D(−1)	−1	−1
2   B2 D3    B3 ...Dl    Bl )ip 	(5)
  This method might not extract  a consistent  tree 
structure,  because the cluster membership  at the 
lower level does not constrain the upper level mem- 
bership. This prevented us from extracting  a Levin 
style hierarchical classification in our initial experi- 
ments. For example, where two verbs were grouped 
together at a lower level, they could belong to sepa- 
rate clusters at an upper level. We therefore propose 
a new tree extraction algorithm (Algorithm  2).
  The new algorithm starts from the top level bipar- 
tite graph, and generates consistent labels for each 
level by taking into account of the tree constraints 
set at upper levels.


each level as input. However, this information is not 
always available (e.g. when the goal is to actually 
learn this information automatically).  We therefore 
propose a method  for inferring  the cluster structure 
from data. As shown in figure 1, a similarity ma- 
trix W models one-hop transitions that follow the 
links from vertices to neighbors. A walker can also 
go to other vertices via multi-hop transitions.  Ac- 
cording to the chain rule of the Markov  process, the 
multi-hop  transitions indicate a decaying similarity 
function on the graph (Yu et al., 2006). After t tran- 
sitions, the similarity matrix (Wt) becomes:
Wt  = Wt−1 D0    W0
Yu et al. (2006) proved the correspondence be-


 		tween the HGFC levels (l) and the random walk time:


A   lgorithm 2 Tree extraction algorithm for HGFC
Require: Given N , (Bl , ml ) on each level for L levels
On the top level L, collect the labels T L (eq. 5)
Define C to be a (mL−1 × mL ) zero matrix, Cij  ← 1, 
where i, j = arg maxi,j {BL }


t = 2l−1.  So the vertices at level l induce  a sim- 
ilarity matrix of verbs after t-hop transitions.  The 
decaying similarity function captures the different 
scales of clustering structure in the data (Azran and 
Ghahramani, 2006b).  The upper levels would have


for


l = L 
− 1 to 
1 do
i = 
1 
to 
N 
do


a 
smalle
r 
numb
er of 
cluster
s 
which 
repres
ent a 
more


for
Compute p(vl |vi ) for each cluster p (eq. 5)



global structure.  After several levels, all the 
verbs


l	l	are expected  to be grouped into one cluster. The


ti = argmaxp {p(vp |vi )|p = 1...ml , Cptl+1    = 0}
end for
Redefine C to be a (ml−1 × ml ) zero matrix, Cij  ←
1, where i, j = arg maxi,j {Bl  }



number of levels and clusters at each level can thus 
be learned automatically.
We therefore propose a method that uses the de-


end for
return Tree consistent labels T L , T L−1 ...T 1


caying similarity function to learn the 
hierarchical
clustering structure. One simple 
modification to al- gorithm 1 is to set the 
number of clusters at level l


(ml ) to be ml−1  − 1. m is denoted as the number
of clusters that have at least one member according
to eq. 5. We start by treating each verb as a cluster 
at the bottom level. The algorithm  stops when all 
the data points are merged into one cluster. The in- 
creasingly decaying similarity causes many clusters 
to have 0 members especially at lower levels, which 
are pruned in the tree extraction.

3.2.4  Adding constraints to HGFC
The basic version  of HGFC makes no prior as-
 

where nij  is the size of the intersection  between 
class i and cluster j.
  We  used normalized  mutual information (NMI) 
and F-Score (F) to evaluate hierarchical clustering 
results on T2 and T3. NMI  measures the amount of 
statistical information  shared by two random vari- 
ables representing the clustering result and the gold- 
standard labels. Given random variables A and B:

NMI(A, B) =		I (A; B) [H 
(A) + H (B)]/2


sumptions  about the classification.	It  is useful


I(A, B) =


|(vk ∩ cj | log N |vk ∩ cj |


for learning novel verb classifications from scratch. 
However, when wishing to extend an existing  clas- 
sification (e.g. VerbNet) it may be desirable to guide 
the clustering performance on the basis of informa- 
tion that is already known.  We propose a constrained 
version of HGFC which  makes uses of labels at the 
bottom level to learn upper level classifications. We 
do this by adding soft constraints to clustering, fol- 
lowing Vlachos et al. (2009).
  We modify the similarity matrix W as follows: If 
two verbs have different  labels (li  = lj ), the simi- 
larity between them is decreased by a factor a, and 
a < 1. We set a to 0.5 in the experiments. The re-


k 	j	N 	|vk ||cj |
 where |vk  ∩ cj | is the number of shared member- 
ship between cluster vk and gold-standard  class cj . 
The normalized variant of mutual information (MI) 
enables the comparison of clustering with different 
cluster numbers (Manning et al., 2008).
  F is the harmonic  mean of precision (P) and re- 
call (R). P is calculated using modified purity – a 
global measure which evaluates the mean precision 
of clusters. Each cluster is associated with its preva- 
lent class. The number of verbs in a cluster K that 
take this class is denoted by nprevalent(K).


sulting tree is generally consistent with the original


L;
nprevalent(ki )>2


nprevalent(ki )


classification.  The influence of the underlying data


mPUR =



number of verbs


(domain or features) is reduced according to a.

4   Experimental evaluation

We applied the clustering  methods introduced  in 
section 3 to the test sets described in section 2 and 
evaluated them both quantitatively  and qualitatively, 
as described  in the subsequent sections.

4.1   Evaluation methods

We used class based accuracy  (ACC) and adjusted 
rand index (Radj ) to evaluate the results on the flat 
test set T1 (see section 2 for details of T1-T3).
   ACC is the proportion of members of dominant 
clusters DOM-CLUSTi within all classes ci.

ACC =     i=1 verbs in DOM-CLUSTi
number of verbs
The formula of Radj is (Hubert and Arabie, 1985):



R is calculated using ACC.
F = 2 · mPUR  · ACC
mPUR + ACC
  F is not suitable for comparing results with dif- 
ferent cluster numbers (Rosenberg and Hirschberg,
2007).  Therefore, we only report NMI  when the 
number of classes in clustering and gold-standard is 
substantially different.
  Finally, we supplemented quantitative evaluation 
with qualitative evaluation of clusters produced by 
different methods.

4.2   Quantitative evaluation

We  first evaluated  AGG   and the basic (uncon- 
strained) HGFC on the small flat test set T1.  The 
main purpose of this evaluation was to compare the 
results of our methods against previously published 
results on the same test set.   The number of clus-


L;	(nij   − L; (ni·   L;


(n·j  /(n 


ters (K ) and levels (L) were inferred 
automatically


Radj  =


i,j	2


i  2 	j	2 	2


2 [L;i ( 2    + L;j ( 2   ] − L;i ( 2    L;j ( 2   /(2 


for HGFC as described  in section 3.2.3. However, to


1 	ni·


n·j


ni·


n·j	n



make the results comparable with previously pub- 
lished ones,  we cut the resulting hierarchy at the 
level of closest match (12 clusters) to the K (13) in 
the gold-standard. For AGG, we cut the hierarchy at
13 clusters.



Nc
Nl
HGFC
uncons
trained
A
G
G


N
MI
F
N
M
I
F
13
0
13
3
57
.3
1
36
.6
5
54
.2
2
32
.6
2
11
4
11
7
54
.6
7
37
.9
6
51
.3
5
32
.4
4
50
51
37
.7
5
40
.0
0
32
.6
1
32
.7
8

Table 2:  Performance on T2 using a pre-defined   tree 
structure.





Table 1:	Comparison  against Stevenson  and Joanis
(2003)’s result on T1 (using similar features).



  Table 1 shows our results and the results of 
Stevenson and Joanis (2003) on T1 when employing 
AGG using Ward as the linkage criterion.  In this ex- 
periment,  we used the same feature set as Stevenson 
and Joanis (2003) (set B, see section 3.1) and were 
therefore able to reproduce their AGG result with a 
difference smaller than 2%. When using this simple 
feature set, HGFC outperforms the best performing 
AGG clearly: 8.5% in ACC and 7.3% in Radj .
We also compared HGFC against the best reported
clustering method on T1 to date – that of spectral 
clustering by Sun and Korhonen  (2009). We used 
the feature  sets C and D which are similar to the 
features (SCF parameterized by lexical prefences) in 
their experiments. HGFC obtains F of 49.93% on T1 
which is 5% lower than the result of Sun and Ko- 
rhonen (2009). The difference comes from the tree 
consistency requirement.  When the HGFC is forced 
to produce a flat clustering (a one level tree only), it 
achieves the F of 52.55% which is very close to the 
performance of spectral clustering.
  We then evaluated our methods on the hierarchi- 
cal test sets T2 and T3.  In the first set of experi- 
ments, we pre-defined the tree structure for HGFC 
by setting L to 3 and K at each level to be the K 
in the hierarchical gold standard. The hierarchy pro- 
duced by AGG was cut into 3 levels according to K s 
in the gold standard. This enabled direct evaluation 
of the results against the 3 level gold standards using 
both NMI and F.
  The results are reported in tables 2 and 3. In these 
tables, Nc is the number of clusters in HGFC cluster- 
ing while Nl is the number of classes in the gold 
standard (the two do not always  correspond per- 
fectly  because a few clusters have zero members).




Table 3:  Performance on T3 using a pre-defined   tree 
structure.


  Table 2 compares the results of the unconstrained 
version of HGFC against those of AGG on our largest 
test set T2.  As with T1, HGFC outperforms AGG 
clearly. The benefit can now be seen at 3 
different levels of hierarchy. On average, the 
HGFC outper- forms AGG 3.5% in NMI  and 4.8% 
in F. The dif- ference between the methods 
becomes clearer when moving towards the upper 
levels of the hierarchy.
  Table 3 shows the results of both unconstrained 
and constrained  versions  of  HGFC  and those of 
AGG on the test set T3 (where singular  classes are 
removed  to enable proper  evaluation  of the 
con- strained method). The results are generally 
gener- ally better on this test set than on T2 – which 
is to be expected since T3 is a refined subset of 
T21.
  Recall that the constrained version of HGFC learns 
the upper levels of classification  on the basis of soft 
constraints set at the bottom level, as described ear- 
lier in section 3.2.4. As a consequence,  NMI  and F 
are both greater than 90% at the bottom level 
and the results at the top level are notably lower 
because the impact of the constraints  degrades 
the further away one moves from the bottom level. 
Yet, the rela- tively high result across all levels 
shows that the con- strained version of HGFC can 
be employed a useful method to extend the 
hierarchical structure of known classifications.

  1 NMI  is higher on T2, however,  because NMI  has a 
higher baseline for larger number of clusters (Vinh et al., 
2009). NMI is not ideal for comparing the results of T2 and 
T3.



T
2
T
3
Nc
Nl
H
GF
C
Nc
Nl
H
GF
C
14
8
13
3
53
.2
6
64
32
54
.9
1
97
11
7
49
.8
5
35
32
50
.8
3
46
51
33
.5
5
20
14
44
.0
2
19
51
25
.8
0
10
14
34
.4
1
9
51
19
.1
7
6
11
32
.2
7
3
51
13
.0
6


Table 4: NMI of unconstrained HGFC when trees for T2 
and T3 are inferred automatically.


  Finally, Table 4 shows the results for the uncon- 
strained HGFC on T2 and and T3 when the tree struc- 
ture is not pre-defined but inferred automatically  as 
described in section 3.2.3. 6 levels are learned for 
T2 and 5 for T3. The number of clusters produced 
ranges from 3 to 148 for T2 and from 6 to 64 for 
T3. We can see that the automatically  detected clus- 
ter numbers distribute  evenly across different  levels. 
The scale of the clustering structure is more com- 
plete here than in the gold standards.
  In the table, Nc  indicates the number of clusters 
in the inferred tree, while Nl indicates the closest 
match to the number of classes in the gold stan- 
dard. This evaluation is not fully reliable  because 
the match between the gold standard and the cluster- 
ing is poor at some levels of hierarchy. However, it 
is encouraging to see that the results do not drop dra- 
matically until the match between the two is really 
poor.

4.3   Qualitative evaluation
To gain a  better insight into the performance of 
HGFC, we conducted further qualitative analysis of 
the clusters the two versions of this method pro- 
duced for T3. We focussed on the top level of 11 
clusters (in the evaluation  against the hierarchical 
gold standard, see table 3) as the impact of soft con- 
straints is the weakest for the constrained method at 
this level.
  As expected, the constrained HGFC kept many in- 
dividual verbs belonging to same Verbnet  subclass 
together (e.g. verbs enjoy, hate, disdain, regret, love, 
despise, detest, dislike, fear for the class 31.2.1) so 
that most clusters simply group lower level classes 
and their members together. Three nearly clean clus- 
ters were produced which only include sub-classes 
of the same class (e.g. 31.2.0 and 31.2.1 which both



belong to 31.2 Admire verbs). However, the remain- 
ing 8 clusters group together sub-classes (and their 
members) belonging to unrelated parent classes. In- 
terestingly, 6 of these make both syntactic  and se- 
mantic  sense.  For example,  several such 37.7 Say 
verbs and 29.5 Conjencture verbs are found together 
which share  the meaning  of communication  and 
which take similar sentential complements.
  In contrast,  none of the clusters  produced  by 
the unconstrained HGFC represent a single VerbNet 
class.  The majority represent  a  high number of 
classes and fewer members per class.  Yet many of 
the clusters make syntactic  and semantic sense. A 
good example is a cluster  which includes member 
verbs from 9.7 Spray/Load verbs, 21.2 Carve verbs,
51.3.1 Roll verbs, and 10.4 Wipe verbs. The verbs 
included in this cluster share the meaning of specific 
type of motion and show similar syntactic behaviour.
  Thorough Levin style investigation of especially 
the unconstrained method would require looking at 
shared diathesis alternations between cluster mem- 
bers.   We  left this for future work.   However, 
the analysis we conducted confirmed that the con- 
strained method could indeed be used for extend- 
ing known classifications, while the unconstrained 
method is more suitable for acquiring novel classi- 
fications from scratch. The errors in clusters pro- 
duced by both methods were mostly due to syntactic 
idiosyncracy and the lack of semantic information in 
clustering. We plan to address the latter problem in 
our future work.

5   Discussion and conclusion

We  have introduced  a new graph-based method – 
HGFC – to hierarchical verb clustering which avoids 
some of the problems (e.g. error propagation, pair- 
wise cluster merging) reported with the frequently 
used AGG method. We modified HGFC so that it can 
be used to automatically  determine the tree struc- 
ture for clustering,  and proposed two extensions to 
it which make it even more suitable for our task. The 
first involves automatically determining the number 
of clusters to be produced, which is useful when 
this is not known in advance.  The second involves 
adding soft constraints to guide the clustering per- 
formance, which is useful when aiming to extend 
existing classification.


  The results reported in the previous  section are 
promising.  On a flat test set (T1), the unconstrained 
version of HGFC outperforms  AGG  and performs 
very similarly with the best current  flat clustering 
method (spectral clustering) evaluated on the same 
dataset.  On the hierarchical  test sets (T2 and T3), 
the unconstrained and constrained versions of HGFC 
outperform AGG clearly at all levels of classification. 
The constrained version of HGFC detects the missing 
hierarchy from the existing gold standards with high 
accuracy.  When the number of clusters and levels 
is learned automatically,  the unconstrained method 
produces  a multi-level hierarchy. Our evaluation 
against a 3-level gold standard shows that such a hi- 
erarchy is fairly accurate.  Finally, the results from 
our qualitative evaluation show that both constrained 
and unconstrained versions of HGFC are capable of 
learning valuable novel information not included in 
the gold standards.
  The previous work on Levin style verb classifica- 
tion has mostly  focussed on flat classifications us- 
ing methods suitable for flat clustering (Schulte im 
Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li
and Brew, 2008; Korhonen et al., 2008; O´ Se´aghdha
and Copestake, 2008; Vlachos et al., 2009). How- 
ever, some works have employed hierarchical clus- 
tering as a method to infer flat clustering.
  For example, Schulte im Walde and Brew (2002) 
employed AGG to initialize the KMeans clustering 
for German verbs.  This gave better  results than 
random initialization. Stevenson and Joanis (2003) 
used AGG for flat clustering on T1. They cut the hi- 
erarchy at the number of classes in the gold standard 
and found that it is difficult to automatically deter- 
mine a good cut-off. Our evaluation in the previous 
section shows that HGFC outperforms their imple- 
mentation of AGG.
   AGG  was also used by Ferrer (2004) who per- 
formed hierarchical clustering of 514 Spanish verbs. 
The results  were evaluated against a hierarchical 
gold standard resembling  that of Levin’s  classifi- 
cation in English (Va´zquez et al., 2000). Radj  of
0.07 was reported for a 15-way classification  which
is comparable to the result of Stevenson and Joanis
(2003).
  Hierarchical clustering has also been performed 
for the related task of semantic verb classification. 
For example, Basili et al. (1993) identified the prob-


lems of AGG, and applied a conceptual clustering al- 
gorithm (Fisher, 1987) to Italian verbs. They used 
semi-automatically  acquired semantic roles and the 
concept  types  as features. No quantitative  results 
were reported. The qualitative evaluation shows that 
the resulting clusters are very fine-grained.
  Schulte im Walde (2008) performed hierarchical 
clustering of German verbs using human verb asso- 
ciation  as features and AGG as a method.  They  fo- 
cussed on two small collections of 56 and 104 verbs 
and evaluated the result against flat gold standard 
extracted  from GermaNet (Kunze and Lemnitzer,
2002) and German FrameNet (Erk et al., 2003), re- 
spectively. They reported F of 62.69% for the 56 
verbs, and F of 34.68% for the 104 verbs.
  In the future, we plan to extend this research line 
in several directions. First, we will  try to deter- 
mine optimal features for different levels of clus- 
tering. For example, the general syntactic features 
(e.g. SCF) may perform the best at top levels of a hi- 
erarchy while more specific or refined features (e.g. 
SCF+pp) may be optimal at lower levels. We also 
plan to investigate incorporating  semantic features, 
like verb selectional preferences, in our feature set. 
It is likely that different levels of clustering require 
more or less specific selectional preferences.  One 
way to obtain the latter is hierarchical clustering of 
relevant noun data.
  In addition, we plan to apply the unconstrained 
HGFC to specific domains to investigate its capabil- 
ity to learn novel, previously unknown classifica- 
tions. As for the constrained version of HGFC, we 
will conduct a larger scale experiment on the Verb- 
Net data to investigate what kind of upper level hi- 
erarchy it can propose for this resource (which cur- 
rently has over 100 top level classes).
  Finally, we plan to compare HGFC to other hier- 
archical clustering methods that are relatively  new 
to NLP but have proved promising  in other fields, 
including Bayesian Hierarchical Clustering (Heller 
and Ghahramani, 2005; Teh et al., 2008) and the 
method of Azran and Ghahramani  (2006a) based on 
spectral clustering.

6   Acknowledgement

Our work was funded by the Royal Society Uni- 
versity Research  Fellowship (AK),  the Dorothy 
Hodgkin Postgraduate  Award (LS), the EPSRC


grants EP/F030061/1 and EP/G051070/1 (UK) and 
the EU FP7 project ’PANACEA’.

References

Arik Azran and Zoubin Ghahramani.   A new approach 
to data driven clustering. In Proceedings of the 23rd 
international  conference on Machine learning, ICML
’06, pages 57–64, New York, NY, USA, 2006a. ISBN
1-59593-383-2.
Arik Azran and Zoubin Ghahramani.  Spectral methods 
for automatic multiscale data clustering.  In Proceed- 
ings of the 2006 IEEE Computer Society Conference 
on Computer Vision and Pattern Recognition-Volume
1, pages 190–197. IEEE Computer Society Washing- 
ton, DC, USA, 2006b.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
The berkeley framenet project. In In COLING-ACL, 
pages 86–90, 1998.
Roberto. Basili, Maria Teresa Pazienza,  and Paola Ve- 
lardi. Hierarchical clustering of verbs. In Proceedings 
of the Workshop on Acquisition of Lexical Knowledge 
from Text, 1993.
Nikoletta Bassiou and Constantine  Kotropoulos. Long 
distance bigram models applied to word clustering. 
Pattern Recogn., 44:145–158,  January 2011.  ISSN
0031-3203.
Ted Briscoe,  John Carroll, and Rebecca Watson. The 
second  release of the rasp system. In Proceedings 
of the COLING/ACL on Interactive  presentation ses- 
sions, 2006.
Hoa Trang Dang. Investigations into the Role of Lexical 
Semantics in Word Sense Disambiguation. PhD thesis, 
CIS, University of Pennsylvania, 2004.
Katrin Erk, Andrea Kowalski, Sebastian Pado´ , and Man- 
fred Pinkal. Towards a resource for lexical semantics: 
a large german corpus with extensive semantic anno- 
tation. In Proceedings of the 41st Annual Meeting on 
Association for Computational Linguistics - Volume
1, ACL ’03, pages 537–544, Stroudsburg, PA, USA,
2003. Association for Computational Linguistics.
Eva  Esteve Ferrer.  Towards a semantic classification 
of spanish verbs based on subcategorisation informa- 
tion.  In Proceedings of the ACL 2004 workshop on 
Student research,  ACLstudent ’04, Stroudsburg, PA, 
USA, 2004. Association for Computational Linguis- 
tics.
Douglas H. Fisher. Knowledge acquisition via incremen- 
tal conceptual clustering. Machine Learning, 2:139–
172, 1987. ISSN 0885-6125.
David Graff. North american news text corpus. Linguistic
Data Consortium, 1995.


Ralph Grishman, Catherine Macleod, and Adam Meyers.
Comlex syntax: Building a computational  lexicon. In
COLING, pages 268–272, 1994.
Katherine A. Heller and Zoubin Ghahramani.  Bayesian 
hierarchical clustering. In Proceedings of the 22nd 
international  conference on Machine learning,  pages
297–304. ACM, 2005. ISBN 1595931805.
Eduard Hovy, Mitchell Marcus,  Martha Palmer, Lance
Ramshaw,  and Ralph Weischedel. Ontonotes: the
90% solution.  In Proceedings of the Human Lan- 
guage Technology  Conference  of the NAACL, Com- 
panion Volume: Short Papers,  NAACL-Short ’06, 
pages 57–60, Stroudsburg, PA, USA, 2006. Associa- 
tion for Computational Linguistics.
Lawrence Hubert and Phipps Arabie.  Comparing par- 
titions.  Journal of Classification,  2:193–218, 1985. 
ISSN 0176-4268.
Eric Joanis,  Suzanne Stevenson,  and David James.   A 
general feature space for automatic verb classification. 
Natural Language Engineering, 14(3):337–367, 2008.
Karin Kipper. VerbNet: A broad-coverage, comprehen- 
sive verb lexicon. 2005.
Anna Korhonen, Yuval Krymolowski,  and Nigel Collier.
The Choice of Features for Classification of Verbs in
Biomedical Texts. In Proceedings of COLING, 2008.
Claudia Kunze and Lothar Lemnitzer.    GermaNet- 
representation, visualization,  application.  In Proceed- 
ings of LREC, 2002.
Geoffrey Leech.  100 million words of english: the 
british national corpus. Language Research, 28(1):1–
13, 1992.
Beth. Levin.  English verb classes and alternations:   A
preliminary investigation. Chicago, IL, 1993.
Jianguo Li and Chris Brew. Which Are the Best Features 
for Automatic Verb Classification.  In Proceedings of 
ACL, 2008.
Yu-Ru Lin, Yun Chi, Shenghuo Zhu, Hari Sundaram, and 
Belle L. Tseng. Facetnet:  a framework  for analyz- 
ing communities and their evolutions in dynamic net- 
works. In Proceeding of the 17th international confer- 
ence on World  Wide Web, pages 685–694, New York, 
NY, USA, 2008. ACM.
Christopher D. Manning, Prabhakar Raghavan, and Hin- 
rich Schtze.  Introduction to Information Retrieval. 
Cambridge University Press,  New York, NY, USA,
2008. ISBN 0521865719, 9780521865715.
Yutaka Matsuo, Takeshi Sakaki, Koˆ ki Uchiyama,  and 
Mitsuru  Ishizuka.  Graph-based word clustering using 
a web search engine. In Proceedings of the EMNLP, 
pages 542–550, 2006.


George A. Miller.  WordNet: a lexical database for En- 
glish.  Communications of the ACM, 38(11):39–41,
1995.

Travis E. Oliphant.	Python for scientific computing.
Computing in  Science  and Engineering, 9:10–20,
2007. ISSN 1521-9615.


Robert Swier and Suzanne  Stevenson. Unsupervised 
semantic role labelling. In Proceedings of EMNLP, 
pages 95–102, 2004.
Yee Whye Teh, Hal Daume´ III, and Daniel Roy. Bayesian 
agglomerative  clustering  with coalescents. In Ad- 
vances in Neural Information Processing  Systems, vol-


Diarmuid O´



Se´aghdha 
and Ann 
Copestake.   
Semantic


ume 20, 
2008.


classification with distributional  kernels. In Proceed- 
ings of COLING, 2008.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.  The 
proposition bank: An annotated corpus of semantic 
roles. Computational Linguistics, 31(1):71–106, 2005.
Judita Preiss, Ted Briscoe, and Anna Korhonen. A sys- 
tem for large-scale acquisition of verbal, nominal and 
adjectival subcategorization frames from corpora. In 
Proceedings of ACL, pages 912–919, 2007.
Andrew  Rosenberg and Julia Hirschberg.  V-measure: A 
conditional  entropy-based external cluster evaluation 
measure. In Proceedings of the 2007 Joint Conference 
on Empirical Methods in Natural Language Process- 
ing and Computational Natural Language Learning,
2007.

Sabine Schulte im Walde. Experiments on the automatic 
induction of german semantic verb classes. Computa- 
tional Linguistics, 32(2), 2006.
Sabine Schulte im Walde. Human associations and the 
choice of features  for semantic  verb classification. 
Research on Language and Computation, 6:79–111,
2008. ISSN 1570-7075.

Sabine Schulte im Walde and Chris Brew. Inducing ger- 
man semantic verb classes from purely syntactic sub- 
categorisation information. In Proceedings of ACL, 
pages 223–230, 2002.
Jianbo Shi and Jitendra Malik. Normalized  cuts and im- 
age segmentation.  IEEE Transactions on pattern anal- 
ysis and machine intelligence, 22(8):888–905, 2000.
Lei Shi and Rada Mihalcea.  Putting pieces together: 
Combining  FrameNet, VerbNet and WordNet for ro- 
bust semantic parsing. In Proceedings of CICLING,
2005.

Suzanne Stevenson  and Eric Joanis. Semi-supervised 
verb class discovery using noisy features. In Proceed- 
ings of HLT-NAACL 2003, pages 71–78, 2003.
Lin Sun and Anna Korhonen. Improving verb clustering 
with automatically acquired selectional preferences. In 
Proceedings of the EMNLP 2009, 2009.
Lin Sun, Anna Korhonen, and Yuval Krymolowski. Verb 
class discovery from rich syntactic data. Lecture Notes 
in Computer Science, 4919:16, 2008.


Akira Ushioda.  Hierarchical clustering of words.  In
Proceedings of the 16th conference on Computational 
linguistics-Volume  2, pages 1159–1162. Association 
for Computational Linguistics, 1996.
Gloria  Va´zquez,    Ana  Ferna´ndez-Montraveta,    and 
M. Anto` nia Mart´ı. Clasificacio´ n verbal:(alternancias 
de dia´ tesis).   Universitat de Lleida, 2000.   ISBN
8484090671.
Nguyen Xuan Vinh, Julien Epps, and James Bailey. Infor- 
mation theoretic measures for clusterings comparison: 
is a correction  for chance necessary?   In ICML ’09: 
Proceedings of the 26th Annual International Confer- 
ence on Machine  Learning,  pages 1073–1080,  New 
York, NY, USA, 2009. ACM. ISBN 978-1-60558-516-
1.
Andreas Vlachos, Anna Korhonen, and Zoubin Ghahra- 
mani. Unsupervised and constrained dirichlet  process 
mixture models for verb clustering. In Proceedings of 
the Workshop on Geometrical Models of Natural Lan- 
guage Semantics, pages 74–82, 2009.
Joe H. Ward Jr. Hierarchical grouping to optimize an ob- 
jective function. Journal of the American statistical as- 
sociation, 58(301):236–244, 1963. ISSN 0162-1459.
Zhenyu Wu and Richard Leahy. An optimal graph the- 
oretic approach to data clustering:  Theory and its ap- 
plication to image segmentation.   IEEE transactions 
on pattern analysis and machine intelligence,  pages
1101–1113, 1993. ISSN 0162-8828.
Kai Yu, Shipeng Yu, and Volker Tresp. Soft clustering on 
graphs. Advances in Neural Information Processing 
Systems, 18:1553,  2006.
Ben˜ at Zapirain, Eneko Agirre, and Llu´ıs Ma`rquez.  Ro- 
bustness and generalization  of role sets: PropBank vs. 
VerbNet. In Proceedings of ACL-08: HLT, pages 550–
558, 2008.

