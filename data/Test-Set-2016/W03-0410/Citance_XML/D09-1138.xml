<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">The work presented in this paper explores a supervised method for learning a probabilistic model of a lexicon of VerbNet classes.</S>
		<S sid ="2" ssid = "2">We intend for the probabilistic model to provide a probability distribution of verb-class associations, over known and unknown verbs, including pol- ysemous words.</S>
		<S sid ="3" ssid = "3">In our approach, training instances are obtained from an existing lexicon and/or from an annotated corpus, while the features, which represent syntactic frames, semantic similarity, and selectional preferences, are extracted from unannotated corpora.</S>
		<S sid ="4" ssid = "4">Our model is evaluated in type-level verb classification tasks: we measure the prediction accuracy of VerbNet classes for unknown verbs, and also measure the dissimilarity between the learned and observed probability distributions.</S>
		<S sid ="5" ssid = "5">We empirically compare several settings for model learning, while we vary the use of features, source corpora for feature extraction, and disambiguated corpora.</S>
		<S sid ="6" ssid = "6">In the task of verb classification into all VerbNet classes, our best model achieved a 10.69% error reduction in the classification accuracy, over the previously proposed model.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="7" ssid = "7">Lexicons are invaluable resources for semantic processing.</S>
			<S sid ="8" ssid = "8">In many cases, lexicons are necessary to restrict a set of semantic classes to be assigned to a word.</S>
			<S sid ="9" ssid = "9">In fact, a considerable number of works on semantic processing implicitly or explicitly presupposes the availability of a lexicon, such as in word sense disambiguation (WSD) (McCarthy et al., 2004), and in token-level verb class disambiguation (Lapata and Brew, 2004; Girju et al., 2005; Li and Brew, 2007; Abend et al., 2008).</S>
			<S sid ="10" ssid = "10">In other words, those methods are heavily dependent on the availability of a semantic lexicon.</S>
			<S sid ="11" ssid = "11">Therefore, recent research efforts have invested in developing semantic resources, such as WordNet (Fellbaum, 1998), FrameNet (Baker et al., 1998), and VerbNet (Kipper et al., 2000; Kipper-Schuler, 2005), which greatly advanced research in semantic processing.</S>
			<S sid ="12" ssid = "12">However, the construction of such resources is expensive, and it is unrealistic to presuppose the availability of full-coverage lexicons; this is the case because unknown words always appear in real texts, and word-semantics associations may vary (Abend et al., 2008).</S>
			<S sid ="13" ssid = "13">This paper explores a method for the supervised learning of a probabilistic model for the VerbNet lexicon.</S>
			<S sid ="14" ssid = "14">We target the automatic classification of arbitrary verbs, including polysemous verbs, into all VerbNet classes; further, we target the estimation of a probabilistic model, which represents the saliences of verb-class associations for polysemous verbs.</S>
			<S sid ="15" ssid = "15">In our approach, an existing lexicon and/or an annotated corpus are used as the training data.</S>
			<S sid ="16" ssid = "16">Since VerbNet classes are designed to represent the distinctions in the syntactic frames that verbs can take, features, representing the statistics of syntactic frames, are extracted from the unannotated corpora.</S>
			<S sid ="17" ssid = "17">Additionally, as the classes represent semantic commonalities, semantically inspired features, like distributionally similar words, are used.</S>
			<S sid ="18" ssid = "18">These features can be considered as a generalized representation of verbs, and we expect that the obtained probabilistic model predicts VerbNet classes of the unknown words.</S>
			<S sid ="19" ssid = "19">Our model is evaluated in two tasks of type- level verb classification: one is the classification of monosemous verbs into a small subset of the classes, which was studied in some previous works (Joanis and Stevenson, 2003; Joanis et al., 2008).</S>
			<S sid ="20" ssid = "20">The other task is the classification of all verbs into the full set of VerbNet classes, which has not yet 1328 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1328–1337, Singapore, 67 August 2009.</S>
			<S sid ="21" ssid = "21">Qc 2009 ACL and AFNLP been attempted.</S>
			<S sid ="22" ssid = "22">In the experiments, training instances are obtained from VerbNet and/or Sem- Link (Loper et al., 2007), while features are extracted from the British National Corpus or from Wall Street Journal.</S>
			<S sid ="23" ssid = "23">We empirically compare several settings for model learning by varying the set of features, the source domain and the size of a corpus for feature extraction, and the use of the token-level statistics obtained from a manually disambiguated corpus.</S>
			<S sid ="24" ssid = "24">We also provide the analysis of the remaining errors, which will lead us to further improve the supervised learning of a probabilistic semantic lexicon.</S>
			<S sid ="25" ssid = "25">Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).</S>
			<S sid ="26" ssid = "26">However, their focus has been limited to a small subset of verb classes, and a limited number of monosemous verbs.</S>
			<S sid ="27" ssid = "27">The main contributions of the present work are: i) to provide empirical results for the automatic classification of all verbs, including polysemous ones, into all VerbNet classes, and ii) to empirically explore the effective settings for the supervised learning of a probabilistic lexicon of verb semantic classes.</S>
	</SECTION>
	<SECTION title="Background. " number = "2">
			<S sid ="28" ssid = "1">2.1 Verb lexicon.</S>
			<S sid ="29" ssid = "2">Levin’s (1993) work on verb classification has broadened the field of computational research that concerns the relationships between the syntactic and semantic structures of verbs.</S>
			<S sid ="30" ssid = "3">The principal idea behind the work is that the meanings of verbs can be identified by observing possible syntactic frames that the verbs can take.</S>
			<S sid ="31" ssid = "4">In other words, with the knowledge of syntactic frames, verbs can be semantically classified.</S>
			<S sid ="32" ssid = "5">This idea provided the computational linguistics community with criteria for the definition and the classification of verb semantics; it has subsequently resulted in the research of the induction of verb classes (Korhonen and Briscoe, 2004), and the construction of a verb lexicon based on Levin’s criteria.</S>
			<S sid ="33" ssid = "6">VerbNet (Kipper et al., 2000; Kipper-Schuler, 2005) is a lexicon of verbs organized into classes that share the same syntactic behaviors and semantics.</S>
			<S sid ="34" ssid = "7">The design of classes originates from Levin (1993), though the design has been considerably reorganized and extends beyond the original clas 43 Emission.</S>
			<S sid ="35" ssid = "8">43.1 Light Emission.</S>
			<S sid ="36" ssid = "9">beam, glow, sparkle, . . .</S>
			<S sid ="37" ssid = "10">43.2 Sound Emission.</S>
			<S sid ="38" ssid = "11">blare, chime, jangle, . . .</S>
			<S sid ="39" ssid = "12">44 Destroy.</S>
			<S sid ="40" ssid = "13">annihilate, destroy, ravage, . . .</S>
			<S sid ="41" ssid = "14">45 Change of State.</S>
			<S sid ="42" ssid = "15">47 Existence.</S>
			<S sid ="43" ssid = "16">47.1 Exist.</S>
			<S sid ="44" ssid = "17">exist, persist, remain, . . .</S>
			<S sid ="45" ssid = "18">47.2 Entity-Specific Modes Being.</S>
			<S sid ="46" ssid = "19">bloom, breathe, foam, . . .</S>
			<S sid ="47" ssid = "20">47.3 Modes of Being with Motion.</S>
			<S sid ="48" ssid = "21">jiggle, sway, waft, . . .</S>
			<S sid ="49" ssid = "22">Figure 1: VerbNet classes 43.2 Sound Emission.</S>
			<S sid ="50" ssid = "23">Theme V Theme V P:loc Location P:loc Location V Theme there V Theme P:loc Location Agent V Theme Theme V Oblique Location V with Theme 47.3 Modes of Being with Motion.</S>
			<S sid ="51" ssid = "24">Theme V Theme V P:loc Location P:loc Location V Theme there V Theme Agent V Theme Figure 2: Syntactic frames for VerbNet classes sification.</S>
			<S sid ="52" ssid = "25">The classes therefore cover more English verbs, and the classification should be more consistent (Korhonen and Briscoe, 2004; Kipper et al., 2006).</S>
			<S sid ="53" ssid = "26">The current version of VerbNet includes 270 classes.1 Figure 1 shows a part of the classes of VerbNet.</S>
			<S sid ="54" ssid = "27">The top-level categories, e.g. Emission and Destroy, represent a coarse classification of verb semantics.</S>
			<S sid ="55" ssid = "28">They are further classified into verb classes, each of which expresses a group of verbs sharing syntactic frames.</S>
			<S sid ="56" ssid = "29">Figure 2 shows an excerpt from VerbNet, which represents the possible syntactic frames for the Sound Emission class, including “chime” and “jangle,” and the Modes of Being with Motion class, including “jiggle” and “waft.” In this figure, each line represents a syntactic frame, where Agent, 1 Throughout this paper, we refer to VerbNet 2.3.</S>
			<S sid ="57" ssid = "30">Sub- classes are ignored in this work, following the setting of Abend et al.</S>
			<S sid ="58" ssid = "31">(2008).</S>
			<S sid ="59" ssid = "32">the walls still shook;VN=47.3 and an evacuation alarm blared;VN=43.2 outside.</S>
			<S sid ="60" ssid = "33">Suddenly the woman begins;VN=55.1 swaying ;VN=47.3 and then . . .</S>
			<S sid ="61" ssid = "34">Figure 3: An excerpt from SemLink Theme, and Location indicate the thematic roles, V denotes a verb, and P specifies a preposition.</S>
			<S sid ="62" ssid = "35">P:loc defines locative prepositions such as: “in” and “at.” For example, the second syntactic frame of Sound Emission, i.e., Theme V P:loc Location, corresponds to the following sentence: 1.</S>
			<S sid ="63" ssid = "36">The coins jangled in my pocket..</S>
			<S sid ="64" ssid = "37">Theme corresponds to “the coins,” V to “jangled,” P:loc to “in,” and Location to “my pocket.” While VerbNet provides associations between verbs and semantic classes, SemLink (Loper et al., 2007) additionally provides mappings among VerbNet, FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and WordNet (Fellbaum, 1998).</S>
			<S sid ="65" ssid = "38">Since FrameNet and PropBank include annotated instances of sentences, SemLink can be considered as a corpus annotated with VerbNet classes.</S>
			<S sid ="66" ssid = "39">Figure 3 presents some annotated sentences obtained from SemLink.</S>
			<S sid ="67" ssid = "40">For example, the annotation “blared;VN=43.2” indicates that the occurrence of “blare” in this context is classified as Sound Emission.</S>
			<S sid ="68" ssid = "41">2.2 Related work.</S>
			<S sid ="69" ssid = "42">There has been much research effort invested in the automatic classification of verbs into lexical semantic classes, in a supervised or unsupervised way.</S>
			<S sid ="70" ssid = "43">The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).</S>
			<S sid ="71" ssid = "44">Our learning framework basically follows the above listed works: features are obtained from an unannotated (automatically parsed) corpus, and gold verb-class associations are used as training instances for machine learning classifiers, such as decision trees and support vector machines.</S>
			<S sid ="72" ssid = "45">However, those works targeted a small subset of Levin classes, and a limited number of monosemous verbs; for example, Merlo and Stevenson (2001) studied three classes and 59 verbs, and Joanis et al.</S>
			<S sid ="73" ssid = "46">(2008) focused on 14 classes and 835 verbs.</S>
			<S sid ="74" ssid = "47">Although these works provided a theoretical framework for supervised verb classification, their results were not readily available for practical applications, because of the limitation in the coverage of the targeted classes/verbs on real texts.</S>
			<S sid ="75" ssid = "48">On the contrary, we target the classification of arbitrary verbs, including polysemous verbs, into all VerbNet classes (270 in total).</S>
			<S sid ="76" ssid = "49">In this realistic situation, we will empirically compare settings for model learning, in order to explore effective conditions to obtain better models.</S>
			<S sid ="77" ssid = "50">Another difference from the aforementioned works is that we aim at obtaining a probabilistic model, which represents saliences of classes of polysemous verbs.</S>
			<S sid ="78" ssid = "51">Lapata and Brew (2004) and Li and Brew (2007) focused on this issue, and described methods for inducing probabilities of verb-class associations.</S>
			<S sid ="79" ssid = "52">The obtained probabilistic model was intended to be incorporated into a token-level disambiguation model.</S>
			<S sid ="80" ssid = "53">Their methods claimed to be unsupervised, meaning that the induction of a probabilistic lexicon did not require any hand-annotated corpora.</S>
			<S sid ="81" ssid = "54">In fact, however, their methods relied on the existence of a full-coverage lexicon, both in training and running time.</S>
			<S sid ="82" ssid = "55">In their methods, a lexicon was necessary for restricting possible classes to which each word belongs.</S>
			<S sid ="83" ssid = "56">Since most verbs are associated with only a couple of classes, such a restriction significantly reduces the search space, and the problem becomes much easier to solve.</S>
			<S sid ="84" ssid = "57">This presupposition is implicitly or explicitly used in other semantic disambiguation tasks (McCarthy et al., 2004), but it is unrealistic for practical applications.</S>
			<S sid ="85" ssid = "58">Clustering methods have also been extensively researched for verb classification (Stevenson and Merlo, 1999; Schulte im Walde, 2000; McCarthy, 2001; Korhonen, 2002; Korhonen et al., 2003; Schulte im Walde, 2003).</S>
			<S sid ="86" ssid = "59">The extensive research is in large part due to the intuition that the set of classes could not be fixed beforehand.</S>
			<S sid ="87" ssid = "60">In particular, it is often problematic to define a static set of semantic classes.</S>
			<S sid ="88" ssid = "61">However, it is reasonable to assume that the set of VerbNet classes is fixed, because Levin-type classes are more static than on- tological classes, like in WordNet synsets.</S>
			<S sid ="89" ssid = "62">Therefore, we can apply supervised classification methods to our task.</S>
			<S sid ="90" ssid = "63">It is true that the current VerbNet classes are imperfect and require revisions, but in this work we adopt them as they are, because as time advances, more stable classifications will become available.</S>
			<S sid ="91" ssid = "64">The problem focused in this work has a close relationship with automatic thesaurus/ontology expansion.</S>
			<S sid ="92" ssid = "65">In fact, we evaluate our method in the task of automatic verb classification, which can be considered as lexicon expansion.</S>
			<S sid ="93" ssid = "66">The most prominent difference of the present work from thesaurus/ontology expansion is that the number of classes is much smaller in our problem, and the set of verb classes can be assumed to be fixed.</S>
			<S sid ="94" ssid = "67">These characteristics indicate that our problem is easier and more well-defined than is the case for automatic thesaurus/ontology expansion.</S>
			<S sid ="95" ssid = "68">Supervised approaches to token-level verb class disambiguation have recently been addressed (Girju et al., 2005; Abend et al., 2008), largely owing to SemLink.</S>
			<S sid ="96" ssid = "69">Their approaches fundamentally follow traditional supervised WSD methods: ex tic classes.</S>
			<S sid ="97" ssid = "70">We do not presuppose the existence of a full-coverage lexicon; instead, we use an existing lexicon for the training data.</S>
			<S sid ="98" ssid = "71">Combined with features extracted from unannotated corpora, a probabilistic model is learned from the existing lexicon.</S>
			<S sid ="99" ssid = "72">Like other supervised learning applications, our probabilistic lexicon can predict classes for words that are not included in the original lexicon.</S>
			<S sid ="100" ssid = "73">Our model is defined in the following way.</S>
			<S sid ="101" ssid = "74">We assume that the set, C , of verb classes is fixed, while a set of verbs is unfixed.</S>
			<S sid ="102" ssid = "75">With this assumption, probabilistic modeling can be reduced to a classification problem.</S>
			<S sid ="103" ssid = "76">Specifically, the goal is to obtain a probability distribution, p(c|v), of verb class c ∈ C for a given verb (lemma) v. We can therefore apply well-known supervised learning methods to estimate p(c|v).</S>
			<S sid ="104" ssid = "77">This probability is modeled in the form of a log- linear model.</S>
			<S sid ="105" ssid = "78">tracting features representing the context in which the target word appears, and training a classification model with an annotated corpus.</S>
			<S sid ="106" ssid = "79">While those 1 p(c|v) = Z exp ( \ ) λifi(c, v) , i works achieved an impressive accuracy (more than 95%), the results may not necessarily indicate the method’s effectiveness; rather, it may imply the importance of a lexicon.</S>
			<S sid ="107" ssid = "80">In fact, these works restrict their target to verb tokens, in which the correct class exists in a given lexicon, and they only consider candidate classes that are registered in the lexicon.</S>
			<S sid ="108" ssid = "81">This setting reduces the ambiguity significantly, and the problem becomes much easier to handle; for example, approximately half of verb tokens are monosemous in their setting.</S>
			<S sid ="109" ssid = "82">Thus, a simple baseline achieves very high accuracy figures.</S>
			<S sid ="110" ssid = "83">However, in our preliminary experiment on token-level verb classification with unknown verbs, we found that the accuracy for unknown verbs (i.e., lemmas not included in the VerbNet lexicon) is catastrophically low.</S>
			<S sid ="111" ssid = "84">This indicates that VerbNet and SemLink are insufficient for unknown verbs, and that we cannot expect the availability of a full-coverage lexicon in the real world.</S>
			<S sid ="112" ssid = "85">Instead of a static lexicon, our probabilistic model is intended to be used as a prior distribution for the token-level disambiguation, as in Lapata and Brew (2004)’s model.</S>
	</SECTION>
	<SECTION title="A probabilistic model for verb " number = "3">
			<S sid ="113" ssid = "1">semantic classes In this work, supervised learning is applied to the probabilistic modeling of a lexicon of verb seman where fi(c, v) are features that represent characteristics of c and v, and λi are model parameters that express weights of the corresponding features.</S>
			<S sid ="114" ssid = "2">Model parameters can be estimated when training instances, i.e., pairs (c, v), and features, fi(c, v), for each instance are given.</S>
			<S sid ="115" ssid = "3">Therefore, what we have to do is to prepare the training in stances (c, v), and effective features fi(c, v) thatcontribute to the better estimation of probabili ties.</S>
			<S sid ="116" ssid = "4">In token tagging tasks, both training instances and features are extracted from annotated corpora.</S>
			<S sid ="117" ssid = "5">However, since our goal is the probabilistic modeling of a lexicon, we have to determine how to derive the training instances and features for lexicon entries, to be discussed in the next section.</S>
			<S sid ="118" ssid = "6">For the parameter estimation of log-linear models, we applied the stochastic gradient descent method.</S>
			<S sid ="119" ssid = "7">A hyperparameter for l2regularization was tuned to minimize the KL-divergence (see Section 4.4) for the development set.</S>
	</SECTION>
	<SECTION title="Experiment design. " number = "4">
			<S sid ="120" ssid = "1">In this work, we empirically compare several settings for the learning of the above probabilistic model, in the two tasks of automatic verb classification.</S>
			<S sid ="121" ssid = "2">In what follows, we explain the training/test data, corpora for extracting features, and the design of the features and evaluation tasks.</S>
			<S sid ="122" ssid = "3">The measures for evaluation are also introduced.</S>
			<S sid ="123" ssid = "4">In addition to the variance of the corpus domains, we vary the size of the corpus to observe the effect of increasing the corpus size.</S>
			<S sid ="124" ssid = "5">These corpora are automatically parsed by Enju 2.3.1 (Miyao and Tsujii, 2008), and the features are extracted from the parsing results.</S>
			<S sid ="125" ssid = "6">Figure 4: Training instances obtained from Verb- Net (upper) and VerbNet+SemLink (lower) 4.1 Data.</S>
			<S sid ="126" ssid = "7">As our goal is the supervised learning of a lexicon of verb semantic classes, VerbNet is used as the training/test data.</S>
			<S sid ="127" ssid = "8">In addition, since we aim at representing the saliences of verb-class associations with probabilities, the gold probabilities are necessary.</S>
			<S sid ="128" ssid = "9">For this purpose, we count the occurrences of each verb-class association in the VerbNetPropBank token mappings in the subset of the SemLink corresponding to sections 2 through 21 of Penn Treebank (Marcus et al., 1994).</S>
			<S sid ="129" ssid = "10">Frequency counts are normalized for each lemma, with the Laplace smoothing (the parameter is 0.5).</S>
			<S sid ="130" ssid = "11">In this work, we compare the two settings for creating training instances.</S>
			<S sid ="131" ssid = "12">By comparing the results of these settings, we evaluate the necessity of an annotated corpus for learning a probabilistic lexicon of verb semantic classes.</S>
			<S sid ="132" ssid = "13">VerbNet We collect all (c, v) pairs registered in VerbNet.</S>
			<S sid ="133" ssid = "14">For each v, all of the associated classes are assigned equal weights (see the upper part of Figure 4).</S>
			<S sid ="134" ssid = "15">VerbNet+SemLink Each pair (c, v) in VerbNet is weighted by the normalized frequency obtained from SemLink (see the lower part of Figure 4).</S>
			<S sid ="135" ssid = "16">Because VerbNet classes represent groups of syntactic frames, and it is impossible to guess the verb class by referring to only one occurrence in a text, it is necessary to have statistics over a sufficient amount of a corpus.</S>
			<S sid ="136" ssid = "17">Hence, features are extracted from a large unannotated corpus.</S>
			<S sid ="137" ssid = "18">In this paper, we use the following two corpora: WSJ Wall Street Journal newspaper articles (around 40 million words).</S>
			<S sid ="138" ssid = "19">BNC British National Corpus, which is a balanced corpus of around 100 million words.</S>
			<S sid ="139" ssid = "20">4.2 Features.</S>
			<S sid ="140" ssid = "21">Levin-like classes, including VerbNet, are designed to represent distinctions in syntactic frames and alternations.</S>
			<S sid ="141" ssid = "22">Hence, if we were given the perfect knowledge of the possible syntactic frames, verbs can be classified into the correct classes almost perfectly (Dorr and Jones, 1996).</S>
			<S sid ="142" ssid = "23">Previous works thus proposed features that express the corpus statistics of syntactic frames.</S>
			<S sid ="143" ssid = "24">However, class boundaries are subtle in some cases; several classes share syntactic frames with each other to a large extent.</S>
			<S sid ="144" ssid = "25">For example, the classes shown in Figure 2 have very similar syntactic frames.</S>
			<S sid ="145" ssid = "26">The difference is indicated in the last two frames of Sound Emission, although they appear much less frequently in real texts.</S>
			<S sid ="146" ssid = "27">Therefore, it is difficult to accurately capture the distinctions between these classes, if we are only provided with the statistics of the syntactic frames that appear in real texts.</S>
			<S sid ="147" ssid = "28">In this case, however, it is easy to observe that the verbs of these classes have different selectional preferences; that is, the Theme of Sound Emission verbs would be objects that make sounds, while the Theme of Modes of Being with Motion is likely to be objects that move.2 Although Levin’s classification initially focused on syntactic alternations, the resulting classes represent some semantic common- alities.</S>
			<S sid ="148" ssid = "29">Hence, it would be reasonable to design features that capture such semantic characteristics.</S>
			<S sid ="149" ssid = "30">In this work, we re-implemented the following features proposed by Joanis et al.</S>
			<S sid ="150" ssid = "31">(2008) as the starting point.</S>
			<S sid ="151" ssid = "32">Syntactic slot Features to count the occurrences of each syntactic slot, such as subject, object, and prepositional phrases.</S>
			<S sid ="152" ssid = "33">For the subject slot, we also count its transitive and intransitive usages separately.</S>
			<S sid ="153" ssid = "34">Additionally, we count the appearances of reflexive pronouns and semantically empty constituents (it and 2 Syntactic frames in VerbNet include specifications of selectional preferences, such as animate and place, although we do not explicitly use them, because it is not apparent to determine the members of these semantic classes.</S>
			<S sid ="154" ssid = "35">Syntactic slot subj:0.885 intranssubj:0.578 Slot overlap overlap-subj-obj:0.299 overlap-obj-in:0.074 Tense, voice, aspect posVBG:0.307 posVBD:0.290 Animacy animsubj:0.244 animobj:0.057 Slot POS subjPRP:0.270 subjNN:0.270 Syntactic frame NP_V:0.326 NP_V_NP:0.307 Similar word sim-rock:0.090 sim-swing:0.083 Slot class subjC82:0.219 objC12:0.081 Figure 5: Example of features for “sway” there).</S>
			<S sid ="155" ssid = "36">Differently from Joanis et al.</S>
			<S sid ="156" ssid = "37">(2008), we consider non-nominal arguments, such as sentential and adjectival complements.</S>
			<S sid ="157" ssid = "38">Slot overlap Features to measure the overlap in words (lemmas) between two syntactic slots of the verb.</S>
			<S sid ="158" ssid = "39">They are intended to approximate argument alternations, such as the ergative alternation.</S>
			<S sid ="159" ssid = "40">For example, for the alternation “The sky cleared”/“The clouds cleared from the sky,” a feature to indicate the overlap between the subject slot and the from slot is added (Joanis et al., 2008).</S>
			<S sid ="160" ssid = "41">The value of this feature is computed by the method of Merlo and Stevenson (2001).</S>
			<S sid ="161" ssid = "42">Tense, voice, aspect Features to approximate the tendency of the tense, voice, and aspect of the target verb.</S>
			<S sid ="162" ssid = "43">The Penn Treebank POS tags for verbs (VB, VBP, VBZ, VBG, VBD, and VBN) are counted.</S>
			<S sid ="163" ssid = "44">In addition, included are the frequency of the co-occurrences with an adverb or an auxiliary verb, and the count of usages as a noun or an adjective.</S>
			<S sid ="164" ssid = "45">Animacy Features to measure the frequency of animate arguments for each syntactic slot.</S>
			<S sid ="165" ssid = "46">Personal pronouns except it are counted as animate, following Joanis et al.</S>
			<S sid ="166" ssid = "47">(2008), while named entity recognition was not used.</S>
			<S sid ="167" ssid = "48">Examples of these features are shown in Figure 5.</S>
			<S sid ="168" ssid = "49">For details, refer to Joanis et al.</S>
			<S sid ="169" ssid = "50">(2008).</S>
			<S sid ="170" ssid = "51">The above features mainly represent syntactic behaviors of target verbs.</S>
			<S sid ="171" ssid = "52">Since our target classes are broader than in the previous works, we further enhance the syntactic features.</S>
			<S sid ="172" ssid = "53">Additionally, as discussed above, semantically motivated features may present strong clues to distinguish among syntactically similar classes.</S>
			<S sid ="173" ssid = "54">We therefore include the following four types of feature; the first two are syntactic, while the other two are intended to capture semantic characteristics: Slot POS In addition to the syntactic slot features, we add features that represent a combination of a syntactic slot and the POS of its head word.</S>
			<S sid ="174" ssid = "55">Since VerbNet includes extended classes that take verbal and adjectival arguments, the POSs of arguments would provide a strong clue to discriminate among these syntactic frames.</S>
			<S sid ="175" ssid = "56">Syntactic frame The number of arguments and their syntactic categories.</S>
			<S sid ="176" ssid = "57">This feature was mentioned as a baseline in Joanis et al.</S>
			<S sid ="177" ssid = "58">(2008), but we include it in our model.</S>
			<S sid ="178" ssid = "59">Similar word Similar words (lemmas) to the target verb.</S>
			<S sid ="179" ssid = "60">Similar words are automatically obtained from a corpus (the same corpus as used for feature extraction) by Lin (1998)’s method.</S>
			<S sid ="180" ssid = "61">This feature is motivated by the hypothesis that distributionally similar words tend to be classified into the same class.</S>
			<S sid ="181" ssid = "62">Because Lin’s method is based on the similarity of words in syntactic slots, the obtained similar words are expected to represent a verb class that share selectional preferences.</S>
			<S sid ="182" ssid = "63">Slot class Semantic classes of the head words of the arguments.</S>
			<S sid ="183" ssid = "64">This feature is also intended to approximate selectional preferences.</S>
			<S sid ="184" ssid = "65">The semantic classes are obtained by clustering nouns, verbs, and adjectives into 200, 100, and 50 classes respectively, by using the k- medoid method with Lin (1998)’s similarity.</S>
			<S sid ="185" ssid = "66">Figure 5 shows an example of the features for “sway,” extracted from the BNC corpus.3 Feature values are defined as relative frequencies for each lemma; while, for similar word features, feature values are weighted by Lin’s similarity measure.</S>
			<S sid ="186" ssid = "67">4.3 Tasks.</S>
			<S sid ="187" ssid = "68">We evaluate our model in the tasks of automatic verb classification (a.k.a. lexicon expansion): given gold verb-class associations for some set of verbs, we predict the classes for unknown 3 “C82” and “C12” are automatically assigned cluster names.</S>
			<S sid ="188" ssid = "69">Verb class Levin class number Recipient 13.1, 13.3 Admire 31.2 Amuse 31.1 Run 51.3.2 Sound Emission 43.2 Light and Substance Emission 43.1, 43.4 Cheat 10.6 Steal and Remove 10.5, 10.1 Wipe 10.4.1, 10.4.2 Spray/Load 9.7 Fill 9.8 Other Verbs of Putting 9.1–6 Change of State 45.1–4 Object Drop 26.1, 26.3, 26.7 Table 1: 14 classes used in Joanis et al.</S>
			<S sid ="189" ssid = "70">(2008) and their corresponding Levin class numbers verbs.</S>
			<S sid ="190" ssid = "71">While our main target is the full set of Verb- Net classes, we also show results for the task studied in the previous work.</S>
			<S sid ="191" ssid = "72">14-class task The task to classify (almost) monosemous verbs into 14 classes.</S>
			<S sid ="192" ssid = "73">Refer to Table 1 for the definition of the 14 classes.</S>
			<S sid ="193" ssid = "74">Following Joanis et al.</S>
			<S sid ="194" ssid = "75">(2008)’s task definition, we removed verbs that belong to multiple classes in these 14 classes, and also removed overly polysemous verbs (in our experiment, verb-class associations that have the relative frequency that is less than 0.5 in SemLink are removed).</S>
			<S sid ="195" ssid = "76">For each class, member verbs are randomly split into 50% (training), 25% (development), and 25% (final test) sets.</S>
			<S sid ="196" ssid = "77">All-class task The task to classify all target verbs into 268 classes.4 Any verbs that did not occur at least 100 times in the BNC corpus were removed.5 The remaining verbs (2517 words) are randomly split into 80% (training), 10% (development), and 10% (final test) sets, under the constraint that at least one instance for each class is included in the training set.6 4.4 Evaluation measures.</S>
			<S sid ="197" ssid = "78">For the 14-class task, we simply measure the classification accuracy.</S>
			<S sid ="198" ssid = "79">However, the evaluation in the 4 Two classes (Being Dressed and Debone) are not used in the experiments because no lemmas belonged to these classes after filtering by the frequency in BNC.</S>
			<S sid ="199" ssid = "80">5 This is the same preprocessing as Joanis et al.</S>
			<S sid ="200" ssid = "81">(2008), although we use VerbNet, while Joanis et al.</S>
			<S sid ="201" ssid = "82">(2008) used the original Levin classifications.</S>
			<S sid ="202" ssid = "83">6 Because polysemous verbs belong to multiple classes, the class-wise data split was not adopted for the all-class task.</S>
			<S sid ="203" ssid = "84">all-class task is not trivial, because verbs may be assigned multiple classes.</S>
			<S sid ="204" ssid = "85">Since our purpose is to obtain a probabilistic model rather than to classify monosemous verbs, the evaluation criterion should be sensitive to the probabilistic distribution on the test data.</S>
			<S sid ="205" ssid = "86">In this paper, we adopt two evaluation measures.</S>
			<S sid ="206" ssid = "87">One is the top-N weighted accuracy; we count thenumber of correct pairs (c, v) in the N -best out puts from the model (where N is the number of gold classes for each lemma), where each count is weighted by the relative frequency (i.e., the counts in SemLink) of the pair in the test set.</S>
			<S sid ="207" ssid = "88">For example, in the case for “blare” in Figure 4, if the model states that Sound Emission has the largest probability, we get 0.7 points.</S>
			<S sid ="208" ssid = "89">If Manner Speaking has the largest probability, we instead obtain 0.3 points.</S>
			<S sid ="209" ssid = "90">Intuitively, the score is higher when the model presents larger probabilities to classes with higher relative frequencies.</S>
			<S sid ="210" ssid = "91">This measure is similar to the top-N precision in information retrieval; it evaluates the ranked output by the model.</S>
			<S sid ="211" ssid = "92">It is intuitively interpretable, but is insufficient for evaluating the quality of probability distributions.</S>
			<S sid ="212" ssid = "93">The other measure is KL-divergence, which is popularly used for measuring the dissimilarity between two probability distributions.</S>
			<S sid ="213" ssid = "94">This is defined as follows: K L(p||q) = ) p(x) log(p(x)) − p(x) log(q(x)).</S>
			<S sid ="214" ssid = "95">x In the experiments, this measure is applied, with the assumption that p is the relative frequencyof (c, v) in the test set, and that q is the estimated probability distribution.</S>
			<S sid ="215" ssid = "96">Although the KL divergence is not a true distance metric, it is sufficient for measuring the fitting of the estimated model to the true distribution.</S>
			<S sid ="216" ssid = "97">We report the KL-divergence averaged over all verbs in the test set.</S>
			<S sid ="217" ssid = "98">Since this measure indicates a dissimilarity, a smaller value is better.</S>
			<S sid ="218" ssid = "99">When p and q are equiva lent, K L(p||q) = 0.</S>
	</SECTION>
	<SECTION title="Experimental results. " number = "5">
			<S sid ="219" ssid = "1">Table 2 shows the accuracy obtained for the 14- class task.</S>
			<S sid ="220" ssid = "2">The first column denotes the incorporated features (“Joanis et al.’s features” or “All features”), and the sources of the features (“WSJ” or “BNC”).</S>
			<S sid ="221" ssid = "3">The two baseline results are also given: “Baseline (random)” indicates that classes are randomly output, and “Baseline (majority)” indicates Accuracy Baseline (random) 7.14 Baseline (majority) 26.47 Joanis et al.’s features/WSJ 56.86 Joanis et al.’s features/BNC 64.22 All features/WSJ 60.29 All features/BNC 68.14 Accuracy KL Baseline (random) 0.37 — Baseline (majority) 8.69 — Joanis et al.’s features/WSJ 29.65 3.67 Joanis et al.’s features/BNC 35.78 3.34 All features/WSJ 34.53 3.40 All features/BNC 42.38 3.02 Table 2: Accuracy for the 14-class task Table 4: Accuracy and KL-divergence for the all- class task (the VerbNet only setting) 50 Accuracy (Joanis et al.’s features, WSJ) Accuracy (Joanis et al.’s features, BNC) Accuracy (all features, WSJ) Accuracy (all features, BNC) 40 30 Table 3: Accuracy and KL-divergence for the all- 20 class task (the VerbNet+SemLink setting) 10 that the majority class (i.e., the class that has the largest number of member verbs) is output to every lemma.</S>
			<S sid ="222" ssid = "4">While these figures cannot be compared directly to the previous works due to the difference in the preprocessing, Joanis et al.</S>
			<S sid ="223" ssid = "5">(2008) achieved 58.4% accuracy for the 14-class task.</S>
			<S sid ="224" ssid = "6">Table 3 and 4 present the results for the all-class task.</S>
			<S sid ="225" ssid = "7">Table 3 gives the accuracy and KL-divergence achieved by the model trained with the VerbNet+SemLink training instances, while Table 4 presents the same measures by the training instances created from VerbNet only.</S>
			<S sid ="226" ssid = "8">Our models performed substantially better on both tasks than the baseline models.</S>
			<S sid ="227" ssid = "9">The results also proved that the features we proposed in this paper contributed to the further improvement of the model from Joanis et al.</S>
			<S sid ="228" ssid = "10">(2008).</S>
			<S sid ="229" ssid = "11">In the all-class task with the VerbNet+SemLink setting, our features achieved 10.69% error reduction in the accuracy over Joanis et al.</S>
			<S sid ="230" ssid = "12">(2008)’s features.</S>
			<S sid ="231" ssid = "13">Another interesting fact is that the model with BNC consistently outperformed the model with WSJ.</S>
			<S sid ="232" ssid = "14">This outcome is somewhat surprising, provided that the relative frequencies in the training/test sets are created from the WSJ portion of SemLink.</S>
			<S sid ="233" ssid = "15">The reason for this is independent of the corpus size, as will be shown below.</S>
			<S sid ="234" ssid = "16">When comparing Table 3 and 4, we can see that using SemLink statistics resulted in a slightly better model.</S>
			<S sid ="235" ssid = "17">This result is predictable, because the evaluation measures are sensitive to the relative frequencies estimated from SemLink.</S>
			<S sid ="236" ssid = "18">However, the difference remained small.</S>
			<S sid ="237" ssid = "19">In both of the tasks and the evaluation measures, the best model was achieved when we use 0 0 20 40 60 80 100 Corpus size (M words) Figure 6: Corpus size vs. accuracy all the features extracted from BNC, and create training instances from VerbNet+SemLink.</S>
			<S sid ="238" ssid = "20">Figure 6 and 7 plot the accuracy and KL- divergence against the size of the unannotated corpus used for feature extraction.</S>
			<S sid ="239" ssid = "21">The result clearly indicates that the learning curve still grows at the corpus size with 100 million words (especially for the all features + BNC setting), which indicates that better models are obtained by increasing the size of the unannotated corpora.</S>
			<S sid ="240" ssid = "22">Therefore, we can claim that the differences between the domains and the size of the unannotated corpora are more influential than the availability of the annotated corpora.</S>
			<S sid ="241" ssid = "23">This indicates that learning only from a lexicon would be a viable solution, when a token-disambiguated corpus like SemLink is unavailable.</S>
			<S sid ="242" ssid = "24">Table 5 shows the contribution of each feature group.</S>
			<S sid ="243" ssid = "25">BNC is used for feature extraction, and VerbNet+SemLink is used for the creation of training instances.</S>
			<S sid ="244" ssid = "26">The results demonstrated the effectiveness of the slot POS features, and in particular, for the all-class task, most likely because Verb- Net covers verbs that take non-nominal arguments.</S>
			<S sid ="245" ssid = "27">Additionally, the similar word features contributed equally or more in both of the tasks.</S>
			<S sid ="246" ssid = "28">This result suggests that we were reasonable in hypothesizing that distributionally similar words tend to be clas 4.5 4 3.5 3 2.5 K L ( J o a n i s e t a l . ’ s f e a t u r e s , W S J ) K L ( J o a n i s e t a l . ’ s f e a t u r e s , B N C ) K L ( a l l f e a t u r e s , W S J ) K L ( a l l f e a t u r e s , B N C ) 0 2 0 4 0 6 0 8 0 1 00 C o r p u s s i z e ( M w o r d s ) Anot her majo r error is in class ifyin g verb s into O t h e r C h a n g e o f S t a t e . E x a m p l e s i n c l u d e : • Amu se verb s: “imp air,” “rec harg e.” • Her d verb s: “agg regat e,” “ma ss.” Becau se Other Chan ge of State is one of the bigges t classe s, superv ised learni ng tends to place a high proba bility to this class.</S>
			<S sid ="247" ssid = "29">Theref ore, when strong clues do not exist, verbs tend to be misclassif ied into this class.</S>
			<S sid ="248" ssid = "30">In additi on, this class is not syntac tically /sema nticall y homo geneo us, and Figure 7: Corpus size vs. KL-divergence 14-classes All classes Accuracy Accuracy KL Baseline (random) 7.14 0.37 — Baseline (majority) 26.47 8.69 — Joanis et al.’s features 64.22 35.66 3.32 + Slot POS 66.67 38.77 3.18 + Syntactic frame 64.71 35.99 3.29 + Similar word 68.14 37.88 3.10 + Slot class 64.71 36.51 3.26 All features 68.14 42.54 2.99 Table 5: Contribution of features sified into the same class.</S>
			<S sid ="249" ssid = "31">Slot classes also contributed to a slight improvement, indicating that selectional preferences are effective clues for predicting VerbNet classes.</S>
			<S sid ="250" ssid = "32">The result of the “All features” model for the all-class task attests that these features worked collaboratively, and using them all resulted in a considerably better model.</S>
			<S sid ="251" ssid = "33">From the analysis of the confusion matrix for the outputs by our best model, we identified several reasons for the remaining misclassification errors.</S>
			<S sid ="252" ssid = "34">A major portion of the errors were caused by confusing the classes that take the same prepositions.</S>
			<S sid ="253" ssid = "35">Examples of these errors include: • Other Change of State verbs were misclassified into the Butter class: “embalm,” “laminate.” (they take “with” phrases) • Judgement verbs were misclassified into the Characterize class: “acclaim,” “hail.” (they take “as” phrases) Since prepositions are strong features for automatic verb classification (Joanis et al., 2008), the classes that take the same prepositions remained confusing.</S>
			<S sid ="254" ssid = "36">The discovery of the features to discriminate among these classes would be crucial for further improvement.</S>
			<S sid ="255" ssid = "37">is likely to introduce noise in the machine learning classifier.</S>
			<S sid ="256" ssid = "38">A possible solution to this problem would be to exclude this class from the classification, and to process the class separately.</S>
	</SECTION>
	<SECTION title="Conclusions. " number = "6">
			<S sid ="257" ssid = "1">We presented a method for the supervised learning of a probabilistic model for a lexicon of Verb- Net classes.</S>
			<S sid ="258" ssid = "2">By combining verb-class associations from VerbNet and SemLink, and features extracted from a large unannotated corpus, we could successfully train a log-linear model in a supervised way.</S>
			<S sid ="259" ssid = "3">The experimental results attested to our success that features proposed in this paper worked effectively in obtaining a better probability distribution.</S>
			<S sid ="260" ssid = "4">Not only syntactic features, but also semantic features were shown to be effective.</S>
			<S sid ="261" ssid = "5">While each of these features could increase the accuracy, they collaboratively contributed to a large improvement.</S>
			<S sid ="262" ssid = "6">In the all-class task, we obtained 10.69% error reduction in the classification accuracy over Joanis et al.</S>
			<S sid ="263" ssid = "7">(2008)’s model.</S>
			<S sid ="264" ssid = "8">We also observed the trend that a larger corpus for feature extraction led to a better model, indicating that a better model will be obtained by increasing the size of an unannotated corpus.</S>
			<S sid ="265" ssid = "9">We could identify the effective features and settings for this problem, but the classification into all VerbNet classes remained challenging.</S>
			<S sid ="266" ssid = "10">One possible direction for this research topic would be to use our model for the semiautomatic construction of verb lexicons, with the help of human curation.</S>
			<S sid ="267" ssid = "11">However, there is also a demand for exploring other types of features that can discriminate among confusing classes.</S>
	</SECTION>
	<SECTION title="Acknowledgments">
			<S sid ="268" ssid = "12">This work was partially supported by Grant-in- Aid for Specially Promoted Research and Grant- in-Aid for Young Scientists (MEXT, Japan).</S>
	</SECTION>
</PAPER>
