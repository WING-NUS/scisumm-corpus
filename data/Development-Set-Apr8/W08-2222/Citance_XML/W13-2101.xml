<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Statistical natural language generation from abstract meaning representations presupposes large corpora consisting of text–meaning pairs.</S>
		<S sid ="2" ssid = "2">Even though such corpora exist nowadays, or could be constructed using robust semantic parsing, the simple alignment between text and meaning representation is too coarse for developing robust (statistical) NLG systems.</S>
		<S sid ="3" ssid = "3">By reformatting semantic representations as graphs, fine-grained alignment can be obtained.</S>
		<S sid ="4" ssid = "4">Given a precise alignment at the word level, the complete surface form of a meaning representations can be deduced using a simple declarative rule.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="5" ssid = "5">Surface Realization is the task of producing fluent text from some kind of formal, abstract representation of meaning (Reiter and Dale, 2000).</S>
			<S sid ="6" ssid = "6">However, while it is obvious what the output of a natural language generation component should be, namely text, there is little to no agreement on what its input formalism should be (Evans et al., 2002).</S>
			<S sid ="7" ssid = "7">Since open-domain semantic parsers are able to produce formal semantic representations nowadays (Bos, 2008; Butler and Yoshimoto, 2012), it would be natural to see generation as a reversed process, and consider such semantic representations as input of a surface realization component.</S>
			<S sid ="8" ssid = "8">The idea of using large text corpora annotated with formal semantic representations for robust generation has been presented recently (Basile and Bos, 2011; Wanner et al., 2012).</S>
			<S sid ="9" ssid = "9">The need for formal semantic representations as a basis for NLG was expressed already much earlier by Power (1999), who derives semantic networks enriched with scope information from knowledge representations for content planning.</S>
			<S sid ="10" ssid = "10">In this paper we take a further step towards the goal of generating text from deep semantic representations, and consider the issue of aligning the representations with surface strings that capture their meaning.</S>
			<S sid ="11" ssid = "11">First we describe the basic idea of aligning semantic representations (logical forms) with surface strings in a formalism-independent way (Section 2).</S>
			<S sid ="12" ssid = "12">Then we apply our method to a well-known and widely-used semantic formalism, namely Discourse Representation Theory (DRT), first demonstrating how to represent Discourse Representation Structures (DRSs) as graphs (Section 3) and showing that the resulting Discourse Representation Graphs (DRGs) are equivalent to DRSs but are more convenient to fulfill word- level alignment (Section 4).</S>
			<S sid ="13" ssid = "13">Finally, in Section 5 we present a method that generates partial surface strings for each discourse referent occurring in the semantic representation of a text, and composes them into a complete surface form.</S>
			<S sid ="14" ssid = "14">All in all, we think this would be a first and important step in surface realization from formal semantic representations.</S>
	</SECTION>
	<SECTION title="Aligning Logic with Text. " number = "2">
			<S sid ="15" ssid = "1">Several different formal semantic representations have been proposed in the literature, and although they might differ in various aspects, they also have a lot in common.</S>
			<S sid ="16" ssid = "2">Many semantic representations (or logical forms as they are sometimes referred to) are variants of first-order logic and share basic building blocks such as entities, properties, and relations, complemented with quantifiers, negation and further scope operators.</S>
			<S sid ="17" ssid = "3">A simple snapshot of a formal meaning representation is the following (with symbols composed out of WordNet (Fellbaum, 1998) synset identifiers to abstract away from natural language): blue#a#1(x) ∧ cup#n#1(x) How could this logical form be expressed in natural language?</S>
			<S sid ="18" ssid = "4">Or put differently, how could we 1 Proceedings of the 14th European Workshop on Natural Language Generation, pages 1–9, Sofia, Bulgaria, August 89 2013.</S>
			<S sid ="19" ssid = "5">Qc 2013 Association for Computational Linguistics realize the variable x in text?</S>
			<S sid ="20" ssid = "6">As simple as it is, x describes “a blue cup”, or if your target language is Italian, “una tazza blu”, or variants hereof, e.g. “every blue cup” (if x happens to be bound by universally quantified) or perhaps as “una tazza azzurra”, using a different adjective to express blue- ness.</S>
			<S sid ="21" ssid = "7">This works for simple examples, but how does it scale up to larger and more complex semantic representations?</S>
			<S sid ="22" ssid = "8">In a way, NLG can be viewed as a machine translation (MT) task, but unlike translating from one natural language into another, the task is here to translate a formal (unambiguous) language into a natural language like English or Italian.</S>
			<S sid ="23" ssid = "9">Current statistical MT techniques are based on large parallel corpora of aligned source and target text.</S>
			<S sid ="24" ssid = "10">In this paper we introduce a method for precise alignment of formal semantic representations and text, with the purpose of creating a large corpus that could be used in NLG research, and one that opens the way for statistical approaches, perhaps similar to those used in MT. Broadly speaking, alignments between semantic representations and surface strings can be made in three different ways.</S>
			<S sid ="25" ssid = "11">The simplest strategy, but also the least informative, is to align a semantic representation with a sentence or complete text without further information on which part of the representation produces what part of the surface form.</S>
			<S sid ="26" ssid = "12">This might be enough to develop statistical NLG systems for small sentences, but probably does not scale up to handle larger texts.</S>
			<S sid ="27" ssid = "13">Alternatively, one could devise more complex schemes that allow for a more fine-grained alignment between parts of the semantic representation and sur a better starting point for the development of a statistical NLG component.</S>
			<S sid ="28" ssid = "14">With sufficient data in the form of aligned texts with semantic representations, these alignments can be automatically learned, thus creating a model to generate surface forms from abstract, logical representations.</S>
			<S sid ="29" ssid = "15">However, aligning semantic representations with words is a difficult enterprise, primarily because formal semantic representations are not flat like a string of words and often form complex structures.</S>
			<S sid ="30" ssid = "16">To overcome this issue we represent formal semantic representations as a set of tuples.</S>
			<S sid ="31" ssid = "17">For instance, returning to our earlier example representation for “blue cup”, we could repre sent part of it by the tuples (blue#a#1,arg,x) and (cup#n#1,arg,x).</S>
			<S sid ="32" ssid = "18">For convenience we can display this as a graph (Figure 1).</S>
			<S sid ="33" ssid = "19">blue#a#1 x cup#n#1 Figure 1: Logical form graph.</S>
			<S sid ="34" ssid = "20">Note that in this example several tuples are not shown for clarity (such as conjunction and the quantifier).</S>
			<S sid ="35" ssid = "21">We show below that we can indeed represent every bit of semantic information in this format without sacrificing the capability of alignment with the text.</S>
			<S sid ="36" ssid = "22">The important thing now is to show how alignments between tuples and words can be realized, which is done by adding an element to each tuple denoting the surface string, for instance (cup#n#1,arg,x,”tazza”), as in Figure 2.</S>
			<S sid ="37" ssid = "23">face strings (words and phrases).</S>
			<S sid ="38" ssid = "24">Here there are two routes to follow, which we call the minimal and maximal alignment.</S>
			<S sid ="39" ssid = "25">In maximal alignment, each single piece of the blue#a#1 cup#n#1 &quot;blue&quot; &quot;a&quot; x &quot;cup&quot; blue#a#1 cup#n#1 &quot;blu&quot; &quot;una&quot; x &quot;tazza&quot; semantic representation corresponds to the words that express that part of the meaning.</S>
			<S sid ="40" ssid = "26">Possible problems with this approach are that perhaps not every bit of the semantic representation corresponds to a surface form, and a single word could also correspond to various pieces in the semantic representation.</S>
			<S sid ="41" ssid = "27">This is an interesting option to explore, but in this paper we present the alternative approach, minimal alignment, which is a method where every word in the surface string points to exactly one part of the semantic representation.</S>
			<S sid ="42" ssid = "28">We think this alignment method forms Figure 2: Logical form graphs aligned with surface forms in two languages.</S>
			<S sid ="43" ssid = "29">We can further refine the alignment by saying something about the local order of surface expressions.</S>
			<S sid ="44" ssid = "30">Again, this is done by adding an element to the tuple, in this case one that denotes the local order of a logical term.</S>
			<S sid ="45" ssid = "31">We will make this clear by continuing with our example, where we add word order encoded as numerical indices in the tuple, e.g.</S>
			<S sid ="46" ssid = "32">(cup#n#1,arg,x,”tazza”,2), as Figure 3 shows.</S>
			<S sid ="47" ssid = "33">From these graphs we can associate the term blue#a#1 cup#n#1 &quot;blue&quot; 2 &quot;a&quot; 1 x &quot;cup&quot; 3 blue#a#1 cup#n#1 &quot;blu&quot; 3 &quot;una&quot; 1 x &quot;tazza&quot; 2 cific tailoring the semantic representa tion to one’s (technical) needs.</S>
			<S sid ="48" ssid = "34">Further, the formalism should have a model theoretic backbone, to ensure that the semantic representa tions one works with actually have an interpretati on, and can consequen tly Figure 3: Encoding local word order.</S>
			<S sid ="49" ssid = "35">x with the surface strings “a blue cup” and “una tazza blu”.</S>
			<S sid ="50" ssid = "36">But the way we express local order is not limited to words and can be employed for partial phrases as well, if one adopts a neoDavidsonian event semantics with explicit thematic roles.</S>
			<S sid ="51" ssid = "37">This can be achieved by using the same kind of numerical indices already used for the alignment of words.</S>
			<S sid ="52" ssid = "38">The example in Figure 4 shows how to represent an event “hit” with its thematic roles, preserving their relative order.</S>
			<S sid ="53" ssid = "39">We call surface forms “partial” or “incomplete” when they contain variables, and “complete” when they only contain tokens.</S>
			<S sid ="54" ssid = "40">The corresponding partial surface form is then “y hit z”, where y and z are place- holders for surface strings.</S>
			<S sid ="55" ssid = "41">agent y 1 hit#v#1 &quot;hit&quot; 2 x 3 theme z Figure 4: Graph for a neoDavidsonian structure.</S>
			<S sid ="56" ssid = "42">This is the basic idea of aligning surface strings with parts of a deep semantic representation.</S>
			<S sid ="57" ssid = "43">Note that precise alignment is only possible for words with a lexical semantics that include first-order variables.</S>
			<S sid ="58" ssid = "44">For words that introduce scope operators (negation particles, coordinating conjuncts) we can’t have the cake and eat it: specifying the local order with respect to an entity or event variable directly and at the same time associating it with an operator isn’t always possible.</S>
			<S sid ="59" ssid = "45">To solve this we introduce surface tuples that complement a semantic representation to facilitate perfect alignment.</S>
			<S sid ="60" ssid = "46">We will explain this in more detail in the following sections.</S>
	</SECTION>
	<SECTION title="Discourse Representation Graphs. " number = "3">
			<S sid ="61" ssid = "1">The choice of semantic formalism should ideally be independent from the application of natural language generation itself, to avoid bias and spe be used in inference tasks using, for instance, automated deduction for first-order logic.</S>
			<S sid ="62" ssid = "2">Given these criteria, a good candidate is Discourse Representation Theory, DRT (Kamp and Reyle, 1993), that captures the meaning of texts in the form of Discourse Representation Structures (DRSs).</S>
			<S sid ="63" ssid = "3">DRSs are capable of effectively representing the meaning of natural language, covering many linguistic phenomena including pronouns, quantifier scope, negation, modals, and presuppositions.</S>
			<S sid ="64" ssid = "4">DRSs are recursive structures put together by logical and non-logical symbols, as in predicate logic, and in fact can be translated into first-order logic formulas (Muskens, 1996).</S>
			<S sid ="65" ssid = "5">The way DRSs are nested inside each other give DRT the ability to explain the behaviour of pronouns and presuppositions (Van der Sandt, 1992).</S>
			<S sid ="66" ssid = "6">Aligning DRSs with texts with fine granularity is hard because words and phrases introduce different kinds of semantic objects in a DRS: discourse referents, predicates, relations, but also logical operators such as negation, disjunction and implication that introduce embedded DRSs.</S>
			<S sid ="67" ssid = "7">A precise alignment of a DRS with its text on the level of words is therefore a nontrivial task.</S>
			<S sid ="68" ssid = "8">To overcome this issue, we apply the idea presented in the previous section to DRSs, making all recursion implicit by representing them as directed graphs.</S>
			<S sid ="69" ssid = "9">We call a graph representing a DRS a Discourse Representation Graph (DRG, in short).</S>
			<S sid ="70" ssid = "10">DRGs encode the same information as DRSs, but are expressed as a set of tuples.</S>
			<S sid ="71" ssid = "11">Essentially, this is done by reification over DRSs — every DRSs gets a unique label, and the arity of DRS conditions increases by one for accommodating a DRS label.</S>
			<S sid ="72" ssid = "12">This allows us to reformulate a DRS as a set of tuples.</S>
			<S sid ="73" ssid = "13">A DRS is an ordered pair of discourse refer- ents (variables over entities) and DRS-conditions.</S>
			<S sid ="74" ssid = "14">DRS-conditions are basic (representing properties or relations) or complex (to handle negation and disjunction).</S>
			<S sid ="75" ssid = "15">To reflect these different constructs, we distinguish three types of tuples in DRGs: • (K,referent,X) means that X is a discourse referent in K (referent tuples); • (K,condition,C) means that C is a condition k1 unary scope k2 k2 referent e1 k2 referent x1 k2 event pay k2 concept customer k1 unary scope k2 event role referent pay agent referent instance e1 internal external k2 role agent concept x1 instance customer instance x1 pay instance e1 agent internal e1 agent external x1 customer Figure 5: DRS and corresponding DRG (in tuples and in graph format) for “A customer did not pay.” in K (condition tuples), with various sub- types: concept, event, relation, role, named, cardinality, attribute, unary, and binary; • (C,argument,A) means that C is a condition with argument A (argument tuples), with the sub-types internal, external, instance, scope, antecedent, and consequence.</S>
			<S sid ="76" ssid = "16">With the help of a concrete example, it is easy to see that DRGs have the same expressive power as DRSs.</S>
			<S sid ="77" ssid = "17">Consider for instance a DRS with negation, before and after labelling it (Figure 6): Note that labelling conditions is crucial to distinguish between syntactically equivalent conditions occurring in different (embedded) DRSs.</S>
			<S sid ="78" ssid = "18">Unlike Power’s scoped semantic network for DRSs, we don’t make the assumption that conditions appear in the DRS in which their discourse referents are introduced (Power, 1999).</S>
			<S sid ="79" ssid = "19">The example in Figure 6 illustrates that this assumption is not sound: the condition p(x) is in a different DRS than where its discourse referent x is introduced.</S>
			<S sid ="80" ssid = "20">Further note that our reification procedure yields “flatter” representations than similar x y r(x,y) z K1 : x y c1 :r(x,y) z formalisms (Copestake et al., 1995; Reyle, 1993), and this makes it more convenient to align surface strings with DRSs with a high granularity, as we p(x) s(z,y) c2 :K2 : c3 :p(x) c4 :s(z,y) will show below.</S>
	</SECTION>
	<SECTION title="Word-. " number = "4">
			<S sid ="81" ssid = "1">Aligned DRGs Figure 6: From DRS to DRG: labelling.</S>
			<S sid ="82" ssid = "2">Now, from the labelled DRS we can derive the following three referent tuples: (K1,referent,x), (K1,referent,y), and (K2,referent,z); the following four condition tuples: (K1,relation,c1:r), (K1,unary,c2:), (K2,concept,c3:p), and (K2,relation,c4:s); and the following argument tuples: (c1:r,internal,x), (c1:r,external,y), (c2:,scope,K2), (c3:p,instance,x), (c4:s,internal,z), and (c4:s,external,y).</S>
			<S sid ="83" ssid = "3">From these tuples, it is straightforward to recreate a labelled DRS, and by dropping the labels subsequently, the original DRS resurfaces again.</S>
			<S sid ="84" ssid = "4">For the sake of readability we sometimes leave out labels in examples throughout this paper.</S>
			<S sid ="85" ssid = "5">In addition, we also show DRGs in graph-like pictures, where the tuples that form a DRG are the edges, and word-alignment information attached at the tuple level is shown as labels on the graph edges, as in Figure 9.</S>
			<S sid ="86" ssid = "6">In such graphs, nodes representing discourse referents are square shaped, and nodes representing conditions are oval shaped.</S>
			<S sid ="87" ssid = "7">In this section we show how the alignment between surface text and its logical representation is realized by adding information of the tuples that make up a DRG.</S>
			<S sid ="88" ssid = "8">This sounds more straightforward than it is. For some word classes this is indeed easy to do.</S>
			<S sid ="89" ssid = "9">For others we need additional machinery in the formalism.</S>
			<S sid ="90" ssid = "10">Let’s start with the straightforward cases.</S>
			<S sid ="91" ssid = "11">Determiners are usually associated with referent tuples.</S>
			<S sid ="92" ssid = "12">Content words, such as nouns, verbs, adverbs and adjectives, are typically directly associated with one-place relation symbols, and can be naturally aligned with argument tuples.</S>
			<S sid ="93" ssid = "13">Verbs are assigned to instance tuples linking its event condition; likewise, nouns are typically aligned to instance tuples which link discourse referents to the concepts they express; adjectives are aligned to tuples of attribute conditions.</S>
			<S sid ="94" ssid = "14">Finally, words expressing relations (such as prepositions), are attached to the external argument tuple linking the relation to the discourse referent playing the role of external argument.</S>
			<S sid ="95" ssid = "15">Although the strategy presented for DRG–text alignment is intuitive and straightforward to implement, there are surface strings that don’t correspond to something explicit in the DRS.</S>
			<S sid ="96" ssid = "16">To this class belong punctuation symbols, and semantically empty words such as (in English) the infinitival particle, pleonastic pronouns, auxiliaries, there insertion, and so on.</S>
			<S sid ="97" ssid = "17">Furthermore, function words such as “not”, “if”, and “or”, introduce semantic material, but for the sake of surface string generation could be better aligned with the event that they take the scope of.</S>
			<S sid ="98" ssid = "18">To deal with all these cases, we extend DRGs with surface tuples of the form (K,surface,X), whose edges are decorated with therequired surface strings.</S>
			<S sid ="99" ssid = "19">Figure 7 shows an exam ple of a DRG extended with such surface tuples.</S>
			<S sid ="100" ssid = "20">k1 unary scope k2 k2 referent e1 k2 referent x1 k2 event pay k2 concept customer k2 role agent customer instance x1 p a y i n s t a n c e e 1 a g e n t i n t e r n a l e 1 a g e n t e x t e r n a l x 1 1 2 4 1 A cu st o m er pa y k2 surface e1 k2 surface e1 k2 surface e1 2 3 5 d i d n o t . Figure 7: Word-aligned DRG for “A customer did not pay.” All alignment information (including surface tuples) is highlighted.</S>
			<S sid ="101" ssid = "21">Note that surface tuples don’t have any influence on the meaning of the original DRS – they just serve for the purpose of alignment of the required surface strings.</S>
			<S sid ="102" ssid = "22">Also note in Figure 7 the indices that were added to some tuples.</S>
			<S sid ="103" ssid = "23">They serve to express the local order of surface information.</S>
			<S sid ="104" ssid = "24">Following the idea sketched in Section 2, the total order of words is transformed into a local ranking of edges relative to discourse referents.</S>
			<S sid ="105" ssid = "25">This is possible because the tuples that have word tokens aligned to them always have a discourse referent as third element (the head of the directed edge, in terms of graphs).</S>
			<S sid ="106" ssid = "26">We group tuples that share the same discourse referent and then assign indices reflecting the relative order of how these tuples are realized in the original text.</S>
			<S sid ="107" ssid = "27">Illustrating this with our example in Figure 7, we got two discourse referents: x1 and e1.</S>
			<S sid ="108" ssid = "28">The discourse referent x1 is associated with three tuples, of which two are indexed (with indices 1 and 2).</S>
			<S sid ="109" ssid = "29">Generating the surface string for x1 succeeds by traversing the edges in the order specified, resulting in [A,customer] for x1.</S>
			<S sid ="110" ssid = "30">The referent e1 associates with six tuples, of which four are indexed (with indices 1–4).</S>
			<S sid ="111" ssid = "31">The order of these tuples would yield the partial surface string [x1,did,not,pay,.]</S>
			<S sid ="112" ssid = "32">for e1.</S>
			<S sid ="113" ssid = "33">Note that the manner in which DRSs are constructed during analysis ensures that all discourse referents are linked to each other by taking the transitive closure of all binary relations appearing in a DRS, and therefore we can reconstruct the total order from composing the local orders.</S>
			<S sid ="114" ssid = "34">In the next section we explain how this is done.</S>
	</SECTION>
	<SECTION title="Surface Composition. " number = "5">
			<S sid ="115" ssid = "1">In this section we show in detail how surface strings can be generated from word-aligned DRGs.</S>
			<S sid ="116" ssid = "2">It consists of two subsequent steps.</S>
			<S sid ="117" ssid = "3">First, a surface form is associated with each discourse referent.</S>
			<S sid ="118" ssid = "4">Secondly, surface forms are put together in a bottom-up fashion, to generate the complete output.</S>
			<S sid ="119" ssid = "5">During the composition, all of the discourse referents are associated with their own surface representation.</S>
			<S sid ="120" ssid = "6">The surface form associated with the discourse unit that contains all other discourse units is then the text aligned with the original DRG.</S>
			<S sid ="121" ssid = "7">Surface forms of discourse referents are lists of tokens and other discourse referents.</S>
			<S sid ="122" ssid = "8">Recall that the order of the elements of a discourse referent’s surface form is reflected by the local ordering of tuples, as explained in the previous section, and tuples with no index are simply ignored when reconstructing surface strings.</S>
			<S sid ="123" ssid = "9">The surface form is composed by taking each tuple belonging to a specific discourse referent, in the correct order, and adding the tokens aligned with the tuple to a list representing the surface string for that discourse referent.</S>
			<S sid ="124" ssid = "10">An important part of this process is that binary DRS relations, represented in the DRG by a pair of internal and external argument tuple, are followed unidirectionally: if the tuple is of the internal type, then the discourse referent on the other end of the relation (i.e. following its external tuple edge) is added to the list.</S>
			<S sid ="125" ssid = "11">Surface forms for embedded DRSs include the discourse referents of the events x1 : Michelle e1 : x1 thinks p1 e1 : Michelle thinks p1 p1 : that e2 x2 : Obama e2 : x2 smokes . e2 : Obama smokes . p1 : that Obama smokes . k1 : e4 e 1 : M i c h e l l e t h i n k s t h a t O b a m a s m o k e s . k 1 : M i c h e l l e t h i n k s t h a t O b a m a s m o k e s . F i g u r e 8 : S u r f a c e c o m p o s i t i o n o f e m b e d d e d s t r u c t u r e s . they contain.</S>
			<S sid ="126" ssid = "12">Typically, discourse units contain exactly one event (the main event of the clause).</S>
			<S sid ="127" ssid = "13">Phenomena such as gerunds (e.g. “the laughing girl”) and relative clauses (e.g. “the man who smokes”) may introduce more than one event in a discourse unit.</S>
			<S sid ="128" ssid = "14">To ensure correct order and grouping, we borrow a technique from description logic (Horrocks and Sattler, 1999) and invert roles in DRGs.</S>
			<S sid ="129" ssid = "15">Rather than representing “the laughing girl” as [girl(x) ∧ agent(e,x) ∧ laugh(e)], we represent it as [girl(x) ∧ agent−1(x,e) ∧ laugh(e)], making use of R(x,y)≡ R−1(y,x) to preserve meaning.</S>
			<S sid ="130" ssid = "16">This “trick” en sures that we can describe the local order of noun phrases with relative clauses and alike.</S>
			<S sid ="131" ssid = "17">To wrap things up, the composition operation is used to derive complete surface forms for DRGs.</S>
			<S sid ="132" ssid = "18">Composition puts together two surface forms, where one of them is complete, and one of them is incomplete.</S>
			<S sid ="133" ssid = "19">It is formally defined as follows:</S>
	</SECTION>
	<SECTION title="Selected. " number = "6">
			<S sid ="134" ssid = "1">Phenomena We implemented a first prototype using our alignment and realization method and tested it on examples taken from the Groningen Meaning Bank, a large annotated corpus of texts paired with DRSs (Basile et al., 2012).</S>
			<S sid ="135" ssid = "2">Naturally, we came across phenomena that are notoriously hard to analyze.</S>
			<S sid ="136" ssid = "3">Most of these we can handle adequately, but some we can’t currently account for and require further work.</S>
			<S sid ="137" ssid = "4">6.1 Embedded.</S>
			<S sid ="138" ssid = "5">Clauses In the variant of DRT that we are using, propositional arguments of verbs introduce embedded DRSs associated with a discourse referent.</S>
			<S sid ="139" ssid = "6">This is a good test for our surface realization formalism, because it would show that it is capable of recursively generating embedded clauses.</S>
			<S sid ="140" ssid = "7">Figure 9 shows the DRG for the sentence “Michelle thinks that Obama smokes.” ρ1 : τ ρ2 : Λ1ρ1Λ2 ρ2 : Λ1τ Λ2 (1) referent michelle instance &quot;Michelle&quot; 1 x1 ext where ρ1 and ρ2 are discourse referents, τ is a list of tokens, and Λ1 and Λ2 are lists of word tokens and discourse referents.</S>
			<S sid ="141" ssid = "8">In the example from Figure 7, the complete surface form for the discourse unit k1 is derived by means of composition as formulated in (1) as follows: x1 : A customer e1 : x1 did not pay named role k1 event role Agent referent think Theme referent &quot;that&quot; subordinates:prop int 1 instance e1 &quot;thinks&quot; 2 3 int ext 1 p1 role named referent Patient obama referent ext x2 int 1 instance &quot;Obama&quot; 1 k2 : e1 e1 : A custom er did not pay . k2 : A customer did not pay . event punctuation &quot;.&quot; 3 e2 2 instance &quot;smokes&quot; The procedure for generation described here is reminiscent of the work of (Shieber, 1988) who also employs a deductive approach.</S>
			<S sid ="142" ssid = "9">In particular our composition operation can be seen as a simplified completion.</S>
			<S sid ="143" ssid = "10">Going back to the example in Section 4, substituting the value of x1 in the incomplete surface form of e1 produces the surface string [A,customer,did,not,pay,.]</S>
			<S sid ="144" ssid = "11">for e1.</S>
			<S sid ="145" ssid = "12">smoke Figure 9: Word-aligned DRG for the sentence “Michelle thinks that Obama smokes.” Here the surface forms of two discourse units (main and embedded) are generated.</S>
			<S sid ="146" ssid = "13">In order to generate the complete surface form, first the embedded clause is generated, and then composed with the incomplete surface form of the main clause.</S>
			<S sid ="147" ssid = "14">As noted earlier, during the composition process, the complete surface form for each discourse referent is generated (Figure 8), showing a clear alignment between the entities of the semantic representation and the surface forms they represent.</S>
			<S sid ="148" ssid = "15">6.2 Coordination.</S>
			<S sid ="149" ssid = "16">Coordination is another good test case for a linguistic formalism.</S>
			<S sid ="150" ssid = "17">Consider for instance “Subsistence fishing and commercial trawling occur within refuge waters”, where two noun phrases are coordinated, giving rise to either a distributive (introducing two events in the DRS) or a collective interpretation (introducing a set formation of discourse referents in the DRS).</S>
			<S sid ="151" ssid = "18">We can account for both interpretations (Figure 10).</S>
			<S sid ="152" ssid = "19">Note that, interestingly, using the distributive interpretation DRG as input to the surface realization component could result, depending on how words are aligned, in a surface form “fishing occurs and trawling occurs”, or as “fishing and trawling occur”.</S>
			<S sid ="153" ssid = "20">6.3 Long-Distance Dependencies.</S>
			<S sid ="154" ssid = "21">Cases of extraction, for instance with WH- movement, could be problematic to capture with our formalism.</S>
			<S sid ="155" ssid = "22">This is in particular an issue when extraction crosses more than one clause boundary, as in “Which car does Bill believe John bought”.</S>
			<S sid ="156" ssid = "23">Even though these cases are rare in the real world, a complete formalism for surface realization must be able to deal with such cases.</S>
			<S sid ="157" ssid = "24">The question is whether this is a separate generation task in the domain of syntax (White et al., 2007), or whether the current formalism needs to be adapted to cover such long-distance dependencies.</S>
			<S sid ="158" ssid = "25">Another range of complications are caused by discontinuous constituents, as in the Dutch sentence “Ik heb kaartjes gekocht voor Berlijn” (literally: “I have tickets bought for Berlin”), where the prepositional phrase “voor Berlijn” is an argument of the noun phrase “kaartjes”.</S>
			<S sid ="159" ssid = "26">In our formalism the only alignment possible would result in the sentence “Ik heb kaartjes voor Berlijn gekocht”, which is arguably a more fluent realization of the sentence, but doesn’t correspond to the original text.</S>
			<S sid ="160" ssid = "27">If one uses the original text as gold standard, this could cause problems in evaluation.</S>
			<S sid ="161" ssid = "28">(One could also benefit from this deficiency, and use it to generate more than one gold standard surface string.</S>
			<S sid ="162" ssid = "29">This is something to explore in future work.)</S>
			<S sid ="163" ssid = "30">6.4 Control Verbs.</S>
			<S sid ="164" ssid = "31">In constructions like “John wants to swim”, the control verb “wants” associates its own subject with the subject of the infinitival clause that it has as argument.</S>
			<S sid ="165" ssid = "32">Semantically, this is realized by variable binding.</S>
			<S sid ="166" ssid = "33">Generating an appropriate surface form for semantic representation with controlled variables is a challenge: a naive approach would generate “John wants John to swim”.</S>
			<S sid ="167" ssid = "34">One possible solution is to add another derivation rule for surface composition dedicated to deal with cases where a placeholder variable occurs in more than one partial surface form, substituting a null string for a variable following some heuristic rules.</S>
			<S sid ="168" ssid = "35">A second, perhaps more elegant solution is to integrate a language model into the surface composition process.</S>
	</SECTION>
	<SECTION title="Related work. " number = "7">
			<S sid ="169" ssid = "1">Over the years, several systems have emerged that aim at generate surface forms from different kind of abstract input representations.</S>
			<S sid ="170" ssid = "2">An overview of the state-of-the-art is showcased by the submissions to the Surface Realization Shared Task (Belz et al., 2012).</S>
			<S sid ="171" ssid = "3">Bohnet et al.</S>
			<S sid ="172" ssid = "4">(2010), for instance, employ deep structures derived from the CoNLL 2009 shared task, essentially sentences annotated with shallow semantics, lemmata and dependency trees; as the authors state, these annotations are not made with generation in mind, and they necessitate complex preprocessing steps in order to derive syntactic trees, and ultimately surface forms.</S>
			<S sid ="173" ssid = "5">The format presented in this work has been especially developed with statistical approaches in mind.</S>
			<S sid ="174" ssid = "6">Nonetheless, there is very little work on robust, wide-scale generation from DRSs, surprisingly perhaps given the large body of theoretical research carried out in the framework of Discourse Representation Theory, and practical implementations and annotated corpora of DRSs that are nowadays available (Basile et al., 2012).</S>
			<S sid ="175" ssid = "7">This is in contrast to the NLG work in the framework of Lexical Functional Grammar (Guo et al., 2011).</S>
			<S sid ="176" ssid = "8">Flat representation of semantic representations, like the DRGs that we present, have also been put forward to facilitate machine translation (Schiehlen et al., 2000) and for evaluation purposes (Allen et al., 2008), and semantic parsing event occur referent instance &quot;occur&quot; 2 e1 referent occur instance &quot;occur&quot; 2 e1 internal 1 event internal 1 role concept theme fishing referent external instance &quot;fishing&quot; 1 x1 role theme referent surface external k1 event occur instance &quot;and&quot; 2 x3 int er na l 1 3 referent surface &quot;occur&quot; 3 e2 k1 relation concept relation superset fishing external internal instance role &quot;and&quot; 1 2 in te rn al superset referent &quot;fishing&quot; 1 x1 external concept theme external trawling referent instance &quot;trawling&quot; 1 x2 concept referent trawling x2 instance 1 &quot;trawling&quot; Figure 10: Analysis of NP coordination, in a distributive (left) and a collective interpretation (right).</S>
			<S sid ="177" ssid = "9">(Le and Zuidema, 2012) just because they’re easier and more efficient to process.</S>
			<S sid ="178" ssid = "10">Packed semantic representations (leaving scope underspecified) also resemble flat representations (Copestake et al., 1995; Reyle, 1993) and can be viewed as graphs, however they show less elaborated reification than the DRGs presented in this paper, and are therefore less suitable for precise alignment with surface strings.</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "8">
			<S sid ="179" ssid = "1">We presented a formalism to align logical forms, in particular Discourse Representation Structures, with surface text strings.</S>
			<S sid ="180" ssid = "2">The resulting graph representations (DRGs), make recursion implicit by reification over nested DRSs.</S>
			<S sid ="181" ssid = "3">Because of their “flat” structure, DRGs can be precisely aligned with the text they represent at the word level.</S>
			<S sid ="182" ssid = "4">This is key to open-domain statistical Surface Realization, where words are learned from abstract, syntactic or semantic, representations, but also useful for other applications such as learning semantic representations directly from text (Le and Zuidema, 2012).</S>
			<S sid ="183" ssid = "5">The actual alignment between the tuples that form a DRG and the surface forms they represent is not trivial, and requires to make several choices.</S>
			<S sid ="184" ssid = "6">Given the alignment with text, we show that it is possible to directly generate surface forms from automatically generated word-aligned DRGs.</S>
			<S sid ="185" ssid = "7">To do so, a declarative procedure is presented, that generates complete surface forms from aligned DRGs in a compositional fashion.</S>
			<S sid ="186" ssid = "8">The method works in a bottom-up way, using discourse ref- erents as starting points, then generating a surface form for each of them, and finally composing all of the surface form together into a complete text.</S>
			<S sid ="187" ssid = "9">We are currently building a large corpus of word-aligned DRSs, and are investigating machine learning methods that could automatically learn the alignments.</S>
			<S sid ="188" ssid = "10">Surprisingly, given that DRT is one of the best studied formalisms in formal semantics, there isn’t much work on generation from DRSs so far.</S>
			<S sid ="189" ssid = "11">The contribution of this paper presents a method to align DRSs with surface strings, that paves the way for robust, statistical methods for surface generation from deep semantic representations.</S>
	</SECTION>
</PAPER>
