Reference Article:  D10-1083.txt | Citing Article:  D11-1056.txt | Citation Marker Offset:  ['264'] | Citation Marker:  2010 | Citation Offset:  ['263','264'] | Citation Text:  <S sid ="263" ssid = "44">Following Lee et al.</S><S sid ="264" ssid = "45">(2010), we report the best and median settings of hyperparameters based on the F- score, in addition to inferred values.</S> | Reference Offset:  ['99'] | Reference Text:  <S sid ="99" ssid = "4">TheFigure 2: Graph of the one-to-one accuracy of our full model (+FEATS) under the best hyperparameter setting by iteration</S> | Discourse Facet:  Results_Citation | Annotator:  Kokil Jaidka |


Reference Article:  D10-1083.txt | Citing Article:  D11-1059.txt | Citation Marker Offset:  ['5'] | Citation Marker:  Lee et al., 2010 | Citation Offset:  ['5'] | Citation Text:  <S sid ="5" ssid = "5">However, despite a recent proliferation of syntactic class induction systems (Biemann, 2006; Goldwater and Griffiths, 2007; Johnson, 2007; Ravi and Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010) | Reference Offset:  ['239'] | Reference Text:  <S sid ="239" ssid = "100">Our empirical results demonstrate that the type-based tagger rivals state-of-the-art tag-level taggers which employ more sophisticated learning mechanisms to exploit similar constraints.</S> | Discourse Facet:  Results_Citation | Annotator:  Kokil Jaidka |


Reference Article:  D10-1083.txt | Citing Article:  D11-1059.txt | Citation Marker Offset:  ['11'] | Citation Marker:  2010 | Citation Offset:  ['10','11'] | Citation Text:  <S sid ="10" ssid = "10">This property is not strictly true of linguistic data, but is a good approximation: as Lee et al.</S><S sid ="11" ssid = "11">(2010) note, assigning each word type to its most frequent part of speech yields an upper bound accuracy of 93% or more for most languages.</S> | Reference Offset:  ['22'] | Reference Text:  <S sid ="22" ssid = "22">In this way we restrict the parameterization of a Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound on tagging accuracy assuming each word type is assigned to majority POS tag.</S> | Discourse Facet:  Results_Citation | Annotator:  Kokil Jaidka |


Reference Article:  D10-1083.txt | Citing Article:  D11-1059.txt | Citation Marker Offset:  ['17'] | Citation Marker:  2010 | Citation Offset:  ['16','17'] | Citation Text:  <S sid ="16" ssid = "16">More recently, Lee et al.</S><S sid ="17" ssid = "17">(2010) presented a new type-based model, and also reported very good results.</S> | Reference Offset:  ['239'] | Reference Text:  <S sid ="239" ssid = "100">Our empirical results demonstrate that the type-based tagger rivals state-of-the-art tag-level taggers which employ more sophisticated learning mechanisms to exploit similar constraints.</S> | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka |


Reference Article:  D10-1083.txt | Citing Article:  D11-1059.txt | Citation Marker Offset:  ['22'] | Citation Marker:  Lee et al., 2010 | Citation Offset:  ['22'] | Citation Text:  <S sid ="22" ssid = "22">Sequence models are by far the most common method of supervised part- of-speech tagging, and have also been widely used for unsupervised part-of-speech tagging both with and without a dictionary (Smith and Eisner, 2005; Haghighi and Klein, 2006; Goldwater and Griffiths, 2007; Johnson, 2007; Ravi and Knight, 2009; Lee et al., 2010).</S> | Reference Offset:  ['52'] | Reference Text:  <S sid ="52" ssid = "1">We consider the unsupervised POS induction problem without the use of a tagging dictionary.</S> | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka |


Reference Article:  D10-1083.txt | Citing Article:  D11-1059.txt | Citation Marker Offset:  ['32'] | Citation Marker:  Lee et al., 2010 | Citation Offset:  ['32'] | Citation Text:  <S sid ="32" ssid = "32">As in previous work (Lee et al., 2010), we find that the one-class-per-type restriction boosts performance considerably over a comparable token- based model and yields results that are comparable to state-of-the-art even without the use of morphology or alignment features.</S> | Reference Offset:  ['52'] | Reference Text:  <S sid ="52" ssid = "1">We consider the unsupervised POS induction problem without the use of a tagging dictionary.</S> | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka |


Reference Article:  D10-1083.txt | Citing Article:  D11-1059.txt | Citation Marker Offset:  ['91'] | Citation Marker:  2010 | Citation Offset:  ['90','91'] | Citation Text:  <S sid ="90" ssid = "57">2 One could approximate this likelihood term by assuming independence between all nj feature tokens of word type j. This is the approach taken by Lee et al.</S><S sid ="91" ssid = "58">(2010).</S> | Reference Offset:  ['112'] | Reference Text:  <S sid ="112" ssid = "3">For all languages we do not make use of a tagging dictionary.</S> | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka |


Reference Article:  D10-1083.txt | Citing Article:  D11-1059.txt | Citation Marker Offset:  ['130'] | Citation Marker:  2010 | Citation Offset:  ['129','130'] | Citation Text:  <S sid ="129" ssid = "35">Following Lee et al.</S><S sid ="130" ssid = "36">(2010) we used only the training sections for each language.</S> | Reference Offset:  ['6'] | Reference Text:  <S sid ="6" ssid = "6">Our experiments consistently demonstrate that this model architecture yields substantial performance gains over more complex tagging counterparts.</S> | Discourse Facet:  Results_Citation | Annotator:  Kokil Jaidka |


Reference Article:  D10-1083.txt | Citing Article:  D12-1086.txt | Citation Marker Offset:  ['74'] | Citation Marker:  Lee et al., 2010 | Citation Offset:  ['74'] | Citation Text:  <S sid ="74" ssid = "33">Given that close to 95% of the word occurrences in human labeled data are tagged with their most frequent part of speech (Lee et al., 2010) | Reference Offset:  ['9'] | Reference Text:  <S sid ="9" ssid = "9">Simply assigning to each word its most frequent associated tag in a corpus achieves 94.6% accuracy on the WSJ portion of the Penn Treebank.</S> | Discourse Facet:  Results_Citation | Annotator:  Kokil Jaidka |


Reference Article:  D10-1083.txt | Citing Article:  D12-1125.txt | Citation Marker Offset:  ['213'] | Citation Marker:  Lee et al., 2010 | Citation Offset:  ['213'] | Citation Text:  <S sid ="213" ssid = "29">vised POS induction algorithm (Lee et al., 2010)</S> | Reference Offset:  ['52'] | Reference Text:  <S sid ="52" ssid = "1">We consider the unsupervised POS induction problem without the use of a tagging dictionary.</S> | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka |


Reference Article:  D10-1083.txt | Citing Article:  D12-1127.txt | Citation Marker Offset:  ['13'] | Citation Marker:  Lee et al., 2010 | Citation Offset:  ['13'] | Citation Text:  <S sid ="13" ssid = "13">Unsupervised induction of POS taggers offers the possibility of avoiding costly annotation, but despite recent progress, the accuracy of unsupervised POS taggers still falls far behind supervised systems, and is not suitable for most applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).</S> | Reference Offset:  ['52'] | Reference Text:  <S sid ="52" ssid = "1">We consider the unsupervised POS induction problem without the use of a tagging dictionary.</S> | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka |


Reference Article:  D10-1083.txt | Citing Article:  D13-1004.txt | Citation Marker Offset:  ['27'] | Citation Marker:  Lee et al., 2010 | Citation Offset:  ['27'] | Citation Text:  <S sid ="27" ssid = "27">Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)</S> | Reference Offset:  ['243'] | Reference Text:  <S sid ="243" ssid = "104">We hypothesize that modeling morphological information will greatly constrain the set of possible tags, thereby further refining the representation of the tag lexicon.</S> | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka |


Reference Article:  D10-1083.txt | Citing Article:  N12-1045.txt | Citation Marker Offset:  ['14'] | Citation Marker:  Lee et al., 2010 | Citation Offset:  ['14'] | Citation Text:  <S sid ="14" ssid = "14">Several unsupervised POS induction systems make use of morphological features (Blunsom and Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011) | Reference Offset:  ['243'] | Reference Text: <S sid ="243" ssid = "104">We hypothesize that modeling morphological information will greatly constrain the set of possible tags, thereby further refining the representation of the tag lexicon.</S> | Discourse Facet:  Aim_Citation | Annotator:  Kokil Jaidka |


Reference Article:  D10-1083.txt | Citing Article:  P11-1087.txt | Citation Marker Offset:  ['41'] | Citation Marker:  2010 | Citation Offset:  ['40','41','42','43'] | Citation Text:  <S sid ="40" ssid = "20">Recently Lee et al.</S><S sid ="41" ssid = "21">(2010) combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity.</S><S sid ="42" ssid = "22">However this work approximated the derivation of the Gibbs sampler (omitting the interdependence between events when sampling from a collapsed model), resulting in a model which underperformed Brown et al.</S><S sid ="43" ssid = "23">(1992)â€™s one-class HMM.</S> | Reference Offset:  ['27','85','97'] | Reference Text:  <S sid ="27" ssid = "27">First, it directly encodes linguistic intuitions about POS tag assignments: the model structure reflects the one-tag-per-word property, and a type- level tag prior captures the skew on tag assignments (e.g., there are fewer unique determiners than unique nouns).</S><S sid ="85" ssid = "34">Learned Tag Prior (PRIOR) We next assume there exists a single prior distribution ψ over tag assignments drawn from DIRICHLET(β, K ).</S><S sid ="97" ssid = "2">During training, we treat as observed the language word types W as well as the token-level corpus w. We utilize Gibbs sampling to approximate our collapsed model posterior:</S> | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka |


Reference Article:  D10-1083.txt | Citing Article:  P11-1087.txt | Citation Marker Offset:  ['153'] | Citation Marker:  2010 | Citation Offset:  ['152','153','154'] | Citation Text:  <S sid ="152" ssid = "33">It is also interesting to compare the bigram PYP1HMM to the closely related model of Lee et al.</S><S sid ="153" ssid = "34">(2010).</S><S sid ="154" ssid = "35">That model incorrectly assumed independence of the conditional sampling distributions, resulting in a accuracy of 66.4%</S> | Reference Offset:  ['155'] | Reference Text: <S sid ="155" ssid = "16">5 60.6 Table 3: Multilingual Results: We report token-level one-to-one and many-to-one accuracy on a variety of languages under several experimental settings (Section 5).</S> | Discourse Facet:  Results_Citation | Annotator:  Kokil Jaidka |


Reference Article:  D10-1083.txt | Citing Article:  P13-1150.txt | Citation Marker Offset:  ['55'] | Citation Marker:  Lee et al., 2010 | Citation Offset:  ['55'] | Citation Text:  <S sid ="55" ssid = "27">Similar constraints have been developed for part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011) | Reference Offset:  ['236'] | Reference Text:  <S sid ="236" ssid = "97">We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model.</S> | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka |


Reference Article:  D10-1083.txt | Citing Article:  W11-0301.txt | Citation Marker Offset:  ['102'] | Citation Marker:  Lee et al., 2010 | Citation Offset:  ['102'] | Citation Text:  <S sid ="102" ssid = "55">Here, W t refers to the set of word types that are generated by tag t. In other words, conditioned on tag t, we can only generate word w from the set of word types in W t which is generated earlier (Lee et al., 2010).</S> | Reference Offset:  ['20','21'] | Reference Text:  <S sid ="20" ssid = "20">The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word.</S><S sid ="21" ssid = "21">Then, token- level HMM emission parameters are drawn conditioned on these assignments such that each word is only allowed probability mass on a single assigned tag.</S> | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka |


Reference Article:  D10-1083.txt | Citing Article:  W12-1914.txt | Citation Marker Offset:  ['8'] | Citation Marker:  2010 | Citation Offset:  ['7','8'] | Citation Text:  <S sid ="7" ssid = "7">Second, learning categories has been cast as unsupervised part-of-speech tagging task (recent work includes Ravi and Knight (2009), Lee et al.</S><S sid ="8" ssid = "8">(2010), Lamar et al.</S> | Reference Offset:  ['236'] | Reference Text:  <S sid ="236" ssid = "97">We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model.</S> | Discourse Facet:  ['Aim_Citation','Method_Citation'] | Annotator:  Kokil Jaidka |