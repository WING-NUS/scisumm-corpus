Underspecified Modelling  of Complex Discourse Constraints





 Markus  Egg 
egg@let.rug.nl 
University of Groningen


Michaela Regneri 
regneri@coli.uni-
sb.de Saarland University









Abstract


We introduce a new type of discourse con- 
straints for the interaction of discourse re- 
lations with the configuration of discourse 
segments.  We examine corpus-extracted 
examples  as soft constraints. We show how 
to use Regular Tree Gramamrs to process 
such constraints,  and how the representa- 
tion of some constraints depends on the ex- 
pressive power of this formalism.


1   Introduction

Discourse structures cannot always be described 
completely, either because  they are ambiguous 
(Stede, 2004), or because a discourse  parser fails 
to analyse them completely. In either  case, un- 
derspecification  formalisms  (UFs) can be used to 
represent partial information on discourse struc- 
ture. UFs are used in semantics to model structural 
ambiguity without disjunctive enumeration of the 
readings (van Deemter and Peters, 1996).
  Underspecified descriptions of discourse must 
handle two kinds of incomplete information,  on 
the configuration of  discourse   segments  (how 
they combine into larger units), and on the dis- 
course relations that bring about this configura- 
tion:  Our corpus studies on the RST Discourse 
Treebank (Carlson et al., 2002) showed interde- 
pendencies between relations and configuration,  a 
phenomenon first noted by (Corston-Oliver, 1998). 
These interdependencies can be formulated  as con- 
straints that contribute to the disambiguation of un- 
derspecified descriptions of discourse structure.
  E.g., in discourse segments constituted  by the 
relation Condition,  the premiss tends to be a dis-

   Qc  2008.     Licensed under the  Creative Commons 
Attribution-Noncommercial-Share Alike 3.0 Unported li- 
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/). 
Some rights reserved.


course atom (or at least, maximally  short).1  Simi- 
larly, there is evidence for an interdependency con- 
straint for the relation Purpose(1) 2. In most cases, 
Purpose(1)  has a discourse atom as its nucleus.
  The corpus evaluation furthermore  shows that 
those patterns never occur exclusively  but only as 
tendencies.  Realised as soft constraints, such ten- 
dencies can help to sort the set of readings ac- 
cording to the established preferences, which al- 
lows to focus on the best reading  or the n-best 
readings.  This is of high value for an UF-based 
approach to discourse structure, which must cope 
with extremely  high numbers  of readings. To 
model interdependency  constraints,  we will  use 
Regular  Tree Grammars (RTGs) (Comon et al.,
2007). RTGs can straightforwardly  be extended 
to weighted Regular Tree Grammars (wRTGs), 
which can represent both soft and hard constraints.
  Apart from our corpus-extracted  examples, we 
also consider  a hard interdependency constraint 
similar to the Right Frontier Constraint. We show 
that we can integrate this attachment constraint 
with our formalism, and how its representation de- 
pends on the expressiveness of RTGs.

2   Underspecified Discourse Structure
We  describe (partial) information on discourse 
structure by expressions of a suitable  UF, here, 
dominance graphs (Althaus et al., 2003). Consider 
e.g. Fig. 1(a), the dominance graph for (1):

(1)  [C1 I try to read a novel] [C2 if I feel bored] 
[C3 because the TV programs disappoint me] 
[C4 but I can’t concentrate on anything.]

  1 Following  Rhetorical Structure Theory (Mann  and 
Thompson, 1988), most discourse relations have a central nu- 
cleus argument, and a peripheral satellite argument. For Con- 
dition, the premiss is the satellite, the nucleus, the conclusion.
     2 ‘(n)’ as part of a relation name indicates that the nucleus 
is its n-th argument; relations with names without such an 
affix are multinuclear,  i.e., link two segments of equal promi- 
nence. We sometimes omit the numbers where the position of 
the nucleus is clear from the context.



35
Coling 2008: Companion volume – Posters and Demonstrations,  pages 35–38
Manchester, August 2008


(a)



Cause(2) 	Contrast


(b)	(c)	(d)	(e)



Condition(1)


(f)


Condition(1)


Contrast


Contrast



Cause(2)


Condition(1)


2 	4 	6


Cause(2) 	Contrast



C1          1



C2           3



C3         5



Condition(1)
C4        7


C4             
Condition(1)
C3 	C1


C4
Cause(2)



Condition(1)



Contrast


C1
Cause(2) 	C4


C1 	Cause(2)
C2 
	Co
ntrast


C1         C2


C2           C3


C1 	C2   C3 	C4


C2            C3


C3          C4


Figure 1: An underspecified discourse structure and its five configurations


{1-7} → Condition({1}, {3-7}) [1]
{3-5} → 	Cause({3}, {5}) [1]
{5-7} → 	Contrast({5}, {7}) [1]
{3-7} → 	Cause({3}, {5-7}) [1]	{1}   →  C1   [1]   {3}   →  C2 [1]	{5}   →  C3   [1]   {7}   →  C4 [1]
Figure 2: A wRTG modelling  the interdependency constraint for Fig. 1




  Such constraints   describe  a  set  of discourse 
structures  (formalised   as binary tree structures). 
Their key ingredient are (reflexive,  transitive and 
antisymmetric) dominance relations, which are in- 
dicated by dotted lines. Dominance of X1 over X2 
means that X2  is part of the structure below (and 
including) X1, but there might be additional mate- 
rial intervening between X1  and X2.
  Fig. 1(a) states that C1 is linked to a part of the 
following discourse (including at least C2) by Con- 
dition, Cause(2) connects two discourse segments 
(comprising  at least C2 and C3, respectively), and 
Contrast links a discourse  segment to its left (in- 
cluding at least C3) to C4.
  This constraint  describes (is compatible with) 
exactly the five  tree structures in Fig. 1(b-f), if 
described tree structures may only comprise ma- 
terial that is already introduced in the constraint. 
They model the potential discourse structures for 
(1) (see Webber  (2004)). Dominance graphs like 
Fig. 1a. are pure chains.  Pure chains describe all 
binary trees with the same leaf language, here the 
discourse  segments, in their textual order. Pure 
chains define a left-to-right  order, in that not only 
the leaves always form the same sequence, but also 
the inner nodes: If a labelled  node X is further to 
the left in the chain than another node Y, in every 
described tree, X will either be Y’s left child, or Y 
will be X’s right child, or there will be a fragment 
F of which X is a successor on the left and Y is a 
right successor. Henceforth  we will refer to frag- 
ments with their index in the chain (indicated by 
encircled numbers in Fig. 1a).

3   Representing Soft Interdependencies

The interdependency constraint for Condition(1) is 
that its satellite tends to be maximally short, i.e., 
mostly consists of only one discourse atom, and 
in most remaining  cases, of two atoms. Thus, (b)


and (d) are preferred among the configurations  in 
Fig. 1, (c) is less preferred, and (e) and (f) are the 
least preferred. Regular Tree Grammars (RTGs) as 
UF (Koller et al., 2008) can express such complex 
constraints straightforwardly,  and provide a con- 
venient framework  to process them. They allow 
to extract a best configuration  with standard algo- 
rithms very efficiently.
  Koller et al. (2008) show how to generate an 
RTG describing the same set of trees  as a domi- 
nance graph. Similar  to a context free grammar, an 
RTG uses production  rules with terminal symbols 
and nonterminal symbols (NTs), whereby the left- 
hand side (LHS) is always a nonterminal  and the 
right-hand side (RHS) contains at least one termi- 
nal symbol. One NT is the start symbol. A tree 
is accepted by the grammar if the grammar con- 
tains a derivation  for it. An example for an RTG is 
given in Fig. 2, which  describes the same trees as 
the dominance graph in Fig. 1a. The start symbol 
is {1-7}. To derive e.g. the tree in Fig. 1d, we first 
select the rule {1-7} → Cause({1-3}, {5-7}) that 
determines Condition  as root for the whole tree. 
The left child of Condition is then derived from
{1-7}, and the right child from {5-7} respectively. 
To emphasize the association with the dominance 
graph, we mark nonterminals as the subgraphs  they 
represent,  e.g., {1-7}  denotes the whole graph. 
The terminal in the RHS of a grammar rule deter- 
mines the root of the LHS subgraph.
  Koller et al. (2008) also use weighted  RTGs 
(wRTGs, an extension of RTG with weights) to 
express soft dominance constraints (which, unlike 
hard constraints, do not restrict but rather rank the 
set of configurations).   We use wRTGs to model 
the soft interdependency constraints.   The gram- 
mar in Fig. 2 is also a wRTG  that assigns a weight 
to each derived tree: Its weight is the product over 
all weights of all rules used  for the derivation. 
Weights appear in squared brackets after the rules.



The (merely expository) weights in our example 
encode the preference of Condition for a maxi- 
mally short right child: There are three grammar 
rules that establish Condition  as the root of a sub- 
graph (shaded in Fig. 2), which are distinguished 
by the size of the right child of the root (one ({3}), 
three ({3-5}) or five ({3-7}) nodes).  The shorter 
the right child, the higher the weight associated 
with the rule. (1 is a neutral weight by definition.) 
The grammar thus assigns different  weights to the 
trees in Fig. 1; (b) and (d) get the maximum weight 
of 9, (b), a medium weight of 3, and (e) and (f), the 
lowest weight of 1.

4   Expressive Power of RTGs

As Koller et al. (2008) show, the expressive power 
of RTGs is superior to other common underspec- 
ification formalism.  We show an important appli- 
cation of the increased expressiveness with Ex. 2, 
where a. can be continued by b. but not by c:

(2)	a. [C1 Max and Mary are falling apart.] 
[C2 They no longer meet for lunch.] 
[C3 And, last night, Max went to the 
pub] [C4 but Mary visited her parents.]
b. [C5a She complained bitterly  about his 
behaviour.]
c. [C5b He left after his fifth pint of lager.]

  Segment  C5a  continues  the preceding  clause 
about Mary’s  visit with additional information 
about the visit, it thus attaches directly  to C4. To 
find a coherent integration  of C5b, we would have 
to connect it to C3, as  it provides more details 
about Max’ night at the pub. However, in the given 
constellation of C3 and C4, that form a Contrast 
together, C3 is not available any longer for attach- 
ment of further discourse units. (This constraint is 
reminiscent of the Right Frontier Constraint,  as it 
is used by Asher and Lascarides (2003). However, 
it is unclear how the Right Frontier Constraint in 
its exact definition can carry over to binary trees.) 
The given attachment constraint is not express- 
ible with dominance graphs: it excludes the config- 
urations of its dominance graph (Fig. 3) in which 
Contrast shows  up as  a  direct left child, e.g., 
(3b/e/f) as opposed  to (3c/d).  For instance, the 
excluded structure emerges in (3e/f) by choosing 
Cause as root of the the subgraph 5-9 (i.e., includ- 
ing the Contrast-  and Sequence-fragments).  For 
convenience, we will talk about this constraint as
the ”left child constraint” (LCC).


S	→    C ontrast(S, S)    L    →    Evid(S, S)
S	→    Sequ(L, S) 	L    →    List(S, S)
S → L
L → C1     L → C2     L → C3     L → C4     L → C5

Figure 5: A filter RTG corresponding to Ex. 2


  This additional  constraint, however, can be ex- 
pressed  by an RTG like  Fig. 4.   We explicitly 
distinguish  between subgraphs (referred  to with 
numbers) and their associated  NTs here.  Cru- 
cially, some subgraphs  can be processed in dif- 
ferent derivations  here, e.g., {5-9} (as right child 
of List, irrespective of the relative  scope of Ev- 
idence and List), or {3-7} (in the expansions of 
both {EvLiC o} and {LiC oSe},  like in (3c) as 
opposed to (3d)). Sometimes this derivation his- 
tory is irrelevant, like in the case of {5-9} (here, 
only Contrast  may be chosen as root anyway),  but 
there are cases where it matters: If {3-7} is the left 
child of Sequence, as in (3b/d), the choice of Con- 
trast as its root is excluded, since this would make 
Contrast the left child of Sequence,  as in (3b). In 
contrast, {3-7} as the right child of Evidence, like 
in (3c), allows both Contrast and List as root, be- 
cause Contrast  emerges as a right child in either 
case. Thus, the two occurrences of {3-7}  are dis- 
tinguished in terms of different NTs in the gram- 
mar, and only in the NT for the latter occurrence is 
there more than one further expansion rule.
  Regular  tree languages are closed under inter- 
section. Thus, one can derive a grammar like Fig. 4 
by intersecting  a completely  underspecified RTG 
(here, the one derived from Fig. 3a) with a suitable 
filter grammar,  e.g., Fig. 4.  The filter grammar 
produces an infinite  language, containing the frag- 
ments of Fig. 3a and excluding any derivation in 
which Sequence is the direct parent of Contrast. 
This is guaranteed by introducing the nonterminal 
L (the left child NT for Sequence), for which there 
is no derivation with Contrast  as its root.
  For an arbitrary pure chain with n fragments, the 
filter grammar generating the LCC is constructed 
as follows: S is the start symbol. For every frag- 
ment i s.t. 0 < i < n, there is a derivation  rule 
with S as its LHS and i in its RHS, thus either 
S → i, for singleton fragments, or S → i(A, S), 
for binary fragments. If i is binary, we must de- 
termine A: If there is at least one fragment f < i 
s.t. the LCC is assumed for f , we create a new 
NT Li; every derivation rule with i on its RHS fol- 
lows the pattern X  → i(Li, S) (thus A = Li in 
particular). If there is no LCC fragment to the left


(a) Evidence(1)



List



Contrast



Sequence


(b)



Evid


(c)



Sequ


(d) Evid


(e)



List



(f)



Evid


2 	4 	6 	8


C1 	Sequ


Evid	C5


C1 	Sequ


Evid


Sequ	C1


List



1        C1



3       C2



5        C3



7        C4



9     C5



List


Contr	C5
C4


C1
List


Contr
C4


List
C2


C5
Contr


C1           C2
Contr


C2 
	Se
qu
Contr	C5
C5


C2	C3


C2 	C3


C3           C4


C3            C4


C3           C4


Figure 3: An underspecified discourse structure for Ex. 2 and five of its configurations


{Ev
LiC 
oSe
}
→
Ev
id(
{C
1 }, 
{Li
C 
oS
e})
{E
vLi
C 
o}
→
Lis
t({
Ev
}, 
{C 
o})
{E
v}
→
Ev
id(
{C
1 }, 
{C
2 })
{Ev
LiC 
oSe
}
→
Li
st(
{E
v}, 
{C 
oS
e})
{
C 
o
S
e
}
→
C 
ont
({
C3 
}, 
{S
e})
{L
i}
→
Lis
t({
C2 
}, 
{C
3 })
{Ev
LiC 
oSe
}
→
C 
ont
({
Ev
Li
}, 
{S
e})
{
E
v
L
i}
→
Ev
id(
{C
1 }, 
{L
i})
{C 
o}
→
C 
ont
({
C3 
}, 
{C
4 })
{Ev
LiC 
oSe
}
→
Se
qu(
{E
vLi
C 
o}, 
{C
5 })
{
E
v
L
i}
→
Li
st(
{E
v}, 
{C
3 })
{S
e}
→
Se
qu(
{C
4 }, 
{C
5 })
{LiC oSe}	→    Sequ({LiC o}L , {C5 
})	{LiC o}L	→    List({C2 }, {C o})
{L
iC 
oS
e}
→
Lis
t({
C2 
}, 
{C 
oS
e})
{
Li
C 
o}
S
→
C 
ont
({
Li}
, 
{C
4 })
{C1 } → 
C1     {C2 
} → C2
{L
iC 
oS
e}
→
C 
ont
({
Li}
, 
{S
e})
{
Li
C 
o}
S
→
Li(
{Li
}, 
{C
4 })
{C
3 } 
→ 
C3
{
E
vL
iC 
o}
→
Ev
id(
{C
1 }, 
{Li
C 
o}S 
)



{C4 } → 
C4     {C5 
} → C5
Figure 4: A RTG integrating the attachment constraint for Contrast from Ex. 2 into Fig. 3




of i,  A  = S.  If a new NT Li was created, we 
need to create its RHSs: For every fragment h s.t.
0 < h < i and there is no LCC for h, there is a 
rewrite rule directly deriving h from Li.  If h is a 
singleton fragment, the rule is Li → h. Otherwise 
the rule is Li  → h(At, S), whereby At   = S, if 
there is no Lh, or At  = Lh  if there is some LCC 
fragment on the left of h.3
  The grammar in Fig. 4 can be generated with 
that scheme; it has been reduced afterwards in that 
a general rule S → L substitutes for all rules of the 
form S → N T for which there is a corresponding 
rule L → N T (e.g., S → Evid(S, S)).

5   Conclusion

Interdependency constraints that arise from the in- 
teraction of discourse relations and their surround- 
ing structures  are introduced   as a new technique 
for disambiguating  discourse structure. We inte- 
grate those constraints in underspecified discourse 
structures by exploiting the expressive power of 
Regular  Tree Grammars  as UF. As the corpus anal- 
ysis yields in many cases  only soft interdepen- 
dency constraints, we use the weighted  extension 
of RTGs, which allows to sort the readings of an 
underspecified representation and to identify pre- 
ferred discourse structures.  We then showed that 
the representation of some discourse constraints 
depend on the expressive  power of RTGs.  For 
notes on implementation and tractability  of our ap- 
proach, see Regneri  et al. (2008).

  3 To model this as a preference  rather than as a hard con- 
straint, no rules for the L-NTs  are omitted, but rather weighted 
low. An intersection with a preference-neutral wRTG  would 
rank the configurations violating the constraint low, and all 
others with neutral weights.


References
Althaus, Ernst, Denys Duchier, Alexander Koller, 
Kurt Mehlhorn, Joachim Niehren, and Sven 
Thiel. 2003. An efficient graph algorithm for 
dominance  con- straints. Journal of Algorithms, 
48:194–219.
Asher, Nicholas and Alex Lascarides. 2003. Logics 
of
Conversation. Cambridge UP, Cambridge.
Carlson, Lynn,  Daniel  Marcu,  and Mary  
Ellen
Okurowski.  2002. RST Discourse Treebank. 
LDC.
Comon, H.,  M.  Dauchet, R. Gilleron, C. Lo¨ 
ding, F. Jacquemard, D. Lugiez, S. Tison, and 
M. Tom- masi.  2007.  Tree Automata  
Techniques and Ap- plications. Available on: 
http://www.grappa. univ-
lille3.fr/tata. Release 12-10-2007.
Corston-Oliver, Simon H. 1998. Computing 
Represen- tations of Discourse Structure. Ph.D. 
thesis, Dept. of Linguistics, University of 
California, Santa Barbara.
van Deemter, Kees and Stanley Peters, editors. 
1996.
Semantic ambiguity and underspecification.  
CSLI, Stanford.
Koller,  Alexander, Michaela Regneri, and 
Stefan Thater. 2008. Regular  tree grammars  as 
a formal- ism for scope underspecification.  In 
Proceedings of the ACL 08.
Mann, William C. and Sandra A. Thompson. 
1988.
Rhetorical Structure Theory: Toward a 
functional theory of text organization. Text, 
8:243–281.
Regneri, Michaela, Markus Egg, and Alexander 
Koller.
2008. Efficient Processing of Underspecified 
Dis- course Representations.  In Proceedings of 
the ACL
08 (Short Papers).
Stede, Manfred.  2004. The Potsdam Commentary 
Cor- pus. In Webber, Bonnie and Donna K. 
Byron, edi- tors, ACL 2004 Workshop on 
Discourse Annotation.
Webber, Bonnie. 2004. D-LTAG: extending 
lexicalized
TAG to discourse. Cognitive Science, 28:751–
779.

