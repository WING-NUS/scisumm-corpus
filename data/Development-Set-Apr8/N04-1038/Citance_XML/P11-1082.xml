<PAPER>
	<ABSTRACT></ABSTRACT>
	<SECTION title="Introduction. " number = "1">
			<S sid ="1" ssid = "1">Noun phrase (NP) coreference resolution is the task of determining which NPs in a text or dialogue refer to the same real-world entity.</S>
			<S sid ="2" ssid = "2">The difficulty of the task stems in part from its reliance on world knowledge (Charniak, 1972).</S>
			<S sid ="3" ssid = "3">To exemplify, consider the following text fragment.</S>
			<S sid ="4" ssid = "4">Martha Stewart is hoping people don’t run out on her.</S>
			<S sid ="5" ssid = "5">The celebrity indicted on charges stemming from . . .</S>
			<S sid ="6" ssid = "6">Having the (world) knowledge that Martha Stewart is a celebrity would be helpful for establishing the coreference relation between the two NPs.</S>
			<S sid ="7" ssid = "7">One may argue that employing heuristics such as subject preference or syntactic parallelism (which prefers resolving an NP to a candidate antecedent that has the same grammatical role) in this example would also allow us to correctly resolve the celebrity (Mitkov, 814 and Marcu (2005), Ng (2007)), and coreference- annotated data (e.g., Bengtson and Roth (2008)).</S>
			<S sid ="8" ssid = "8">While each of these three sources of world knowledge has been shown to improve coreference resolution, the improvements were typically obtained by incorporating world knowledge (as features) into a baseline resolver composed of a rather weak coreference model (i.e., the mention-pair model) and a small set of features (i.e., the 12 features adopted by Soon et al.’s (2001) knowledge-lean approach).</S>
			<S sid ="9" ssid = "9">As a result, some questions naturally arise.</S>
			<S sid ="10" ssid = "10">First, can world knowledge still offer benefits when used in combination with a richer set of features?</S>
			<S sid ="11" ssid = "11">Second, since automatically extracted world knowledge is typically noisy (Ponzetto and Poesio, 2009), are recently-developed coreference models more noise- tolerant than the mention-pair model, and if so, can they profit more from the noisily extracted world knowledge?</S>
			<S sid ="12" ssid = "12">Finally, while different world knowl Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 814–824, Portland, Oregon, June 1924, 2011.</S>
			<S sid ="13" ssid = "13">Qc 2011 Association for Computational Linguistics edge sources have been shown to be useful when applied in isolation to a coreference system, do they offer complementary benefits and therefore can further improve a resolver when applied in combination?</S>
			<S sid ="14" ssid = "14">We seek answers to these questions by conducting a systematic evaluation of different world knowledge sources for learning-based coreference resolution.</S>
			<S sid ="15" ssid = "15">Specifically, we (1) derive world knowledge from encyclopedic sources that are under- investigated for coreference resolution, including FrameNet (Baker et al., 1998) and YAGO (Suchanek et al., 2007), in addition to coreference-annotated data and unannotated data; (2) incorporate such knowledge as features into a richer baseline feature set that we previously employed (Rahman and Ng, 2009); and (3) evaluate their utility using two coreference models, the traditional mention-pair model (Soon et al., 2001) and the recently developed cluster-ranking model (Rahman and Ng, 2009).</S>
			<S sid ="16" ssid = "16">Our evaluation corpus contains 410 documents, which are coreference-annotated using the ACE annotation scheme as well as the OntoNotes annotation scheme (Hovy et al., 2006).</S>
			<S sid ="17" ssid = "17">By evaluating on two sets of coreference annotations for the same set of documents, we can determine whether the usefulness of world knowledge sources for coreference resolution is dependent on the underlying annotation scheme used to annotate the documents.</S>
	</SECTION>
	<SECTION title="Preliminaries. " number = "2">
			<S sid ="18" ssid = "1">In this section, we describe the corpus, the NP extraction methods, the coreference models, and the evaluation measures we will use in our evaluation.</S>
			<S sid ="19" ssid = "2">2.1 Data Set.</S>
			<S sid ="20" ssid = "3">We evaluate on documents that are coreference- annotated using both the ACE annotation scheme and the OntoNotes annotation scheme, so that we can examine whether the usefulness of our world knowledge sources is dependent on the underlying coreference annotation scheme.</S>
			<S sid ="21" ssid = "4">Specifically, our data set is composed of the 410 English newswire articles that appear in both OntoNotes2 and ACE 2004/2005.</S>
			<S sid ="22" ssid = "5">We partition the documents into a training set and a test set following a 80/20 ratio.</S>
			<S sid ="23" ssid = "6">ACE and OntoNotes employ different guidelines to annotate coreference chains.</S>
			<S sid ="24" ssid = "7">A major difference between the two annotation schemes is that ACE only concerns establishing coreference chains among NPs that belong to the ACE entity types, whereas OntoNotes does not have this restriction.</S>
			<S sid ="25" ssid = "8">Hence, the OntoNotes annotation scheme should produce more coreference chains (i.e., non- singleton coreference clusters) than the ACE annotation scheme for a given set of documents.</S>
			<S sid ="26" ssid = "9">For our data set, the OntoNotes scheme yielded 4500 chains, whereas the ACE scheme yielded only 3637 chains.</S>
			<S sid ="27" ssid = "10">Another difference between the two annotation schemes is that singleton clusters are annotated in ACE but not OntoNotes.</S>
			<S sid ="28" ssid = "11">As discussed below, the presence of singleton clusters may have an impact on NP extraction and coreference evaluation.</S>
			<S sid ="29" ssid = "12">2.2 NP Extraction.</S>
			<S sid ="30" ssid = "13">Following common practice, we employ different methods to extract NPs from the documents annotated with the two annotation schemes.</S>
			<S sid ="31" ssid = "14">To extract NPs from the ACE-annotated documents, we train a mention extractor on the training texts (see Section 5.1 of Rahman and Ng (2009) for details), which recalls 83.6% of the NPs in the test set.</S>
			<S sid ="32" ssid = "15">On the other hand, to extract NPs from the OntoNotes-annotated documents, the same method should not be applied.</S>
			<S sid ="33" ssid = "16">To see the reason, recall that only the NPs in non-singleton clusters are annotated in these documents.</S>
			<S sid ="34" ssid = "17">Training a mention extractor on these NPs implies that we are learning to extract non-singleton NPs, which are typically much smaller in number than the entire set of NPs.</S>
			<S sid ="35" ssid = "18">In other words, doing so could substantially simplify the coreference task.</S>
			<S sid ="36" ssid = "19">Consequently, we follow the approach adopted by traditional learning-based re- solvers and employ an NP chunker to extract NPs.</S>
			<S sid ="37" ssid = "20">Specifically, we use the markable identification system in the Reconcile resolver (Stoyanov et al., 2010) to extract NPs from the training and test texts.</S>
			<S sid ="38" ssid = "21">This identifier recalls 77.4% of the NPs in the test set.</S>
			<S sid ="39" ssid = "22">2.3 Coreference Models.</S>
			<S sid ="40" ssid = "23">We evaluate the utility of world knowledge using the mention-pair model and the cluster-ranking model.</S>
			<S sid ="41" ssid = "24">2.3.1 Mention-Pair Model The mention-pair (MP) model is a classifier that determines whether two NPs are coreferent or not.</S>
			<S sid ="42" ssid = "25">Each instance i(N Pj , N Pk ) corresponds to N Pj and N Pk , and is represented by a Baseline feature set consisting of 39 features.</S>
			<S sid ="43" ssid = "26">Linguistically, these features can be divided into four categories: string-matching, grammatical, semantic, and positional.</S>
			<S sid ="44" ssid = "27">These features can also be categorized based on whether they are relational or not.</S>
			<S sid ="45" ssid = "28">Relational features capture the relationship between N Pj and N Pk , whereas non- relational features capture the linguistic property of one of these two NPs.</S>
			<S sid ="46" ssid = "29">Since space limitations preclude a description of these features, we refer the reader to Rahman and Ng (2009) for details.</S>
			<S sid ="47" ssid = "30">We follow Soon et al.’s (2001) method for creating training instances: we create (1) a positive instance for each anaphoric NP, N Pk , and its closest antecedent, N Pj ; and (2) a negative instance for N Pk paired with each of the intervening NPs, N Pj+1, N Pj+2, . . ., N Pk−1.</S>
			<S sid ="48" ssid = "31">The classification of a training instance is either positive or negative, depending on whether the two NPs are coreferent in the associated text.</S>
			<S sid ="49" ssid = "32">To train the MP model, we use the SVM learning algorithm from SVMlight (Joachims, 2002).1 After training, the classifier is used to identify an antecedent for an NP in a test text.</S>
			<S sid ="50" ssid = "33">Specifically, each NP, N Pk , is compared in turn to each preceding NP, N Pj , from right to left, and N Pj is selected as its antecedent if the pair is classified as coreferent.</S>
			<S sid ="51" ssid = "34">The process terminates as soon as an antecedent is found for N Pk or the beginning of the text is reached.</S>
			<S sid ="52" ssid = "35">Despite its popularity, the MP model has two major weaknesses.</S>
			<S sid ="53" ssid = "36">First, since each candidate antecedent for an NP to be resolved (henceforth an active NP) is considered independently of the others, this model only determines how good a candidate antecedent is relative to the active NP, but not how good a candidate antecedent is relative to other candidates.</S>
			<S sid ="54" ssid = "37">So, it fails to answer the critical question of which candidate antecedent is most probable.</S>
			<S sid ="55" ssid = "38">Second, it has limitations in its expressiveness: the information extracted from the two NPs alone may not be sufficient for making a coreference decision.</S>
			<S sid ="56" ssid = "39">2.3.2 Cluster-Ranking Model The cluster-ranking (CR) model addresses the two weaknesses of the MP model by combining the strengths of the entity-mention model (e.g., Luo et 1 For this and subsequent uses of the SVM learner in our experiments, we set all parameters to their default values.</S>
			<S sid ="57" ssid = "40">al.</S>
			<S sid ="58" ssid = "41">(2004), Yang et al.</S>
			<S sid ="59" ssid = "42">(2008)) and the mention- ranking model (e.g., Denis and Baldridge (2008)).</S>
			<S sid ="60" ssid = "43">Specifically, the CR model ranks the preceding clusters for an active NP so that the highest-ranked cluster is the one to which the active NP should be linked.</S>
			<S sid ="61" ssid = "44">Employing a ranker addresses the first weakness, as a ranker allows all candidates to be compared simultaneously.</S>
			<S sid ="62" ssid = "45">Considering preceding clusters rather than antecedents as candidates addresses the second weakness, as cluster-level features (i.e., features that are defined over any subset of NPs in a preceding cluster) can be employed.</S>
			<S sid ="63" ssid = "46">Details of the CR model can be found in Rahman and Ng (2009).</S>
			<S sid ="64" ssid = "47">Since the CR model ranks preceding clusters, a training instance i(cj , N Pk ) represents a preceding cluster, cj , and an anaphoric NP, N Pk . Each instance consists of features that are computed based solely on N Pk as well as cluster-level features, which describe the relationship between cj and N Pk . Motivated in part by Culotta et al.</S>
			<S sid ="65" ssid = "48">(2007), we create cluster-level features from the relational features in our feature set using four predicates: N O N E, MO ST- FA LSE, MO STTRU E, and A LL.</S>
			<S sid ="66" ssid = "49">Specifically, for each relational feature X, we first convert X into an equivalent set of binary-valued features if it is multi- valued.</S>
			<S sid ="67" ssid = "50">Then, for each resulting binary-valued feature Xb , we create four binary-valued cluster-level features: (1) N O N E-Xb is true when Xb is false between N Pk and each NP in cj ; (2) MO ST-FA LSEXb is true when Xb is true between N Pk and less than half (but at least one) of the NPs in cj ; (3) MO STTRU E- Xb is true when Xb is true between N Pk and at least half (but not all) of the NPs in cj ; and (4) A LLXb is true when Xb is true between N Pk and each NP in cj . We train a cluster ranker to jointly learn anaphoricity determination and coreference resolution using SVMlight’s ranker-learning algorithm.</S>
			<S sid ="68" ssid = "51">Specifically, for each NP, N Pk , we create a training instance between N Pk and each preceding cluster cj using the features described above.</S>
			<S sid ="69" ssid = "52">Since we are learning a joint model, we need to provide the ranker with the option to start a new cluster by creating an additional training instance that contains the non- relational features describing N Pk . The rank value of a training instance i(cj , N Pk ) created for N Pk is the rank of cj among the competing clusters.</S>
			<S sid ="70" ssid = "53">If N Pk is anaphoric, its rank is HIG H if N Pk belongs to cj , and LOW otherwise.</S>
			<S sid ="71" ssid = "54">If N Pk is non-anaphoric, its rank is LOW unless it is the additional training instance described above, which has rank HIG H. After training, the cluster ranker processes the NPs in a test text in a left-to-right manner.</S>
			<S sid ="72" ssid = "55">For each active NP, N Pk , we create test instances for it by pairing it with each of its preceding clusters.</S>
			<S sid ="73" ssid = "56">To allow for the possibility that N Pk is non-anaphoric, we create an additional test instance as during training.</S>
			<S sid ="74" ssid = "57">All these test instances are then presented to the ranker.</S>
			<S sid ="75" ssid = "58">If the additional test instance is assigned the highest rank value, then we create a new cluster containing N Pk . Otherwise, N Pk is linked to the cluster that has the highest rank.</S>
			<S sid ="76" ssid = "59">Note that the partial clusters preceding N Pk are formed incrementally based on the predictions of the ranker for the first k − 1 NPs.</S>
			<S sid ="77" ssid = "60">2.4 Evaluation Measures.</S>
			<S sid ="78" ssid = "61">We employ two commonly-used scoring programs, B3 (Bagga and Baldwin, 1998) and CEAF (Luo, 2005), both of which report results in terms of recall (R), precision (P), and F-measure (F) by comparing the gold-standard (i.e., key) partition, K P , against the system-generated (i.e., response) partition, RP . Briefly, B3 computes the R and P values of each NP and averages these values at the end.</S>
			<S sid ="79" ssid = "62">Specifically, for each NP, N Pj , B3 first computes the number of common NPs in K Pj and RPj , the clusters containing N Pj in K P and RP , respectively, and then divides this number by |K Pj | and |RPj | to obtain the R and P values of N Pj , respectively.</S>
			<S sid ="80" ssid = "63">On the other hand, CEAF finds the best one-to-one alignment between the key clusters and the response clusters.</S>
			<S sid ="81" ssid = "64">A complication arises when B3 is used to score a response partition containing automatically extracted NPs.</S>
			<S sid ="82" ssid = "65">Recall that B3 constructs a mapping between the NPs in the response and those in the key.</S>
			<S sid ="83" ssid = "66">Hence, if the response is generated using gold- standard NPs, then every NP in the response is mapped to some NP in the key and vice versa.</S>
			<S sid ="84" ssid = "67">In other words, there are no twinless (i.e., unmapped) NPs (Stoyanov et al., 2009).</S>
			<S sid ="85" ssid = "68">This is not the case when automatically extracted NPs are used, but the original description of B3 does not specify how twinless NPs should be scored (Bagga and Baldwin, 1998).</S>
			<S sid ="86" ssid = "69">To address this problem, we set the recall and precision of a twinless NP to zero, regardless of whether the NP appears in the key or the response.</S>
			<S sid ="87" ssid = "70">Note that CEAF can compare partitions with twin less NPs without any modification, since it operates by finding the best alignment between the clusters in the two partitions.</S>
			<S sid ="88" ssid = "71">Additionally, in order not to over-penalize a response partition, we remove all the twinless NPs in the response that are singletons.</S>
			<S sid ="89" ssid = "72">The rationale is simple: since the resolver has successfully identified these NPs as singletons, it should not be penalized, and removing them avoids such penalty.</S>
			<S sid ="90" ssid = "73">Since B3 and CEAF align NPs/clusters, the lack of singleton clusters in the OntoNotes annotations implies that the resulting scores reflect solely how well a resolver identifies coreference links and do not take into account how well it identifies singleton clusters.</S>
	</SECTION>
	<SECTION title="Extracting World Knowledge. " number = "3">
			<S sid ="91" ssid = "1">In this section, we describe how we extract world knowledge for coreference resolution from three different sources: large-scale knowledge bases, coreference-annotated data and unannotated data.</S>
			<S sid ="92" ssid = "2">3.1 World Knowledge from Knowledge Bases.</S>
			<S sid ="93" ssid = "3">We extract world knowledge from two large-scale knowledge bases, YAGO and FrameNet.</S>
			<S sid ="94" ssid = "4">3.1.1 Extracting Knowledge from YAGO We choose to employ YAGO rather than the more popularly-used Wikipedia due to its potentially richer knowledge, which comprises 5 million facts extracted from Wikipedia and WordNet.</S>
			<S sid ="95" ssid = "5">Each fact is represented as a triple (N Pj , rel, N Pk ), where rel is one of the 90 YAGO relation types defined on two NPs, N Pj and N Pk . Motivated in part by previous work (Bryl et al., 2010; Uryupina et al., 2011), we employ the two relation types that we believe are most useful for coreference resolution, TY PE and MEA N S. TY PE is essentially an IS-A relation.</S>
			<S sid ="96" ssid = "6">For instance, the triple (AlbertEinstein, TY PE, physicist) denotes the fact that Albert Einstein is a physicist.</S>
			<S sid ="97" ssid = "7">MEA N S provides different ways of expressing an entity, and therefore allows us to deal with synonymy and ambiguity.</S>
			<S sid ="98" ssid = "8">For instance, the two triples (Einstein, MEA N S, AlbertEinstein) and (Einstein, MEA N S, AlfredEinstein) denote the facts that Einstein may refer to the physicist Albert Einstein and the musicologist Alfred Einstein, respectively.</S>
			<S sid ="99" ssid = "9">Hence, the presence of one or both of these relations between two NPs provides strong evidence that the two NPs are coreferent.</S>
			<S sid ="100" ssid = "10">YAGO’s unification of the information in Wikipedia and WordNet enables it to extract facts that cannot be extracted with Wikipedia or WordNet alone, such as (MarthaStewart, TY PE, celebrity).</S>
			<S sid ="101" ssid = "11">To better appreciate YAGO’s strengths, let us see how this fact was extracted.</S>
			<S sid ="102" ssid = "12">YAGO first heuristically maps each of the Wiki categories in the Wiki page for Martha Stewart to its semantically closest WordNet synset.</S>
			<S sid ="103" ssid = "13">For instance, the Wiki category AMER IC A N TELE- V ISIO N PER SO NA LITIE S is mapped to the synset corresponding to sense #2 of the word personality.</S>
			<S sid ="104" ssid = "14">Then, given that personality is a direct hyponym of celebrity in WordNet, YAGO extracts the desired fact.</S>
			<S sid ="105" ssid = "15">This enables YAGO to extract facts that cannot be extracted with Wikipedia or WordNet alone.</S>
			<S sid ="106" ssid = "16">We incorporate the world knowledge from YAGO into our coreference models as a binary-valued feature.</S>
			<S sid ="107" ssid = "17">If the MP model is used, the YAGO feature for an instance will have the value 1 if and only if the two NPs involved are in a TY PE or MEA N S relation.</S>
			<S sid ="108" ssid = "18">On the other hand, if the CR model is used, the YAGO feature for an instance involving N Pk and preceding cluster c will have the value 1 if and only if N Pk has a TY PE or MEA N S relation with any of the NPs in c. Since knowledge extraction from web- based encyclopedia is typically noisy (Ponzetto and Poesio, 2009), we use YAGO to determine whether two NPs have a relation only if one NP is a named entity (NE) of type person, organization, or location according to the Stanford NE recognizer (Finkel et al., 2005) and the other NP is a common noun.</S>
			<S sid ="109" ssid = "19">3.1.2 Extracting Knowledge from FrameNet FrameNet is a lexico-semantic resource focused on semantic frames (Baker et al., 1998).</S>
			<S sid ="110" ssid = "20">As a schematic representation of a situation, a frame contains the lexical predicates that can invoke it as well as the frame elements (i.e., semantic roles).</S>
			<S sid ="111" ssid = "21">For example, the JU D G MEN T C O MMU N IC ATIO N frame describes situations in which a CO MMU N IC ATO R communicates a judgment of an EVA LU EE to an AD D R ESSEE.</S>
			<S sid ="112" ssid = "22">This frame has CO MMU N IC ATO R and EVA LU EE as its core frame elements and AD D R ESSEE as its non- core frame elements, and can be invoked by more than 40 predicates, such as acclaim, accuse, com mend, decry, denounce, praise, and slam.</S>
			<S sid ="113" ssid = "23">To better understand why FrameNet contains potentially useful knowledge for coreference resolution, consider the following text segment: Peter Anthony decries program trading as “limiting the game to a few,” but he is not sure whether he wants to denounce it because ...</S>
			<S sid ="114" ssid = "24">To establish the coreference relation between it and program trading, it may be helpful to know that decry and denounce appear in the same frame and the two NPs have the same semantic role.</S>
			<S sid ="115" ssid = "25">This example suggests that features encoding both the semantic roles of the two NPs under consideration and whether the associated predicates are “related” to each other in FrameNet (i.e., whether they appear in the same frame) could be useful for identifying coreference relations.</S>
			<S sid ="116" ssid = "26">Two points regarding our implementation of these features deserve mention.</S>
			<S sid ="117" ssid = "27">First, since we do not employ verb sense disambiguation, we consider two predicates related as long as there is at least one semantic frame in which they both appear.</S>
			<S sid ="118" ssid = "28">Second, since FrameNet-style semantic role labelers are not publicly available, we use ASSERT (Pradhan et al., 2004), a semantic role labeler that provides PropBank-style semantic roles such as AR G 0 (the PROTOAG EN T, which is typically the subject of a transitive verb) and AR G 1 (the PROTO PATIEN T, which is typically its direct object).</S>
			<S sid ="119" ssid = "29">Now, assuming that N Pj and N Pk are the arguments of two stemmed predicates, predj and predk , we create 15 features using the knowledge extracted from FrameNet and ASSERT as follows.</S>
			<S sid ="120" ssid = "30">First, we encode the knowledge extracted from FrameNet as one of three possible values: (1) predj and predk are in the same frame; (2) they are both predicates in FrameNet but never appear in the same frame; and (3) one or both predicates do not appear in FrameNet.</S>
			<S sid ="121" ssid = "31">Second, we encode the semantic roles of N Pj and N Pk as one of five possible values: AR G 0AR G 0, AR G 1AR G 1, AR G 0AR G 1, AR G 1AR G 0, and OTH ER S (the default case).2 Finally, we create 15 binary-valued features by pairing the 3 possible values extracted from FrameNet and the 5 possible values provided by ASSERT.</S>
			<S sid ="122" ssid = "32">Since these features 2 We focus primarily on AR G 0 and AR G 1 because they are the most important core arguments of a predicate and may provide more useful information than other semantic roles.</S>
			<S sid ="123" ssid = "33">are computed over two NPs, we can employ them directly for the MP model.</S>
			<S sid ="124" ssid = "34">Note that by construction, exactly one of these features will have a nonzero value.</S>
			<S sid ="125" ssid = "35">For the CR model, we extend their definitions so that they can be computed between an NP, N Pk , and a preceding cluster, c. Specifically, the value of a feature is 1 if and only if its value between N Pk and one of the NPs in c is 1 under its original definition.</S>
			<S sid ="126" ssid = "36">The above discussion assumes that the two NPs under consideration serve as predicate arguments.</S>
			<S sid ="127" ssid = "37">If this assumption fails, we will not create any features based on FrameNet for these two NPs.</S>
			<S sid ="128" ssid = "38">To our knowledge, FrameNet has not been exploited for coreference resolution.</S>
			<S sid ="129" ssid = "39">However, the use of related verbs is similar in spirit to Bean and Riloff’s (2004) use of patterns for inducing contextual role knowledge, and the use of semantic roles is also discussed in Ponzetto and Strube (2006).</S>
			<S sid ="130" ssid = "40">3.2 World Knowledge from Annotated Data.</S>
			<S sid ="131" ssid = "41">Since world knowledge is needed for coreference resolution, a human annotator must have employed world knowledge when coreference-annotating a document.</S>
			<S sid ="132" ssid = "42">We aim to design features that can “recover” such world knowledge from annotated data.</S>
			<S sid ="133" ssid = "43">3.2.1 Features Based on Noun Pairs A natural question is: what kind of world knowledge can we extract from annotated data?</S>
			<S sid ="134" ssid = "44">We may gather the knowledge that Barack Obama is a U.S. president if we see these two NPs appearing in the same coreference chain.</S>
			<S sid ="135" ssid = "45">Equally importantly, we may gather the commonsense knowledge needed for determining non-coreference.</S>
			<S sid ="136" ssid = "46">For instance, we may discover that a lion and a tiger are unlikely to refer to the same real-world entity after realizing that they never appear in the same chain in a large number of annotated documents.</S>
			<S sid ="137" ssid = "47">Note that any features computed based on WordNet distance or distributional similarity are likely to incorrectly suggest that lion and tiger are coreferent, since the two nouns are similar distributionally and according to WordNet.</S>
			<S sid ="138" ssid = "48">Given these observations, one may collect the noun pairs from the (coreference-annotated) training data and use them as features to train a resolver.</S>
			<S sid ="139" ssid = "49">However, for these features to be effective, we need to address data sparseness, as many noun pairs in the training data may not appear in the test data.</S>
			<S sid ="140" ssid = "50">To improve generalization, we instead create different kinds of noun-pair-based features given an annotated text.</S>
			<S sid ="141" ssid = "51">To begin with, we preprocess each document.</S>
			<S sid ="142" ssid = "52">A training text is preprocessed by randomly replacing 10% of its common nouns with the label U N SEEN.</S>
			<S sid ="143" ssid = "53">If an NP, N Pk , is replaced with U N - SEEN, all NPs that have the same string as N Pk will also be replaced with U N SEEN.</S>
			<S sid ="144" ssid = "54">A test text is preprocessed differently: we simply replace all NPs whose strings are not seen in the training data with U N - SEEN.</S>
			<S sid ="145" ssid = "55">Hence, artificially creating U N SEEN labels from a training text will allow a learner to learn how to handle unseen words in a test text.</S>
			<S sid ="146" ssid = "56">Next, we create noun-pair-based features for the MP model, which will be used to augment the Baseline feature set.</S>
			<S sid ="147" ssid = "57">Here, each instance corresponds to two NPs, N Pj and N Pk , and is represented by three groups of binary-valued features.</S>
			<S sid ="148" ssid = "58">Unseen features are applicable when both N Pj and N Pk are U N SEEN.</S>
			<S sid ="149" ssid = "59">Either an U N SEEN -SA ME feature or an U N SEEN -D IFF feature is created, depending on whether the two NPs are the same string before being replaced with the U N SEEN token.</S>
			<S sid ="150" ssid = "60">Lexical features are applicable when neither N Pj nor N Pk is U N SEEN.</S>
			<S sid ="151" ssid = "61">A lexical feature is an ordered pair consisting of the heads of the NPs.</S>
			<S sid ="152" ssid = "62">For a pronoun or a common noun, the head is the last word of the NP; for a proper name, the head is the entire NP.</S>
			<S sid ="153" ssid = "63">Semi-lexical features aim to improve generalization, and are applicable when neither N Pj nor N Pk is U N SEEN.</S>
			<S sid ="154" ssid = "64">If exactly one of N Pj and N Pk is tagged as a NE by the Stanford NE recognizer, we create a semi-lexical feature that is identical to the lexical feature described above, except that the NE is replaced with its NE label.</S>
			<S sid ="155" ssid = "65">On the other hand, if both NPs are NEs, we check whether they are the same string.</S>
			<S sid ="156" ssid = "66">If so, we create a *N E*-SA ME feature, where *N E* is replaced with the corresponding NE label.</S>
			<S sid ="157" ssid = "67">Otherwise, we check whether they have the same NE tag and a word-subset match (i.e., whether the word tokens in one NP appears in the other’s list of word tokens).</S>
			<S sid ="158" ssid = "68">If so, we create a *N E*-SU B SA ME feature, where *N E* is replaced with their NE label.</S>
			<S sid ="159" ssid = "69">Otherwise, we create a feature that is the concatenation of the NE labels of the two NPs.</S>
			<S sid ="160" ssid = "70">The noun-pair-based features for the CR model can be generated using essentially the same method.</S>
			<S sid ="161" ssid = "71">Specifically, since each instance now corresponds to an NP, N Pk , and a preceding cluster, c, we can generate a noun-pair-based feature by applying the above method to N Pk and each of the NPs in c, and its value is the number of times it is applicable to N Pk and c. 3.2.2 Features Based on Verb Pairs As discussed above, features encoding the semantic roles of two NPs and the relatedness of the associated verbs could be useful for coreference resolution.</S>
			<S sid ="162" ssid = "72">Rather than encoding verb relatedness, we may replace verb relatedness with the verbs themselves in these features, and have the learner learn directly from coreference-annotated data whether two NPs serving as the objects of decry and denounce are likely to be coreferent or not, for instance.</S>
			<S sid ="163" ssid = "73">Specifically, assuming that N Pj and N Pk are the arguments of two stemmed predicates, predj and predk , in the training data, we create five features as follows.</S>
			<S sid ="164" ssid = "74">First, we encode the semantic roles of N Pj and N Pk as one of five possible values: AR G 0AR G 0, AR G 1AR G 1, AR G 0AR G 1, AR G 1AR G 0, and OTH ER S (the default case).</S>
			<S sid ="165" ssid = "75">Second, we create five binary-valued features by pairing each of these five values with the two stemmed predicates.</S>
			<S sid ="166" ssid = "76">Since these features are computed over two NPs, we can employ them directly for the MP model.</S>
			<S sid ="167" ssid = "77">Note that by construction, exactly one of these features will have a nonzero value.</S>
			<S sid ="168" ssid = "78">For the CR model, we extend their definitions so that they can be computed between an NP, N Pk , and a preceding cluster, c. Specifically, the value of a feature is 1 if and only if its value between N Pk and one of the NPs in c is 1 under its original definition.</S>
			<S sid ="169" ssid = "79">The above discussion assumes that the two NPs under consideration serve as predicate arguments.</S>
			<S sid ="170" ssid = "80">If this assumption fails, we will not create any features based on verb pairs for these two NPs.</S>
			<S sid ="171" ssid = "81">3.3 World Knowledge from Unannotated Data.</S>
			<S sid ="172" ssid = "82">Previous work has shown that syntactic appositions, which can be extracted using heuristics from unannotated documents or parse trees, are a useful source of world knowledge for coreference resolution (e.g., Daume´ III and Marcu (2005), Ng (2007), Haghighi and Klein (2009)).</S>
			<S sid ="173" ssid = "83">Each extraction is an NP pair such as &lt;Barack Obama, the president&gt; and &lt;Eastern Airlines, the carrier&gt;, where the first NP in the pair is a proper name and the second NP is a common NP.</S>
			<S sid ="174" ssid = "84">Low-frequency extractions are typically assumed to be noisy and discarded.</S>
			<S sid ="175" ssid = "85">We combine the extractions produced by Fleischman et al.</S>
			<S sid ="176" ssid = "86">(2003) and Ng (2007) to form a database consisting of 1.057 million NP pairs, and create a binary-valued feature for our coreference models using this database.</S>
			<S sid ="177" ssid = "87">If the MP model is used, this feature will have the value 1 if and only if the two NPs appear as a pair in the database.</S>
			<S sid ="178" ssid = "88">On the other hand, if the CR model is used, the feature for an instance involving N Pk and preceding cluster c will have the value 1 if and only if N Pk and at least one of the NPs in c appears as a pair in the database.</S>
	</SECTION>
	<SECTION title="Evaluation. " number = "4">
			<S sid ="179" ssid = "1">4.1 Experimental Setup.</S>
			<S sid ="180" ssid = "2">As described in Section 2, we use as our evaluation corpus the 411 documents that are coreference- annotated using the ACE and OntoNotes annotation schemes.</S>
			<S sid ="181" ssid = "3">Specifically, we divide these documents into five (disjoint) folds of roughly the same size, training the MP model and the CR model using SVMlight on four folds and evaluate their performance on the remaining fold.</S>
			<S sid ="182" ssid = "4">The linguistic features, as well as the NPs used to create the training and test instances, are computed automatically.</S>
			<S sid ="183" ssid = "5">We employ B3 and CEAF as described in Section 2.3 to score the output of a coreference system.</S>
			<S sid ="184" ssid = "6">4.2 Results and Discussion.</S>
			<S sid ="185" ssid = "7">4.2.1 Baseline Models Since our goal is to evaluate the effectiveness of the features encoding world knowledge for learning- based coreference resolution, we employ as our baselines the MR model and the CR model trained on the Baseline feature set, which does not contain any features encoding world knowledge.</S>
			<S sid ="186" ssid = "8">For the MP model, the Baseline feature set consists of the 39 features described in Section 2.3.1; for the CR model, the Baseline feature set consists of the cluster-level features derived from the 39 features used in the Baseline MP model (see Section 2.3.2).</S>
			<S sid ="187" ssid = "9">Results of the MP model and the CR model employing the Baseline feature set are shown in rows 1 and 8 of Table 1, respectively.</S>
			<S sid ="188" ssid = "10">Each row contains the B3 and CEAF results of the corresponding coreference model when it is evaluated using the ACE and 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Table 1: Results obtained by applying different types of features in isolation to the Baseline system.</S>
			<S sid ="189" ssid = "11">1 2 3 4 5 6 7 8 9 10 11 12 13 14 Table 2: Results obtained by adding different types of features incrementally to the Baseline system.</S>
			<S sid ="190" ssid = "12">OntoNotes annotations as the gold standard.</S>
			<S sid ="191" ssid = "13">As we can see, the MP model achieves F-measure scores of 62.4 (B3) and 60.0 (CEAF) on ACE and 53.3 (B3 ) and 51.5 (CEAF) on OntoNotes, and the CR model achieves F-measure scores of 66.1 (B3) and 63.8 (CEAF) on ACE and 56.2 (B3) and 54.0 (CEAF) on OntoNotes.</S>
			<S sid ="192" ssid = "14">Also, the results show that the CR model is stronger than the MP model, corroborating previous empirical findings (Rahman and Ng, 2009).</S>
			<S sid ="193" ssid = "15">4.2.2 Incorporating World Knowledge Next, we examine the usefulness of world knowledge for coreference resolution.</S>
			<S sid ="194" ssid = "16">The remaining rows in Table 1 show the results obtained when different types of features encoding world knowledge are applied to the Baseline system in isolation.</S>
			<S sid ="195" ssid = "17">The best result for each combination of data set, evaluation measure, and coreference model is boldfaced.</S>
			<S sid ="196" ssid = "18">Two points deserve mention.</S>
			<S sid ="197" ssid = "19">First, each type of features improves the Baseline, regardless of the coreference model, the evaluation measure, and the annotation scheme used.</S>
			<S sid ="198" ssid = "20">This suggests that all these feature types are indeed useful for coreference resolution.</S>
			<S sid ="199" ssid = "21">It is worth noting that in all but a few cases involving the FrameNet-based and appositive based features, the rise in F-measure is accompanied by a 1.</S>
			<S sid ="200" ssid = "22">The Bush White House is breeding non-duck ducks the same way the Nixon White House did: It hops on an.</S>
			<S sid ="201" ssid = "23">issue that is unopposable – cleaner air, better treatment of the disabled, better child care.</S>
			<S sid ="202" ssid = "24">The President came up with a good bill, but now may end up signing the awful bureaucratic creature hatched on Capitol Hill.</S>
			<S sid ="203" ssid = "25">2.</S>
			<S sid ="204" ssid = "26">The tumor, he suggested, developed when the second, normal copy also was damaged.</S>
			<S sid ="205" ssid = "27">He believed colon.</S>
			<S sid ="206" ssid = "28">cancer might also arise from multiple “hits” on cancer suppressor genes, as it often seems to develop in stages.</S>
			<S sid ="207" ssid = "29">Table 3: Examples errors introduced by YAGO and FrameNet.</S>
			<S sid ="208" ssid = "30">simultaneous rise in recall and precision.</S>
			<S sid ="209" ssid = "31">This is perhaps not surprising: as the use of world knowledge helps discover coreference links, recall increases; and as more (relevant) knowledge is available to make coreference decisions, precision increases.</S>
			<S sid ="210" ssid = "32">Second, the feature types that yield the best improvement over the Baseline are YAGO TY PE and Noun Pairs.</S>
			<S sid ="211" ssid = "33">When the MP model is used, the best coreference system improves the Baseline by 1– 1.3% (B3) and 1.3–2.8% (CEAF) in F-measure.</S>
			<S sid ="212" ssid = "34">On the other hand, when the CR model is used, the best system improves the Baseline by 2.3–2.6% (B3 ) and 1.7–2.2% (CEAF) in F-measure.</S>
			<S sid ="213" ssid = "35">Table 2 shows the results obtained when the different types of features are added to the Baseline one after the other.</S>
			<S sid ="214" ssid = "36">Specifically, we add the feature types in this order: YAGO TY PE, YAGO MEA N S, Noun Pairs, FrameNet, Verb Pairs, and Appositives.</S>
			<S sid ="215" ssid = "37">In comparison to the results in Table 1, we can see that better results are obtained when the different types of features are applied to the Baseline in combination than in isolation, regardless of the coreference model, the evaluation measure, and the annotation scheme used.</S>
			<S sid ="216" ssid = "38">The best-performing system, which employs all but the Appositive features, outperforms the Baseline by 3.1–3.3% in F-measure when the MR model is used and by 4.1–4.8% in F-measure when the CR model is used.</S>
			<S sid ="217" ssid = "39">In both cases, the gains in F-measure are accompanied by a simultaneous rise in recall and precision.</S>
			<S sid ="218" ssid = "40">Overall, these results seem to suggest that the CR model is making more effective use of the available knowledge than the MR model, and that the different feature types are providing complementary information for the two coreference models.</S>
			<S sid ="219" ssid = "41">4.3 Example Errors.</S>
			<S sid ="220" ssid = "42">While the different types of features we considered improve the performance of the Baseline primarily via the establishment of coreference links, some of these links are spurious.</S>
			<S sid ="221" ssid = "43">Sentences 1 and 2 of Table 3 show the spurious coreference links introduced by the CR model when YAGO and FrameNet are used, respectively.</S>
			<S sid ="222" ssid = "44">In sentence 1, while The President and Bush are coreferent, YAGO caused the CR model to establish the spurious link between The President and Nixon owing to the proximity of the two NPs and the presence of this NP pair in the YAGO TY PE relation.</S>
			<S sid ="223" ssid = "45">In sentence 2, FrameNet caused the CR model to establish the spurious link between The tumor and colon cancer because these two NPs are the AR G 0 arguments of develop and arise, which appear in the same semantic frame in FrameNet.</S>
	</SECTION>
	<SECTION title="Conclusions. " number = "5">
			<S sid ="224" ssid = "1">We have examined the utility of three major sources of world knowledge for coreference resolution, namely, large-scale knowledge bases (YAGO, FrameNet), coreference-annotated data (Noun Pairs, Verb Pairs), and unannotated data (Appositives), by applying them to two learning-based coreference models, the mention-pair model and the cluster- ranking model, and evaluating them on documents annotated with the ACE and OntoNotes annotation schemes.</S>
			<S sid ="225" ssid = "2">When applying the different types of features in isolation to a Baseline system that does not employ world knowledge, we found that all of them improved the Baseline regardless of the underlying coreference model, the evaluation measure, and the annotation scheme, with YAGO TY PE and Noun Pairs yielding the largest performance gains.</S>
			<S sid ="226" ssid = "3">Nevertheless, the best results were obtained when they were applied in combination to the Baseline system.</S>
			<S sid ="227" ssid = "4">We conclude from these results that the different feature types we considered are providing complementary world knowledge to the coreference resolvers, and while each of them provides fairly small gains, their cumulative benefits can be substantial.</S>
	</SECTION>
	<SECTION title="Acknowledgments">
			<S sid ="228" ssid = "5">We thank the three reviewers for their invaluable comments on an earlier draft of the paper.</S>
			<S sid ="229" ssid = "6">This work was supported in part by NSF Grant IIS0812261.</S>
	</SECTION>
</PAPER>
