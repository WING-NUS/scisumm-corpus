<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">We present two semi-supervised learning techniques to improve a state-of-the-art multilingual name tagger.</S>
		<S sid ="2" ssid = "2">For English and Chinese, the overall system obtains 1.7% - 2.1% improvement in F-measure, representing a 13.5% - 17.4% relative reduction in the spurious, missing, and incorrect tags.</S>
		<S sid ="3" ssid = "3">We also conclude that simply relying upon large corpora is not in itself sufficient: we must pay attention to unlabeled data selection too.</S>
		<S sid ="4" ssid = "4">We describe effective measures to automatically select documents and sentences.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="5" ssid = "5">When applying machine learning approaches to natural language processing tasks, it is time- consuming and expensive to hand-label the large amounts of training data necessary for good performance.</S>
			<S sid ="6" ssid = "6">Unlabeled data can be collected in much larger quantities.</S>
			<S sid ="7" ssid = "7">Therefore, a natural question is whether we can use unlabeled data to build a more accurate learner, given the same amount of labeled data.</S>
			<S sid ="8" ssid = "8">This problem is often referred to as semi-supervised learning.</S>
			<S sid ="9" ssid = "9">It significantly reduces the effort needed to develop a training set.</S>
			<S sid ="10" ssid = "10">It has shown promise in improving the performance of many tasks such as name tagging (Miller et al., 2004), semantic class extraction (Lin et al., 2003), chunking (Ando and Zhang, 2005), coreference resolution (Bean and Riloff, 2004) and text classification (Blum and Mitchell, 1998).</S>
			<S sid ="11" ssid = "11">However, it is not clear, when semi-supervised learning is applied to improve a learner, how the system should effectively select unlabeled data, and how the size and relevance of data impact the performance.</S>
			<S sid ="12" ssid = "12">In this paper we apply two semi-supervised learning algorithms to improve a state-of-the-art name tagger.</S>
			<S sid ="13" ssid = "13">We run the baseline name tagger on a large unlabeled corpus (bootstrapping) and the test set (self-training), and automatically generate high-confidence machine-labeled sentences as additional ‘training data’.</S>
			<S sid ="14" ssid = "14">We then iteratively retrain the model on the increased ‘training data’.</S>
			<S sid ="15" ssid = "15">We first investigated whether we can improve the system by simply using a lot of unlabeled data.</S>
			<S sid ="16" ssid = "16">By dramatically increasing the size of the corpus with unlabeled data, we did get a significant improvement compared to the baseline system.</S>
			<S sid ="17" ssid = "17">But we found that adding off-topic unlabeled data sometimes makes the performance worse.</S>
			<S sid ="18" ssid = "18">Then we tried to select relevant documents from the unlabeled data in advance, and got clear further improvements.</S>
			<S sid ="19" ssid = "19">We also obtained significant improvement by self-training (boot- strapping on the test data) without any additional unlabeled data.</S>
			<S sid ="20" ssid = "20">Therefore, in contrast to the claim in (Banko and Brill, 2001), we concluded that, for some applications, effective use of large unlabeled corpora demands good data selection measures.</S>
			<S sid ="21" ssid = "21">We propose and quantify some effective measures to select documents and sentences in this paper.</S>
			<S sid ="22" ssid = "22">The rest of this paper is structured as follows.</S>
			<S sid ="23" ssid = "23">Section 2 briefly describes the efforts made by previous researchers to use semi-supervised learning as well as the work of (Banko and Brill, 2001).</S>
			<S sid ="24" ssid = "24">Section 3 presents our baseline name tag- ger.</S>
			<S sid ="25" ssid = "25">Section 4 describes the motivation for our approach while Section 5 presents the details of two semi-supervised learning methods.</S>
			<S sid ="26" ssid = "26">Section 6 presents and discusses the experimental results on both English and Chinese.</S>
			<S sid ="27" ssid = "27">Section 7 presents our conclusions and directions for future work.</S>
	</SECTION>
	<SECTION title="Prior Work. " number = "2">
			<S sid ="28" ssid = "1">This work presented here extends a substantial body of previous work (Blum and Mitchell, 1998; Riloff and Jones, 1999; Ando and Zhang, 2005) 48 Proceedings of the Workshop on Information Extraction Beyond The Document, pages 48–55, Sydney, July 2006.</S>
			<S sid ="29" ssid = "2">Qc 2006 Association for Computational Linguistics that all focus on reducing annotation requirements.</S>
			<S sid ="30" ssid = "3">For the specific task of named entity annotation, some researchers have emphasized the creation of taggers from minimal seed sets (Strzalkowski and Wang, 1996; Collins and Singer, 1999; Lin et al., 2003) while another line of inquiry (which we are pursuing) has sought to improve on high-performance baseline taggers (Miller et al., 2004).</S>
			<S sid ="31" ssid = "4">Banko and Brill (2001) suggested that the development of very large training corpora may be most effective for progress in empirical natural language processing.</S>
			<S sid ="32" ssid = "5">Their experiments show a logarithmic trend in performance as corpus size increases without performance reaching an upper bound.</S>
			<S sid ="33" ssid = "6">Recent work has replicated their work on thesaurus extraction (Curran and Moens, 2002) and is-a relation extraction (Ravichandran et al., 2004), showing that collecting data over a very large corpus significantly improves system performance.</S>
			<S sid ="34" ssid = "7">However, (Curran, 2002) and (Curran and Osborne, 2002) claimed that the choice of statistical model is more important than relying upon large corpora.</S>
	</SECTION>
	<SECTION title="Motivation. " number = "3">
			<S sid ="35" ssid = "1">The performance of name taggers has been limited in part by the amount of labeled training data available.</S>
			<S sid ="36" ssid = "2">How can an unlabeled corpus help to address this problem?</S>
			<S sid ="37" ssid = "3">Based on its original training (on the labeled corpus), there will be some tags (in the unlabeled corpus) that the tagger will be very sure about.</S>
			<S sid ="38" ssid = "4">For example, there will be contexts that were always followed by a person name (e.g., &quot;Capt.&quot;) in the training corpus.</S>
			<S sid ="39" ssid = "5">If we find a new token T in this context in the unlabeled corpus, we can be quite certain it is a person name.</S>
			<S sid ="40" ssid = "6">If the tagger can learn this fact about T, it can successfully tag T when it appears in the test corpus without any indicative context.</S>
			<S sid ="41" ssid = "7">In the same way, if a previously-unseen context appears consistently in the unlabeled corpus before known person names, the tagger should learn that this is a predictive context.</S>
			<S sid ="42" ssid = "8">We have adopted a simple learning approach: we take the unlabeled text about which the tagger has greatest confidence in its decisions, tag it, add it to the training set, and retrain the tagger.</S>
			<S sid ="43" ssid = "9">This process is performed repeatedly to bootstrap ourselves to higher performance.</S>
			<S sid ="44" ssid = "10">This approach can be used with any supervised-learning tagger that can produce some reliable measure of confidence in its decisions.</S>
	</SECTION>
	<SECTION title="Baseline Multi-lingual Name Tagger. " number = "4">
			<S sid ="45" ssid = "1">Our baseline name tagger is based on an HMM that generally follows the Nymble model (Bikel et al, 1997).</S>
			<S sid ="46" ssid = "2">Then it uses best-first search to generate NBest hypotheses, and also computes the margin – the difference between the log probabilities of the top two hypotheses.</S>
			<S sid ="47" ssid = "3">This is used as a rough measure of confidence in our name tagging.1 In processing Chinese, to take advantage of name structures, we do name structure parsing using an extended HMM which includes a larger number of states (14).</S>
			<S sid ="48" ssid = "4">This new HMM can handle name prefixes and suffixes, and transliterated foreign names separately.</S>
			<S sid ="49" ssid = "5">We also augmented the HMM model with a set of post-processing rules to correct some omissions and systematic errors.</S>
			<S sid ="50" ssid = "6">The name tagger identifies three name types: Person (PER), Organization (ORG) and Geopolitical (GPE) entities (locations which are also political units, such as countries, counties, and cities).</S>
	</SECTION>
	<SECTION title="Two Semi-Supervised Learning Meth-. " number = "5">
			<S sid ="51" ssid = "1">ods for Name Tagging We have applied this bootstrapping approach to two sources of data: first, to a large corpus of unlabeled data and second, to the test set.</S>
			<S sid ="52" ssid = "2">To distinguish the two, we shall label the first &quot;boot- strapping&quot; and the second &quot;self-training&quot;.</S>
			<S sid ="53" ssid = "3">We begin (Sections 5.1 and 5.2) by describing the basic algorithms used for these two processes.</S>
			<S sid ="54" ssid = "4">We expected that these basic methods would provide a substantial performance boost, but our experiments showed that, for best gain, the additional training data should be related to the target problem, namely, our test set.</S>
			<S sid ="55" ssid = "5">We present measures to select documents (Section 5.3) and sentences (Section 5.4), and show (in Section 6) the effectiveness of these measures.</S>
			<S sid ="56" ssid = "6">5.1 Bootstrapping.</S>
			<S sid ="57" ssid = "7">We divided the large unlabeled corpus into segments based on news sources and dates in order to: 1) create segments of manageable size; 2) separately evaluate the contribution of each segment (using a labeled development test set) and reject those which do not help; and 3) apply the latest updated best model to each subsequent 1 We have also used this metric in the context of rescoring of.</S>
			<S sid ="58" ssid = "8">name hypotheses (Ji and Grishman, 2005); Scheffer et al.</S>
			<S sid ="59" ssid = "9">(2001) used a similar metric for active learning of name tags.</S>
			<S sid ="60" ssid = "10">segment.</S>
			<S sid ="61" ssid = "11">The procedure can be formalized as follows.</S>
			<S sid ="62" ssid = "12">1.</S>
			<S sid ="63" ssid = "13">Select a related set RelatedC from a large cor-.</S>
			<S sid ="64" ssid = "14">pus of unlabeled data with respect to the test set TestT, using the document selection method described in section 5.3.</S>
			<S sid ="65" ssid = "15">2. Split RelatedC into n subsets and mark them.</S>
			<S sid ="66" ssid = "16">C1, C2…Cn.</S>
			<S sid ="67" ssid = "17">Call the updated HMM name tagger NameM (initially the baseline tagger), and a development test set DevT.</S>
			<S sid ="68" ssid = "18">3.</S>
			<S sid ="69" ssid = "19">For i=1 to n.</S>
			<S sid ="70" ssid = "20">(1) Run NameM on Ci; (2) For each tagged sentence S in Ci, if S is tagged with high confidence, then keep S; otherwise remove S; (3) Relabel the current name tagger (NameM) as OldNameM, add Ci to the training data, and retrain the name tagger, producing an updated model NameM; (4) Run NameM on DevT; if the performance gets worse, don’t use Ci and reset NameM = OldNameM; 5.2 Self-training.</S>
			<S sid ="71" ssid = "21">An analogous approach can be used to tag the test set.</S>
			<S sid ="72" ssid = "22">The basic intuition is that the sentences in which the learner has low confidence may get support from those sentences previously labeled with high confidence.</S>
			<S sid ="73" ssid = "23">Initially, we build the baseline name tagger from the labeled examples, then gradually add the most confidently tagged test sentences into the training corpus, and reuse them for the next iteration, until all sentences are labeled.</S>
			<S sid ="74" ssid = "24">The procedure can be formalized as follows.</S>
			<S sid ="75" ssid = "25">1.</S>
			<S sid ="76" ssid = "26">Cluster the test set TestT into n clusters T1,.</S>
			<S sid ="77" ssid = "27">T2, …,Tn, by collecting document pairs with low cross entropy (described in section 5.3.2) into the same cluster.</S>
			<S sid ="78" ssid = "28">2.</S>
			<S sid ="79" ssid = "29">For i=1 to n.</S>
			<S sid ="80" ssid = "30">(1) NameM = baseline HMM name tagger; c. For each tagged sentence S in Ti, if S is tagged with high confidence, add S to the training data; d. Retrain the name tagger NameM with augmented training data.</S>
			<S sid ="81" ssid = "31">At each iteration, we lower the threshold so that about 5% of the sentences (with the largest margin) are added to the training corpus.2 As an example, this yielded the following gradually improving performance for one English cluster including 7 documents and 190 sentences.</S>
			<S sid ="82" ssid = "32">N o . o f it e r a ti o n s N o . o f s e n t e n c e s a d d e d N o . o f t a g s c h a n ge dF M ea su re 0 0 0 9 1 . 4 1 3 7 2 8 9 1 . 9 2 6 9 2 2 9 2 . 1 3 1 0 7 2 1 9 2 . 4 4 1 2 8 1 1 9 2 . 6 5 1 4 6 9 9 2 . 7 6 1 6 3 8 9 2 . 8 7 1 7 8 6 9 2 . 8 8 1 9 0 0 9 2 . 8 Table 1.</S>
			<S sid ="83" ssid = "33">Incremental Improvement from Self-training (English) Self-training can be considered a cache model variant, operating across the entire test collection.</S>
			<S sid ="84" ssid = "34">But it uses confidence measures as weights for each name candidate, and relies on names tagged with high confidence to readjust the prediction of the remaining names, while in a cache model, all name candidates are equally weighted for voting (independent of the learner’s confidence).</S>
			<S sid ="85" ssid = "35">5.3 Unlabeled Document Selection.</S>
			<S sid ="86" ssid = "36">To further investigate the benefits of using very large corpora in bootstrapping, and also inspired by the gain from the “essence” of self-training, which aims to gradually emphasize the predictions from related sentences within the test set, we reconsidered the assumptions of our approach.</S>
			<S sid ="87" ssid = "37">The bootstrapping method implicitly assumes that the unlabeled data is reliable (not noisy) and uniformly useful, namely: (2) While (there are new sentences tagged with confidence higher than a threshold) a. Run NameM on Ti; b. Set an appropriate threshold for margin; 2 To be precise, we repeatedly reduce the threshold by 0.1.</S>
			<S sid ="88" ssid = "38">until an additional 5% or more of the sentences are included; however, if more than an additional 20% of the sentences are captured because many sentences have the same margin, we add back 0.1 to the threshold.</S>
			<S sid ="89" ssid = "39">• The unlabeled data supports the acquisition of new names and contexts, to provide new evidence to be incorporated in HMM and reduce the sparse data problem; • The unlabeled data won’t make the old esti mates worse by adding too many names whose tags are incorrect, or at least are incorrect in the context of the labeled training data and the test data.</S>
			<S sid ="90" ssid = "40">If the unlabeled data is noisy or unrelated to the test data, it can hurt rather than improve the learner’s performance on the test set.</S>
			<S sid ="91" ssid = "41">So it is necessary to coarsely measure the relevance of the unlabeled data to our target test set.</S>
			<S sid ="92" ssid = "42">We define an IR (information retrieval) - style relevance measure between the test set TestT and an unlabeled document d as follows.</S>
			<S sid ="93" ssid = "43">5.3.1 ‘Query set’ construction We model the information expected from the unlabeled data by a &apos;bag of words&apos; technique.</S>
			<S sid ="94" ssid = "44">We construct a query term set from the test corpus TestT to check whether each unlabeled document d is useful or not.</S>
			<S sid ="95" ssid = "45">• We prefer not to use all the words in TestT as key words, since we are only concerned about the distribution of name candidates.</S>
			<S sid ="96" ssid = "46">(Adding off-topic documents may in fact introduce noise into the model).</S>
			<S sid ="97" ssid = "47">For example, if one document in TestT talks about the presidential election in France while d talks about the presidential election in the US, they may share many common words such as &apos;election&apos;, ’voting’, &apos;poll&apos;, and ‘camp’, but we would expect more gain from other unlabeled documents talking about the French election, since they may share many name candidates.</S>
			<S sid ="98" ssid = "48">• On the other hand it is insufficient to only take the name candidates in the top one hypothesis for each sentence (since we are particularly concerned with tokens which might be names but are not so labeled in the top hypothesis).</S>
			<S sid ="99" ssid = "49">So our solution is to take all the name candidates in the top N best hypotheses for each sentence to construct a query set Q. 5.3.2 Cross-entropy Measure Using Q, we compute the cross entropy H(TestT, d) between TestT and d by: H(TestT, d) = −∑ prob(x | TestT) × log2 prob(x | d) x∈Q where x is a name candidate in Q, and prob(x|TestT) is the probability (frequency) of x appearing in TestT while prob(x|d) is the probability of x in d. If H(T, d) is smaller than a threshold then we consider d a useful unlabeled document3.</S>
			<S sid ="100" ssid = "50">5.4 Sentence.</S>
			<S sid ="101" ssid = "51">Selection We don’t want to add all the tagged sentences in a relevant document to the training corpus because incorrectly tagged or irrelevant sentences can lead to degradation in model performance.</S>
			<S sid ="102" ssid = "52">The value of larger corpora is partly dependent on how much new information is extracted from each sentence of the unlabeled data compared to the training corpus that we already have.</S>
			<S sid ="103" ssid = "53">The following confidence measures were applied to assist the semi-supervised learning algorithm in selecting useful sentences for retraining the model.</S>
			<S sid ="104" ssid = "54">5.4.1 Margin to find reliable sentences For each sentence, we compute the HMM hypothesis margin (the difference in log probabilities) between the first hypothesis and the second hypothesis.</S>
			<S sid ="105" ssid = "55">We select the sentences with margins larger than a threshold4 to be added to the training data.</S>
			<S sid ="106" ssid = "56">Unfortunately, the margin often comes down to whether a specific word has previously been observed in training; if the system has seen the word, it is certain, if not, it is uncertain.</S>
			<S sid ="107" ssid = "57">Therefore the sentences with high margins are a mix of interesting and uninteresting samples.</S>
			<S sid ="108" ssid = "58">We need to apply additional measures to remove the uninteresting ones.</S>
			<S sid ="109" ssid = "59">On the other hand, we may have confidence in a tagging due to evidence external to the HMM, so we explored measures beyond the HMM margin in order to recover additional sentences.</S>
			<S sid ="110" ssid = "60">3 We also tried a single match method, using the query set.</S>
			<S sid ="111" ssid = "61">to find all the relevant documents that include any names belonging to Q, and got approximately the same result as cross-entropy.</S>
			<S sid ="112" ssid = "62">In addition to this relevance selection, we used one other simple filter: we removed a document if it includes fewer than five names, because it is unlikely to be news.</S>
			<S sid ="113" ssid = "63">4 In bootstrapping, this margin threshold is selected by.</S>
			<S sid ="114" ssid = "64">test ing on the development set, to achieve more than 93% F- Measure.</S>
			<S sid ="115" ssid = "65">Unlabeled Data Cross-entropy based Document Selection Test Set Cross-entropy based Document Clustering C1 … Ci … Cn T1 … Ti … Tn iÅ1 iÅ1 Yes i &lt; n?</S>
			<S sid ="116" ssid = "66">NameM Å baseline tagger Yes i &lt; n?</S>
			<S sid ="117" ssid = "67">NameM Å baseline tagger OldNameM Å NameM Ci’ÅCi tagged with NameM C ”Å sentences selected from C ’ Ti’Å Ti tagged with NameM Set margin threshold i=i+1 Save Ti’ as system output i i Add Ci” to training corpus i=i+1 Ti”Å sentences selected from Ti’ Retrain NameM Ti” Empty?</S>
			<S sid ="118" ssid = "68">Yes NameM performs better on dev set?</S>
			<S sid ="119" ssid = "69">No NameM Å OldNameM Yes No Add Ti” to training corpus R e t r a i n N a m e M Figure 1.</S>
			<S sid ="120" ssid = "70">Bootstrapping for Name Tagging Figure 2.</S>
			<S sid ="121" ssid = "71">Self-Training for Name Tagging D a t a E n g l i s h C h i n e s e B a s e l i n e T r a i n i n g d a t a AC E0 2,0 3,0 4 98 9,0 03 wo rds Bei jin g Co rp us +A CE 03, 04, 05 1,4 60, 64 8 wo rds U n l a b e l e d D a t a T o t a l 19 6,4 94 do cs in Mar Ju n of 20 03 (69 M wo rds ) fro m AC E0 5 unl ab ele d dat a 41 06 1 do cs in N ov ,D ec of 20 00 , an d Ja n of 20 01 (2 5 M w or ds ) fr o m A C E 05 an d T D T 4 tr an sc ri pt s S e l e c t e d D o c s 62 58 4 do cs (1, 31 4,1 48 Se nte nce s) 14, 53 7 do cs (22 2,3 59 sen ten ces ) S el e ct e d Se nt en ce s 29 0,9 73 sen ten ces (6, 04 9,3 78 wo rds ) 55, 38 5 sen ten ces (1, 12 8,5 05 wo rds ) D e v S e t 20 AC E0 4 tex ts in Oc t of 20 00 90 AC E0 5 tex ts in Oc t of 20 00 T e s t S e t 20 AC E0 4 tex ts in Oc t of 20 00 a n d 8 0 A C E 0 5 t e x t s i n M a r M a y o f 2 0 0 3 ( 3 0 9 3 n a m e s , 1 2 0 5 P E R s , 1 0 2 1 G P E s , 8 6 7 O R Gs ) 90 AC E0 5 tex ts in Oc t of 20 00 (30 93 na me s, 10 13 PE Rs, 69 5 GP Es, 76 9 O R Gs ) Table 2.</S>
			<S sid ="122" ssid = "72">Data Description 5.4.2 Name coreference to find more reliable sentences Names introduced in an article are likely to be referred to again, so a name coreferred to by more other names is more likely to have been correctly tagged.</S>
			<S sid ="123" ssid = "73">In this paper, we use simple coreference resolution between names such as substring matching and name abbreviation reso lution.</S>
			<S sid ="124" ssid = "74">We present in section 6.2 – 6.4 the overall performance of precision (P), recall (R) and F- measure (F) for both languages, and also some diagnostic experiment results.</S>
			<S sid ="125" ssid = "75">For significance testing (using the sign test), we split the test set into 5 folders, 20 texts in each folder of English, and 18 texts in each folder of Chinese.</S>
			<SUBSECTION>6.2 Overall Performance.</SUBSECTION>
			<S sid ="126" ssid = "76">Table 3 and Table 4 present the overall perform 6In the bootstrapping method we apply single anceby applying the two semi-supervised learn document coreference for each individual unlabeled text.</S>
			<S sid ="127" ssid = "77">In self-training, in order to further benefit from global contexts, we consider each cluster of relevant texts as one single big document, and then apply cross-document coreference.</S>
			<S sid ="128" ssid = "78">Assume S is one sentence in the document, and there are k names tagged in S: {N1, N2 .…..</S>
			<S sid ="129" ssid = "79">Nk}, which are coreferred to by {CorefNum1, CorefNum2, …CorefNumk} other names separately.</S>
			<S sid ="130" ssid = "80">Then we use the following average name coreference count AveCoref as a confidence measure for tagging S:5 ing methods, separately and in combination, to our baseline name tagger.</S>
			<S sid ="131" ssid = "81">L e a r n e r P R F B a s e l i n e 87.</S>
			<S sid ="132" ssid = "82">3 87.</S>
			<S sid ="133" ssid = "83">6 87 .4 B o o t s t r a p p i n g w i t h d a t a s e l e c t i o n 88.</S>
			<S sid ="134" ssid = "84">2 88.</S>
			<S sid ="135" ssid = "85">6 88 .4 S e l f t r a i n i n g 88.</S>
			<S sid ="136" ssid = "86">1 88.</S>
			<S sid ="137" ssid = "87">4 88 .2 B o o t s t r a p p i n g w i t h d a t a s e l e c t i o n + S e l f t r a i n i n g 89.</S>
			<S sid ="138" ssid = "88">0 89.</S>
			<S sid ="139" ssid = "89">2 89 .1 Table 3.</S>
			<S sid ="140" ssid = "90">English Name Tagger AveCoref k = (∑ CorefNumi ) / k i=1 5.4.3 Name count and sentence length to remove uninteresting sentences In bootstrapping on unlabeled data, the margin criterion often selects some sentences which are too short or don’t include any names.</S>
			<S sid ="141" ssid = "91">Although they are tagged with high confidence, they may make the model worse if added into the training data (for example, by artificially increasing the probability of non-names).</S>
			<S sid ="142" ssid = "92">In our experiments we don’t use a sentence if it includes fewer than six words, or doesn’t include any names.</S>
			<S sid ="143" ssid = "93">5.5 Data Flow.</S>
			<S sid ="144" ssid = "94">We depict the above two semi-supervised learning methods in Figure 1 and Figure 2.</S>
	</SECTION>
	<SECTION title="Evaluation Results and Discussions. " number = "6">
			<S sid ="145" ssid = "1">6.1 Data.</S>
			<S sid ="146" ssid = "2">We evaluated our system on two languages: English and Chinese.</S>
			<S sid ="147" ssid = "3">Table 2 shows the data used in our experiments.</S>
			<S sid ="148" ssid = "4">5 For the experiments reported here, sentences were selected.</S>
			<S sid ="149" ssid = "5">if AveCoref &gt; 3.1 (or 3.1×number of documents for cross document coreference) or the sentence margin exceeded the margin threshold.</S>
			<S sid ="150" ssid = "6">Table 4.</S>
			<S sid ="151" ssid = "7">Chinese Name Tagger For English, the overall system achieves a 13.4% relative reduction on the spurious and incorrect tags, and 12.9% reduction in the missing rate.</S>
			<S sid ="152" ssid = "8">For Chinese, it achieves a 16.9% relative reduction on the spurious and incorrect tags, and 16.9% reduction in the missing rate.7 For each of the five folders, we found that both bootstrapping and self-training produced an improvement in F score for each folder, and the combination of two methods is always better than each method alone.</S>
			<S sid ="153" ssid = "9">This allows us to reject the hypothesis that these 6 Only names which exactly match the key in both extent.</S>
			<S sid ="154" ssid = "10">and type are counted as correct; unlike MUC scoring, no partial credit is given.</S>
	</SECTION>
	<SECTION title="The performance achieved should be considered in light of. " number = "7">
			<S sid ="155" ssid = "1">human performance on this task.</S>
			<S sid ="156" ssid = "2">The ACE keys used for the evaluations were obtained by dual annotation and adjudication.</S>
			<S sid ="157" ssid = "3">A single annotator, evaluated against the key, scored F=93.6% to 94.1% for English and 92.5% to 92.7% for Chinese.</S>
			<S sid ="158" ssid = "4">A second key, created independently by dual annotation and adjudication for a small amount of the Eng lish data, scored F=96.5% against the original key.</S>
			<S sid ="159" ssid = "5">improvements were random at a 95% confidence level.</S>
			<S sid ="160" ssid = "6">6.3 Analysis of Bootstrapping.</S>
			<S sid ="161" ssid = "7">6.3.1 Impact of Data Size Figure 3 and 4 below show the results as each segment of the unlabeled data is added to the training corpus.</S>
			<S sid ="162" ssid = "8">Figure 3.</S>
			<S sid ="163" ssid = "9">Impact of Data Size (English) Figure 4.</S>
			<S sid ="164" ssid = "10">Impact of Data Size (Chinese) We can see some flattening of the gain at the end, particularly for the larger English corpus, and that some segments do not help to boost the performance (reflected as dips in the Dev Set curve and gaps in the Test Set curve).</S>
			<S sid ="165" ssid = "11">6.3.2 Impact of Data Selection In order to investigate the contribution of document selection in bootstrapping, we performed diagnostic experiments for Chinese, whose results are shown in Table 5.</S>
			<S sid ="166" ssid = "12">All the bootstrapping tests (rows 24) use margin for sentence selection; row 4 augments this with the selection methods described in sections 5.4.2 and 5.4.3.</S>
			<S sid ="167" ssid = "13">Table 5.</S>
			<S sid ="168" ssid = "14">Impact of Data Selection (Chinese) Comparing row 2 with row 3, we find that notusing document selection, even though it multi plies the size of the corpus, results in 0.3% lower performance (0.30.4% loss for each folder).</S>
			<S sid ="169" ssid = "15">This leads us to conclude that simply relying upon large corpora is not in itself sufficient.</S>
			<S sid ="170" ssid = "16">Effective use of large corpora demands good confidence measures for document selection to remove off- topic material.</S>
			<S sid ="171" ssid = "17">By adding sentence selection (results in row 4) the system obtained 0.5% further improvement in F-Measure (0.40.7% for each folder).</S>
			<S sid ="172" ssid = "18">All improvements are statistically significant at the 95% confidence level.</S>
			<S sid ="173" ssid = "19">6.4 Analysis of Self-training.</S>
			<S sid ="174" ssid = "20">We have applied and evaluated different measures to extract high-confidence sentences in self- training.</S>
			<S sid ="175" ssid = "21">The contributions of these confidence measures to F-Measure are presented in Table 6.</S>
			<S sid ="176" ssid = "22">C o n f i d e n c e M e a s u r e E n gl is h C hi ne se B a s e l i n e 8 7 . 4 8 7 . 9 M a r g i n 8 7 . 8 8 8 . 3 M a r g i n + s i n g l e d o c n a m e c o r e f e r e n c e 8 8 . 0 8 8 . 7 M a r g i n + c r o s s d o c n a m e c o r e f e r e n c e 8 8 . 2 8 8 . 9 Table 6.</S>
			<S sid ="177" ssid = "23">Impact of Confidence Measures It shows that Chinese benefits more from adding name coreference, mainly because there are more coreference links between name abbreviations and full names.</S>
			<S sid ="178" ssid = "24">And we also can see that the margin is an important measure for both languages.</S>
			<S sid ="179" ssid = "25">All differences are statistically significant at the 95% confidence level except for the gain using cross-document information for the Chinese name tagging.</S>
			<S sid ="180" ssid = "26">7 Conclusions and Future Work.</S>
			<S sid ="181" ssid = "27">This paper demonstrates the effectiveness of two straightforward semi-supervised learning methods for improving a state-of-art name tagger, and investigates the importance of data selection for this application.</S>
			<S sid ="182" ssid = "28">Banko and Brill (2001) suggested that the development of very large training corpora may be central to progress in empirical natural language processing.</S>
			<S sid ="183" ssid = "29">When using large amounts of unlabeled data, as expected, we did get improvement by using unsupervised bootstrapping.</S>
			<S sid ="184" ssid = "30">However, exploiting a very large corpus did not by itself produce the greatest performance gain.</S>
			<S sid ="185" ssid = "31">Rather, we observed that good measures to select relevant unlabeled documents and useful labeled sentences are important.</S>
			<S sid ="186" ssid = "32">The work described here complements the active learning research described by (Scheffer et al., 2001).</S>
			<S sid ="187" ssid = "33">They presented an effective active learning approach that selects “difficult” (small margin) sentences to label by hand and then add to the training set.</S>
			<S sid ="188" ssid = "34">Our approach selects “easy” sentences – those with large margins – to add automatically to the training set.</S>
			<S sid ="189" ssid = "35">Combining these methods can magnify the gains possible with active learning.</S>
			<S sid ="190" ssid = "36">In the future we plan to try topic identification techniques to select relevant unlabeled documents, and use the downstream information extraction components such as coreference resolution and relation detection to measure the confidence of the tagging for sentences.</S>
			<S sid ="191" ssid = "37">We are also interested in applying clustering as a pre- processing step for bootstrapping.</S>
	</SECTION>
	<SECTION title="Acknowledgment">
			<S sid ="192" ssid = "38">This material is based upon work supported by the Defense Advanced Research Projects Agency under Contract No.</S>
			<S sid ="193" ssid = "39">HR001106-C-0023, and the National Science Foundation under Grant IIS 00325657.</S>
			<S sid ="194" ssid = "40">Any opinions, findings and conclusions expressed in this material are those of the authors and do not necessarily reflect the views of the U. S. Government.</S>
	</SECTION>
</PAPER>
