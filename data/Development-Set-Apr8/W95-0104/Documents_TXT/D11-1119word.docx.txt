Exploiting Syntactic and Distributional  Information  for 
Spelling Correction with Web-Scale N-gram Models
Wei Xuc,∗ Joel Tetreaulta  Martin Chodorowb Ralph Grishmanc Le Zhaod 
aEducational Testing Service, Princeton, NJ, USA jtetreault@ets.org
bHunter College of CUNY, New York, NY, USA 
martin.chodorow@hunter.cuny.edu cNew 
York University, NY, USA
{xuwei,grishman}@cs.nyu.edu dCarnegie 
Mellon University, Pittsburgh, PA, USA 
lezhao@cs.cmu.edu




Abstract

We propose a novel way of incorporating de- 
pendency  parse and word co-occurrence in- 
formation into a state-of-the-art  web-scale n- 
gram model for spelling correction. The syn- 
tactic and distributional information provides 
extra evidence in addition to that provided by a 
web-scale n-gram corpus and especially helps 
with  data sparsity problems.  Experimental 
results show that introducing syntactic fea- 
tures into n-gram based models significantly 
reduces errors by up to 12.4% over the current 
state-of-the-art. The word co-occurrence in- 
formation shows potential but only improves 
overall accuracy slightly.


1   Introduction

The function of context-sensitive text correction is 
to identify word-choice errors in text (Bergsma et 
al., 2009). It can be viewed  as a lexical disambigua- 
tion task (Lapata and Keller, 2005), where a system 
selects from a predefined confusion  word set, such 
as {affect,  effect}  or {complement,  compliment}, 
and provides the most appropriate word choice given 
the context. Typically, one determines if a word has 
been used correctly based on lexical, syntactic and 
semantic information from the context of the word. 
One of the top performing models of spelling cor- 
rection (Bergsma et al., 2010) is based on web-scale 
n-gram counts, which reflect both syntax and mean- 
ing. However,  even with a large-scale n-gram  cor- 
pus, data sparsity can hurt performance in two ways.
   ∗ This work was done when the first author was an intern 
for Educational Testing Service.


First, n-gram based methods require exact word and 
order matches.  If there is a low frequency word in 
the context,  such as a person’s  name, there will be 
little,  if  any, evidence in the n-gram data to sup- 
port the usage. Second, if the target confusable word 
is rare, there will not be enough n-gram support or 
training data to render a confident decision. Because 
of the data sparsity problem, language modeling is 
not always sufficient to capture the meaning of the 
sentence and the correct usage of the word.

  Take  a sentence from The New York Times 
(NYT) for example: “‘This fellow’s won a war,’ the 
dean of the capital’s  press corps, David Broder, an- 
nounced on ‘Meet the Press’ after complimenting 
the president on the ‘great sense of authority and 
command’ he exhibited in a flight suit.”   Unfortu- 
nately, neither the phrase “complementing the pres- 
ident” nor “complimenting the president” exists in 
the web-scale Google N-gram corpus (Brants and 
Franz, 2006).  The n-gram models decide solely 
based on the frequency of the bi-grams “after com- 
ple(i)menting” and “comple(i)menting the”, which 
are common  usages for both words. The real ques- 
tion is whether we are more likely to “compliment” 
or “complement” a person, the “president”. Several 
clues could help us answer that question.  A de- 
pendency parser can identify the word “president” 
as  the subject of “compliment” or “complement” 
which also may be the case in some of the train- 
ing data. Lexical co-occurrence (Edmonds, 1997) 
and semantic word relatedness measurements, such 
as Random  Indexing (Sahlgren, 2006), could pro- 
vide evidence that “compliment” is more likely to 
co-occur with “president” than “complement”. Fur-




1291


Proceedings of the 2011 Conference on Empirical  Methods in Natural  Language Processing, pages 1291–1300, 
Edinburgh, Scotland, UK, July 27–31, 2011. Qc 2011 Association for Computational Linguistics


thermore, some important clues can be quite distant 
from the target word, e.g. outside the 9-word context 
window Bergsma et al. (2010) and Carlson (2007) 
used. Consider another sentence in the NYT corpus, 
“GM says the addition of OnStar, which includes  a 
system that automatically notifies an OnStar opera- 
tor if the vehicle is involved in a collision, comple- 
ments the Vue’s top five-star safety rating for the 
driver and front passenger in both front- and side- 
impact crash tests.” The dependency parser finds the 
object of “complement” is “rating”, which is outside 
the 9-word window.
  We propose enhancing state-of-the-art web-scale 
n-gram models for spelling correction with syntac- 
tic structures and distributional  information.  For our 
work, we build on a baseline system that combines 
n-gram and lexical features (Bergsma et al., 2010). 
Specifically, this paper makes the following contri- 
butions:

1. We  show that the baseline system can be 
improved by augmenting it with dependency 
parse features.

2. We show that the impact of parse features can 
be further augmented when combined with dis- 
tributional information, specifically word co- 
occurrence information.

  In  the following  section, we describe related 
work and how our approach differs from these ap- 
proaches. In Sections 3 and 4, we discuss our meth- 
ods for using parse features and word co-occurrence 
information. In Section 5, we present experimental 
results and analysis.

2   Related Work

A  variety of approaches  have  been proposed for 
context-sensitive spelling correction ranging from 
semantic methods to machine learning classifiers to 
large-scale n-gram models.
  Some semantics-based systems have been devel- 
oped based on an intuitive assumption that the in- 
tended word is more likely to be semantically coher- 
ent with the context than is a spelling error. Jones 
and Martin (1997) made use of the semantic simi- 
larity produced by Latent Semantic Analysis. Bu- 
danitsky and Hirst (2001) investigated the effective- 
ness of predicting words based on different semantic


similarity/distance measures in WordNet. Both sys- 
tems report performance that is lower than systems 
developed more recently.
  A variety of machine-learning methods have been 
proposed in spelling correction and preposition and 
article error correction  fields, such as Bayesian clas- 
sifiers (Golding, 1995; Golding and Roth, 1996), 
Winnow-based learning (Golding and Roth, 1999), 
decision lists (Golding, 1995), transformation-based 
learning (Mangu and Brill, 1997), augmented mix- 
ture models (Cucerzan  and Yarowsky, 2002) and 
maximum entropy classifiers (Izumi et al., 2003; 
Han et al., 2006; Chodorow et al., 2007; Tetreault 
and Chodorow, 2008; Felice and Pulman, 2008). 
Despite their differences,  these approaches mainly 
use contextual  features to capture the lexical,  seman- 
tic and/or syntactic environment of the target word.
  The use of distributional similarity measures for 
spelling correction has been previously  explored in 
(Mohammad and Hist, 2006). In our work, distribu- 
tional similarity is not the primary contribution but 
we show the impact it can have when used in con- 
junction with a large scale n-gram model and with 
parse features,  which allows the system to select 
words outside the local window for distributional 
similarity.  In the prior work, the words for distri- 
butional similarity are constrained to the local win- 
dow, and positional information of the words is not 
encoded.
  Recent work (Carlson and Fette, 2007; Gamon 
et al., 2008; Bergsma  et al., 2009) has demon- 
strated that large-scale  language  modeling is ex- 
tremely helpful for contextual spelling correction 
and other lexical disambiguation tasks. These sys- 
tems make the word choice depending on how fre- 
quently each candidate word has been seen in the 
given context in web-scale data. As n-gram data has 
become more readily available,  such as the Google 
N-gram Corpus, the likelihood of a word being used 
in a certain context can be better estimated.
  Bergsma et al.  (2009; 2010) presented a series 
of simple but powerful models which relied heavily 
on web-scale n-gram counts. From the Google Web 
N-gram Corpus, they retrieve counts of n-grams of 
different sizes (2-5) and positions that span the tar- 
get word w0 within a  window of 9 words.  For 
example, for the following sentence:  “The system 
tried to decide {among, between} the two confus-


able words.”, the method would extract the five 5- 
gram patterns, shown below in Figure 2, where w0 
can be either word in the confusion set {among, be- 
tween} in this particular example.  Similarly, there 
are four 4-grams, three 3-grams, and two 2-grams, 
in total, 14 n-grams for each of the words in the con- 
fusion set.

system tried to decide w0
tried to decide w0 the
to decide w0 the two
decide w0 the two confusable
w0 the two confusable words


  We  briefly  describe three of  Bergsma et al.’s 
(2009; 2010) best systems below, which are reported 
to achieve state-of-the-art accuracy (NG = n-gram; 
LEX = lexical).


1. sumLM: For each candidate word, (Bergsma 
et al., 2009) sum the log-counts of all 14 pat- 
terns filled with the candidate, and choose the 
candidate with the highest total.

2. NG: Bergsma et al.  (2009) exploit  each can- 
didate’s 14 log-counts of n-gram patterns  as 
features in a Support  Vector Machine (SVM) 
model.

3. NG+LEX: Bergsma et al. (2010) augment the 
NG model with lexical features (described in 
detail in Section 3.1).


  Bergsma et al. (2009; 2010) restricted their exper- 
iments to only five confusion sets where the reported 
performance in (Golding and Roth, 1999) was below
90%: {among, between}, {amount, number}, {cite, 
sight, site}, {peace, piece} and {raise, rise}.  They 
reported that the SVM model with NG features out- 
performed its unsupervised version, sumLM. How- 
ever, the limited confusion word sets they evaluated 
may not comprehensively  represent the word usage 
errors that writers typically make. In this paper, we 
test nine additional commonly confused word pairs 
to expand the scope of the evaluation.  These words 
were selected based on their lower frequencies com- 
pared to the five pairs in the above work (as shown 
later in Table 2).


3   Enhanced N-gram Models with Parse
Features

To our knowledge, only (Elmi and Evans, 1998) 
have used parsing for spell correction. They focus 
on using  a parser as a filter to discriminate between 
possible real-world corrections where the part-of- 
speech differs. In our work, we show that parse fea- 
tures are effective  when used directly in the classifi- 
cation mode (as opposed to as a final filter) to select 
the best correction  regardless of whether or not the 
part-of-speech of the choices differ.
  Statistical  parsers have also seen limited use in 
the sister tasks of preposition and article error detec- 
tion (Hermet et al., 2008; Lee and Knutsson, 2008; 
Felice and Pulman, 2009; Tetreault et al., 2010) 
and verb sense disambiguation (Dligach and Palmer,
2008). In those instances where parsers have been 
used, they have mainly provided shallow analyses 
or relations involving specific target words, such as 
a  preposition or verb.  Unlike  preposition errors, 
spelling errors can occur in any word.
  In this paper, we propose  a novel way to incor- 
porate the parse into spelling correction, applying 
the parser to sentences filled by each candidate word 
equivalently and extracting salient features. This 
overcomes two problem in the existing methods: 1) 
the parse trees of the same sentence filled by differ- 
ent confusion words can be different. However, in 
the test phase, we do not know which word should 
be put in the sentences to create parse features for 
test examples.    Previous studies (Tetreault et al.,
2010) failed to discuss this issue.  2) Some existing 
work (Whitelaw et al., 2009; Rozovskaya and Roth,
2010) in the text correction field introduced artificial 
errors into training data to adapt the system to bet- 
ter handle ill-formed text. But this method will en- 
counter serious data sparsity problems when facing 
rare words.

3.1   Baseline System

We chose one of the leading spelling correction sys- 
tems, (Bergsma et al., 2010), as our primary base- 
line.  As noted earlier, it is an SVM-based system 
combining web-scale n-gram counts (NG) and con- 
textual words (LEX) as features.  To simplify the ex- 
planation, throughout the paper, we will  only con- 
sider the situation with two confusion words. The


problem with more than two words in pre-defined 
confusion  sets can be solved similarly by using a 
one-vs.-all strategy. As we mentioned in Section 2, 
NG features include log-counts of 3-to-5-gram pat- 
terns for each candidate word with the given context. 
LEX features can be broken down into three sub- 
categories:  1) bag-of-words (words at all positions 
in a 9-word window around the target word), 2) in- 
dicators for the words preceding or following the tar- 
get word, and 3) indicators for all n-grams and their 
positions. For the sentence “The system tried to de- 
cide {among, between} the two confusable words.”, 
examples of bag-of-word features would be “tried”, 
“two”,  etc., the two positional bigrams would be 
“decide” and “the”, and examples of the n-gram fea- 
tures would be right-trigram  = “among the two” and 
left-4-gram = “tried to decide between”.

3.2   Parse Features

The benefit of introducing dependency  parse fea- 
tures is that 1) parse features capture contextual  in- 
formation in a larger context window; 2) parse fea- 
tures specify which words in the context are salient 
to the usage of the target word while purely lexi- 
cally based approaches treat all words in the context 
equally. We use the Stanford dependency parser (de 
Marneffe et al., 2006) to extract six relevant feature 
classes.
Parse Features (PAR):

1. relation names (target word as head)

2. complement of the target word

3. combination of 1 and 2

4. relation names (target word as complement)

5. head of the target word

6. combination of 4 and 5

  Each of these  six classes  of PAR features can 
contain zero to many values, since the target word 
can be involved in none to multiple grammatical 
relations and features of different filler words are 
merged together. The PAR features, like the LEX 
features, are binary. In Table 1, we present the parse 
features for an example sentence. The parse fea- 
tures here are listed as string values, but are later


converted into binary numbers in the vectors for the
SVM model.

4   Distributional Word Co-occurrence

Though lexical and parse features are complemen- 
tary to n-gram models, they are learned from a nor- 
mal training corpus and may not have enough cov- 
erage due to data sparsity.  Take a sentence from the 
NYT for example: “An economist, he began his ca- 
reer as a professor  – he is still called ‘the professor,’ 
by friends  as a compliment and by foes as an insult – 
and taught at Harvard and Stanford .” If the most in- 
dicative word “friends” does not appear or does not 
appear enough times in the local context or depen- 
dencies with “compliment” as compared  to “com- 
plement” in the training corpus, then the classifier 
may be unable to make the correct selection.
  It is impractical and computationally costly to en- 
large the training corpus without limit  to include 
all possible language phenomena. A good compro- 
mise is to use word co-occurrence information  from 
web-scale data. The other option is to make use of 
high-order word co-occurrence, which is included in 
many semantic word relatedness measures, such as 
Latent Semantic Analysis (LSA) (Landauer et al.,
1998; Deerwester et al., 1990) or Random Indexing, 
both of which can be estimated from a moderate-size 
corpus.
  Our intuition is to choose  the confusion word 
which is most relevant to a given context. We define 
the salient words in context  as a set M=m1, m2, m3,
..., and the relevance between two words  as a func- 
tion Relevance(w1, w2), which can either be calcu- 
lated from word co-occurrence or Random Indexing. 
The score of each candidate word c in the confusion 
set given a context with meaningful words M is cal- 
culated by the following formula:
Score(c) = )"' Relevance(c, m)
m∈M

  In this paper, we experiment with first-order word 
co-occurrence and Random Indexing as relevance 
measures.  And we define salient contextual words 
as heads  or complements in the dependency rela- 
tions with the target word. In this way, we use the 
parse information to constrain the two distribution 
models. Thus the word co-occurrence information



Fe
atu
re 
Na
m
e
PA
R 
Fe
atu
res 
(c
o
m
pli
m
en
t)
PA
R 
Fe
atu
res 
(c
o
m
ple
m
en
t)
1. 
He
ad 
Re
lati
on 
Na
m
e
cc
o
m
p
ap
po
s
2. 
He
ad 
of 
Re
lati
on
sa
ys
col
lisi
on
s
3. 
He
ad 
Co
m
bin
ati
on
cc
o
m
p 
sa
ys
ap
po
s 
col
lisi
on
s
4. 
Co
mp 
Re
lati
on 
Na
m
e
ns
ubj
de
p
5. 
Co
mp 
of 
Re
lati
on
ad
diti
on
rat
ing
6. 
Co
mp 
Co
m
bin
ati
on
ns
ub 
ad
diti
on
de
p 
rat
ing

Table 1: Parse Feature Example  for the sentence:  “GM says the addition of OnStar, which includes  a system that 
automatically notifies an OnStar operator if the vehicle is involved in a collision, complements the Vue’s top five-star 
safety rating for the driver and front passenger in both front- and side-impact crash tests.”




considerably overlaps with some values of the PAR 
features, but provides extra evidence from web-scale 
data rather than a limited amount of training data.

4.1   First-order Word Co-occurrence

The  relevance based on  first-order  word  co- 
occurrence is calculated from the Google Web 5- 
gram Corpus in a fashion similar to how we dealt 
with n-gram counts in the previous section. Given 
two words, w1 and w2, we consider all 8 possible 
patterns that appear in a local context (5-word win- 
dow), where we use wildcard (*) to indicate any to- 
ken:

w1 w2
w1  *  w2
w1  *	*  w2
w1  *	*	*  w2 
w2 w1
w2  *  w1
w2  *	*  w1
w2  *	*	*  w1


  The relevance is then calculated by summing the 
logarithm of each of the 8 different counts. Finally, 
we compare the score of each candidate word and 
output the one with higher score.

4.2   Random Indexing

The relevance  scores based on Random Indexing 
are provided by a tool FRanI (Higgins, 2004) and 
a model trained on the Touchstone Applied Science 
Associates (TASA) corpus which contains 750k sen- 
tences and covers diverse topics (from a diversity  of 
textbooks up to the college level). Take the sentence 
at the beginning of this section for example, where 
only the words “a” and “friends” are related to the


target word (either “complement” or “compliment”) 
by either relevance measure. The relevance  based 
on Random Indexing for (complement, friends) is
0.08, (compliment, friends) is 0.19 and both (com- 
pliment, a) and (complement, a) are 0 because “a” 
is in the stop word list.  Meanwhile, the relevance 
based on first order word co-occurrence for (com- 
pliment, friends) is 7.39, (complement, friends) is
5.38, (compliment, a) is 13.25, and (complement, a) 
is 13.42. The system with either kind of relevance 
outputs “compliment”.

4.3   System Combination

Since the numeric measurement  of word co- 
occurrence is not as specific as the PAR features and 
less trustworthy, adding word co-occurrence infor- 
mation as features  into the classifier along with n- 
gram counts, lexical and parse features will hurt the 
overall performance. It is more practical to combine 
the two approaches in the following fashion:

1. When the SVM classifier (using NG, LEX and 
PAR features) has high confidence (over a cer- 
tain threshold) in the output label, output that 
label;

2. Otherwise, output the results of  the word 
relatedness/co-occurrence-based system.

5   Evaluation

We evaluate the effectiveness of syntactic and dis- 
tributional information on spelling correction. The 
performance of the system is measured by accu- 
racy: the percentage of sentences in the test data 
for which the system chooses the correct word. We 
compare our results against two baselines: 1) MA- 
JOR chooses the most frequent candidate from the



confusion set in the training corpus, and 2) Bergsma 
et al.’s (2010) best systems, NG+LEX. We include 
inflectional variants (“-ing”,  “-ed”, “-s”,  “-ly”)  of 
confusion words in the evaluation,  such as comple- 
menting, complimenting in addition to complement, 
compliment,  because this better corresponds to the 
range of errors that may be encountered in actual 
use and thus increases the scope of the system  as a 
real world application. Also following Bergsma et 
al. (2010), we use a linear SVM, more exactly, the 
L2-regularized L2-loss dual SVM in LIBLINEAR 
(Fan et al., 2008). Unlike Bergsma et al., who used 
development data to optimize parameters, we always 
use default parameters, since training data is limited 
for many of the words we are dealing with.

5.1   Data

Following Bergsma et al.   (2009; 2010), the test 
examples are extracted from The New York Times 
(NYT) portion of Gigaword1, but constrained to a
9-month publication time frame from October 2005 
to July 2006. Unlike Bergsma et al.  who use the 
same source as training  data for the lexical features, 
our training data (for both lexical  and parse features) 
comes from larger and more diverse news sources. 
We use the very large database from Sekine’s n-gram 
search engine (Sekine, 2008) as training  data, which 
consists of 1.9B words of newspaper text spanning
89 years from NYT, BBC, WSJ, Xinhua, etc.
  We evaluate our systems on 5 confusion  sets from 
Bergsma et al. (2009; 2010) and 9 commonly con- 
fused word pairs with moderate frequency in daily 
usage (randomly selected from those listed in En- 
glish educational resources2). Shown in Table 2, 
these 9 sets of words appear much less frequently 
than the words selected  by Bergsma  et al., even 
given the fact that we are using a considerably  large 
training corpus.
  For each confusable  word pair, sentences  that 
contain either of the words are extracted to form 
training and test data. The word that appears in the 
original sentences of the news article is treated  as 
the gold standard.  For frequently occurring confu- 
sion word sets used by Bergsma et al., we extract 
up to 10k examples for testing, and up to 100k ex-

1 Available from the LDC as LDC2003T05
     2 Such    as      an    English    learning    blog    post    at 
http://elisaenglish.pixnet.net/blog/post/1335194



W
or
d 
Co
nfu
sio
n 
Se
t
# 
in 
Tr
ain
ing 
Co
rp
us
ad
ver
se 
/ 
av
ers
e
13.
5k 
/ 
1.
8k
ad
vic
e / 
ad
vis
e
62
.k / 
12
.9k
all
usi
on 
/ 
illu
sio
n
1.
0k 
/ 
5.
4k
co
mp
le
me
nt / 
co
m
pli
m
en
t
6.
8k 
/ 
3.
1k
co
nfi
da
nt / 
co
nfi
de
nt
2.
4k 
/ 
63
.6k
de
ser
t / 
de
ss
ert
24.
7k 
/ 
3.
7k
dis
cre
et / 
dis
cr
et
e
0.
7k 
/ 
2.
4k
eli
cit 
/ 
illi
cit
1.
9k 
/ 
10
.0k
sta
tio
nar
y / 
sta
tio
ne
ry
2.
5k/
2.
3k
wa
nd
er 
/ 
wo
nd
er
3.
3k 
/ 
39
.5k

Table 2: Training Data Sizes for Common ESL Confused
Words


amples for training. For the 9 less frequent confu- 
sion word sets, we extract all the unique examples 
for training and testing from the above sources. The 
spelling correction system is evaluated by measur- 
ing its accuracy in comparison to the gold standard 
in test data. The error rate is the complement of ac- 
curacy.
  Following Carlson et al.   (2007) and Bergsma 
et al.  (2009; 2010), we obtain the n-gram counts 
from the Google Web 1T 5-gram Corpus (Brants and 
Franz, 2006).

5.2   Experimental  Results

We present the results for each set separately be- 
cause each set may behave very differently, depend- 
ing upon its frequency, part-of-speech, number of 
senses and other differences between the words in 
each confusion set. The overall accuracy across con- 
fusion sets is also presented to show the effective- 
ness of different approaches.  The results are tested 
for statistical significance using McNemar’s test of 
correlated proportions. The performance differences 
are marked as significant  when p < 0.05.

5.2.1   Effectiveness of Parse Features
  We exploit the n-gram counts (NG), lexical fea- 
tures (LEX) of Bergsma et al. (2010) and our own 
parse features (PAR) in linear SVM models.
  The first comparison is between the supervised 
learning systems  with  LEX  and LEX+PAR.  As 
shown in Table 3, by exploiting our unique parse 
features, for the total 14 confusion sets, the accuracy 
increases on 12 sets and decreases on 2 sets. Over- 
all, the spelling correction accuracy improves an ab-


solute 1.35% for our 9 confusion  sets and 0.60% for
Bergsma et al.’s 5 confusion sets.
  The second comparison is to see how parse fea- 
tures interact with n-gram count features in  a su- 
pervised classifier. The best system from (Bergsma 
et al., 2010) is listed in the table as ”NG+LEX”. 
As shown in Table 3, the parse features proved to 
be beneficial when augmenting this baseline, except 
for the decrease in accuracy on adverse, averse by 
only 2 cases out of 368, and among, between by
2 cases out of 10227. For all other confusion  sets, 
parse features decrease the error rate by as much  as
2.74% (absolute) and as much as 38.5%  (relative). 
Improvements are statistically  significant on all con- 
fusion sets together, although for each separate set, 
improvements are significant  on only 5 sets, in part 
due to an insufficient  number of test cases.
  The reason that parse features are occasionally not 
helpful is because they sometimes include an un- 
common word in dependencies, which happens to 
appear once with the wrong word but not with the 
correct word in the training data; or they sometimes 
include too common words, which bias the classifier 
in favor of the more frequent word in the confusion 
set. We also noticed that lexical features are not al- 
ways helpful when added to n-gram count features, 
even for in-domain applications (i.e., with training 
data and test data coming from the same domain or 
corpus), as marked by underlines. However, lexical 
and parse features together  show more significant 
and constant improvement over n-gram count-based 
models, as marked by α.
  Of the six systems, every system that uses parse 
features gets the example correct in Section 1, “com- 
plementing the president”; LEX by itself also gets 
the example correct, but NG and NG+LEX fail.
  In summary, our system NG+LEX+PAR outper- 
forms the state-of-the-art  system NG+LEX. It re- 
duces the error rate by 12.4% across our 9 confusion 
sets and by 8.4% across Bergsma et al.’s 5 confusion 
sets. Both improvements are significant  (p < 0.05) 
by the McNemar test. In addition, while NG+LEX 
is not always better than NG, NG+LEX+PAR is con- 
sistently better than NG.

5.2.2   Impact of Word Co-occurrence
  The LIBLINEAR  tool does not provide probabil- 
ity estimates for SVM models but Logistic Regres-


sion can. In this set of experiments, we train a Logis- 
tic Regression model with NG+LEX+PAR features 
and empirically set the confidence threshold at 0.6, 
as described  in Section 4, based on the performance 
on two word pairs. In the combined system, when 
the Logistic Regression model estimates a probabil- 
ity higher than the threshold we output its results, 
otherwise we output the result of the system based 
on word co-occurrence.


  Surprisingly, although Random Indexing takes 
into account more information than first-order word 
co-occurrence, it lowered overall performance sub- 
stantially. Thus in Table 4, we only present results 
of using first-order word co-occurrence rather than 
Random Indexing. For all 12 confusion  sets, distri- 
butional word co-occurrence information improves
9 sets and hurts 5 sets. Overall, it reduces the er- 
ror rate slightly by 0.2% for our 9 sets and 1.5% for 
Bergsma et al.’s sets.


  We  believe  there are two  reasons  why  Ran- 
dom Indexing fared worse than first-order word 
co-occurrence:  1) Random Indexing considers co- 
occurrence on a document  level, while our first- 
order word co-occurrence is limited to a 5-word  win- 
dow context. The latter is more suitable to context- 
sensitive spelling correction. 2) The model for Ran- 
dom Indexing is trained on a relatively small size 
corpus compared to the web-scale data we used to 
get n-gram count features for the classifier and thus 
is not able to introduce much new evidence besides 
the information carried by NG+LEX+PAR features.


  Reason 2)  also suggests why  first-order co- 
occurrence helps on some occasions while not on 
other occasions. Its impact is limited because the 
word co-occurrence information  overlaps with some 
of the PAR feature values as mentioned  earlier. It 
improves  some cases because it provides some new 
evidence from web-scale data to the system based on 
NG+LEX+PAR features. It introduces new errors 
because it simply favors the word that co-occurred 
more often regardless of other factors. Its impact is 
also limited because it is only considered when clas- 
sifiers with NG+LEX+PAR features are not confi- 
dent.



C
O
NF
U
SI
O
N 
SE
T
# 
TE
ST
M
AJ
O
R
LE
X
LE
X+
PA
R
N
G
N
G
+L
E
X
N
G+
LE
X+
PA
R 
(&
)
9
 
c
o
m
m
o
n
l
y
 
c
i
t
e
d
 
E
S
L
 
c
o
n
f
u
s
i
o
n
 
p
a
i
r
s
ad
ver
se 
/ 
av
ers
e
36
8
85
.8
7
97
.0
1
96
.7
4
91
.0
3
97
.5
5
97.
01 
(+
22.
2%
) α
all
usi
on 
/ 
illu
sio
n
53
5
76
.6
4
91
.2
2
91
.4
0
91
.4
0
92
.5
2
93.
08 
(-
7.
5
%) 
α
co
mp
le
me
nt / 
co
m
pli
m
en
t
86
0
51
.5
1
83
.8
4
85
.1
2
88
.4
9
88
.3
7
89.
53 
(-
10
.0
%)
co
nfi
da
nt / 
co
nfi
de
nt
24
16
94
.4
1
97
.9
7
98
.3
0
98
.5
1
99
.0
5
99.
09 
(-
4.
3
%) 
α
de
ser
t / 
de
ss
ert
23
57
70
.8
1
90
.7
1
91
.5
6
87
.3
1
93
.6
8
94.
57 
(-
14
.1
%) 
α*
dis
cre
et / 
dis
cr
et
e
21
9
79
.4
5
84
.4
8
85
.8
4
85
.8
4
90
.4
1
91.
32 
(-
9.
5
%) 
α
eli
cit 
/ 
illi
cit
56
3
53
.4
6
82
.7
7
95
.5
6
97
.5
1
97
.5
1
98.
22 
(-
28
.6
%)
sta
tio
nar
y / 
sta
tio
ne
ry
18
2
62
.6
4
87
.3
6
92
.3
1*
93
.9
6
92
.8
6
95.
60 
(-
38
.5
%)
wa
nd
er 
/ 
wo
nd
er
65
06
86
.3
7
96
.4
2
97
.4
2*
97
.5
6
98
.2
3
98.
48 
(-
13
.9
%) 
α*
Tot
al
13
97
2
81
.0
8
93
.9
4
95
.2
9*
94
.8
2
96
.5
6
96.
99 
(-
12
.4
%) 
α*
5
 
O
r
i
g
i
n
a
l
 
B
e
r
g
s
m
a
 
p
a
i
r
s
# 
am
on
g / 
be
tw
ee
n
10
22
7
57
.4
6
91
.8
9
91
.8
6
88
.3
4
93
.6
0
93.
58 
(+
3.1
%) 
α
# 
am
ou
nt / 
nu
m
be
r
73
98
76
.4
4
92
.3
4
93
.1
6*
93
.0
3
93
.4
2
94.
08 
(-
10
.1
%) 
α*
# 
cit
e / 
sit
e
10
18
5
95
.7
1
99
.4
2
99
.5
3
99
.1
6
99
.5
2
99.
63 
(-
22
.4
%)
α
# 
pe
ac
e / 
pie
ce
73
30
56
.8
1
95
.0
1
97
.0
1*
95
.5
5
96
.7
4
97.
46 
(-
22
.2
%)
α 
*
# 
rai
se 
/ 
ris
e
94
64
55
.9
8
96
.1
2
96
.6
4*
94
.4
5
96
.6
8
97.
05 
(-
11
.5
%) 
α
Tot
al
44
60
4
68
.9
2
95
.0
9
95
.6
9*
94
.0
7
96
.0
9
96.
42 
(-
8.4
%) 
α

Table 3: Spelling correction precision (%), impact of adding parse features
SVM trained on 1G words of news text, tested on 9-months of NYT data.
*: Improvement of (NG+)LEX+PAR vs. (NG+)LEX is statistically significant.
α: Improvement of NG+LEX+PAR vs. NG is statistically significant.
&: Relative increase or decrease of error rate compared to ”NG+LEX”
#: As in Bergsma et al. (2009; 2010) no morphological variants of the words are used in evaluation



C
O
NF
U
SI
O
N 
SE
T
# 
TE
ST
M
AJ
O
R
CL
AS
SI
FI
E
R
C
O
M
BI
N
E
D 
SY
ST
E
M 
(&
)
9
 
c
o
m
m
o
n
l
y
 
c
i
t
e
d
 
E
S
L
 
c
o
n
f
u
s
i
o
n
 
p
a
i
r
s
ad
ver
se 
/ 
av
ers
e
36
8
85
.8
7
97
.5
5
96.
74 
(+
33
.3
%)
all
usi
on 
/ 
illu
sio
n
53
5
76
.6
4
92
.3
4
92.
34 
(- 
0.
0
%)
co
mp
le
me
nt / 
co
m
pli
m
en
t
86
0
51
.5
1
89
.8
8
90.
81 
(-
9.
2
%)
co
nfi
da
nt / 
co
nfi
de
nt
24
16
94
.4
1
99
.1
3
99.
05 
(+
9.
5
%)
de
ser
t / 
de
ss
ert
23
57
70
.8
1
93
.9
8
94.
23 
(-
3.
7
%)
dis
cre
et / 
dis
cr
et
e
21
9
79
.4
5
90
.4
1
91.
78 
(-
14
.3
%)
eli
cit 
/ 
illi
cit
56
3
53
.4
6
98
.4
0
98.
76 
(-
22
.2
%)
sta
tio
nar
y / 
sta
tio
ne
ry
18
2
62
.6
4
93
.4
1
93.
96 
(-
9.
1
%)
wa
nd
er 
/ 
wo
nd
er
65
06
86
.3
7
98
.4
9
98.
36 
(+
9.
2
%)
5
 
O
r
i
g
i
n
a
l
 
B
e
r
g
s
m
a
 
p
a
i
r
s
# 
am
on
g / 
be
tw
ee
n
10
22
7
57
.4
6
92
.7
3
92.
73 
(-
0.
1
%)
# 
am
ou
nt / 
nu
m
be
r
73
98
76
.4
4
93
.4
4
93.
76 
(-
4.
74
%)
# 
cit
e / 
sit
e
10
18
5
95
.7
1
99
.4
9
99.
47 
(+
3.
8
%)
# 
pe
ac
e / 
pie
ce
73
30
56
.8
1
96
.1
9
96.
38 
(-
5.
0
%)
# 
rai
se 
/ 
ris
e
94
64
55
.9
8
96
.6
6
96.
59 
(+
2.
2
%)

Table 4: Spelling correction accuracy (%), impact of combining word co-occurrence
CLASSIFIER: Logistic Regression trained on 1G words of news text, tested on 9-months NYT data. 
COMBINED SYSTEM: CLASSIFER  plus system based on first-order word co-occurrence.
&: Relative increase or decrease in error rate compared to CLASSIFIER
#: As in Bergsma et al. (2009; 2010), no morphological variants of the words are used in evaluation


6   Conclusions

We  propose a   novel  approach that uses  parse 
features  and lexical features  together to improve 
the performance of web-scale n-gram models for 
spelling correction. This method is especially adap- 
tive when less training data are available,  which is 
the case for confusable words that are not very fre- 
quently used. We also investigate the effectiveness 
of incorporating web-scale word co-occurrence and 
corpus-based semantic word relatedness (Random 
Indexing).
  For future work, we will investigate using seman- 
tic information (e.g.  WordNet) to extend n-gram 
models. It will be interesting to see if the usage of 
the word “compliment” in “complimenting the pres- 
ident” can be estimated by considering similar us- 
ages in the corpus, such as “complimenting  the stu- 
dent” or by creating an n-gram  database of synset 
patterns. We will investigate extending, to other ap- 
plications, this general methodology combining dis- 
tributional, semantic and syntactic information with 
language models.

Acknowledgments

We wish to thank Michael Flor of Educational 
Testing Service for his TrendStream  tool, which 
provides fast access and easy manipulation  of the 
Google N-gram Corpus. We also thank Derrick Hig- 
gins of Educational Testing Service for his Random 
Indexing support. We also thank Satoshi Sekine of 
New York University, Matthew Snover of City Uni- 
versity of New York, and Jing Jiang of Singapore 
Management University for their advice.


References

Shane Bergsma, Dekang Lin, and Randy Goebel. 2009.
Web-scale n-gram models for lexical disambiguation. 
In IJCAI.
Shane Bergsma,  Emily Pitler, and Dekang Lin.   2010.
Creating robust supervised classifiers via web-scale n- 
gram data. In ACL.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram 
version 1. Available at http://www.ldc.upenn.edu.
Alexander Budanitsky and Graeme Hirst. 2001. Seman- 
tic distance in wordnet: An experimental, application- 
oriented evaluation of five measures. In ACL Work- 
shop on WordNet and Other Lexical Resources.


Andrew Carlson and Ian Fette.  2007.  Memory-based 
context sensitive spelling correction at web scale. In 
ICMLA.
Martin Chodorow, Joel Tetreault, and Na-Rae Han. 2007.
Detection of  grammatical errors involving  preposi- 
tions.   In Proceedings of the Fourth ACL-SIGSEM 
Workshop on Prepositions,  pages 25–30.
Silviu  Cucerzan  and David Yarowsky.   2002.   Aug- 
mented mixture models for lexical disambigua-tion. In 
EMNLP.
Marie-Catherine de Marneffe,  Bill   MacCartney, and 
Christopher D. Manning. 2006. Generating typed de- 
pendency parses from phrase structure parses. In Pro- 
ceedings of LREC, Genoa, Italy.
Scott  Deerwester, Susan Dumais,  George Furmas, 
Thomas Landauer, and Richar Harshman. 1990. In- 
dexing by latent semantic analysis. The American So- 
ciety for Information Science.
Dmitriy Dligach and Martha Palmer. 2008.  Novel se- 
mantic features for verb sense  disambiguation.  In 
ACL.
Philip Edmonds. 1997. Choosing the word most typical 
in context using a lexical co-occurrence network. In 
EACL.
Mohammed Ali Elmi and Martha Evans. 1998. Spelling 
correction using context. In COLING.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui 
Wang, and Chih-Jen Lin.  2008. Liblinear: A library 
for large linear classification. Machine Learning Re- 
search, 9(1871-1874).
Rachele De Felice and Stephen G. Pulman.	2008.
A classifier-based approach to preposition and deter- 
miner error correction in L2 English. In Proceedings 
of COLING, Manchester, UK.
Rachele De Felice and Stephen G. Pulman. 2009. Auto- 
matic detection of preposition errors in learner writing. 
CALICO Journal, 26(3).
Michael Gamon, Jianfeng Gao, Chris Brockett, Alex 
Klementiev, William B. Dolan, Dmitriy Belenko, and 
Lucy Vanderwende. 2008.  Using contextual speller 
techniques and language modeling  for ESL error cor- 
rection. In Proceedings of the International Joint Con- 
ference on Natural Language Processing (IJCNLP), 
pages 449–456, Hyderabad, India.
Andrew Golding and Dan Roth. 1996. Applying Win- 
now to context-sensitive spelling correction. In Pro- 
ceedings of the International Conference on Machine 
Learning (ICML), pages 182–190.
Andrew Golding and Dan Roth. 1999. A winnow-based 
approach to context-sensitive spelling correction. Ma- 
chine Learning, 34(1-3):107–130.
Andrew Golding. 1995. A Bayesian hybrid method for 
context sensitive spelling correction. In Proceedings


of the Third Workshop on Very Large Corpora (WVLC-
3), pages 39–53.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006.  Detecting errors in English article usage by 
non-native  speakers. Natural Language Engineering,
12(2):115–129.
Matthieu Hermet, Alain De´silets, and Stan Szpakowicz.
2008.  Using the web as a linguistic resource to au- 
tomatically correct lexico-syntactic errors.   In Pro- 
ceedings of the Sixth International  Conference on Lan- 
guage Resources and Evaluation  (LREC), pages 390–
396, Marrekech, Morocco.
Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thepchai 
Supnithi, and Hitoshi Isahara. 2003.  Automatic er- 
ror detection in the Japanese learners’  English  spoken 
data. In Companion Volume to the Proceedings of the
41st Annual Meeting of the Association for Computa- 
tional Linguistics (ACL), pages 145–148.
Michael Jones and James Martin.   1997.   Contextual 
spelling correction using latent semantic analysis. In 
ANLC.
Thomas Landauer, Darrell Laham, and Peter Foltz. 1998.
Learning human-like knowledge by singular value de- 
composition: A progress report. Advances in Neural 
Information Processing  Systems, 10:45–51.
Mirella Lapata and Frank Keller. 2005. Web-based mod- 
els for natural language processing. ACM Transac- 
tions on Speech and Language Processing,  21:1–31.
John Lee and Ola Knutsson. 2008. The role of pp attach- 
ment in preposition generation. In CICLING.
Lidia Mangu and Eric Brill.  1997. Automatic rule acqui- 
sition for spelling correction. In ICML.
Saif Mohammad and Graeme Hist. 2006. Distributional 
measures of concept distance: A task-oriented evalua- 
tion. In EMNLP.
Alla  Rozovskaya and Dan Roth.    2010.    Training 
paradigms for correcting errors in grammar and usage. 
In ACL.
Magnus Sahlgren. 2006.  The Word-Space Model: us- 
ing distributional analysis to represent syntagmatic 
and paradigmatic relations between words in high- 
dimensional vector spaces. Ph.D. thesis.
Joel Tetreault and Martin Chodorow. 2008. The ups and 
downs of prepostion error detection in esl writing.  In 
COLING.
Joel Tetreault, Jennifer Foster, and Martin Chodorow.
2010.  Using parse features for preposition selection 
and error detection. In ACL.
Casey Whitelaw, Ben Hutchinson, Grace Y. Chung, and 
Gerard Ellis. 2009. Using the web for language inde- 
pendent spellchecking and autocorrection. In ACL.

