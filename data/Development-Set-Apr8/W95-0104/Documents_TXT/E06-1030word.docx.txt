Web Text Corpus for Natural Language Processing



Vinci Liu  and James R. Curran 
School of Information Technologies 
University of Sydney
NSW 2006, Australia
{vinci,james}@it.usyd.edu.au








Abstract

Web text has been successfully used as 
training data for many NLP applications. 
While most previous work accesses web 
text through search engine hit counts, we 
created a Web Corpus by downloading 
web pages to create a topic-diverse collec- 
tion of 10 billion words of English.  We 
show that for context-sensitive spelling 
correction the Web Corpus results are bet- 
ter than using a search engine.  For the- 
saurus extraction, it achieved similar over- 
all results to a corpus of newspaper text. 
With many more words available on the 
web, better results can be obtained by col- 
lecting much larger web corpora.
1   Introduction

Traditional written corpora for linguistics research 
are created primarily from printed text, such as 
newspaper articles and books. With the growth of 
the World Wide Web as an information resource, it 
is increasingly being used as training data in Nat- 
ural Language Processing (NLP) tasks.
  There are many advantages to creating a corpus 
from web data rather than printed text.  All web 
data is already in electronic form and therefore 
readable by computers, whereas not all printed 
data is available electronically.  The vast amount 
of text available on the web is a major advantage, 
with Keller and Lapata (2003) estimating that over
98 billion words were indexed by Google in 2003.
  The performance of NLP systems tends to im- 
prove with increasing amount of training data. 
Banko and Brill (2001) showed that for context- 
sensitive spelling correction, increasing the train- 
ing data size increases the accuracy, for up to 1 
billion words in their experiments.
  

To date, most NLP tasks that have utilised web 
data have accessed it through search engines, us- 
ing only the hit counts or examining a limited 
number of results pages.  The tasks are reduced 
to determining n-gram probabilities which are 
then estimated by hit counts from search engine 
queries.  This method only gathers information 
from the hit counts but does not require the com- 
putationally expensive downloading of actual text 
for analysis.  Unfortunately search engines were 
not designed for NLP research and the reported hit 
counts are subject to uncontrolled variations and 
approximations (Nakov and Hearst, 2005).  Volk 
(2002) proposed a linguistic search engine to ex- 
tract word relationships more accurately.
  We created a 10 billion word topic-diverse Web 
Corpus by spidering websites from a set of seed 
URLs.  The seed set is selected from the Open 
Directory to ensure that a diverse range of top- 
ics is included in the corpus.  A process of text 
cleaning transforms the HTML text into a form 
useable by most NLP systems – tokenised words, 
one sentence per line.  Text filtering removes un- 
wanted text from the corpus, such as non-English 
sentences and most lines of text that are not gram- 
matical sentences. We compare the vocabulary of 
the Web Corpus with newswire.
  Our Web Corpus is evaluated on two NLP tasks. 
Context-sensitive spelling correction is a disam- 
biguation problem, where the correction word in a
confusion set (e.g. {their, they’re}) needs to be se-
lected for a given context. Thesaurus extraction is
a similarity task, where synonyms of a target word 
are extracted from a corpus of unlabelled text. Our 
evaluation demonstrates that web text can be used 
for the same tasks as search engine hit counts and 
newspaper text.  However, there is a much larger 
quantity of freely available web text to exploit.


2   Existing Web Corpora

The web has become an indispensible resource 
with a vast amount of information available. Many 
NLP tasks have successfully utilised web data, in- 
cluding machine translation (Grefenstette, 1999), 
prepositional phrase attachment (Volk, 2001), and 
other-anaphora resolution (Modjeska et al., 2003).

2.1   Search Engine Hit Counts

Most NLP systems that have used the web access 
it via search engines such as Altavista and Google. 
N-gram counts are approximated by literal queries 
“w1  ...  wn”.   Relations between two words are 
approximated in Altavista by the NEAR operator 
(which locates word pairs within 10 tokens of each 
other).  The overall coverage of the queries can 
be expanded by morphological expansion of the 
search terms.
  Keller and Lapata (2003) demonstrated a high 
degree of correlation between n-gram estimates 
from search engine hit counts and n-gram frequen- 
cies obtained from traditional corpora such as the 
British National Corpus (BNC). The hit counts 
also had a higher correlation to human plausibil- 
ity judgements than the BNC counts.
  The web count method contrasts with tradi- 
tional methods where the frequencies are obtained 
from a corpus of locally available text. While the 
corpus is much smaller than the web, an accu- 
rate count and further text processing is possible 
because all of the contexts are readily accessible. 
The web count method obtains only an approxi- 
mate number of matches on the web, with no con- 
trol over which pages are indexed by the search 
engines and with no further analysis possible.
  There are a number of limitations in the search 
engine approximations.  As many search engines 
discard punctuation information (especially when 
using the NEAR operator), words considered ad- 
jacent to each other could actually lie in differ- 
ent sentences or paragraphs. For example in Volk 
(2001), the system assumes that a preposition at- 
taches to a noun simply when the noun appears 
within a fixed context window of the preposition. 
The preposition and noun could in fact be related 
differently or be in different sentences altogether.
  The speed of querying search engines is another 
concern. Keller and Lapata (2003) needed to ob- 
tain the frequency counts of 26,271 test adjective 
pairs from the web and from the BNC for the task 
of prenominal adjective ordering.  While extract-


ing this information from the BNC presented no 
difficulty, making so many queries to the Altavista 
was too time-consuming. They had to reduce the 
size of the test set to obtain a result.
  Lapata and Keller (2005) performed a wide 
range of NLP tasks using web data by querying 
Altavista and Google.  This included variety of 
generation tasks (e.g.  machine translation candi- 
date selection) and analysis tasks (e.g.  preposi- 
tional phrase attachment, countability detection). 
They showed that while web counts usually out- 
performed BNC counts and consistently outper- 
formed the baseline, the best performing system 
is usually a supervised method trained on anno- 
tated data. Keller and Lapata concluded that hav- 
ing access linguistic information (accurate n-gram 
counts, POS tags, and parses) outperforms using a 
large amount of web data.

2.2   Spidered Web Corpora

A few projects have utilised data downloaded from 
the web.  Ravichandran et al. (2005) used a col- 
lection of 31 million web pages to produce noun 
similarity lists.  They found that most NLP algo- 
rithms are unable to run on web scale data, espe- 
cially those with quadratic running time. Halacsy 
et al. (2004) created a Hungarian corpus from the 
web by downloading text from the .hu domain. 
From a 18 million page crawl of the web a 1 bil- 
lion word corpus is created (removing duplicates 
and non-Hungarian text).
  A terabyte-sized corpus of the web was col- 
lected at the University of Waterloo in 2001.  A 
breadth first search from a seed set of university 
home pages yielded over 53 billion words, requir- 
ing 960GB of storage.  Clarke et al. (2002) and 
Terra and Clarke (2003) used this corpus for their 
question answering system.   They obtained in- 
creasing performance with increasing corpus size 
but began reaching asymptotic behaviour at the
300-500GB range.

3   Creating the Web Corpus

There are many challenges in creating a web cor- 
pus, as the World Wide Web is unstructured and 
without a definitive directory. No simple method 
exists to collect a large representative sample of 
the web.  Two main approaches exist for collect- 
ing representative web samples – IP address sam- 
pling and random walks.  The IP address sam- 
pling technique randomly generates IP addresses


and explores any websites found (Lawrence and 
Giles, 1999). This method requires substantial re- 
sources as many attempts are made for each web- 
site found. Lawrence and Giles reported that 1 in
269 tries found a web server.
  Random walk techniques attempt to simulate a 
regular undirected web graph (Henzinger et al.,
2000). In such a graph, a random walk would pro- 
duce a uniform sample of the nodes (i.e. the web 
pages). However, only an approximation of such a 
graph is possible, as the web is directed (i.e. you 
cannot easily determine all web pages linking to 
a particular page). Most implementations of ran- 
dom walks approximates the number of backward 
links by using information from search engines.

3.1   Web Spidering

We created a 10 billion word Web Corpus by spi- 
dering the web. While the corpus is not designed 
to be a representative sample of the web, we at- 
tempt to sample a topic-diverse collection of web 
sites. Our web spider is seeded with links from the 
Open Directory1.
  The Open Directory has a broad coverage of 
many topics on the web and allows us to create 
a topic-diverse collection of pages. Before the di- 
rectory can be use, we had to address several cov- 
erage skews. Some topics have many more links 
in the Open Directory than others,  simply due 
to the availability of editors for different topics. 
For example, we found that the topic University of 
Connecticut has roughly the same number of links 
as Ontario Universities.   We would normally ex- 
pect universities in a whole province of Canada to 
have more coverage than a single university in the 
United States. The directory was also constructed 
without keeping more general topics higher in the 
tree. For example, we found that Chicken Salad is 
higher in the hierarchy than Catholicism. The Open 
Directory is flattened by a rule-based algorithm 
which is designed to take into account the cover- 
age skews of some topics to produce a list of 358 
general topics.
  From the seed URLs, the spider performs a 
breadth-first search.  It randomly selects a topic 
node from the list and next unvisited URL from the 
node. It visits the website associated from the link 
and samples pages within the same section of the 
website until a minimum number of words have 
been collected or all of the pages were visited.

1 The Open Directory Project, http://www.dmoz.org


External links encountered during this process are 
added to the link collection of the topic node re- 
gardless of the actual topic of the link. Although 
websites of one topic tends to link to other web- 
sites of the same topic, this process contributes to 
a topic drift.  As the spider traverses away from 
the original seed URLs, we are less certain of the 
topic included in the collection.

3.2   Text Cleaning

Text cleaning is the term we used to describe the 
overall process of converting raw HTML found on 
the web into a form useable by NLP algorithms
– white space delimited words, separated into one 
sentence per line.  It consists of many low-level 
processes which are often accomplished by sim- 
ple rule-based scripts. Our text cleaning process is 
divided into four major steps.
  First, different character encoding of HTML 
pages are transform into ISO Latin-1 and HTML 
named-entities (e.g. &nbsp; and &amp;) translated 
into their single character equivalents.
  Second, sentence boundaries are marked. Such 
boundaries are difficult to identify on web text as 
it does not always consists of grammatical sen- 
tences.  A section of a web page may be math- 
ematical equations or lines of C++ code.  Gram- 
matical sentences need to be separated from each 
other and from other non-sentence text. Sentence 
boundary detection for web text is a much harder 
problem than newspaper text.
  We use a machine learning approach to identify- 
ing sentence boundaries. We trained a Maximum 
Entropy classifier following Ratnaparkhi (1998) 
to disambiguate sentence boundary on web text, 
training on 153 manually marked web pages. Sys- 
tems for newspaper text only use regular text fea- 
tures, such as words and punctuations. Our system 
for web text uses HTML tag features in addition 
to regular text features.  HTML tag features are 
essential for marking sentence boundaries in web 
text, as many boundaries in web text are only indi- 
cated by HTML tags and not by the text. Our sys- 
tem using HTML tag features achieves 95.1% ac- 
curacy in disambiguating sentence boundaries in 
web text compared to 88.9% without using such 
features.
  Third, tokenisation is accomplished using the 
sed script used for the Penn Treebank project 
(MacIntyre, 1995), modified to correctly tokenise 
URLs, emails, and other web-specific text.


  The final step is filtering, where unwanted text 
is removed from the corpus.  A rule-based com- 
ponent analyses each web page and each sentence 
within a page to identify sections that are unlikely 
to be useful text.  Our rules are similar to those 
employed by Halacsy et al. (2004), where the per- 
centage of non-dictionary words in a sentence or 
document helps identify non-Hungarian text. We 
classify tokens into dictionary words, word-like 
tokens, numbers, punctuation, and other tokens. 
Sentences or documents with too few dictionary 
words or too many numbers, punctuation, or other 
tokens are discarded.

4   Corpus Statistics

Comparing the vocabulary of the Web Corpus and 
existing corpora is revealing. We compared with 
the Gigaword Corpus, a 2 billion token collection 
(1.75 billion words before tokenisation) of news- 
paper text (Graff, 2003). For example, what types 
of tokens appears more frequently on the web than 
in newspaper text?   From each corpus, we ran- 
domly select a 1 billion word sample and classified 
the tokens into seven disjoint categories:

Numeric  – At least one digit and zero or more 
punctuation characters, e.g. 2, 3.14, $5.50
Uppercase  – Only uppercase, e.g. REUTERS
Title Case  – An uppercase letter followed by one 
or more lowercase letters, e.g. Dilbert
Lowercase  – Only lowercase, e.g. violin
Alphanumeric – At least one alphabetic and one 
digit (allowing for other characters), e.g. B2B, 
mp3, RedHat-9
Hyphenated Word  – Alphabetic characters and 
hyphens, e.g. serb-dominated, vis-a-vis
Other  – Any other tokens

4.1   Token Type Analysis
An analysis by token type shows big differences 
between the two corpora (see Table 1). The same 
size samples of the Gigaword and the Web Corpus 
have very different number of token types.  Title 
case tokens is a significant percentage of the token 
types encountered in both corpora, possibly repre- 
senting named-entities in the text. There are also a 
significant number of tokens classified as others in 
the Web Corpus, possibly representing URLs and 
email addresses. While 2.2 million token types are 
found in the 1 billion word sample of the Giga- 
word, about twice as many (4.8 million) are found 
in an equivalent sample of the Web Corpus.




G
i
g
a
w
o
r
d
W
e
b
 
C
o
r
p
u
s
T
o
k
e
n
s
T
o
ke
n 
T
y
pe
s
1
 
b
i
l
l
i
o
n
2
.
2
 
m
i
l
l
i
o
n
1
 
b
i
l
l
i
o
n
4
.
8
 
m
i
l
l
i
o
n
N
u
m
e
r
i
c
U
p
p
e
r
c
a
s
e 
T
it
l
e 
C
a
s
e 
L
o
w
e
r
c
a
s
e 
A
l
p
h
a
- 
n
u
m
e
r
i
c 
H
y
p
h
e
n
a
t
e
d 
O
t
h
e
r
34
3k	15.6%
9
5
k	4.3%
64
5k	29.3%
26
3k	12.0%
16
5k	7.6%

53
3k	24.3%
15
0k	6.8%
3
7
4
k	7.7%
2
4
1
k	5.0%
9
4
6
k	19.6%
7
3
4
k	15.2%
4
1
7
k	8.6%

9
7
0
k	20.1%
1,
14
6k	23.7%

Table 1: Classification of corpus token by type


G
i
g
a
w
o
r
d
W
eb 
C
or
pu
s
r
r
e
c
e
i
v
e
r
e
c
c
e
i
v
e
 
r
e
c
e
i
e
v
e
 
r
e
c
i
v
e
 
r
e
c
e
i
v
re
ec
ei
ve
re
c
e
si
v
e 
re
ci
ei
v
e 
re
c
ei
v
c
e 
re
c
ei
v
e
e 
re
c
ei
v
e
a 
rc
ei
v
e
r
e
c
e
i
e
v
e
r
e
c
i
v
e
 
r
e
c
v
e
i
v
e
 
r
e
c
e
i
v
v
e
 
r
e
c
e
v
e
 
r
e
c
e
i
v
 
r
e
y
c
e
i
v
e
1
.
7
 
m
i
s
s
p
e
l
l
i
n
g
s
 
p
e
r
 
d
i
c
t
i
o
n
a
r
y
 
w
o
r
d
3.7 
mi
ssp
elli
ng
s 
per 
dic
tio
nar
y 
wo
rd
3.
1
m 
m
is
sp
el
li
n
gs 
in
6
9
9
m
 
d
i
c
t
.
 
w
o
r
d
s
5.6m 
missp
elling
s in
66
9m 
dic
t. 
wo
rds

Table 2: Misspellings of 
receive



4.2   
Misspelling

One factor contributing to the larger number of 
to- ken types in the Web Corpus, as compared 
with the Gigaword, is the misspelling of words. 
Web docu- ments are authored by people with a 
widely vary- ing command of English and their 
pages are not as carefully edited as newspaper 
articles.  Thus, we anticipate a significantly 
larger number of mis- spellings and 
typographical errors.
  We identify some of the misspellings by 
let- ter combinations that are one transformation 
away from a correctly spelled word.  Consider a 
target word, correctly spelled. Misspellings can 
be gen- erated by inserting, deleting, or 
substituting one letter, or by reordering any two 
adjacent letters (al- though we keep the first letter 
of the original word, as very few misspellings 
change the first letter).
  Table 2 shows some of the misspellings of the 
word receive found in the Gigaword and the Web 
Corpus.   While only 5 such misspellings were 
found in the Gigaword, 16 were found in the 
Web



Al
go
rit
h
m
Tr
ai
ni
ng
Te
sti
ng
A
A
W
A
A
U
np
ru
ne
d
W
in
no
w
Br
o
w
n
80
%
Br
o
w
n
20
%
94
.1
9
6.
4
U
np
ru
ne
d
W
in
no
w
Br
o
w
n
80
%
W
SJ
40
%
89
.5
9
4.
5
W
in
no
w
Se
mi
-
Su
p.
Br
o
w
n
80
%
*
W
SJ
40
%
93
.1
9
6.
6
Se
ar
ch
En
gi
ne
Al
ta
vis
ta
Br
o
w
n
10
0
%
89
.3
N
/
A

Table 3: Context-sensitive spelling correction
(* denotes also using 60% WSJ, 5% corrupted)



Corpus.  For all words found in the Unix dictio- 
nary, an average of 1.7 misspellings are found per 
word in the Gigaword by type. The proportion of 
mistakes found in the Web Corpus is roughly dou- 
ble that of the Gigaword, at 3.7 misspellings per 
dictionary word. However, misspellings only rep- 
resent a small portion of tokens (5.6 million out of
699 million instances of dictionary word are mis- 
spellings in the Web Corpus).

5   Context-Sensitive Spelling Correction

A confusion set is a collection of words which 
are commonly misused by even native speakers 
of a language because of their similarity.   For
example,  the  words  {it’s,   its},   {affect,   effect},
and {weather, whether} are often mistakenly inter-
changed.  Context-sensitive spelling correction is
the task of selecting the correct confusion word 
in a given context.  Two different metrics have 
been used to evaluate the performance of context- 
sensitive spelling correction algorithms. The Av- 
erage Accuracy (AA) is the performance by type 
whereas the Weighted Average Accuracy (WAA) 
is the performance by token.

5.1   Related Work

Golding and Roth (1999) used the Winnow mul- 
tiplicative weight-updating algorithm for context- 
sensitive spelling correction.   They found that 
when a system is tested on text from a different 
from the training set the performance drops sub- 
stantially (see Table 3). Using the same algorithm 
and 80% of the Brown Corpus, the WAA dropped 
from 96.4% to 94.5% when tested on 40% WSJ 
instead of 20% Brown.
For  cross  corpus  experiments,  Golding  and
Roth devised a semi-supervised algorithm that is


trained on a fixed training set but also extracts in- 
formation from the same corpus as the testing set. 
Their experiments showed that even if up to 20% 
of the testing set is corrupted (using wrong con- 
fusion words), a system that trained on both the 
training and testing sets outperformed the system 
that only trained on the training set. The Winnow 
Semi-Supervised method increases the WAA back 
up to 96.6%.
  Lapata and Keller (2005) utilised web counts 
from Altavista for confusion set disambiguation. 
Their unsupervised method uses collocation fea- 
tures  (one  word  to  the  left  and  right)  where 
co-occurrence estimates are obtained from web 
counts of bigrams. This method achieves a stated 
accuracy of 89.3% AA, similar to the cross corpus 
experiment for Unpruned Winnow.

5.2   Implementation
Context-sensitive spelling correction is an ideal 
task for unannotated web data as unmarked text 
is essentially labelled data for this particular task, 
as words in a reasonably well-written text are pos- 
itive examples of the correct usage of confusion 
words.
  To demonstrate the utility of a large collection 
of web data on a disambiguation problem, we im- 
plemented the simple memory-based learner from 
Banko and Brill (2001).  The learner trains on 
simple collocation features, keeping a count of 
(wi−1,wi+1), wi−1, and wi+1  for each confusion 
word wi. The classifier first chooses the confusion 
word which appears with the context bigram most 
frequently, followed by the left unigram, right uni- 
gram, and then the most frequent confusion word.
  Three data sets were used in the experiments: 
the 2 billion word Gigaword Corpus, a 2 billion 
word sample of our 10 billion word Web Corpus, 
and the full 10 billion word Web Corpus.

5.3   Results
Our experiments compare the results when the 
three corpora were trained using the same algo- 
rithm. The memory-based learner was tested using 
the 18 confusion word sets from Golding (1995) 
on the WSJ section of the Penn Treebank and the 
Brown Corpus.
  For the WSJ testing set, the 2 billion word Web 
Corpus does not achieve the performance of the 
Gigaword (see Table 4).  However, the 10 billion 
word Web Corpus results approach that of the Gi- 
gaword.   Training on the Gigaword and testing



Tr
ai
ni
ng
Te
sti
ng
A
A
W
A
A
Gi
ga
w
or
d
2 
bil
lio
n
W
SJ
10
0
%
93
.7
9
6.
1
W
eb 
Co
rp
us
2 
bil
lio
n
W
SJ
10
0
%
92
.7
9
4.
1
W
eb 
Co
rp
us
10 
bil
lio
n
W
SJ
10
0
%
93
.3
9
5.
1
Gi
ga
w
or
d
2 
bil
lio
n
Br
o
w
n
10
0
%
90
.7
9
4.
6
W
eb 
Co
rp
us
2 
bil
lio
n
Br
o
w
n
10
0
%
90
.8
9
4.
8
W
eb 
Co
rp
us
10 
bil
lio
n
Br
o
w
n
10
0
%
91
.8
9
5.
4

Table 4: Memory-based learner results



on WSJ is not considered a true cross-corpus ex- 
periment, as the two corpora belong to the same 
genre of newspaper text.  Compared to the Win- 
now method, the 10 billion word Web Corpus out- 
performs the cross corpus experiment but not the 
semi-supervised method.
  For the Brown Corpus testing set, the 2 billion 
word Web Corpus and the 2 billion word Giga- 
word achieved similar results. The 10 billion word 
Web Corpus achieved 95.4% WAA, higher than 
the 94.6% from the 2 billion Gigaword. This and 
the above result with the WSJ suggests that the 
Web Corpus approach is comparable with training 
on a corpus of printed text such as the Gigaword.
  The 91.8% AA of the 10 billion word Web Cor- 
pus testing on the WSJ is better than the 89.3% 
AA achieved by Lapata and Keller (2005) us- 
ing the Altavista search engine.   This suggests 
that a web collected corpus may be a more accu- 
rate method of estimating n-gram frequencies than 
through search engine hit counts.

6   Thesaurus Extraction

Thesaurus extraction is a word similarity task. It is 
a natural candidate for using web corpora as most 
systems extract synonyms of a target word from an 
unlabelled corpus. Automatic thesaurus extraction 
is a good alternative to manual construction meth- 
ods, as such thesauri can be updated more easily 
and quickly.  They do not suffer from bias, low 
coverage, and inconsistency that human creators 
of thesauri introduce.
Thesauri are useful in many NLP and Informa-



expand the coverage of system but providing al- 
ternatives to the inputed search terms. For n-gram 
estimation using search engine queries, some NLP 
applications can boost the hit count by offering al- 
ternative combination of terms. This is especially 
helpful if the initial hit counts are too low to be 
reliable.  In IR applications, synonyms of search 
terms help identify more relevant documents.

6.1   Method

We use the thesaurus extraction system imple- 
mented in Curran (2004).  It operates on the dis- 
tributional hypothesis that similar words appear 
in similar contexts. This system only extracts one 
word synonyms of nouns (and not multi-word ex- 
pressions or synonyms of other parts of speech). 
The extraction process is divided into two parts. 
First, target nouns and their surrounding contexts 
are encoded in relation pairs.  Six different types 
of relationships are considered:
• Between a noun and a modifying adjective
• Between a noun and a noun modifier
• Between a verb and its subject
• Between a verb and its direct object
• Between a verb and its indirect object
• Between a noun and the head of a modifying
prepositional phrase

The nouns (including subject and objects) are the 
target headwords and the relationships are repre- 
sented in context vectors.  In the second stage of 
the extraction process, a comparison is made be- 
tween context vectors of headwords in the corpus 
to determine the most similar terms.

6.2   Evaluation

The evaluation of a list of synonyms of a target 
word is subject to human judgement. We use the 
evaluation method of Curran (2004), against gold 
standard thesauri results.  The gold standard list 
is created by combining the terms found in four 
thesauri: Macquarie, Moby, Oxford and Roget’s.
  The inverse rank (InvR) metric allows a com- 
parison to be made between the extracted rank list 
of synonyms and the unranked gold standard list. 
For example, if the extracted terms at ranks 3, 5, 
and 28 are found in the gold standard list, then


tion Retrieval (IR) applications.  Synonyms help


I nvR =  1 +  1 +  1


∼= 0.569.


3	5	28



C
o
r
p
u
s
IN 
V 
R
IN 
V 
R 
M 
A 
X
G
i
g
a
w
o
r
d
W
eb 
C
or
p
us
1.
8
6
1.
8
1
5
.
9
2
5
.
9
2

Table 5: Average IN V R for 300 headwords


6.3   
Results

We used the same 300 evaluation headwords 
as Curran (2004) and extracted the top 200 
synonyms for each headword.  The 
evaluation headwords were extracted from two 
corpora for comparison – a 2 billion word 
sample of our Web Corpus and the
2 billion words in the Gigaword Corpus. 
Table 5 shows the average InvR scores over 
the 300 head- words for the two corpora – one 
of web text and the other newspaper text.  The 
InvR values differ by a negligible 0.05 (out of 
a maximum of 5.92).







.	.	.


6
.
4
   
A
n
a
l
y
s
i
s
.
However on a per 
word basis one 
corpus can sigif-
icantly outperform 
the other. Table 6 
ranks the 300 
headwords by 
difference in the 
InvR score. While 
much better results 
were extracted for 
words like home 
from the Gigaword, 
much better results 
were extracted for 
words like chain 
from the Web Cor-


Table 6: InvR scores ranked by difference, Giga-
word to Web Corpus


Gi
ga
wo
rd 
(24 
ma
tch
es 
out 
of 
20
0)
h
o
us
e  
a
p
ar
t
m
e
nt 
b
ui
ld
in
g 
ru
n 
of
fi
ce 
re
si
d
e
nt 
re
si
d
e
n
ce
h
e
a
d
q
u
ar
te
rs 
vi
ct
or
y 
na
ti
ve 
pl
ac
e 
m
an
si
o
n 
ro
o
m 
tri
p 
m
il
e 
fa
m
il
y 
ni
g
ht 
h
o
m
et
o
w
n 
to
w
n 
w
in 
ne
ig
h
b
or
h
o
o
d 
lif
e 
su
b- 
ur
b 
sc
h
o
ol 
re
st
au
ra
nt 
h
ot
el 
st
or
e 
ci
ty 
st
re
et 
se
as
o
n 
ar
ea 
ro
ad 
h
o
m
er 
da
y 
ca
r 
sh
o
p 
h
os
pi
ta
l 
fri
en
d 
ga
m
e 
fa
r
m 
fa
ci
lit
y 
ce
n- 
te
r 
n
or
th 
ch
il
d 
la
n
d 
w
ee
ke
n
d 
co
m
m
u
ni
ty 
lo
ss 
re
tu
rn 
h
o
ur
.
 
.
 
.
We
b 
Co
rpu
s 
(18 
ma
tch
es 
out 
of 
20
0)
p
a
g
e 
lo
a
n 
c
o
nt
ac
t 
h
o
us
e 
us 
o
w
n
er 
se
ar
c
h 
fi
n
a
n
ce 
m
or
tg
a
g
e
of
fi
ce 
m
ap 
li
n
ks 
b
ui
ld
in
g 
fa
q 
eq
ui
ty 
ne
w
s 
ce
nt
er 
es
ta
te 
pr
i- 
va
cy 
co
m
m
u
ni
ty 
in
fo 
b
us
in
es
s 
ca
r 
sit
e 
w
eb 
i
m
pr
o
ve
m
en
t 
ex
te
nt
io
n 
he
at
in
g 
ra
te 
di
re
ct
or
y 
ro
o
m 
a
p
ar
t
m
e
nt 
fa
m
il
y 
se
rv
ic
e 
re
nt
al 
cr
ed
it 
sh
o
p 
lif
e 
ci
ty 
sc
h
o
ol 
pr
o
p
er
ty 
pl
ac
e 
lo
ca
ti
o
n 
jo
b 
o
nl
in
e 
va
ca
ti
o
n 
st
or
e 
fa
ci
lit
y 
li
br
ar
y 
fr
ee 
. . 
.

Table 7: Synonyms for home


Gi
ga
wo
rd 
(9 
ma
tch
es 
out 
of 
20
0)
st
or
e 
re
ta
il
er 
su
p
er
m
ar
ke
t 
re
st
a
ur
a
nt 
o
ut
le
t 
o
p
er
at
or 
sh
o
p
sh
el
f 
o
w
ne
r 
gr
oc
er
y 
co
m
pa
n
y 
h
ot
el 
m
an
uf
ac
tu
re
r 
re
ta
il 
fr
an
ch
is
e 
cl
er
k 
m
ak
er 
di
sc
o
u
nt 
b
us
in
es
s 
sa
le 
su
pe
rs
to
re 
br
an
d 
cl
ot
hi
n
g 
fo
o
d 
gi
an
t 
sh
o
p
pi
n
g 
fir
m 
re
ta
ili
n
g 
in
d
us
tr
y 
dr
u
gs
to
re 
di
st
ri
b
ut
or 
su
p
pl
ie
r 
ba
r 
in
su
re
r 
in
c.  
c
o
n
gl
o
m
er
- 
at
e 
ne
t
w
or
k 
u
ni
t 
ap
pa
re
l 
b
o
ut
iq
ue 
m
al
l 
el
ec
tr
o
ni
cs 
ca
rri
er 
di
vi
si
o
n 
br
o
ke
ra
ge 
to
y 
pr
o
d
uc
er 
p
ha
r
m
ac
y 
ai
rli
ne 
in
c . 
. .
We
b 
Co
rpu
s 
(53 
ma
tch
es 
out 
of 
20
0)
n
ec
kl
ac
e  
su
p
pl
y 
br
ac
el
et  
p
e
n
d
a
nt 
ro
p
e 
b
el
t  
ri
n
g  
e
ar
-
ri
n
g 
g
ol
d 
be
ad 
sil
ve
r 
pi
n 
w
ir
e 
co
rd 
re
ac
ti
o
n 
cl
as
p 
je
w
el
ry 
ch
ar
m 
fr
a
m
e 
ba
n
gl
e 
st
ra
p  
st
er
li
n
g 
lo
o
p 
ti
m
in
g 
pl
at
e 
m
et
al 
co
ll
ar  
tu
rn 
h
o
o
k 
ar
m 
le
n
gt
h 
st
ri
n
g  
re
ta
il
er 
re
pa
ir 
st
ra
n
d 
pl
u
g 
di
a
m
o
n
d 
w
he
el 
in
d
us
tr
y 
tu
be 
su
rf
ac
e 
ne
ck 
b
ro
o
c
h 
st
or
e 
m
ol
ec
ul
e 
ri
b
b
o
n  
p
u
m
p 
ch
o
ke
r 
sh
af
t 
b
o
d
y 
. . 
.

Table 8: Synonyms for chain


pus.
  Table 7 shows the top 50 synoyms extracted for 
the headword home from the Gigaword and the 
Web Corpus. While similar number of correct syn- 
onyms were extracted from both corpora, the Gi- 
gaword matches were higher in the extracted list 
and received a much higher InvR score. In the list 
extracted from the Web Corpus, web-related collo- 
cations such as home page and search home appear.
  Table 8 shows the top 50 synoyms extracted 
for the headword chain from both corpora. While 
there are only a total of 9 matches from the Giga- 
word Corpus, there are 53 matches from the Web 
Corpus. A closer examination shows that the syn- 
onyms extracted from the Gigaword belong only 
to one sense of the word chain, as in chain stores. 
The gold standard list and the Web Corpus results 
both contain the necklace sense of the word chain. 
The Gigaword results show a skew towards the 
business sense of the word chain, while the Web 
Corpus covers both senses of the word.
  While individual words can achieve better re- 
sults in either the Gigaword or the Web Corpus 
than the other, the aggregate results of synonym 
extraction for the 300 headwords are the same. For 
this task, the Web Corpus can replace the Giga- 
word without affecting the overall result.  How- 
ever, as some words are perform better under dif- 
ferent corpora, an aggregate of the Web Corpus 
and the Gigaword may produce the best result.


7   Conclusion

In this paper, the accuracy of natural language ap- 
plication training on a 10 billion word Web Corpus 
is compared with other methods using search en- 
gine hit counts and corpora of printed text.
  In the context-sensitive spelling correction task, 
a simple memory-based learner trained on our 
Web Corpus achieved better results than method 
based on search engine queries. It also rival some 
of the state-of-the-art systems, exceeding the ac- 
curacy of the Unpruned Winnow method (the only 
other true cross-corpus experiment). In the task of 
thesaurus extraction, the same overall results are 
obtained extracting from the Web Corpus as a tra- 
ditional corpus of printed texts.
  The Web Corpus contrasts with other NLP ap- 
proaches that access web data through search en- 
gine queries. Although the 10 billion words Web 
Corpus is smaller than the number of words in- 
dexed by search engines, better results have been 
achieved using the smaller collection. This is due 
to the more accurate n-gram counts in the down- 
loaded text. Other NLP tasks that require further 
analysis of the downloaded text, such a PP attach- 
ment, may benefit more from the Web Corpus.
  We have demonstrated that carefully collected 
and filtered web corpora can be as useful as 
newswire corpora of equivalent sizes.  Using the 
same framework describe here, it is possible to 
collect a much larger corpus of freely available 
web text than our 10 billion word corpus. As NLP 
algorithms tend to perform better when more data 
is available, we expect state-of-the-art results for 
many tasks will come from exploiting web text.

Acknowledgements

We like to thank our anonymous reviewers and the 
Language Technology Research Group at the Uni- 
versity of Sydney for their comments. This work 
has been supported by the Australian Research 
Council under Discovery Project DP0453131.


References

Michele Banko and Eric Brill.  2001.  Scaling to very very 
large corpora for natural language disambiguation.   In 
Proceedings of the ACL, pages 26–33, Toulouse, France,
9–11 July.

Charles L.A. Clarke, Gordon V. Cormack, M. Laszlo, 
Thomas R. Lynam, and Egidio Terra.   2002.   The im- 
pact of corpus size on question answering performance. 
In Proceedings of the ACM SIGIR, pages 369–370, Tam- 
pere, Finland.



James Curran. 2004. From Distributional to Semantic Simi- 
larity. PhD thesis, University of Edinburgh, UK.

Andrew R. Golding and Dan Roth.  1999. A winnow-based 
approach to context-sensitive spelling correction.   Ma- 
chine Learning, 34(1-3):107–130.

Andrew R. Golding.  1995.  A bayesian hybrid method for 
context-sensitive spelling correction.   In Proceedings of 
the Third Workshop on Very Large Corpora, pages 39–53, 
Somerset, NJ USA. ACL.

David Graff.  2003.  English Gigaword.  Technical Report 
LDC2003T05, Linguistic Data Consortium, Philadelphia, 
PA USA.

Gregory Grefenstette.  1999.  The WWW as a resource for 
example-based MT tasks.  In the ASLIB Translating and 
the Computer Conference, London, UK, October.

Peter Halacsy, Andras Kornai, Laszlo Nemeth, Andras Rung, 
Istvan Szakadat, and Vikto Tron.  2004.  Creating open 
language resources for Hungarian. In Proceedings of the 
LREC, Lisbon, Portugal.

M. R. Henzinger, A. Heydon, M. Mitzenmacher, and M. Na- 
jork. 2000. On near-uniform URL sampling. In Proceed- 
ings of the 9th International World Wide Web Conference.

Frank Keller and Mirella Lapata. 2003. Using the web to ob- 
tain frequencies for unseen bigrams. Computational Lin- 
guistics, 29(3):459–484.

Mirella Lapata and Frank Keller.  2005.  Web-based models 
for natural language processing.   ACM Transactions on 
Speech and Language Processing.

Steve Lawrence and C. Lee Giles.  1999.  Accessibility of 
information on the web. Nature, 400:107–109, 8 July.

Robert MacIntyre.   1995.   Sed script to produce Penn 
Treebank tokenization on arbitrary raw text.    From 
http://www.cis.upenn.edu/ treebank/tokenizer.sed.

Natalia N. Modjeska, Katja Markert, and Malvina Nissim.
2003.   Using the web in machine learning for other- 
anaphora resolution. In Proceedings of the EMNLP, pages
176–183, Sapporo, Japan, 11–12 July.

Preslav Nakov and Marti Hearst.  2005.  A study of using 
search engine page hits as a proxy for n-gram frequencies. 
In Recent Advances in Natural Language Processing.

Adwait Ratnaparkhi.  1998.  Maximum Entropy Models for 
Natural  Language  Ambiguity Resolution.    PhD thesis, 
University of Pennsylvania, Philadelphia, PA USA.

Deepak Ravichandran,  Patrick Pantel,  and Eduard Hovy.
2005.  Randomized algorithms and NLP: Using locality 
sensitive hash functions for high speed noun clustering. 
In Proceedings of the ACL, pages 622–629.

E. L. Terra and Charles L.A. Clarke.  2003.  Frequency es- 
timates for statistical word similarity measures.  In Pro- 
ceedings of the HLT, Edmonton, Canada, May.

Martin Volk.   2001.   Exploiting the WWW as a corpus to 
resolve PP attachment ambiguities. In Proceedings of the 
Corpus Linguistics 2001, Lancaster, UK, March.

Martin Volk.   2002.   Using the web as corpus for linguis- 
tic research. Ta¨ hendusepu¨ u¨ ja. Catcher of the Meaning. A
Festschrift for Professor Haldur O˜ im.

