



Spelling correction as an iterative process
that exploits the collective knowledge of web users

Silviu Cucerzan and Eric Brill
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052
{silviu,brill}@microsoft.com




Abstract

Logs of user queries to an internet search engine pro- 
vide a large amount of implicit and explicit informa- 
tion  about  language.  In  this  paper,  we  investigate 
their  use  in spelling  correction  of  search  queries,  a 
task which poses many additional challenges  beyond 
the traditional  spelling  correction  problem.  We pre- 
sent an approach that uses an iterative transformation 
of the input query strings into other strings that corre- 
spond to more and more likely queries according to 
statistics extracted from internet search query logs.

1   Introduction

The task of general purpose spelling correction has 
a long history (e.g. Damerau, 1964; Rieseman and 
Hanson, 1974; McIlroy, 1982), traditionally focus- 
ing on resolving typographical errors such as in- 
sertions,   deletions,   substitutions,   and 
transpositions   of  letters  that  result  in  unknown 
words (i.e. words not found in a trusted lexicon of 
the language). Typical word processing spell 
checkers compute for each unknown word a small 
set of in-lexicon alternatives to be proposed as 
possible corrections, relying on information  about 
in-lexicon-word frequencies and about the most 
common keyboard mistakes (such as typing m in- 
stead of n) and phonetic/cognitive  mistakes,  both 
at word level (e.g. the use of acceptible instead of 
acceptable) and at character level (e.g. the misuse 
of f instead of ph). Very few spell checkers attempt 
to  detect  and  correct  word  substitution   errors, 
which refer to the use of in-lexicon words in inap- 
propriate  contexts  and  can  also  be  the  result  of 
both typographical mistakes (such as typing coed 
instead of cord) and cognitive mistakes (e.g. prin- 
cipal  and  principle).   Some  research   efforts  to 
tackle this problem have been made; for example 
Heidorn et al. (1982) and Garside et al. (1987) de- 
veloped systems that rely on syntactic patterns to 
detect substitution errors, while Mays et al. (1991) 
employed  word  co-occurrence   evidence  from  a 
large  corpus  to  detect  and  correct  such  errors. 
The former approaches were based on the imprac- 
tical  assumption  that  all  possible  syntactic  uses 
of all words (i.e. part-of-speech)  are known,  and


presented both recall and precision problems be- 
cause many of the substitution errors are not syn- 
tactically anomalous and many unusual syntactic 
constructions do not contain errors. The latter ap- 
proach had very limited success under the assump- 
tions that each sentence contains at most one 
misspelled word, each misspelling is the result of a 
single  point  change  (insertion,  deletion,  substitu- 
tion, or transposition), and the defect rate (the rela- 
tive number of errors in the text) is known. A 
different body of work (e.g. Golding, 1995; Gold- 
ing and Roth,  1996; Mangu  and Brill,  1997)  fo- 
cused on resolving a limited number of cognitive 
substitution errors, in the framework of context 
sensitive spelling correction (CSSC). Although 
promising  results  were  obtained  (92-95%  accu- 
racy), the scope of this work was very limited as it 
only addressed known sets of commonly confused 
words, such as {peace, piece}).

1.1 Spell Checking of Search Engine Queries

The  task  of  web-query   spelling  correction   ad- 
dressed in this work has many similarities to tradi- 
tional spelling correction but also poses additional 
challenges. Both the frequency and severity of 
spelling errors for search queries are significantly 
greater than in word processing.  Roughly 10-15% 
of the queries sent to search engines contain errors. 
Typically,  the  validity  of a query  cannot  be  de- 
cided by lexicon look-up or by checking its gram- 
maticality. Because web queries are very short (on 
average, less than 3 words), techniques that use a 
multitude of features based on relatively wide con- 
text windows, such as those investigated in CSSC, 
are  difficult  to  apply.  Rather  than  being  well- 
formed sentences, most queries consist of one con- 
cept or an enumeration of concepts, many times 
containing  legitimate  words that are not found in 
any traditional lexicon.
 Just defining what a valid web query is represents 
a difficult enterprise. We clearly cannot use only a 
static trusted lexicon, as many new names and 
concepts (such as aznar, blog, naboo, nimh, nsync, 
and shrek) become popular every day and it would








be extremely difficult if not impossible to maintain 
a  high-coverage  lexicon.  In  addition,  employing 
very large lexicons can result in more errors sur- 
facing as word substitutions, which are very diffi- 
cult to detect, rather than as unknown words.
 One alternative investigated in this work is to ex- 
ploit  the  continuously  evolving  expertise  of mil- 
lions of people that use web search engines, as 
collected in search query logs (seen as histograms 
over the queries received by a search engine). In 
some  sense,  we  could  say  that  the  validity  of a 
word can be inferred from its frequency in what 
people  are  querying   for,  similarly   to  Wittgen- 
stein’s (1968) observation that “the meaning of a 
word is its use in the language”. Such an approach 
has its own caveats. For example, it would be er-


ple, it can be the ratio between the number of let- 
ters two words do not have in common and the 
number of letters they share.1 The two most used 
classes of distances in spelling correction  are edit
distances, as proposed by Damerau (1964) and 
Levenshtein (1965), and correlation matrix dis- 
tances (Cherkassky et al., 1974). In our study, we 
use   a   modified   version   of   the   Damerau-Lev- 
enshtein edit distance, as presented in Section 3.
 One flaw of the preceding  formulation  is that it 
does not take into account the frequency of words 
in a language. A simple solution to this problem is 
to compute the probability of words in the target 
language as maximum likelihood estimates (MLE) 
over a large corpus and reformulate the general 
spelling-correction problem as follows:


roneous to simply extract from web-query logs all 
the queries whose frequencies are above a certain 
value and consider them valid. Misspelled queries



Given 	w
dist (w, w' )


* \ L , 	find and P(w' )


w' 	L
max
v  L:dist ( w,v )



such 	that
P(v) .


such as britny spears are much more popular than
correctly spelled queries such as bayesian nets and 
amd processors. Our challenge is to try to utilize 
query logs to learn what queries are valid, and to 
build a model for valid query probabilities, despite 
the fact that a large percentage of the logged que- 
ries are misspelled and there is no trivial way to 
determine the valid from invalid queries.

2   Problem Formulation. Prior Work

Comprehensive reviews of the spelling correction 
literature  were  provided  by Peterson  (1980), 
Kukich (1992), and Jurafsky and Martin (2000). In 
this section, we survey a few lexicon-based  spell-


In this formulation, all in-lexicon words that are 
within some “reasonable” distance       of the un- 
known  word  are  considered  as  good  candidates, 
the correction being chosen based on its prior 
probability in the language. While there is an im- 
plicit conditioning on the original spelling because 
of the domain on which the best correction is 
searched,  this  objective  function  only  uses  the 
prior probability of words in the language and not 
the  actual  distances  between  each  candidate  and 
the input word
 One solution that allows using a probabilistic edit 
distance is to condition the probability of a correc- 
tion on the original spelling  P(v | w) :


ing correction approaches by using a series of for-
mal definitions of the task and presenting concrete 
examples showing the strengths and the limits cor-


Given 	w dist (w, w' )


* \ L , 	find and P(w' | w)


w' 	L
max
v  L:dist ( w,v )


such 	that
P(v | w) .


responding  to each situation.  We iteratively  rede- 
fine the problem, starting from an approach purely 
based on a trusted lexicon and ending up with an 
approach in which the role of the trusted lexicon is
 

In  a  noisy  channel  model  framework,  as  em- 
ployed for spelling correction by Kernigham et al. 
(1990),  the  objective  function  can  be written  by
using Bayesian  inversion  as the product  between


greatly diminished. While doing so, we also make
concrete forward steps in our attempt to provide a


the prior probability of words in a language


P(v)


definition of valid web queries.
Let 	be the alphabet of a language and  L


(the language  model),  and the 
likelihood  of mis-
* a 	spelling a word v as w, P(w | v) (which 
models the


broad-coverage lexicon of the language. The sim- 
plest and historically the first definition of lexicon- 
based   spelling   correction   (Damerau,   1964)   is:


noisy channel and will be called the error model).
In  the  above  formulations,  unknown  words  are 
corrected  in isolation. This is a rather major flaw


Given  an unknown  word  w


* \ L , find  w'   L


because context is extremely 
important for spelling



such that dist (w, w' )



min dist (w, v) .
v  L


correction, as 
illustrated in the 
following example:
p
o
w
e
r
 
c
r
d
 
�
 
p
o
w
e
r
 
c
o
r
d


i.e. for any out-of-lexicon  word in a text, find the


video crd � video card


closest  word  form(s)  in  the  available  lexicon  and	 	


hypothesize  it as  the  correct  spelling  alternative.
dist  can be any string-based  function;  for exam-


1 Note that the function does not have to be symmetric; thus, 
the notation dist(w,w ) is used with a loose sense.








 The misspelled  word  crd should  be corrected  to 
two different words depending on its contexts.2
 A formulation  of the spelling correction problem 
that takes into account context is the following:


formulation,  making it more general and allowing 
it  to  cover  two  other  important   spelling  error 
classes, concatenation and splitting, e.g.:
power point slides � powerpoint slides



Given a string  s


* ,  s



cl wcr



, with  w


*  \ L


c
h
a
t 
i
n
s
p
a
n
i
c
h  
�
 
c
h
a
t 
i
n 
s
p
a
n
i
s
h
Yet, it still does 
not account for 
another important


and cl , cr


L* , find w'


L such that 
dist (w, w' )


class  of  
cases  in  
web  
query  
correction  
which  is


and P(w' | cl wcr )


max
v  L:dist ( w,v )


P(v | cl wcr ) .


represented by 
out-of-lexicon  
words that are 
valid
in certain contexts 
(therefore,  s'   L* ), 
for example:


Spaces  and other word delimiters  are ignored  in
this formulation  and the subsequent  formulations 
for  simplicity,  although  text  tokenization  repre- 
sents an important part of the spelling-correction 
process, as discussed in Sections 5 and 6.
 The task definitions  enumerated  up to this point 
(on which most traditional spelling correction sys- 
tems are based) ignore word substitution errors. In 
the case of web searches, it is extremely important 
to provide correction suggestions for valid words 
when they are more meaningful as a search query 
than the original query, for example:
golf war � gulf war sap 
opera � soap opera
This problem is partially addressed by the task of
CSSC, which can be formalized as follows:



amd processors � amd processors (no change)
 The above phrase represents a legitimate query, 
despite the fact that it may contain unknown words 
when employing a traditional English lexicon.
 Some even more interesting cases not handled by 
traditional  spellers  and  also  not  covered  by  the 
latter  formulation  are  those  in  which  in-lexicon 
words should be changed to out-of-lexicon  words, 
as  in  the  following  examples,  where  two  valid 
words must be concatenated into an out of lexicon 
word:
gun dam planet � gundam planet
limp biz kit � limp bizkit
 These observations lead to an even more general 
formulation of the spelling-correction problem:


Given   a   set   of   confusable   valid   word   forms


Given s


* , find  s'


*   such that


dist(s, s' )


in  a  language   W


{w1 , w2 ,..., wn }


and  a  string


and P(s' | s)


max
t     * :dist ( s ,t )


P(t | s) .


s 	cl wi cr , 	choose


w j 	W


such 	that


For  the  first  time,  
the  formulation   no  
longer


P(w


| c w c  
)


max 
P(w


| c w c  
) .


makes 
explic
it use 
of a 
lexico
n of 
the 
langu
age.3  
In


j 	l     i   r


k  1..n


k 	l     i   r



some sense, 
the actual 
language in 
which the web


In the CSSC literature, the sets of confusables are
presumed known, but they could also be built for


queries are expressed becomes less important than 
the query-log data from which the string probabili-


each  in-lexicon  word  w  as  all  words


w'   with


ties are estimated.  This probability  
model can be


dist (w, w' )


, similarly  
to the 
approach  
investi-


seen as a 
substitute 
for a 
measure of 
the 
meaning-


gated by Mays et al. (1991), in which they chose a
   1 and employed an edit distance with all point 
changes having the same cost 1.
 The generalized  problem of phrasal spelling cor- 
rection can then be formulated as follows:


fulness of strings as web-queries. For example, an 
implausible random noun phrase in any of the tra- 
ditional corpora such as sad tomatoes is meaning- 
ful in the context of web search (being the name of 
a somewhat popular music band).



Given s


* , find


s'   L*



such that



dist(s, s' )



3 	The 
Error 
Model. 
String 
Edit 
Functio
ns


and P(s' | s)


max
t  L* :dist ( s ,t )


P(t | s) .



All  
formulations  
of  the  
spelling  
correction  
task


 Typically,  a correction  is desirable  when  s    L* 
(i.e. at least one of the component words is un- 
known)  but,  as  shown  above,  there  are  frequent 
cases  (e.g.  golf  war)  when  sequences  of  valid 
words should be changed to other word sequences. 
Note that word boundaries are hidden in this latter


given in the previous section used a string distance 
function  and  a  threshold  to  restrict  the  space  in 
which alternative spellings are searched. Various 
previous work has addressed  the problem of 
choosing appropriate functions (e.g. Kernigham et 
al. 1990, Brill and Moore, 2002; Toutanova and 
Moore, 2003).





2 To simplify the exposition, we only consider two highly 
probable corrections, but other valid alternatives exist, e.g. 
video cd.



3  A trusted lexicon may still be used in the estimation of the 
language model probability for the computation of  P(t | s) .








The choice of distance function d and threshold 
could be extremely important for the accuracy of a 
speller. At one extreme, the use of a too restrictive 
function/threshold   combination  can  result  in  not 
finding the best correction  for a given query. For 
example,  using  the  vanilla  Damerau-Levenshtein 
edit distance (defined as the minimum number of 
point  changes  required  to transform  a string into 
another, where a point change is one of the follow- 
ing operations:  insertion  of a letter, deletion  of a 
letter,  and substitution  of one letter  with  another 
letter) and a threshold 	1 , the correction donadl 
duck � donald duck would not be possible. At the 
other extreme,  the use of a less limiting function 
might   have   as   consequence    suggesting    very 
unlikely corrections. For example, using the same 
classical  Levenshtein  distance  and 	2   would 
allow the correction of the string donadl duck, but 
will also lead to bad corrections such as log wood
� dog food (based on the frequency of the queries,


alternative occurs frequently enough in the em- 
ployed  query  log.  However,  detroittigers   itself 
could be corrected to detroit tigers if presented as 
a stand-alone query to this spell checker, based on 
similar query-log frequency facts, which naturally 
leads  to  the  idea  of  an  iterative  correction  ap- 
proach.

albert einstein                   4834 
albert einstien                     525 
albert einstine                     149 
albert einsten                        27 
albert einsteins                     25 
albert einstain                       11 
albert einstin                         10 
albert eintein                           9 
albeart einstein                       6 
aolbert einstein                       6 
alber einstein                          4 
albert einseint                         3 
albert einsteirn                        3


as incorporated  in


P(s) ).  Nonetheless,  
large dis-


albert einsterin 	3


tance corrections are still desirable in a diversity of 
situations, for example:
platnuin rings �  platinum rings 
ditroitigers  � detroit tigers
 The system described in this paper makes use of a 
modified context-dependent weighted Damerau- 
Levenshtein edit function which allows insertion, 
deletion, substitution, immediate transposition, and 
long-distance  movement  of  letters  as  point 
changes, for which the weights were interactively 
refined using statistics from query logs.

4   The Language Model. Exploiting Large
Web Query Logs

A misspelling such as ditroitigers is far from the 
correct alternative and thus, it might be extremely 
difficult to find its correct spelling based solely on 
edit distance. Nonetheless, the correct alternative 
could be reached by allowing intermediate valid 
correction  steps, such  as ditroitigers  �  detroitti- 
gers � detroit tigers. But what makes detroittigers 
a valid correction step? Recall that the last formu-
lation of spelling  correction  in Section  3 did not


albert eintien                           3
alberto einstein                       3 
albrecht einstein                     3 
alvert einstein                         3

Table 1. Counts of different (mis)spellings of Albert
Einstein’s name in a web query log.

 Essential  to  such  an  approach  are  three  typical 
properties of the query logs (e.g. see Table 1):

words in the query logs are misspelled in vari- 
ous ways, from relatively easy-to-correct mis- 
spellings to very-difficult-to-correct ones, that 
make the user’s intent almost impossible to 
recognize;
the less malign (difficult to correct) a misspell- 
ing is the more frequent it is;
the correct spellings  tend to be more frequent 
than misspellings.

 In  this  context,  the  spelling  correction  problem 
can be given the following iterative formulation:

Given 	a 	string 	s 	* , 	find 	a 	sequence
0
*


explicitly  use  a lexicon  of the  language.  Rather,


s1 , s2 ,...sn


such   that


dist(si , si  1 )	,


any string that appears  in the query log used for 
training can be considered  a valid correction  and


P(si  1 | si )


max
t     * :dist ( si ,t )


P(t | si ) ,


i	0..n   1 ,


can be suggested  as an alternative  to the current 
web query based on the relative frequency of the


and P(s n


| s n )


max
t     * :dist ( sn ,t )


P(t | s n ) .


query and the alternative spelling. Thus, a spell 
checker built according to this formulation could 
suggest  the  correction  detroittigers  because  this


An  example  of  correction  that  can  be  made  by
iteratively applying the base spell checker is:
anol scwartegger  �  arnold schwarzenegger








Misspelled query: 	anol scwartegger
First iteration: 	arnold schwartnegger 
Second iteration: 	arnold schwarznegger 
Third iteration: 	arnold schwarzenegger 
Fourth iteration: 	no further correction

 Up to this point, we underspecified the notion of 
string in the task formulations given. One possibil- 
ity is to consider whole queries as the strings to be 
corrected and iteratively search for better logged 
queries according to the agreement between their 
relative frequencies and the character error model. 
This is equivalent to identifying all queries in the 
query log that are misspellings of other queries and 
for any new query, find a correction sequence of 
logged  queries.  While  such  an approach  exploits 
the vast information available in web-query logs, it 
only covers exact matches of the queries that ap- 
pear in these logs and provides a low coverage of 
infrequent queries. For example, a query such as 
britnet  spear  inconcert  could  not be corrected  if 
the correction britney spears in concert does not 
appear in the employed query log, although the 
substring britnet spear could be corrected  to brit- 
ney spears.
 To address the shortcomings of such an approach, 
we propose a system based on the following for- 
mulation, which uses query substrings:


words and word bigrams. The tokenization process 
uses space and punctuation  delimiters  in addition 
to the information provided about multi-word 
compounds (e.g. add-on and back-up) by a trusted 
English  lexicon  with approximately  200k entries. 
By using the tokenization process described above, 
we extracted word unigram and bigram statistics 
from query  logs  to be used  as the  system’s  lan- 
guage model.

5   Query Correction

An input query is tokenized using the same space 
and word-delimiter information in addition to the 
available  lexical information  as used for process- 
ing the query log. For each token, a set of alterna- 
tives is computed using the weighted Levenshtein 
distance function described in Section 3 and two 
different   thresholds   for   in-lexicon   and   out-of- 
lexicon tokens
 Matches are searched in the space of word uni- 
grams and bigrams extracted from query logs in 
addition to the trusted lexicon. Unigrams and bi- 
grams  are  stored  in  the  same  data  structure  on 
which  the  search  for  correction   alternatives   is 
done.  Because  of this,  the proposed  system  han- 
dles  concatenation  and  splitting  of  words  in  ex- 
actly     the     same     manner     as     it     handles


Given   s


* ,  find  a  
sequence


s , s ,...s


* , 
	trans
formations 
of words 
to other 
words.


0 	1     2 	n


such  that for each  i


0..n


1  there  exist  the de-


Once the sets of all 
possible alternatives 
are com-


compositions


1
i 	i ,0


...wli


, s i  1


1
i  1,1


li
i  1,1


puted for each 
word form in 
the query, a 
modified
Viterbi search 
(in which the 
transition 
probabilities


where  wk

k


are words or 
groups of 
words such 
that

k


are 
computed 
using 
bigram and 
unigram 
query-log 
statistics 
and output 
probabilitie
s are 
replaced 
with


dist (wi ,0 , wi  1,1 )


, 	i 	0..n   1,


k   1..li


and


inverse  distances  
between  words)  
is employed  to


P(si  1


| si )


ma
x
t     * :dist ( si 
,t )


P(t | s 
) ,
*


i 	0..n   
1 ,


find 
the 
best 
possibl
e 
alterna
tive 
string 
to the 
input 
query 
under 
the 
follow
ing 
constr
aint: 
no two 
adja-


and P(s n


| s n )


max
t     * :dist ( sn ,t )


P(t | s  ) .
*


cent  in-
vocabular
y  words  
are  
allowed  
to  
change 
simultane
ously.  
This  
constraint  
prevents  
changes


Note  that  the  length  of the  string  decomposition
may vary from one iteration  to the next one, for 
example:


such as log wood � dog food. An algorithmic con- 
sequence of this constraint is that there is no need 
to search all the possible paths in the trellis, which 
makes the modified search procedure much faster,


s0 	britenetspear   inconcert

s1 	britneyspears  in concert


s2 	britney spears in concert


l0 	2

l1 	3


l2 	4


as described further. We assume that 
the list of alternatives for each word 
is randomly ordered but the input 
word  is on the first position  of the 
list when  the  word  is  in  the  trusted  
lexicon.  In  this case, the searched 
paths form what we call fringes. 
Figure 1 presents an example of a 
trellis in which w1, w2  and w3  are 
in-lexicon word forms. Observe
that instead of computing the cost of 
k1·k2 possible


s 	britney spears in concert

 In the implementation  evaluated in this paper, we 
allowed   decompositions   of   query   strings   into


paths between the alternatives corresponding to w1 
and w2, we only need to compute the cost of k1+k2 
paths.












w1	w2	w3
1	2	3
1	1	1
1	2	3
2	2	2

 	 	 

1	2	3
k1 	k2 	k3






w4	w5	w6
	w7
5	6	7
1	1	1
5	6	7
2	2	2

 	 	 
4	5	6	7
1	k5 	k6 	k7
4
2
 
4
k4


diate corrections 
conditionally more 
probable than the 
left-hand-side 
query (log wood) 
and less prob- able 
than the right-hand-
side query (dog 
food).
 An iterative 
process is prone to 
other types of 
problems. Short 
queries can be 
iteratively trans- 
formed into other 
un-related queries; 
therefore, changing 
such queries is 
restricted 
additionally  in our 
system.  Another  
restriction  we 
imposed  is to not 
allow changes of 
in-lexicon words in 
the first iteration, 
so that easy-to-fix 
unknown-word  
errors are handled 
before any word 
substitution error.

6
   
E
v
a
l
u
a
t
i
o
n

For  this  work,  we  
are  concerned  
primarily  with


Figure 1. Example of trellis of the modified Viterbi search

 Because   we   use   word-bigram   statistics,   stop 
words such as prepositions and conjunctions may 
interfere negatively with the best path search. For 
example,  in correcting  a query such as platunum 
and rigs, the language model based on word bi- 
grams  would  not provide  a good  context  for the 
word form rigs.
 To avoid  this type of problems,  stop words  and 
their most likely misspelling are given a special 
treatment.  The  search  is  done  by  first  ignoring 
them, as in Figure 1, where w4  is presumed to be
such a word. Once a best path is found by ignoring 
stop  words,  the  best  alternatives  for  the  skipped 
stop words (or their misspellings) are computed in 
a second Viterbi search with fringes in which the 
extremities are fixed, as presented in Figure 2.


recall because providing good suggestions for mis- 
spelled queries  can be viewed  as more important 
than abstaining to provide alternative query sug- 
gestions for valid queries as long as these sugges- 
tions are reasonable  (for example, suggesting 
cowboy ropes for cowboy robes may not have ma- 
jor cost to a user). A real system would have a 
component that decides whether to surface a spell- 
ing suggestion  based on where we want to be on 
the ROC curve, thus negotiating between precision 
and recall.
 One problem with evaluating a spell checker de- 
signed to correct search queries is that evaluation 
data is hard to get. Even if the system were used 
by a search engine and click-through information 
were  available,  such  information  would  provide 
only a crude measure of precision and would not 
allow us to measure recall, by capturing only cases 
in which the corrections proposed by that particu- 
lar speller are clicked on by the users.
We  performed  two  different  evaluations  of  the


w1 	w2 	w3
1	2	3
1	1	1
1	2	3
2	2	2

 	 	 

1	2	3
k1 	k2 	k3


w4 	w5 	w6 
	w7
5	6	7
1	1	1
5	6	7
2	2	2

 	 	 
4	5	6	7
1	k5 	k6 	k7
4
2
 
4
k4


proposed  system.4   
The first  
evaluation  was  
done on a test set 
comprising 1044 
unique randomly 
sampled  queries  
from  a  daily  
query  log,  which 
were  annotated  
by  two  
annotators.  Their  
inter-
agreement  rate  
was  91.3%.  864  
of  these  queries 
were considered 
valid by both 
annotators; for the 
other 180, the 
annotators 
provided spelling 
correc- tions.  The 
overall  agreement  
of our  system  
with the annotators  
was 81.8%.  The 
system  suggested
131 alternative 
queries for the 
valid set, counted 
as
false positives, and 
156 alternative 
queries for the


Figure 2. Modified Viterbi search – stop-word treatment

 The approach of search with fringes coupled with 
an  iterative  correction  process  is both  very  effi- 
cient  and  very  effective.  In  each  iteration,  the


misspelled  set.  Table  2  shows  the  accuracy  ob- 
tained by the proposed system and results from an 
ablation study where we disabled various compo- 
nents of the system, to measure their influence on 
performance.


search space is much reduced. Changes such as log	 	


wood � dog food are avoided because they can not 
be made in one iteration and there are no interme-


4     The   test   data   sets   can   be   downloaded    from 
http://research.microsoft.com/~silviu/Work










All 
quer
ies
V
a
li
d
Mi
ssp
ell
ed
N
r
. 
q
u
e
r
i
e
s
1
0
4
4
8
6
4
1
8
0
F
u
ll 
s
y
s
t
e
m
8
1
.
8
8
4
.
8
6
7
.
2
N
o
 
l
e
x
i
c
o
n
7
0
.
3
7
2
.
2
6
1
.
1
N
o 
q
u
er
y 
lo
g
7
7
.
0
8
2
.
1
5
2
.
8
Al
l 
ed
its 
eq
ua
l
8
0
.
4
8
3
.
3
6
6
.
1
Un
igr
am
s 
on
ly
5
4
.
7
5
7
.
4
4
1
.
7
1 
ite
rat
ion 
on
ly
8
0
.
9
8
8
.
0
4
7
.
2
2 
iter
atio
ns 
onl
y
8
1
.
3
8
4
.
4
6
6
.
7
N
o
 
f
r
i
n
g
e
s
8
0
.
6
8
3
.
3
6
7
.
2

Table 2. Accuracy of various instantiations of the system

 By completely removing the trusted lexicon, the 
accuracy   of  the  system   on  misspelled   queries 
(61.1%) was higher than in the case of only using 
a trusted lexicon and no query log data (52.8%). It 
can also be observed that the language model built 
using query logs is by far more important than the


A post-analysis of the results showed that the sys- 
tem  suggested  in  many  cases  reasonable  correc- 
tions  but  different  from  the  gold  standard  ones. 
Many false positives could be considered reason- 
able suggestions,  although  it is not clear whether 
they  would  have  been  helpful  to  the  users  (e.g.
2002 kawasaki ninja zx6e � 2002 kawasaki ninja 
zx6r was counted as an error, although the sugges- 
tion represents a more popular motorcycle model). 
In  the  case  of  misspelled  queries  in  which  the 
user’s intent was not clear, the suggestion made by 
the system  could  be considered  valid  despite  the 
fact that it disagreed  with the annotators’  choice 
(e.g. gogle � google instead of the gold standard 
correction goggle).
 To address the problems generated by the fact that 
the annotators could only guess the user intent, we 
performed a second evaluation, on a set of queries 
randomly  extracted  from query log data, by sam-


channel model employed: using a poorer character


pling pairs of successive  queries


(q1 , q2 )


sent by


error model  by setting  all edit weights  equal did
not have a major impact on performance (66.1% 
recall), while using a poorer language model that 
only  employs  unigram  statistics  from  the  query 
logs crippled the system (41.7% recall). Another 
interesting aspect is related to the number of itera- 
tions. Because the first iteration is more conserva- 
tive than the following iterations, using only one 
iteration led to fewer false positives but also to a 
much lower recall (47.2%). Two iterations were 
sufficient to correct most of the misspelled queries 
that  the  full  system  could  correct.  While  fringes 
did not have a major impact on recall, they helped 
avoid false positives (and had a major impact on 
speed).


the  same  users  in  which  the  queries  differ  from 
one another by an un-weighted  edit distance of 
at most  1+(len( q1 )+len( q2 ))/10  (i.e.  allow  a  
point change for every 5 letters). We then 
presented the list to human annotators who had 
the option to re- ject a pair, choose one of the 
queries as a valid cor- rection  of  the  other,  or  
propose  a  correction  for both  when  none  of  
them  were  valid  but  the  in- tended valid query 
was easy to guess from the se- quence, as in 
example 3 below:
(audio flie, audio file) => audio file
(bueavista, buena vista) => buena vista
(carrabean nooms, carrabean rooms) => caribbean 
rooms

 Table 3 shows the performance obtained by dif- 
ferent instantiations of the system on this set.


85
80.7
80

75

70 	66.1
65



81.8



67.2


81.6




68.9



81.2

All queries
Mispelled queries
6
9
.
4


1 month 	2 months 	3 months 	4 months

Figure 3. Accuracy and recall as functions of the number of 
monthly query logs used to train the language model

 Figure 3 shows the performance of the full system 
as a function of the number of monthly query logs 
employed. While both the total accuracy and the 
recall increased when using 2 months of data in- 
stead of 1 month, by using more query log data (3 
and  4  month),  the  recall  (or  accuracy  on  mis- 
spelled  queries)  still improves  but at the expense 
of having more false positives for valid queries, 
which leads to an overall slightly smaller accuracy.







Table 3. Accuracy of the proposed system on a set which 
contains misspelled queries that the users had reformulated

 The main system disagreed 99 times with the gold 
standard, in 80 of these cases suggesting a differ- 
ent correction. 40 of the corrections were not ap- 
propriate (e.g. porat was corrected  by our system 
to  pirate  instead  of  port  in  chinese  porat  also 
called  xiamen),  15  were  functionally  equivalent








corrections  given  our  target  search  engine  (e.g. 
audio flie � audio files instead of audio file), 17 
were different valid suggestions (e.g. bellsouth 
lphone isting � bellsouth phone listings instead of 
bellsouth  telephone  listing),  while  8  represented 
gold standard errors (e.g. the speller correctly sug- 
gested  brandy  sniffters  �  brandy  snifters  instead 
of brandy sniffers). Out of 19 cases in which the 
system did not make a suggestion, 13 were genu- 
ine errors (e.g. paul waskiewiscz with the correct 
spelling paul waskiewicz),  4 were cases in which 
the original input was correct, although different 
from the user’s intent (e.g. cooed instead of coed) 
and 2 were gold standard errors (e.g. commandos 3 
walkthrough  had the wrong correction  commando
3 walkthrough,  as this  query  refers  to  a popular
videogame called “commandos 3”).

Diff
eren
ces
Gold 
std 
error
s
For
mat
Diff
. 
vali
d
Real 
Erro
rs
80
+1
9
8
+
2
15
+0
1
7
+
4
40
+1
3

The above table shows a synthesis of this error 
analysis on the second evaluation set. The first 
number in each column refers to a precision error 
(i.e. the speller suggested something different than 
the  gold  standard),  while  the  second  refers  to  a 
recall error (i.e. no suggestion).
 As a result of this error analysis, we could argua- 
bly  consider  that  while  the  agreement  with  the 
gold standard  experiments  are useful for measur- 
ing the relative importance of components, they do 
not give us an absolute measure of  system useful- 
ness/accuracy.

Agr
eem
ent
Corr
ectn
ess
Pre
cis
ion
R
e
c
al
l
7
3
.
1
8
5
.
5
8
8
.
4
8
5
.
4

 In the above table, we consider correctness as the 
relative number of times the suggestion made by 
the speller was correct or reasonable; precision 
measures the number of correct suggestions in the 
total number of spelling suggestions made by the 
system; recall is computed as the relative number 
of correct/reasonable suggestions made when such 
suggestions were needed.
 As an additional verification and to confirm the 
difficulty of the test queries, we sent a set of them 
to Google and observed that Google speller’s 
agreement  with  the  gold  standard  was  slightly 
lower than our system’s agreement.

7   Conclusion

To our knowledge, this paper is the first to show a 
successful attempt of using the collective knowl- 
edge stored  in search  query  logs for the spelling


correction task. We presented a technique to mine 
this extremely informative but very noisy resource 
that actually exploits the errors made by people as 
a way to do effective query spelling correction. A 
direction that we plan to investigate is the adapta- 
tion of such a technique to the general purpose 
spelling correction, by using statistics from both 
query-logs and large office document collections.

Acknowledgements

We wish to thank Robert Ragno and Robert Roun- 
thwaite for helpful comments and discussions.

References

Brill, E. and R. Moore. 2000. An improved error model for 
noisy channel spelling correction. In Proceedings of the ACL
2000, pages 286-293.
Cherkassky, V., N. Vassilas, G.L. Brodt, R.A. Wagner, and 
M.J. Fisher. 1974. The string to string correction problem. In 
Journal of ACM, 21(1):168-178.
Damerau, F.J. 1964. A technique for computer detection and 
correction of spelling errors. In Communications of ACM,
7(3):171-176.
Garside, R., G. Leech and G. Sampson. 1987. Computational 
analysis of English: a corpus-based approach, Longman.
Golding, A.R. 1995. A Bayesian hybrid method for context- 
sensitive spelling correction. In Proceedings of the Work- 
shop on Very Large Corpora, pages 39-53.
Golding, A.R. and D. Roth. 1996. Applying winnow to con- 
text-sensitive spelling correction. In Proceedings of ICML
1996, pages 182-190.
Heidorn, G.E., K. Jensen, L.A. Miller, R.J. Byrd and M.S. 
Chodorow. 1982. The EPISTLE text-critiquing system. In 
IBM Systems Journal, 21(3):305-326.
Jurafsky, D. and J.H. Martin. 2000. Speech and language 
processing. Prentice-Hall.
Kernighan, M., K. Church, and W. Gale. 1990. A spelling 
correction program based on a noisy channel model. In Pro- 
ceedings of COLING 1990.
Kukich, K. 1992. Techniques for automatically correcting 
words in a text. In Computing Surveys, 24(4):377-439.
Mays,  E.,  F.J.  Damerau and  R.L.  Mercer. 1991.  Context- 
based spelling correction. In Information Processing and 
Management, 27(5):517-522.
Mangu, L. and E. Brill. 1997. Automatic rule acquisition for 
spelling correction. In Proceedings of the ICML 1997, pages
734-741.
McIlroy, M.D. 1982. Development of a spelling list. In J- 
IEEE-TRANS-COMM, 30(1);91-99.
Peterson, J.L. 1980. Computer programs for spelling correc- 
tion: an experiment in program design. Springer-Verlag. 
Rieseman, E.M. and A.R. Hanson. 1974. A contextual post-
processing system for error correction using binary n-grams.
In IEEE Transactions on Computers, 23(5):480-493.
Toutanova, K. and R. C. Moore. 2002. Pronunciation Model- 
ing for Improved Spelling Correction. In Proceedings of the 
ACL 2002.pages 141-151.
Wittgenstein,  L.  1968.  Philosophical  Investigations.  Basil
Blackwell, Oxford.
