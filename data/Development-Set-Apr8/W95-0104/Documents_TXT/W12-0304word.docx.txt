Google Books N-gram  Corpus used as a Grammar Checker



Rogelio Nazar	Irene Renau 
University Institute of Applied Linguistics 
Universitat Pompeu Fabra
Roc Boronat 138
08018 Barcelona, Spain
{rogelio.nazar,irene.renau}@upf.edu







Abstract

In this research  we explore the possibil- 
ity of using a large n-gram corpus (Google 
Books) to derive lexical transition probabil- 
ities from the frequency of word n-grams 
and then use them to check and suggest cor- 
rections in a target text without  the need for 
grammar rules. We conduct several experi- 
ments in Spanish, although our conclusions 
also reach other languages since the proce- 
dure is corpus-driven. The paper reports 
on experiments  involving different types 
of grammar errors, which are conducted 
to test different grammar-checking proce- 
dures, namely, spotting possible errors, de- 
ciding between different lexical possibili- 
ties and filling-in the blanks in a text.


1   Introduction

This paper discusses a series of early experiments 
on a methodology  for the detection and correc- 
tion of grammatical errors based on co-occurrence 
statistics using an extensive corpus of n-grams 
(Google Books, compiled by Michel et al., 2011). 
We start from two complementary  assumptions: 
on the one hand, books are published accurately, 
that is to say, they usually go through different 
phases of revision and correction with high stan- 
dards and thus a large proportion  of these texts 
can be used as a reference corpus for inferring the 
grammar rules of a language. On the other hand, 
we hypothesise that with a sufficiently  large cor- 
pus a high percentage of the information  about 
these rules can be extracted with word n-grams. 
Thus, although there are still many grammatical 
errors that cannot be detected with this method, 
there is also another important group which can be


identified  and corrected successfully,  as we will 
see in Section 4.
  Grammatical  errors are the most difficult and 
complex type of language errors, because gram- 
mar is made up of a very extensive number of 
rules and exceptions.  Furthermore,  when gram- 
mar is observed in actual texts, the panorama be- 
comes far more complicated,   as the number of 
exceptions grows and the variety and complexity 
of syntactical structures increase to an extent that 
is not predicted by theoretical  studies of gram- 
mar.  Grammar errors are extremely important, 
and the majority  of them cannot be considered to 
be performance-based  because it is the meaning 
of the text and therefore,  the success or failure of 
communication, that is compromised.
  To our knowledge, no grammar book or dictio- 
nary has yet provided  a solution  to all the prob- 
lems  a person may have when he or she writes 
and tries to follow the grammar rules of language. 
Doubts that arise during the writing process are 
not always clearly associated to a lexical  unit, or 
the writer is not able to detect such an associa- 
tion, and this makes it difficult to find the solution 
using a reference book.
  In recent years, some advances have been made 
in the automatic detection of grammar mistakes 
(see Section  2).  Effective  rule-based methods 
have been reported, but at the cost of a very time- 
consuming task and with an inherent lack of flex- 
ibility.  In contrast, statistical  methods are easier 
and faster to implement,   as well as being  more 
flexible and adaptable.   The experiment we will 
describe in the following sections is the first part 
of a more  extensive  study. Most probably, the 
logical step to follow in order to continue such 
a study will be a hybrid  approach, based on both




27

Proceedings of the EACL 2012 Workshop on Computational Linguistics and Writing, pages 27–34, 
Avignon, France, April 23, 2012. Qc 2012 Association for Computational Linguistics


statistics and rules. Hence, this paper aims to con- 
tribute to the statistical approach applied to gram- 
mar checking.
  The  Google Books N-gram Corpus is  a 
database of n-grams of sequences of up to 5 words 
and records the frequency distribution of each unit 
in each year from 1500 onwards. The bulk of the 
corpus, however, starts from 1970, and that is the 
year we took as a starting  point for the material 
that we used to compile our reference corpus.
  The idea of using this database  as a grammar 
checker is to analyse an input text and detect any 
sequence of words that cannot be found in the 
n-gram  database (which only contains n-grams 
with frequency equal to or greater than 40) and, 
eventually, to replace  a unit in the text with one 
that makes a frequent n-gram.  More specifically, 
we conduct four types of operations:  accepting 
a text and spotting possible errors; inflecting a 
lemma into the appropriate form in a given  con- 
text; filling-in the blanks in a text; and selecting, 
from a number of options, the most probable word 
form for a given context.  In order to evaluate the 
algorithm, we applied it to solve exercises from a 
Spanish grammar book and also tested the detec- 
tion of errors in a corpus  of real errors made by 
second language learners.
  The paper is organised  as follows: we first of- 
fer a brief description of related work, and then 
explain our methodology for each of the experi- 
ments. In the next section, we show the evaluation 
of the results in comparison to the Microsoft  Word 
grammar checker and, finally, we draw some con- 
clusions and discuss lines of future work.

2   Related Work

Rule-based  grammar checking started in  the


proach needs mechanisms to take into account er- 
rors within a rigid system of rules, and thus differ- 
ent strategies were implemented to gain flexibility 
(Weischedel and Black, 1980; Douglas and Dale,
1992; Schneider and McCoy, 1998 and others). 
Bolt (1992) and Kohut and Gorman (1995) eval- 
uated several grammar  checkers available  at the 
time and concluded that, in general, none of the 
proposed strategies achieved high percentages of 
success.
  There  are reasons to believe that the limita- 
tions of rule-based methods could be overcome 
with statistical or knowledge-poor  approaches, 
which started  to be used  for natural language 
processing in the late 1980s and 1990s. Atwell 
(1987) was among the first to use a statistical  and 
knowledge-poor  approach to detect grammatical 
errors in POS-tagging. Other studies,  such  as 
those by Knight and Chandler (1994) or Han et 
al. (2006), for instance, proved more successful 
than rule-based systems in the task of detecting 
article-related errors. There are also other studies 
(Yarowsky, 1994; Golding, 1995 or Golding and 
Roth, 1996) that report the application of deci- 
sion lists and Bayesian classifiers for spell check- 
ing; however, these models cannot be applied to 
grammar error detection.  Burstein et al. (2004) 
present an idea similar  to the present paper, since 
they use n-grams for grammar checking. In their 
case, however, the model is much more compli- 
cated since it uses a machine learning  approach 
trained on a corpus  of correct English and using 
POS-tags bigrams as features apart from word bi- 
grams. In addition,  they use a series of statistical 
association  measures instead of using plain fre- 
quency.
Other proposals of a similar nature are those


1980s and crystallised  in the implementation of


which use  the web as  a  corpus   (More´


et al.,


different tools:  papers  by MacDonald  (1983), 
Heidorn et al. (1982) or Richardson and Braden- 
Harder (1988) describe some of them (see Lea- 
cock et al., 2010, for a state  of the art related 
to studies focused on language learning). This 
approach has continued  to be used until recently 
(see Arppe,  2000;  Johannessen et al., 2002; and 
many others) and is the basis  of the work re- 
lated with the popular grammar checker in Mi- 
crosoft Word (different aspects  of the tool are 
described in Dolan et al., 1993; Jensen  et al.,
1993; Gamon et al., 1997 and Heidorn, 2000:
181-207, among others). The knowledge-rich ap-


2004; Yin et al., 2008; Whitelaw  et al., 2009), al- 
though the majority of these authors also apply 
different  degrees of processing of the input text, 
such as lemmatisation, POS-tagging and chunk- 
ing.  Whitelaw et al.  (2009), working on spell 
checking,  are among the few who disregard ex- 
plicit linguistic knowledge. Sjo¨ bergh (2009) at- 
tempted  a similar approach for grammar check- 
ing in Swedish, but with modest results. Nazar 
(in press) reports  on an experiment  where cor- 
pus statistics are used to solve a German-language 
multiple choice exam,  the result being a score 
similar to that of a  native  speaker. The sys-


tem does not use any kind of explicit knowledge 
of German grammar or vocabulary:   answers are 
found by simply  querying a search engine and se- 
lecting the most frequent combination of words. 
The present paper is a continuation  and extension 
of that idea,  now with a specific  application  to 
the practical problem of checking the grammar of 
texts in Spanish.
  In spite of decades of work on the subject of 
grammar-checking  algorithms,  as summarised in 
the previous lines, the general  experience  with 
commercial grammar checkers is still disappoint- 
ing, the most serious problem being that in the 
vast majority of cases errors in the analysed texts 
are left undetected. We believe that, in this con- 
text, a very simple grammar checker based on cor- 
pus statistics could prove to be helpful,  at least as 
a complement  to the standard procedures.

3   Methodology

In essence, the idea for this experiment is rather 
simple. In all the operations, we contrast the se- 
quences of words  as they are found in an input 
text with those recorded in Google’s database. In 
the error detection phase, the algorithm  will flag 
as an error any sequence of two words that is not 
found in the database, unless either of the two 
words is not found individually in the database, 
in which  case the sequence is ignored.  The idea is 
that in a correction  phase the algorithm  will out- 
put a ranked list of suggestions to replace each de- 
tected error in order to make the text match the n- 
grams of the database. The following subsections 
offer a detailed description of the methodology of 
each experiment.   For the evaluation, we tested 
whether the algorithm could solve grammar exer- 
cises from a text-book (Montol´ıo, 2000), which is 
one of the most widely used Spanish text-books 
for academic writing for native speakers, cover- 
ing various topics such as pronouns, determiners, 
prepositions,  verb tenses, and so on. In addition, 
for error detection we used a corpus of L2 learners 
(Lozano, 2009).

3.1   Error Detection
Error detection is, logically, the first phase  of 
a grammar checking  algorithm  and, in practice, 
would be followed by some correction operation, 
such as those described  in 3.2 to 3.4. In the er- 
ror detection procedure, the algorithm  accepts an 
input sentence or text and retrieves the frequency


of all word types (of forms as they appear in the 
text and not the lemmata)  as well as all the dif- 
ferent  bigrams  as sequences of word forms, ex- 
cluding punctuation signs. The output of this pro- 
cess is the same text with two different types of 
flags indicating,  on the one hand, that a particular 
word is not found or is not frequent enough and, 
on the other hand, that a bigram  is not frequent. 
The frequency threshold can be an arbitrary pa- 
rameter, which would  measure the “sensitivity” of 
the grammar checker. As already mentioned, the 
minimum frequency of Google n-grams is 40.
  As the corpus is very large,  there are a large 
number of proper nouns, even names that are un- 
usual in Spanish. For example, in the sentence En
1988 Jack Nicholson, Helen Hunt y Kim Basinger 
recibieron sendos Oscar  (‘In 1988 Jack Nichol- 
son, Helen Hunt and Kim Basinger each received 
one Oscar’), bigrams such as y Kim or, of course, 
others like Jack Nicholson  are considered frequent 
by the system because these actors are famous in 
the Spanish context,  but this is not the case for 
the bigram Mart´ın Fiz, belonging to another sen- 
tence, which is considered infrequent and treated 
as an error (false positive),  because the name of 
this Spanish athlete  does not appear with suffi- 
cient frequency. Future versions will address this 
issue.

3.2   Multiple  Choice Exercises

In this scenario, the algorithm is fed with a sen- 
tence or text which  has a missing word and a se- 
ries of possibilities from which to decide the most 
appropriate one for that particular context.
  For instance, given an input  sentence such as El 
coche se precipito´ por *un,una*  pendiente (‘The 
car plunged down a  slope’), the algorithm  has 
to choose the correct option between un and una 
(i.e., the masculine and feminine forms of the in- 
definite article).
  Confronted with this input data, the algorithm 
composes different  trigrams with each possibility 
and one word immediately to the left and right 
of the target position. Thus, in this case, one of 
the trigrams would be por un pendiente and, sim- 
ilarly, the other would be por una pendiente. As 
in 3.1., the selection procedure is based on a fre- 
quency comparison of the trigrams in the n-gram 
database, which in this case favours the first op- 
tion, which is the correct one.
In case the trigram  is not found in the database,


there are two back-off operations, consisting in 
separating each trigram  into two bigrams, with the 
first and second position in one case and the sec- 
ond and third in the other. The selected option 
will be the one with the two bigrams that, added 
together, have the highest frequency.

3.3   Inflection

In this case, the exercise consists in selecting the 
appropriate word form of a  given lemma in a 
given context.  Thus, for instance, in another ex- 
ercise from Montol´ıo’s book, No le *satisfacer* 
en absoluto el acuerdo al que llegaron con sus 
socios alemanes (‘[He/She]  is not at all satisfied 
with the agreement reached with [his/her] Ger- 
man partners’), the algorithm has to select the cor- 
rect verbal inflection of the lemma satisfacer.
  This operation is similar to the previous one, 
the only difference being that in this case we use 
a lexical  database of Spanish that allows us to ob- 
tain all the inflected forms of a given lemma.  In


Word 2007 with the same dataset. In the first col- 
umn of the table we divide the errors into differ- 
ent types as classified  in Montol´ıo’s book. Perfor- 
mance figures  are represented as usual in infor- 
mation retrieval (for details, see Manning  et al.,
2008): the columns represent the numbers of true 
positives (t p), which are those errors that were ef- 
fectively  detected by each system; false negatives 
( f n) referring to errors that were not detected, 
and false positives ( f p), consisting in those cases 
that were correct, but which the system wrongly 
flagged as errors. These values allowed  us to de- 
fine precision (P) as t p/(t p + f p), recall (R) as 
t p/(t p + f n) and F 1 as 2.P.R/(P + R).
The algorithm  detects (with a success rate of
80.59%),  for example,  verbs with an incorrect 
morphology,  such as *apreto (instead of aprieto,
‘I press’). Nevertheless, the system also makes 
more interesting detections, such as the incorrect 
selection of the verb tense, which requires infor- 
mation provided by the context: Si os vuelve a


this case, then, the algorithm  searches for the tri-


molestar,  no *volved a hablar con


e´l  (‘If  [he]


gram le * en, where * is defined  as all the inflec- 
tional paradigm of the lemma.

3.4   Fill-in the blanks

The operation of filling-in the blank spaces  in 
a sentence  is another typical grammar exercise. 
In this case, the algorithm  accepts an input sen- 
tence such as Los asuntos * ma´ s preocupan  a la 
sociedad son los relacionados con la econom´ıa 
(‘The issues of greatest concern  to society  are 
those related  to the economy’), from the same 
source,  and suggests a list of candidates.  As in 
the previous  cases, the algorithm will search for a 
trigram  such as asuntos * ma´ s, where the * wild- 
card in this case means any word, or more pre- 
cisely, the most frequent word in that position. In 
the case of the previous example, which is an ex- 
ercise about relative pronouns, the most frequent 
word in the corpus and the correct option is que.



4   Results and Evaluation

4.1   Result of error detection

The results of our experiments are summarised 
in Table 1, where we distinguish between differ- 
ent types of grammar errors and correction opera- 
tions. The table also offers a comparison of the 
performance of the algorithm  against Microsoft


bothers you again, do not talk to him again’). In 
this sentence, the correct tense for the second verb 
is volva´ is, as the imperative  in negative sentences 
is made with the subjunctive.  In the same way, 
it is possible to detect incorrect  uses of the ad- 
jective  sendos (‘for each other’),  which cannot be 
put after the noun, among other particular  con- 
straints: combinations such as *los sendos actores 
(‘both actors’) or *han  cerrado filiales sendas 
(‘they have closed both subsidiaries’)  are marked 
as incorrect  by the system.
  In order to try to balance the bias inherent to 
a grammar text-book, we decided to replicate the 
experiment with real errors. The decision to ex- 
tract exercises from a grammar  book was based 
on the idea that this book would contain  a di- 
verse sample of the most typical mistakes,  and 
in this sense it is representative.  But as the ex- 
amples given by the authors are invented,  they 
are often uncommon and unnatural, and of course 
this frequently   has a negative  effect on perfor- 
mance.  We  thus repeated  the experiment  us- 
ing sentences from the CEDEL2 corpus (Lozano,
2009), which is a corpus  of essays in Spanish 
written by non-native  speakers with different lev- 
els of proficiency.
  For this experiment, we only used essays writ- 
ten by students classified as “very advanced”. We 
extracted 65 sentences, each containing  one error.




T
h
i
s
 
E
x
p
e
r
i
m
e
n
t
W
o
r
d
 
2
0
0
7
Typ
e 
of 
err
or
t
p
f
n
f
p
% 
P
% 
R
% 
F1
t
p
f
n
fp
% 
P
% 
R
% 
F1
ger
un
d
ve
rb 
m
or
p
h
ol
o
gy 
n
u
m
er
al
s 
gr
a
m
m
ati
ca
l 
nu
m
be
r 
pr
e
p
os
iti
o
ns
adj
ecti
ve 
“se
nd
os”
v
a
r
i
o
u
s
 
t
o
t
a
l
9
5
4
4
1
0
2
5
5
5
5
16
2
8
1
7
9
8
4
0
0
5
2
13
4
9
1
3
7
1
1
7
1
5
2
10
0
5
0
80.
59
36.
36
90.
90
59.
52
83.
33
51.
40
61.
83
52.
94
76.
05
30.
76
55.
55
38.
46
10
0
51.
40
54.
72
51.
42
78.
25
33.
32
68.
95
46.
72
90.
90
51.
40
58.
05
9
6
0
6
1
0
1
3
1
3
3
13
2
8
1
1
7
8
5
2
4
7
4
16
4
1
3
0
1
0
0
10
15
9
0
95.
23
1
0
0
90.
90
1
0
0
1
0
0
76.
74
89.
79
52.
94
84.
50
46.
15
55.
55
2
0
2
0
30.
84
44.
59
66.
66
89.
54
63.
15
68.
95
33.
33
33.
33
43.
99
59.
58

Table 1: Summary of the results obtained by our algorithm in comparison to Word 2007




Since the idea was to check grammar, we only se- 
lected material that was orthographically correct, 
any minor typos being corrected beforehand.  In 
comparison with the mistakes dealt with in the 
grammar book, the kind of grammatical problems 
that students make are of course very different. 
The most frequent type of errors in this sample 
were gender agreement (typical in students with 
English  as L1), lexical errors, prepositions and 
others such as problems  with pronouns or with 
transitive verbs, among others.
  Results  of this second  experiment   are  sum- 
marised in Table 2. Again, we compare perfor- 
mance against Word 2007 on the same dataset. In 
the case of this experiment, lexical errors and gen- 
der agreement show the best performance because 
these phenomena  appear at the bigram level, as 
in *Despue´s del boda (‘after the wedding’) which 
should be feminine (de la boda), or *una tranv´ıa 
ele´ctrica (‘electric tram’) which should be mas- 
culine (un tranv´ıa).  But there  are other cases 
where the error involves  elements that are sep- 
arated from each other by long distances and of 
course will not be solved with the type of strategy 
we are discussing, as in the case of *un pa´ıs donde 
el estilo de vida es avanzada  (‘a country with 
an advanced lifestyle’), where the adjective avan- 
zada is wrongly put in feminine when it should be 
masculine (avanzado),  because it modifies a mas- 
culine noun estilo.
  In general, results of the detection  phase are 
far from perfect but at least comparable to those 
achieved by Word in these categories.  The main 
difference between the performance of the two al- 
gorithms is that ours tends to flag a much  larger 
number of errors, incurring  in many false posi- 
tives and severely degrading performance.   The 
behaviour of Word is the opposite, it tends to flag 
fewer errors, thus leaving many errors undetected. 
It can be argued that, in a task like this, it is prefer- 
able to have false positives rather than false neg-


atives,  because the difficult part of producing  a 
text is to find the errors.  However,  a system that 
produces many false positives will lose the con- 
fidence of the user. In any case, more important 
than a difference  in precision is the fact that both 
systems tend to detect very different types of er- 
rors, which reinforces the idea that statistical al- 
gorithms could be a useful complement to a rule- 
based system.

4.2   Result of multiple  choice exercise

The results of the multiple  choice exercise in the 
book are shown in Table 3. Again, we compared 
performance with that achieved by Word. In order 
to make this program solve a multiple  choice ex- 
ercise we submitted the different possibilities for 
each sentence and checked whether it was able to 
detect errors in the wrong  sentences and leave the 
correct ones unflagged.
  Results  in  this case  are  similar in  general 
to those reported  in Section 4.1.  An example 
of a  correct trial is with the fragment  *el,la* 
ge´nesis del problema (‘the genesis of the prob- 
lem’), where the option selected by the algorithm 
is la ge´nesis (feminine  gender).  In contrast, it is 
not capable of giving the correct answer when the 
context is very general, such as in *los,las* pen- 
dientes son uno de los complementos ma´ s vendi- 
dos como regalo (‘Earrings are one of the acces- 
sories most frequently sold as a gift’), in which 
the words to choose from are at the beginning  of 
the sentence and they are followed  by son (‘they 
are’), which comes from ser, perhaps the most 
frequent and polysemous Spanish verb. The cor- 
rect answer is los (masculine article), but the sys- 
tem offers the incorrect las (feminine) because of 
the polysemy of the word, since las pendientes 
also exist, but means ‘the slopes’ or even ‘the ones 
pending’.




This Experiment
W
o
r
d
 
2
0
0
7
Typ
e 
of 
err
or
tp
fn
fp
% 
P
% 
R
% 
F1
tp
fn
fp
% 
P
% 
R
% 
F1
gen
der 
agr
ee
me
nt
l
e
x
i
c
a
l 
s
e
l
e
c
t
i
o
n
 
p
r
e
p
o
s
i
t
i
o
n
s
 
v
a
r
i
o
u
s
tot
al
9
16
2
4
31
6
10
11
7
34
3
4
2
5
17
7
5
8
0
5
0
44.
44
64.
58
6
0
61.
53
15.
38
36.
36
47.
69
66.
66
69.
56
23.
52
39.
99
54.
86
7
4
0
3
14
8
22
13
8
51
0
0
0
3
3
1
0
0
1
0
0
0
5
0
82.
35
46.
66
15.
38
0
27.
27
21.
53
63
.6
3
26
.6
6
0
35
.2
9
34
.1
4

Table 2: Replication of the experiment with a corpus of non-native speakers (CEDEL2, Lozano, 2009)


Tri
als
This 
Experim
ent
Word 
2007
Typ
e 
of 
err
or

Co
rre
ct
% 
P
Co
rre
ct
% 
P
adv
erb
s
ge
nre
con
fusi
on 
DO
-IO
9
1
0
4
8
7
2
88.
89
70.
00
50.
00
5
3
2
55.
55
3
0
5
0

Table 3: Solution of the multiple choice exercise






4.3   Result of inflection exercise

Results in the case of the inflection  exercise are 
summarised in Table 4. When giving verb forms, 
results are correct  in 66.67% of the cases.   For 
instance, in the case of La mayor´ıa de la gente
*creer* que...  (‘The majority of people think
that...’), the correct answer is cree, among other 
possibilities  such as creen (plural)  or cre´ıa (past). 
But results are generally  unsuccessful (22.22%) 
when choosing the correct  tense, such as in the 
case of Si el problema me *atan˜ er* a m´ı, ya hu- 
biera hecho algo para remediarlo (‘If  the prob- 
lem was of my concern, I would have already 
done something to solve it’). In this example, the 
correct verb tense is atan˜ era or atan˜ ese, both of 
which are forms for the third person past subjunc- 
tive used in conditional  clauses, but the system 
gives atan˜ e, a correct  form for the verb atan˜ er 
that, nevertheless, cannot be used in this sentence. 
As it can be seen, the problem  is extremely diffi- 
cult for a statistical  procedure (there are around
60 verb forms in Spanish), and this may explain 
why the results of this type of exercise were more 
disappointing.

Typ
e 
of 
err
or
Tri
als
Co
rre
ct
% 
P
ver
b 
nu
mb
er
ver
b 
ten
se
9
9
6
2
66.
67
22.
22

Table 4: Results of the inflection exercise


4.4   Result of filling-in the blanks

When asked to restore a missing  word in a sen- 
tence, the algorithm is capable of offering the cor- 
rect answer in cases such  as El abogado * de- 
fendio´ al peligroso asesino... (‘The lawyer -who-


defended the dangerous murderer...’),  where the 
missing word is que. Other cases were not solved 
correctly,  as the fragment * a´ cida manzana (‘the 
acid apple’),  because the bigram la a´ cida is much 
less frequent  than lluvia a´ cida, ‘acid rain’, the 
wrong candidate proposed by the system. Results 
of this exercise are summarised in Table 5.

Typ
e 
of 
err
or
Tri
als
Co
rre
ct
% 
P
arti
cle
s
pro
no
un
s
7
7
4
3
57.
14
42.
86

Table 5: Results of the fill-in-the-blank  exercise



5   Conclusions and Future  Work

In the previous sections we have outlined  a first 
experiment  in the detection  of different types 
of grammar errors. In summary, the algorithm 
is able to detect difficult mistakes  such  as *in- 
formes conteniendo (instead of informes que con- 
ten´ıan ‘reports that contained’: a wrong use of 
the gerund) or *ma´ scaras antigases (instead of 
ma´ scaras antiga´ s ‘gas masks’,  an irregular plu- 
ral), which are errors that were not detected by 
MS Word.
  One of the difficulties we found is that, despite 
the fact that the corpus used is probably the most 
extensive corpus ever compiled,  there are bigrams 
that are not present in it.  This is not surprising, 
since one of the functions of linguistic compe- 
tence is the capacity to represent and make com- 
prehensible strings  of words which have  never 
been produced before. Another problem is that 
frequency is not always useful for detecting mis- 
takes,  because the norm can be very separated 
from real use. An example of this is that, in one of 
the error detection exercises, the system considers


that the participle fre´ıdos (‘fried’) is incorrect be- 
cause it is not in the corpus, but the participle is 
actually correct, even when the majority  of speak- 
ers think that only the irregular form (frito) is nor- 
mative.   The opposite is also true: some incor- 
rect structures are very frequently used and many 
speakers perceive  them  as correct,  such as ayer 
noche instead of ayer por la noche (‘last night’), 
or some very common Gallicisms  such as *medi- 
das a tomar instead of medidas por tomar ‘mea- 
sures to be taken’, or *asunto a discutir (‘matter 
to discuss’) which should be asunto para discutir.
  Several ideas have been put forward  to address 
these difficulties in future improvements to this


ingleses (the British), indios (indians) and so on. 
Such a line of study could produce interesting re- 
sults and greatly improve the rate of success of 
our grammar checker.

Acknowledgments

This research  has  been  made possible  thanks 
to funding from the Spanish  Ministry of Sci- 
ence   and Innovation, project:   “Agrupacio´ n 
sema´ntica   y   relaciones lexicolo´ gicas en  el 
diccionario”,   lead  researcher J.   DeCesaris 
(HUM2009-07588/FILO);  APLE:    “Procesos 
de actualizacio´ n del le´xico  del espan˜ ol a partir 
de la prensa”,  2010-2012,  lead researcher:  M.


research, such as the use of trigrams and longer


T.  Cabre´


(FFI2009-12188-C05-01/FILO)   and


n-grams instead of only bigrams for error detec- 
tion. POS-tagging and proper noun detection are 
also essential. Another possibility  is to comple- 
ment the corpus with different  Spanish corpora, 
including press articles  and other sources. We 
are also planning  to repeat the experiment with 
a new version of the n-gram  database this time 
not as  plain word forms but as  classes  of ob- 
jects such that the corpus will have greater power 
of generalisation.  Following another line of re- 
search that we have already  started (Nazar  and 
Renau, in preparation), we will produce clusters 
of words according to their distributional similar- 
ity, which will result in a sort of Spanish taxon- 
omy. This can be accomplished  because all the 
words that represent,  say, the category of vehi- 
cles are, in general, very similar as regards  their 
distribution.  Once we have organised  the lex- 
icon of the corpus into categories,  we will  re- 
place those words  by the name of the category 
they belong to, for instance, PERSON, NUMBER, 
VEHICLE, COUNTRY, ORGANISATION, BEVER- 
AGE, ANIMAL,  PLANT and so on. By doing this, 
the Google n-gram corpus will be useful to repre- 
sent a much more diverse variety of n-grams than 
those it actually contains. The implications of this 
idea go far beyond the particular field of grammar 
checking and include the study of collocations 
and of predicate-argument  structures in general. 
We could ask, for instance, which are the most 
typical agents of the Spanish verb disparar (to 
shoot). Searching for the trigram los * dispararon 
in the database, we can learn, for instance, that 
those agents can be soldados (soldiers), espan˜ oles 
(Spaniards), guardias (guards), polic´ıas (police- 
men), can˜ ones (cannons), militares (the military),


Fundacio´ n Comillas in relation with the project 
“Diccionario de aprendizaje  del espan˜ ol como 
lengua extranjera”.    The authors would like  to 
thank the anonymous reviewers for their helpful 
comments,  Cristo´ bal Lozano for providing the 
non-native  speaker corpus CEDEL2, Mark An- 
drews for proofreading, the team of the CIBER 
HPC Platform of  Universitat Pompeu Fabra 
(Silvina  Re and Milton Hoz) and the people that 
compiled and decided to share the Google Books 
N-gram corpus with the rest of the community 
(Michel et al., 2010).


References

Antti Arppe. 2000. Developing  a Grammar Checker 
for Swedish. Proceedings of the Twelfth Nordic 
Conference in Computational Linguistics. Trond- 
heim, Norway, pp. 5–77.
Eric Steven Atwell. 1987. How to Detect Grammati-
cal Errors in a Text without  Parsing it. Proceedings 
of the Third Conference of the European Associ- 
ation for Computational Linguistics, Copenhagen, 
Denmark, pp. 38–45.
Philip Bolt.	1992.	An Evaluation of Grammar-
Checking Programs as Self-help Learning Aids for 
Learners of English  as a Foreign  Language.  Com- 
puter Assisted Language Learning, 5(1):49–91.
Jill Burstein,  Martin Chodorow,  Claudia Leacock.
2004. Automated Essay Evaluation:  the Criterion
 Writing Service. AI Magazine, 25(3):27–36. 
William B. Dolan, Lucy Vanderwende,  Stephen D.
Richardson.  1993. Automatically Deriving Struc- 
tured Knowledge  Base from On-Line Dictionaries. 
Proceedings of the Pacific ACL. Vancouver, BC.
Shona Douglas, Robert Dale. 1992. Towards robust
PATR.  Proceedings of the 15th International  Con- 
ference on Computational Linguistics,  Nantes, pp.
468–474.


Michael Gamon, Carmen Lozano, Jessie Pinkham, 
Tom Reutter.  1997.  Practical  Experience with 
Grammar Sharing in Multilingual NLP.  Proceed- 
ings of the Workshop on Making NLP Work. ACL 
Conference, Madrid.
Andrew Golding. 1995. A Bayesian Hybrid Method
for Context Sensitive Spelling Correction. Proceed- 
ings of the Third Workshop on Very Large Corpora, 
pp. 39–53.
Andrew Golding, Dan Roth. 1996. Applying Win-
now to Context Sensitive Spelling Correction. Pro- 
ceedings of the International  Conference on Ma- 
chine Learning, pp. 182–190.
Na-Rae Han, Martin Chodorow,  Claudia Leacock.
2006. Detecting Errors in English Article Usage by 
non-Native Speakers. Natural Language Engineer- 
ing, 12(2), pp. 115–129.
George E. Heidorn. 2000. Intelligent writing assis-
tance. In Dale, R, Moisl H, Somers H, eds. Hand- 
book of Natural Language Processing:  Techniques 
and Applications for the Processing of Language as 
Text. New York: Marcel Dekker.
George E. Heidorn,  Karen Jensen, Lance A. Miller,
Roy J. Byrd, Martin Chodorow.  1982. The EPIS- 
TLE text-critiquing  system. IBM Systems Journal,
21, pp. 305–326.
Karen Jensen, George E. Heidorn, Stephen Richard- 
son, eds. 1993. Natural  Language Processing: The 
PNLP Approach. Kluwer Academic Publishers.
Jane Bondi Johannessen, Kristin Hagen, Pia Lane.
2002.  The Performance of a Grammar  Checker 
with Deviant Language Input. Proceedings of the
19th International Conference  on Computational
Linguistics. Taipei, Taiwan, pp. 1–8.
Kevin Knight, Ishwar Chandler. 1994. Automated 
Postediting of Documents. Proceedings of National 
Conference on Artificial Intelligence, Seattle, USA, 
pp. 779–784.
Gary F. Kohut, Kevin  J.  Gorman.   1995.   The
Effectiveness of Leading Grammar/Style Software 
Packages in Analyzing  Business Students’ Writing. 
Journal of Business and Technical Communication,
9:341–361.
Claudia Leacock, Martin Chodorow, Michael Gamon, 
Joel Tetreault.  2010. Automated Grammatical Er- 
ror Detection for Language Learners.  USA: Mor- 
gal and Claypool.
Cristo´ bal Lozano. 2009. CEDEL2: Corpus Escrito
del Espan˜ ol L2. In: Bretones Callejas, Carmen M. 
et al. (eds) Applied Linguistics Now: Understand- 
ing Language and Mind. Almer´ıa: Universidad de 
Almer´ıa. Almer´ıa, pp. 197–212.
Nina H. Macdonald. 1983. The UNIX Writer’s Work-
bench Software: Rationale and Design. Bell System
Technical Journal, 62, pp. 1891–1908.
Chistopher D. Manning,  Prabhakar Raghavan, Hin- 
rich Schu¨ tze.  2008. Introduction to Information 
Retrieval. Cambridge University  Press.


Jean-Baptiste Michel, Yuan Kui Shen, Aviva Presser 
Aiden, Adrian Veres, Matthew K. Gray, The Google 
Books Team, Joseph P. Pickett, Dale Hoiberg, Dan 
Clancy,  Peter Norvig, Jon Orwant,  Steven Pinker, 
Martin A. Nowak, Erez Lieberman Aiden.  2011. 
Quantitative Analysis of Culture Using Millions of 
Digitized Books. Science 331(6014), pp. 176–182. 
Estrella Montol´ıo, ed. 2000. Manual pra´ ctico  de es-
critura acade´mica. Barcelona: Ariel.
Joaquim More´,  Salvador Climent, Antoni Oliver.
2004. A Grammar and Style Checker Based on In- 
ternet Searches.  Proceedings of LREC 2004, Lis- 
bon, Portugal.
Rogelio Nazar. In press.  Algorithm qualifies for C1
courses in German exam without previous knowl- 
edge of the language:  an example of how corpus 
linguistics  can be a new paradigm in Artificial In- 
telligence. Proceedings of Corpus Linguistics Con- 
ference, Birmingham, 20-22 July 2011.
Rogelio Nazar, Irene Renau. In preparation.  A co-
ocurrence taxonomy from a general language  cor- 
pus. Proceedings of the 15th EURALEX Interna- 
tional Congress, Oslo, 7-11 August 2012.
Stephen Richardson,  Lisa Braden-Harder. 1988.
The Experience of Developing  a Large-Scale Natu- 
ral Language Text Processing System: CRITIQUE. 
Proceedings of the Second Conference on Applied 
Natural Language  Processing  (ANLC ’88). ACL, 
Stroudsburg, PA, USA, pp. 195–202.
David Schneider, Kathleen McCoy. 1998. Recogniz-
ing Syntactic Errors in the Writing of Second Lan- 
guage Learners. Proceedings of the 36th Annual 
Meeting of the ACL and 17th International Con- 
ference  on Computational Linguistics, Montreal, 
Canada, pp. 1198–1204.
Jonas Sjo¨ bergh. 2009.  The Internet  as a Norma-
tive Corpus: Grammar Checking with a Search En- 
gine. Techical Report, Dept. of Theoretical Com- 
puter Science, Kungliga Tekniska Ho¨ gskolan.
Ralph M.  Weischedel, John Black.      1980.
Responding-to  potentially unparseable  sentences.
American  Journal of Computational  Linguistics,
6:97–109.
Casey Whitelaw,  Ben Hutchinson, Grace Y. Chung, 
Gerard Ellis.  2009. Using the Web for Language 
Independent  Spell Checking and Autocorrection. 
Proceedings of the 2009 Conference on Empirical 
Methods in Natural Language Processing, Singa- 
pore, pp. 890–899.
David Yarowsky.  1994  Decision Lists for Lexi-
cal Ambiguity Resolution:  Application to Accent 
Restoration in Spanish and French. Proceedings of 
the ACL Conference, pp. 88–95.
Xing Yin, Jiangfeng Gao, William B. Dolan. 2008.
A Web-based English Proofing System for English 
as a Second  Language  Users. Proceedings of the
3rd International  Joint Conference on Natural Lan- 
guage Processing, Hyderabad, India, pp. 619–624.

