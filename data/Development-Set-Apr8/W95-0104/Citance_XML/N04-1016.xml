<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Previous work demonstrated that web counts can be used to approximate bigram frequencies, and thus should be useful for a wide variety of NLP tasks.</S>
		<S sid ="2" ssid = "2">So far, only two generation tasks (candidate selection for machine translation and confusion-set disambiguation) have been tested using web-scale data sets.</S>
		<S sid ="3" ssid = "3">The present paper investigates if these results generalize to tasks covering both syntax and semantics, both generation and analysis, and a larger range of n-grams.</S>
		<S sid ="4" ssid = "4">For the majority of tasks, we fi nd that simple, unsupervised models perform better when n-gram frequencies are obtained from the web rather than from a large corpus.</S>
		<S sid ="5" ssid = "5">However, in most cases, web-based models fail to outperform more sophisticated state-of-the- art models trained on small corpora.</S>
		<S sid ="6" ssid = "6">We argue that web-based models should therefore be used as a baseline for, rather than an alternative to, standard models.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="7" ssid = "7">Keller and Lapata (2003) investigated the validity of web counts for a range of predicate-argument bigrams (verb- object, adjective-noun, and noun-noun bigrams).</S>
			<S sid ="8" ssid = "8">They presented a simple method for retrieving bigram counts from the web by querying a search engine and demonstrated that web counts (a) correlate with frequencies obtained from a carefully edited, balanced corpus such as the 100M words British National Corpus (BNC), (b) correlate with frequencies recreated using smoothing methods in the case of unseen bigrams, (c) reliably predict human plausibility judgments, and (d) yield state-of-the-art performance on pseudo-disambiguation tasks.</S>
			<S sid ="9" ssid = "9">Keller and Lapata’s (2003) results suggest that web- based frequencies can be a viable alternative to bigram frequencies obtained from smaller corpora or recreated using smoothing.</S>
			<S sid ="10" ssid = "10">However, they do not demonstrate that realistic NLP tasks can benefi t from web counts.</S>
			<S sid ="11" ssid = "11">In order to show this, web counts would have to be applied to a diverse range of NLP tasks, both syntactic and seman Task n POS Ling Type MT candidate select.</S>
			<S sid ="12" ssid = "12">1,2 V, N Sem Generation Spelling correction 1,2,3 Any Syn/Sem Generation Adjective ordering 1,2 Adj Sem Generation Compound bracketing 1,2 N Syn Analysis Compound interpret.</S>
			<S sid ="13" ssid = "13">1,2,3 N, P Sem Analysis Countability detection 1,2 N, Det Sem Analysis Table 1: Overview of the tasks investigated in this paper (n: size of n-gram; POS: parts of speech; Ling: linguistic knowledge; Type: type of task) tic, involving analysis (e.g., disambiguation) and generation (e.g., selection among competing outputs).</S>
			<S sid ="14" ssid = "14">Also, it remains to be shown that the web-based approach scales up to larger n-grams (e.g., trigrams), and to combinations of different parts of speech (Keller and Lapata 2003 only tested bigrams involving nouns, verbs, and adjectives).</S>
			<S sid ="15" ssid = "15">Another important question is whether web-based methods, which are by defi nition unsupervised, can be competitive alternatives to supervised approaches used for most tasks in the literature.</S>
			<S sid ="16" ssid = "16">This paper aims to address these questions.</S>
			<S sid ="17" ssid = "17">We start by using web counts for two generation tasks for which the use of large data sets has shown promising results: (a) target language candidate selection for machine translation (Grefenstette, 1998) and (b) context sensitive spelling correction (Banko and Brill, 2001a,b).</S>
			<S sid ="18" ssid = "18">Then we investigate the generality of the web-based approach by applying it to a range of analysis and generations tasks, involving both syntactic and semantic knowledge: (c) ordering of prenominal adjectives, (d) compound noun bracketing, (e) compound noun interpretation, and (f) noun count- ability detection.</S>
			<S sid ="19" ssid = "19">Table 1 gives an overview of these tasks and their properties.</S>
			<S sid ="20" ssid = "20">In all cases, we propose a simple, unsupervised n-gram based model whose parameters are estimated using web counts.</S>
			<S sid ="21" ssid = "21">We compare this model both against a baseline (same model, but parameters estimated on the BNC) and against state-of-the-art models from the literature, which are either supervised (i.e., use annotated training data) or unsupervised but rely on taxonomies to recreate missing counts.</S>
	</SECTION>
	<SECTION title="Method. " number = "2">
			<S sid ="22" ssid = "1">Following Keller and Lapata (2003), web counts for n- grams were obtained using a simple heuristic based on queries to the search engine Altavista.1 In this approach, the web count for a given n-gram is simply the number of hits (pages) returned by the search engine for the queries generated for this n-gram.</S>
			<S sid ="23" ssid = "2">Three different types of queries were used for the NLP tasks in the present paper: Literal queries use the quoted n-gram directly as a search term for Altavista (e.g., the bigram history changes expands to the query &quot;history changes&quot;).</S>
			<S sid ="24" ssid = "3">Near queries use Altavista’s NEAR operator to expand the n-gram; a NEAR b means that a has to occur in the same ten word window as b; the window is treated as a bag of words (e.g., history changes expands to &quot;history&quot; NEAR &quot;changes&quot;).</S>
			<S sid ="25" ssid = "4">Inflected queries are performed by expanding an n-gram into all its morphological forms.</S>
			<S sid ="26" ssid = "5">These forms are then submitted as literal queries, and the resulting hits are summed up (e.g., history changes expands to &quot;history change&quot;, &quot;histories change&quot;, &quot;history changed&quot;, etc.).</S>
			<S sid ="27" ssid = "6">John Carroll’s suite of morphological tools (morpha, morphg, and ana) was used to generate inflected forms of verbs and nouns.2 In certain cases (detailed below), determiners were inserted before nouns in order to make it possible to recognize simple NPs.</S>
			<S sid ="28" ssid = "7">This insertion was limited to a/an, the, and the empty determiner (for bare plurals).</S>
			<S sid ="29" ssid = "8">All queries (other than the ones using the NEAR operator) were performed as exact matches (using quotation marks in Altavista).</S>
			<S sid ="30" ssid = "9">All search terms were submitted to the search engine in lower case.</S>
			<S sid ="31" ssid = "10">If a query consists of a single, highly frequent word (such as the ), Altavista will return an error message.</S>
			<S sid ="32" ssid = "11">In these cases, we set the web count to a large constant (108 ).</S>
			<S sid ="33" ssid = "12">This problem is limited to unigrams, which were used in some of the models detailed below.</S>
			<S sid ="34" ssid = "13">Sometimes the search engine fails to return a hit for a given n-gram (for any of its morphological variants).</S>
			<S sid ="35" ssid = "14">We smooth zero counts by setting them to .5.</S>
			<S sid ="36" ssid = "15">For all tasks, the web-based models are compared against identical models whose parameters were estimated from the BNC (Burnard, 1995).</S>
			<S sid ="37" ssid = "16">The BNC is a static 100M word corpus of British English, which is about 1000 times smaller than the web (Keller and La- pata, 2003).</S>
			<S sid ="38" ssid = "17">Comparing the performance of the same model on the web and on the BNC allows us to assess how much improvement can be expected simply by using a larger data set.</S>
			<S sid ="39" ssid = "18">The BNC counts were retrieved using the Gsearch corpus query tool (Corley et al., 2001); the morphological query expansion was the same as for web queries; the NEAR operator was simulated by assuming a window of fi ve words to the left and fi ve to the right.</S>
			<S sid ="40" ssid = "19">1 We did not use Google counts, as Google limits the number of queries to 1000 per day, which makes the process of retrieving a large number of web counts very time consuming.</S>
			<S sid ="41" ssid = "20">2 The tools can be downloaded from http://www.cogs.</S>
			<S sid ="42" ssid = "21">susx.ac.uk/lab/nlp/carroll/morph.html.</S>
			<S sid ="43" ssid = "22"># best model on development set ∗ / ∗ (not) sign.</S>
			<S sid ="44" ssid = "23">different from best BNC model on test set † / † (not) sign.</S>
			<S sid ="45" ssid = "24">different from baseline ‡ / ‡ (not) sign.</S>
			<S sid ="46" ssid = "25">different from best model in the literature Table 2: Meaning of diacritics indicating statistical signifi cance ( 2 tests) Gsearch was used to search solely for adjacent words; no POS information was incorporated in the queries, and no parsing was performed.</S>
			<S sid ="47" ssid = "26">For all of our tasks, we have to select either the best of several possible models or the best parameter setting for a single model.</S>
			<S sid ="48" ssid = "27">We therefore require a separate development set.</S>
			<S sid ="49" ssid = "28">This was achieved by using the gold standard data set from the literature for a given task and randomly dividing it into a development set and a test set (of equal size).</S>
			<S sid ="50" ssid = "29">We report the test set performance for all models for a given task, and indicate which model shows optimal performance on the development set (marked by a ‘#’ in all subsequent tables).</S>
			<S sid ="51" ssid = "30">We then compare the test set performance of this optimal model to the performance of the models reported in the literature.</S>
			<S sid ="52" ssid = "31">It is important to note that the fi gures taken from the literature were typically obtained on the whole gold standard data set, and hence may differ from the performance on our test set.</S>
			<S sid ="53" ssid = "32">We work on the assumption that such differences are negligible.</S>
			<S sid ="54" ssid = "33">We use 2 tests to determine whether the performance of the best web model on the test set is signifi cantly different from that of the best BNC model.</S>
			<S sid ="55" ssid = "34">We also determine whether both models differ signifi cantly from the baseline and from the best model in the literature.</S>
			<S sid ="56" ssid = "35">A set of diacritics is used to indicate signifi cance throughout this paper, see Table 2.</S>
	</SECTION>
	<SECTION title="Candidate Selection for Machine. " number = "3">
			<S sid ="57" ssid = "1">Translation Target word selection is a generation task that occurs in machine translation (MT).</S>
			<S sid ="58" ssid = "2">A word in a source language can often be translated into different words in the target language and the choice of the appropriate translation depends on a variety of semantic and pragmatic factors.</S>
			<S sid ="59" ssid = "3">The task is illustrated in (1) where there are fi ve translation alternatives for the German noun Geschichte listed in curly brackets, the fi rst being the correct one.</S>
			<S sid ="60" ssid = "4">(1) a. Die Geschichte a¨ndert sich, nicht jedoch die Geographie.</S>
			<S sid ="61" ssid = "5">b. {History, story, tale, saga, strip } changes but geography does not.</S>
			<S sid ="62" ssid = "6">Statistical approaches to target word selection rely on bilingual lexica to provide all possible translations of words in the source language.</S>
			<S sid ="63" ssid = "7">Once the set of translation candidates is generated, statistical information gathered from target language corpora is used to select the most appropriate alternative (Dagan and Itai, 1994).</S>
			<S sid ="64" ssid = "8">The task is somewhat simplifi ed by Grefenstette (1998) and Prescher et al.</S>
			<S sid ="65" ssid = "9">(2000) who do not produce a translation of the entire sentence.</S>
			<S sid ="66" ssid = "10">Instead, they focus on specifi c syntactic relations.</S>
			<S sid ="67" ssid = "11">Grefenstette translates compounds from German and Spanish into English, and uses BNC frequencies as a fi lter for candidate translations.</S>
			<S sid ="68" ssid = "12">He observes that this approach suffers from an acute data sparseness problem through web searches, thus achieving a translation accuracy of 86–87%.</S>
			<S sid ="69" ssid = "13">Prescher et al.</S>
			<S sid ="70" ssid = "14">(2000) concentrate on verbs and their objects.</S>
			<S sid ="71" ssid = "15">Assuming that the target language translation of the verb is known, they select from the candidate translations the noun that is semantically most compatible with the verb.</S>
			<S sid ="72" ssid = "16">The semantic fi t between a verb and its argument is modeled using a class-based lexicon that is derived from unlabeled data using the expectation maximization algorithm (verb-argument model).</S>
			<S sid ="73" ssid = "17">Prescher et al. also propose a refi ned version of this approach that only models the fi t between a verb and its object (verb-object model), disregarding other arguments of the verb.</S>
			<S sid ="74" ssid = "18">The two models are trained on the BNC and evaluated against two corpora of 1,340 and 814 bilingual sentence pairs, with an average of 8.63 and 2.83 translations for the object noun, respectively.</S>
			<S sid ="75" ssid = "19">Table 4 lists Prescher et al.’s results for the two corpora and for both models together with a random baseline (select a target noun at random) and a frequency baseline (select the most frequent target noun).</S>
			<S sid ="76" ssid = "20">Grefenstette’s (1998) evaluation was restricted to compounds that are listed in a dictionary.</S>
			<S sid ="77" ssid = "21">These compounds are presumably well-established and fairly frequent, which makes it easy to obtain reliable web frequencies.</S>
			<S sid ="78" ssid = "22">We wanted to test if the web-based approach extends from lexicalized compounds to productive syntactic units for which dictionary entries do not exist.</S>
			<S sid ="79" ssid = "23">We therefore performed our evaluation using Prescher et al.’s (2000) test set of verb-object pairs.</S>
			<S sid ="80" ssid = "24">Web counts were retrieved for all possible verb-object translations; the most likely one was selected using either co-occurrence frequency ( f (v, n)) or conditional probability ( f (v, n)/ f (n)).</S>
			<S sid ="81" ssid = "25">The web counts were gathered using inflected queries involving the verb, a determiner, and the object (see Section 2).</S>
			<S sid ="82" ssid = "26">Table 3 compares the web-based models against the BNC models.</S>
			<S sid ="83" ssid = "27">For both the high ambiguity and the low ambiguity data set, we fi nd that the performance of the best Altavista model is not signifi cantly different from that of the best BNC model.</S>
			<S sid ="84" ssid = "28">Table 4 compares our simple, unsupervised methods with the two sophisticated class-based models discussed above.</S>
			<S sid ="85" ssid = "29">The results show that there is no signifi cant difference in performance between the best model reported in the literature and the best Altavista or the best BNC model.</S>
			<S sid ="86" ssid = "30">However, both models signifi cantly outperform the baseline.</S>
			<S sid ="87" ssid = "31">This holds for both the high and low ambiguity data sets.</S>
			<S sid ="88" ssid = "32">Altavista BNC high low high low Model ambig ambig ambig ambig f (v, n) 45.74 68.73#/ ∗ 45.89# 70.06# f (v, n)/ f (n) 45.16#/ ∗ 64.96 46.18 66.07 Table 3: Performance of Altavista counts and BNC counts 2000) high low Model ambig ambig Random baseline 14.20 45.90 Frequency baseline 31.90 45.50 Prescher et al.</S>
			<S sid ="89" ssid = "33">(2000): verb-argument 43.30 61.50 Best Altavista 45.16†/ ‡ 68.73†/ ‡ Best BNC 45.89†/ ‡ 70.06†/ ‡ Prescher et al.</S>
			<S sid ="90" ssid = "34">(2000): verb-object 49.40 68.20 Table 4: Performance comparison with the literature for candidate selection for MT</S>
	</SECTION>
	<SECTION title="Context-sensitive Spelling Correction. " number = "4">
			<S sid ="91" ssid = "1">Context-sensitive spelling correction is the task of correcting spelling errors that result in valid words.</S>
			<S sid ="92" ssid = "2">Such a spelling error is illustrated in (4) where principal was typed when principle was intended.</S>
			<S sid ="93" ssid = "3">(2) Introduction of the dialogue principal proved strikingly effective.</S>
			<S sid ="94" ssid = "4">The task can be viewed as generation task, as it consists of choosing between alternative surface realizations of a word.</S>
			<S sid ="95" ssid = "5">This choice is typically modeled by confusion sets such as {principal, principle} or {then, than} under the assumption that each word in the set could be mistakenly typed when another word in the set was intended.</S>
			<S sid ="96" ssid = "6">The task is to infer which word in a confusion set is the correct one in a given context.</S>
			<S sid ="97" ssid = "7">This choice can be either syn tactic (as for {then, than}) or semantic (as for {principal, principle}).A number of machine learning methods have been pro posed for context-sensitive spelling correction.</S>
			<S sid ="98" ssid = "8">These include a variety of Bayesian classifi ers (Golding, 1995; Golding and Schabes, 1996), decision lists (Golding, 1995) transformation-based learning (Mangu and Brill, 1997), Latent Semantic Analysis (LSA) (Jones and Martin, 1997), multiplicative weight update algorithms (Golding and Roth, 1999), and augmented mixture models (Cucerzan and Yarowsky, 2002).</S>
			<S sid ="99" ssid = "9">Despite their differences, most approaches use two types of features: context words and collocations.</S>
			<S sid ="100" ssid = "10">Context word features record the presence of a word within a fi xed window around the target word (bag of words); collocational features capture the syntactic environment of the target word and are usually represented by a small number of words and/or part- of-speech tags to the left or right of the target word.</S>
			<S sid ="101" ssid = "11">The results obtained by a variety of classifi cation methods are given in Table 6.</S>
			<S sid ="102" ssid = "12">All methods use either the full set or a subset of 18 confusion sets originally gathered by Golding (1995).</S>
			<S sid ="103" ssid = "13">Most methods are trained and tested on Model Alta BNC Model Alta BNC f (t ) 72.98 70.00 f (w1 , t , w2 )/ f (t ) 87.77 76.33 f (w1 , t ) 84.40 83.02 f (w1 , w2 , t )/ f (t ) 86.27 74.47 f (t , w1 ) 84.89 82.74 f (t , w2 , w2 )/ f (t ) 84.94 74.23 f (w1 , t , w2 ) 89.24#*77.13 f (w1 , t , w2 )/ f (w1 , t ) 80.70 73.69 f (t , w1 , w2 ) 84.68 75.08 f (w1 , w2 , t )/ f (w2 , t ) 72.11 69.28 f (w1 , t )/ f (t ) 82.81 77.84 f (t , w1 , w2 )/ f (t , w1 ) 75.65 72.57 f (t , w1 )/ f (t ) 77.49 80.71# Table 5: Performance of Altavista counts and BNC counts for context sensitive spelling correction (data from Cucerzan and Yarowsky 2002) Model Accuracy Baseline BNC 70.00 Baseline Altavista 72.98 Best BNC 80.71†‡ Golding (1995) 81.40 Jones and Martin (1997) 84.26 Best Altavista 89.24†‡ Golding and Schabes (1996) 89.82 Mangu and Brill (1997) 92.79 Cucerzan and Yarowsky (2002) 92.20 Golding and Roth (1999) 94.23 Table 6: Performance comparison with the literature for context sensitive spelling correction the Brown corpus, using 80% for training and 20% for testing.3 We devised a simple, unsupervised method for performing spelling correction using web counts.</S>
			<S sid ="104" ssid = "14">The method takes into account collocational features, i.e., words that are adjacent to the target word.</S>
			<S sid ="105" ssid = "15">For each word in the confusion set, we used the web to estimate how frequently it co-occurs with a word or a pair of words immediately to its left or right.</S>
			<S sid ="106" ssid = "16">Disambiguation is then performed by selecting the word in the confusion set with the highest co-occurrence frequency or probability.</S>
			<S sid ="107" ssid = "17">The web counts were retrieved using literal queries (see Section 2).</S>
			<S sid ="108" ssid = "18">Ties are resolved by comparing the unigram frequencies of the words in the confusion set and defaulting to the word with the highest one.</S>
			<S sid ="109" ssid = "19">Table 5 shows the types of collocations we considered and their corresponding accuracy.</S>
			<S sid ="110" ssid = "20">The baseline ( f (t )) in Table 5 was obtained by always choosing the most frequent unigram in the confusion set.</S>
			<S sid ="111" ssid = "21">We used the same test set (2056 tokens from the Brown corpus) and confusion sets as Golding and Schabes (1996), Mangu and Brill (1997), and Cucerzan and Yarowsky (2002).</S>
			<S sid ="112" ssid = "22">Table 5 shows that the best result (89.24%) for the web- based approach is obtained with a context of one word to the left and one word to the right of the target word ( f (w1 , t , w2 )).</S>
			<S sid ="113" ssid = "23">The BNC-based models perform consistently worse than the web-based models with the exception of f (t , w1 )/t ; the best Altavista model performs sig nifi cantly better than the best BNC model.</S>
			<S sid ="114" ssid = "24">Table 6 shows 3 An exception is Golding (1995), who uses the entire Brown corpus for training (1M words) and 3/4 of the Wall Street Journal corpus (Marcus et al., 1993) for testing.</S>
			<S sid ="115" ssid = "25">that both the best Altavista model and the best BNC model outperform their respective baselines.</S>
			<S sid ="116" ssid = "26">A comparison with the literature shows that the best Altavista model outperforms Golding (1995), Jones and Martin (1997) highest accuracy on the task is achieved by the class of multiplicative weight-update algorithms such as Winnow (Golding and Roth, 1999).</S>
			<S sid ="117" ssid = "27">Both the best BNC model and the best Altavista model perform signifi cantly worse than this model.</S>
			<S sid ="118" ssid = "28">Note that Golding and Roth (1999) use algorithms that can handle large numbers of features and are robust to noise.</S>
			<S sid ="119" ssid = "29">Our method uses a very small feature set, it relies only on co-occurrence frequencies and does not have access to POS information (the latter has been shown to have an improvement on confusion sets whose words belong to different parts of speech).</S>
			<S sid ="120" ssid = "30">An advantage of our method is that it can be used for a large number of confusion sets without relying on the availability of training data.</S>
	</SECTION>
	<SECTION title="Ordering of Prenominal Adjectives. " number = "5">
			<S sid ="121" ssid = "1">The ordering of prenominal modifi ers is important for natural language generation systems where the text must be both fluent and grammatical.</S>
			<S sid ="122" ssid = "2">For example, the sequence big fat Greek wedding is perfectly acceptable, whereas fat Greek big wedding sounds odd.</S>
			<S sid ="123" ssid = "3">The ordering of prenominal adjectives has sparked a great deal of theoretical debate (see Shaw and Hatzivassiloglou 1999 for an overview) and efforts have concentrated on defi ning rules based on semantic criteria that account for different orders (e.g., age ≺ color, value ≺ dimension).</S>
			<S sid ="124" ssid = "4">Data intensive approaches to the ordering problem rely on corpora for gathering evidence for the likelihood of different orders.</S>
			<S sid ="125" ssid = "5">They rest on the hypothesis that the relative order of premodifi ers is fi xed, and independent of context and the noun being modifi ed.</S>
			<S sid ="126" ssid = "6">The simplest strategy is what Shaw and Hatzivassiloglou (1999) call di rect evidence.</S>
			<S sid ="127" ssid = "7">Given an adjective pair {a, b}, they count how many times (a, b) and (b, a) appear in the corpus and choose the pair with the highest frequency.</S>
			<S sid ="128" ssid = "8">Unfortunately the direct evidence method performs poorly when a given order is unseen in the training data.</S>
			<S sid ="129" ssid = "9">To compensate for this, Shaw and Hatzivassiloglou (1999) propose to compute the transitive closure of theordering relation: if a ≺ c and c ≺ b, then a ≺ b. Mal ouf (2000) further proposes a back-off bigram model of adjective pairs for choosing among alternative orders (P((a, b)|{a, b}) vs. P((b, a)|{a, b})).</S>
			<S sid ="130" ssid = "10">He also proposes positional probabilities as a means of estimating how likely it is for a given adjective a to appear fi rst in a sequence by looking at each pair in the training data that contains the adjective a and recording its position.</S>
			<S sid ="131" ssid = "11">Finally, he uses memory-based learning as a means to encode morphological and semantic similarities among different adjective orders.</S>
			<S sid ="132" ssid = "12">Each adjective pair ab is encoded as a vector of 16 features (the last eight characters of a and the last eight characters of b) and a class ((a, b) or Model Altavista BNC f (a1 , a2 ) : f (a2 , a1 ) 89.6#∗/ ‡ 80.4#‡ f (a1 , a2 )/ f (a2 ) : f (a2 , a1 )/ f (a1 ) 83.2 77.0 f (a1 , a2 )/ f (a1 ) : f (a2 , a1 )/ f (a2 ) 80.2 80.6 Malouf (2000): memory-based – 91.0 Table 7: Performance of Altavista counts and BNC counts for adjective ordering (data from Malouf 2000) (b, a)).</S>
			<S sid ="133" ssid = "13">Malouf (2000) extracted 263,838 individual pairs of adjectives from the BNC which he randomly partitioned into test (10%) and training data (90%) and evaluated all the above methods for ordering prenominal adjectives.</S>
			<S sid ="134" ssid = "14">His results showed that a memory-based classifi er that uses morphological information as well as positional probabilities as features outperforms all other methods (see Table 7).</S>
			<S sid ="135" ssid = "15">For the ordering task we restricted ourselves to the direct evidence strategy which simply chooses the adjective order with the highest frequency or probability (see Table 7).</S>
			<S sid ="136" ssid = "16">Web counts were obtained by submitting literal queries to Altavista (see Section 2).</S>
			<S sid ="137" ssid = "17">We used the same 263,838 adjective pairs that Malouf extracted from the BNC.</S>
			<S sid ="138" ssid = "18">These were randomly partitioned into a training (90%) and test corpus (10%).</S>
			<S sid ="139" ssid = "19">The test corpus contained 26,271 adjective pairs.</S>
			<S sid ="140" ssid = "20">Given that submitting 26,271 queries to Altavista would be fairly time- consuming, a random sample of 1000 sequences was obtained from the test corpus and the web frequencies of these pairs were retrieved.</S>
			<S sid ="141" ssid = "21">The best Altavista model signifi cantly outperformed the best BNC model, as indicated in Table 7.</S>
			<S sid ="142" ssid = "22">We also found that there was no signifi cant difference between the best Altavista model and the best model reported by Malouf, a supervised method using Model Alta BNC Baseline 63.93 63.93 f (n1 , n2 ) : f (n2 , n3 ) 77.86 66.39 f (n1 , n2 ) : f (n1 , n3 ) 78.68#∗ 65.57 f (n1 , n2 )/ f (n1 ) : f (n2 , n3 )/ f (n2 ) 68.85 65.57 f (n1 , n2 )/ f (n2 ) : f (n2 , n3 )/ f (n3 ) 70.49 63.11 f (n1 , n2 )/ f (n2 ) : f (n1 , n3 )/ f (n3 ) 80.32 66.39 f (n1 , n2 ) : f (n2 , n3 ) (NEAR) 68.03 63.11 f (n1 , n2 ) : f (n1 , n3 ) (NEAR) 71.31 67.21 f (n1 , n2 )/ f (n1 ) : f (n2 , n3 )/ f (n2 ) (NEAR) 61.47 62.29 f (n1 , n2 )/ f (n2 ) : f (n2 , n3 )/ f (n3 ) (NEAR) 65.57 57.37 f (n1 , n2 )/ f (n2 ) : f (n1 , n3 )/ f (n3 ) (NEAR) 75.40 68.03# Table 8: Performance of Altavista counts and BNC counts for compound bracketing (data from Lauer 1995) Model Accuracy Baseline 63.93 Best BNC 68.03/ †‡ Lauer (1995): adjacency 68.90 Lauer (1995): dependency 77.50 Best Altavista 78.68†/ ‡ Lauer (1995): tuned 80.70 Upper bound 81.50 Table 9: Performance comparison with the literature for compound bracketing 1993).</S>
			<S sid ="143" ssid = "23">Lauer (1995) proposes an unsupervised method for estimating the frequencies of the competing bracketings based on a taxonomy or a thesaurus.</S>
			<S sid ="144" ssid = "24">He uses a probability ratio to compare the probability of the left- branching analysis to that of the right-branching (see (4) for the dependency model and (5) for the adjacency model).</S>
			<S sid ="145" ssid = "25">P(t1 → t2 )P(t2 → t3 ) ti ∈cats(wi ) positional probability estimates from the BNC and morphological variants.</S>
			<S sid ="146" ssid = "26">(4) Rdep = ti ∈cats(wi ) P(t 1 → t3 )P(t 2 → t3 )</S>
	</SECTION>
	<SECTION title="Bracketing of Compound Nouns. " number = "6">
			<S sid ="147" ssid = "1">t cats(w ) P(t1 → t2 )The fi rst analysis task we consider is the syntactic disam (5) Radj = i ∈ i P(t2 → t3 ) biguation of compound nouns, which has received a fair amount of attention in the NLP literature (Pustejovsky et al., 1993; Resnik, 1993; Lauer, 1995).</S>
			<S sid ="148" ssid = "2">The task can be summarized as follows: given a three word compound n1 n3 n3 , determine the correct binary bracketing of the word sequence (see (3) for an example).</S>
			<S sid ="149" ssid = "3">(3) a. [[backup compiler] disk] b. [backup [compiler disk]] Previous approaches typically compare different bracketings and choose the most likely one.</S>
			<S sid ="150" ssid = "4">The adjacency model compares [n1 n2 ] against [n2 n3 ] and adopts a right branching analysis if [n2 n3 ] is more likely than [n1 n2 ].</S>
			<S sid ="151" ssid = "5">The dependency model compares [n1 n2 ] against [n1 n3 ] and adopts a right branching analysis if [n1 n3 ] is more likely than [n1 n2 ].</S>
			<S sid ="152" ssid = "6">The simplest model of compound noun disambiguation compares the frequencies of the two competing analyses and opts for the most frequent one (Pustejovsky et al., ti ∈cats(wi ) Here t1 , t2 and t3 are conceptual categories in the taxonomy or thesaurus, and the nouns w1 . . .</S>
			<S sid ="153" ssid = "7">wi are members of these categories.</S>
			<S sid ="154" ssid = "8">The estimation of probabilities over concepts (rather than words) reduces the number of model parameters and effectively decreases the amount of train ing data required.</S>
			<S sid ="155" ssid = "9">The probability P(t1 → t2 ) denotes the modifi cation of a category t2 by a category t1 .Lauer (1995) tested both the adjacency and de pendency models on 244 compounds extracted from Grolier’s encyclopedia, a corpus of 8 million words.</S>
			<S sid ="156" ssid = "10">Frequencies for the two models were obtained from the same corpus and from Roget’s thesaurus (version 1911) by counting pairs of nouns that are either strictly adjacent or co-occur within a window of a fi xed size (e.g., two, three, fi fty, or hundred words).</S>
			<S sid ="157" ssid = "11">The majority of the bracketings in our test set were left-branching, yielding a baseline of 63.93% (see Table 9).</S>
			<S sid ="158" ssid = "12">Lauer’s best results (77.50%) were obtained with the dependency model and a training scheme which takes strictly adjacent nouns into account.</S>
			<S sid ="159" ssid = "13">Performance increased further by 3.2% when POS tags were taken into account.</S>
			<S sid ="160" ssid = "14">The results for this tuned model are also given in Table 9.</S>
			<S sid ="161" ssid = "15">Finally, Lauer conducted an experiment with human judges to assess the upper bound for the bracketing task.</S>
			<S sid ="162" ssid = "16">An average accuracy of 81.50% was obtained.</S>
			<S sid ="163" ssid = "17">We replicated Lauer’s (1995) results for compound noun bracketing using the same test set.</S>
			<S sid ="164" ssid = "18">We compared the performance of the adjacency and dependency models (see (4) and (5)), but instead of relying on a corpus and a thesaurus, we estimated the relevant probabilities using web counts.</S>
			<S sid ="165" ssid = "19">The latter were obtained using inflected queries (see Section 2) and Altavista’s NEAR operator.</S>
			<S sid ="166" ssid = "20">Ties were resolved by defaulting to the most frequent analysis (i.e., left-branching).</S>
			<S sid ="167" ssid = "21">To gauge the performance of the web-based models we compared them against their BNC-based alternatives; the performance of the best Altavista model was signifi cantly higher than that of the best BNC model (see Table 8).</S>
			<S sid ="168" ssid = "22">A comparison with the literature (see Table 9) shows that the best BNC model fails to signifi cantly outperform the baseline, and it performs signifi cantly worse than the best model in the liter Table 10: Performance of Altavista counts and BNC counts for compound interpretation (data from Lauer 1995) Model Accuracy Best BNC 27.85/ †‡ Lauer (1995): concept-based 28.00 Baseline 33.00 Lauer (1995): word-based 40.00 Best Altavista 55.71†‡ Table 11: Performance comparison with the literature for compound interpretation hundreds of millions of training instances would be necessary.</S>
			<S sid ="169" ssid = "23">As an alternative to (6), he proposes the model in (7) which combines the probability of the modifi er given a certain preposition with the probability of the head given the same preposition, and assumes that these two probabilities are independent.</S>
			<S sid ="170" ssid = "24">ature (Lauer’s tuned model).</S>
			<S sid ="171" ssid = "25">The best Altavista model, on the other hand, is not signifi cantly different from Lauer’s tuned model and signifi cantly outperforms the baseline.</S>
			<S sid ="172" ssid = "26">Hence we achieve the same performance as Lauer with (7) p∗ = arg max p t1 ∈ cats(n1 ) t2 ∈ cats(n2 ) P(t1 |p)P(t2 |p) out recourse to a predefi ned taxonomy or a thesaurus.</S>
	</SECTION>
	<SECTION title="Interpretation of Compound Nouns. " number = "7">
			<S sid ="173" ssid = "1">The second analysis task we consider is the semantic interpretation of compound nouns.</S>
			<S sid ="174" ssid = "2">Most previous approaches to this problem have focused on the interpretation of two word compounds whose nouns are related via a basic set of semantic relations (e.g., CAUSE relates onion tears, FOR relates pet spray ).</S>
			<S sid ="175" ssid = "3">The majority of proposals are symbolic and therefore limited to a specifi c domain due to the large effort involved in hand-coding semantic information (see Lauer 1995 for an extensive overview).</S>
			<S sid ="176" ssid = "4">Lauer (1995) is the fi rst to propose and evaluate an unsupervised probabilistic model of compound noun interpretation for domain independent text.</S>
			<S sid ="177" ssid = "5">By recasting the interpretation problem in terms of paraphrasing, Lauer assumes that the semantic relations of compound heads and modifi ers can be expressed via prepositions that (in contrast to abstract semantic relations) can be found in a corpus.</S>
			<S sid ="178" ssid = "6">For example, in order to interpret war story, one needs to fi nd in a corpus related paraphrases: story about the war, story of the war, story in the war, etc. Lauer uses eight prepositions for the paraphrasing task (of, for, in, at, on, from, with, about ).</S>
			<S sid ="179" ssid = "7">A simple model of compound noun paraphrasing is shown in (6): Here, t1 and t2 represent concepts in Roget’s thesaurus.Lauer (1995) also experimented with a lexicalized ver sion of (7) where probabilities are calculated on the basis of word (rather than concept) frequencies which Lauer obtained from Grolier’s encyclopedia heuristically via pattern matching.</S>
			<S sid ="180" ssid = "8">Lauer (1995) tested the model in (7) on 282 compounds that he selected randomly from Grolier’s encyclopedia and annotated with their paraphrasing prepositions.</S>
			<S sid ="181" ssid = "9">The preposition of accounted for 33% of the paraphrases in this data set (see Baseline in Table 11).</S>
			<S sid ="182" ssid = "10">The concept- based model (see (7)) achieved an accuracy of 28% on this test set, whereas its lexicalized version reached an accuracy of 40% (see Table 11).</S>
			<S sid ="183" ssid = "11">We attempted the interpretation task with the lexicalized version of the bigram model (see (7)), but also tried the more data intensive trigram model (see (6)), again in its lexicalized form.</S>
			<S sid ="184" ssid = "12">Furthermore, we experimented with several conditional and unconditional variants of (7) and (6).</S>
			<S sid ="185" ssid = "13">Co-occurrence frequencies were estimated from the web using inflected queries (see Section 2).</S>
			<S sid ="186" ssid = "14">Determiners were inserted before nouns resulting in queries of the type story/stories about and about the/a/0 war/wars for the compound war story.</S>
			<S sid ="187" ssid = "15">As shown in Table 10, the best performance was obtained using the web-based trigram model ( f (n1 , p, n2 )); it signifi cantly outperformed the best BNC model.</S>
			<S sid ="188" ssid = "16">The (6) p∗ = arg max P( p n1 , n2 ) p co mp aris on wit h the lite rat ure in Tab le 11 sho we d that the bes t Alt avi sta mo del sig nifi can tly out per for me d bot h Lauer (1995) points out that the above model contains one parameter for every triple (p, n1 , n2 ), and as a result the baseline and the best model in the literature (Lauer’s word-based model).</S>
			<S sid ="189" ssid = "17">The BNC model, on the other hand, Altavista BNC Model Count Uncount Count Uncount f (n) 87.01 90.13 87.32# 90.39# f (det, n) 88.38#/ ∗ 91.22#/ ∗ 51.01 50.23 f (det, n)/ f (n) 83.19 85.38 50.95 50.23 Backoff 87.01 89.80 – – Table 12: Performance of Altavista counts and BNC counts for noun countability detection (data from Baldwin and Bond 2003) achieved a performance that is not signifi cantly different from the baseline, and signifi cantly worse than Lauer’s best model.</S>
	</SECTION>
	<SECTION title="Noun Countability Detection. " number = "8">
			<S sid ="190" ssid = "1">The next analysis task that we consider is the problem of determining the countability of nouns.</S>
			<S sid ="191" ssid = "2">Countability is the semantic property that determines whether a noun can occur in singular and plural forms, and affects the range of permissible modifi ers.</S>
			<S sid ="192" ssid = "3">In English, nouns are typically either countable (e.g., one dog, two dogs ) or uncountable (e.g., some peace, *one peace, *two peaces ).</S>
			<S sid ="193" ssid = "4">Baldwin and Bond (2003) propose a method for automatically learning the countability of English nouns from the BNC.</S>
			<S sid ="194" ssid = "5">They obtain information about noun countability by merging lexical entries from COMLEX (Grishman et al., 1994) and the ALTJ/E Japanese-to-English semantic transfer dictionary (Ikehara et al., 1991).</S>
			<S sid ="195" ssid = "6">Words are classifi ed into four classes: countable, uncountable, bipartite (e.g., trousers ), and plural only (e.g., goods ).</S>
			<S sid ="196" ssid = "7">A memory-based classifi er is used to learn the four-way distinction on the basis of several linguistically motivated features such as: number of the head noun, number of the modifi er, subject-verb agreement, plural determiners.</S>
			<S sid ="197" ssid = "8">We devised unsupervised models for the countability learning task and evaluated their performance on Baldwin and Bond’s (2003) test data.</S>
			<S sid ="198" ssid = "9">We concentrated solely on countable and uncountable nouns, as they account for the vast majority of the data.</S>
			<S sid ="199" ssid = "10">Four models were tested: (a) compare the frequency of the singular and plural forms of the noun; (b) compare the frequency of determiner-noun pairs that are characteristic of countable or uncountable nouns; the determiners used were many for countable and much for uncountable ones; (c) same as model (b), but the det-noun frequencies are normalized by the frequency of the noun; (d) backoff: try to make a decision using det-noun frequencies; if these are too sparse, back off to singular/plural frequencies.</S>
			<S sid ="200" ssid = "11">Unigram and bigram frequencies were estimated from the web using literal queries; for models (a)–(c) a threshold parameter was optimized on the development set (this parameter determines the ratio of singular/plural frequencies or det-noun frequencies above which a noun was considered as countable).</S>
			<S sid ="201" ssid = "12">For model (b), an additional backoff parameter was used, specifying the minimum frequency that triggers backoff.</S>
			<S sid ="202" ssid = "13">The models and their performance on the test set are Model Count Uncount Baseline 74.60 78.30 Best BNC 87.32†‡ 90.39†‡ Best Altavista 88.38†‡ 91.22†‡ Baldwin and Bond (2003) 93.90 95.20 Table 13: Performance comparison with the literature for noun countability detection listed in Table 12.</S>
			<S sid ="203" ssid = "14">The best Altavista model is the conditional det-noun model ( f (det, n)/ f (n)), which achieves 88.38% on countable and 91.22% on uncountable nouns.</S>
			<S sid ="204" ssid = "15">On the BNC, the simple unigram model performs best.</S>
			<S sid ="205" ssid = "16">Its performance is not statistically different from that of the best Altavista model.</S>
			<S sid ="206" ssid = "17">Note that for the BNC models, data sparseness means the det-noun models perform poorly, which is why the backoff model was not attempted here.</S>
			<S sid ="207" ssid = "18">Table 13 shows that both the Altavista model and BNC model signifi cantly outperform the baseline (relative frequency of the majority class on the gold-standard data).</S>
			<S sid ="208" ssid = "19">The comparison with the literature shows that both the Altavista and the BNC model perform signifi cantly worse than the best model proposed by Baldwin and Bond (2003); this is a supervised model that uses many more features than just singular/plural frequency and det-noun frequency.</S>
	</SECTION>
	<SECTION title="Conclusions. " number = "9">
			<S sid ="209" ssid = "1">We showed that simple, unsupervised models using web counts can be devised for a variety of NLP tasks.</S>
			<S sid ="210" ssid = "2">The tasks were selected so that they cover both syntax and semantics, both generation and analysis, and a wider range of n-grams than have been previously used.</S>
			<S sid ="211" ssid = "3">For all but two tasks (candidate selection for MT and noun countability detection) we found that simple, unsupervised models perform signifi cantly better when n- gram frequencies are obtained from the web rather than from a standard large corpus.</S>
			<S sid ="212" ssid = "4">This result is consistent with Keller and Lapata’s (2003) fi ndings that the web yields better counts than the BNC.</S>
			<S sid ="213" ssid = "5">The reason for this seems to be that the web is much larger than the BNC (about 1000 times); the size seems to compensate for the fact that simple heuristics were used to obtain web counts, and for the noise inherent in web data.</S>
			<S sid ="214" ssid = "6">Our results were less encouraging when it comes to comparisons with state-of-the-art models.</S>
			<S sid ="215" ssid = "7">We found that in all but one case, web-based models fail to signifi cantly outperform the state of the art.</S>
			<S sid ="216" ssid = "8">The exception was compound noun interpretation, for which the Altavista model was signifi cantly better than the Lauer’s (1995) model.</S>
			<S sid ="217" ssid = "9">For three tasks (candidate selection for MT, adjective ordering, and compound noun bracketing), we found that the performance of the web-based models was not significantly different from the performance of the best models reported in the literature.</S>
			<S sid ="218" ssid = "10">Note that for all the tasks we investigated, the best performance in the literature was obtained by supervised models that have access not only to simple bigram or tri gram frequencies, but also to linguistic information such as part-of-speech tags, semantic restrictions, or context (or a thesaurus, in the case of Lauer’s models).</S>
			<S sid ="219" ssid = "11">When unsupervised web-based models are compared against supervised methods that employ a wide variety of features, we observe that having access to linguistic information makes up for the lack of vast amounts of data.</S>
			<S sid ="220" ssid = "12">Our results therefore indicate that large data sets such as those obtained from the web are not the panacea that they are claimed to be (at least implicitly) by authors such as Grefenstette (1998) and Keller and Lapata (2003).</S>
			<S sid ="221" ssid = "13">Rather, in our opinion, web-based models should be used as a new baseline for NLP tasks.</S>
			<S sid ="222" ssid = "14">The web baseline indicates how much can be achieved with a simple, unsupervised model based on n-grams with access to a huge data set.</S>
			<S sid ="223" ssid = "15">This baseline is more realistic than baselines obtained from standard corpora; it is generally harder to beat, as our comparisons with the BNC baseline throughout this paper have shown.</S>
			<S sid ="224" ssid = "16">Note that for certain tasks, the performance of a web baseline model might actually be suffi cient, so that the effort of constructing a sophisticated supervised model and annotating the necessary training data can be avoided.</S>
			<S sid ="225" ssid = "17">Another possibility that needs further investigation is the combination of web-based models with supervised methods.</S>
			<S sid ="226" ssid = "18">This can be done with ensemble learning methods or simply by using web-based frequencies (or probabilities) as features (in addition to linguistically motivated features) to train supervised classifi ers.</S>
	</SECTION>
	<SECTION title="Acknowledgments">
			<S sid ="227" ssid = "19">We are grateful to Tim Baldwin, Silviu Cucerzan, Mark Lauer, Rob Malouf, Detelef Prescher, and Adwait Ratnaparkhi for making their data sets available.</S>
	</SECTION>
</PAPER>
