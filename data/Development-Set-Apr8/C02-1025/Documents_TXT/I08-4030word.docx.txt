


CRF-based Hybrid Model for Word Segmentation,  NER and even
POS Tagging

Zhiting Xu, Xian Qian, Yuejie Zhang,  Yaqian Zhou
Department of Computer Science & Engineering,
Shanghai Key Laboratory of Intelligent Information Processing, 
Fudan University, Shanghai 200433, P. R. China
{zhiting, qianxian, yjzhang, zhouyaqian}@fudan.edu.cn






Abstract

This paper presents systems submitted to 
the close track of Fourth SIGHAN Bakeoff. 
We built up three systems based on Condi- 
tional Random Field for Chinese Word 
Segmentation, Named Entity Recognition 
and Part-Of-Speech Tagging respectively. 
Our  systems  employed  basic  features  as 
well as a large number of linguistic features. 
For segmentation task, we adjusted the BIO 
tags according to confidence of each char- 
acter. Our final system achieve a F-score of
94.18 at CTB, 92.86 at NCC, 94.59 at SXU
on   Segmentation,   85.26   at   MSRA   on
Named  Entity  Recognition,  and  90.65  at
PKU on Part-Of-Speech Tagging.

1     Introduction

Fourth SIGHAN Bakeoff includes three tasks, that 
is, Word Segmentation, Named Entity Recognition 
(NER) and Part-Of-Speech (POS) Tagging. In the 
POS Tagging task, the testing corpora are pre- 
segmented.  Word  Segmentation,  NER  and  POS
Tagging could be viewed as classification prob-
  

We attended the close track of CTB, NCC, SXU 
on  Segmentation,  MSRA  on  NER  and  PKU  on 
POS Tagging. In the close track, we cannot use 
any external resource, and thus we extracted sev- 
eral word lists from training corpora to form multi- 
ple features beside basic features. Then we trained 
CRF models based on these feature sets. In CRF 
models, a margin of each character can be gotten, 
and the margin could be considered as the confi- 
dence of that character. For the Segmentation task, 
we performed the Maximum Probability Segmen- 
tation first, through which each character is as- 
signed a BIO tag (B represents the Beginning of a 
word, I represents In a word and O represents Out 
of a word). If the confidence of a character is lower 
than the threshold, the tag of that character will be 
adjusted to the tag assigned by the Maximum 
Probability Segmentation (R. Zhang et al., 2006).

2     Conditional Random  Fields

Conditional Random Fields (CRFs) are a class of 
undirected graphical models with exponent distri- 
bution (Lafferty et al., 2001). A common used spe- 
cial case of CRFs is linear chain, which has a dis- 
tribution of:
T



lems.  In  a  Segmentation  task,  each  character


P  ( yr | xr) =


1  exp(∑ ∑ λ



f k ( y



t −1


, y , xr, t ))



(1)


should be classified into three classes, B, I, O, in-


Z xr



t =1    k
r


dicating whether this character is the Beginning of
a word, In a word or Out of a word. For NER, each


where


f k ( yt −1 , yt x, t ) is a function which is usu-


character is assigned a tag indicating what kind of


ally an indicator function; λk


is the learned weight


Named Entity (NE) this character is (Beginning of


of feature


f k ; and Z xr is the normalization factor.


a Person Name (PN), In a PN, Beginning of a Lo-
cation Name (LN), In a LN, Beginning of an Or- 
ganization Name (ON), In an ON or not-a-NE). In 
POS tagging task defined by Fourth SIGHAN Ba- 
keoff, we only need to give a POS tag for each 
given word in a context.


The feature function actually consists of two kinds 
of features, that is, the feature of single state and 
the feature of transferring between states. Features 
will be discussed in section 3.







  Several methods (e.g. GIS, IIS, L-BFGS) could 
be  used  to  estimate λk  ,  and  L-BFGS  has  been 
showed to converge faster than GIS and IIS. To
build up our system, we used Pocket CRF1.

3     Feature Representation

We used three feature sets for three tasks respec- 
tively, and will describe them respectively.

3.1     Word Segmentation

We mainly adopted features from (H. T. Ng et al.,
2004, Y. Shi et al., 2007), as following:
a)    Cn(n=-2, -1, 0, 1, 2) 
b)    CnCn+1(n=-2,-1,0,1) 
c)    C-1C1
d)    CnCn+1Cn+2 (n=-1, 0, 1)
e)    Pu(C0)
f)    T(C-2)T(C-1)T(C0)T(C1)T(C2)
g)    LBegin(C0), Lend(C0)
h)    Single(C0)
where C0 represents the current character and Cn 
represents the nst character from the current charac- 
ter.  Pu(C0)  indicates  whether  current  word  is  a
punctuation. this feature template helps to indicate
the end of a sentence. T(C) represents the type of 
character C. There are four types we used: (1) Chi-
nese Number (“一/one”, “二/two”, “十/ten”); (2)
Chinese Dates (“日/day”, “月/month”, “年/year”);
(3) English letters; and (4) other characters. The (f)
feature template is used to recognize the Chinese 
dates for the construction of Chinese dates may 
cause the sparseness problem. LBegin(C0) represents 
the maximum length of the word beginning with 
the character C0, and Lend(C0) presents the maxi- 
mum length of the word ending with the character 
C0. The (g) feature template is used to decide the 
boundary of a word. Single(C0) shows whether cur- 
rent character can form a word solely.

3.2     Named Entity Recognition

Most features described in (Y. Wu et al., 2005) are 
used in our systems. Specifically, the following is 
the feature templates we used:
a)    Surname(C0): Whether current character is in 
a Surname List, which includes all first char- 
acters of PNs in the training corpora.



1
http://sourceforge.net/project/showfiles.php?group_id=201943


b)    PersonName(C0C1C2, C0C1): Whether C0C1C2, 
C0C1 is in the Person Name List, which con- 
tains all PNs in the training corpora.
c)   PersonTitle(C-2C-1): Whether C-2C-1 is in the 
Person Title List, which is extracted from the 
previous two characters of each PN in the 
training corpora.
d)	LocationName(C0C1,C0C1C2,C0C1C2C3): 
Whether C0C1,C0C1C2,C0C1C2C3 is in the Lo- 
cation Name List, which includes all LNs in 
the training corpora.
e)    LocationSuffix(C0): Whether current character 
is in the Location Suffix List, which is con- 
structed using the last character of each LN in 
the training corpora.
f)    OrgSuffix(C0): Whether current character is in 
the Organization Suffix List, which contains 
the last-two-character of each ON in the train- 
ing corpora.

3.3     Part-Of-Speech Tagging

We employed part of feature templates described 
in (H. T. Ng et al., 2004, Y. Shi et al., 2007). Since 
we are in the close track, we cannot use morpho- 
logical features from external resources such as 
HowNet, and we used features that are available 
just from the training corpora.
a)    Wn, (n=-2,-1,0,1,2)
b)    WnWn+1, (n=-2,-1,0,1)
c)    W-1W1
d)    Wn-1WnWn+1 (n=-1, 1)
e)    Cn(W0) (n=0,1,2,3)
f)    Length(W0)
where Cn represents the nth character of the current 
word, and Length(W0) indicates the length of the
current word.

4     Reliability Evaluation

In the task of Word Segmentation, the label of each 
character is adjusted according to their reliability. 
For each sentence, we perform Maximum Prob- 
ability Segmentation first, through which we can 
get a BIO tagging for each character in the sen- 
tence.
  After that, the features are extracted according 
to the feature templates, and the weight of each
feature has already been estimated in the step of
training. Then marginal probability for each char- 
acter can be computed as follows:







p( y | xr) =


1
Z ( x)



exp(λi


f ( xr, y))



(2)



The value of


p( y | x ) 
becomes the 
original re-


liability value of BIO label y for the current char- 
acter under the current contexts. If the probability
of  y with the largest probability is lower than 0.75,
which is decided according to the experiment re- 
sults, the tag given by Maximum Probability Seg-
mentation will be used instead of tag given by CRF.
The motivation of this method is to use the Maxi- 
mum Probability method to enhance the F-measure
of  In-Vocabulary  (IV)  Words.  According  to  the
results reported in (R. Zhang et al., 2006), CRF 
performs relatively better on Out-of-Vocabulary 
(OOV) words while Maximum Probability per- 
forms well on IV words, so a model combining the 
advantages of these two methods is appealing. One 
simplest way to combine them is the method we 
described. Besides, there are some complex meth- 
ods, such as estimation using Support Vector Ma- 
chine (SVM) for CRF, CRF combining boosting 
and combining Margin Infused Relaxed Algorithm 
(MIRA) with CRF, that might perform better. 
However, we did not have enough time to imple- 
ment these methods, and we will compare them 
detailedly in the future work.

5     Experiments

5.1     Results on Fourth SIGHAN Bakeoff

We participated in the close track on Word Seg- 
mentation on CTB, NCC and SXU corpora, NER 
on MSRA corpora and POS Tagging on PKU cor- 
pora.
  For Word Segmentation and NER, our memory 
was enough to use all features. However, for POS
tagging, we did not have enough memory to use all
features, and we set a frequency cutoff of 10; that 
is, we could only estimate variables for those fea- 
tures that occurred more than ten times.
  Our results of Segmentation are listed in the Ta- 
bel 1, the results of NER are listed in the Tabel 2,
and the results of POS Tagging are listed in the
Tabel 3.





Tabel 1. Results of Word Segmentation




Tabel 2. Results of NER


Total-A
IV-R
OOV-R
MT-R
PKU
0.9065
0.9259
0.5836
0.8903
Tabel 3. Results of POS Tagging

5.2     Errors Analysis

Observing our results of Word Segmentation and 
POS Tagging, we found that the recall of OOV is 
relatively low, this may be improved through in- 
troducing features aiming to enhance the perform- 
ance of OOV.
On NER task, we noticed that precision of PN
recognition is relative low, and we found that our 
system may classify some ONs as PNs, such as “吉 
尼斯(Guinness)/ORG” and “世界记录(World Re-
cord)/)”. Besides, the bound of PN is sometimes 
confusing and may cause problems. For example,
“胡绳/PER 曾/ 有/ 题词” may be segmented as
“胡绳曾/PER 有/ 题词”. Further, some words be-
ginning with Chinese surname, such as “丁丑盛
夏”, may be classified as PN.
  For List may not be the real suffix. For example, 
“玉峰山麓” should be a LN, but it is very likely 
that “玉峰山” is recognized as a LN for its suffix 
“山”.  Another problem involves the characters in
the Location Name list may not a LN all the time. 
In the context “华裔/ 作家/”, for example, “华”
means Chinese rather than China.
  For ONs, the correlative dictionary also exists. 
Consider sequence “人大代表”, which should be a 
single word, “人大” is in the Organization Name
List and thus it is recognized as an ON in our sys- 
tem. Another involves the subsequence of a word.
For example, the sequence “湖北钟祥市工业局
长”, which should be a person title, but “湖北钟祥
市工业局” is an ON. Besides, our recall of ON is
low for the length of an ON could be very long.

6     Conclusions and Future Works

We built up our systems based on the CRF model 
and employed multiple linguistics features based 
on the knowledge extracted from training corpora.







We found that these features could greatly improve 
the performance of all tasks. Besides, we adjusted 
the tag of segmentation result according to the reli- 
ability of each character, which also helped to en- 
hance the performance of segmentation.
  As many other NLP applications, feature plays a 
very important role in sequential labeling tasks. In
our POS tagging task, we could only use features 
with high frequency, but some low-frequency fea-
tures may also play a vital role in the task; good 
non-redundant features could greatly improve clas- 
sification  performance  while  save  memory  re-
quirement of classifiers. In our further research, we 
will focus on feature selection on CRFs.

Acknowledgement

This research was sponsored by National Natural
Science Foundation of China (No. 60773124, No.
60503070).

References

O. Bender, F. J. Och, and H. Ney. 2003. Maximum En- 
tropy Models for Named Entity Recognition. Pro- 
ceeding of CoNLL-2003.

A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra.
1996.  A  Maximum  Entropy  Approach  to  Natural
Language   Processing.  Computational   Linguistics,
22(1).

H. L. Chieu, H. T. Ng. 2002. Named Entity Recognition: 
A Maximum Entropy Approach Using Global Infor- 
mation. International Conference on Computational 
Linguistics (COLING).

J. N. Darroch and D. Ratcliff. 1972. Generalized Itera- 
tive Scaling for Log-Linear Models. The Annals of 
Mathematical Statistics, 43(5).

J. Lafferty, A McCallum, and F. Pereira..2001. Condi- 
tional Random Fields: Probabilistic Models for Seg- 
menting and Labeling Sequence Data. In Proceed- 
ings of the 18th International Conf. on Machine 
Learning (ICML).

R. Li, J. Wang, X. Chen, X. Tao, and Y. Hu. 2004. Us- 
ing Maximum Entropy Model for Chinese Text 
Categorization. Computer Research and Develop- 
ment, 41(4).

H. T. Ng and J. K. Low. 2004. Chinese Part-Of-Speech 
Tagging: One-at-a-Time or All-at-Once? Word-Base 
or Character-Based? Proceedings of Conference on 
Empirical Methods in Natural Language Processing 
(EMNLP).


A. Ratnaparkhi. 1997. A Simple Introduction to Maxi- 
mum Entropy Models for Natural Language Process- 
ing. Institute for  Research in Cognitive Science Re- 
port, 97(8).

F. Sha and F.Pereira. 2003. Shallow parsing with condi- 
tional random fields. In Proceedings of HLT-NAACL.

Y. Shi and M. Wang. 2007. A Dual-Layer CRFs Based 
Joint Decoding Method for Cascaded Segmentation 
and Labeling Tasks. In International Joint Confer- 
ences on Artificial Intelligence (IJCAI).

C. A. Sutton, K. Rohanimanesh, A. McCallum. 2004.
Dynamic conditional random fields: factorized prob- 
abilistic models for labeling and segmenting se- 
quence data. In International Conference on Machine 
Learning (ICML).

M. Volk, and S. Clematide. 2001. Learn - Filter - Apply
-- Forget Mixed Approaches to Named Entity Rec- 
ognition. Proceeding of the 6th International Work- 
shop on Applications of Natural Language for Infor- 
mation Systems.

Y.  Wu,  J.  Zhao,  B.  Xu  and  H.  Yu.  2005.  Chinese 
Named Entity Recognition Based on Multiple Fea- 
tures. Proceedings of Human Language Technology 
Conference and Conference on Empirical Methods in 
Natural Language Processing (HLT/EMNLP).

H. Zhang, Q. Liu, H. Zhang, and X. Cheng. 2002. Au- 
tomatic Recognition of Chinese Unknown Words 
Based on Roles Tagging. Proceeding of the 19th  In- 
ternational Conference on Computational Linguistics.

R.  Zhang, G.  Kikui and  E.  Sumita. 2006. Subword- 
based tagging by conditional random fields for Chi- 
neseword segmentation. Companion volume to the- 
proceedings of the North American chapter of the 
Association for Computational Linguistics (NAACL).

Y. Zhou, Y. Guo, X. Huang, and L. Wu. 2003. Chinese 
and English BaseNP Recognition Based on a Maxi- 
mum Entropy Model. Journal of Computer Research 
and Development, 40(3).




