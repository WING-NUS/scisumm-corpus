Kernel-based Reranking for Named-Entity Extraction



Truc-Vien T. Nguyen  and Alessandro Moschitti  and Giuseppe Riccardi 
Department of Information  Engineering and Computer Science University  of 
Trento nguyenthi,moschitti,riccardi@disi.unitn.it







Abstract

We present novel kernels based on struc- 
tured and unstructured features for rerank- 
ing the N-best hypotheses of conditional 
random fields (CRFs) applied to entity ex- 
traction. The former features are gener- 
ated by a polynomial  kernel encoding en- 
tity features whereas tree kernels are used 
to model dependencies amongst  tagged 
candidate examples.  The experiments on 
two standard corpora  in two languages, 
i.e. the Italian EVALITA 2009 and the En- 
glish CoNLL 2003 datasets, show a large 
improvement on CRFs in F-measure, i.e. 
from 80.34% to 84.33% and from 84.86% 
to 88.16%, respectively.  Our analysis re- 
veals that both kernels provide a compara- 
ble improvement over the CRFs baseline. 
Additionally, their combination improves 
CRFs much more than the sum of the indi- 
vidual contributions,  suggesting an inter- 
esting kernel synergy.

1   Introduction
Reranking is a promising  computational  frame- 
work, which has drawn  special attention  in the 
Natural Language Processing (NLP) community. 
Basically, this method first employs a probabilis- 
tic model to generate a list of top-n candidates and 
then reranks this n-best list with additional fea- 
tures. One appeal of this approach is its flexibility 
of incorporating arbitrary features into a model. 
These features help in discriminating good from 
bad hypotheses and consequently  their automatic 
learning. Various algorithms have been applied 
for reranking in NLP applications (Huang, 2008;


Shen et al., 2004; Collins, 2002b; Collins and 
Koo, 2000), including parsing, name tagging and 
machine translation. This work has exploited  the 
disciminative  property as one of the key criterion 
of the reranking algorithm.

  Reranking  appears extremely  interesting  if cou- 
pled with kernel methods (Dinarelli et al., 2009; 
Moschitti, 2004; Collins and Duffy, 2001), as the 
latter allow for extracting from the ranking hy- 
potheses a huge  amount  of features along with 
their dependencies.  Indeed, while feature-based 
learning algorithms involve only the dot-product 
between  feature  vectors,  kernel methods  allow 
for a higher  generalization  by replacing the dot- 
product with a function  between pairs of linguis- 
tic objects. Such functions are a kind of similarity 
measure satisfying  certain properties.  An exam- 
ple is the tree kernel (Collins and Duffy, 2001), 
where the objects are syntactic  trees that encode 
grammatical  derivations  and the kernel function 
computes the number of common subtrees. Simi- 
larly, sequence kernels (Lodhi et al., 2002) count 
the number of common  subsequences shared by 
two input strings.

  Named-entities (NEs) are essential  for defin- 
ing the semantics of a document. NEs are ob- 
jects that can be referred by names (Chinchor  and 
Robinson,  1998),  such as people,  organizations, 
and locations. The research on NER has been 
promoted by the Message Understanding  Con- 
ferences (MUCs, 1987-1998),  the shared task of 
the Conference  on Natural Language Learning 
(CoNLL, 2002-2003), and the Automatic Content 
Extraction program (ACE, 2002-2005). In the lit- 
erature, there exist  various  learning  approaches 
to extract named-entities from text. A NER sys-







901
Coling  2010: Poster Volume, pages 901–909, 
Beijing,  August 2010


tem often builds some generative/discriminative 
model, then, either uses only one classifier (Car- 
reras et al., 2002) or combines many classifiers us- 
ing some heuristics (Florian et al., 2003).
  To the best of our knowledge,  reranking  has 
not been applied  to NER except for the rerank- 
ing algorithms defined in (Collins, 2002b; Collins,
2002a), which only targeted the entity detection 
(and not entity classification) task. Besides, since 
kernel methods offer a natural way to exploit lin- 
guistic properties, applying kernels for NE rerank- 
ing is worthwhile.
  In this paper, we describe how kernel methods 
can be applied for reranking, i.e.  detection and 
classification of named-entities, in standard cor- 
pora for Italian and English. The key aspect of 
our reranking approach is how structured and flat 
features can be employed in discriminating candi- 
date tagged sequences. For this purpose, we apply 
tree kernels to a tree structure encoding NE tags of 
a sentence and combined  them with a polynomial 
kernel, which efficiently exploits global features.
  Our main contribution is to show that (a) tree 
kernels can be used to define general features (not 
merely syntactic) and (b) using appropriate al- 
gorithms  and features, reranking  can be very ef- 
fective for named-entity recognition. Our study 
demonstrates that the composite kernel is very 
effective for reranking named-entity sequences. 
Without the need of producing and heuristically 
combining learning models like previous work on 
NER, the composite kernel not only captures most 
of the flat features but also efficiently exploits 
structured features. More interestingly, this kernel 
yields significant  improvement when applied to 
two corpora of two different languages. The eval- 
uation in the Italian corpus shows that our method 
outperforms the best reported methods whereas on 
the English data it reaches the state-of-the-art.

2   Background

2.1   The data

Different languages  exhibit  different linguistic 
phenomena and challenges.   A robust NER sys- 
tem is expected  to be well-adapted to multiple 
domains and languages.   Therefore,  we experi- 
mented with two datasets:  the EVALITA  2009


Italian corpus and the well-known CoNLL 2003
English shared task corpus.
  The EVALITA  2009 Italian dataset  is based 
on  I-CAB,   the  Italian  Content Annotation 
Bank (Magnini et al., 2006), annotated with four 
entity types: Person (PER), Organization (ORG), 
Geo-Political Entity (GPE) and Location (LOC). 
The training data, taken from the local newspa- 
per “L’Adige”, consists of 525 news stories which 
belong to five categories: News Stories, Cultural 
News, Economic  News, Sports News and Local 
News. Test data, on the other hand, consist of 
completely  new data, taken from the same news- 
paper and consists of 180 news stories.
  The CoNLL 2003 English dataset  is created 
within the shared  task of CoNLL-2003 (Sang 
and Meulder, 2003). It is a collection  of news 
wire articles from the Reuters Corpus, annotated 
with four entity types: Person (PER),  Location 
(LOC), Organization (ORG) and Miscellaneous 
name (MISC). The training  and the development 
datasets are news feeds from August 1996, while 
the test set contains  news feeds from December
1996. Accordingly,  the named entities in the test 
dataset are considerably  different  from those that 
appear in the training or the development set.

Italian
GPE
LOC
ORG
PER

Train
2813
24.65%
362
3.17%
3658
32.06%
4577
40.11%

Test
1143
23.02%
156
3.14%
1289
25.96%
2378
47.89%


English
LOC
MISC
ORG
PER

Train
7140
30.38%
3438
14.63%
6321
26.90%
6600
28.09%

Dev
1837
30.92%
922
15.52%
1341
22.57%
1842
31.00%

Test
1668
29.53%
702
12.43%
1661
29.41%
1617
28.63%

Table 1: Statistics on the Italian EVALITA 2009 
and English CoNLL 2003 corpora.


2.2   The baseline algorithm

We selected Conditional Random Fields (Lafferty 
et al., 2001)  as the baseline model. Conditional


random fields (CRFs)  are a probabilistic  frame- 
work for labeling  and segmenting sequence data. 
They present several advantages over other purely 
generative  models such as Hidden Markov models 
(HMMs) by relaxing the independence assump- 
tions required by HMMs.  Besides, HMMs and 
other discriminative  Markov  models are prone to 
the label bias problem, which is effectively solved 
by CRFs.
  The named-entity recognition  (NER) task is 
framed  as assigning  label sequences to a set of 
observation sequences. We follow the IOB nota- 
tion where the NE tags have the format B-TYPE, 
I-TYPE or O, which mean that the word is a be- 
ginning, a continuation of an entity, or not part of 
an entity at all. For example, consider the sentence 
with their corresponding NE tags, each word is la-
  

The gazetteer  lists are  built with names  im- 
ported from different sources. For English, the 
geographic  features are imported  from NIMA’s 
GEOnet Names Server (GNS)2,  The Alexandria 
Digital Library (ADL) gazetteer3.  The company 
data is included with all the publicly traded com- 
panies listed in Google directory4,  the European 
business directory5. For Italian, the generic proper 
nouns are extracted from Wikipedia  and various 
Italian sites.

2.3   Support Vector Machines (SVMs)
Support Vector Machines refer to a supervised 
machine learning  technique based on the latest re- 
sults of the statistical learning theory. Given a 
vector space and a set of training points, i.e. posi- 
tive and negative examples, SVMs find a separat-


beled with a tag indicating  its appropriate named-


ing hyperplane H ( x)  =


ω   × x + b  = 0 where


entity, resulting in annotated text, such as:
  Il/O  presidente/O  della/O Fifa/B-ORG Sepp/B-PER 
Blatter/I-PER  affermando/O  che/O il/O torneo/O era/O 
stato/O ottimo/O (FIFA president Sepp Blatter says that the 
tournament was excellent)
For our experiments, we used CRF++ 1 to build
our recognizer, which is a model trained discrim- 
inatively with the unigram  and bigram features. 
These are extracted  from a window at k words 
centered in the target word w (i.e. the one we want 
to classify with the B, O, I tags). More in detail 
such features are:
• The word itself, its prefixes,  suffixes, and
part-of-speech
• Orthographic/Word  features.    These  are 
binary and mutually  exclusive features that 
test whether a word contains all upper-cased, 
initial  letter upper-cased,  all lower-cased,
roman-number, dots,  hyphens, acronym, 
lonely initial, punctuation mark, single-char, 
and functional-word.
• Gazetteer  features. 	Class (geographical,


ω  ∈ Rn  and b ∈ R are learned by applying the
Structural Risk Minimization principle (Vapnik,
1998). SVMs are a binary classifier, but they can 
be easily extended to multi-class classifier, e.g. by 
means of the one-vs-all method (Rifkin and Pog- 
gio, 2002).
  One strong point of SVMs is the possibility to 
apply kernel  methods to implicitly map data in 
a new space where the examples are more easily 
separable as described in the next section.

2.4   Kernel methods
Kernel methods (Scho¨ lkopf and Smola, 2001) are 
an attractive alternative to feature-based methods 
since the applied learning algorithm only needs 
to compute  a product  between  a pair of objects 
(by means of kernel functions), avoiding the ex- 
plicit feature representation.   A kernel function 
is a scalar product in a possibly unknown feature 
space. More precisely, The object o is mapped in
 x with a feature function  φ : O → 'Rn, where O
is the set of the objects.
  The kernel trick allows  us to rewrite the deci- 
sion hyperplane as:


first name, surname, organization prefix, lo-
cation prefix) of words in the window.


H ( x) =     
i=1..l


yiαi xi  · x + b =


• Left Predictions.  The predicted tags on the 
left of the word in the current classification.

1 http://crfpp.sourceforge.net



2 http://www.nima.mil/gns/html
3 http://www.alexandria.ucsb.edu
4 http://directory.google.com/Top/Business
5 http://www.europages.net




i=1..l


yiαixi 
· x + b 
=




i=1..l


yiαiφ
(oi) · 
φ(o) 
+ b,



where yi   is equal to 1 for positive and -1 for 
negative  examples,  αi    ∈  'R with αi    ≥ 0, oi
∀i  ∈ {1, .., l} are the training  instances and the 
product K (oi, o) =  φ(oi) · φ(o)) is the kernel
function  associated with the mapping φ.
  Kernel engineering can be carried out by com- 
bining basic kernels with additive or multiplica- 
tive operators or by designing specific data objects 
(vectors,  sequences and tree structures) for the tar- 
get tasks.
  Regarding NLP applications,  kernel methods 
have attracted much interest due to the ability of 
implicitly exploring  huge amounts of structural 
features. The parse tree kernel (Collins and Duffy,
2001) and string kernel (Lodhi et al., 2002)  are 
examples of the well-known  convolution kernels 
used in various NLP tasks.

2.5   Tree Kernels

Tree kernels represent trees in terms of their sub- 
structures (called tree fragments). Such fragments 
form  a feature space which, in turn, is mapped into 
a vector space. Tree kernels measure the similar- 
ity between pair of trees by counting the number 
of fragments in common.  There are three impor- 
tant characterizations of fragment type: the Sub- 
Trees (ST), the SubSet Trees (SST) and the Partial 
Trees (PT). For sake of space, we do not report the 
mathematical description of them, which is avail- 
able in (Vishwanathan and Smola, 2002), (Collins 
and Duffy, 2001) and (Moschitti, 2006), respec- 
tively. In contrast, we report some descriptions in 
terms of feature space that may be useful to un- 
derstand the new engineered kernels.
  In principle,  a SubTree (ST) is defined by tak- 
ing any node along with its descendants. A SubSet 
Tree (SST) is a more general structure which does 
not necessarily include all the descendants.  The 
distinction is that an SST must be generated by ap- 
plying the same grammatical  rule set which gen- 
erated the original  tree, as pointed out in (Collins 
and Duffy, 2001). A Partial Tree (PT) is a more 
general form of sub-structures obtained by relax- 
ing constraints over the SSTs. Figure 1 shows the 
overall fragment set of the ST, SST and PT kernels 
for the syntactic  parse tree of the sentence frag-
















     Figure 1: Three kinds of tree kernels. 
ment: gives a talk .
  In the next section, we will define new struc- 
tures for tagged sequences of NEs which along 
with the application of the PT kernel produce in- 
novative tagging kernels for reranking.

3   Reranking Method

3.1   Reranking Strategy

As a baseline we trained the CRFs model to gen- 
erate 10-best candidates per sentence, along with 
their probabilities.   Each candidate was then rep- 
resented by a semantic tree together with a feature 
vector.  We consider our reranking  task as a binary 
classification  problem  where examples are pairs 
of hypotheses < Hi, Hj  >.
Given a  sentence  “South  African Breweries Ltd
bought stakes in the Lech and Tychy brewers” and three 
of its candidate tagged sequences:
H1   B-ORG I-ORG I-ORG I-ORG O O O O B-ORG O 
B-ORG O (the correct sequence)
H2   B-MISC I-MISC B-ORG I-ORG O O O O B-ORG 
I-ORG I-ORG O
H3  B-ORG I-ORG I-ORG I-ORG O O O O B-ORG O 
B-LOC O
  where B-ORG, I-ORG, B-LOC, O are the gen- 
erated NE tags according to IOB notation  as de- 
scribed in Section 3.2.
  With the above data (an original sentence to- 
gether with a list of candidate tagged sequences), 
the following pairs of hypotheses will be gener-


ated < H1, H2  >, < H1, H3  >,< H2, H1  > and
< H3, H1 >, where the first two pairs are positive 
and the latter pairs are negative instances. Then a 
binary  classifier  based on SVMs  and kernel meth- 
ods can be trained to discriminate  between the 
best hypothesis, i.e. < H1  > and the others.  At 
testing time the hypothesis receiving the highest 
score is selected (Collins  and Duffy, 2001).

3.2   Representation of Tagged Sequences in
Semantic Trees

We now consider the representation that exploits 
the most discriminative aspects of candidate struc- 
tures.  As in the case  of NER, an input can- 
didate is a  sequence  of  word/tag pairs x   =
{w1/t1...wn/tn} where wi  is the i th word and
ti is the i th NE tag for that word. The first repre-
sentation we consider is the tree structure.  See fig- 
ure 2 as an example of candidate tagged sequence 
and its semantic tree.
  With the sentence “South African Breweries Ltd 
bought stakes in the Lech and Tychy brewers” and three 
of its candidate tagged sequences in the previous 
section, the training algorithm considers to con- 
struct a tree for each sequence, with the named- 
entity tags as  pre-terminals   and the words as 
leaves. See figure 2 for an example of the seman- 
tic tree for the first tagged sequence.
  With this tree representation, for a word wi, the 
target NE tag would  be set at parent and the fea- 
tures for this word are at child nodes. This allows 
us to best exploit the inner product between com- 
peting candidates.   Indeed,  in the kernel space, 
the inner product counts the number of common 
subtrees thus sequences with similar NE tags are 
likely to have higher score. For example, the sim- 
ilarity between H1 and H3 will be higher than the 
similarity of the previous hypotheses with H2; this 
is reasonable since these two also have higher F1.
It is worth noting that another useful modifica-
tion is the flexibility of incorporate diverse, ar- 
bitrary features into this tree structure by adding 
children to the parent node that contains entity tag. 
These characteristics can be exploited  efficiently 
with the PT kernel, which relaxes constraints of 
production rules. The inner product can implicitly 
include  these features and deal better with sparse 
data.


3.3	Global features

Mixed n-grams features
  In previous works, some global  features have 
been used (Collins, 2002b; Collins, 2002a) but the 
employed algorithm just exploited arbitrary infor- 
mation regarding word types and linguistic pat- 
terns. In contrast, we define and study diverse 
features by also considering n-grams patterns pre- 
ceding, and following the target entity.

Complementary  context
  In supervised learning, NER systems often suf- 
fer from low recall, which is caused by lack of 
both resource and context. For example,  a word 
like “Arkansas” may not appear in the training set 
and in the test set, there may not be enough con- 
text to infer its NE tag.  In such  cases, neither 
global features (Chieu and Ng, 2002) nor aggre- 
gated contexts (Chieu and Ng, 2003) can help.
  To overcome this deficiency, we employed the 
following unsupervised procedure: first, the base- 
line NER is applied to the target un-annotated cor- 
pus. Second, we associate each word of the corpus 
with the most frequent NE category assigned in 
the previous step. Finally,  the above tags are used 
as features  during the training of the improved 
NER and also for building the feature represen- 
tation for a new classification  instance.
  This way, for any unknown word w of the test 
set, we can rely on the most probable NE category 
as feature.  The advantage is that we derived it by 
using the average over many possible contexts of 
w, which are in the different instances of the un- 
nanotated corpus.
  The unlabeled corpus for Italian was collected 
from La Repubblica 6 and it contains over 20 mil- 
lions words. Whereas the unlabeled corpus for 
English was collected mainly from The New York 
Times 7 and BBC news stories 8 with more than
35 millions words.

Head word
  As the head word of an entity plays an impor- 
tant role in information extraction (Bunescu and 
Mooney,  2005a; Surdeanu et al., 2003), it is in-

6 http://www.repubblica.it/
7 http://www.nytimes.com/
8 http://news.bbc.co.uk/



 

Figure 2: Semantic structure of the first sequence




cluded in the global set together  with its ortho- 
graphic feature. We now describe some primitives 
for our global feature framework.


1. wi for i = 1 . . . n is the i th word

2. ti is the NE tag of wi

3. gi is the gazetteer feature of the word wi


3.4   Reranking with Composite Kernel
In this section we describe our novel tagging ker- 
nels based on diverse global features  as well as 
semantic trees for reranking  candidate tagged se- 
quences.  As mentioned in the previous section, 
we can engineer kernels by combining  tree and 
entity kernels.  Thus we focus on the problem to 
define structure embedding the desired relational 
information  among tagged sequences.
The Partial Tree Kernel


4. fi is the most frequent NE tag seen in a large
corpus of wi


Let F  = f1, f2, . . . , f



|F |


be a tree fragment




5. hi  is the head word of the entity. We nor- 
mally set the head word of an entity  as its last


space of type PTs and let the indicator function
Ii(n) be equal to 1 if the target f1 is rooted at node
n and 0 otherwise, we define the PT kernel as:


word. However, when a preposition exists in 
the entity string, its head word is set as the 
last word before the preposition.  For exam-


K (T1, T2) =



n1 ∈NT1  n2 ∈NT2


∆(n1, n2)


ple, the head word of the entity “University 
of Pennsylvania” is “University”.

6. Mixed n-grams  features of the words and 
their gazetteers/frequent-tag before/after the 
start/end  of an entity.   In addition to the 
normal n-grams solely based on words, we 
mixed words with  gazetteers/frequent-tag 
seen from a large  corpus  and create mixed 
n-grams features.


  Table 2 shows the full set of global features in 
our reranking  framework.   Features are anchored 
to each entity instance and adapted to entity types. 
This helps to discriminate different entities with 
the same surface forms. Moreover, they can be 
combined with n-grams patterns to learn and ex- 
plicitly push the score of the correct  sequence 
above the score of competing sequences.


where NT1      and NT2      are   the set   of  nodes
in  T1  and T2  respectively  and ∆(n1, n2)   =
  i=1 Ii(n1)Ii(n2), i.e.  the number of common 
fragments rooted at the n1  and n2  nodes of the 
type shown in Figure 1.c.
The Polynomial Kernel
  The polynomial  kernel between two candidate 
tagged sequences is defined as:
K (x, y) = (1 + x1 · x2)2,

where x1 and x2 are two feature vectors extracted 
from the two sequences with the global feature 
template.
The Tagging Kernels
  In our reranking framework, we incorporate the 
probability from the original model with the tree 
structure as well as the feature  vectors.  Let us con- 
sider the following notations:



Feature
Description
ws  ws+1  . . . we
Entity string
gs  gs+1   . . . ge
The gazetteer feature within the entity
fs  fs+1   . . . fe
The most frequent NE tag feature (seen from a
large corpus) within the entity
hw
The head word of the entity
lhw
Indicates whether the head word is lower-cased
ws−1  ws ; ws−1  gs ; gs−1  ws ; gs−1  gs
Mixed bigrams of the words/gazetteer features
before/after the start of the entity
we  we+1 ; we  ge+1 ; ge we+1 ; ge ge+1
Mixed bigrams of the words/gazetteer features
before/after the end of the entity
ws−1  ws ; ws−1  fs ; fs−1  ws ; fs−1  fs
Mixed bigrams of the words/frequent-tag fea-
tures before/after the start of the entity
we  we+1 ; we  fe+1 ; fe  we+1 ; fe  fe+1
Mixed bigrams of the words/frequent-tag fea-
tures before/after the end of the entity
ws−2  ws−1  ws ; ws−1  ws  ws+1 ; we−1  we  we+1 ; we−2  we−1  we
Trigram features of the words before/after the
start/end of the entity
ws−2  ws−1  gs ; ws−2  gs−1  ws ; ws−2  gs−1  gs ;
gs−2  ws−1  ws ; gs−2  ws−1  gs ; gs−2  gs−1  ws ; gs−2  gs−1  gs ;
ws−1  ws  gs+1 ; ws−1  gs  ws+1 ; ws−1  gs  gs+1 ;
gs−1  ws  ws+1 ; gs−1  ws  gs+1 ; gs−1  gs  ws+1 ; gs−1  gs  gs+1
Mixed trigrams of the words/gazetteer features
before/after the start of the entity
we−1  we  ge+1 ; we−1  ge we+1 ; we−1  ge ge+1 ;
ge−1  we  we+1 ; ge−1  we  ge+1 ; ge−1  ge we+1 ; ge−1  ge ge+1 ;
we−2  we−1  ge ; we−2  ge−1  we ; we−2  ge−1  ge ;
ge−2  we−1  we ; ge−2  we−1  ge ; ge−2  ge−1  we ; ge−2  ge−1  ge
Mixed trigrams of the words/gazetteer features
before/after the end of the entity
ws−2  ws−1  fs ; ws−2  fs−1  ws ; ws−2  fs−1  fs ;
fs−2  ws−1  ws ; fs−2  ws−1  fs ; fs−2  fs−1  ws ; fs−2  fs−1  fs ;
ws−1  ws  fs+1 ; ws−1  fs  ws+1 ; ws−1  fs  fs+1 ;
fs−1  ws  ws+1 ; fs−1  ws  fs+1 ; fs−1  fs  ws+1 ; fs−1  fs  fs+1
Mixed trigrams of the words/frequent-tag fea-
tures before/after the start of the entity
we−1  we  fe+1 ; we−1  fe  we+1 ; we−1  fe  fe+1 ;
fe−1  we  we+1 ; fe−1  we  fe+1 ; fe−1  fe  we+1 ; fe−1  fe  fe+1 ;
we−2  we−1  fe ; we−2  fe−1  we ; we−2  fe−1  fe ;
fe−2  we−1  we ; fe−2  we−1  fe ; fe−2  fe−1  we ; fe−2  fe−1  fe
Mixed trigrams of the words/frequent-tag fea-
tures before/after the end of the entity

Table 2: Global features in the entity kernel for reranking.  These features are anchored for each entity 
instance and adapted to entity categories.  For example, the entity string (first feature) of the entity 
“United Nations” with entity type “ORG” is “ORG United Nations”.




• K (x, y) = L(x)  · L(y)  is the basic kernel
where L(x)  is the log probability of a can-
didate tagged sequence x under the original 
probability model.
• T K (x, y) = t(x) · t(y) is the partial tree ker- 
nel under the structure representation

• F K (x, y) = f (x) · f (y) is the polynomial 
kernel under the global features

  The tagging  kernels  between two tagged  se- 
quences are defined  in the following combina- 
tions:
1. C T K = α · K + (1 − α) · T K
2. C F K = β · K + (1 − β) · F K
3. C T F K = γ · K + (1 − γ) · (T K + F K )
  

where α, β, γ are parameters weighting the two 
participating terms. Experiments on the validation 
set showed that these combinations  yield the best 
performance with α  = 0.2 for both languages, 
β = 0.4 for English and β = 0.3 for and Italian, 
γ = 0.24 for English and γ = 0.2 for Italian.

4   Experimens and Results

4.1   Experimental Setup

As a baseline  we trained the CRFs classifier on 
the full training  portion  (11,227 sentences in the 
Italian and 14,987  sentences in the English cor- 
pus). In developing  a reranking  strategy for both 
English and Italian, the training data was split into
5 sections, and in each case the baseline classifier 
was trained on 4/5 of the data, then used to decode 
the remaining 1/5.


  The top 10 hypotheses together with their log 
probabilities were recovered for each training  sen- 
tence. Similarly, a model  trained  on the whole 
training  data was used to produce 10 hypotheses 
for each sentence in the development set. For the 
reranking experiments, we applied different ker- 
nel setups to the two corpora described in Section
2.1. The three kernels were trained on the training 
portion.

Italian Test
P
R
F
C RF s
83.43
77.48
80.34
C T K
84.97
78.03
81.35
C F K
84.93
79.13
81.93
CTFK
85.99
82.73
84.33
(Zanoli et al., 2009)
84.07
80.02
82.00


English Test
P
R
F
C RF s
85.37
84.35
84.86
C T K
87.19
84.79
85.97
C F K
86.53
86.75
86.64
CTFK
88.07
88.25
88.16
(Ratinov and Roth, )
N/A
N/A
90.57

Table 3: Reranking results of the three tagging 
kernels on the Italian and English testset.

4.2   Discussion
Table 3 presents the reranking  results on the test 
data of both corpora. The results show a 20.29% 
relative improvement in F-measure for Italian and
21.79% for English.
  C F K based on unstructured features achieves 
higher accuracy than C T K based on structured 
features.  However,  the huge amount of subtrees 
generated by the PT kernel may limit the expres- 
sivity of some structural features, e.g. many frag- 
ments may only generate noise. This problem is 
less important  with the polynomial  kernel where 
global features are tailored for individual entities.
  In any case, the experiments demonstrate that 
both tagging kernels C T K and C F K give im- 
provement over the CRFs baseline in both lan- 
guages. This suggests that structured and unstruc- 
tured features are effective in discriminating be- 
tween competing NE annotations.
  Furthermore, the combination of the two tag- 
ging kernels on both standard  corpora  shows  a


large improvement in F-measure from 80.34% to
84.33% for Italian and from 84.86% to 88.16% 
for English  data. This suggests that these two ker- 
nels, corresponding to two kinds of feature, com- 
plement each other.
  To better collocate  our results with previous 
work, we report the best NER outcome on the 
Italian (Zanoli et al., 2009) and the English (Rati- 
nov and Roth, ) datasets, in the last row (in italic) 
of each table.  This shows that our model outper- 
forms the best Italian NER system and it is close 
to the state-of-art model for English, which ex- 
ploits many complex features9. Also note that we 
are very close to the F1 achieved by the best sys- 
tem of CoNLL 2003, i.e. 88.8.

5   Conclusion

We  analyzed  the impact of  kernel-based  ap- 
proaches  for  modeling dependencies  between 
tagged sequences for NER. Our study illustrates 
that each individual  kernel, either with structured 
or with flat features clearly gives improvement to 
the base model. Most interestingly, as we showed, 
these contributions  are independent  and, the ap- 
proaches can be used together to yield better re- 
sults. The composite kernel, which combines both 
kinds of features, can outperform the state-of-the- 
art.
  In the future, it  will  be very interesting  to 
use syntactic/semantic  kernels, as for example in 
(Basili et al., 2005; Bloehdorn and Moschitti,
2007a;  Bloehdorn  and Moschitti, 2007b). An- 
other promising direction is the use of syntactic 
trees,  feature  sequences and pairs of instances, 
e.g. (Nguyen et al., 2009; Moschitti,  2008).

Acknowledgments

We  would like  to thank Roberto Zanoli and 
Marco   Dinarelli    for    helpful   explanation 
about their work.    This work has  been par- 
tially  funded by  the LiveMemories project 
(http://www.livememories.org/) and   Expert 
System (http://www.expertsystem.net/)  research 
grant.



  9 In the future we will be able to integrate them with the 
authors collaboration.


References

Basili, Roberto, Marco Cammisa,  and Alessandro 
Moschitti. 2005. Effective  use of WordNet seman- 
tics via kernel-based learning. In CoNLL.

Bloehdorn, Stephan and Alessandro Moschitti. 2007a.
Combined syntactic and semantic kernels for text 
classification. In ECIR.

Bloehdorn, Stephan and Alessandro Moschitti. 2007b.
Structure and semantics for expressive text kernels. 
In CIKM.

Bunescu, Razvan C. and Raymond J. Mooney.  2005a.
A shortest path dependency kernel for relation ex- 
traction. In EMNLP.

Carreras,  Xavier, Llu´ıs Ma`rques,  and Llus Padro´ .
2002. Named entity extraction using Adaboost.  In
CoNLL.

Chieu, Hai Leong and Hwee Tou Ng. 2002. Named 
entity recognition: A maximum  entropy approach 
using global information. In COLING.

Chieu, Hai Leong and Hwee Tou Ng. 2003. Named 
entity recognition  with a  maximum entropy ap- 
proach. In CoNLL.

Chinchor, Nancy and Patricia Robinson. 1998. Muc-7 
named entity task definition.  In the MUC.

Collins, Michael and Nigel Duffy. 2001. Convolution 
kernels for natural language. In NIPS.

Collins, Michael and Terry Koo. 2000. Discriminative 
reranking for natural language parsing. In ICML.

Collins, Michael. 2002a. New ranking algorithms for 
parsing and tagging: Kernels over discrete struc- 
tures, and the voted perceptron. In ACL.

Collins, Michael.  2002b.  Ranking algorithms for 
named-entity extraction boosting and the voted per- 
ceptron. In ACL.

Dinarelli,  Marco, Alessandro Moschitti,  and Giuseppe 
Riccardi.  2009.  Re-ranking  models based on small 
training data for spoken language understanding.  In 
EMNLP.

Florian, Radu, Abe Ittycheriah, Hongyan Jing, and 
Tong Zhang.  2003.   Named  entity recognition 
through classifier combination. In CoNLL.

Huang, Liang. 2008. Forest reranking: Discriminative 
parsing with non-local features. In ACL-HLT.

Lafferty,  John, Andrew McCallum, and Fernando 
Pereira. 2001. Conditional  random fields: Prob- 
abilistic models  for segmenting  and labeling se- 
quence data. In ICML.


Lodhi, Huma, Craig Saunders,  John  Shawe  Taylor, 
Nello Cristianini, , and Chris Watkins. 2002. Text 
classification using string kernels. Journal of Ma- 
chine Learning  Research, pages 419–444.

Magnini,  Bernardo, Emmanuele Pianta, Christian Gi- 
rardi, Matteo Negri, Lorenza  Romano,  Manuela 
Speranza, Valentina  Bartalesi  Lenzi, and Rachele 
Sprugnoli. 2006. I-CAB: the italian content anno- 
tation bank. In LREC.

Moschitti, Alessandro. 2004. A study on convolution 
kernels for shallow semantic parsing. In ACL.

Moschitti, Alessandro. 2006.  Efficient convolution 
kernels  for dependency  and constituent syntactic 
trees. In ICML.

Moschitti, Alessandro. 2008. Kernel methods, syntax 
and semantics for relational text categorization.  In 
CIKM.

Nguyen, Truc-Vien T.,  Alessandro  Moschitti, and 
Giuseppe Riccardi. 2009. Convolution  kernels on 
constituent,  dependency and sequential structures 
for relation extraction. In EMNLP.

Ratinov, Lev and Dan Roth.  Design challenges and 
misconceptions in named  entity recognition.  In 
CoNLL.

Rifkin, Ryan Michael and Tomaso Poggio. 2002. Ev- 
erything old is new again: a fresh look at historical 
approaches in machine learning.  PhD thesis, MIT.

Sang, Erik  F. Tjong Kim  and Fien De Meulder.
2003. Introduction to the conll-2003  shared task: 
Language-independent  named  entity recognition. 
In CoNLL.

Scho¨ lkopf, Bernhard and Alexander  J. Smola. 2001.
Learning with Kernels: Support Vector Machines, 
Regularization, Optimization,  and Beyond.  MIT 
Press, Cambridge,  MA, USA.

Shen, Libin,  Anoop Sarkar, and Franz Josef  Och.
2004. Discriminative reranking for machine transla- 
tion. In HLT-NAACL, Boston, Massachusetts, USA.

Surdeanu, Mihai, Sanda Harabagiu,  John Williams, 
and Paul Aarseth. 2003. Using predicate-argument 
structures for information extraction. In ACL.

Vapnik, Vladimir N. 1998. Statistical Learning The- 
ory. John Wiley and Sons, New York.

Vishwanathan, S.V.N. and Alexander J. Smola.  2002.
Fast kernels on strings and trees. In NIPS.

Zanoli, Roberto, Emanuele Pianta, and Claudio Giu- 
liano. 2009. Named entity recognition through re- 
dundancy driven classifiers. In EVALITA.

