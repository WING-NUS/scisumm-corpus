<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">This paper presents an approach that uses structural information for Japanese named entity recognition (NER).</S>
		<S sid ="2" ssid = "2">Our NER system is based on Support Vector Machine (SVM), and utilizes four types of structural information: cache features, coreference relations, syntactic features and caseframe features, which are obtained from structural analyses.</S>
		<S sid ="3" ssid = "3">We evaluated our approach on CRL NE data and obtained a higher F-measure than existing approaches that do not use structural information.</S>
		<S sid ="4" ssid = "4">We also conducted experiments on IREX NE data and an NE-annotated web corpus and conﬁrmed that structural information improves the performance of NER.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="5" ssid = "5">Named entity recognition (NER) is the task of identifying and classifying phrases into certain classes of named entities (NEs), such as names of persons, organizations and locations.</S>
			<S sid ="6" ssid = "6">Japanese texts, which we focus on, are written without using blank spaces.</S>
			<S sid ="7" ssid = "7">Therefore, Japanese NER has tight relation with morphological analysis, and thus it is often performed immediately after morphological analysis (Masayuki and Matsumoto, 2003; Yamada, 2007).</S>
			<S sid ="8" ssid = "8">However, such approaches rely only on local context.</S>
			<S sid ="9" ssid = "9">The Japanese NER system proposed in (Nakano and Hirai, 2004), which achieved the highest F-measure among conventional systems, introduced the bunsetsu1 feature in order to consider wider context, but considers only adjacent bunsetsus.</S>
			<S sid ="10" ssid = "10">*Research Fellow of the Japan Society for the Promotion of Science (JSPS) 1 Bunsetsu is a commonly used linguistic unit in Japanese, consisting of one or more adjacent content words and zero or more following functional words.</S>
			<S sid ="11" ssid = "11">On the other hand, as for English or Chinese, various NER systems have explored global information and reported their effectiveness.</S>
			<S sid ="12" ssid = "12">In (Malouf, 2002; Chieu and Ng, 2002), information about features assigned to other instances of the same token is utilized.</S>
			<S sid ="13" ssid = "13">(Ji and Grishman, 2005) uses the information obtained from coreference analysis for NER.</S>
			<S sid ="14" ssid = "14">(Mohit and Hwa, 2005) uses syntactic features in building a semi-supervised NE tagger.</S>
			<S sid ="15" ssid = "15">In this paper, we present a Japanese NER system that uses global information obtained from several structural analyses.</S>
			<S sid ="16" ssid = "16">To be more speciﬁc, our system is based on SVM, recognizes NEs after syntactic, case and coreference analyses and uses information obtained from these analyses and the NER results for the previous context, integrally.</S>
			<S sid ="17" ssid = "17">At this point, it is true that NER results are useful for syntactic, case and coreference analyses, and thus these analyses and NER should be performed in a complementary way.</S>
			<S sid ="18" ssid = "18">However, since we focus on NER, we recognize NE after these structural analyses.</S>
	</SECTION>
	<SECTION title="Japanese NER Task. " number = "2">
			<S sid ="19" ssid = "1">A common standard deﬁnition for Japanese NER task is provided by IREX workshop (IREX Committee, 1999).</S>
			<S sid ="20" ssid = "2">IREX deﬁned eight NE classes as shown in Table 1.</S>
			<S sid ="21" ssid = "3">Compared with the MUC6 NE task definition (MUC, 1995), the NE class “ARTIFACT,” which contains book titles, laws, brand names and so on, is added.</S>
			<S sid ="22" ssid = "4">NER task can be deﬁned as a chunking problem to identify token sequences that compose NEs.</S>
			<S sid ="23" ssid = "5">The chunking problem is solved by annotating chunk tags to tokens.</S>
			<S sid ="24" ssid = "6">Five chunk tag sets, IOB1, IOB2, IOE1, IOE2 and IOBES are commonly used.</S>
			<S sid ="25" ssid = "7">In this paper, we use the IOBES model, in which “S” denotes a chunk itself, and “B,” “I” and “E” denote the Table 1: Deﬁnition of NE in IREX.</S>
			<S sid ="26" ssid = "8">NE class Examples ORGANIZATION NHK Symphony Orchestra PERSON Kawasaki Kenjiro LOCATION Rome, Sinuiju ARTIFACT Nobel Prize DATE July 17, April this year TIME twelve o’clock noon MONEY sixty thousand dollars PERCENT 20%, thirty percents beginning, intermediate and end parts of a chunk.</S>
			<S sid ="27" ssid = "9">If a token does not belong to any named entity, it is tagged as “O.” Since IREX deﬁned eight NE classes, tokens are classiﬁed into 33 (= 8 × 4 + 1) NE tags.</S>
			<S sid ="28" ssid = "10">For example, NE tags are assigned as following: (1) Kotoshi 4 gatsu Roma ni itta.</S>
			<S sid ="29" ssid = "11">this year April Rome to went B-DATE I-DATE E-DATE S-LOCATION O O (φ went to Rome on April this year.)</S>
	</SECTION>
	<SECTION title="Motivation for Our Approach. " number = "3">
			<S sid ="30" ssid = "1">Our NER system utilizes structural information.</S>
			<S sid ="31" ssid = "2">In this section, we describe the motivation for our approach.</S>
			<S sid ="32" ssid = "3">High-performance Japanese NER systems are often based on supervised learning, and most of them use only local features, such as features obtained from the target token, two preceding tokens and two succeeding tokens.</S>
			<S sid ="33" ssid = "4">However, in some cases, NEs cannot be recognized by using only local features.</S>
			<S sid ="34" ssid = "5">For example, while “Kawasaki” in the second sentence of (2) is the name of a person, “Kawasaki” in the second sentence of (3) is the name of a soccer team.</S>
			<S sid ="35" ssid = "6">However, the second sentences of (2) and (3) are exactly the same, and thus it is impossible to correctly distinguish these NE classes by only using information obtained from the second sentences.</S>
			<S sid ="36" ssid = "7">(2) Kachi-ha senpatsu-no Kawasaki Kenjiro.</S>
			<S sid ="37" ssid = "8">winner starter Kawasaki-ha genzai 4 shou 3 pai.</S>
			<S sid ="38" ssid = "9">now won lost (The winning pitcher is the starter Kenjiro Kawasaki.</S>
			<S sid ="39" ssid = "10">Kawasaki has won 4 and lost 3.)</S>
			<S sid ="40" ssid = "11">(3) Dai 10 setsuwa Kawasaki Frontale-to taisen.</S>
			<S sid ="41" ssid = "12">the round against Kawasaki-ha genzai 4 shou 3 pai.</S>
			<S sid ="42" ssid = "13">now won lost (The 10th round is against Kawasaki Frontale.</S>
			<S sid ="43" ssid = "14">Kawasaki has won 4 and lost 3.)</S>
			<S sid ="44" ssid = "15">In order to recognize these NE classes, it is essential to use the information obtained from the previous context.</S>
			<S sid ="45" ssid = "16">Therefore, we utilize information obtained from the NER for the previous context: cache feature and coreference relation.</S>
			<S sid ="46" ssid = "17">For another example, “Shingishu” in (4) is the name of city in North Korea.</S>
			<S sid ="47" ssid = "18">The most important clue for recognizing “Shingishu” as “LOCATION” may be the information obtained from the head verb, “wataru (get across).” (4) Shingishukara Ouryokkowo wataru.</S>
			<S sid ="48" ssid = "19">Sinuiju from Amnokkang get across (φ gets across the Amnokkang River from Sinuiju.)</S>
			<S sid ="49" ssid = "20">However, when using only local features, the word “wataru” is not taken into consideration because there are more than two morphemes between “shu2 ” and “wataru.” In order to deal with such problem, we use the information obtained from the head verb: syntactic feature and caseframe feature.</S>
	</SECTION>
	<SECTION title="NER Using Structural Information. " number = "4">
			<S sid ="50" ssid = "1">4.1 Outline of Our NER System.</S>
			<S sid ="51" ssid = "2">Our NER system performs the chunking process based on morpheme units because character-based methods do not outperform morpheme-based methods (Masayuki and Matsumoto, 2003) and are not suitable for considering wider context.</S>
			<S sid ="52" ssid = "3">A wide variety of trainable models have been applied to Japanese NER task, including maximum entropy models (Utsuro et al., 2002), support vector machines (Nakano and Hirai, 2004; Yamada, 2007) and conditional random ﬁelds (Fukuoka, 2006).</S>
			<S sid ="53" ssid = "4">Our system applies SVMs because, for Japanese NER, SVM-based systems achieved higher F-measure than the other systems.</S>
			<S sid ="54" ssid = "5">(Isozaki and Kazawa, 2003) proposed an SVM-based NER system with Viterbi search, which outperforms an SVM-based NER system with sequential determination, and our system basically follows this system.</S>
			<S sid ="55" ssid = "6">Our NER system consists of the following four steps: 1.</S>
			<S sid ="56" ssid = "7">Morphological analysis.</S>
			<S sid ="57" ssid = "8">2.</S>
			<S sid ="58" ssid = "9">Syntactic, case and coreference analyses.</S>
			<S sid ="59" ssid = "10">3.</S>
			<S sid ="60" ssid = "11">Feature extraction for chunking.</S>
			<S sid ="61" ssid = "12">4.</S>
			<S sid ="62" ssid = "13">SVM and Viterbi search based chunking.</S>
			<S sid ="63" ssid = "14">The following sections describe each of these steps in detail.</S>
			<S sid ="64" ssid = "15">2 Since the dictionary for morphological analysis has no entry “Shingishu,” “Shingishu” is analyzed as consisting of three morphemes: “shin,” “gi” and “shu.” Input sentence: Gai mu sho no shin Bei ha . foreign affairs ministry in pro America group (Pro-America group in the Ministry of Foreign Affairs.)</S>
			<S sid ="65" ssid = "16">Output of JUMAN: Gaimu sho no shin Bei ha . noun noun particle noun noun noun Output of ChaSen: Gaimusho noun Figure 1: Example of morphological analyses.</S>
			<S sid ="66" ssid = "17">4.2 Morphological Analysis.</S>
			<S sid ="67" ssid = "18">While most existing Japanese NER systems use ChaSen (Matsumoto et al., 2003) as a morphological analyzer, our NER system uses a Japanese morphological analyzer JUMAN (Kurohashi and Kawahara, 2005) because of the following two reasons.</S>
			<S sid ="68" ssid = "19">First, JUMAN tends to segment a sentence into smaller morphemes than ChaSen, and this is a good tendency for morpheme-based NER systems because the boundary contradictions between morphological analysis and NEs are considered to be reduced.</S>
			<S sid ="69" ssid = "20">Figure 1 shows an example of the outputs of JUMAN and ChaSen.</S>
			<S sid ="70" ssid = "21">Although both analyses are reasonable, JUMAN divided “Gaimusho” and “shin-Bei” into two morphemes, while ChaSen left them as a single morpheme.</S>
			<S sid ="71" ssid = "22">Second, JUMAN adds categories to some morphemes, which can be utilized for NER.</S>
			<S sid ="72" ssid = "23">In JUMAN, about thirty categories are deﬁned and tagged to about one ﬁfth of morphemes.</S>
			<S sid ="73" ssid = "24">For example, “ringo (apple),” “inu (dog)” and “byoin (hospital)” are tagged as “FOOD,” “ANIMAL” and “FACILITY,” respectively.</S>
			<S sid ="74" ssid = "25">4.3 Syntactic, Case and Coreference Analyses.</S>
			<S sid ="75" ssid = "26">syntactic analysis Syntactic analysis is performed by using the Japanese parser KNP (Kurohashi and Nagao, 1994).</S>
			<S sid ="76" ssid = "27">KNP employs some heuristic rules to determine the head of a modiﬁer.</S>
			<S sid ="77" ssid = "28">case analysis Case analysis is performed by using the system proposed in (Kawahara and Kurohashi, 2002).</S>
			<S sid ="78" ssid = "29">This system uses Japanese case frames that are automatically constructed from a large corpus.</S>
			<S sid ="79" ssid = "30">To utilize case analysis for NER, we constructed case frames that include NE labels in advance.</S>
			<S sid ="80" ssid = "31">We explain details in Section 4.4.2.</S>
			<S sid ="81" ssid = "32">The case analysis is applied to each predicate in an input sentence.</S>
			<S sid ="82" ssid = "33">For details see (Kawahara and Kurohashi, 2002).</S>
			<S sid ="83" ssid = "34">coreference analysis Coreference analysis is performed by using the coreference analyzer proposed by (Sasano et al., 2007).</S>
			<S sid ="84" ssid = "35">As will be mentioned in Section 4.4.2, our NER system uses coreference relations only when coreferential expressions do not share same morphemes.</S>
			<S sid ="85" ssid = "36">Basically, such coreference relations are recognized by using automatically acquired synonym knowledge.</S>
			<S sid ="86" ssid = "37">4.4 Feature Extraction.</S>
			<S sid ="87" ssid = "38">4.4.1 Basic Features As basic features for chunking, our NER system uses the morpheme itself, character type, POS tag and category if it exists.</S>
			<S sid ="88" ssid = "39">As character types, we deﬁned seven types: “kanji,” “hiragana,” “katakana,” “kanji with hiragana,” “punctuation mark,” “alphabet” and “digit.” As for POS tag, more than one POS feature are extracted if the target morpheme has POS ambiguity.</S>
			<S sid ="89" ssid = "40">In addition, besides POS tag obtained by JU- MAN, our system also uses POS tag obtained from Japanese morphological analyzer MeCab3 that uses IPADIC as a word dictionary (Asahara and Mat- sumoto, 2002).</S>
			<S sid ="90" ssid = "41">The JUMAN dictionary has few named entity entries; thus our system supplements the lack of lexical knowledge by using MeCab.</S>
			<S sid ="91" ssid = "42">4.4.2 Structural Features Our NER system uses three types of global features: cache features, syntactic features and case- frame features, and a rule that reﬂects coreference relations.</S>
			<S sid ="92" ssid = "43">Although the coreference relations are not used as features, we describe how to use them in this section.</S>
			<S sid ="93" ssid = "44">cache feature If the same morpheme appears multiple times in a single document, in most cases the NE tags of these morphemes have some relation to each other, and the NER results for previous parts of the document can be a clue for the analysis for following parts.</S>
			<S sid ="94" ssid = "45">We consider the examples (2) and (3) again.</S>
			<S sid ="95" ssid = "46">Although the second sentences of (2) and (3) are exactly the same, we can recognize “Kawasaki” in the second sentence of (2) is “S-PERSON” and “Kawasaki” in the second sentence of (3) is “S- ORGANIZATION” by reading the ﬁrst sentences.</S>
			<S sid ="96" ssid = "47">To utilize the information obtained from previous parts of the document, our system uses the NER results for previous parts of the document as features, called cache features.</S>
			<S sid ="97" ssid = "48">When analyzing (2), our system uses the outputs of NE recognizer for 3 http://mecab.sourceforge.jp/ “Kawasaki” in the ﬁrst sentence as a feature for “Kawasaki” in the second sentence.</S>
			<S sid ="98" ssid = "49">For simplicity, our system uses correct NE tags when training.</S>
			<S sid ="99" ssid = "50">That is, as a feature for “Kawasaki” in the second sentence of (2), the correct feature “B-PERSON” is always added when training, not always added when analyzing.</S>
			<S sid ="100" ssid = "51">coreference rule Coreference relation can be a clue for NER.</S>
			<S sid ="101" ssid = "52">This clue is considered by using cache features to a certain extent.</S>
			<S sid ="102" ssid = "53">However, if the same morpheme is not used, cache features cannot work.</S>
			<S sid ="103" ssid = "54">For example, “NHK kokyo gakudan” and “N-kyo” in (5) have coreference relation, but they do not share the same morpheme.</S>
			<S sid ="104" ssid = "55">(5) NHK kokyo gakudan-no ongaku kantokuni symphony orchestra musical director shuunin.</S>
			<S sid ="105" ssid = "56">N-kyo-to kyoenshite irai ...</S>
			<S sid ="106" ssid = "57">became perform together since (He became musical director of the NHK Symphony Orchestra.</S>
			<S sid ="107" ssid = "58">Since performing together with N-kyo ...</S>
			<S sid ="108" ssid = "59">In this case, “NHK kokyo gakudan” can easily be recognized as “ORGANIZATION,” because it ends with “kokyo gakudan (symphony orchestra).” Meanwhile, “N-kyo,” the abbreviation of “NHK kokyo gakudan,” cannot easily be recognized as “ORGANIZATION.” Therefore, our system uses a heuristic rule that if a morpheme sequence is analyzed to be coreferential to a previous morpheme sequence that is recognized as an NE class, the latter morpheme sequence is recognized as the same NE class.</S>
			<S sid ="109" ssid = "60">Since this heuristic rule is introduced in order to utilize the coreference relation that is not reﬂected by cache features, our system applies this rule only when coreferential expressions do not have any morphemes in common.</S>
			<S sid ="110" ssid = "61">syntactic feature As mentioned in Section 3, our system utilizes the information obtained from the head verb.</S>
			<S sid ="111" ssid = "62">As syntactic features, our system uses the head verb itself and the surface case of the bunsetsu that includes the target morpheme.</S>
			<S sid ="112" ssid = "63">For the morpheme “shin” in example (4), the head verb “wataru (get across)” and the surface case “kara (from)” are added as syntactic features.</S>
			<S sid ="113" ssid = "64">caseframe feature Syntactic features cannot work if the head verb does not appear in the training data.</S>
			<S sid ="114" ssid = "65">To overcome this data sparseness problem, case- frame features are introduced.</S>
			<S sid ="115" ssid = "66">Table 2: Case frame of “haken (dispatch).” case examples ga Japan:23,party:13,country:12,government:7, (nominative) company6,ward:6,corps:5,UN:4,US:4,Korea:4, team:4,.</S>
			<S sid ="116" ssid = "67">(ORGANIZATION,LOCATION) wo party:1249,him:1017,soldier:932,ofﬁcial:906, (objective) company6:214,instructor:823,expert:799, helper:694,staff:398,army:347,.</S>
			<S sid ="117" ssid = "68">ni Iraq:700,on-the-scene:576,abroad:335, (locative) home:172,Japan:171,Indirect Ocean:142, scene:141,China:125,.</S>
			<S sid ="118" ssid = "69">(LOCATION) For example, although the head verb “haken (dispatch)” can be a clue for recognizing “ICAO” in (6) as “ORGANIZATION,” syntactic features cannot work if “haken (dispatch)” did not appear in the training data.</S>
			<S sid ="119" ssid = "70">(6) ICAO-ha genchini senmonkawo hakenshita.</S>
			<S sid ="120" ssid = "71">scene to expert dispatched (ICAO dispatched experts to the scene) However, this clue can be utilized if there is knowledge that the “ga (nominative)” case of “haken (dispatch)” is often assigned by “ORGANIZATION.” Therefore, we construct case frames that include NE labels in advance.</S>
			<S sid ="121" ssid = "72">Case frames describe what kinds of cases each verb has and what kinds of nouns can ﬁll a case slot.</S>
			<S sid ="122" ssid = "73">We construct them from about ﬁve hundred million sentences.</S>
			<S sid ="123" ssid = "74">We ﬁrst recognize NEs appearing in the sentences by using a primitive NER system that uses only local features, and then construct the case frames from the NE-recognized sentences.</S>
			<S sid ="124" ssid = "75">To be more speciﬁc, if one tenth of the examples of a case are classiﬁed as a certain NE class, the corresponding label is attached to the case.</S>
			<S sid ="125" ssid = "76">Table 2 shows the constructed case frame of “haken (dispatch).” In the “ga (nominative)” case, the NE labels, “ORGANIZATION” and “LOCATION” are attached.</S>
			<S sid ="126" ssid = "77">We then explain how to utilize these case frames.</S>
			<S sid ="127" ssid = "78">Our system ﬁrst performs case analysis, and uses as caseframe features the NE labels attached in the case to which the target morpheme is assigned.</S>
			<S sid ="128" ssid = "79">For instance, by the case analyzer, the postpositional particle “-ha” in (6) is recognized as meaning nominative and “ICAO” is assigned to the “ga (nominative)” case of the case frame of “haken (dispatch).”Therefore, the caseframe features, “ORGANIZATION” and “LOCATION” are added to the features for the morpheme “ICAO.” 4.5 SVM and Viterbi Search Based Chunking.</S>
			<S sid ="129" ssid = "80">To utilize cache features obtained from the previous parts of the same sentence, our system determines Table 3: Experimental results (F-measure).</S>
			<S sid ="130" ssid = "81">signiﬁcant at the .1 level:*, .01 level:**, .001 level:*** NE tags clause by clause.</S>
			<S sid ="131" ssid = "82">The features extracted from two preceding morphemes and two succeeding morphemes are also used for chunking a target morpheme.</S>
			<S sid ="132" ssid = "83">Since SVM can solve only a two-class problem, we have to extend a binary classiﬁer SVM to n-class classiﬁer.</S>
			<S sid ="133" ssid = "84">Here, we employ the one versus rest method, in which we prepared n binary classi- ﬁers and each classiﬁer is trained to distinguish a class from the rest of the classes.</S>
			<S sid ="134" ssid = "85">To consider consistency of NE tags in a clause, our system uses Viterbi search with some constraints such as a “B-DATE” must be followed by “I-DATE” or “E-DATE.” Since SVMs do not output probabilities, our system uses the SVM+sigmoid method (Platt et al., 2000).</S>
			<S sid ="135" ssid = "86">That is, a sigmoid function s(x) = 1/(1+exp(−βx)) is applied to map the output of SVM to a probability-like value.</S>
			<S sid ="136" ssid = "87">Our system determines NE tags by using these probability-like values.</S>
			<S sid ="137" ssid = "88">Our system is trained by TinySVM0.094 with C = 0.1 and uses a ﬁxed value β = 10.</S>
			<S sid ="138" ssid = "89">This process is almost the same as the process proposed by Isozaki and Kazawa and for details see (Isozaki and Kazawa, 2003).</S>
	</SECTION>
	<SECTION title="Experiments. " number = "5">
			<S sid ="139" ssid = "1">5.1 Data.</S>
			<S sid ="140" ssid = "2">For training, we use CRL NE data, which was prepared for IREX.</S>
			<S sid ="141" ssid = "3">CRL NE data has 18,677 NEs on 1,174 articles in Mainichi Newspaper.</S>
			<S sid ="142" ssid = "4">For evaluation, we use three data: CRL NE data, IREX’s formal test data called GENERAL and WEB NE data.</S>
			<S sid ="143" ssid = "5">When using CRL NE data for evaluation, we perform ﬁve-fold cross-validation.</S>
			<S sid ="144" ssid = "6">IREX test data has 1,510 NEs in 71 articles from Mainichi Newspaper.</S>
			<S sid ="145" ssid = "7">Although both CRL NE data and IREX test data use Mainichi Newspaper, these formats are not the same.</S>
			<S sid ="146" ssid = "8">For example, CRL NE data removes parenthesis expressions, but IREX test data does not.</S>
			<S sid ="147" ssid = "9">WEB NE data, which we annotated NEs on corpus collected from the Web, has 1,686 NEs in 354 arti 4 http://chasen.org/ taku/software/TinySVM/ cles.</S>
			<S sid ="148" ssid = "10">Although the domain of the web corpus differs from that of CRL NE data, the format of the web corpus is the same as CRL NE data format.</S>
			<S sid ="149" ssid = "11">5.2 Experiments and Discussion.</S>
			<S sid ="150" ssid = "12">To conﬁrm the effect of each feature, we conducted experiments on seven conditions as follows: 1.</S>
			<S sid ="151" ssid = "13">Use only basic features (baseline).</S>
			<S sid ="152" ssid = "14">2.</S>
			<S sid ="153" ssid = "15">Add cache features to baseline.</S>
			<S sid ="154" ssid = "16">3.</S>
			<S sid ="155" ssid = "17">Add the coreference rule to baseline.</S>
			<S sid ="156" ssid = "18">4.</S>
			<S sid ="157" ssid = "19">Add parent features to baseline.</S>
			<S sid ="158" ssid = "20">5.</S>
			<S sid ="159" ssid = "21">Add caseframe features to baseline.</S>
	</SECTION>
	<SECTION title="Add thesaurus features to baseline. " number = "6">
			<S sid ="160" ssid = "1">7.</S>
			<S sid ="161" ssid = "2">Use all structural information and thesaurus Since (Masayuki and Matsumoto, 2003; Nakano and Hirai, 2004) reported the performance of NER system was improved by using a thesaurus, we also conducted experiment in which semantic classes obtained from a Japanese thesaurus “Bunrui Goi Hyo” (NLRI, 1993) were added to the SVM features.</S>
			<S sid ="162" ssid = "3">Table 3 shows the experimental results.To judge the statistical signiﬁcance of the dif ferences between the performance of the baseline system and that of the others, we conducted a McNemar-like test.</S>
			<S sid ="163" ssid = "4">First, we extract the outputs that differ between the baseline method and the target method.</S>
			<S sid ="164" ssid = "5">Then, we count the number of the outputs that only baseline method is correct and that only target method is correct.</S>
			<S sid ="165" ssid = "6">Here, we assume that these outputs have the binomial distribution and apply binomial test.</S>
			<S sid ="166" ssid = "7">As signiﬁcance level, we use .1 level, .01 level and .001 level.</S>
			<S sid ="167" ssid = "8">The results of the signiﬁ- cance tests are also shown in Table 3.</S>
			<S sid ="168" ssid = "9">When comparing the performance between data sets, we can say that the performance for WEB NE data is much worse than the others.</S>
			<S sid ="169" ssid = "10">This may be because the domain of the WEB corpus differs from that of CRL NE data.</S>
			<S sid ="170" ssid = "11">As for the differences in the same data set, cache features and syntactic features improve the performance not dramatically but consistently and independently from the data set.</S>
			<S sid ="171" ssid = "12">The coreference rule also improves the performance for all data sets, but especially for IREX test data.</S>
			<S sid ="172" ssid = "13">This may be because IREX test data does not remove parenthesis expressions, and thus there are a many coreferential expressions in the data.</S>
			<S sid ="173" ssid = "14">Caseframe features improve the performance for WEB NE data, but do not contribute to the performance for CRL NE data and Table 4: Comparison with previous work.</S>
			<S sid ="174" ssid = "15">CRL cross IREX Learning Analysis Features validation test data Method Units (Isozaki and Kazawa, 2003) 86.77 85.10 SVM + Viterbi morpheme basic features (Masayuki and Matsumoto , 2003) 87.21 SVM character +thesaurus (Fukuoka, 2006) 87.71 Semi-Markov CRF character basic features (Yamada, 2007) 88.33 SVM + Shift-Reduce morpheme +bunsetsu features (Nakano and Hirai, 2004) 89.03 SVM character +bunsetsu features &amp; thesaurus Our system 89.40 87.72 SVM + Viterbi morpheme +structural information &amp; thesaurus IREX test data.</S>
			<S sid ="175" ssid = "16">This result shows that caseframe features are very generalized features and effective for data of different domain.</S>
			<S sid ="176" ssid = "17">On the other hand, thesaurus features improve the performance for CRL NE data and IREX test data, but worsen the performance for WEB NE data.</S>
			<S sid ="177" ssid = "18">The main cause for this may be overﬁtting to the domain of the training data.</S>
			<S sid ="178" ssid = "19">By using all structural information, the performance is signiﬁcantly improved for all data sets, and thus we can say that the structural information improves the performance of NER.</S>
			<S sid ="179" ssid = "20">5.3 Comparison with Previous Work.</S>
			<S sid ="180" ssid = "21">Table 4 shows the comparison with previous work for CRL NE data and IREX test data.</S>
			<S sid ="181" ssid = "22">Our system outperforms all other systems, and thus we can con- ﬁrm the effectiveness of our approach.</S>
			<S sid ="182" ssid = "23">6 Conclusion.</S>
			<S sid ="183" ssid = "24">In this paper, we presented an approach that uses structural information for Japanese NER.</S>
			<S sid ="184" ssid = "25">We introduced four types of structural information to an SVM-based NER system: cache features, coreference relations, syntactic features and caseframe features, and conducted NER experiments on three data.</S>
			<S sid ="185" ssid = "26">As a consequence, the performance of NER was improved by using structural information and our approach achieved a higher F-measure than existing approaches.</S>
	</SECTION>
</PAPER>
