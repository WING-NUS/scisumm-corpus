Citance Number: 1 | Reference Article:  C02-1025.txt | Citing Article:  C10-2104.txt | Citation Marker Offset:  ['115'] | Citation Marker:  Chieu and Ng, 2002 | Citation Offset:  ['115'] | Citation Text:  <S sid ="115" ssid = "27">In such cases, neither global features (Chieu and Ng, 2002) nor aggregated contexts (Chieu and Ng, 2003) can help.</S> | Reference Offset:  ['63'] | Reference Text:  <S sid ="63" ssid = "3">Global features are extracted from other occurrences of the same token in the whole document.</S> | Discourse Facet:  Method_Citation | Annotator:  Aakansha Gehlot |


Citance Number: 2 | Reference Article:  C02-1025.txt | Citing Article:  C10-2167.txt | Citation Marker Offset:  ['65'] | Citation Marker:  Chieu et al., 2002 | Citation Offset:  ['65'] | Citation Text:  <S sid ="65" ssid = "25">In statistical methods, the most popular models are Hidden Markov Models (HMM) (Rabiner, 1989), Maximum Entropy Models (ME) (Chieu et al., 2002) and Conditional Random Fields (CRF) (Lafferty et al., 2001).</S> | Reference Offset:  ['4'] | Reference Text:  <S sid ="4" ssid = "4">In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data.</S> | Discourse Facet:  Results_Citation | Annotator:  Aakansha Gehlot |


Citance Number: 3 | Reference Article:  C02-1025.txt | Citing Article:  I05-3013.txt | Citation Marker Offset:  ['88'] | Citation Marker:  Chieu and Ng, 2002 | Citation Offset:  ['88'] | Citation Text:  <S sid ="88" ssid = "6">The latter is currently dominating in NER amongst which the most popular methods are decision tree (Sekine et al., 1998; Pailouras et al., 2000), Hidden Markov Model (Zhang et al., 2003; Zhao, 2004), maximum entropy (Chieu and Ng, 2002; Bender et al., 2003), and support vector machines (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield, 2003).From the linguistic perspective, NIL expres sions are rather different from named entities in nature.</S> | Reference Offset:  ['4'] | Reference Text:  <S sid ="4" ssid = "4">In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data.</S> | Discourse Facet:  Results_Citation | Annotator:  Aakansha Gehlot |


Citance Number: 4 | Reference Article:  C02-1025.txt | Citing Article:  I05-3030.txt | Citation Marker Offset:  ['33'] | Citation Marker:  Chieu 2002 | Citation Offset:  ['33'] | Citation Text:  <S sid ="33" ssid = "33">modelâ€™s conditional probability is defined as Another model is Maximum Entropy (Zhao Jian 2005, Hai Leong Chieu 2002).</S> | Reference Offset:  ['196'] | Reference Text: <S sid ="196" ssid = "1">We have shown that the maximum entropy framework is able to use global information directly.</S> | Discourse Facet: Results_Citation | Annotator:  Aakansha Gehlot |


Citance Number: 5 | Reference Article:  C02-1025.txt | Citing Article:  I08-2080.txt | Citation Marker Offset:  ['12'] | Citation Marker:  Chieu and Ng, 2002 | Citation Offset:  ['12'] | Citation Text:  <S sid ="12" ssid = "12">In (Malouf, 2002; Chieu and Ng, 2002), information about features assigned to other instances of the same token is utilized.</S> | Reference Offset:  ['68','69'] | Reference Text:  <S sid ="68" ssid = "8">In the maximum entropy framework, there is no such constraint.</S><S sid ="69" ssid = "9">Multiple features can be used for the same token.</S> | Discourse Facet:  Method_Citation | Annotator:  Aakansha Gehlot |


Citance Number: 6 | Reference Article:  C02-1025.txt | Citing Article:  P02-1061.txt | Citation Marker Offset:  ['51'] | Citation Marker:  Chieu and Ng, 2002 | Citation Offset:  ['51'] | Citation Text:  <S sid ="51" ssid = "4">More details of this mixed case NER and its performance are given in (Chieu and Ng, 2002).</S> | Reference Offset:  ['86'] | Reference Text:  <S sid ="86" ssid = "26">Case and Zone of and : Similarly, if (or ) is initCaps, a feature (initCaps, zone) (or (initCaps, zone) ) is set to 1, etc. Token Information: This group consists of 10 features based on the string , as listed in Table 1.</S> | Discourse Facet:  Method_Citation | Annotator:  Aakansha Gehlot |


Citance Number: 7 | Reference Article:  C02-1025.txt | Citing Article:  P03-1028.txt | Citation Marker Offset:  ['161'] | Citation Marker:  Chieu and Ng, 2002a | Citation Offset:  ['161'] | Citation Text:  <S sid ="161" ssid = "86">They are automatically derived based on the correlation metric value used in (Chieu and Ng, 2002a).</S> | Reference Offset:  ['6'] | Reference Text:  <S sid ="6" ssid = "6">A named entity recognizer (NER) is useful in many NLP applications such as information extraction, question answering, etc. On its own, a NER can also provide users who are looking for person or organization names with quick information.</S> | Discourse Facet:  Implication_Citation | Annotator:  Aakansha Gehlot |


Citance Number: 8 | Reference Article:  C02-1025.txt | Citing Article:  P03-1028.txt | Citation Marker Offset:  ['13'] | Citation Marker:  Chieu and Ng, 2002a | Citation Offset:  ['13'] | Citation Text:  <S sid ="13" ssid = "13">Several benchmark data sets have been used to evaluate IE approaches on semi- structured texts (Soderland, 1999; Ciravegna, 2001; Chieu and Ng, 2002a).</S> | Reference Offset:  ['9'] | Reference Text:  <S sid ="9" ssid = "9">We propose maximizing , where is the sequence of named- entity tags assigned to the words in the sentence , and is the information that can be extracted from the whole document containing . Our system is built on a maximum entropy classifier.</S> | Discourse Facet:  Method_Citation | Annotator:  Aakansha Gehlot |


Citance Number: 9 | Reference Article:  C02-1025.txt | Citing Article:  P03-1028.txt | Citation Marker Offset:  ['46'] | Citation Marker:  Chieu and Ng, 2002b | Citation Offset:  ['46'] | Citation Text:  <S sid ="46" ssid = "23">More recently, machine learning approaches have been used for IE from semi-structured texts (Califf and Mooney, 1999; Soderland, 1999; Roth and Yih, 2001; Ciravegna, 2001; Chieu and Ng, 2002a), named entity extraction (Chieu and Ng, 2002b), template element extraction, and template relation extraction (Miller et al., 1998).</S> | Reference Offset:  ['48'] | Reference Text:  <S sid ="48" ssid = "7">Such constraints are derived from training data, expressing some relationship between features and outcome.</S> | Discourse Facet:  Method_Citation | Annotator:  Aakansha Gehlot |


Citance Number: 10 | Reference Article:  C02-1025.txt | Citing Article:  P03-1028.txt | Citation Marker Offset:  ['52'] | Citation Marker:  2002a | Citation Offset:  ['52'] | Citation Text:  <S sid ="52" ssid = "29">Soder- land (1999) and Chieu and Ng (2002a) attempted machine learning approaches for a scaled-down version of the ST task, where it was assumed that the information needed to fill one template came from one sentence only.</S> | Reference Offset:  ['2','3'] | Reference Text:  <S sid ="2" ssid = "2">It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier.</S><S sid ="3" ssid = "3">Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentence- based classifier.</S> | Discourse Facet:  Method_Citation | Annotator:  Aakansha Gehlot |


Citance Number: 11 | Reference Article:  C02-1025.txt | Citing Article:  P03-1028.txt | Citation Marker Offset:  ['54'] | Citation Marker:  Chieu and Ng, 2002a | Citation Offset:  ['54'] | Citation Text:  <S sid ="54" ssid = "31">The task we tackle is considerably more complex than that of (Soderland, 1999; Chieu and Ng, 2002a), since we need to deal with merging information from multiple sentences to fill one template.</S> | Reference Offset:  ['9'] | Reference Text: <S sid ="9" ssid = "9">We propose maximizing , where is the sequence of named- entity tags assigned to the words in the sentence , and is the information that can be extracted from the whole document containing . Our system is built on a maximum entropy classifier.</S> | Discourse Facet: Method_Citation | Annotator:  Aakansha Gehlot |


Citance Number: 12 | Reference Article:  C02-1025.txt | Citing Article:  P05-1045.txt | Citation Marker Offset:  ['174'] | Citation Marker:  2002 | Citation Offset:  ['174'] | Citation Text:  <S sid ="174" ssid = "9">Chieu and Ng (2002) propose a solution to this problem: for each token, they define additional features taken from other occurrences of the same token in the document.</S> | Reference Offset:  ['63'] | Reference Text:  <S sid ="63" ssid = "3">Global features are extracted from other occurrences of the same token in the whole document.</S> | Discourse Facet:  Method_Citation | Annotator:  Aakansha Gehlot |


Citance Number: 13 | Reference Article:  C02-1025.txt | Citing Article:  P05-1051.txt | Citation Marker Offset:  ['27'] | Citation Marker:  Chieu and Ng, 2002 | Citation Offset:  ['27'] | Citation Text:  <S sid ="27" ssid = "4">(Borthwick, 1999) made a second tagging pass which uses information on token sequences tagged in the first pass; (Chieu and Ng, 2002) used as features information about features assigned to other instances of the same token.Recently, in (Ji and Grishman, 2004) we pro posed a name tagging method which applied an SVM based on coreference information to filter the names with low confidence, and used coreference rules to correct and recover some names.</S> | Reference Offset:  ['63'] | Reference Text:  <S sid ="63" ssid = "3">Global features are extracted from other occurrences of the same token in the whole document.</S> | Discourse Facet:  Method_Citation | Annotator:  Aakansha Gehlot |


Citance Number: 14 | Reference Article:  C02-1025.txt | Citing Article:  P06-1141.txt | Citation Marker Offset:  ['119'] | Citation Marker:  Chieu and Ng, 2002 | Citation Offset:  ['119'] | Citation Text:  <S sid ="119" ssid = "14">â€¢ Most work has looked to model non-local dependencies only within a document (Finkel et al., 2005; Chieu and Ng, 2002; Sutton and McCallum, 2004; Bunescu and Mooney, 2004).</S> | Reference Offset:  ['9'] | Reference Text:  <S sid ="9" ssid = "9">We propose maximizing , where is the sequence of named- entity tags assigned to the words in the sentence , and is the information that can be extracted from the whole document containing . Our system is built on a maximum entropy classifier.</S> | Discourse Facet:  Method_Citation | Annotator:  Aakansha Gehlot |


Citance Number: 15 | Reference Article:  C02-1025.txt | Citing Article:  W03-0423.txt | Citation Marker Offset:  ['12'] | Citation Marker:  Chieu and Ng, 2002b | Citation Offset:  ['12'] | Citation Text:  <S sid ="12" ssid = "12">Such global features enhance the performance of NER (Chieu and Ng, 2002b).</S> | Reference Offset:  ['4'] | Reference Text:  <S sid ="4" ssid = "4">In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data.</S> | Discourse Facet:  Results_Citation | Annotator:  Aakansha Gehlot |


Citance Number: 16 | Reference Article:  C02-1025.txt | Citing Article:  W03-0423.txt | Citation Marker Offset:  ['32'] | Citation Marker:  Chieu and Ng, 2002a | Citation Offset:  ['32'] | Citation Text:  <S sid ="32" ssid = "8">Useful Unigrams (UNI) For each name class, words that precede the name class are ranked using correlation metric (Chieu and Ng, 2002a), and the top 20 are compiled into a list.</S> | Reference Offset:  ['102','103','104'] | Reference Text: <S sid ="102" ssid = "42">For all lists except locations, the lists are processed into a list of tokens (unigrams).</S><S sid ="103" ssid = "43">Location list is processed into a list of unigrams and bigrams (e.g., New York).</S><S sid ="104" ssid = "44">For locations, tokens are matched against unigrams, and sequences of two consecutive tokens are matched against bigrams.</S> | Discourse Facet:  Method_Citation | Annotator:  Aakansha Gehlot |


Citance Number: 17 | Reference Article:  C02-1025.txt | Citing Article:  W03-0423.txt | Citation Marker Offset:  ['44'] | Citation Marker:  Chieu and Ng, 2002b | Citation Offset:  ['44'] | Citation Text:  <S sid ="44" ssid = "20">The basic features used by both ME1 and ME2 can be divided into two classes: local and global (Chieu and Ng, 2002b).</S> | Reference Offset:  ['61','63'] | Reference Text:  <S sid ="61" ssid = "1">The features we used can be divided into 2 classes: local and global.</S><S sid ="62" ssid = "2">Local features are features that are based on neighboring tokens, as well as the token itself.</S><S sid ="63" ssid = "3">Global features are extracted from other occurrences of the same token in the whole document.</S> | Discourse Facet:  Method_Citation | Annotator:  Aakansha Gehlot |


Citance Number: 18 | Reference Article:  C02-1025.txt | Citing Article:  W03-0423.txt | Citation Marker Offset:  ['62'] | Citation Marker:  Chieu and Ng, 2002b | Citation Offset:  ['62'] | Citation Text:  <S sid ="62" ssid = "38">Token Information These features are based on the string w, such as contains-digits, contains-dollar-sign, etc (Chieu and Ng, 2002b).</S> | Reference Offset:  ['86'] | Reference Text:  <S sid ="86" ssid = "26">Case and Zone of and : Similarly, if (or ) is initCaps, a feature (initCaps, zone) (or (initCaps, zone) ) is set to 1, etc. Token Information: This group consists of 10 features based on the string , as listed in Table 1.</S> | Discourse Facet:  Method_Citation | Annotator:  Aakansha Gehlot |


Citance Number: 19 | Reference Article:  C02-1025.txt | Citing Article:  W03-0423.txt | Citation Marker Offset:  ['46'] | Citation Marker:  Chieu and Ng, 2002b | Citation Offset:  ['46'] | Citation Text:  <S sid ="46" ssid = "22">In this paper, w−i refers to the ith word before w, and w+i refers to the ith word after w. The features used are similar to those used in (Chieu and Ng, 2002b).</S> | Reference Offset:  ['61','62','63'] | Reference Text:  <S sid ="61" ssid = "1">The features we used can be divided into 2 classes: local and global.</S><S sid ="62" ssid = "2">Local features are features that are based on neighboring tokens, as well as the token itself.</S><S sid ="63" ssid = "3">Global features are extracted from other occurrences of the same token in the whole document.</S> | Discourse Facet:  Method_Citation | Annotator:  Aakansha Gehlot |


Citance Number: 20 | Reference Article:  C02-1025.txt | Citing Article:  W03-0432.txt | Citation Marker Offset:  ['44'] | Citation Marker:  Chieu and Ng, 2002a | Citation Offset:  ['44'] | Citation Text:  <S sid ="44" ssid = "7">Chieu and Ng used global information such as the occurrence of the same word with other capitalisation in the same document (Chieu and Ng, 2002a), and have also used a mixed-case classifier to teach a â€œweakerâ€ classifier that did not use case information at all (Chieu and Ng, 2002b).</S> | Reference Offset:  ['136','137'] | Reference Text:  <S sid ="136" ssid = "76">The global feature groups are: InitCaps of Other Occurrences (ICOC): There are 2 features in this group, checking for whether the first occurrence of the same word in an unambiguous position (non first-words in the TXT or TEXT zones) in the same document is initCaps or not-initCaps.</S><S sid ="137" ssid = "77">For a word whose initCaps might be due to its position rather than its meaning (in headlines, first word of a sentence, etc), the case information of other occurrences might be more accurate than its own.</S> | Discourse Facet:  Method_Citation | Annotator:  Aakansha Gehlot |


Citance Number: 21 | Reference Article:  C02-1025.txt | Citing Article:  W04-0705.txt | Citation Marker Offset:  ['8'] | Citation Marker:  Chieu and Ng 2002 | Citation Offset:  ['8'] | Citation Text:  <S sid ="8" ssid = "8">AMaximum Entropy methods (Borthwick et al. 1998, Chieu and Ng 2002)</S> | Reference Offset:  ['11'] | Reference Text:  <S sid ="11" ssid = "11">We will refer to our system as MENERGI (Maximum Entropy Named Entity Recognizer using Global Information).</S> | Discourse Facet:  Method_Citation | Annotator:  Aakansha Gehlot |


Citance Number: 22 | Reference Article:  C02-1025.txt | Citing Article:  W04-0705.txt | Citation Marker Offset:  ['147'] | Citation Marker:  Chieu and Ng 2002 | Citation Offset:  ['147'] | Citation Text:  <S sid ="147" ssid = "30">Other systems have made a second tagging pass which uses information on token sequences tagged in the first pass (Borthwick 1999), or have used as features information about features assigned to other instances of the same token (Chieu and Ng 2002).</S> | Reference Offset:  ['63'] | Reference Text:  <S sid ="63" ssid = "3">Global features are extracted from other occurrences of the same token in the whole document.</S> | Discourse Facet:  Method_Citation | Annotator:  Aakansha Gehlot |


Citance Number: 23 | Reference Article:  C02-1025.txt | Citing Article:  W06-0119.txt | Citation Marker Offset:  ['11'] | Citation Marker:  Chieu et al. 2002 | Citation Offset:  ['11'] | Citation Text:  <S sid ="11" ssid = "11">To develop UWI, there are three approaches: (1) Statistical approach, researchers use common statistical features, such as maximum entropy (Chieu et al. 2002), association strength, mutual information, ambiguous matching, and multi-statistical features for unknown word detection and extraction;</S> | Reference Offset:  ['4'] | Reference Text:  <S sid ="4" ssid = "4">In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data.</S> | Discourse Facet:  Results_Citation | Annotator:  Aakansha Gehlot |
