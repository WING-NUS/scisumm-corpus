<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">This paper describes a series of experiments to test the hypothesis that the parallel application of multiple NLP tools and the integration of their results improves the correctness and robustness of the resulting analysis.</S>
		<S sid ="2" ssid = "2">It is shown how annotations created by seven NLP tools are mapped onto tool- independent descriptions that are defined with reference to an ontology of linguistic annotations, and how a majority vote and ontological consistency constraints can be used to integrate multiple alternative analyses of the same token in a consistent way.</S>
		<S sid ="3" ssid = "3">For morphosyntactic (parts of speech) and morphological annotations of three German corpora, the resulting merged sets of ontological descriptions are evaluated in comparison to (ontological representation of) existing reference annotations.</S>
	</ABSTRACT>
	<SECTION title="overview" number = "1">
			<S sid ="4" ssid = "4">NLP systems for higher-level operations or complex annotations often integrate redundant modules that provide alternative analyses for the same linguistic phenomenon in order to benefit from their respective strengths and to compensate for their respective weaknesses, e.g., in parsing (Crysmann et al., 2002), or in machine translation (Carl et al., 2000).</S>
			<S sid ="5" ssid = "5">The current trend to parallel and distributed NLP architectures (Aschenbrenner et al., 2006; Gietz et al., 2006; Egner et al., 2007; Lu´ıs and de Matos, 2009) opens the possibility of exploring the potential of redundant parallel annotations also for lower levels of linguistic analysis.</S>
			<S sid ="6" ssid = "6">This paper evaluates the potential benefits of such an approach with respect to morphosyntax (parts of speech, pos) and morphology in German: In comparison to English, German shows a rich and polysemous morphology, and a considerable number of NLP tools are available, making it a promising candidate for such an experiment.</S>
			<S sid ="7" ssid = "7">Previous research indicates that the integration of multiple part of speech taggers leads to more accurate analyses.</S>
			<S sid ="8" ssid = "8">So far, however, this line of research focused on tools that were trained on the same corpus (Brill and Wu, 1998; Halteren et al., 2001), or that specialize to different subsets of the same tagset (Zavrel and Daelemans, 2000; Tufis¸, 2000; Borin, 2000).</S>
			<S sid ="9" ssid = "9">An even more substantial increase in accuracy and detail can be expected if tools are combined that make use of different annotation schemes.</S>
			<S sid ="10" ssid = "10">For this task, ontologies of linguistic annotations are employed to assess the linguistic information conveyed in a particular annotation and to integrate the resulting ontological descriptions in a consistent and tool-independent way.</S>
			<S sid ="11" ssid = "11">The merged set of ontological descriptions is then evaluated with reference to morphosyntactic and morphological annotations of three corpora of German newspaper articles, the NEGRA corpus (Skut et al., 1998), the TIGER corpus (Brants et al., 2002) and the Potsdam Commentary Corpus (Stede, 2004, PCC).</S>
	</SECTION>
	<SECTION title="Ontologies and annotations. " number = "2">
			<S sid ="12" ssid = "1">Various repositories of linguistic annotation terminology have been developed in the last decades, ranging from early texts on annotation standards (Bakker et al., 1993; Leech and Wilson, 1996) over relational data base models (Bickel and Nichols, 2000; Bickel and Nichols, 2002) to more recent formalizations in OWL/RDF (or with OWL/RDF export), e.g., the General Ontology of Linguistic Description (Farrar and Langendoen, 2003, GOLD), the ISO TC37/SC4 Data Category Registry (Ide and Romary, 2004; Kemps 659 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 659–670, Uppsala, Sweden, 1116 July 2010.</S>
			<S sid ="13" ssid = "2">Qc 2010 Association for Computational Linguistics Snijders et al., 2009, DCR), the OntoTag ontology (Aguado de Cea et al., 2002), or the Typological Database System ontology (Saulwick et al., 2005, TDS).</S>
			<S sid ="14" ssid = "3">Despite their common level of representation, however, these efforts have not yet converged into a unified and generally accepted ontology of linguistic annotation terminology, but rather, different resources are maintained by different communities, so that a considerable amount of disagreement between them and their respective definitions can be observed.1 Such conceptual mismatches and incompatibilities between existing terminological repositories have been the motivation to develop the OLiA architecture (Chiarcos, 2008) that employs a shallow Reference Model to mediate between (onto- logical models of) annotation schemes and several existing terminology repositories, incl.</S>
			<S sid ="15" ssid = "4">GOLD, the DCR, and OntoTag.</S>
			<S sid ="16" ssid = "5">When an annotation receives a representation in the OLiA Reference Model, it is thus also interpretable with respect to other linguistic ontologies.</S>
			<S sid ="17" ssid = "6">Therefore, the findings for the OLiA Reference Model in the experiments described below entail similar results for an application of GOLD or the DCR to the same task.</S>
			<S sid ="18" ssid = "7">2.1 The OLiA ontologies.</S>
			<S sid ="19" ssid = "8">The Ontologies of Linguistic Annotations – briefly, OLiA ontologies (Chiarcos, 2008) – represent an architecture of modular OWL/DL on- tologies that formalize several intermediate steps of the mapping between concrete annotations, a Reference Model and existing terminology repositories (‘External Reference Models’ in OLiA terminology) such as the DCR.2 The OLiA ontologies were originally developed as part of an infrastructure for the sustainable maintenance of linguistic resources (Schmidt et al., 2006) where they were originally applied 1 As one example, a GOLD Numeral is a Determiner (Numeral C: Quantifier C: Determiner, http://linguistics-ontology.org/gold/2008/ Numeral), whereas a DCR Numeral is defined on the basis of its semantic function, without any references to syntactic categories (http://www.isocat.org/datcat/DC1334).</S>
			<S sid ="20" ssid = "9">Thus, two in two of them is a DCR Numeral but not a GOLD Numeral.</S>
			<S sid ="21" ssid = "10">2 The OLiA Reference Model is accessible via http://nachhalt.sfb632.unipotsdam.de/owl/ olia.owl.</S>
			<S sid ="22" ssid = "11">Several annotation models, e.g., stts.owl, tiger.owl, connexor.owl, morphisto.owl can be found in the same directory together with the corresponding linking files stts-link.rdf, tiger-link.rdf, connexor-link.rdf and morphisto-link.rdf.</S>
			<S sid ="23" ssid = "12">to the formal representation and documentation of annotation schemes, and for concept-based annotation queries over to multiple, heterogeneous corpora annotated with different annotation schemes (Rehm et al., 2007; Chiarcos et al., 2008).</S>
			<S sid ="24" ssid = "13">NLP applications of the OLiA ontologies include a proposal to integrate them with the OntoTag ontologies and to use them for interface specifications between modules in NLP pipeline architectures (Buyko et al., 2008).</S>
			<S sid ="25" ssid = "14">Further, Hellmann (2010) described the application of the OLiA ontologies within NLP2RDF, an OWL-based blackboard approach to assess the meaning of text from grammatical analyses and subsequent enrichment with ontological knowledge sources.</S>
			<S sid ="26" ssid = "15">OLiA distinguishes three different classes of ontologies: • The OLIA REFERENCE MODEL specifies the common terminology that different annotation schemes can refer to.</S>
			<S sid ="27" ssid = "16">It is primarily based on a blend of concepts of EAGLES and GOLD, and further extended in accordance with different annotation schemes, with the TDS ontology and with the DCR (Chiarcos, 2010).</S>
			<S sid ="28" ssid = "17">• Multiple OLIA ANNOTATION MODELs formalize annotation schemes and tag sets.</S>
			<S sid ="29" ssid = "18">Annotation Models are based on the original documentation and data samples, so that they provide an authentic representation of the annotation not biased with respect to any particular interpretation.</S>
			<S sid ="30" ssid = "19">• For every Annotation Model, a LINKINGMODEL defines subClassOf (�) relation ships between concepts/properties in the respective Annotation Model and the Reference Model.</S>
			<S sid ="31" ssid = "20">Linking Models are interpretations of Annotation Model concepts and properties in terms of the Reference Model, and thus multiple alternative Linking Models for the same Annotation Model are possible.</S>
			<S sid ="32" ssid = "21">Other Linking Models specify � re lationships between Reference Model concepts/properties and concepts/properties of an External Reference Model such as GOLD or the DCR.</S>
			<S sid ="33" ssid = "22">The OLiA Reference Model (namespace olia) specifies concepts that describe linguistic categories (e.g., olia:Determiner) and grammatical features (e.g., olia:Accusative), as well Figure 1: Attributive demonstrative pronouns (PDAT) in the STTS Annotation Model Figure 3: Individuals for accusative and singular in the TIGER Annotation Model Figure 2: Selected morphosyntactic categories in the OLiA Reference Model Figure 4: Selected morphological features in the OLiA Reference Model as properties that define possible relations between those (e.g., olia:hasCase).</S>
			<S sid ="34" ssid = "23">More general concepts that represent organizational information rather than possible annotations (e.g., MorphosyntacticCategory and CaseFeature) are stored in a separate ontology (namespace olia top).</S>
			<S sid ="35" ssid = "24">The Reference Model is a shallow ontology: It does not specify disjointness conditions of concepts and cardinality or domain restrictions of properties.</S>
			<S sid ="36" ssid = "25">Instead, it assumes that such con straints are inherited by means of � relationships from an External Reference Model.</S>
			<S sid ="37" ssid = "26">Different External Reference Models may take different positions on the issue – as languages do3 –, so that this aspect is left underspecified in the Reference Model.</S>
			<S sid ="38" ssid = "27">3 Based on primary experience with Western European languages, for example, one might assume that a hasGender property applies to nouns, adjectives, pronouns and determiners only.</S>
			<S sid ="39" ssid = "28">Yet, this is language-specific restriction: Russian finite verbs, for example, show gender congruency in past tense.</S>
			<S sid ="40" ssid = "29">Figs.</S>
			<S sid ="41" ssid = "30">2 and 4 show excerpts of category and feature hierarchies in the Reference Model.</S>
			<S sid ="42" ssid = "31">With respect to morphosyntactic annotations (parts of speech, pos) and morphological annotations (morph), five Annotation Models for German are currently available: STTS (Schiller et al., 1999, pos), TIGER (Brants and Hansen, 2002, morph), Morphisto (Zielinski and Simon, 2008, pos, morph), RFTagger (Schmid and Laws, 2008, pos, morph), Connexor (Tapanainen and Ja¨rvinen, 1997, pos, morph).</S>
			<S sid ="43" ssid = "32">Further Annotation Models for pos and morph cover five different annotation schemes for English (Marcus et al., 1994; Sampson, 1995; Mandel, 2006; Kim et al., 2003, Connexor), two annotation schemes for Russian (Meyer, 2003; Sharoff et al., 2008), an annotation scheme designed for typological research and currently applied to approx.</S>
			<S sid ="44" ssid = "33">30 different languages (Dipper et al., 2007), an annotation scheme for Old High German (Petrova et al., 2009), and an annotation scheme for Tibetan (Wagner and Zeisler, 2004).</S>
			<S sid ="45" ssid = "34">or the DCR).</S>
			<S sid ="46" ssid = "35">As an example, consider the attributive demonstrative pronoun diese in (1).</S>
			<S sid ="47" ssid = "36">Di es e ni ch t n e u e Er ke n nt ni s ko nn te (1) thi s n ot ne w in si g ht co ul d der the Markt market der of.the Mo¨ glichkeiten possibilities am on.the Sonnabend in Saturday in Treuenbrietzen Treuenbrietzen bestens in.the.best.way unterstreichen . underline Figure 5: The STTS tags PDAT and ART, their representation in the Annotation Model and linking with the Reference Model.</S>
			<S sid ="48" ssid = "37">Annotation Models differ from the Reference Model mostly in that they include not only concepts and properties, but also individuals: Annotation Model concepts reflect an abstract conceptual categorization, whereas individuals represent concrete values used to annotate the corresponding phenomenon.</S>
			<S sid ="49" ssid = "38">An individual is applicable to all annotations that match the string value specified by this individual’s hasTag, hasTagContaining, hasTagStartingWith, or hasTagEndingWith properties.</S>
			<S sid ="50" ssid = "39">Fig.</S>
			<S sid ="51" ssid = "40">1 illustrates the structure of the STTS Annotation Model (namespace stts) for the individual stts:PDAT that represents the tag used for attributive demonstrative pronouns (demonstrative determiners).</S>
			<S sid ="52" ssid = "41">Fig.</S>
			<S sid ="53" ssid = "42">3 illustrates the individuals tiger:accusative and tiger:singular from the hierarchy of morphological features in the TIGER Annotation Model (namespace tiger).</S>
			<S sid ="54" ssid = "43">Fig.</S>
			<S sid ="55" ssid = "44">5 illustrates the linking between the STTS Annotation Model and the OLiA Reference Model for the individuals stts:PDAT and stts:ART.</S>
			<S sid ="56" ssid = "45">2.2 Integrating different morphosyntactic.</S>
			<S sid ="57" ssid = "46">and morphological analyses With the OLiA ontologies as described above, annotations from different annotation schemes can now be interpreted in terms of the OLiA Reference Model (or External Reference Models like GOLD ‘The ‘Market of Possibilities’, held this Saturday in Treuenbrietzen, provided best evidence for this well known (lit.</S>
			<S sid ="58" ssid = "47">‘not new’) insight.’ (PCC, #4794) The phrase diese nicht neue Erkenntnis poses two challenges.</S>
			<S sid ="59" ssid = "48">First, it has to be recognized that the demonstrative pronoun is attributive, although it is separated from adjective and noun by nicht ‘not’.</S>
			<S sid ="60" ssid = "49">Second, the phrase is in accusative case, although the morphology is ambiguous between accusative and nominative, and nominative case would be expected for a sentence-initial NP.</S>
			<S sid ="61" ssid = "50">The Connexor analysis (Tapanainen and Ja¨rvinen, 1997) actually fails in both aspects (2).</S>
			<S sid ="62" ssid = "51">(2) PRON Dem FEM SG NOM (Connexor) The ontological analysis of this annotation begins by identifying the set of individuals from the Con- nexor Annotation Model that match it according to their hasTag (etc.) properties.</S>
			<S sid ="63" ssid = "52">The RDF triplet connexor:NOM connexor:hasTagContaining ‘NOM’4 indicates that the tag is an application of the individual connexor:NOM, an instance of connexor:Case.</S>
			<S sid ="64" ssid = "53">Further, the annotation matches connexor:PRON (an instance of connexor:Pronoun), etc. The result is a set of individuals that express different aspects of the meaning of the annotation.</S>
			<S sid ="65" ssid = "54">For these individuals, the Annotation Model specifies superclasses (rdf:type) and other properties, i.e., connexor:NOM connexor:hasCase connexor:NOM, etc. The linguistic unit represented by the actual token can now be characterized by these properties: Every property applicable to a member in the individual set is assumed to be applicable to the linguistic unit as well.</S>
			<S sid ="66" ssid = "55">In order to save space, we use a notation closer to predicate logic (with the token as implicit subject).</S>
			<S sid ="67" ssid = "56">In terms of the Annotation Model, the token diese is thus described by the following descriptions: 4 RDF triplets are quoted in simplified form, with XML.</S>
			<S sid ="68" ssid = "57">namespaces replacing the actual URIs.</S>
			<S sid ="69" ssid = "58">(3) rdf:type(connexor:Pronoun) connexor:hasCase(connexor:NOM) ...</S>
			<S sid ="70" ssid = "59">The Linking Model connexor-link.rdf provides us with the information that (i) connexor:Pronoun is a subclass of the Reference Model concept olia:Pronoun, (ii) connexor:NOM is an instance of the Reference Model concept olia:Nominative, and (iii) olia:hasCase is a subproperty of olia:hasCase.</S>
			<S sid ="71" ssid = "60">Accordingly, the predicates that describe the token diese can be reformulated in terms of the Reference Model.</S>
			<S sid ="72" ssid = "61">rdf:type(connexor:Pronoun) entails rdf:type(olia:Pronoun), etc. Similarly, we know that for some i:olia:Nominative it is true that olia:hasCase(i), abbreviated here as olia:hasCase(some olia:Nominative).</S>
			<S sid ="73" ssid = "62">In this way, the grammatical information conveyed in the original Connexor annotation can be represented in an annotation-independent and tagset-neutral way as shown for the Connexor analysis in (4).</S>
			<S sid ="74" ssid = "63">(4) rdf:type(olia:PronounOrDeterminer) rdf:type(olia:Pronoun) olia:hasNumber(some olia:Singular) olia:hasGender(some olia:Feminine) rdf:type(olia:DemonstrativePronoun) olia:hasCase(some olia:Nominative) Analogously, the corresponding RFTagger analysis (Schmid and Laws, 2008) given in (5) can be transformed into a description in terms of the OLiA Reference Model such as in (6).</S>
			<S sid ="75" ssid = "64">(5) PRO.Dem.Attr.-3.Acc.Sg.Fem (RFTagger) (6) rdf:type(olia:PronounOrDeterminer) olia:hasNumber(some olia:Singular) olia:hasGender(some olia:Feminine) olia:hasCase(some olia:Accusative) rdf:type(olia:DemonstrativeDeterminer) rdf:type(olia:Determiner) For every description obtained from these (and further) analyses, an integrated and consistent generalization can be established as described in the following section.</S>
	</SECTION>
	<SECTION title="Processing linguistic annotations. " number = "3">
			<S sid ="76" ssid = "1">3.1 Evaluation setup.</S>
			<S sid ="77" ssid = "2">Fig.</S>
			<S sid ="78" ssid = "3">6 sketches the architecture of the evaluation environment set up for this study.5 The input to the system is a set of documents with 5 The code used for the evaluation setup is available under.</S>
			<S sid ="79" ssid = "4">http://multiparse.sourceforge.net.</S>
			<S sid ="80" ssid = "5">Figure 6: Evaluation setup TIGER/NEGRA-style morphosyntactic or morphological annotation (Skut et al., 1998; Brants and Hansen, 2002) whose annotations are used as gold standard.</S>
			<S sid ="81" ssid = "6">From the annotated document, the plain tokenized text is extracted and analyzed by one or more of the following NLP tools: (i) Morphisto, a morphological analyzer without contextual disambiguation (Zielinski and Simon, 2008), (ii) two part of speech taggers: the TreeTagger (Schmid, 1994) and the Stanford Tagger (Toutanova et al., 2003), (iii) the RFTagger that performs part of speech and morphological analysis (Schmid and Laws, 2008), (iv) two PCFG parsers: the StanfordParser (Klein and Manning, 2003) and the BerkeleyParser (Petrov and Klein, 2007), and (v) the Connexor dependency parser (Tapanainen and Ja¨rvinen, 1997).</S>
			<S sid ="82" ssid = "7">These tools annotate parts of speech, and those in (i), (iii) and (v) also provide morphological features.</S>
			<S sid ="83" ssid = "8">All components ran in parallel threads on the same machine, with the exception of Morphisto that was addressed as a web service.</S>
			<S sid ="84" ssid = "9">The set of matching Annotation Model individuals for every annotation and the respective set of Reference Model descriptions are determined by means of OLiA description Morphisto Connexor RF Tree Stanford Stanford Berkeley T a g g e r Ta gg er Ta gg er Pa rs er P a r s e r wor d cla ss ty pe (...</S>
			<S sid ="85" ssid = "10">) Pro no un Or Det er min er 7 1( 4/ 4) ∗ 1 1 1 1 1 1 De ter mi ner 5 . 5 0 . 5 ∗ ∗ 0 1 1 1 1 1 De mo nst rati ve De ter mi ner 5 . 5 0 . 5 ∗ ∗ 0 1 1 1 1 1 Pro nou n 1 . 5 0 . 5 ∗ ∗ 1 0 0 0 0 0 De mo nst rati ve Pro no un 1 . 5 0 . 5 ∗ ∗ 1 0 0 0 0 0 mo rph olo gy ha sX Y(.</S>
			<S sid ="86" ssid = "11">n / a n / a n / a n / a hasNum ber(som e Singul ar) 2.5 0.5 (2/ 4) 1 1 ∗ Morphisto produces four alternative candidate analyses hasGen der(som e Feminin e) 2.5 0.5 (2/ 4) 1 1 for this example, so every alternative analysis receives the hasCase (some Accusa tive) 1.5 0.5 (2/ 4) 0 1 confidence score 0.25 hasCase (some Nomina tive) 1.5 0.5 (2/ 4) 1 0 ∗∗ Morphisto does not distinguish attributive and substitutive hasNum ber(som e Plural) 0.5 0.5 (2/ 4) 0 0 pronouns, it predicts type(Determiner U Pronoun) Table 1: Confidence scores for diese in ex.</S>
			<S sid ="87" ssid = "12">(1) the Pellet reasoner (Sirin et al., 2007) as described above.</S>
			<S sid ="88" ssid = "13">A disambiguation routine (see below) then determines the maximal consistent set of ontological descriptions.</S>
			<S sid ="89" ssid = "14">Finally, the outcome of this process is compared to the set of descriptions corresponding to the original annotation in the corpus.</S>
			<S sid ="90" ssid = "15">3.2 Disambiguation.</S>
			<S sid ="91" ssid = "16">Returning to examples (4) and (6) above, we see that the resulting set of descriptions conveys properties that are obviously contradicting, e.g., hasCase(some Nominative) besides hasCase(some Accusative).</S>
			<S sid ="92" ssid = "17">Our approach to disambiguation combines on- tological consistency criteria with a confidence ranking.</S>
			<S sid ="93" ssid = "18">As we simulate an uninformed approach, the confidence ranking follows a majority vote.</S>
			<S sid ="94" ssid = "19">For diese in (1), the consultation of all seven tools results a confidence ranking as shown in Tab.</S>
			<S sid ="95" ssid = "20">1: If a tool supports a description with its analysis, the confidence score is increased by 1 (or by 1/n if the tool proposes n alternative annotations).</S>
			<S sid ="96" ssid = "21">A maximal consistent set of descriptions is then established as follows: (i) Given a confidence-ranked list of available descriptions S = (s1, ..., sn) and a result set T = ∅.</S>
			<S sid ="97" ssid = "22">(ii) Let s1 be the first element of S = (s1, ..., sn).</S>
			<S sid ="98" ssid = "23">The consistency of ontological descriptions is defined here as follows:6 • Two concepts A and B are consistent iff A ≡ B or A � B or B � A Otherwise, A and B are disjoint.</S>
			<S sid ="99" ssid = "24">• Two descriptions pred1(A) and pred2(B) are consistent iff A and B are consistent or pred1 is neither a subproperty nor a superproperty of pred2 This heuristic formalizes an implicit disjoint- ness assumption for all concepts in the on- tology (all concepts are disjoint unless one is a subconcept of the other).</S>
			<S sid ="100" ssid = "25">Further, it imposes an implicit cardinality constraint on properties (e.g., hasCase(some Accusative) and hasCase(some Nominative) are inconsistent because Accusative and Nominative are sibling concepts and thus disjoint).</S>
			<S sid ="101" ssid = "26">For the example diese, the descriptions type(Pronoun) and type(DemonstrativePro- noun) are inconsistent with type(Determiner), and hasNumber(some Plural) is inconsistent with hasNumber(some Singular) (Figs.</S>
			<S sid ="102" ssid = "27">2 and 4); these descriptions are thus ruled out.</S>
			<S sid ="103" ssid = "28">The hasCase descriptions have identical confidence scores, so that the first hasCase description that the algorithm encounters is chosen for the set of resulting descriptions, the other one is ruled out because of their inconsistency.</S>
			<S sid ="104" ssid = "29">(iii) If s1 is consistent with every description t ∈ T , then add s1 to T : T := T ∪ {s1} (iv) Remove s1 from S and iterate in (ii) until S is empty.</S>
			<S sid ="105" ssid = "30">6 The OLiA Reference Model does not specify disjoint-.</S>
			<S sid ="106" ssid = "31">ness constraints, and neither do GOLD or the DCR as External Reference Models.</S>
			<S sid ="107" ssid = "32">The axioms of the OntoTag ontologies, however, are specific to Spanish and cannot be directly applied to German.</S>
			<S sid ="108" ssid = "33">PCC TIGER NEGRA best-performing tool (StanfordTagger) .960 .956 .990∗ average (and std.</S>
			<S sid ="109" ssid = "34">deviation) for tool combinations 1 tool .868 (.109) .864 (.122) .870 (.113) 2 tools .928 (.018) .931 (.021) .943 (.028) 3 tools .947 (.014) .948 (.013) .956 (.018) 4 tools .956 (.006) .955 (.009) .963 (.013) 5 tools .959 (.006) .960 (.007) .964 (.009) 6 tools .963 (.003) .963 (.007) .965 (.007) all tools .967 .960 .965 ∗ The Stanford Tagger was trained on the NEGRA corpus.</S>
			<S sid ="110" ssid = "35">Table 2: Recall for rdf:type descriptions for word classes TIGER NEGRA 1 tool .678 (.106) .660 (.091) Morphisto .573 .568 Connexor .674 .662 RFTagger .786 .751 2 tools .761 (.019) .740 (.012) C+M .738 .730 M+R .769 .737 C+R .773 .753 all tools .791 .770 Table 3: Recall for morphological hasXY() descriptions The resulting, maximal consistent set of descriptions is then compared with the ontological descriptions that correspond to the original annotation in the corpus.</S>
	</SECTION>
	<SECTION title="Evaluation. " number = "4">
			<S sid ="111" ssid = "1">Six experiments were conducted with the goal to evaluate the prediction of word classes and morphological features on parts of three corpora of German newspaper articles: NEGRA (Skut et al., 1998), TIGER (Brants et al., 2002), and the Potsdam Commentary Corpus (Stede, 2004, PCC).</S>
			<S sid ="112" ssid = "2">From every corpus 10,000 tokens were considered for the analysis.</S>
			<S sid ="113" ssid = "3">TIGER and NEGRA are well-known resources that also influenced the design of several of the tools considered.</S>
			<S sid ="114" ssid = "4">For this reason, the PCC was consulted, a small collection of newspaper commentaries, 30,000 tokens in total, annotated with TIGER-style parts of speech and syntax (by members of the TIGER project).</S>
			<S sid ="115" ssid = "5">None of the tools considered here were trained on this data, so that it provides independent test data.</S>
			<S sid ="116" ssid = "6">The ontological descriptions were evaluated for recall:7 (7) recall(T ) = i=1 | Dpredicted (ti )∩Dtarget (ti )| i=1 | target i | In (7), T is a text (a list of tokens) with T = (t1, ..., tn), Dpredicted(t) are descriptions retrieved from the NLP analyses of the token t, and Dtarget(t) is the set of descriptions that correspond to the original annotation of t in the corpus.</S>
			<S sid ="117" ssid = "7">7 Precision and accuracy may not be appropriate measurements in this case: Annotation schemes differ in their expressiveness, so that a description predicted by an NLP tool but not found in the reference annotation may nevertheless be correct.</S>
			<S sid ="118" ssid = "8">The RFTagger, for example, assigns demonstrative pronouns the feature ‘3rd person’, that is not found in TIGER/NEGRA-style annotation because of its redundancy.</S>
			<S sid ="119" ssid = "9">4.1 Word classes.</S>
			<S sid ="120" ssid = "10">Table 2 shows that the recall of rdf:type descriptions (for word classes) increases continuously with the number of NLP tools applied.</S>
			<S sid ="121" ssid = "11">The combination of all seven tools actually shows a better recall than the best-performing single NLP tool.</S>
			<S sid ="122" ssid = "12">(The NEGRA corpus is an apparent exception only; the exceptionally high recall of the Stanford Tagger reflects the fact that it was trained on NEGRA.)</S>
			<S sid ="123" ssid = "13">A particularly high increase in recall occurs when tools are combined that compensate for their respective deficits.</S>
			<S sid ="124" ssid = "14">Morphisto, for example, generates alternative morphological analyses, so that the disambiguation algorithm performs a random choice between these.</S>
			<S sid ="125" ssid = "15">Morphisto has thus the worst recall among all tools considered (PCC .69, TIGER .65, NEGRA .70 for word classes).</S>
			<S sid ="126" ssid = "16">As compared to this, Connexor performs a contextual disambiguation; its recall is, however, limited by its coarse-grained word classes (PCC .73, TIGER .72, NEGRA .73).</S>
			<S sid ="127" ssid = "17">The combination of both tools yields a more detailed and context-sensitive analysis and thus results in a boost in recall by more than 13% (PCC .87, TIGER .86, NEGRA .86).</S>
			<S sid ="128" ssid = "18">4.2 Morphological features.</S>
			<S sid ="129" ssid = "19">For morphological features, Tab.</S>
			<S sid ="130" ssid = "20">3 shows the same tendencies that were also observed for word classes: The more tools are combined, the greater the recall of the generated descriptions, and the recall of combined tools often outperforms the recall of individual tools.</S>
			<S sid ="131" ssid = "21">The three tools that provide morphological annotations (Morphisto, Connexor, RFTagger) were evaluated against 10,000 tokens from TIGER and NEGRA respectively.</S>
			<S sid ="132" ssid = "22">The best-performing tool was the RFTagger, which possibly reflects the fact that it was trained on TIGER-style annotations, whereas Morphisto and Connexor were developed on the basis of independent resources and thus differ from the reference annotation in their respective degree of granularity.</S>
	</SECTION>
	<SECTION title="Summary and Discussion. " number = "5">
			<S sid ="133" ssid = "1">With the ontology-based approach described in this paper, the performance of annotation tools can be evaluated on a conceptual basis rather than by means of a string comparison with target annotations.</S>
			<S sid ="134" ssid = "2">A formal model of linguistic concepts is ex- tensible, finer-grained and, thus, potentially more adequate for the integration of linguistic annotations than string-based representations, especially for heterogeneous annotations, if the tagsets involved are structured according to different design principles (e.g., due to different terminological traditions, different communities involved, etc.).</S>
			<S sid ="135" ssid = "3">It has been shown that by abstracting from tool-specific representations of linguistic annotations, annotations from different tagsets can be represented with reference to the OLiA ontologies (and/or with other OWL/RDF-based terminology repositories linked as External Reference Models).</S>
			<S sid ="136" ssid = "4">In particular, it is possible to compare an existing reference annotation with annotations produced by NLP tools that use independently developed and differently structured annotation schemes (such as Connexor vs. RFTagger vs. Morphisto).</S>
			<S sid ="137" ssid = "5">Further, an algorithm for the integration of different annotations has been proposed that makes use of a majority-based confidence ranking and ontological consistency conditions.</S>
			<S sid ="138" ssid = "6">As consistency conditions are not formally defined in the OLiA Reference Model (which is expected to inherit such constraints from External Reference Models), a heuristic, structure-based definition of consistency was applied.</S>
			<S sid ="139" ssid = "7">This heuristic consistency definition is overly rigid and rules out a number of consistent alternative analyses, as it is the case for overlapping a particular approach (e.g., differences in the coverage of the linguistic context).</S>
			<S sid ="140" ssid = "8">It can thus be stated that the integration of multiple alternative analyses has the potential to produce linguistic analyses that are both more robust and more detailed than those of the original tools.</S>
			<S sid ="141" ssid = "9">The primary field of application of this approach is most likely to be seen in a context where applications are designed that make direct use of OWL/RDF representations as described, for example, by Hellmann (2010).</S>
			<S sid ="142" ssid = "10">It is, however, also possible to use ontological representations to bootstrap novel and more detailed annotation schemes, cf.</S>
			<S sid ="143" ssid = "11">Zavrel and Daelemans (2000).</S>
			<S sid ="144" ssid = "12">Further, the conversion from string-based representations to ontological descriptions is reversible, so that results of ontology-based disambiguation and validation can also be reintegrated with the original annotation scheme.</S>
			<S sid ="145" ssid = "13">The idea of such a reversion algorithm was sketched by Buyko et al.</S>
			<S sid ="146" ssid = "14">(2008) where the OLiA ontologies were suggested as a means to translate between different annotation schemes.9</S>
	</SECTION>
	<SECTION title="Extensions and Related Research. " number = "6">
			<S sid ="147" ssid = "1">Natural extensions of the approach described in this paper include: (i) Experiments with formally defined consistency conditions (e.g., with respect to restrictions on the domain of properties).</S>
			<S sid ="148" ssid = "2">(ii) Context-sensitive disambiguation of morphological features (e.g., by combination with a chunker and adjustment of confidence scores for morphological features over all tokens in the current chunk, cf.</S>
			<S sid ="149" ssid = "3">Kermes and Evert, 2002).</S>
			<S sid ="150" ssid = "4">(iii) Replacement of majority vote by more elaborate strategies to merge grammatical analyses.</S>
			<S sid ="151" ssid = "5">categories.8 Despite this rigidity, we witness an increase of recall when multiple alternative analyses are integrated.</S>
			<S sid ="152" ssid = "6">This increase of recall may result from a compensation of tool-specific deficits, e.g., with respect to annotation granularity.</S>
			<S sid ="153" ssid = "7">Also, the improved recall can be explained by a compensation of overfitting, or deficits that are inherent to 8 Preposition-determiner compounds like German am ‘on the’, for example, are both prepositions and determiners.</S>
			<S sid ="154" ssid = "8">9 The mapping from ontological descriptions to tags of a. particular scheme is possible, but neither trivial nor necessarily lossless: Information of ontological descriptions that cannot be expressed in the annotation scheme under consideration (e.g., the distinction between attributive and substitutive pronouns in the Morphisto scheme) will be missing in the resulting string representation.</S>
			<S sid ="155" ssid = "9">For complex annotations, where ontological descriptions correspond to different sub- strings, an additional ‘tag grammar’ may be necessary to determine the appropriate ordering of substrings according to the annotation scheme (e.g., in the Connexor analysis).</S>
			<S sid ="156" ssid = "10">(iv) Application of the algorithm for the ontological processing of node labels and edge labels in syntax annotations.</S>
			<S sid ="157" ssid = "11">(v) Integration with other ontological knowledge sources in order to improve the recall of morphosyntactic and morphological analyses (e.g., for disambiguating grammatical case).</S>
			<S sid ="158" ssid = "12">Extensions (iii) and (iv) are currently pursued in an ongoing research effort described by Chiarcos et al.</S>
			<S sid ="159" ssid = "13">(2010).</S>
			<S sid ="160" ssid = "14">Like morphosyntactic and morphological features, node and edge labels of syntactic trees are ontologically represented in several Annotation Models, the OLiA Reference Model, and External Reference Models, the merging algorithm as described above can thus be applied for syntax, as well.</S>
			<S sid ="161" ssid = "15">Syntactic annotations, however, involve the additional challenge to align different structures before node and edge labels can be addressed, an issue not further discussed here for reasons of space limitations.</S>
			<S sid ="162" ssid = "16">Alternative strategies to merge grammatical analyses may include alternative voting strategies as discussed in literature on classifier combination, e.g., weighted majority vote, pairwise voting (Halteren et al., 1998), credibility profiles (Tufis¸, 2000), or handcrafted rules (Borin, 2000).</S>
			<S sid ="163" ssid = "17">A novel feature of our approach as compared to existing applications of these methods is that confidence scores are not attached to plain strings, but to ontological descriptions: Tufis¸, for example, assigned confidence scores not to tools (as in a weighted majority vote), but rather, assessed the ‘credibility’ of a tool with respect to the predicted tag.</S>
			<S sid ="164" ssid = "18">If this approach is applied to ontological descriptions in place of tags, it allows us to consider the credibility of pieces of information regardless of the actual string representation of tags.</S>
			<S sid ="165" ssid = "19">For example, the credibility of hasCase descriptions can be assessed independently from the credibility of hasGender descriptions even if the original annotation merged both aspects in one single tag (as the RFTagger does, for example, cf.</S>
			<S sid ="166" ssid = "20">ex.</S>
			<S sid ="167" ssid = "21">5).</S>
			<S sid ="168" ssid = "22">Extension (v) has been addressed in previous research, although mostly with the opposite perspective: Already Cimiano and Reyle (2003) noted that the integration of grammatical and semantic analyses may be used to resolve ambiguity and underspecifications, and this insight has also motivated the ontological representation of linguistic resources such as WordNet (Gangemi et al., 2003), FrameNet (Scheffczyk et al., 2006), the linking of corpora with such ontologies (Hovy et al., 2006), the modelling of entire corpora in OWL/DL (Bur- chardt et al., 2008), and the extension of existing ontologies with ontological representations of selected linguistic features (Buitelaar et al., 2006; Davis et al., 2008).</S>
			<S sid ="169" ssid = "23">Aguado de Cea et al.</S>
			<S sid ="170" ssid = "24">(2004) sketched an architecture for the closer ontology-based integration of grammatical and semantic information using OntoTag and several NLP tools for Spanish.</S>
			<S sid ="171" ssid = "25">Aguado de Cea et al.</S>
			<S sid ="172" ssid = "26">(2008) evaluate the benefits of this approach for the Spanish particle se, and conclude for this example that the combination of multiple tools yields more detailed and more accurate linguistic analyses of particularly problematic, polysemous function words.</S>
			<S sid ="173" ssid = "27">A similar increase in accuracy has also been repeatedly reported for ensemble combination approaches, that are, however, limited to tools that produce annotations according to the same tagset (Brill and Wu, 1998; Halteren et al., 2001).</S>
			<S sid ="174" ssid = "28">These observations provide further support for our conclusion that the ontology-based integration of morphosyntactic analyses enhances both the robustness and the level of detail of morphosyntactic and morphological analyses.</S>
			<S sid ="175" ssid = "29">Our approach extends the philosophy of ensemble combination approaches to NLP tools that do not only employ different strategies and philosophies, but also different annotation schemes.</S>
	</SECTION>
	<SECTION title="Acknowledgements">
			<S sid ="176" ssid = "30">From 2005 to 2008, the research on linguistic ontologies described in this paper was funded by the German Research Foundation (DFG) in the context of the Collaborative Research Center (SFB) 441 “Linguistic Data Structures”, Project C2 “Sustainability of Linguistic Resources” (University of Tu¨ bingen), and since 2007 in the context of the SFB 632 “Information Structure”, Project D1 “Linguistic Database” (University of Potsdam).</S>
			<S sid ="177" ssid = "31">The author would also like to thank Julia Ritz, Angela Lahee, Olga Chiarcos and three anonymous reviewers for helpful hints and comments.</S>
	</SECTION>
</PAPER>
