<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">We present a new reordering model estimated as a standard n-gram language model with units built from morpho- syntactic information of the source and target languages.</S>
		<S sid ="2" ssid = "2">It can be seen as a model that translates the morpho-syntactic structure of the input sentence, in contrast to standard translation models which take care of the surface word forms.</S>
		<S sid ="3" ssid = "3">We take advantage from the fact that such units are less sparse than standard translation units to increase the size of bilingual context that is considered during the translation process, thus effectively accounting for mid-range reorderings.</S>
		<S sid ="4" ssid = "4">Empirical results on French-English and GermanEnglish translation tasks show that our model achieves higher translation accuracy levels than those obtained with the widely used lexicalized reordering model.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="5" ssid = "5">Word ordering is one of the major issues in statistical machine translation (SMT), due to the many word order peculiarities of each language.</S>
			<S sid ="6" ssid = "6">It is widely accepted that there is a need for structural information to account for such differences.</S>
			<S sid ="7" ssid = "7">Structural information, such as Part-of-speech (POS) tags, chunks or constituency/dependency parse trees, offers a greater potential to learn generalizations about relationships between languages than models based on word surface forms, because such “surfacist” models fail to infer generalizations from the training data.</S>
			<S sid ="8" ssid = "8">The word ordering problem is typically decomposed in a number of related problems which can be further explained by a variety of linguistic phenomena.</S>
			<S sid ="9" ssid = "9">Accordingly, we can sort out the reordering problems into three categories based on the kind of linguistic units involved and/or the typical distortion distance they imply.</S>
			<S sid ="10" ssid = "10">Roughly speaking, we face short-range reorderings when single words are reordered within a relatively small window distance.</S>
			<S sid ="11" ssid = "11">It consist of the easiest case as typically, the use of phrases (in the sense of translation units of the phrase-based approach to SMT) is believed to adequately perform such reorderings.</S>
			<S sid ="12" ssid = "12">Mid-range reorderings involve reorderings between two or more phrases (translation units) which are closely positioned, typically within a window of about 6 words.</S>
			<S sid ="13" ssid = "13">Many alternatives have been proposed to tackle mid- range reorderings through the introduction of linguistic information in MT systems.</S>
			<S sid ="14" ssid = "14">To the best of our knowledge, the authors of (Xia and Mc- Cord, 2004) were the first to address this problem in the statistical MT paradigm.</S>
			<S sid ="15" ssid = "15">They automatically build a set of linguistically grounded rewrite rules, aimed at reordering the source sentence so as to match the word order of the target side.</S>
			<S sid ="16" ssid = "16">Similarly, (Collins, et al 2005) and (Popovic and Ney, 2006) reorder the source sentence using a small set of handcrafted rules for GermanEnglish translation.</S>
			<S sid ="17" ssid = "17">(Crego and Marin˜ o, 2007) show that the ordering problem can be more accurately solved by building a source-sentence word lattice containing the most promising reordering hypotheses, allowing the decoder to decide for the best word order hypothesis.</S>
			<S sid ="18" ssid = "18">Word lattices are built by means of rewrite rules operating on POS tags; such rules are automatically extracted from the training bi-text.</S>
			<S sid ="19" ssid = "19">(Zhang, et al 2007) introduce shallow parse (chunk) information to reorder the source sentence, aiming at extending the scope of their rewrite rules, encoding reordering hypotheses in the form of a confusion network that is then passed to the decoder.</S>
			<S sid ="20" ssid = "20">These studies tackle mid-range reorderings by predicting more or less accurate reordering hypotheses.</S>
			<S sid ="21" ssid = "21">However, none 197 Coling 2010: Poster Volume, pages 197–205, Beijing, August 2010 of them introduce a reordering model to be used in decoding time.</S>
			<S sid ="22" ssid = "22">Nowadays, most of SMT systems implement the well known lexicalized reordering model (Tillman, 2004).</S>
			<S sid ="23" ssid = "23">Basically, for each translation unit it estimates the probability of being translated monotone, swapped or placed discontiguous with respect to its previous translation unit.</S>
			<S sid ="24" ssid = "24">Integrated within the Moses (Koehn, et al 2007) decoder, the model achieves state-of- the-art results for many translation tasks.</S>
			<S sid ="25" ssid = "25">One of the main reasons that explains the success of the model is that it considers information of the source- and target-side surface forms, while the above mentionned approaches attempt to hypothesize reorderings relying only on the information contained on the source-side words.</S>
			<S sid ="26" ssid = "26">Finally, long-range reorderings imply reorder- ings in the structure of the sentence.</S>
			<S sid ="27" ssid = "27">Such reorderings are necessary to model the translation for pairs like ArabicEnglish, as English typically follows the SVO order, while Arabic sentences have different structures.</S>
			<S sid ="28" ssid = "28">Even if several attempts exist which follow the above idea of making the ordering of the source sentence similar to the target sentence before decoding (Niehues and Kolss, 2009), long-range reorderings are typically better addressed by syntax-based and hierarchical (Chi- ang, 2007) models.</S>
			<S sid ="29" ssid = "29">In (Zollmann et al., 2008), ables to account for arbitrary large sequences of units.</S>
			<S sid ="30" ssid = "30">Hence, the proposed model takes care of the translation adequacy of the structural information present in translation hypotheses, here introduced in the form of POS tags.</S>
			<S sid ="31" ssid = "31">We also show how the new model compares to a widely used lexical- ized reordering model, which we have also implemented in our particular bilingual n-gram approach to SMT, as well as to the widely known Moses SMT decoder, a state-of-the-art decoder performing lexicalized reordering.</S>
			<S sid ="32" ssid = "32">The remaining of this paper is as follows.</S>
			<S sid ="33" ssid = "33">In Section 2 we briefly describe the bilingual n-gram SMT system.</S>
			<S sid ="34" ssid = "34">Section 3 details the bilingual n- gram reordering model, the main contribution of this paper, and introduces additional well known reordering models.</S>
			<S sid ="35" ssid = "35">In Section 4, we analyze the reordering needs of the language pairs considered in this work and we carry out evaluation experiments.</S>
			<S sid ="36" ssid = "36">Finally, we conclude and outline further work in Section 5.</S>
	</SECTION>
	<SECTION title="Bilingual n-gram SMT. " number = "2">
			<S sid ="37" ssid = "1">Our SMT system defines a translation hypothesis t given a source sentence s, as the sentence which maximizes a linear combination of feature functions: an interesting comparison between phrase-based,hierarchical and syntax-augmented models is car ( M 1 = arg max ) λmhm(sJ I (1)ried out, concluding that hierarchical and syntax I 1 m=1 1 , t1 ) based models slightly outperform phrase-based models under large data conditions and for sufficiently non-monotonic language pairs.</S>
			<S sid ="38" ssid = "2">Encouraged by the work reported in (Hoang and Koehn, 2009), we tackle the mid-range reordering problem in SMT by introducing a n- gram language model of bilingual units built from POS information.</S>
			<S sid ="39" ssid = "3">The rationale behind such a model is double: on the one hand we aim at introducing morpho-syntactic information into the reordering model, as we believe it plays an im where λm is the weight associated with the feature hm(s, t).</S>
			<S sid ="40" ssid = "4">The main feature is the log-score of the translation model based on bilingual n-grams.</S>
			<S sid ="41" ssid = "5">This model constitutes a language model of a particular bi-language composed of bilingual units which are typically referred to as tuples (Marin˜ o et al., 2006).</S>
			<S sid ="42" ssid = "6">In this way, the translation model probabilities at the sentence level are approximated by using n-grams of tuples: K p(sJ I n portant role for predicting systematic word ordering differences between language pairs; at the 1 , t1) = k=1 p((s, t)k |(s, t)k−1 . . .</S>
			<S sid ="43" ssid = "7">(s, t)k−n+1) same time that it drastically reduces the sparseness problem of standard translation units built from surface forms.</S>
			<S sid ="44" ssid = "8">On the other hand, n-gram language modeling is a robust approach, that en where s refers to source t to target and (s, t)k to the kth tuple of the given bilingual sentence pairs, J and tI . It is important to notice that, since both languages are linked up in tuples, the context information provided by this translation model is bilingual.</S>
			<S sid ="45" ssid = "9">As for any standard n-gram language model, our translation model is estimated over a training corpus composed of sentences of the language being modeled, in this case, sentences of the bi-language previously introduced.</S>
			<S sid ="46" ssid = "10">Translation units consist of the core elements of any SMT system.</S>
			<S sid ="47" ssid = "11">In our case, tuples are extracted from a word aligned corpus in such a way that a unique segmentation of the bilingual corpus is achieved, allowing to estimate the n-gram model.</S>
			<S sid ="48" ssid = "12">Figure 1 presents a simple example illustrating the unique tuple segmentation for a given word-aligned pair of sentences (top).</S>
			<S sid ="49" ssid = "13">alignments.</S>
			<S sid ="50" ssid = "14">Following on the previous example, the ruleperfect translations ❀ translations perfect produces the swap of the English words that is ob served for the French and English pair.</S>
			<S sid ="51" ssid = "15">Typically, POS information is used to increase the generalization power of such rules.</S>
			<S sid ="52" ssid = "16">Hence, rewrite rules are built using POS instead of surface word forms.</S>
			<S sid ="53" ssid = "17">See (Crego and Marin˜ o, 2007) for details on tuples extraction and reordering rules.</S>
	</SECTION>
	<SECTION title="Reordering Models. " number = "3">
			<S sid ="54" ssid = "1">In this section, we detail three different reordering models implemented in our SMT system.</S>
			<S sid ="55" ssid = "2">As previously outlined, the purpose of reordering models is to accurately learn generalizations for the word order modifications introduced on the source side during the tuple extraction process.</S>
			<S sid ="56" ssid = "3">3.1 Source n-gram Language Model.</S>
			<S sid ="57" ssid = "4">We employ a n-gram language model estimated over the source words of the training corpus after being reordered in the tuple extraction process.</S>
			<S sid ="58" ssid = "5">Therefore, the model scores a given source-side reordering hypothesis according to the reorder ings performed in the training sentences.</S>
			<S sid ="59" ssid = "6">POS tags are used instead of surface formsFigure 1: Tuple extraction from an aligned sen tence pair.</S>
			<S sid ="60" ssid = "7">The resulting sequence of tuples (1) is further refined to avoid NULL words in source side of the tuples (2).</S>
			<S sid ="61" ssid = "8">Once the whole bilingual training data in order to improve generalization and to reduce sparseness.</S>
			<S sid ="62" ssid = "9">The model is estimated as any standard n-gram language model, described by the following equation: J is segmented into tuples, n-gram language model p(sJ I n j j 1 j n+1 probabilities can be estimated.</S>
			<S sid ="63" ssid = "10">Notice from the 1 , t1) = j=1 p(st st − , . . .</S>
			<S sid ="64" ssid = "11">, st − ) (2) example that the English source words perfect and where st relates to the POS tag used for the jth translations have been reordered in the final tuple segmentation, while the French target words are kept in their original order.</S>
			<S sid ="65" ssid = "12">During decoding, sentences to be translated are encoded in the form of word lattices containing the most promising reordering hypotheses, so as to reproduce the word order modifications introduced during the tuple extraction process.</S>
			<S sid ="66" ssid = "13">Hence, at decoding time, only those reordering hypotheses encoded in the word lattice are examined.</S>
			<S sid ="67" ssid = "14">Reordering hypotheses are introduced following a set of reordering rules automatically learned from the bi-text corpus word source word.</S>
			<S sid ="68" ssid = "15">The main drawback of this model is the lack of knowledge of the hypotheses on the target- side.</S>
			<S sid ="69" ssid = "16">The probability assigned to a sequence of source words is only conditioned to the sequence of source words.</S>
			<S sid ="70" ssid = "17">3.2 Lexicalized Reordering Model.</S>
			<S sid ="71" ssid = "18">A broadly used reordering model for phrase-based systems is lexicalized reordering (Tillman, 2004).</S>
			<S sid ="72" ssid = "19">It introduces a probability distribution for each phrase pair that indicates the likelihood of being translated monotone, swapped or placed discontiguous to its previous phrase.</S>
			<S sid ="73" ssid = "20">The ordering of the next phrase with respect to the current phrase is typically also modeled.</S>
			<S sid ="74" ssid = "21">In our implementation, we modified the three orientation types and Figure 2 shows the sequence of translation units built from POS tags, used in our previous example.</S>
			<S sid ="75" ssid = "22">consider: a consecutive type, where the original monotone and swap orientations are lumped together, a forward type, specifying discontiguous forward orientation, and a backward type, specifying discontiguous backward orientation.</S>
			<S sid ="76" ssid = "23">Empirical results showed that in our case, the new orientations slightly outperform the original ones.</S>
			<S sid ="77" ssid = "24">This may be explained by the fact that the model is applied over tuples instead of phrases.</S>
			<S sid ="78" ssid = "25">Counts of these three types are updated for each unit collected during the training process.</S>
			<S sid ="79" ssid = "26">Given these counts, we can learn probability dis tributions of the form pr (orientation|(st)) where orientation ∈ {c, f, b} (consecutive, forward and backward) and (st) is a translation unit.</S>
			<S sid ="80" ssid = "27">Counts are typically smoothed for the estimation of the probability distribution.</S>
			<S sid ="81" ssid = "28">A major weakness of the lexicalized reordering model is due to the fact that it does not considers phrase neighboring, i.e. a single probability is learned for each phrase pair without considering its context.</S>
			<S sid ="82" ssid = "29">An additional concern is the problem of sparse data: translation units may occur only a few times in the training data, making it hard to estimate reliable probability distributions.</S>
			<S sid ="83" ssid = "30">3.3 Linguistically Informed Bilingual.</S>
			<S sid ="84" ssid = "31">n-gram Language Model The bilingual n-gram LM is estimated as a standard n-gram LM over translation units built from POS tags represented as: K Figure 2: Sequence of POS-tagged units used to estimate the bilingual n-gram LM.</S>
			<S sid ="85" ssid = "32">POS-tagged units used in our model are expected to be much less sparse than those built from surface forms, allowing to estimate higher order language models.</S>
			<S sid ="86" ssid = "33">Therefore, larger bilingual context are introduced in the translation process.</S>
			<S sid ="87" ssid = "34">This model can also be seen as a translation model of the sentence structure.</S>
			<S sid ="88" ssid = "35">It models the adequacy of translating sequences of source POS tags into target POS tags.</S>
			<S sid ="89" ssid = "36">Note that the model is not limited to using POS information.</S>
			<S sid ="90" ssid = "37">Rather, many other information sources could be used (supertags, additional morphology features, etc.), allowing to model different translation properties.</S>
			<S sid ="91" ssid = "38">However, we must take into account that the degree of sparsity of the model units, which is directly related to the information they contain, affects the level of bilingual context finally introduced in the translation process.</S>
			<S sid ="92" ssid = "39">Since more informed units may yield more accurate predictions, more informed units may also force the model to fall to lower n-grams.</S>
			<S sid ="93" ssid = "40">Hence, the degree of accuracy and generalization power of the model units must be carefully balanced to allow good reordering predictions for contexts as large as possible.</S>
			<S sid ="94" ssid = "41">As any standard language model, smoothing is needed.</S>
			<S sid ="95" ssid = "42">Empirical results showed that KneserNey smoothing (Kneser and Ney, 1995) achieved p(sJ , tI ) = n p((st)t |(st)t . . .</S>
			<S sid ="96" ssid = "43">(st)t ) the best performance among other options (mea 1 1 k=1 k k−1 k−n+1 sured in terms of translation accuracy).</S>
			<S sid ="97" ssid = "44">where (st)t relates to the kth translation unit, 3.4 Decodi ng Issues (st)k , built from POS tags instead of words.</S>
			<S sid ="98" ssid = "45">This model aims at alleviating the drawbacks of the previous two reordering models.</S>
			<S sid ="99" ssid = "46">On the one hand it takes into account bilingual information to model reordering.</S>
			<S sid ="100" ssid = "47">On the other hand it considers the phrase neighboring when estimating the reordering probability of a given translation unit.</S>
			<S sid ="101" ssid = "48">A straightforward implementation of the three models is carried out by extending the log-linear combination of equation (1) with the new features.</S>
			<S sid ="102" ssid = "49">Note that no additional decoding complexity is introduced in the baseline decoding implementation.</S>
			<S sid ="103" ssid = "50">Considering the bilingual n-gram language model, the decoder must know the POS tags for each tuple.</S>
			<S sid ="104" ssid = "51">However, each tuple may be tagged differently, as words with same surface form may have different POS tags.</S>
			<S sid ="105" ssid = "52">We have implemented two solutions for this situation.</S>
			<S sid ="106" ssid = "53">Firstly, we assume that each tuple has a single POS-tagged version.</S>
			<S sid ="107" ssid = "54">Accordingly, we select a single POS-tagged version out of the multiple choices (the most frequent).</S>
			<S sid ="108" ssid = "55">Secondly, all POS-tagged versions of each tuple are allowed.</S>
			<S sid ="109" ssid = "56">The second choice implies using more accurate POS-tagged tuples to model reordering, however, it overpopulates the search space with spurious hypotheses, as multiple identical units (with different POS tags) are considered.</S>
			<S sid ="110" ssid = "57">Our first empirical findings showed no differences in translation accuracy for both configurations.</S>
			<S sid ="111" ssid = "58">Hence, in the remaining of this paper we only consider the first solution (a single POS- tagged version of each tuple).</S>
			<S sid ="112" ssid = "59">The training corpus composed of tagged units out of which our new model is estimated is accordingly modified to contain only those tagged units considered in decoding.</S>
			<S sid ="113" ssid = "60">Note that most of the ambiguity present in word tagging is resolved by the fact that translation units may contain multiple source and target side words.</S>
	</SECTION>
	<SECTION title="Evaluation Framework. " number = "4">
			<S sid ="114" ssid = "1">In this section, we perform evaluation experiments of our novel reordering model.</S>
			<S sid ="115" ssid = "2">First, we give details of the corpora and baseline system employed in our experiments and analyze the reordering needs of the translation tasks, French- English and GermanEnglish (in both directions).</S>
			<S sid ="116" ssid = "3">Finally, we evaluate the performance of our model and contrast results with other reordering models and translation systems.</S>
			<S sid ="117" ssid = "4">4.1 Corpora.</S>
			<S sid ="118" ssid = "5">We have used the fifth version of the EPPS and the News Commentary corpora made available in the context of the Fifth ACL Workshop on Statistical Machine Translation.</S>
			<S sid ="119" ssid = "6">Table 1 presents the basic statistics for the training and test data sets.</S>
			<S sid ="120" ssid = "7">Our test sets correspond to news-test2008 and new- stest2009 file sets, hereinafter referred to as Tune and Test respectively.</S>
			<S sid ="121" ssid = "8">French, German and English Part-of-speech tags are computed by means of the TreeTagger 1 toolkit.</S>
			<S sid ="122" ssid = "9">Additional German tags are obtained using the RFTagger 2 toolkit, which annotates text with fine-grained part-of-speech tags (Schmid and Laws, 2008) with a vocabulary of more than 700 tags containing rich morpho-syntactic information (gender, number, case, tense, etc.).</S>
			<S sid ="123" ssid = "10">L a n g. S e nt . W or ds V o c. O O V Ref s Train Fr en ch En gli sh 1.7 5 M 1.7 5 M 52.</S>
			<S sid ="124" ssid = "11">4 M 47.</S>
			<S sid ="125" ssid = "12">4 M 13 7 k 13 8 k − − − − Tune Fr en ch En gli sh 2, 05 1 2, 05 1 55.</S>
			<S sid ="126" ssid = "13">3 k 49.</S>
			<S sid ="127" ssid = "14">2 k 8, 95 7 8, 35 9 1, 28 2 1, 34 4 1 1 Test Fr en ch En gli sh 2, 52 5 2, 52 5 72.</S>
			<S sid ="128" ssid = "15">8 k 65.</S>
			<S sid ="129" ssid = "16">1 k 10, 83 2 9, 56 8 1, 74 9 1, 72 4 1 1 Train Ge rm an En gli sh 1, 61 M 1, 61 M 42.</S>
			<S sid ="130" ssid = "17">2 M 44.</S>
			<S sid ="131" ssid = "18">2 M 38 1 k 13 7 k − − − − Tune Ge rm an En gli sh 2, 05 1 2, 05 1 47, 8 k 49, 2 k 10, 99 4 8, 35 9 2, 15 3 1, 49 1 1 1 Test Ge rm an En gli sh 2, 52 5 2, 52 5 62, 8 k 65, 1 k 12, 85 6 9, 56 8 2, 70 4 1, 81 0 1 1 Table 1: Statistics for the training, tune and test data sets.</S>
			<S sid ="132" ssid = "19">4.2 System Details.</S>
			<S sid ="133" ssid = "20">After preprocessing the corpora with standard tokenization tools, word-to-word alignments are performed in both directions, source-to-target and target-to-source.</S>
			<S sid ="134" ssid = "21">In our system implementation, the GIZA++ toolkit3 is used to compute the word alignments.</S>
			<S sid ="135" ssid = "22">Then, the grow-diag-final-and (Koehn et al., 2005) heuristic is used to obtain the alignments from which tuples are extracted.</S>
			<S sid ="136" ssid = "23">In addition to the tuple n-gram translation model, our SMT system implements six additional feature functions which are linearly com 1 www.ims.unistuttgart.de/projekte/corplex/TreeTagger 2 www.ims.unistuttgart.de/projekte/corplex/RFTagger 3 http://www.fjoch.com/GIZA++.html bined following a discriminative modeling framework (Och and Ney, 2002): a target-language model which provides information about the target language structure and fluency; two lexicon models, which constitute complementary translation models computed for each given tuple; a ’weak’ distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which are used in order to compensate for the system preference for short translations.</S>
			<S sid ="137" ssid = "24">All language models used in this work are estimated using the SRI language modeling toolkit4.</S>
			<S sid ="138" ssid = "25">According to our experience, KneserNey smoothing (Kneser and Ney, 1995) and interpolation of lower and higher n-grams options are used as they typically achieve the best performance.</S>
			<S sid ="139" ssid = "26">Optimization work is carried out by means of the widely used MERT toolkit5 which has been slightly modified to perform optimizations embedding our decoder.</S>
			<S sid ="140" ssid = "27">The BLEU (Pap- ineni et al., 2002) score is used as objective function for MERT and to evaluate test performance.</S>
			<S sid ="141" ssid = "28">4.3 Reordering in GermanEnglish and.</S>
			<S sid ="142" ssid = "29">French-English Translation Two factors are found to greatly impact the overall translation performance: the morphological mismatch between languages, and their reordering needs.</S>
			<S sid ="143" ssid = "30">The vocabulary size is strongly influenced by the number of word forms for number, case, tense, mood, etc., while reordering needs refer to the difference in their syntactic structure.</S>
			<S sid ="144" ssid = "31">In this work, we are primarily interested on the reordering needs of each language pair.</S>
			<S sid ="145" ssid = "32">Figure 3 displays a quantitative analysis of the reordering needs for the language pairs under study.</S>
			<S sid ="146" ssid = "33">Figure 3 displays the (%) distribution of the reordered sequences, according to their size, observed for the training bi-texts of both translation tasks.</S>
			<S sid ="147" ssid = "34">Word alignments are used to determine reorderings.</S>
			<S sid ="148" ssid = "35">A reordering sequence can also be seen as the sequence of words implied in a reordering rule.</S>
			<S sid ="149" ssid = "36">Hence, we used the reordering rules extracted from the training corpus to account for reordering sequences.</S>
			<S sid ="150" ssid = "37">Coming back to the example of Figure 1, a single reordering sequence is found, 4 http://www.speech.sri.com/projects/srilm/ 5 http://www.statmt.org/moses/ which considers the source words perfect translations.</S>
			<S sid ="151" ssid = "38">50 fren deen 45 40 35 30 25 20 15 10 5 2 3 4 5 6 7 &gt;=8 Size (words) Figure 3: Size (in words) of reorderings (%) observed in training bi-texts.</S>
			<S sid ="152" ssid = "39">As can be seen, the French-English and GermanEnglish pairs follow a different distribution of reorderings according to their size.</S>
			<S sid ="153" ssid = "40">A lower number of short-range reorderings are observed for the GermanEnglish task while a higher number of long-range reorderings.</S>
			<S sid ="154" ssid = "41">Considering mid-range reorderings (from 5 to 7 words), the French-English pair shows a lower percentage (∼14%) than the GermanEnglish (∼ 22%).</S>
			<S sid ="155" ssid = "42">A simi lar performance is expected when considering the opposite translation directions.</S>
			<S sid ="156" ssid = "43">Note that reorderings are extracted from word-alignments, an automatic process which is far notoriously error- prone.</S>
			<S sid ="157" ssid = "44">The above statistics must be accordingly considered.</S>
			<S sid ="158" ssid = "45">4.4 Results.</S>
			<S sid ="159" ssid = "46">Translation accuracy (BLEU) results are given in table 2 for the same baseline system performing different reordering models: source 6-gram LM (sLM); lexicalized reordering (lex); bilingual 6- gram LM (bLM) assuming a single POS-tagged version of each tuple.</S>
			<S sid ="160" ssid = "47">In the case of the GermanEnglish translation task we also report results for the bilingual 5-gram LM built from POS tags obtained from RFTagger containing a richer vocabulary tag set (b+LM).</S>
			<S sid ="161" ssid = "48">For comparison purposes, we also show the scores obtained by the Moses phrase-based system performing lexicalized reordering.</S>
			<S sid ="162" ssid = "49">Models of both systems are built sharing the same training data and word alignments.</S>
			<S sid ="163" ssid = "50">The worst results are obtained by the sLM model.</S>
			<S sid ="164" ssid = "51">The fact that it only considers source- language information results clearly relevant to accurately model reordering.</S>
			<S sid ="165" ssid = "52">A very similar performance is shown by our bilingual n-gram system and Moses under lexicalized reordering (bLM and Moses), slightly lower results are obtained by the n-gram system under French- English translation.</S>
			<S sid ="166" ssid = "53">test set.</S>
			<S sid ="167" ssid = "54">Table 3 shows the BLEU results of the reordering task.</S>
			<S sid ="168" ssid = "55">Bigram, trigram and 4gram precision scores are also given.</S>
			<S sid ="169" ssid = "56">Table 3: Reordering accuracy (BLEU) results.</S>
			<S sid ="170" ssid = "57">Table 2: Translation accuracy (BLEU) results.</S>
			<S sid ="171" ssid = "58">When moving from lex to bLM, our system increases its accuracy results for both tasks and translation directions.</S>
			<S sid ="172" ssid = "59">In this case, results are slightly higher than those obtained by Moses (same results for English-to-French).</S>
			<S sid ="173" ssid = "60">Finally, results for translations performed with the bilingual n-gram reordering model built from rich German POS tags (b+LM) achieve the highest accuracy results for both directions of the GermanEnglish task.</S>
			<S sid ="174" ssid = "61">Even though results are consistent for all translation tasks and directions they fall within the statistical confidence margin.</S>
			<S sid ="175" ssid = "62">Add ±2.36to French-English results and ±1.25 to German English results for a 95% confidence level.</S>
			<S sid ="176" ssid = "63">Very similar results were obtained when estimating our model for orders from 5 to 7.</S>
			<S sid ="177" ssid = "64">In order to better understand the impact of the proposed reordering model, we have measured the accuracy of the reordering task.</S>
			<S sid ="178" ssid = "65">Hence, isolating the reordering problem from the more general translation problem.</S>
			<S sid ="179" ssid = "66">We use BLEU to account the n-gram matching between the sequence of source words aligned to the 1-best translation hypothesis, i.e. the permutation of the source words output by the decoder, and the permutation of source words that monotonizes the word alignments with respect to the target reference.</S>
			<S sid ="180" ssid = "67">Note that in order to obtain the word alignments of the test sets we re-aligned the entire corpus after including the As can be seen, the bilingual n-gram reordering model shows higher results for both translation tasks and directions than lexicalized reordering, specially for GermanEnglish translation.</S>
			<S sid ="181" ssid = "68">Our model also obtains higher values of n-gram precision for all values of n. Next, we validate the introduction of additional bilingual context in the translation process.</S>
			<S sid ="182" ssid = "69">Figure 4 shows the average size of the translation unit n-grams used for the test set according to different models (GermanEnglish), the surface form 3-gram language model (main translation model), and the new reordering model when built from the reduced POS tagset (POS) and using the rich POS tagset (POS+).</S>
			<S sid ="183" ssid = "70">40 word-based bilingual units POS-based bilingual units POS+-based bilingual units 35 30 25 20 15 10 5 0 0 1 2 3 4 5 6 7 8 9 Size (units) Figure 4: Size of translation unit n-grams (%) seen in test for different n-gram models.</S>
			<S sid ="184" ssid = "71">As expected, translation units built from the reduced POS tagset are less sparse, enabling us to introduce larger n-grams in the translation process.</S>
			<S sid ="185" ssid = "72">However, the fact that they achieve lower translation accuracy scores (see Table 2) indicates that the probabilities associated to these large n- grams are less accurate.</S>
			<S sid ="186" ssid = "73">It can also be seen that the model built from the rich POS tagset uses a higher number of large n-grams than the language model built from surface forms.</S>
			<S sid ="187" ssid = "74">The availability of mid-range n-grams validates the introduction of additional bilingual context achieved by the new model, leading to effectively modeling mid-range reorderings.</S>
			<S sid ="188" ssid = "75">Notice additionally that considering the language model built from surface forms, only a few 4-grams of the test set are seen in the training set, which explains the small reduction in performance observed when translating with a bilingual 4-gram language model (internal results).</S>
			<S sid ="189" ssid = "76">Similarly, the results shown in Figure 4 validates the choice of using bilingual 5-grams for b+LM and 6-grams for bLM . Finally, we evaluate the mismatch between the reorderings collected on the training data, and those output by the decoder.</S>
			<S sid ="190" ssid = "77">Table 4 shows the percentage of reordered sequences found for the1-best translation hypothesis of the test set ac cording to their size.</S>
			<S sid ="191" ssid = "78">The French-to-English and German-to-English tasks are considered.</S>
			<S sid ="192" ssid = "79">P a i r Co nfi g 2 3 4</S>
	</SECTION>
	<SECTION title="6	7" number = "5">
			<S sid ="193" ssid = "1">≥ 8 Fr ❀ En l e x b L M 58 23 10 57 23 11 5 2 1 4 2.</S>
			<S sid ="194" ssid = "2">5 1.</S>
			<S sid ="195" ssid = "3">5 1 1 De ❀ En lex b+ L M 33 24 22 35 25 19 14 5 1.5 13 5 2.5 0.</S>
			<S sid ="196" ssid = "4">5 0.</S>
			<S sid ="197" ssid = "5">5 Table 4: Size (%) of the reordered sequences observed when translating the test set.</S>
			<S sid ="198" ssid = "6">Very similar distributions are observed for both reordering models.</S>
			<S sid ="199" ssid = "7">In parallel, distributions are also comparable to those presented in Figure 3 for reorderings collected from the training bi-text, with the exception of long-range and very short- range reorderings.</S>
			<S sid ="200" ssid = "8">This may be explained by the fact that system models, in special the distortion penalty model, typically prefer monotonic translations, while the system lacks a model to support large-range reorderings.</S>
			<S sid ="201" ssid = "9">5 Conclusions and Further Work.</S>
			<S sid ="202" ssid = "10">We have presented a new reordering model based on bilingual n-grams with units built from linguistic information, aiming at modeling the structural adequacy of translations.</S>
			<S sid ="203" ssid = "11">We compared our new reordering model to the widely used lexical- ized reordering model when implemented in our bilingual n-gram system as well as using Moses, a state-of-the-art phrase-based SMT system.</S>
			<S sid ="204" ssid = "12">Our model obtained slightly higher translation accuracy (BLEU) results.</S>
			<S sid ="205" ssid = "13">We also analysed the quality of the reorderings output by our system when performing the new reordering model, which also outperformed the quality of those output by the system performing lexicalized reordering.</S>
			<S sid ="206" ssid = "14">The back-off procedure used by standard language models allows to dynamically adapt the scope of the context used.</S>
			<S sid ="207" ssid = "15">Therefore, in the case of our reordering model, back-off allows to consider always as much bilingual context (n-grams) as possible.</S>
			<S sid ="208" ssid = "16">The new model was straightforward implemented in our bilingual n-gram system by extending the log-linear combination implemented by our decoder.</S>
			<S sid ="209" ssid = "17">No additional decoding complexity was introduced in the baseline decoding implementation.</S>
			<S sid ="210" ssid = "18">Finally, we showed that mid-range reorderings are present in French-English and GermanEnglish translations and that our reordering model effectively tackles such reorderings.</S>
			<S sid ="211" ssid = "19">However, we saw that long-range reorderings, also present in these tasks, are yet to be addressed.</S>
			<S sid ="212" ssid = "20">We plan to further investigate the use of different structural information, such as supertags, and tags conveying different levels of morphology information (gender, number, tense, mood, etc.) for different language pairs.</S>
	</SECTION>
	<SECTION title="Acknowledgments">
			<S sid ="213" ssid = "21">This work has been partially funded by OSEO under the Quaero program.</S>
	</SECTION>
</PAPER>
