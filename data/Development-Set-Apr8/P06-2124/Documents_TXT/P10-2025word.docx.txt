Word Alignment with Synonym Regularization


Hiroyuki Shindo, Akinori Fujino, and Masaaki Nagata
NTT Communication Science Laboratories, NTT Corp.
2-4 Hikaridai Seika-cho Soraku-gun Kyoto 619-0237 Japan
{shindo,a.fujino}@cslab.kecl.ntt.co.jp 
nagata.masaaki@lab.ntt.co.jp





Abstract

We present a novel framework for word 
alignment that incorporates synonym 
knowledge collected from monolingual 
linguistic resources in a bilingual proba- 
bilistic model.  Synonym information is 
helpful for word alignment because we 
can expect a synonym to correspond to 
the same word in  a  different language. 
We design a generative model for word 
alignment that uses synonym information 
as a regularization term. The experimental 
results show that our proposed method 
significantly improves word alignment 
quality.

1   Introduction

Word alignment is an essential step in most phrase 
and syntax based statistical machine translation 
(SMT). It is an inference problem of word cor- 
respondences between different languages given 
parallel sentence pairs. Accurate word alignment 
can induce high quality phrase detection and trans- 
lation probability, which leads to a significant im- 
provement in SMT performance.   Many word 
alignment approaches based on generative mod- 
els have been proposed and they learn from bilin- 
gual sentences in an unsupervised manner (Vo- 
gel et al., 1996; Och and Ney, 2003; Fraser and 
Marcu, 2007).
  One way to improve word alignment quality 
is to add linguistic knowledge derived from a 
monolingual corpus.   This monolingual knowl- 
edge makes it easier to determine corresponding 
words correctly.  For instance, functional words 
in one language tend to correspond to functional 
words in another language (Deng and Gao, 2007), 
and the syntactic dependency of words in each lan- 
guage can help the alignment process (Ma et al.,
2008).  It has been shown that such grammatical


information works as a constraint in word align- 
ment models and improves word alignment qual- 
ity.
  A large number of monolingual lexical seman- 
tic resources such as WordNet (Miller, 1995) have 
been constructed in more than fifty languages 
(Sagot and  Fiser,  2008).    They include word- 
level relations such as synonyms, hypernyms and 
hyponyms.  Synonym information is particularly 
helpful for word alignment because we can ex- 
pect a synonym to correspond to the same word 
in a different language. In this paper, we explore a 
method for using synonym information effectively 
to improve word alignment quality.
  In general, synonym relations are defined in 
terms of word sense, not in terms of word form. In 
other words, synonym relations are usually con- 
text or domain dependent.  For instance, ‘head’ 
and ‘chief’ are synonyms in contexts referring to 
working environment, while ‘head’ and ‘forefront’ 
are synonyms in contexts referring to physical po- 
sitions. It is difficult, however, to imagine a con- 
text where ‘chief’ and ‘forefront’ are synonyms. 
Therefore, it is easy to imagine that simply replac- 
ing all occurrences of ‘chief’ and ‘forefront’ with
‘head’ do sometimes harm with word alignment 
accuracy, and we have to model either the context 
or senses of words.
  We propose a novel method that incorporates 
synonyms from monolingual resources in a bilin- 
gual word alignment model. We formulate a syn- 
onym pair generative model with a topic variable 
and use this model as a regularization term with a 
bilingual word alignment model.  The topic vari- 
able in our synonym model is helpful for disam- 
biguating the meanings of synonyms. We extend 
HM-BiTAM, which is a HMM-based word align- 
ment model with a latent topic, with a novel syn- 
onym pair generative model. We applied the pro- 
posed method to an English-French word align- 
ment task and successfully improved the word




137

Proceedings of the ACL 2010 Conference Short Papers, pages 137–141, Uppsala, 
Sweden, 11-16 July 2010. Qc 2010 Association for Computational Linguistics



translation probability from e to f under the kth 
topic: p (f |e, z = k ). T = {Ti,i′ } is a state tran-
sition probability of a first order Markov process. 
Fig. 1 shows a graphical model of HM-BiTAM.
The total likelihood of bilingual sentence pairs
{E, F } can be obtained by marginalizing out la-
tent variables z, a and θ,
p (F, E; Ψ) = ∑ ∑ f p (F, E, z, a, θ; Ψ) dθ, 	(1)
z	a




Figure 1: Graphical model of HM-BiTAM


alignment quality.

2   Bilingual Word Alignment Model

In this section, we review a conventional gener- 
ative word alignment model, HM-BiTAM (Zhao 
and Xing, 2008).
  HM-BiTAM is a bilingual generative model 
with topic z, alignment a and topic weight vec- 
tor θ as latent variables.   Topic variables such 
as ‘science’ or ‘economy’ assigned to individual 
sentences help to disambiguate the meanings of 
words.  HM-BiTAM assumes that the nth bilin- 
gual sentence pair, (En , Fn ), is generated under a
given latent topic zn ∈ {1, . . . , k, . . . , K }, where
K is the number of latent topics.  Let N  be the
number of sentence pairs, and In and Jn be the 
lengths of En and Fn , respectively. In this frame-
work, all of the bilingual sentence pairs {E, F } =
{(En , Fn )}n=1  are generated as follows.
1. θ ∼ Dirichlet (α):  sample topic-weight vector

2. For each sentence pair (En , Fn )
(a) zn ∼ M ultinomial (θ): sample the topic
(b) en,i:In  |zn     ∼  p (En |zn ; β ):   sample English
words from a monolingual unigram model given
topic zn
(c) For each position jn = 1, . . . , Jn
i. ajn   ∼ p (ajn  |ajn −1 ; T ): sample an align-


where Ψ = {α, β, T , B} is a parameter set. In
this model, we can infer word alignment a by max-
imizing the likelihood above.

3   Proposed Method

3.1   Synonym Pair Generative Model

We design a generative model for synonym pairs
{f, f ′ } in language F ,  which assumes that the
synonyms are collected from monolingual linguis-
tic resources. We assume that each synonym pair
(f, f ′ ) is generated independently given the same
‘sense’ s.  Under this assumption, the probability 
of synonym pair (f, f ′ ) can be formulated as,
p (f, f ′ ) ∝ ∑ p (f |s ) p (f ′ |s ) p (s) . 	(2)
s

  We define a pair (e, k)  as a representation of 
the sense s, where e and k are a word in a dif- 
ferent language E and a latent topic, respectively. 
It has been shown that a word e in a different 
language is an appropriate representation of s in 
synonym modeling (Bannard and Callison-Burch,
2005). We assume that adding a latent topic k for 
the sense is very useful for disambiguating word 
meaning, and thus that (e, k) gives us a good ap- 
proximation of s. Under this assumption, the syn- 
onym pair generative model can be defined as fol- 
lows.

p ({f, f ′ } ; Ψ� )


ment link ajn  from a first order Markov pro-


∝  ∏ ∑ p(f |e, k; Ψ� )p(f ′ |e, k; Ψ� )p(e, k; Ψ� ), (3)


cess
ii. fjn  ∼ p (fjn  |En , ajn , zn ; B ):  sample a
target  word  fjn  given  an  aligned  source
word and topic

where alignment ajn   = i denotes source word ei
and target word fjn  are aligned.  α is a parame- 
ter over the topic weight vector θ, β  = {βk,e } is
the source word probability given the kth topic:
p (e |z = k ).  B  = {Bf,e,k } represents the word


(f,f ′ ) e,k

where Ψ� is the parameter set of our model.

3.2   Word Alignment with Synonym
Regularization

In this section, we extend the bilingual genera- 
tive model (HM-BiTAM) with our synonym pair 
model.   Our expectation is that synonym pairs



 

Figure 2: Graphical model of synonym pair gen- 
erative process


correspond to the same word in a different lan- 
guage, thus they make it easy to infer accurate 
word alignment.  HM-BiTAM and the synonym 
model share parameters in order to incorporate 
monolingual synonym information into the bilin- 
gual word alignment model. This can be achieved
via reparameterizing Ψ� in eq. 3 as,

(  	)
 
 
p (e, k; Ψ� )  ≡   p (e |k; β ) p (k; α) . 	(5)

  Overall, we re-define the synonym pair model 
with the HM-BiTAM parameter set Ψ,

p({f, f ′ } ; Ψ)


4   Experiments

4.1   Experimental Setting

For an empirical evaluation of the proposed 
method, we used a bilingual parallel corpus of 
English-French Hansards (Mihalcea and Pedersen,
2003). The corpus consists of over 1 million sen- 
tence pairs, which include 447 manually word- 
aligned sentences. We selected 100 sentence pairs 
randomly from the manually word-aligned sen- 
tences as development data for tuning the regu- 
larization weight ζ , and used the 347 remaining 
sentence pairs as evaluation data.   We also ran- 
domly selected 10k, 50k, and 100k sized sentence 
pairs from the corpus as additional training data. 
We ran the unsupervised training of our proposed 
word alignment model on the additional training 
data and the 347 sentence pairs of the evaluation 
data.   Note that manual word alignment of the
347 sentence pairs was not used for the unsuper- 
vised training. After the unsupervised training, we 
evaluated the word alignment performance of our 
proposed method by comparing the manual word 
alignment of the 347 sentence pairs with the pre- 
diction provided by the trained model.
  We collected English and French synonym pairs 
from WordNet 2.1 (Miller, 1995) and WOLF 0.1.4 
(Sagot and Fiser, 2008), respectively. WOLF is a 
semantic resource constructed from the Princeton


1 	∏ ∑
∝ ∑



αk βk,e Bf,e,k Bf ′ ,e,k .  (6)


WordNet and various 
multilingual resources. We
selected synonym pairs 
where both words were in-


k′ αk′   (f,f ′ ) k,e

  Fig. 2 shows a graphical model of the synonym 
pair generative process.  We estimate the param- 
eter values to maximize the likelihood of HM- 
BiTAM with respect to bilingual sentences and 
that of the synonym model with respect to syn- 
onym pairs collected from monolingual resources. 
Namely, the parameter estimate, Ψˆ , is computed 
as

Ψˆ  = arg max {log p(F, E; Ψ) + ζ log p({f, f ′ } ; Ψ)} ,
Ψ
(7)
  where ζ is a regularization weight that should 
be set for training. We can expect that the second 
term of eq.  7 to constrain parameter set Ψ and 
avoid overfitting for the bilingual word alignment 
model. We resort to the variational EM approach 
(Bernardo et al., 2003) to infer Ψˆ  following HM- 
BiTAM. We omit the parameter update equation 
due to lack of space.



cluded in the bilingual training set.
  We compared the word alignment performance 
of our model with that of GIZA++ 1.03 1  (Vo- 
gel et al., 1996; Och and Ney, 2003), and HM- 
BiTAM (Zhao and Xing, 2008) implemented by 
us. GIZA++ is an implementation of IBM-model
4 and HMM, and HM-BiTAM corresponds to ζ =
0 in eq. 7. We adopted K = 3 topics, following 
the setting in (Zhao and Xing, 2006).
  We trained the word alignment in two direc- 
tions: English to French, and French to English. 
The alignment results for both directions were re- 
fined with ‘GROW’ heuristics to yield high preci- 
sion and high recall in accordance with previous 
work (Och and Ney, 2003; Zhao and Xing, 2006). 
We evaluated these results for precision, recall, F- 
measure and alignment error rate (AER), which 
are standard metrics for word alignment accuracy 
(Och and Ney, 2000).

1 http://fjoch.com/GIZA++.html



1
0
k
Prec
isio
n
Reca
ll
F-
mea
sure
AE
R
GI
Z
A
++
 sta
ndar
d 
with 
SR
H
0.
85
6
0.
87
4
0.71
8
0.72
0
0.
7
8
1
0.
7
8
9
0.20
7
0.19
8
HM-
BiT
AM
stan
dar
d
with 
SR
H
0.
86
9
0.
88
4
0.78
8
0.79
0
0.
8
2
6
0.
8
3
4
0.16
9
0.16
0
Pr
op
os
ed
0.
94
1
0.80
8
0.
8
7
0
0.12
3


(a)

5
0
k
Prec
isio
n
Reca
ll
F-
mea
sure
AE
R
GI
Z
A
++
 sta
ndar
d 
with 
SR
H
0.
90
5
0.
90
3
0.77
0
0.75
9
0.
8
3
2
0.
8
2
5
0.15
6
0.16
4
HM-
BiT
AM
stan
dar
d
with 
SR
H
0.
90
1
0.
89
9
0.81
4
0.80
8
0.
8
5
5
0.
8
5
3
0.14
0
0.14
5
Pr
op
os
ed
0.
94
7
0.82
4
0.
8
8
1
0.11
2
(b)

1
0
0
k
Prec
isio
n
Reca
ll
F-
mea
sure
AE
R
GI
Z
A
++
 sta
ndar
d 
with 
SR
H
0.
92
5
0.
93
4
0.79
1
0.80
3
0.
8
5
3
0.
8
6
4
0.13
6
0.12
6
HM-
BiT
AM
stan
dar
d
with 
SR
H
0.
89
8
0.
90
9
0.85
1
0.86
0
0.
8
7
4
0.
8
7
9
0.12
4
0.11
4
Pr
op
os
ed
0.
92
7
0.86
2
0.
8
9
3
0.10
3
(c)


Table 1: Comparison of word alignment accuracy. 
The best results are indicated in bold type.  The 
additional data set sizes are (a) 10k, (b) 50k, (c)
100k.



4.2   Results and Discussion

Table 1 shows the word alignment accuracy of the 
three methods trained with 10k, 50k, and 100k ad- 
ditional sentence pairs.  For all settings, our pro- 
posed method outperformed other conventional 
methods.  This result shows that synonym infor- 
mation is effective for improving word alignment 
quality as we expected.
  As mentioned in Sections 1 and 3.1, the main 
idea of our proposed method is to introduce la- 
tent topics for modeling synonym pairs, and then 
to utilize the synonym pair model for the regu- 
larization of word alignment models.  We expect 
the latent topics to be useful for modeling poly- 
semous words included in synonym pairs and to 
enable us to incorporate synonym information ef- 
fectively into word alignment models.  To con- 
firm the effect of the synonym pair model with 
latent topics, we also tested GIZA++ and HM- 
BiTAM with what we call Synonym Replacement 
Heuristics (SRH), where all of the synonym pairs 
in the bilingual training sentences were simply re- 
placed with a representative word.  For instance, 
the words ‘sick’ and ‘ill’ in the bilingual sentences


Table 2: The number of vocabularies in the 10k,
50k and 100k data sets.


were replaced with the word ‘sick’. As shown in 
Table 2, the number of vocabularies in the English 
and French data sets decreased as a result of em- 
ploying the SRH.
  We show the performance of GIZA++ and HM- 
BiTAM with the SRH in the lines entitled “with 
SRH” in Table 1. The GIZA++ and HM-BiTAM 
with the SRH slightly outperformed the standard 
GIZA++ and HM-BiTAM for the 10k and 100k 
data sets, but underperformed with the 50k data 
set. We assume that the SRH mitigated the over- 
fitting of these models into low-frequency word 
pairs in bilingual sentences, and then improved the 
word alignment performance.  The SRH regards 
all of the different words coupled with the same 
word in the synonym pairs as synonyms. For in- 
stance, the words ‘head’, ‘chief’ and ‘forefront’ in 
the bilingual sentences are replaced with ‘chief’, 
since (‘head’, ‘chief’) and (‘head’, ‘forefront’) are 
synonyms.   Obviously, (‘chief’, ‘forefront’) are 
not synonyms, which is detrimented to word align- 
ment.
  The proposed method consistently outper- 
formed GIZA++ and HM-BiTAM with the SRH 
in 10k,  50k and 100k data sets in F-measure. 
The synonym pair model in our proposed method 
can automatically learn that (‘head’, ‘chief’) and 
(‘head’, ‘forefront’) are individual synonyms with 
different meanings by assigning these pairs to dif- 
ferent topics.   By sharing latent topics between 
the synonym pair model and the word alignment 
model, the synonym information incorporated in 
the synonym pair model is used directly for train- 
ing word alignment model. The experimental re- 
sults show that our proposed method was effec- 
tive in improving the performance of the word 
alignment model by using synonym pairs includ- 
ing such ambiguous synonym words.
  Finally, we discuss the data set size used for un- 
supervised training.  As shown in Table 1, using 
a large number of additional sentence pairs im- 
proved the performance of all the models.  In all 
our experimental settings, all the additional sen-


tence pairs and the evaluation data were selected 
from the Hansards data set.  These experimental 
results show that a larger number of sentence pairs 
was more effective in improving word alignment 
performance when the sentence pairs were col- 
lected from a homogeneous data source. However, 
in practice, it might be difficult to collect a large 
number of such homogeneous sentence pairs for 
a specific target domain and language pair.  One 
direction for future work is to confirm the effect 
of the proposed method when training the word 
alignment model by using a large number of sen- 
tence pairs collected from various data sources in- 
cluding many topics for a specific language pair.

5   Conclusions and Future Work

We proposed a novel framework that incorpo- 
rates synonyms from monolingual linguistic re- 
sources in a word alignment generative model. 
This approach utilizes both bilingual and mono- 
lingual synonym resources effectively for word 
alignment.  Our proposed method uses a latent 
topic for bilingual sentences and monolingual syn- 
onym pairs, which is helpful in terms of word 
sense disambiguation. Our proposed method im- 
proved word alignment quality with both small 
and large data sets.  Future work will involve ex- 
amining the proposed method for different lan- 
guage pairs such as English-Chinese and English- 
Japanese and evaluating the impact of our pro- 
posed method on SMT performance. We will also 
apply our proposed method to a larger data sets 
of multiple domains since we can expect a fur- 
ther improvement in word alignment accuracy if 
we use more bilingual sentences and more mono- 
lingual knowledge.



References

C. Bannard and C. Callison-Burch. 2005.  Paraphras- 
ing with bilingual parallel corpora.    In Proceed- 
ings of the 43rd Annual Meeting on Association for 
Computational Linguistics, pages 597–604. Asso- 
ciation for Computational Linguistics Morristown, 
NJ, USA.

J. M. Bernardo, M. J. Bayarri, J. O. Berger,  A. P.
Dawid, D. Heckerman, A. F. M. Smith, and M. West.
2003. The variational bayesian EM algorithm for in- 
complete data: with application to scoring graphical 
model structures. In Bayesian Statistics 7: Proceed- 
ings of the 7th Valencia International Meeting, June
2-6, 2002, page 453. Oxford University Press, USA.



Y. Deng and Y. Gao.  2007.  Guiding statistical word 
alignment models with prior knowledge.  In Pro- 
ceedings of the 45th Annual Meeting of the As- 
sociation of Computational Linguistics, pages 1–8, 
Prague, Czech Republic, June. Association for Com- 
putational Linguistics.

A. Fraser and D. Marcu.   2007.   Getting the struc- 
ture right for word alignment:  LEAF.   In Pro- 
ceedings of the 2007 Joint Conference on Empirical 
Methods in Natural Language Processing and Com- 
putational Natural Language Learning (EMNLP- 
CoNLL), pages 51–60, Prague, Czech Republic, 
June. Association for Computational Linguistics.

Y. Ma, S. Ozdowska, Y. Sun, and A. Way.	2008.
Improving word alignment using syntactic depen- 
dencies.  In Proceedings of the ACL-08: HLT Sec- 
ond Workshop on Syntax and Structure in Statisti- 
cal Translation (SSST-2), pages 69–77, Columbus, 
Ohio, June. Association for Computational Linguis- 
tics.

R. Mihalcea and T. Pedersen.   2003.   An evaluation 
exercise for word alignment. In Proceedings of the 
HLT-NAACL 2003 Workshop on building and using 
parallel texts: data driven machine translation and 
beyond-Volume 3, page 10. Association for Compu- 
tational Linguistics.

G. A. Miller.  1995.  WordNet: a lexical database for
English. Communications of the ACM, 38(11):41.

F. J. Och and H. Ney. 2000. Improved statistical align- 
ment models.   In Proceedings of the 38th Annual 
Meeting on Association for Computational Linguis- 
tics, pages 440–447. Association for Computational 
Linguistics Morristown, NJ, USA.

F. J. Och and H. Ney. 2003. A systematic comparison 
of various statistical alignment models.  Computa- 
tional Linguistics, 29(1):19–51.

B. Sagot and D. Fiser.  2008.  Building a free French 
wordnet from multilingual resources.  In Proceed- 
ings of Ontolex.

S. Vogel, H. Ney, and C. Tillmann.  1996.  HMM- 
based word alignment in statistical translation.  In 
Proceedings of the 16th Conference on Computa- 
tional Linguistics-Volume 2, pages 836–841. Asso- 
ciation for Computational Linguistics Morristown, 
NJ, USA.

B. Zhao and E. P. Xing.   2006.   BiTAM: Bilingual 
topic admixture models for word alignment. In Pro- 
ceedings of the COLING/ACL on Main Conference 
Poster Sessions, page 976. Association for Compu- 
tational Linguistics.

B. Zhao and E. P. Xing. 2008. HM-BiTAM: Bilingual 
topic exploration, word alignment, and translation. 
In Advances in Neural Information Processing Sys- 
tems 20, pages 1689–1696, Cambridge, MA. MIT 
Press.

