Bayesian Word Alignment for Statistical Machine Translation





Cos¸kun Mermer1,2
1BILGEM 
TUBITAK
Gebze 41470 Kocaeli, Turkey
coskun@uekae.tubitak.gov.tr


Murat Sarac¸lar2
2Electrical and Electronics Eng. Dept.
Bogazici University  Bebek 34342 
Istanbul, Turkey 
murat.saraclar@boun.edu.tr










Abstract

In this work, we compare the translation 
performance  of  word alignments obtained 
via Bayesian inference to those obtained via 
expectation-maximization (EM). We propose 
a Gibbs sampler for fully Bayesian inference 
in IBM Model 1, integrating over all possi- 
ble parameter values in finding the alignment 
distribution.  We show that Bayesian inference 
outperforms EM in all of the tested language 
pairs, domains and data set sizes, by up to 2.99
BLEU points. We also show that the proposed 
method effectively addresses the well-known 
rare word problem in EM-estimated models; 
and at the same time induces a much smaller 
dictionary of bilingual word-pairs.


1   Introduction

Word alignment is a crucial early step in the training 
of most statistical machine translation (SMT) sys- 
tems, in which the estimated alignments are used for 
constraining  the set of candidates in phrase/grammar 
extraction (Koehn et al., 2003; Chiang, 2007; Galley 
et al., 2006). State-of-the-art word alignment mod- 
els, such as IBM Models (Brown et al., 1993), HMM 
(Vogel et al., 1996), and the jointly-trained symmet- 
ric HMM (Liang et al., 2006), contain a large num- 
ber of parameters (e.g., word translation probabili- 
ties) that need to be estimated in addition to the de- 
sired hidden alignment variables.
  The most common method of inference in such 
models is expectation-maximization (EM) (Demp- 
ster et al., 1977) or an approximation  to EM when 
exact EM is intractable. However,  being  a maxi-


mization (e.g., maximum likelihood (ML) or max- 
imum a posteriori (MAP)) technique, EM is gen- 
erally prone to local optima and overfitting.  In 
essence, the alignment distribution obtained via EM 
takes into account only the most likely point in the 
parameter space, but does not consider contributions 
from other points.
  Problems  with the standard  EM estimation  of 
IBM Model 1 was pointed out by Moore (2004) and 
a number of heuristic changes to the estimation pro- 
cedure, such as smoothing  the parameter estimates, 
were shown to reduce the alignment error rate, but 
the effects on translation performance was not re- 
ported.  Zhao and Xing (2006) note that the param- 
eter estimation (for which they use variational EM) 
suffers from data sparsity and use symmetric  Dirich- 
let priors, but they find the MAP solution.
  Bayesian inference,  the approach in this paper, 
have recently been applied to several unsupervised 
learning problems in NLP (Goldwater and Griffiths,
2007; Johnson et al., 2007) as well as to other tasks 
in SMT such as synchronous grammar induction 
(Blunsom et al., 2009) and learning  phrase align- 
ments directly (DeNero et al., 2008).
  Word alignment learning problem was addressed 
jointly  with  segmentation  learning in  Xu  et al. 
(2008), Nguyen et al. (2010), and Chung and Gildea 
(2009). The former two works place nonparametric 
priors (also known as cache models) on the param- 
eters and utilize Gibbs sampling. However, align- 
ment inference in neither of these works is exactly 
Bayesian since the alignments  are updated by run- 
ning GIZA++ (Xu et al., 2008) or by local maxi- 
mization (Nguyen et al., 2010). On the other hand,



182


Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 182–187, 
Portland, Oregon, June 19-24, 2011. Qc 2011 Association for Computational Linguistics


Chung and Gildea (2009) apply a sparse Dirichlet 
prior on the multinomial parameters to prevent over- 
fitting. They use variational Bayes for inference, but 
they do not investigate the effect of Bayesian infer- 
ence to word alignment in isolation. Recently, Zhao 
and Gildea (2010) proposed fertility extensions to 
IBM Model 1 and HMM, but they do not place any 
prior on the parameters and their inference method is 
actually stochastic EM (also known as Monte Carlo 
EM), a ML technique in which sampling is used to


fj is associated with a hidden alignment variable aj 
whose value ranges over the word positions in the 
corresponding  source sentence.   The set of align- 
ments for a sentence (corpus)  is denoted by a (A).
The model parameters consist  of a  VE  × VF  ta-
ble T of word translation probabilities  such that
te,f = P (f |e).
The joint distribution of the Model-1 variables is
given by the following generative model3:
n


approximate the expected counts in the E-step. Even 
though they report substantial reductions in align-


P (E, F, A; T) =


P (e)P (a|e)P (f |a, e; T) (1)
s
J


ment error rate, the translation BLEU scores do not
improve.
Our approach in this paper is fully Bayesian in


= n	P (e)
(I + 1)J
s


n t
j=1



eaj ,fj



(2)


which the alignment  probabilities  are inferred by 
integrating over all possible  parameter  values  as- 
suming an intuitive,   sparse prior.   We  develop  a 
Gibbs sampler for alignments under IBM Model 1,
  

In the proposed Bayesian setting, we treat T as a 
random variable with a prior P (T).  To find a suit- 
able prior for T, we re-write (2) as:


(e)


VE   VF


which is relevant for the state-of-the-art SMT sys-
tems since: (1) Model 1 is used in bootstrapping 
the parameter settings for EM training of higher-


P (E, F, A|T) = n
s


P
(I + 1)J


n n (t
e=1 f =1



e,f


)ne,f  (3)


VE   VF


P (e)


order alignment models, and (2) many state-of-the-


= n n (te,f )Ne,f  n


J (4)


art SMT systems use Model 1 translation probabil-
ities as features in their log-linear model. We eval-



e=1 f =1


(I + 1)
s


uate the inferred alignments in terms of the end-to- 
end translation performance, where we show the re- 
sults with a variety of input data to illustrate the gen- 
eral applicability of the proposed technique. To our 
knowledge, this is the first work to directly investi- 
gate the effects of Bayesian alignment inference on 
translation performance.

2   Bayesian Inference with IBM Model 1

Given a sentence-aligned parallel corpus (E, F), let 
ei  (fj ) denote the i-th (j-th) source (target)1  word 
in e (f ), which in turn consists of I (J ) words and 
denotes the s-th sentence in E (F).2   Each source 
sentence is also hypothesized to have an additional 
imaginary “null” word e0. Also let VE (VF ) denote 
the size of the observed source (target) vocabulary.
In Model 1 (Brown  et al., 1993), each target word

  1 We use the “source”  and “target” labels following the gen- 
erative process, in which E generates F (cf. Eq. 1).
2 Dependence of the sentence-level variables e, f , I , J (and


where in (3) the count variable ne,f   denotes  the 
number of times the source word type e is aligned 
to the target word type f in the sentence-pair s, and
in (4) Ne,f   = L.s  ne,f . Since the distribution  over
{te,f } in (4) is in the exponential family, specifically
being a multinomial distribution, we choose the con-
jugate prior, in this case the Dirichlet distribution, 
for computational convenience.
  For each source word type e, we assume the prior 
distribution for te   = te,1 · · · te,VF , which is itself
a  distribution over the target vocabulary,  to be a

Dirichlet distribution (with its own set of hyperpa- 
rameters Θe  = θe,1 · · · θe,VF ) independent from the
priors of other source word types:

te  ∼ Dirichlet(te; Θe)
fj |a, e, T ∼ Multinomial(fj ; teaj )

  We choose symmetric Dirichlet  priors identically 
for all source words e with θe,f   = θ = 0.0001 to 
obtain a sparse Dirichlet prior. A sparse prior  favors


a and n, which are introduced later) on the sentence index s	 	


should be understood even though not explicitly indicated for 
notational simplicity.
  

3 We omit P (J |e) since both J and e are observed and so 
this term does not affect the inference of hidden variables.


distributions that peak at a single  target word and


Input: E, F;  Output: K samples of A


pen
aliz
es 
flat
ter 
tran
slat
ion 
dist
ribu
tion
s,  
eve
n 
for
1
Ini
tia
liz
e 
A
rare 
wo
rds
. 
Thi
s 
cho
ice 
add
res
ses 
the 
wel
l-
kn
ow
n
2
fo
r k 
= 
1 
to 
K 
d
o
pro
ble
m 
in 
the 
IB
M 
Mo
del
s,  
and 
mor
e 
sev
erel
y 
in
3
f
o
r
 
e
a
c
h
 
s
e
n
t
e
n
c
e
-
p
a
i
r
 
s
 
i
n
 
(
E
,
 
F
)
 
d
o
Mo
del 
1, 
in 
whi
ch 
rare 
wor
ds  
act 
as 
“gar
bag
e  
col
-
4
f
o
r
 
j
 
=
 
1
 
t
o
 
J
 
d
o
lect
ors
” 
(Oc
h 
and 
Ne
y, 
200
3) 
and 
get 
assi
gne
d 
ex-
5
f
o
r
 
i
 
=
 
0
 
t
o
 
I
 
d
o
ces
sive
ly 
larg
e 
nu
mb
er 
of 
wor
d 
ali
gn
me
nts
.
T
h
e
n 
w
e
 
o
b
t
a
i
n 
t
h
e
 
j
o
i
n
t 
d
i
s
t
ri
b
u
ti
o
n 
o
f 
a
ll 
(
o
b
-
6
C
a
l
c
u
l
a
t
e
 
P
 
(
a
j
  
=
 
i
|
 
·
 
·
 
·
 
)
a
c
c
o
r
d
i
n
g
 
t
o
 
(
7
)
ser
ved 
+ 
hid
den
) 
vari
abl
es 
as:
7
S
a
m
p
l
e
 
a
 
n
e
w
 
v
a
l
u
e
 
f
o
r
 
a
j


P (E, F, A, T; Θ) = P (T; Θ) P (E, F, A|T)  (5)
where Θ = Θ1 · · · ΘVE .
To infer the posterior distribution of the align-
ments,  we use  Gibbs sampling  (Geman and Ge- 
man, 1984). One possible method is to derive the 
Gibbs sampler from P (E, F, A, T; Θ) obtained in 
(5) and sample the unknowns A and T in turn, re- 
sulting in an explicit Gibbs sampler.  In this work, 
we marginalize out T by:
r



Table 1: Gibbs sampling algorithm for IBM Model 1 (im- 
plemented in the accompanying software).


3   Experimental Setup
For Turkish↔English   experiments,  we used  the
20K-sentence travel domain BTEC dataset (Kikui
et al., 2006) from the yearly IWSLT evaluations6 
for training,  the CSTAR  2003 test set for develop- 
ment, and the IWSLT 2004 test set for testing7. For
Czech↔English, we used the 95K-sentence news


P (E, F, A; Θ) =


P (E, F, A, T; Θ) 	(6)
T



commentary parallel 
corpus from the WMT 
shared


and obtain a collapsed  Gibbs sampler, which sam- 
ples only the alignment variables.
  Using P (E, F, A; Θ) obtained in (6), the Gibbs 
sampling formula for the individual alignments is 
derived as:4
P (aj  = i|E, F, Aj ; Θ)


task8 for training, news2008  set for development,
news2009  set for testing, and the 438M-word En- 
glish and 81.7M-word  Czech monolingual news cor- 
pora for additional language model (LM) training.
For Arabic↔English,  we used  the 65K-sentence
LDC2004T18  (news from 2001-2004) for training,
the AFP portion of LDC2004T17 (news from 1998,


j
=	ei ,fj


+ θei ,fj



(7)


single reference) for 
development and testing 
(about


L.VF	j	VF
f =1 N  ,f + L.f =1 θei ,f
where the superscript j  denotes the exclusion of
the current value of aj .
  The algorithm is given in Table 1. Initialization 
of A in Step 1 can be arbitrary, but for faster conver- 
gence special initializations have been used, e.g., us- 
ing the output of EM (Chiang et al., 2010). Once the 
Gibbs sampler is deemed to have converged after B 
burn-in iterations, we collect M samples of A with
L iterations in-between5 to estimate P (A|E, F). To
obtain the Viterbi alignments, which are required for
phrase extraction (Koehn et al., 2003), we select for 
each aj  the most frequent value in the M collected 
samples.

  4 The derivation is quite standard  and similar to other 
Dirichlet-multinomial Gibbs sampler derivations, e.g. (Resnik 
and Hardisty, 2010).
5 A lag is introduced to reduce correlation  between samples.


875 sentences each),  and the 298M-word English
and 215M-word Arabic AFP and Xinhua subsets of 
the respective Gigaword corpora (LDC2007T07 and 
LDC2007T40) for additional LM training. All lan- 
guage models are 4-gram in the travel domain exper- 
iments and 5-gram in the news domain experiments.
  For each  language  pair,  we trained standard 
phrase-based SMT systems in both directions (in- 
cluding alignment  symmetrization  and log-linear 
model tuning) using Moses (Koehn  et al., 2007), 
SRILM  (Stolcke,   2002), and ZMERT (Zaidan,
2009) tools and evaluated using BLEU (Papineni et 
al., 2002). To obtain word alignments, we used the 
accompanying Perl code for Bayesian inference and

  6 International Workshop on Spoken Language Translation. 
http://iwslt2010.fbk.eu
7 Using only the first English reference for symmetry.
     8 Workshop          on           Machine           Translation. 
http://www.statmt.org/wmt10/translation-task.html



Met
hod
T
E
E
T
C
E
E
C
A
E
E
A
E
M-
5
EM
-80
38.
91
39.
19
26.
52
26.
47
14.
62
14.
95
10.
07
10.
69
15.
50
15.
66
15.
17
15.
02
G
S
-
N
 
G
S
-
5
GS
-80
41.
14
40.
63
41.
78
27.
55
27.
24
29.
51
14.
99
15.
45
15.
01
10.
85
10.
57
10.
68
14.
64
16.
41
15.
92
15.
89
15.
82
16.
02
M
4
39.
94
27.
47
15.
47
11.
15
16.
46
15.
43

Table 2: BLEU scores in translation experiments. E: En- 
glish, T: Turkish, C: Czech, A: Arabic.





GIZA++ (Och and Ney, 2003) for EM.
  For each translation  task, we report two EM es- 
timates, obtained after 5 and 80 iterations (EM-5 
and EM-80), respectively; and three Gibbs sampling 
estimates, two of which were initialized with those 
two EM Viterbi alignments (GS-5 and GS-80) and a 
third was initialized naively9 (GS-N). Sampling set-
tings were B = 400 for T↔E, 4000 for C↔E and
8000 for A↔E; M  = 100, and L = 10. For refer-
ence, we also report the results with IBM Model 4
alignments (M4) trained in the standard bootstrap- 
ping regimen of 15H 53343.

4   Results

Table 2 compares the BLEU scores of Bayesian in- 
ference and EM estimation. In all translation tasks, 
Bayesian inference outperforms EM. The improve- 
ment range is from 2.59 (in Turkish-to-English) 
up to 2.99 (in English-to-Turkish) BLEU points in 
travel domain and from 0.16 (in English-to-Czech) 
up to 0.85 (in English-to-Arabic) BLEU points in 
news domain. Compared to the state-of-the-art IBM 
Model 4, the Bayesian Model 1 is better in all travel 
domain tasks and is comparable or better in the news 
domain.
  Fertility of a source word is defined  as the num- 
ber of target words aligned to it. Table 3 shows the 
distribution of fertilities in alignments obtained from 
different methods. Compared to EM estimation, in- 
cluding Model 4, the proposed Bayesian inference 
dramatically  reduces “questionable” high-fertility (4
≤ fertility ≤ 7) alignments and almost entirely elim-



Table 3: Distribution of inferred alignment fertilities. 
The four blocks of rows from top to bottom correspond 
to (in order) the total number of source tokens, source 
tokens with fertilities in the range 4–7, source tokens 
with fertil- ities higher than 7, and the maximum  
observed fertility. The first language listed is the source 
in alignment (Sec- tion 2).


Met
hod
T
E
E
T
C
E
E
C
A
E
E
A
EM-
80
52.
5K
38.
5K
44
0K
46
1K
38
3K
38
8K
M
4
57.
6K
40.
5K
43
9K
44
1K
42
2K
40
5K
GS-
80
23.
5K
25.
4K
18
0K
20
9K
15
8K
17
6K

Table 4: Sizes of bilingual dictionaries induced by differ- 
ent alignment methods.


inates “excessive” alignments (fertility ≥ 
8)10.
The number of distinct word-pairs induced by an
alignment has been recently proposed as an objec- 
tive function for word alignment (Bodrumlu et 
al.,
2009).  Small dictionary  sizes are preferred  
over large ones. Table 4 shows that the 
proposed in- ference method substantially  reduces 
the alignment dictionary size, in most cases by 
more than 50%.

5   Conclusion

We developed a Gibbs sampling-based Bayesian in- 
ference method for IBM Model 1 word alignments 
and showed that it outperforms EM estimation in 
terms of translation BLEU scores across several 
lan- guage pairs,  data sizes and domains. As a 
result of this increase, Bayesian Model 1 
alignments per- form close to or better than the 
state-of-the-art IBM


9 Each target word was aligned to the source candidate that	 	


co-occured the most number of times with that target word in 
the entire parallel corpus.
  

10 The GIZA++ implementation of Model 4 artificially limits 
fertility parameter values to at most nine.


Model 4. The proposed method learns a compact, 
sparse translation distribution, overcoming the well- 
known “garbage collection”  problem of rare words 
in EM-estimated current models.

Acknowledgments

Murat Sarac¸lar is supported by the TU¨ BA-GEB˙IP
award.


References

Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os- 
borne.	2009.   A Gibbs sampler  for phrasal  syn- 
chronous grammar induction. In Proceedings of the 
Joint Conference of the 47th Annual Meeting of the 
ACL and the 4th International Joint Conference on 
Natural Language Processing of the AFNLP, pages
782–790, Suntec, Singapore, August.
Tugba Bodrumlu, Kevin Knight, and Sujith Ravi. 2009.
A new objective function for word alignment. In Pro- 
ceedings of the NAACL HLT Workshop on Integer Lin- 
ear Programming for Natural Language Processing, 
pages 28–35, Boulder,  Colorado,  June. Association for 
Computational Linguistics.
Peter F. Brown, Vincent J.  Della Pietra, Stephen  A.
Della Pietra, and Robert L. Mercer. 1993. The mathe- 
matics of statistical machine translation: parameter es- 
timation. Computational Linguistics, 19(2):263–311.
David Chiang,  Jonathan Graehl,  Kevin  Knight, Adam 
Pauls,  and Sujith Ravi.  2010.  Bayesian inference 
for finite-state transducers. In Human Language Tech- 
nologies: The 2010 Annual Conference of the North 
American Chapter of the Association for Computa- 
tional Linguistics, pages 447–455, Los Angeles, Cali- 
fornia, June.
David Chiang. 2007. Hierarchical phrase-based transla- 
tion. Computational Linguistics, 33(2):201–228.
Tagyoung Chung and Daniel Gildea. 2009. Unsuper- 
vised tokenization for machine translation. In Pro- 
ceedings of the 2009 Conference on Empirical Meth- 
ods in Natural  Language Processing, pages 718–726, 
Singapore, August.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Max- 
imum likelihood from incomplete data via the EM al- 
gorithm. Journal of the Royal Statistical  Society, Se- 
ries B, 39(1):1–38.
John DeNero, Alexandre Bouchard-Coˆ te´, and Dan Klein.
2008. Sampling alignment structure under a Bayesian 
translation model. In Proceedings of the 2008 Confer- 
ence on Empirical Methods in Natural Language Pro- 
cessing, pages 314–323, Honolulu, Hawaii, October.


Michel Galley, Jonathan Graehl,  Kevin Knight, Daniel 
Marcu, Steve  DeNeefe, Wei  Wang, and Ignacio 
Thayer.  2006.  Scalable inference  and training of 
context-rich syntactic translation models. In Proceed- 
ings of the 21st International  Conference on Computa- 
tional Linguistics and 44th Annual Meeting of the As- 
sociation for Computational  Linguistics,  pages 961–
968, Sydney, Australia, July.
Stuart Geman and Donald Geman. 1984. Stochastic re- 
laxation, Gibbs distributions,  and the Bayesian restora- 
tion of images. IEEE Transactions On Pattern Analy- 
sis And Machine Intelligence, 6(6):721–741, Novem- 
ber.
Sharon Goldwater  and Tom Griffiths.  2007.  A fully 
Bayesian approach to unsupervised part-of-speech tag- 
ging. In Proceedings of the 45th Annual Meeting of the 
Association of Computational Linguistics, pages 744–
751, Prague, Czech Republic, June.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa- 
ter. 2007. Bayesian inference for PCFGs via Markov 
chain Monte Carlo. In Human Language Technologies
2007: The Conference of the North American Chap- 
ter of the Association for Computational Linguistics, 
pages 139–146, Rochester, New York, April.
Genichiro	Kikui,     Seiichi   Yamamoto,   Toshiyuki 
Takezawa,   and Eiichiro Sumita.    2006.    Com- 
parative  study on corpora for  speech translation. 
IEEE Transactions on Audio, Speech and Language 
Processing, 14(5):1674–1682.
Philipp Koehn, Franz Josef  Och, and Daniel Marcu.
2003. Statistical phrase-based translation.  In Proceed- 
ings of HLT-NAACL 2003, Main Papers, pages 48–54, 
Edmonton, May-June.
Philipp Koehn, Hieu Hoang, Alexandra  Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, Richard 
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con- 
stantin, and Evan Herbst. 2007. Moses: open source 
toolkit for statistical machine translation. In Proceed- 
ings of the 45th Annual Meeting of the Association for 
Computational Linguistics Companion Volume Pro- 
ceedings of the Demo and Poster Sessions, pages 177–
180, Prague, Czech Republic, June.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align- 
ment by agreement. In Proceedings of the Human 
Language Technology Conference of the NAACL, Main 
Conference,  pages 104–111,  New York City, USA, 
June.
Robert C. Moore. 2004. Improving IBM word alignment 
Model 1. In Proceedings of the 42nd Meeting of the 
Association for Computational Linguistics (ACL’04), 
Main Volume, pages 518–525, Barcelona, Spain, July. 
ThuyLinh  Nguyen, Stephan Vogel, and Noah A. Smith.
2010.   Nonparametric  word segmentation  for ma-


chine translation. In Proceedings of the 23rd Interna- 
tional Conference on Computational Linguistics (Col- 
ing 2010), pages 815–823, Beijing, China, August.
Franz Josef Och and Hermann Ney.  2003. A system- 
atic comparison of various statistical alignment mod- 
els. Computational Linguistics, 29(1):19–51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei- 
Jing Zhu. 2002. BLEU: a method for automatic eval- 
uation of machine translation.  In Proceedings of 40th 
Annual Meeting of the Association for Computational 
Linguistics,  pages 311–318, Philadelphia,  Pennsylva- 
nia, USA, July.
Philip Resnik and Eric Hardisty. 2010. Gibbs sampling 
for the uninitiated.  University of Maryland Computer 
Science Department;  CS-TR-4956, June.
Andreas Stolcke. 2002. SRILM – an extensible language 
modeling toolkit. In Seventh International Conference 
on Spoken Language Processing, volume 3.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans- 
lation. In COLING, pages 836–841.
Jia Xu,  Jianfeng  Gao, Kristina Toutanova,  and Her- 
mann Ney. 2008. Bayesian semi-supervised Chinese 
word segmentation for statistical machine translation. 
In Proceedings of the 22nd International  Conference 
on Computational Linguistics (Coling 2008), pages
1017–10124, Manchester, UK, August.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable 
open source tool for minimum error rate training of 
machine translation systems.  The Prague Bulletin of 
Mathematical Linguistics, 91(1):79–88.
Shaojun Zhao and Daniel Gildea. 2010. A fast fertil- 
ity hidden Markov model for word alignment using 
MCMC.  In Proceedings of the 2010 Conference on 
Empirical Methods in Natural Language Processing, 
pages 596–605, Cambridge,  MA, October.
Bing Zhao and Eric P. Xing.  2006. BiTAM: Bilingual 
topic admixture models for word alignment. In Pro- 
ceedings of the COLING/ACL  2006 Main Conference 
Poster  Sessions, pages 969–976, Sydney, Australia, 
July. Association for Computational Linguistics.

