Citance Number: 1 | Reference Article:  P06-2124.txt | Citing Article:  D10-1005.txt | Citation Marker Offset:  ['184'] | Citation Marker:  Zhao and Xing, 2006 | Citation Offset:  ['184'] | Citation Text:  <S sid ="184" ssid = "55">Other multilingual topic models require parallel text, either at the document (Ni et al., 2009; Mimno et al., 2009) or word-level (Kim and Khudanpur, 2004; Zhao and Xing, 2006).</S> | Reference Offset:  ['3','15'] | Reference Text:  <S sid ="3" ssid = "3">Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels).</S><S sid ="15" ssid = "15">In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of document- pairs.</S> | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka |


Citance Number: 2 | Reference Article:  P06-2124.txt | Citing Article:  D11-1084.txt | Citation Marker Offset:  ['50'] | Citation Marker:  2006 | Citation Offset:  ['48','49','50','51'] | Citation Text:  <S sid ="48" ssid = "1">There are only a few studies on document-level SMT.</S><S sid ="49" ssid = "2">Representative work includes Zhao et al.</S><S sid ="50" ssid = "3">(2006), Tam et al.</S><S sid ="51" ssid = "4">(2007), Carpuat (2009).</S> | Reference Offset:  ['15'] | Reference Text:  <S sid ="15" ssid = "15">In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of document- pairs.</S> | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka |


Citance Number: 3 | Reference Article:  P06-2124.txt | Citing Article:  D11-1084.txt | Citation Marker Offset:  ['53'] | Citation Marker:  2006 | Citation Offset:  ['52','53','54'] | Citation Text:  <S sid ="52" ssid = "5">Zhao et al.</S><S sid ="53" ssid = "6">(2006) assumed that the parallel sentence pairs within a document pair constitute a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model.</S><S sid ="54" ssid = "7">It shows that the performance of word alignment can be improved with the help of document-level information, which indirectly improves the quality of SMT.</S> | Reference Offset:  ['2','15'] | Reference Text:  <S sid ="2" ssid = "2">Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model.</S><S sid ="15" ssid = "15">In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of document- pairs.</S> | Discourse Facet:  ['Hypothesis_Citation','Method_Citation'] | Annotator:  Kokil Jaidka |


Citance Number: 4 | Reference Article:  P06-2124.txt | Citing Article:  D13-1141.txt | Citation Marker Offset:  ['39'] | Citation Marker:  Zhao and Xing, 2006 | Citation Offset:  ['39'] | Citation Text:  <S sid ="39" ssid = "17">These methods ensure that bilingual embeddings retain els (Peirsman and PadoÂ´ , 2010; Sumita, 2000), their translational equivalence while their distribu and with unsupervised algorithms such as LDA and LSA (BoydGraber and Resnik, 2010; Tam et al., 2007; Zhao and Xing, 2006).</S> | Reference Offset:  ['83'] | Reference Text:  <S sid ="83" ssid = "44">Specifically, the latent Dirichlet allocation (LDA) in (Blei et al., 2003) can be viewed as a special case of the BiTAM3, in which the target sentence 1 n p(f n n j=1 |eanj , Bzn ).</S> | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka |


Citance Number: 5 | Reference Article:  P06-2124.txt | Citing Article:  P07-1066.txt | Citation Marker Offset:  ['28'] | Citation Marker:  Zhao and Xing, 2006 | Citation Offset:  ['28','29'] | Citation Text:  <S sid ="28" ssid = "28">Related work includes the Bilingual Topic Admixture Model (BiTAM) for word alignment proposed by (Zhao and Xing, 2006).</S><S sid ="29" ssid = "29">Basically, the BiTAM model consists of topic-dependent transla tion lexicons modeling P r(c|e, k) where c, e and k denotes the source Chinese word, target English word and the topic index respectively.</S> | Reference Offset:  ['21', '37', '152'] | Reference Text:  <S sid ="21" ssid = "21">We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.</S><S sid ="37" ssid = "7">The translation lexicon p(f |e) is the key component in this generative process.</S><S sid ="152" ssid = "25">Topic-specific translation lexicons are learned by a 3-topic BiTAM1.</S> | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka |


Citance Number: 6 | Reference Article:  P06-2124.txt | Citing Article:  P10-1115.txt | Citation Marker Offset:  ['36'] | Citation Marker:  Zhao and Xing, 2006 | Citation Offset:  ['36'] | Citation Text:  <S sid ="36" ssid = "5">Some previous work on multilingual topic models assume documents in multiple languages are aligned either at the document level, sentence level or by time stamps (Mimno et al., 2009; Zhao and Xing, 2006; Kim and Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).</S> | Reference Offset:  ['15', '21'] | Reference Text:  <S sid ="15" ssid = "15">In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of document- pairs.</S><S sid ="21" ssid = "21">We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.</S> | Discourse Facet:  ['Method_Citation','Hypothesis_Citation'] | Annotator:  Kokil Jaidka |


Citance Number: 7 | Reference Article:  P06-2124.txt | Citing Article:  P10-2025.txt | Citation Marker Offset:  ['84']  | Citation Marker:  Zhao and Xing, 2006  | Citation Offset:  ['84'] | Citation Text:  <S sid ="84" ssid = "22">We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006).</S> | Reference Offset:  ['152'] | Reference Text:  <S sid ="152" ssid = "25">Topic-specific translation lexicons are learned by a 3-topic BiTAM1.</S> | Discourse Facet:  Results_Citation | Annotator:  Kokil Jaidka |


Citance Number: 8 | Reference Article:  P06-2124.txt | Citing Article:  P10-2025.txt | Citation Marker Offset:  ['86']  | Citation Marker:  Zhao and Xing, 2006  | Citation Offset:  ['86'] | Citation Text:  <S sid ="86" ssid = "24">The alignment results for both directions were refined with ‘GROW’ heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006).</S> | Reference Offset:  ['124', '194'] | Reference Text:  <S sid ="124" ssid = "34">Two word-alignment retrieval schemes are designed for BiTAMs: the uni-direction alignment (UDA) and the bi-direction alignment (BDA).</S><S sid ="194" ssid = "67">Inter takes the intersection of the two directions and generates high-precision alignments;</S> | Discourse Facet:  ['Method_Citation','Results_Citation'] | Annotator:  Kokil Jaidka |


Citance Number: 9 | Reference Article:  P06-2124.txt | Citing Article:  P11-2032.txt | Citation Marker Offset:  ['11'] | Citation Marker:  2006 | Citation Offset:  ['11'] | Citation Text:  <S sid ="11" ssid = "11">Zhao and Xing (2006) note that the parameter estimation (for which they use variational EM) suffers from data sparsity and use symmetric Dirichlet priors, but they find the MAP solution.</S> | Reference Offset:  ['115','116','117'] | Reference Text:  <S sid ="115" ssid = "25">The translation lexicons Bf,e,k have a potential size of V 2K , assuming the vocabulary sizes for both languages are V . The data sparsity (i.e., lack of large volume of document-pairs) poses a more serious problem in estimating Bf,e,k than the monolingual case, for instance, in (Blei et al., 2003).</S><S sid ="116" ssid = "26">To reduce the data sparsity problem, we introduce two remedies in our models.</S><S sid ="117" ssid = "27">First: Laplace smoothing.</S> | Discourse Facet:  ['Implication_Citation','Method_Citation'] | Annotator:  Kokil Jaidka |


Citance Number: 10 | Reference Article:  P06-2124.txt | Citing Article:  P12-1048.txt | Citation Marker Offset:  ['17'] | Citation Marker:  Zhao and Xing, 2006 | Citation Offset:  ['17'] | Citation Text:  <S sid ="17" ssid = "17">Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection.</S> | Reference Offset:  ['2','3','21'] | Reference Text:  <S sid ="2" ssid = "2">Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model.</S><S sid ="3" ssid = "3">Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels).</S><S sid ="21" ssid = "21">We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.</S> | Discourse Facet:  ['Hypothesis_Citation','Method_Citation'] | Annotator:  Kokil Jaidka |


Citance Number: 11 | Reference Article:  P06-2124.txt | Citing Article:  P12-1048.txt | Citation Marker Offset:  ['149'] | Citation Marker:  2006 | Citation Offset:  ['149'] | Citation Text:  <S sid ="149" ssid = "13">Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model, Zhao and Xing (2006,2007) presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity.</S> | Reference Offset:  ['7', '10'] | Reference Text:  <S sid ="7" ssid = "7">Our preliminary experiments show that the proposed models improve word alignment accuracy, and lead to better translation quality.</S><S sid ="10" ssid = "10">Beyond the sentence-level, corpus- level word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices.</S> | Discourse Facet:  Implication_Citation | Annotator:  Kokil Jaidka |


Citance Number: 12 | Reference Article:  P06-2124.txt | Citing Article:  P12-1048.txt | Citation Marker Offset:  ['160'] | Citation Marker:  Zhao and Xing, 2006 | Citation Offset:  ['160'] | Citation Text:  <S sid ="160" ssid = "24">â€¢ In addition to the utilization of in-domain monolingual corpora, our method is different from the previous works (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010) in the following aspects: (1) we use a different topic model â€” HTMM which has different assumption from PLSA and LDA; (2) rather than modeling topic-dependent translation lexicons in the training process, we estimate topic-specific lexical probability by taking account of topical context when extracting word pairs, so our method can also be directly applied to topic-dependent phrase probability modeling.</S> | Reference Offset:  ['10', '83'] | Reference Text:  <S sid ="10" ssid = "10">Beyond the sentence-level, corpus- level word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices.</S><S sid ="83" ssid = "44">Specifically, the latent Dirichlet allocation (LDA) in (Blei et al., 2003) can be viewed as a special case of the BiTAM3, in which the target sentence 1 n p(f n n j=1 |eanj , Bzn ).</S> | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka |


Citance Number: 13 | Reference Article:  P06-2124.txt | Citing Article:  P12-1079.txt | Citation Marker Offset:  ['8'] | Citation Marker:  Zhao and Xing, 2006 | Citation Offset:  ['8'] | Citation Text:  <S sid ="8" ssid = "8">To exploit topic information for statistical machine translation (SMT), researchers have proposed various topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007) to improve translation quality.</S> | Reference Offset:  ['10','13'] | Reference Text:  <S sid ="10" ssid = "10">Beyond the sentence-level, corpus- level word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices.</S><S sid ="13" ssid = "13">For example, the word shot in â€œIt was a nice shot.â€ should be translated differently depending on the context of the sentence: a goal in the context of sports, or a photo within the context of sightseeing.</S> | Discourse Facet:  Implication_Citation | Annotator:  Kokil Jaidka |


Citance Number: 14 | Reference Article:  P06-2124.txt | Citing Article:  P12-1079.txt | Citation Marker Offset:  ['40'] | Citation Marker:  Zhao and Xing, 2006 | Citation Offset:  ['40'] | Citation Text:  <S sid ="40" ssid = "1">Sentences should be translated in consistence with their topics (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007).</S> | Reference Offset:  ['21'] | Reference Text:  <S sid ="21" ssid = "21">We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.</S> | Discourse Facet:  ['Hypothesis_Citation','Aim_Citation'] | Annotator:  Kokil Jaidka |


Citance Number: 15 | Reference Article:  P06-2124.txt | Citing Article:  P12-2023.txt | Citation Marker Offset:  ['30'] | Citation Marker:  Zhao and Xing, 2006 | Citation Offset:  ['30'] | Citation Text:  <S sid ="30" ssid = "30">Topic modeling has received some use in SMT, for instance Bilingual LSA adaptation (Tam et al., 2007), and the BiTAM model (Zhao and Xing, 2006), which uses a bilingual topic model for learning alignment.</S> | Reference Offset:  ['17', '21'] | Reference Text:  <S sid ="17" ssid = "17">Previous works on topical translation models concern mainly explicit logical representations of semantics for machine translation.</S><S sid ="21" ssid = "21">We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.</S> | Discourse Facet:  ['Hypothesis_Citation','Aim_Citation'] | Annotator:  Kokil Jaidka |


Citance Number: 16 | Reference Article:  P06-2124.txt | Citing Article:  P13-2122.txt | Citation Marker Offset:  ['27'] | Citation Marker:  Zhao and Xing, 2006 | Citation Offset:  ['27'] | Citation Text:  <S sid ="27" ssid = "4">To avoid the need for hard decisions about domain membership, some have used topic modeling to improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) or â€˜biTAMâ€™ (Zhao and Xing, 2006).</S> | Reference Offset:  ['21'] | Reference Text:  <S sid ="21" ssid = "21">We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.</S> | Discourse Facet:  ['Hypothesis_Citation','Aim_Citation'] | Annotator:  Kokil Jaidka |


Citance Number: 17 | Reference Article:  P06-2124.txt | Citing Article:  W07-0722.txt | Citation Marker Offset:  ['13'] | Citation Marker:  Zhao and Xing, 2006 | Citation Offset:  ['13','14','15'] | Citation Text:  <S sid ="13" ssid = "13">In (Zhao and Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 as a baseline model, were presented under the bilingual topic admixture model formalism.</S><S sid ="14" ssid = "14">These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence.</S><S sid ="15" ssid = "15">The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task.</S> | Reference Offset:  ['21', '39','45','192','203'] | Reference Text: <S sid ="21" ssid = "21">We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.</S><S sid ="39" ssid = "9">We start from IBM1 as our baseline model, while higher-order alignment models can be embedded similarly within the proposed framework.</S><S sid ="45" ssid = "6">Because of this coupling of sentence-pairs (via topic sharing across sentence-pairs according to a common topic-weight vector), BiTAM is likely to improve the coherency of translations by treating the document as a whole entity</S><S sid ="192" ssid = "65">Notably, BiTAM allows to test alignments in two directions: English-to Chinese (EC) and Chinese-to-English (CE).</S><S sid ="203" ssid = "76">As shown in Table 4, the baseline IBM1 gives its best performance of 36.27% in the CE direc tion; the UDA alignments from BiTAM1∼3 give 40.13%, 40.26%, and 40.47%, respectively, which are significantly better than IBM1.</S> |  Discourse Facet:  ['Method_Citation','Results_Citation'] | Annotator:  Kokil Jaidka |


Citance Number: 18 | Reference Article:  P06-2124.txt | Citing Article:  W07-0722.txt | Citation Marker Offset:  ['62'] | Citation Marker:  Zhao and Xing, 2006 | Citation Offset:  ['62'] | Citation Text:  <S sid ="62" ssid = "39">A possible solution is the implementation of interpolation techniques to smooth sharp distributions estimated on few events (Och and Ney, 2003; Zhao and Xing, 2006).</S> | Reference Offset:  ['116','119','120'] | Reference Text:  <S sid ="116" ssid = "26">To reduce the data sparsity problem, we introduce two remedies in our models.</S><S sid ="119" ssid = "29">Second: interpolation smoothing.</S><S sid ="120" ssid = "30">Empirically, we can employ a linear interpolation with IBM1 to avoid overfitting:</S> | Discourse Facet:  Method_Citation | Annotator:  Kokil Jaidka |
