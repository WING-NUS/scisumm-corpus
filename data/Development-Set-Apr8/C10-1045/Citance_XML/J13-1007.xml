<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">We present a constituency parsing system for Modern Hebrew.</S>
		<S sid ="2" ssid = "2">The system is based on the PCFG-LA parsing method of Petrov et al.</S>
		<S sid ="3" ssid = "3">(2006), which is extended in various ways in order to accommodate the specificities of Hebrew as a morphologically rich language with a small treebank.</S>
		<S sid ="4" ssid = "4">We show that parsing performance can be enhanced by utilizing a language resource external to the treebank, specifically, a lexicon-based morphological analyzer.</S>
		<S sid ="5" ssid = "5">We present a computational model of interfacing the external lexicon and a treebank-based parser, also in the common case where the lexicon and the treebank follow different annotation schemes.</S>
		<S sid ="6" ssid = "6">We show that Hebrew word-segmentation and constituency-parsing can be performed jointly using CKY lattice parsing.</S>
		<S sid ="7" ssid = "7">Performing the tasks jointly is effective, and substantially outperforms a pipeline- based model.</S>
		<S sid ="8" ssid = "8">We suggest modeling grammatical agreement in a constituency-based parser as a filter mechanism that is orthogonal to the grammar, and present a concrete implementation of the method.</S>
		<S sid ="9" ssid = "9">Although the constituency parser does not make many agreement mistakes to begin with, the filter mechanism is effective in fixing the agreement mistakes that the parser does make.</S>
		<S sid ="10" ssid = "10">These contributions extend outside of the scope of Hebrew processing, and are of general applicability to the NLP community.</S>
		<S sid ="11" ssid = "11">Hebrew is a specific case of a morphologically rich language, and ideas presented in this work are useful also for processing other languages, including English.</S>
		<S sid ="12" ssid = "12">The lattice-based parsing methodology is useful in any case where the input is uncertain.</S>
		<S sid ="13" ssid = "13">Extending the lexical coverage of a treebank-derived parser using an external lexicon is relevant for any language with a small treebank.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="14" ssid = "14">Different languages have different syntactic properties.</S>
			<S sid ="15" ssid = "15">In English, word order is relatively fixed, whereas in other languages word order is much more flexible (in Hebrew, the subject may appear either before or after a verb).</S>
			<S sid ="16" ssid = "16">In languages with a flexible word order, the meaning of the sentence is realized using other structural elements, like word ∗ Computer Science Department, Ben Gurion University of the Negev, Israel.</S>
			<S sid ="17" ssid = "17">Email: yoav.goldberg@gmail.com.</S>
			<S sid ="18" ssid = "18">∗∗ Computer Science Department, Ben Gurion University of the Negev, Israel.</S>
			<S sid ="19" ssid = "19">Email: elhadad@cs.bgu.ac.il.</S>
			<S sid ="20" ssid = "20">Submission received: 30 September 2011; revised submission received: 19 May 2012; accepted for publication: 3 August 2012..</S>
			<S sid ="21" ssid = "21">© 2013 Association for Computational Linguistics inflections or markers, which are referred to as morphology (in Hebrew, the marker תא is used to mark definite objects, distinguishing them from subjects in the same position.</S>
			<S sid ="22" ssid = "22">In addition, verbs and nouns are marked for gender and number, and subject and verb must share the same gender and number).</S>
			<S sid ="23" ssid = "23">A limited form of morphology also exists in English: the -s and -ed suffixes are examples of English morphological markings.</S>
			<S sid ="24" ssid = "24">In other languages, morphological processes may be much more involved.</S>
			<S sid ="25" ssid = "25">The lexical units (words) in English are always separated by white space.</S>
			<S sid ="26" ssid = "26">In Chinese, such separation is not available.</S>
			<S sid ="27" ssid = "27">In Hebrew (and Arabic), most words are separated by white space, but many of the function words (determiners like the, conjunctions such as and, and prepositions like in or of ) do not stand on their own but are instead attached to the following words.</S>
			<S sid ="28" ssid = "28">A large part of the parsing literature is devoted to automatic parsing of English, a language with a relatively simple morphology, relatively fixed word order, and a large treebank.</S>
			<S sid ="29" ssid = "29">Data-driven English parsing is now at the state where naturally occurring text in the news domain can be automatically parsed with accuracies of around 90% (according to standard parsing evaluation measures).</S>
			<S sid ="30" ssid = "30">When moving from English to languages with richer morphologies and less-rigid word orders, however, the parsing algorithms developed for English exhibit a large drop in accuracy.</S>
			<S sid ="31" ssid = "31">In addition, whereas English has a large treebank, containing over one million annotated words, many other languages have much smaller treebanks, which also contribute to the drop in the accuracies of the data-driven parsers.</S>
			<S sid ="32" ssid = "32">A similar drop in parsing accuracy is also exhibited in English when moving from the news domain, on which parsers have traditionally been trained, to other genres such as prose, blogs, poetry, product reviews, or biomedical texts, which use different vocabularies and, to some extent, different syntactic rules.</S>
			<S sid ="33" ssid = "33">This work focuses on constituency parsing of Modern Hebrew, a Semitic language with a rich and productive morphology, relatively free word order,1 and a small tree- bank.</S>
			<S sid ="34" ssid = "34">Several natural questions arise: Can the small size of the treebank be compensated for using other available resources or sources of information?</S>
			<S sid ="35" ssid = "35">How should the word segmentation issue (that function words do not appear in isolation but attach to the next word, forming ambiguous letter patterns) be handled?</S>
			<S sid ="36" ssid = "36">Can morphological information be used effectively in order to improve parsing accuracy?</S>
			<S sid ="37" ssid = "37">We present a system which is based on a state-of-the-art model for constituency parsing, namely, the probabilistic context-free grammar (PCFG) with latent annotations (PCFG-LA) model of Petrov et al.</S>
			<S sid ="38" ssid = "38">(2006), as implemented in the BerkeleyParser.</S>
			<S sid ="39" ssid = "39">After evaluating the out-of-the-box performance of the BerkeleyParser on the Hebrew tree- bank, we discuss some of its limitations and then go on to extend the PCFG-LA parsing model in several directions, making it more suitable for parsing Hebrew and related languages.</S>
			<S sid ="40" ssid = "40">Our extensions are based on the following themes.</S>
			<S sid ="41" ssid = "41">Separation of lexical and syntactic knowledge.</S>
			<S sid ="42" ssid = "42">There are two kinds of knowledge inherent in a parsing system.</S>
			<S sid ="43" ssid = "43">One of them is syntactic knowledge governing the way in which words can be combined to form structures, which, in turn, can be combined to form ever larger structures.</S>
			<S sid ="44" ssid = "44">The other is lexical knowledge about the identities of individual words, the word classes they belong to, and the kinds of syntactic structures they can participate in.</S>
			<S sid ="45" ssid = "45">We argue that the amount of syntactic knowledge needed for a parsing system is relatively limited, and that sufficiently large parts of it can be captured also 1 To be more precise, in Hebrew the order of constituents is relatively free, whereas the order of the words.</S>
			<S sid ="46" ssid = "46">within certain constituents is relatively fixed.</S>
			<S sid ="47" ssid = "47">122 based on a relatively small treebank.</S>
			<S sid ="48" ssid = "48">Lexical knowledge, on the other hand, is much more vast, and we should not rely on a treebank (small or large) to provide adequate lexical coverage.</S>
			<S sid ="49" ssid = "49">Instead, we should aim to find ways of integrating lexical knowledge, which is external to the treebank, into the parsing process.</S>
			<S sid ="50" ssid = "50">We extend the lexical coverage of a treebank-based parser using a dictionary-based morphological analyzer.</S>
			<S sid ="51" ssid = "51">We present a way of integrating the two resources also for the common case where their annotations schemes diverge.</S>
			<S sid ="52" ssid = "52">This method is very effective in improving parsing accuracy.</S>
			<S sid ="53" ssid = "53">Encoding input uncertainty using a lattice-based representation.</S>
			<S sid ="54" ssid = "54">Sometimes, the language signal (the input to the parser) may be uncertain.</S>
			<S sid ="55" ssid = "55">This happens in Hebrew when a space-delimited token such as לצב can represent either a single word (‘[an] onion’) or a sequence of two words or three words (‘in shadow’ and ‘in the shadow,’ respectively).</S>
			<S sid ="56" ssid = "56">When computationally feasible, it is best to let the uncertainty be resolved by the parser rather than in a separate preprocessing step.</S>
			<S sid ="57" ssid = "57">We propose encoding the input-uncertainty in a word lattice, and use lattice parsing (Chappelier et al. 1999; Hall 2005) to perform joint word segmentation and syntactic disambiguation (Cohen and Smith 2007; Goldberg and Tsarfaty 2008).</S>
			<S sid ="58" ssid = "58">Performing the tasks jointly is effective, and substantially outperforms a pipeline-based model.</S>
			<S sid ="59" ssid = "59">Using morphological information to improve parsing accuracy.</S>
			<S sid ="60" ssid = "60">Morphology provides useful hints for resolving syntactic ambiguity, and the parsing model should have a way of utilizing these hints.</S>
			<S sid ="61" ssid = "61">There is a range of morphological hints than can be utilized: from functional marking elements (such as the תא marker indicating a definite direct object); to elements marking syntactic properties such as definiteness (such as the Hebrew ה marker); to agreement patterns requiring a compatibility in properties such as gender, number, and person between syntactic constituents (such as a verb and its subject or an adjective and the noun it modifies).</S>
			<S sid ="62" ssid = "62">We suggest modeling agreement as a filtering process that is orthogonal to the grammar.</S>
			<S sid ="63" ssid = "63">Although the constituency parser does not make many agreement mistakes to begin with, the filter mechanism is effective in fixing the agreement mistakes that the parser does make, without introducing new mistakes.</S>
			<S sid ="64" ssid = "64">Aspects of the work presented in this article are discussed in earlier publications.</S>
			<S sid ="65" ssid = "65">Goldberg and Tsarfaty (2008) suggest the lattice-parsing mechanism, Goldberg et al.</S>
			<S sid ="66" ssid = "66">(2009) discuss ways of interfacing a treebank-derived PCFG-parser with an external lexicon, and Goldberg and Elhadad (2011) present experiments using the PCFG-LA BerkeleyParser.</S>
			<S sid ="67" ssid = "67">Here we provide a cohesive presentation of the entire system, as well as a more detailed description and an expanded evaluation.</S>
			<S sid ="68" ssid = "68">We also extend the previous work in several dimensions: We introduce a new method of interfacing the parser and the external lexicon, which contributes to an improved parsing accuracy, and suggest incorporating agreement information as a filter.</S>
			<S sid ="69" ssid = "69">The methodologies we suggest extend outside the scope of Hebrew processing, and are of general applicability to the NLP community.</S>
			<S sid ="70" ssid = "70">Hebrew is a specific case of a morphologically rich language, and ideas presented in this work are useful also for processing other languages, including English.</S>
			<S sid ="71" ssid = "71">The lattice-based parsing methodology is useful in any case where the input is uncertain.</S>
			<S sid ="72" ssid = "72">Indeed, we have used it to solve the problem of parsing while recovering null elements in both English and Chinese (Cai, Chiang, and Goldberg 2011), and others have used it for the joint segmentation and parsing of Arabic (Green and Manning 2010).</S>
			<S sid ="73" ssid = "73">Extending the lexical coverage of a treebank-derived parser using an external lexicon is relevant for any language with 123 a small treebank, and also for domain adaptation scenarios for English.</S>
			<S sid ="74" ssid = "74">Finally, the agreement-as-filter methodology is applicable to any morphologically rich language, and although its contribution to the parsing task may be limited, it is of wide applicability to syntactic generation tasks, such as target-side-syntax machine translation in a morphologically rich language.</S>
	</SECTION>
	<SECTION title="Modern Hebrew. " number = "2">
			<S sid ="75" ssid = "1">2.1 Lexical and Syntactic Properties.</S>
			<S sid ="76" ssid = "2">Some relevant lexical and syntactic properties of Modern Hebrew are highlighted in this section.</S>
			<S sid ="77" ssid = "3">2.1.1 Unvocalized Orthography.</S>
			<S sid ="78" ssid = "4">Most vowels are not marked in everyday Hebrew text, which results in a very high level of lexical and morphological ambiguity.</S>
			<S sid ="79" ssid = "5">Some tokens can admit as many as 15 distinct readings, and the average number of possible morphological analyses per token in Hebrew text is 2.7, compared with 1.4 in English (Adler 2007).</S>
			<S sid ="80" ssid = "6">The word תויפכ can be read in at least eight different ways (‘spoons,’ ‘square cotton headkerchiefs,’ ‘coercions,’ ‘as mouths,’ ‘as spouts,’ ‘as fairies,’ ‘ungratefulness,’ ‘fun/adjectivefeminine,plural ’), the word בתכ in at least six ways (‘a journalist,’ ‘writing,’ ‘script,’ ‘wrote,’ ‘added someone as a recipient,’ ‘was added as a recipient’) and the word תא can be read as a very common case-marker (appearing before definite direct objects), a very common pronoun (‘you/feminine ’), and a noun (‘shovel’).</S>
			<S sid ="81" ssid = "7">2.1.2 Affixation.</S>
			<S sid ="82" ssid = "8">Eight common prepositions, conjunctions, and articles may never appear in isolation and must always be attached as prefixes to the following word.2 These include the function words מ (‘from’), ש (‘which’/‘who’/‘that’), שכ (‘when’), ה (‘the’), ו (‘and’), כ (‘like’), ל (‘to’), and ב (‘in’),.</S>
			<S sid ="83" ssid = "9">Several such elements may attach together, producing forms such as שמשהמשו (שמש-ה-מ-ש-ו ‘and-that-from-the-sun’).</S>
			<S sid ="84" ssid = "10">Notice that when it appears by itself, the last part of the token, the noun שמש (‘sun’), can also be interpreted as the sequence שמ-ש (‘who moved’).</S>
			<S sid ="85" ssid = "11">The linear order of such elements within a token is fixed (disallowing the reading שמ-ש-ה-מ-ש-ו in the previous example).</S>
			<S sid ="86" ssid = "12">The syntactic relations of these elements with respect to the rest of the sentence are rather free, however.</S>
			<S sid ="87" ssid = "13">The relativizer ש (‘that’), for example, may attach to an arbitrarily long relative clause that goes beyond token boundaries.</S>
			<S sid ="88" ssid = "14">The attachment in such cases encompasses a long-distance dependency that cannot be captured by local- context (or Markovian) sequential processes that are typically used for morphological disambiguation.</S>
			<S sid ="89" ssid = "15">The same argument holds for resolving PP attachment of a prefixed preposition or marking conjunction of elements of any kind.</S>
			<S sid ="90" ssid = "16">To further complicate matters, the definite article ה (‘the’) is not realized in writing when following the particles ב (‘in’), כ (‘like’), and ל (‘to’).</S>
			<S sid ="91" ssid = "17">Thus, the form תיבב can be interpreted as either תיב-ב (‘in house’) or תיב-ה-ב (‘in the house’).3 2 In what follows, we indicate the correct segmentations of the different forms.</S>
			<S sid ="92" ssid = "18">Naturally occurring.</S>
			<S sid ="93" ssid = "19">Hebrew text does not have such indications.</S>
			<S sid ="94" ssid = "20">3 This overt element is in fact indicated by vocalization, but is not realized in standard written text..</S>
			<S sid ="95" ssid = "21">124 In addition, pronominal elements (clitics) may attach to nouns, verbs, adverbs, prepositions, and others as suffixes (e.g., ןאיבה [ןה-איבה, ‘brought-them’], םהילע [-ילעםה,‘on them’]).</S>
			<S sid ="96" ssid = "22">These affixations result in highly ambiguous token segmentations: ורפסמ (‘[they] assigned numbers’) vs. ו-רפסמ (‘his number ’ or ‘the one who cuts his hair ’) vs. -רפס-מו (‘from his book’ or ‘from his barber ’), תבכרה (‘putting together ’) vs. תבכר-ה (‘the train’), and לצב (‘an onion’) vs. לצ-ב (‘in the shadow’) are only a few examples of ambiguities that may arise.</S>
			<S sid ="97" ssid = "23">Quantitatively, 99,896 out of 567,483 forms (17%) in a wide-coverage lexicon of Hebrew can admit both segmented and unsegmented analyses.</S>
			<S sid ="98" ssid = "24">In many cases the correct segmentation cannot be determined from local context alone, but can be disambiguated by more global syntactic constraints (in םימש יתיאר םילוחכ, the middle token is ambiguous between םימש [‘sky’] and םימ-ש [‘that/rel water ’], and the sequence can be interpreted as either ‘I saw blue skies’ or ‘I saw that blue water.’ On the other hand, ראבה ןמ וצרפ םילוחכ םימש יתיאר is unambiguous because the past verb וצרפ requires the relativizer ש, allowing only the segmented םימ-ש reading ‘I saw that blue water broke from the well’.</S>
			<S sid ="99" ssid = "25">In the other direction, יתכלהו םילוחכ םימש יתיאר ןושיל is also unambiguous, allowing only the unsegmented reading ‘I saw blue skies and went to sleep’.)</S>
			<S sid ="100" ssid = "26">2.1.3 Rich Templatic Morphology.</S>
			<S sid ="101" ssid = "27">Hebrew words follow a complex morphological structure, which is based on a root + template system, with both derivational and inflectional elements.</S>
			<S sid ="102" ssid = "28">Word forms can encode gender, number, person, and tense, and in addition noun-compounding is also morphologically marked (see Section 2.1.7).</S>
			<S sid ="103" ssid = "29">Although the exact details of the system are irrelevant (but see Adler [2007] and Glinert [1989] for a good overview), we note that this word formation mechanism results in a very high number of possible word forms, and that it is hard to guess the part-of-speech of words based on prefixes and suffixes alone, a method frequently used in other languages.</S>
			<S sid ="104" ssid = "30">2.1.4 The Participle Form.</S>
			<S sid ="105" ssid = "31">The Hebrew participle form (ינוניב, literally the “middle form” of verbs) is a form that shares morphological and syntactic properties of nouns, verbs, and adjectives.</S>
			<S sid ="106" ssid = "32">This form causes many disagreements between human annotators, and large disagreement is found also between major Hebrew dictionaries regarding many word forms (see Adler et al. [2008b] for a discussion from tag set design and annotation guidelines, including many syntactic, semantic, and lexical considerations).</S>
			<S sid ="107" ssid = "33">For the purpose of this work, this form is of interest as it highlights the inherent ambiguity between adjectival, nominal, and verbal readings of many words, which are hard to disambiguate even in context.</S>
			<S sid ="108" ssid = "34">2.1.5 Relatively Free Constituent Order.</S>
			<S sid ="109" ssid = "35">The ordering of constituents inside a phrase is relatively free.</S>
			<S sid ="110" ssid = "36">This is most notably apparent in verbal phrases and sentential levels.</S>
			<S sid ="111" ssid = "37">In particular, whereas most sentences follow a subject-verb-object order (SVO), OVS and VSO configurations are also possible (counting in the Hebrew Treebank reveals 5,720 SV cases and 2,613 VS cases, compared with 81,135 SV and 3,018 VS constructions in the English WSJ Treebank).</S>
			<S sid ="112" ssid = "38">In addition, verbal arguments can appear before or after the verb, and in many orders.</S>
			<S sid ="113" ssid = "39">Such variations in constituent order are easy to capture using ‘flat’ S structures putting the verbs and all of its arguments on the same clausal level, and this is the annotation approach adopted by the Hebrew Treebank (as well as by treebanks of other languages, such as French [Abeille´ , Cle´ ment, and Toussenel 2003]).</S>
			<S sid ="114" ssid = "40">These flat structures result in the grammar having more and longer rules and the treebank having fewer instances of each rule type, however, causing a data sparseness 125 problem for statistical estimation methods based on treebank counts, and making it more difficult to reliably estimate the grammar parameters.</S>
			<S sid ="115" ssid = "41">2.1.6 Verbless Constructions.</S>
			<S sid ="116" ssid = "42">Several constructions in which the verb is not realized are common in Hebrew.</S>
			<S sid ="117" ssid = "43">These include the possessive constructions such as םיעוצעצ ודיעל םיבר (‘to-Ido toys many’ meaning ‘ido has many toys’), which also feature a flexible constituent order ודיעל םיבר םיעוצעצ (‘toys many to-Ido’, ‘ido has many toys’), and copular constructions such as דומח דליה (‘the-boy cute’ ‘the boy is cute’) and עגושמ דליה (‘the-boy crazy’ ‘the boy is crazy’).</S>
			<S sid ="118" ssid = "44">2.1.7 NP Structure and Construct-State.</S>
			<S sid ="119" ssid = "45">Although constituent order may vary, NP internal structure is rigid.</S>
			<S sid ="120" ssid = "46">A special morphological marker (construct state, or תוכימס) is used to mark noun-compounds as well as similar phenomena (this is similar to the idafa construction in Arabic).4 Noun compounding in Modern Hebrew is productive and very frequent—about a quarter of the noun tokens in the Hebrew Treebank are in the construct state.</S>
			<S sid ="121" ssid = "47">Construct-state nouns can be highly ambiguous with non-construct- state nouns.</S>
			<S sid ="122" ssid = "48">Some forms are morphologically marked but the marking is not present in unvocalized text (תונב/banot vs. תונב/bnot), and some forms are not marked at all (ךרוע).</S>
			<S sid ="123" ssid = "49">The construct-state marker, although ambiguous, is essential for analyzing NP internal structure.</S>
			<S sid ="124" ssid = "50">Where regular nouns are marked for definiteness using the definite marker ה, construct-nouns acquire the definite status of the noun-phrase they compound to.</S>
			<S sid ="125" ssid = "51">Construct constructions may be nested, as in םיחופתה תספוק הסכמ עבצ ןווג (‘shadeconst colorconst lidconst boxconst the apples,’ meaning ‘the shade of the color of the lid of the box of the apples’).</S>
			<S sid ="126" ssid = "52">2.1.8 Definiteness.</S>
			<S sid ="127" ssid = "53">Definiteness is spread across many elements in the NP.</S>
			<S sid ="128" ssid = "54">All elements in a definite NP, except for construct-nouns and proper-names, are explicitly marked using the functional element ה that is prefixed to the token.</S>
			<S sid ="129" ssid = "55">Proper-names are inherently definite and cannot take the definite marker, and construct-nouns acquire their definiteness status from the NP they dominate (definiteness is not explicitly marked on construct-nouns).</S>
			<S sid ="130" ssid = "56">2.1.9 Case Marking.</S>
			<S sid ="131" ssid = "57">Definite direct objects are marked.</S>
			<S sid ="132" ssid = "58">The case marker in this case is the function word תא appearing before the direct object.</S>
			<S sid ="133" ssid = "59">Subjects, indirect objects, and non-definite direct objects are not marked.</S>
			<S sid ="134" ssid = "60">2.1.10 Agreement.</S>
			<S sid ="135" ssid = "61">Hebrew grammar forces morphological agreement between adjectives and nominals (adjectives appear after the noun, and agree in gender, number, and definiteness), and between subjects and verbs (including the verbless copular constructions), which agree in gender, number, and person.</S>
			<S sid ="136" ssid = "62">Agreement in the predicative case is a bit complex: When the verb is overt and the predicative-complement is a noun, as in ץורית איה העיסנה (‘the-tripfem isfem an-excusemasc ’), gender and number agreement are required between the subject and the verb (but not the predicative-complement), but in the verbless case, the subject and the predicate-complement noun must agree (ץורית העיסנה* ‘the-tripfem an-excusemasc ’).</S>
			<S sid ="137" ssid = "63">When the predicate-complement is an adjective, gender and number agreement between the subject and the predicate-complement 4 The construct state is not restricted to nouns, and can also appear on numbers (e.g., םידלי תורשע/‘tens-of.</S>
			<S sid ="138" ssid = "64">kids’) and adjectives (םירפוסה לודג/‘biggest-of authors’).</S>
			<S sid ="139" ssid = "65">126 is required regardless of the realization of the verb/copular element: הובג דליה, הדליה הובג*, הובג אוה דליה, הובג איה הדליה* (‘the-boy tallmasc ’, ‘*the-boy tallfem ’, ‘the-boy ismasc tallmasc ’, ‘*the-girl isfem tallmasc ’).</S>
			<S sid ="140" ssid = "66">2.2 Implications for Parsing.</S>
			<S sid ="141" ssid = "67">After surveying some lexical and syntactic properties of Modern Hebrew, we turn to highlight some aspects in which Modern Hebrew differs from English from the perspective of parsing system design.</S>
			<S sid ="142" ssid = "68">2.2.1 Small Amount of Annotated Data.</S>
			<S sid ="143" ssid = "69">Whereas the English Treebank is relatively large (49,208 sentences, or 1,173,766 words), the Hebrew Treebank (Guthmann et al. 2009) is much smaller, containing only 6,220 sentences, or 115,661 tokens (156,316 words5 ).</S>
			<S sid ="144" ssid = "70">The small size of the Hebrew Treebank implies a smaller training set for learning- algorithms used to construct the parser.</S>
			<S sid ="145" ssid = "71">2.2.2 Ambiguous Word Segmentation.</S>
			<S sid ="146" ssid = "72">Syntactic parsing systems treat the input sentence as observed data—the leaves (in constituency parsing) of the tree are known in advance, and the parser is expected to build a parse tree around them.</S>
			<S sid ="147" ssid = "73">This is not the case in Hebrew, where many function words are not separated by white space but instead are prefixed to the next word and appear within the same token.</S>
			<S sid ="148" ssid = "74">This makes the word sequence unobserved to the parser, which has to infer both the syntactic-structure and the token segmentation.6 One possible solution to the unobserved word-sequence problem is a pipeline system in which an initial model is in charge of token-segmentation, and the output of the initial model is fed as the input to a second stage parser.</S>
			<S sid ="149" ssid = "75">This is a popular approach in parsing systems for Arabic and Chinese (Jiang, Huang, and Liu 2009; Green and Manning 2010).</S>
			<S sid ="150" ssid = "76">As discussed in Section 2.1.2 (as well as in Tsarfaty [2006a], Goldberg and Tsarfaty [2008], and Cohen and Smith [2007]), however, the token-segmentation and syntactic-parsing tasks are closely intertwined and are better performed jointly instead of in a pipeline fashion, which is the approach we explore in this work.</S>
			<S sid ="151" ssid = "77">2.2.3 Morphological Variation and High Out-of-Vocabulary Rate.</S>
			<S sid ="152" ssid = "78">The intrinsic deficiency caused by the small amount of training data is made even more severe due to Hebrew’s rich morphological inflection patterns.</S>
			<S sid ="153" ssid = "79">The high amount of morphological variation means that many word forms will not be observed in the training data, making it harder to reliably estimate lexical probabilities based on the annotated resources alone.</S>
			<S sid ="154" ssid = "80">Unlike English, where parts-of-speech for words are relatively easy to guess based on simple orthographic features (words starting with capital letters are proper nouns, words ending in -ed are usually verbs, etc.), this is not the case for Hebrew.</S>
			<S sid ="155" ssid = "81">Among the 773 words appearing in English test data but not in the training data, 269 start with a capital letter, 58 end with -ed, and 49 end with -ing.</S>
			<S sid ="156" ssid = "82">Together, these three simple heuristics cover almost half of the unobserved tokens.</S>
			<S sid ="157" ssid = "83">Such heuristics are not available for Hebrew in the common case of unvocalized text: Proper names are not marked 5 Because of agglutination, a Hebrew token may consist of several words, for example the token תיבב.</S>
			<S sid ="158" ssid = "84">comprises the two words ב(‘in’) and תיב(‘house’).</S>
			<S sid ="159" ssid = "85">6 Token segmentation is sometimes (erroneously) referred to as morphological segmentation..</S>
			<S sid ="160" ssid = "86">127 in writing, and word prefixes and suffixes are not indicative of the part-of-speech tags.7 Thus, the out-of-vocabulary (OOV) problem is much harder in Hebrew than in English and other European languages: On the one hand many words are unobserved in training, and on the other, it is more difficult to guess the analysis of such unknown words.</S>
			<S sid ="161" ssid = "87">A system for handling automatic processing of Hebrew text cannot rely solely on manually annotated corpora, as such corpora cannot provide adequate lexical coverage.</S>
			<S sid ="162" ssid = "88">Systems that attempt to perform disambiguation on the lexical level (such as sequence- based morphological disambiguators, or syntactic parsers that perform morphological disambiguation as part of the parsing process) should be designed to incorporate lexical knowledge from sources external to the annotated corpora.</S>
			<S sid ="163" ssid = "89">We discuss methods of enhancing the system’s performance based on a resource that is external to the treebank: A lexicon-based broad-coverage morphological analyzer enhanced with semi-supervised probability estimates based on expectation maximization (EM) training of a hidden Markov model (HMM) tagger on a large amount of unannotated text.</S>
			<S sid ="164" ssid = "90">2.2.4 Morphological Agreement.</S>
			<S sid ="165" ssid = "91">The rich morphological system also means that words carry large amounts of extra information: definiteness, gender, number, tense, and person.</S>
			<S sid ="166" ssid = "92">Some of this information interacts with syntax through agreement constraints.</S>
			<S sid ="167" ssid = "93">Specifically, nouns and adjectives should agree in gender and number, and subjects and verbs should agree in gender, number, and person.</S>
			<S sid ="168" ssid = "94">Agreement constraints can provide useful hints for disambiguating the syntactic structure.</S>
			<S sid ="169" ssid = "95">Consider for example the sentence חופתה תא הלכאש םדאה תשא (‘wife of the man who ate the apple’).</S>
			<S sid ="170" ssid = "96">The English sentence is ambiguous with respect to the entity who ate the apple, but the Hebrew version is not—the verb הלכא (‘ate’) is in feminine form, indicating that it was the wife who did the eating.</S>
			<S sid ="171" ssid = "97">Can a parsing system make use of such information?</S>
			<S sid ="172" ssid = "98">This issue is investigated further in Section 8.2.</S>
			<S sid ="173" ssid = "99">2.3 Existing Resources for Hebrew Text Processing.</S>
			<S sid ="174" ssid = "100">Several linguistic resources are available for Hebrew, and are used as building blocks for the parsing systems described in this work.</S>
			<S sid ="175" ssid = "101">2.3.1 The Hebrew Constituency-Treebank.</S>
			<S sid ="176" ssid = "102">A constituency treebank of Modern Hebrew, incrementally developed at the Technion over the course of more than eight years (Sima’an et al. 2001; Guthmann et al. 2009), is maintained by MILA, the knowledge center for processing Hebrew.8 The current version of the treebank (Version 2) contains 6,220 sentences taken from the Israeli daily newspaper ץראה (Ha’aretz).</S>
			<S sid ="177" ssid = "103">The sentences are manually annotated on both the lexical and the syntactic levels.</S>
			<S sid ="178" ssid = "104">Each token9 is segmented into words, and each word is assigned a part of speech tag that also captures, 7 Although the suffixes are good indicators of gender and number (םי is usually plural masculine, ה is. usually singular feminine), they are not good at indicating the core part-of-speech (ה is a suffix can appear in adjectives הפי, verbs הרמש, nouns הלחמ, and similarly for םי (םיפי, םישפחתמ, םילוענמ).</S>
			<S sid ="179" ssid = "105">Furthermore, due to the root+template system, in most cases the first and last letters of the word are part of the root and not of the pattern רמוש,רומשי, making the suffixes even less indicative.</S>
			<S sid ="180" ssid = "106">8 http:/ www.mila.cs.technion.ac.il/mila/eng/index.html.</S>
			<S sid ="181" ssid = "107">9 As discussed in Section 2.1.2, Hebrew tokens (entities separated by white space and punctuation symbols).</S>
			<S sid ="182" ssid = "108">do not necessarily correspond to Hebrew words.</S>
			<S sid ="183" ssid = "109">A single token may contain several words.</S>
			<S sid ="184" ssid = "110">128 where applicable, the morphological properties of the word such as number, gender, and person.</S>
			<S sid ="185" ssid = "111">Then a constituency tree is built on top of the segmented words.</S>
			<S sid ="186" ssid = "112">The annotation of NPs is relatively nested, and the sentence level structures are relatively flat (the verb and all of its arguments reside on one level under S).</S>
			<S sid ="187" ssid = "113">The treebank has 115,661 tokens and 156,764 words.</S>
			<S sid ="188" ssid = "114">The POS tagging scheme in the treebank is highly syntactic in nature: A part-of- speech is chosen to reflect the syntactic function of the given word in context.</S>
			<S sid ="189" ssid = "115">For example, demonstrative pronouns are tagged in the treebank as adjectives when appearing in an adjectival position (הז דלי, ‘this/JJ child/NN’), and a special MOD tag is used to mark non-adverbial clausal level modification (that is, modifications that can be treated as adverbial, but that are used to modify something other than a verb).</S>
			<S sid ="190" ssid = "116">For a more detailed description of the Constituency Treebank see Sima’an et al.</S>
			<S sid ="191" ssid = "117">(2001), Guthmann et al.</S>
			<S sid ="192" ssid = "118">(2009), and Tsarfaty (2010, pages 199–216), as well as the annotation guidelines.10 2.3.2 Train/dev/test Splits.</S>
			<S sid ="193" ssid = "119">Throughout the article, we follow the established train/ dev/test split for the treebank, namely, sentences 1–483 are used for development, sentences 484–5,740 are used for training the parser, and sentences 5,741 to 6,220 are used as the final test set.</S>
			<S sid ="194" ssid = "120">2.3.3 The MILA Broad-Coverage Lexicon.</S>
			<S sid ="195" ssid = "121">Aside from the Constituency Treebank, Hebrew has a wide-coverage, lexicon-based morphological analyzer which can assign morphological analyses (prefixes, suffixes, core POS, gender, number, person, etc.) to Hebrew tokens.</S>
			<S sid ="196" ssid = "122">The lexicon (henceforth the KC Analyzer) is developed and maintained by the Knowledge Center for Processing Hebrew (Itai and Wintner 2008).</S>
			<S sid ="197" ssid = "123">It is based on a lexicon of roughly 25,000 word lemmas and their inflection patterns.</S>
			<S sid ="198" ssid = "124">From these, 562,439 unique word forms are derived.</S>
			<S sid ="199" ssid = "125">These are then prefixed (subject to constraints) by 73 prepositional prefixes.</S>
			<S sid ="200" ssid = "126">Even with this seemingly large vocabulary, the KC Analyzer ’s coverage is not perfect.</S>
			<S sid ="201" ssid = "127">In Adler et al.</S>
			<S sid ="202" ssid = "128">(2008a), we present a machine-learning method that is trained on the basis of the analyzer and that can guess possible analyses for words unknown to the analyzer with reasonable accuracies.</S>
			<S sid ="203" ssid = "129">Using this extension, the analyzer has perfect coverage (even though the quality is obviously better for words that are present in the analyzer ’s database).</S>
			<S sid ="204" ssid = "130">The tag set used by the lexicon/analyzer is lexicographic in nature, and is discussed in depth in BGU Computational Linguistics Group (2008).</S>
			<S sid ="205" ssid = "131">Creating a resource such as the morphological analyzer for a morphologically rich language is a worthwhile and cost-effective effort: After establishing the tag set, it is relatively straightforward to add lemmas to the lexicon, and the automatic inflection process guarantees good coverage of all the possible inflections.</S>
			<S sid ="206" ssid = "132">This is much more efficient than annotating enough text to obtain a similar coverage.</S>
			<S sid ="207" ssid = "133">2.3.4 Hebrew Morphological Disambiguator.</S>
			<S sid ="208" ssid = "134">The morphological analyzer provides the possible set of analyses for each token, but does not disambiguate the correct analysis in context.</S>
			<S sid ="209" ssid = "135">A morphological disambiguator (henceforth “the Hebrew tagger ” or “tagger ”) was developed by Meni Adler at BenGurion University of the Negev (Adler and Elhadad 2006; Adler 2007; Goldberg, Adler, and Elhadad 2008).</S>
			<S sid ="210" ssid = "136">After the (extended) morphological analyzer assigns the possible analyses for each token in an 10 http:/ www.mila.cs.technion.ac.il/mila/files/treebank/Decisions-Corpus1-5001.v1.pdf.</S>
			<S sid ="211" ssid = "137">129 input sentence, the tagger takes the output of the analyzer as input and chooses the single best analysis for the entire sentence (performing both token segmentation of words and part-of-speech assignment for each word).</S>
			<S sid ="212" ssid = "138">The tagger is an HMM-based sequential model that is trained in a semi-supervised fashion using EM based on the output of the morphological analyzer on a large amount (about 70M words) of unannotated Hebrew text.</S>
			<S sid ="213" ssid = "139">The tagger is described in Adler and Elhadad (2006) and Adler (2007).</S>
			<S sid ="214" ssid = "140">The tagger is relatively accurate: It achieves 93% accuracy in predicting segmentation and tagging when measured on the POS accuracy, and 90% accuracy when measured on the complete tag set, which includes the complete set of morphological features.</S>
			<S sid ="215" ssid = "141">Because the tagger is not trained on a particular annotated training set but instead on a very large corpus of text spanning multiple genres, its performance is robust across domains.</S>
			<S sid ="216" ssid = "142">The tagger ’s success is due in part to a smart initialization procedure to the EM training process.</S>
			<S sid ="217" ssid = "143">This initialization procedure takes the output of the analyzer and assigns a conditional probability distribution P(tag|word) for each word.</S>
			<S sid ="218" ssid = "144">In other words, it assigns an a priori, context-free likelihood for each analysis of a word (although the word broke can be either a verb in the past tense or an adjective, it is more likely to be the former; such preferences can be modeled as probability distributions, and the initialization procedure attempts to learn the values of these distributions automatically from raw data).</S>
			<S sid ="219" ssid = "145">This initialization procedure is described in Goldberg, Adler, and Elhadad (2008).</S>
			<S sid ="220" ssid = "146">A side effect of the EM–HMM training of the tagger is pseudo-counts for (word, tag) events, which are based on patterns observed in the unannotated training data.</S>
			<S sid ="221" ssid = "147">We use these counts in order to improve the lexical-disambiguation capacity of the parser.</S>
			<S sid ="222" ssid = "148">2.3.5 A Resource Incompatibility Issue.</S>
			<S sid ="223" ssid = "149">Unfortunately, the KC Analyzer adopted a different tag set than the one used in the treebank, and analyses produced by the KC Analyzer (and hence by the morphological disambiguator) are incompatible with the Hebrew Treebank.</S>
			<S sid ="224" ssid = "150">These are not mere technical differences, but derive from different perspectives on the data.</S>
			<S sid ="225" ssid = "151">The Hebrew Treebank (TB) tag set is syntactic in nature (“if the word in this particular position functions as an adverb, tag it as an adverb, even though it is listed in the dictionary only as a noun”), whereas the KC tag set (Adler 2007; Netzer et al. 2007; Adler et al. 2008b) takes a lexical approach to POS tagging (“a word can assume only POS tags that would be assigned to it in a dictionary”).</S>
			<S sid ="226" ssid = "152">The lexical approach does not accommodate generic modification POS tags such as MOD, nor does it allow listing of demonstrative pronouns as adjectives.</S>
			<S sid ="227" ssid = "153">These divergent perspectives are reflected in different guidelines to human taggers, different principles underlying tag definitions, and different verification procedures.</S>
			<S sid ="228" ssid = "154">This difference in perspective yields different performances for parsers induced from tagged data, and a simple mapping between the two schemes is impossible to define.</S>
			<S sid ="229" ssid = "155">Some Hebrew forms, particularly the present participle and modal forms, are inherently hard to define, and the wide disagreement about their status is reflected in practically all Hebrew dictionaries.</S>
			<S sid ="230" ssid = "156">This kind of disagreement naturally appears also between the KC and TB.</S>
			<S sid ="231" ssid = "157">See Adler et al.</S>
			<S sid ="232" ssid = "158">(2008b) and Netzer et al.</S>
			<S sid ="233" ssid = "159">(2007) for further discussion on these two interesting cases.</S>
			<S sid ="234" ssid = "160">Bridging the discrepancy between the two resources is an important aspect in the creation of a successful parsing system.</S>
			<S sid ="235" ssid = "161">On the one hand the syntactic annotations in the treebank are needed in order to train the parser, and on the other hand the information provided by the morphological analyzer is needed in order to provide a good lexical coverage.</S>
			<S sid ="236" ssid = "162">We discuss an approach to bridging this discrepancy in Section 6.</S>
			<S sid ="237" ssid = "163">130 2.4 Section Summary.</S>
			<S sid ="238" ssid = "164">To summarize, the Hebrew language and its analysis poses several challenges to parser design: The amount of annotated material is relatively small, precluding the possibility of learning robust lexical parameters from the annotated corpora.</S>
			<S sid ="239" ssid = "165">The productive nature of the morphology results in many word forms, adding another obstacle to estimating lexical parameters from annotated data.</S>
			<S sid ="240" ssid = "166">The nature of the word-formation mechanism in Hebrew makes it hard to guess the morphological analysis of a word based on its prefix and suffix alone as is done in other languages, requiring the use of a more complex system for handling unknown words.</S>
			<S sid ="241" ssid = "167">Many function words in Hebrew are not separated by white space but are instead attached to the next token, making the observed word sequence ambiguous.</S>
			<S sid ="242" ssid = "168">Word segmentation needs to be performed in addition to syntactic disambiguation.</S>
			<S sid ="243" ssid = "169">Successful word segmentation may rely on syntactic disambiguation, suggesting that it is better to perform the segmentation and syntactic-disambiguation tasks jointly.</S>
			<S sid ="244" ssid = "170">Finally, Hebrew grammar requires various forms of morphological agreement, a fact which hopefully can help disambiguate otherwise ambiguous syntactic structures.</S>
			<S sid ="245" ssid = "171">The syntactic parser should be able to make use of agreement information.</S>
			<S sid ="246" ssid = "172">In terms of existing resources, Hebrew has a small treebank annotated with constituency structure and a broad-coverage, manually constructed, lexicon-based morphological analyzer.</S>
			<S sid ="247" ssid = "173">The morphological analyzer is capable of providing the possible morphological analyses for many lexical forms, and it is extended using a machine- learning technique to also provide possible analyses for word-forms not covered by the lexicon.</S>
			<S sid ="248" ssid = "174">The extended lexicon provides a good lexical coverage of Hebrew.</S>
			<S sid ="249" ssid = "175">Also available is a morphological disambiguator that is capable of associating probabilities to the possible analyses of the lexical forms in the lexicon, and disambiguating the analyses of a sequence of lexical items in context based on a sequential model.</S>
			<S sid ="250" ssid = "176">The constituency treebank can be used to learn the parameters of a syntactic-model of Hebrew, and the morphological analyzer can be used to provide broad-coverage lexical knowledge.</S>
			<S sid ="251" ssid = "177">Unfortunately, the treebank and the lexicon/disambiguator follow different annotation schemes, and are therefore incompatible with each other.</S>
			<S sid ="252" ssid = "178">The annotation gap between the two resources must be bridged before they can be used together.</S>
			<S sid ="253" ssid = "179">We now turn to survey the components of our Hebrew parsing system.</S>
	</SECTION>
	<SECTION title="Latent-Annotation State-Split Grammars (PCFG-LA). " number = "3">
			<S sid ="254" ssid = "1">Klein and Manning (2003) demonstrated that linguistically informed splitting of non- terminal symbols in treebank-derived grammars can result in accurate grammars.</S>
			<S sid ="255" ssid = "2">Their work triggered investigations in automatic grammar refinement and state-splitting (Matsuzaki, Miyao, and Tsujii 2005; Prescher 2005), which was then perfected in work by Petrov and colleagues (Petrov et al. 2006; Petrov and Klein 2007; Petrov 2009).</S>
			<S sid ="256" ssid = "3">State-split models assume that each non-terminal label has a latent annotation that should be recovered.</S>
			<S sid ="257" ssid = "4">Instead of a single NP symbol, these models hypothesize that there are many different NP symbols, NP1 , . . .</S>
			<S sid ="258" ssid = "5">, NPk , and each is used in a different context.</S>
			<S sid ="259" ssid = "6">The labels are hidden, however, and we can only observe the core category label (NP).</S>
			<S sid ="260" ssid = "7">The job of the training process is to come up with the hidden set of label assignments to non-terminals, such that the resulting grammar assigns a high probability to the observed treebank data.</S>
			<S sid ="261" ssid = "8">Such models are called PCFG with latent annotations (PCFG- LA) and are shown empirically to produce very accurate parsing results.</S>
			<S sid ="262" ssid = "9">131 The model of Petrov et al.</S>
			<S sid ="263" ssid = "10">(2006) and its publicly available implementation, the BerkeleyParser,11 learns the latent annotations by starting with a bare-bones treebank- derived grammar and automatically refining it in split-merge-smooth cycles, setting the parameters using EM.</S>
			<S sid ="264" ssid = "11">We provide a brief description of the model and learning process (refer to Petrov et al. 2006; Petrov and Klein 2007; Petrov 2009 for the full details).</S>
			<S sid ="265" ssid = "12">The learning works by following an iterative split-merge-smooth cycle, in which the following steps are performed repetitively: Splitting each non-terminal category in two All of the grammar symbols are split.</S>
			<S sid ="266" ssid = "13">In the first round, NP is split into NP1 and NP2 . In the second round these are split into NP11 , NP12 , NP21 , NP22 , and so forth.</S>
			<S sid ="267" ssid = "14">Each splitting round results in new grammar in which a rule of the form A → BC is replaced by eight rules, the result of splitting each A, B, and C in two.</S>
			<S sid ="268" ssid = "15">An EM procedure is then used to set the probabilities of each of the split rules.</S>
			<S sid ="269" ssid = "16">The EM training is constrained by the grammar on the one hand and by the annotated tree structures on the other.</S>
			<S sid ="270" ssid = "17">Merging back non-effective splits Not all of the splits are useful.</S>
			<S sid ="271" ssid = "18">For example, the punctuation POS tag will always result in punctuation, and there is no reason to split it into two punctuation POS tags.</S>
			<S sid ="272" ssid = "19">Having a grammar with too many states is difficult to manage in terms of memory, storage, and parsing time, and is also prone to overfitting the data.</S>
			<S sid ="273" ssid = "20">Thus, the model aims to undo splits if they are not useful.</S>
			<S sid ="274" ssid = "21">The splits are evaluated based on an information gain criteria, and splits that are not useful are merged back into their parent symbol, resulting in a smaller grammar (if the symbols B1 and B2 are merged back into B, the rules A → B1 C and A → B2 C are merged into A → B C).</S>
			<S sid ="275" ssid = "22">The merging step is also followed by an EM procedure for setting the rule probabilities for the resulting grammar.</S>
			<S sid ="276" ssid = "23">Smoothing the split non-terminals toward their shared ancestor Finally, split symbols may still share some information (although an NP in subject position and an NP in object position behave differently, they also retain some common properties).</S>
			<S sid ="277" ssid = "24">The smoothing procedure joggles the probability mass of the grammar and moves some probability from the split symbol to its parent.</S>
			<S sid ="278" ssid = "25">This step is also followed by parameter re-estimation using EM.</S>
			<S sid ="279" ssid = "26">Performing five or six such split-merge-smooth cycles results in accurate grammars, with annotations that capture many latent syntactic interactions.</S>
			<S sid ="280" ssid = "27">Six cycles mean that symbols can have as many as 64 different substates.</S>
			<S sid ="281" ssid = "28">At inference time, the latent annotations are (approximately) marginalized out, resulting in the (approximate) most probable unannotated tree according to the refined grammar (the score of the unsplit rule A → B C is taken to be L:;x L:;y L:;z Ax → By Cz ).</S>
			<S sid ="282" ssid = "29">The grammar learning process is applied to binarized parse trees, with first-order vertical and zeroth-order horizontal markovization (Klein and Manning 2003).</S>
			<S sid ="283" ssid = "30">This means that in the initial grammar, each of the non-terminal symbols is effectively conditioned on its parent alone, and is independent of its sisters.</S>
			<S sid ="284" ssid = "31">For example, the rule S → NP VP NP PP is binarized as: S → NP @S @S → VP @S @S → NP @S @S → PP 11 http:/ code.google.com/p/berkeleyparser/.</S>
			<S sid ="285" ssid = "32">132 indicating that S rules start with an NP, can be followed by a sequence of zero or more NPs and VPs, and end with a PP.</S>
			<S sid ="286" ssid = "33">Such an extreme markovization suggests a very strong independence assumption, and is too permissive on its own.</S>
			<S sid ="287" ssid = "34">It allows the resulting refined grammar to encode its own set of dependencies between a node and its sisters, however, as well as ordering preferences in long, flat rules.</S>
			<S sid ="288" ssid = "35">For example, the binarized grammar allows the production S → NP NP PP, which may be incorrect.</S>
			<S sid ="289" ssid = "36">However, by annotating the symbols as follows: S → NP @S1 @S1 → VP @S2 @S2 → NP @S2 @S2 → PP the grammar now forces the VP to be produced before the NP, but still allows the NP to be dropped.</S>
			<S sid ="290" ssid = "37">Similarly, by annotating the symbols as: S → NP @S1 @S1 → VP @S2 @S2 → NP @S3 @S3 → PP the grammar effectively allows only the original rule to be produced.</S>
			<S sid ="291" ssid = "38">Initial experiments on Hebrew confirm that moving to higher order horizontal markovization (encoding more context in the initial binarized rules) degrades parsing performance, while producing much larger grammars.</S>
			<S sid ="292" ssid = "39">The PCFG-LA parsing methodology is very robust, producing state-of-the-art accuracies for English, as well as many other languages including German (Petrov and Klein 2008), French (Candito, Crabbe´ , and Seddah 2009), and Chinese (Huang and Harper 2009).</S>
	</SECTION>
	<SECTION title="Baseline Experiments. " number = "4">
			<S sid ="293" ssid = "1">The baseline system is an “out-of-the-box” PCFG-LA parser, as described in Petrov et al.</S>
			<S sid ="294" ssid = "2">(2006) and Petrov and Klein (2007) and implemented in the BerkeleyParser.12 The parser is trained on the Modern Hebrew Treebank (see Section 9 for the exact experimental settings) after stripping all the functional and morphological information from the non-terminals.</S>
			<S sid ="295" ssid = "3">We evaluate the resulting models on the development set, and consider three settings: Seg+POS Oracle: The parser has access to the gold segmentation and POS tags.</S>
			<S sid ="296" ssid = "4">Seg Oracle: The parser has access to the gold segmentation, but not the POS tags.</S>
			<S sid ="297" ssid = "5">Pipeline: A POStagger is used to perform word segmentation, which is then used as parser input.</S>
			<S sid ="298" ssid = "6">A better tag set.</S>
			<S sid ="299" ssid = "7">Glossing over the parses revealed that the parser failed to learn the distinction between finite and non-finite verbs.</S>
			<S sid ="300" ssid = "8">The importance of this linguistic 12 http:/ code.google.com/p/berkeleyparser/.</S>
			<S sid ="301" ssid = "9">133 Table 1 Baseline: Out-of-the-box BerkeleyParser performance on the dev-set.</S>
			<S sid ="302" ssid = "10">Sett ing T a g s e t F1 (4 cyc les ) F1 (5 cyc les ) Seg +PO S Ora cle Co re 8 9 . 7 8 9 . 5 Seg Ora cle Co re 8 2 . 6 8 3 . 6 Pip elin e Co re 7 6 . 3 7 7 . 2 Seg +PO S Ora cle Co re +V er bs 8 9 . 9 9 0 . 9 Seg Ora cle Co re +V er bs 8 3 . 3 8 3 . 6 Pip elin e Co re +V erb s 7 7 . 1 7 7 . 3 distinction for parsing is obvious, and was also noted in Klein and Manning (2003) for English and in our previous work on parsing Hebrew (Goldberg and Tsarfaty 2008).</S>
			<S sid ="303" ssid = "11">Finite and non-finite verbs are easily distinguishable from each other based on surface form alone.</S>
			<S sid ="304" ssid = "12">Although finiteness is clearly annotated in the treebank, it is not on the “core” part of the POS tags and was removed prior to training the parser.</S>
			<S sid ="305" ssid = "13">In a second set of experiments the core tag set of the parser was modified to distinguish finite verbs, infinitives, and modals.13 The original core–tag set already includes some important distinctions, such as construct from non-construct nouns.</S>
			<S sid ="306" ssid = "14">Results and discussion.</S>
			<S sid ="307" ssid = "15">Table 1 presents the parsing results on the development set.</S>
			<S sid ="308" ssid = "16">With gold POS tags and segmentation, the results are very high.</S>
			<S sid ="309" ssid = "17">Accuracy drops considerably when the parser is not given access to the gold tags (from about 90 to less than 84 F1 ), indicating that the POS tags are both informative and ambiguous.</S>
			<S sid ="310" ssid = "18">Results drop even further (from 84 to 77) in the pipeline case where the gold segmentation is not available, indicating that correct segmentation also provides valuable information to the parser and that segmentation mistakes are costly.</S>
			<S sid ="311" ssid = "19">Enriching the tag set to distinguish modals and finite and infinite verbs proved useful, with an increase of about 1 F1 points (absolute) after four split-merge-smooth cycles, and a smaller increase after five cycles.</S>
			<S sid ="312" ssid = "20">This stresses the importance of the core representation: The automatic learning procedure goes a long way, but it can be aided by linguistically motivated manual interventions in some cases.</S>
			<S sid ="313" ssid = "21">4.1 Analyzing the Learned PCFG-LA Grammar.</S>
			<S sid ="314" ssid = "22">4.1.1 Terminal-Level (Lexical) Splits.</S>
			<S sid ="315" ssid = "23">We begin by inspecting the splits at the part-of- speech level.</S>
			<S sid ="316" ssid = "24">Table 2 displays the number of splits learned for each of the parts-of-speech symbols.</S>
			<S sid ="317" ssid = "25">Prepositions are the most heavily split, followed closely by the somewhat- generic MOD tag and the nouns.</S>
			<S sid ="318" ssid = "26">Nouns and adjectives.</S>
			<S sid ="319" ssid = "27">The noun and adjective splits are somewhat hard to decipher.</S>
			<S sid ="320" ssid = "28">Some of the groups are obvious (things appearing after numbers, last names, parts-of-dates, time related, places, etc.).</S>
			<S sid ="321" ssid = "29">Others are are much harder to interpret.</S>
			<S sid ="322" ssid = "30">13 Unlike previous work, the distinction is retained only at the POS tag level and not propagated to the.</S>
			<S sid ="323" ssid = "31">phrase level.</S>
			<S sid ="324" ssid = "32">The tag-level information is sufficient for the parser to learn the phrase-level distinctions on its own.</S>
			<S sid ="325" ssid = "33">Similar observations regarding the usefulness and sufficiency of linguistically motivated manual state-splitting of preterminals (as opposed to tree-internal nodes) prior to training a latent-variable grammar were also made by Crabbe´ and Candito (2008).</S>
			<S sid ="326" ssid = "34">134 Table 2 Number of learned splits per POS category after five split-merge cycles.</S>
			<S sid ="327" ssid = "35">Tag # Splits Tag # Splits H 1 C D T 6 HA M 1 C C 7 PO S 1 D T 7 RE L 1 JJ 7 VB 1 V B I N F 7 AT 2 P R P 8 CO M 2 C D 1 0 JJT 2 R B 1 3 QW 2 N N 1 6 RB R 2 N N P 1 7 VB MD 2 N N T 2 2 WD T 2 M O D 2 4 AG R 4 I N 2 6 AU X 6 MOD.</S>
			<S sid ="328" ssid = "36">For the general-modification POS tags, most categories clearly single out one or two words with very specific usage patterns, such as אל (‘no’), םג (‘also’), קר (‘only’), וליפא (‘even’), רבעשל (‘former ’), and so forth.</S>
			<S sid ="329" ssid = "37">The other categories are harder to interpret.</S>
			<S sid ="330" ssid = "38">Verbs.</S>
			<S sid ="331" ssid = "39">Finite-verbs are not split at all, even though they form an open-class category.</S>
			<S sid ="332" ssid = "40">Modal verbs are split into two groups: One of them is dominated by nine modals (רשפא, שי, הארנ, השק, ןיא, ןתינ, המוד, בושח, ןוכנ, roughly corresponding to the English could, should, seem/appear, hard, shouldn’t, possible, appear/seem, important, fitting/required); and the second contains all the others.</S>
			<S sid ="333" ssid = "41">This is an interesting distinction, as the nine singled- out modals never take a subject, whereas the modals in the other group do.14 Infinitive verbs are split into seven categories, six of which are dominated by one or two words each, and the last is a catchall category.</S>
			<S sid ="334" ssid = "42">Coordination and question-words.</S>
			<S sid ="335" ssid = "43">Coordination words are heavily split, each of the categories dominated by one or two words, indicating different usage patterns.</S>
			<S sid ="336" ssid = "44">The question words המ (‘what’) and ימ (‘who’) are singled out from the rest.</S>
			<S sid ="337" ssid = "45">Gender/number agreement.</S>
			<S sid ="338" ssid = "46">The verbs are not split at all, indicating that the learned grammar cannot model subject–verb agreement.</S>
			<S sid ="339" ssid = "47">Pronouns are split by type (personals, demonstrative, and subtypes of demonstratives), but not by gender and number.</S>
			<S sid ="340" ssid = "48">Noun and adjective splits are sometimes hard to decipher, but they do not exhibit any grouping based on gender or number properties, indicating that the grammar cannot model adjective–noun agreement.</S>
			<S sid ="341" ssid = "49">Category splits for the AGR tag do show a clear division that follows gender and number, but it is unclear what is captured by this division as the information cannot interact with nouns, adjectives, verbs, or pronouns.</S>
			<S sid ="342" ssid = "50">14 In fact, the nine modals are very similar in characterization to the words identified in Netzer et al.</S>
			<S sid ="343" ssid = "51">(2007).</S>
			<S sid ="344" ssid = "52">as modals, whereas many of the modals in the other group are not necessarily considered as modal outside of the treebank guidelines.</S>
			<S sid ="345" ssid = "53">135 Table 3 Number of learned splits per NT-category after five split-merge cycles.</S>
			<S sid ="346" ssid = "54">Tag # Splits Tag # Splits FR AG Q 1 A D V P 1 6 INT J 6 S 1 6 FR AG 7 P P 2 2 SQ 7 V P 2 2 PR N 8 P R E D P 2 5 ADJ P 1 4 N P 3 2 SB AR 1 4 4.1.2 Grammar-Level Splits.</S>
			<S sid ="347" ssid = "55">Table 3 shows the number of splits learned for each grammar non-terminal.</S>
			<S sid ="348" ssid = "56">The NP category is the most heavily split, followed by predicative phrases, verb phrases, and PPs.</S>
			<S sid ="349" ssid = "57">With the exception of the FRAGQ category, all symbols are split into at least six substates.</S>
			<S sid ="350" ssid = "58">What information is encapsulated in the state splits?</S>
			<S sid ="351" ssid = "59">As noted by Petrov et al.</S>
			<S sid ="352" ssid = "60">(2006), the latent state-splits learned for the grammar symbols are harder to analyze.</S>
			<S sid ="353" ssid = "61">One way of shedding some light on the meanings of the split-states is by using the grammar in generation mode and by sampling word sequences from each of the states.15 By looking at the resulting strings, one can sometimes infer the kind of information encoded in the grammar.</S>
			<S sid ="354" ssid = "62">NP.</S>
			<S sid ="355" ssid = "63">The split-NPs encode phrase length (some splits result in very long NPs, some in very short, some in very specific one- or two-word patterns).</S>
			<S sid ="356" ssid = "64">They also encode the definiteness rules (either an NP is definite or not), the interaction between definiteness and the AT marker, and a limited interaction between definiteness and construct nouns.</S>
			<S sid ="357" ssid = "65">Other NP splits are dedicated to pronouns or to question words, or encode proper names, monetary units, and numbers.</S>
			<S sid ="358" ssid = "66">SBAR.</S>
			<S sid ="359" ssid = "67">The split-SBARs are split according to the word introducing the SBAR.</S>
			<S sid ="360" ssid = "68">In addition, some split-SBARs encode quoted and parenthetical items.</S>
			<S sid ="361" ssid = "69">S. The split-Ss differ by length.</S>
			<S sid ="362" ssid = "70">In addition, some S splits seem to be modeling verb-less sentences, variations in word order, and sentence-level coordination.</S>
			<S sid ="363" ssid = "71">4.2 Limitation of PCFG-LA Parsing of Modern Hebrew.</S>
			<S sid ="364" ssid = "72">The PCFG-LA baseline is a strong one, and is substantially higher than all previous reported results for Hebrew parsing in each of the setups (Seg+POS oracle, Seg Oracle, and no Oracle).</S>
			<S sid ="365" ssid = "73">We also identify some of its limitations, namely: Missed splits.</S>
			<S sid ="366" ssid = "74">The learning procedure is not perfect, and fails to capture some linguistically meaningful state-splits.</S>
			<S sid ="367" ssid = "75">When such splits are manually supplied (i.e., the trivial split of verbal types) accuracy improves.</S>
			<S sid ="368" ssid = "76">15 Sampling a word sequence is performed by starting at a given state (a split grammar symbol), randomly.</S>
			<S sid ="369" ssid = "77">choosing a right-hand-side based on the PCFG-induced distribution, expanding the state into the chosen right-hand side, and continuing recursively until we are left with only strings.</S>
			<S sid ="370" ssid = "78">136 Sensitivity to non-gold POS.</S>
			<S sid ="371" ssid = "79">The substantial drop in accuracy when the POS tags are unobserved and need to be predicted is staggering, which suggests that it is difficult for the parser to assign part-of-speech tags.</S>
			<S sid ="372" ssid = "80">Of the 698 part-of-speech errors, 314 are on words not seen in training.</S>
			<S sid ="373" ssid = "81">Sensitivity to non-gold segmentation.</S>
			<S sid ="374" ssid = "82">The accuracy drops even further when the parser is presented with predicted segmentation.</S>
			<S sid ="375" ssid = "83">Segmentation errors are detrimental to the parser.</S>
			<S sid ="376" ssid = "84">Not encoding grammatical agreement.</S>
			<S sid ="377" ssid = "85">Finally, the learned grammar does not encode grammatical agreement.</S>
			<S sid ="378" ssid = "86">Whereas the majority of the parser mistakes are due to the flexible constituent order or “standard” ambiguities such as coordination and PP attachment, a handful of them could be resolved using agreement information.</S>
			<S sid ="379" ssid = "87">In what follows, we address these four limitations, and substantially increase the parser accuracy for the realistic case where gold segmentation and POS tags are not available.</S>
	</SECTION>
	<SECTION title="Manual State-Splits. " number = "5">
			<S sid ="380" ssid = "1">We experimented with several linguistically motivated state-splits which were added as tree-annotations prior to running the parser.</S>
			<S sid ="381" ssid = "2">Most of them did not help on their own and slightly degraded parser performance when combined with other splits.</S>
			<S sid ="382" ssid = "3">These include splits which were proven useful in previous work, such as marking of definite NPs, and distinguishing possessive from other PPs.</S>
			<S sid ="383" ssid = "4">We also experimented with splits based on morphological agreement features, which are discussed in Section 8.1.</S>
			<S sid ="384" ssid = "5">Overall, the learning procedure is capable of producing good splits on its own.</S>
			<S sid ="385" ssid = "6">We did, however, manage to improve upon it with the following annotation (the annotations were removed prior to evaluation).</S>
			<S sid ="386" ssid = "7">Subject NPs.</S>
			<S sid ="387" ssid = "8">Hebrew phrase order is rather flexible, and the subject can appear before or after the verb.</S>
			<S sid ="388" ssid = "9">Identifying the subject can thus help in grounding the overall structure of the sentence.</S>
			<S sid ="389" ssid = "10">The subject is also dependent on agreement constraints with the verb.</S>
			<S sid ="390" ssid = "11">Following Johnson (1998), Klein and Manning (2003) implicitly annotate subject-NPs in English using parent annotation (distinguishing NPs under S from other NPs), with good results.</S>
			<S sid ="391" ssid = "12">When applied to English, the PCFG-LA also learns to model subject NPs well.</S>
			<S sid ="392" ssid = "13">Hebrew’s non-configurationality, however, put both Subjects and Objects directly under S, making it much harder to learn the distinction automatically.</S>
			<S sid ="393" ssid = "14">Explicit marking of subject NPs contributes slightly to the accuracy of the parser.</S>
			<S sid ="394" ssid = "15">Perhaps more important than the small increase in accuracy is the fact that the parser can identify subjects relatively well.</S>
			<S sid ="395" ssid = "16">In contrast, marking of object NPs did not help by itself and slightly degraded the parsing accuracy when combined with other annotations.</S>
			<S sid ="396" ssid = "17">Note, however, that Hebrew definite objects are already clearly marked using the תא marker, making them an easy target for the parser.</S>
	</SECTION>
	<SECTION title="Better Lexical Coverage with an External Lexicon. " number = "6">
			<S sid ="397" ssid = "1">The drop in parsing accuracy when gold core POS tags are not available and need to be inferred by the parser is huge (from above 90 to less than 84 F1 ).</S>
			<S sid ="398" ssid = "2">137 The large number of possible word forms make it very difficult for manually annotated corpora to provide adequate lexical coverage.</S>
			<S sid ="399" ssid = "3">The problem is even more severe with the case of the Hebrew Treebank, which is especially small.</S>
			<S sid ="400" ssid = "4">Although it is big enough to learn meaningful syntactic generalizations (as demonstrated by the high performance of the baseline system) it is far too small to learn a good lexical model (as evidenced by the drop in accuracy when gold tags are not available).</S>
			<S sid ="401" ssid = "5">We suggest increasing the lexical coverage of the parser using an external resource, namely, a lexicon-based morphological analyzer.</S>
			<S sid ="402" ssid = "6">We further extend the utility of the analyzer with lexical tagging probabilities learned from an unannotated corpus.</S>
			<S sid ="403" ssid = "7">6.1 A Unified Lexical Probability Model We would like to use the KC Analyzer (Section 2.3.3) to increase the lexical coverage of the treebank-trained parser.</S>
			<S sid ="404" ssid = "8">That is, we would like to improve the lexical model P(T → W ) of the generative parser.</S>
			<S sid ="405" ssid = "9">As discussed in Section 2.3.5, however, the tag sets used by the two resources differ.</S>
			<S sid ="406" ssid = "10">How can this difference be reconciled?</S>
			<S sid ="407" ssid = "11">One possibility is to re-tag the treebank with the KC tag set and then train on this unified resource.</S>
			<S sid ="408" ssid = "12">In Goldberg et al.</S>
			<S sid ="409" ssid = "13">(2009), we show that this procedure degrades parser performance.</S>
			<S sid ="410" ssid = "14">Instead, Goldberg et al. suggest a layered generative approach that retains the benefits of the treebank tagging for frequent words and resorts to the KC tag set only for rare and unseen words.</S>
			<S sid ="411" ssid = "15">Under this approach, frequent words are generated from treebank POS tags as usual, but rare words follow a generative process in which first the treebank tag generates a KC tag, and then the KC-tag generates the word.</S>
			<S sid ="412" ssid = "16">A sample derivation using this layered representation is presented in Figure 1.</S>
			<S sid ="413" ssid = "17">The Treebank-to-KC tag generation probabilities represent a fuzzy, probabilistic mapping between the two resources.</S>
			<S sid ="414" ssid = "18">In Goldberg et al.</S>
			<S sid ="415" ssid = "19">(2009), the estimation of these probabilities was done based on a re-tagging of the treebank to use the KC tag set.</S>
			<S sid ="416" ssid = "20">The re-tagging process was far from trivial, and many tagging cases required extensive debates between human annotators.</S>
			<S sid ="417" ssid = "21">Here, we present a new procedure which does not require the treebank to be re- tagged with a new tag set.</S>
			<S sid ="418" ssid = "22">It still uses the layered representation, but instead of forcing one unique KC analysis for each location, it embraces the uncertainty and allows all of them.</S>
			<S sid ="419" ssid = "23">This is done by treating the KC-tag assignments as hidden variables, learning the TB-KC mapping probabilities as part of the grammar training EM process, and marginalizing the KC tags out for the final tree.</S>
			<S sid ="420" ssid = "24">The procedure is based on the following assumptions: • We have access to trees in which the POS tags ttb are taken from a given tag set TTB . . JJTB PRP-M-S-3-DEMExt הז Figure 1 A layered POS tag representation.</S>
			<S sid ="421" ssid = "25">138 . . .</S>
			<S sid ="422" ssid = "26">NNTB NN-M-SExt NNT-M-SExt VB-M-S-3-PastExt VB-M-S-3-ImpExt עקר Figure 2 A latent layered POS tag representation.</S>
			<S sid ="423" ssid = "27">• We have additional access to an external resource (lexicon) mapping words to tags text from a different tag set TExt . • Probabilities involving words which are frequent in the treebank can and should be based on treebank counts.</S>
			<S sid ="424" ssid = "28">• Probabilities involving less frequent words should be smoothed in with information from the external lexicon.</S>
			<S sid ="425" ssid = "29">• Smoothing should have a greater effect on less-frequent words.</S>
			<S sid ="426" ssid = "30">• Probabilities for unseen words should be based solely on the external lexicon.</S>
			<S sid ="427" ssid = "31">Figure 2 illustrates the representation used for words which are rare or unseen in the treebank training data.</S>
			<S sid ="428" ssid = "32">The treebank tag NNTB (upper level) generates the word-form עקר (lower level) by considering all the possible KC POS tags allowed for the word in the morphological analyzer (the middle level).</S>
			<S sid ="429" ssid = "33">The probabilities related to generating the KC POS tags are summed, and all the other probabilities are multiplied.</S>
			<S sid ="430" ssid = "34">The exact equations are detailed in the following.</S>
			<S sid ="431" ssid = "35">Although the needed quantity is the emission probability P(TTB → W ) = P(W|TTB ), it is more convenient (for a reason which will be discussed later) to work with the tagging probability P(TTB |W ).</S>
			<S sid ="432" ssid = "36">Once the tagging probabilites P(TTB |W ) are available, they can easily be converted to emission probabilities using Bayesian inversion, based on the relative-frequency estimates of P(W ) and P(TTB ) which are calculated from the treebank:16 P(ttb |w)P(w) = P(w|t P(ttb ) tb ) = P(ttb → w) (1) 16 Our notation uses capital letters to denote random variables, and lowercase letters to denote specific.</S>
			<S sid ="433" ssid = "37">events.</S>
			<S sid ="434" ssid = "38">Thus, P(T|W ) refers the distributions in which a tag ∈ T is condition on a word ∈ W, P(T|w) refers to the conditional distribution of tags t ∈ T given a specific word w, and P(t|w) refers to the probability mass of the specific tag t given word w. 139 Let us now focus on estimating the tagging probabilities P(TTB |W ) for the cases of frequent, rare, and OOV words.</S>
			<S sid ="435" ssid = "39">For frequent words that are seen more than K times in the treebank, we simply use treebank-based relative-frequency estimates:17 Ptb (ttb|w) = c(w, ttb ) c(w) (2) where c(·) is a counting function.</S>
			<S sid ="436" ssid = "40">For OOV words that are not seen in the treebank, the tagging probability is estimated using: Poov (ttb|w) = )&apos; text ∈TExt P(text |w)P(ttb |text ) (3) where P(TExt |W ) is a tagging probability using the external tag set, and P(TTB |TExt ) is a transfer probability relating the tags from the two tag sets (the estimation of these two probabilities is discussed subsequently).</S>
			<S sid ="437" ssid = "41">What this does is assume a process in which the word is tagged by first choosing a tag according to the external lexicon, and then choosing a tag from the TB tag set based on the external one.</S>
			<S sid ="438" ssid = "42">The external tag assignments are then treated as latent variables, and are marginalized out.</S>
			<S sid ="439" ssid = "43">Finally, for rare words that are seen only a few times in the treebank, we interpolate the two quantities, weighted by the word’s frequency in the treebank: c(w)Ptb (ttb|w) + Poov (ttb |w) Prare (ttb |w) = 1 + c(w) (4) We now turn to describing the estimation of the external tagging probability P(TExt |W ) and the tag transfer probability P(TTB |TExt ).</S>
			<S sid ="440" ssid = "44">Estimating P(TExt |W ).</S>
			<S sid ="441" ssid = "45">The tagging probability follows the morphological analyzer.</S>
			<S sid ="442" ssid = "46">The analyzer provides the possible analyses, but does not provide probabilities for them.</S>
			<S sid ="443" ssid = "47">One simple option would be to assign each possible analysis (tag) a uniform probability, and assign 0 probability for tags not allowed by the lexicon for the given word.</S>
			<S sid ="444" ssid = "48">This method is referred to as Punif (TExt |W ).</S>
			<S sid ="445" ssid = "49">We know that not all the possible analyses for a given word are equally likely, however, and in practice, the actual tagging distribution is usually biased toward one or two of the tags.</S>
			<S sid ="446" ssid = "50">These tagging preferences can be learned in an unsupervised manner given the lexicon and a large corpus of unannotated text, using EM training of an HMM tagging model.</S>
			<S sid ="447" ssid = "51">Adler and Elhadad (2006) suggest such a model for accurate tagging of Hebrew, and Adler (2007) and Goldberg, Adler, and Elhadad (2008) extend it to provide state-of-the-art tagging accuracies for Hebrew using a smart initialization.</S>
			<S sid ="448" ssid = "52">Here, we use the pseudo-counts from 17 In practice, a small amount of smoothing is added to allow tagging a word with open-class tags if it.</S>
			<S sid ="449" ssid = "53">wasn’t seen within the treebank: Ptb (ttb |w) = (c(w, ttb ) + 0.0001 ∗ P(ttb ))/(c(w) + 0.0001).</S>
			<S sid ="450" ssid = "54">140 the final round of EM training in this tagging model in order to compute Pem (TExt |W ).</S>
			<S sid ="451" ssid = "55">We show in Section 9 that this unsupervised lexical probabilities estimation does indeed provide better parsing results.</S>
			<S sid ="452" ssid = "56">Estimating P(TTB |TExt ).</S>
			<S sid ="453" ssid = "57">The tagset-transfer probabilities capture the patterns of transfer between the syntactic tagging scheme of the treebank and the other tagging scheme of the external resource.</S>
			<S sid ="454" ssid = "58">They are estimated using treebank counts and the tagging distribution P(TExt |W ): c(ttb, text ) L:;w c(ttb, w)P(text |w) P(ttb|text ) = c(t = ext ) L:; w c(w)P(text|w) (5) Integration into the PCFG-LA model.</S>
			<S sid ="455" ssid = "59">The estimation procedure is incorporated into the training process of the PCFG-LA model.</S>
			<S sid ="456" ssid = "60">Note that in the PCFG-LA model the treebank tag set TTB is gradually split, and each tag takes the form (tag, substate), where substate is a latent variable indicating a specific split of the given tag.</S>
			<S sid ="457" ssid = "61">This means that the treebank tagging probability and the tag set–transfer probabilities are also defined over these split tags.</S>
			<S sid ="458" ssid = "62">Whereas the external tagging probabilities P(TExt |W ) are fixed prior to PCFG-LA training, the other distributions (P(TTBsubstate |W ) and P(TTBsubstate |TExt )) are re estimated in the EM process following each of the split, merge, and smooth stages.</S>
			<S sid ="459" ssid = "63">This is done by replacing the corpus counts c(·) in Equations (2) and (5) with pseudo-counts (expectations, marginal scores) of the same events in the E step of the EM procedure.</S>
			<S sid ="460" ssid = "64">The main reason for using the Bayesian inversion (Equation (1)) instead of working with the emission probability P(W|T) directly is that the emission probability is highly dependent on the vocabulary size.</S>
			<S sid ="461" ssid = "65">The treebank estimates are based on a small vocabulary, the external lexicon estimates are based on a very large vocabulary, and a proper combination of the two emission probabilities is not trivial.</S>
			<S sid ="462" ssid = "66">In contrast, the tagging probabilities do not depend on the vocabulary size, allowing a very simple combination.</S>
			<S sid ="463" ssid = "67">We can then base the counts for the emission probability on the treebank vocabulary alone, and estimate P(W ) for words unseen in training as if they were seen once.</S>
	</SECTION>
	<SECTION title="Joint Segmentation and Parsing. " number = "7">
			<S sid ="464" ssid = "1">When applied to real text (for which the gold word-segmentation is not available), the baseline PCFG-LA parser is supplied with word segmentation produced by a separate tagging process.18 This seriously degrades parsing performance.</S>
			<S sid ="465" ssid = "2">A major reason for the performance drop is that the word-segmentation task and the syntactic-disambiguation task are highly related.</S>
			<S sid ="466" ssid = "3">Segmentation mistakes drive the parser toward wrong syntactic structures, and many segmentation decisions require long-distance information that is not available to a sequential process (Tsarfaty 2006a).</S>
			<S sid ="467" ssid = "4">For these reasons, we claim that parsing and segmentation should be performed jointly.</S>
			<S sid ="468" ssid = "5">18 Although the tagger also produces POS tag assignments, we ignore them and use only the word.</S>
			<S sid ="469" ssid = "6">segmentation.</S>
			<S sid ="470" ssid = "7">This is done for two reasons: first, the tag set of the tagger is the one used by the morphological analyzer, and is not compatible with the treebank.</S>
			<S sid ="471" ssid = "8">Second, we believe it is better for the parser to produce its own tag assignments.</S>
			<S sid ="472" ssid = "9">141 Figure 3 The lattice for the Hebrew sequence םיענה םלצב (see footnote 19).</S>
			<S sid ="473" ssid = "10">Joint segmentation and parsing can be achieved using lattice parsing.</S>
			<S sid ="474" ssid = "11">Instead of parsing over a fixed input string, the parser operates on a lattice—a structure encoding all the possible segmentations.</S>
			<S sid ="475" ssid = "12">7.1 Lattice Representation.</S>
			<S sid ="476" ssid = "13">Formally, a lattice is a directed acyclic graph in which all paths lead from the initial state to the end state.</S>
			<S sid ="477" ssid = "14">For the Hebrew segmentation task, all word segmentations of a given sentence are represented using a lattice structure.</S>
			<S sid ="478" ssid = "15">Each lattice arc corresponds to a word and its corresponding POS tag, and a path through the lattice corresponds to a specific word- segmentation and POS tagging of the sentence.</S>
			<S sid ="479" ssid = "16">This is by now a fairly standard representation for multiple morphological segmentations of Hebrew utterances (Adler 2001; Bar-Haim, Sima’an, and Winter 2005; Adler 2007; Cohen and Smith 2007; Goldberg, Adler, and Elhadad 2008; Goldberg and Tsarfaty 2008; Goldberg and Elhadad 2011).</S>
			<S sid ="480" ssid = "17">It is also used for Arabic (Green and Manning 2010) and other languages (Smith, Smith, and Tromble 2005).</S>
			<S sid ="481" ssid = "18">Figure 3 depicts the lattice for the two-words sentence םיענה םלצב.19 Double-circles indicate the space-delimited token boundaries.</S>
			<S sid ="482" ssid = "19">Note that in this construction arcs can never cross token boundaries.</S>
			<S sid ="483" ssid = "20">Every token is independent of the others, and the sentence lattice is in fact a concatenation of smaller lattices, one for each token.</S>
			<S sid ="484" ssid = "21">Furthermore, some of the arcs represent lexemes not present in the input tokens (e.g., ה/DT, לש/POS), although these are parts of valid analyses of the token.</S>
			<S sid ="485" ssid = "22">Segments with the same surface form but different POS tags are treated as different lexemes, and are represented as separate arcs (e.g., the two arcs labeled םיענ from node 6 to 7).</S>
			<S sid ="486" ssid = "23">A similar structure is used in speech recognition.</S>
			<S sid ="487" ssid = "24">There, a lattice is used to represent the possible sentences resulting from an interpretation of an acoustic model.</S>
			<S sid ="488" ssid = "25">In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.</S>
			<S sid ="489" ssid = "26">Given that weights on all outgoing arcs sum up to one, weights induce a probability distribution on the lattice paths.</S>
			<S sid ="490" ssid = "27">In sequential tagging models such as Smith, Smith, and Tromble (2005), Adler and Elhadad (2006), and Bar-Haim, Sima’an, and Winter (2008) weights are assigned according to a tagging model based on linear context.</S>
			<S sid ="491" ssid = "28">For the case of parsing, context-free weighting of lattice arcs is used: each arc 19 Whereas Hebrew is written right-to-left, the lattice is to be read left-to-right.</S>
			<S sid ="492" ssid = "29">The words on each arc. follow the Hebrew writing directions, and are written right-to-left.</S>
			<S sid ="493" ssid = "30">142 corresponds to a (tag, word) pair, and is weighted according to the emission distribution P(tag → word).20 7.2 Lattice Parsing.</S>
			<S sid ="494" ssid = "31">The CKY parsing algorithm can be extended to accept a lattice, instead of a predefined list of tokens, as its input (Chappelier et al. 1999).</S>
			<S sid ="495" ssid = "32">The CKY search then finds a tree spanning from the start-state to the end-state of the lattice, where the leaves of the tree are lattice arcs.</S>
			<S sid ="496" ssid = "33">The lattice extension of the CKY algorithm is performed by indexing lexical items according to their start- and end-states in the lattice instead of by their sentence position, and changing the initialization procedure of CKY to allow terminal and preterminal symbols of spans of sizes &gt; 1.</S>
			<S sid ="497" ssid = "34">It is then relatively straightforward to modify the parsing mechanism to support this change: not giving special treatments for spans of size 1, and distinguishing lexical items from non-terminals by a specified marking instead of by their position in the chart.</S>
			<S sid ="498" ssid = "35">Figure 4 shows the CKY chart for the lattice in Figure 3, together with an (incorrect) parse over the lattice.</S>
			<S sid ="499" ssid = "36">The chart is initialized with parts of speech corresponding to the lattice arcs.</S>
			<S sid ="500" ssid = "37">Phrase-structures are then built on top of the POS tags (in blue).</S>
			<S sid ="501" ssid = "38">The proposed structure must span the entire chart, and correspond to a path through the lattice from the initial state (0) to the last one (7).</S>
			<S sid ="502" ssid = "39">At training time the correct segmentation is fully observed, and the generative parser is trained as usual over the treebank.</S>
			<S sid ="503" ssid = "40">At inference (test) time, the correct segmentation is unknown, and the decoding is applied to the segmentation lattice.</S>
			<S sid ="504" ssid = "41">The best derivation returned by the parser forces a specific segmentation.</S>
			<S sid ="505" ssid = "42">The returned parse tree is the most probable (segmentation, tree) pair according to the grammar.21 We modified the PCFG-LA BerkeleyParser to accept lattice input at inference time.</S>
			<S sid ="506" ssid = "43">Lattice parsing allows us to preserve the segmentation ambiguity and present it to the parser, instead of committing to a specific segmentation prior to parsing.</S>
			<S sid ="507" ssid = "44">This way segmentation decisions are performed in the parser as part of the global search for the most probable structure, and can be affected by global syntactic considerations.</S>
			<S sid ="508" ssid = "45">We show in Section 9 that this methodology is indeed superior to the pipeline approach.</S>
			<S sid ="509" ssid = "46">Early descriptions of algorithms for parsing over word lattices can be found in Lang (1974, 1988) and Billott and Lang (1989).</S>
			<S sid ="510" ssid = "47">Lattice parsing was explored in the context of parsing of speech signals by Chappelier et al.</S>
			<S sid ="511" ssid = "48">(1999), Sima’an (1999), and Hall (2005), and in the context of joint word-segmentation and syntactic disambiguation in Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Green and Manning (2010).</S>
			<S sid ="512" ssid = "49">20 Lattice parsing for Hebrew is explored also in Cohen and Smith (2007).</S>
			<S sid ="513" ssid = "50">There, lattice arc weights are.</S>
			<S sid ="514" ssid = "51">assigned based on aggregate quantities (forward-backward tagging marginals) derived from a discriminative CRF tagging model.</S>
			<S sid ="515" ssid = "52">This approach is not ideal from a modeling perspective, as it makes each POS tag be accounted for twice: once by the syntactic model, and once by the sequential one.</S>
			<S sid ="516" ssid = "53">In this work, a sequential tagging model is not used at all.</S>
			<S sid ="517" ssid = "54">If the use of a sequential model is desired, an alternative method for integrating a sequence model and a syntactic model is making the models “negotiate” an agreed upon structure that maximizes the score under both models, using optimization techniques such as dual decomposition (Dantzig and Wolfe 1960), which was recently introduced into natural language processing (Rush et al. 2010).</S>
			<S sid ="518" ssid = "55">21 Note that finding the most probable segmentation requires summing over all the trees resulting in each.</S>
			<S sid ="519" ssid = "56">segmentation—a much harder task, proven to be NP-complete in Sima’an (1996).</S>
			<S sid ="520" ssid = "57">143 Figure 4 Lattice initialization of the CKY chart.</S>
	</SECTION>
	<SECTION title="Incorporating Morphological Agreement. " number = "8">
			<S sid ="521" ssid = "1">Inspecting the learned grammars reveal that they do not encode any knowledge of morphological agreement: The split categories for nouns, verbs, and adjectives do not group words according to any relevant morphological property such as gender or number, making it impossible for the grammar to model agreement patterns.</S>
			<S sid ="522" ssid = "2">At the same time, inspecting some of the bad parses reveals several clear cases of agreement mistakes.</S>
			<S sid ="523" ssid = "3">Can morphological agreement be incorporated in the parsing model?</S>
			<S sid ="524" ssid = "4">8.1 Forcing Morphologically Motivated Splits.</S>
			<S sid ="525" ssid = "5">Our initial attempts focused on making the PCFG-LA learning procedure pick up on agreement-relevant state-splits.</S>
			<S sid ="526" ssid = "6">When neither the core tag set nor the non-terminals encode gender and number information, it is very hard for the parser to pick up on agreement patterns.22 We attempted to train the parser on trees which mark the agreement features (either the gender, the number, or both) either on the POS tags, the relevant constituents, or 22 In the external lexicon case, the external lexicon tags do encode the morphological features, making.</S>
			<S sid ="527" ssid = "7">it possible in principle for the parser to learn to map certain substates to certain agreement features.</S>
			<S sid ="528" ssid = "8">This did not happen in practice, arguably because other structural factors were more powerful than the agreement ones.</S>
			<S sid ="529" ssid = "9">144 both.</S>
			<S sid ="530" ssid = "10">Annotating agreement features on the POS tag–level made the parsing much slower, but did make the parser assign certain split categories to certain gender–number combinations, and sampling utterances from the learned grammar did indicate a notion of grammatical agreement.</S>
			<S sid ="531" ssid = "11">This did not improve parsing accuracy, however—and even slightly degraded it.</S>
			<S sid ="532" ssid = "12">When propagating the agreement features and annotating them on the constituent level, parsing accuracy dropped considerably.</S>
			<S sid ="533" ssid = "13">When inspecting the learned grammar we observe that most of the agreement-annotated constituents (e.g., NPMasc,Plural ) were still fully split, indicating that the parser picked on patterns which were orthogonal to the agreement mechanism.</S>
			<S sid ="534" ssid = "14">The pre-splitting according to agreement-features properties caused data sparseness, aided over-fitting, and hurt parsing performance: The smoothing procedure of the BerkeleyParser shares some probability-mass between various splits of the same symbol, but was not applied in our case (no information flowed between, for example, NPMasc,Plural and NPMasc,Singular ).</S>
			<S sid ="535" ssid = "15">We attempted to counter this effect by changing the smoothing mechanism of the BerkeleyParser to share information also between the manually split symbols.</S>
			<S sid ="536" ssid = "16">This brought parsing accuracy back to the initial level, but also caused the parser to, again, not model agreement very well.</S>
			<S sid ="537" ssid = "17">The reason for this is clear in hindsight: Morphological agreement is an absolute concept, not a fuzzy one (things can either agree or not).</S>
			<S sid ="538" ssid = "18">Smoothing the probabilities between the different morphology-based split-licensed grammar rules that allow morphological disagreement, and made the grammar lose its discrimination power.</S>
			<S sid ="539" ssid = "19">This was then reinforced by the training process, which picked on other syntactic factors instead, and further phased out the agreement knowledge.</S>
			<S sid ="540" ssid = "20">A note on product-grammars.</S>
			<S sid ="541" ssid = "21">In recent work, Petrov (2010) showed that a committee of latent-variable grammars encoding different grammatical preferences can be combined into a product-grammar that is better than the individual ensemble members.</S>
			<S sid ="542" ssid = "22">Petrov created the ensemble by training several PCFG-LA parsers on the same data, but using different random seeds when initializing the EM starting point.</S>
			<S sid ="543" ssid = "23">We attempted to create a similar ensemble by providing the learning process with different linguistically motivated tree annotations (with and without encoding agreement features, with and without encoding definiteness, etc.).</S>
			<S sid ="544" ssid = "24">The combined parser did increase the performance level over that of the individual parsers, but an ensemble with the same number of components that was produced using the random-seeds approach produced far superior results.</S>
			<S sid ="545" ssid = "25">This reinforces the findings of Petrov (2010) who also reports that the ensemble creation using random initialization is exceptionally strong and outperforms other methods of ensemble creation.23 8.2 Agreement as Filter.</S>
			<S sid ="546" ssid = "26">We now turn to suggest an approach to modeling agreement, which rests on the following principles: • Agreement can be modeled as a set of hard (not probabilistic) constraints.</S>
			<S sid ="547" ssid = "27">• Agreement is completely orthogonal to the other aspects of the grammar.</S>
			<S sid ="548" ssid = "28">23 The product grammar approach with random seeds works well and is effective for improving the.</S>
			<S sid ="549" ssid = "29">accuracy of Hebrew parsing.</S>
			<S sid ="550" ssid = "30">As it is completely orthogonal to the approaches presented in this article, however, we chose not to discuss it further other than commenting on its applicability.</S>
			<S sid ="551" ssid = "31">145 Based on these principles, we suggest treating agreement as a filter, a device that can rule out illegal parses.</S>
			<S sid ="552" ssid = "32">Under the agreement-as-filter framework, we want the parser to produce the most probable parse according to its grammar and subject to hard agreement constraints.</S>
			<S sid ="553" ssid = "33">This approach completely decouples the grammar from the agreement verification mechanism.</S>
			<S sid ="554" ssid = "34">The agreement information is not modeled in the grammar and is not used to guide the search for the best parse.</S>
			<S sid ="555" ssid = "35">Instead, it is a separate process that imposes hard constraints on the search space and rules out parts of it completely.</S>
			<S sid ="556" ssid = "36">That is, agreement is a part of the parser and not of the grammar.</S>
			<S sid ="557" ssid = "37">This is similar in spirit to ideas from constraint-based grammars such as LFG (Falk 2001) and HPSG (Pollard and Sag 1994), which also model aspects of the syntax as Boolean constraints.</S>
			<S sid ="558" ssid = "38">Grammatical agreement is a relation between constituents.</S>
			<S sid ="559" ssid = "39">The relevant morphological features are propagated from one of the leaves up to the constituent level.</S>
			<S sid ="560" ssid = "40">When constituents are combined to form a larger constituent, their morphological features are assigned to the newly created constituent according to language-specific rules (it is possible that different morphological features will be assigned by different constituents).</S>
			<S sid ="561" ssid = "41">An agreement violation occurs when two or more constituents assign conflicting features to their parent.</S>
			<S sid ="562" ssid = "42">Implementation.</S>
			<S sid ="563" ssid = "43">In the implementation, an agreement-verification mechanism is manually constructed (not learned) based on a set of simple, language-dependent rules.</S>
			<S sid ="564" ssid = "44">First, we provide a set of rules to propagate the morphological agreement features from the leaves to the constituents.</S>
			<S sid ="565" ssid = "45">Then, we specify an additional set of rules to inspect local tree configuration and identify agreement violations (the Hebrew set of rules is described later, along with a concrete example).</S>
			<S sid ="566" ssid = "46">The feature-propagation mechanism works bottom–up and the agreement verification rules are very local, making it possible to integrate the filtering mechanism into a bottom–up CKY parsing algorithm (refusing to complete a constituent if it violates an agreement constraint).</S>
			<S sid ="567" ssid = "47">We did not pursue this route for the experiments in this work, however.</S>
			<S sid ="568" ssid = "48">Instead, we opted for an approximation in which we take the 100-best trees for each sentence, and choose the first tree that does not have an agreement violation (this is an approximation because the 100-best trees may not contain a valid tree, in which case we accept the agreement violation and choose the first-best tree).</S>
			<S sid ="569" ssid = "49">The specific details of the Hebrew agreement filter are given in the appendix.</S>
			<S sid ="570" ssid = "50">Verifying the hard-constraint property.</S>
			<S sid ="571" ssid = "51">We verified that the hard constraint assumption works and that the agreement verification mechanism is valid by applying the procedure to the gold-standard trees in the training-set and checking that (1) the propagated features agree with the manually marked ones, and (2) none of the training-set trees were filtered due to agreement violation.</S>
			<S sid ="572" ssid = "52">We did find a few cases in which the propagated features disagreed with the manually marked ones, and a few gold-standard trees that the mechanism marked as containing an agreement violation.</S>
			<S sid ="573" ssid = "53">All of these cases were due to mistakes in the manual annotation.</S>
			<S sid ="574" ssid = "54">Connections to parse-reranking.</S>
			<S sid ="575" ssid = "55">Our implementation is similar to parse-reranking (Charniak and Johnson 2005; Collins and Koo 2005).</S>
			<S sid ="576" ssid = "56">Indeed, if we were to model agreement as soft constraints, we could have incorporated this information as features in a reranking model.</S>
			<S sid ="577" ssid = "57">The filter approach differs in that it poses hard constraints and not soft ones, pruning away parts of the search space entirely.</S>
			<S sid ="578" ssid = "58">Thus, the use of k-best list is merely a technical detail in our implementation—the agreement information is 146 easily decomposable and the hard constraints can be efficiently incorporated into the CKY search procedure.</S>
	</SECTION>
	<SECTION title="Evaluation and Results. " number = "9">
			<S sid ="579" ssid = "1">Data set.</S>
			<S sid ="580" ssid = "2">For all the experiments we use Version 2 of the Hebrew Treebank (Guthmann et al. 2009), with the established test-train-dev splits: Sentences 484–5,740 are used for training, sentences 1–483 are the development set, and sentences 5,741–6,220 are used for the final test set.</S>
			<S sid ="581" ssid = "3">Evaluation Measure.</S>
			<S sid ="582" ssid = "4">In the cases where the gold segmentation is given, we use the well- known evalb F1 score.</S>
			<S sid ="583" ssid = "5">Namely, each tree is treated as a set of labeled constituents.24 Each constituent is represented as a 3tuple (i, j, L), in which i and j are the indices of the first and the last words in the constituent, respectively, and L is the constituency label.</S>
			<S sid ="584" ssid = "6">For example, (2, 4, NP) indicates an NP spanning from word 2 to word 4.</S>
			<S sid ="585" ssid = "7">The performance of a parser is evaluated based on the amount of constituents it recovered correctly.</S>
			<S sid ="586" ssid = "8">Let G denote the set of constituents in a gold-standard constituency tree, and P denote the set of constituents in a predicted tree.</S>
			<S sid ="587" ssid = "9">Precision (P), recall (R), and F1 are defined as: precision = |G ∩ P| |P| recall = |G ∩ P| |G| F1 = 2 1 1 precision + recall F1 ranges from 0 to 1, and it is 1 iff both precision and recall are 1, indicating the trees are identical.</S>
			<S sid ="588" ssid = "10">We report numbers in precentages rather than fractions.</S>
			<S sid ="589" ssid = "11">When measuring the performance of models in which the token-segmentation is predicted and can contradict the gold-standard, a generalization of these measures isused.</S>
			<S sid ="590" ssid = "12">Instead of representing a constituent by a triplet (i, j, L), each constituent is repre sented by a pair containing the concatenation of the words at its yield, and its label L. This measure was suggested by Tsarfaty (2006a) and used in subsequent work (Tsarfaty 2006b; Goldberg and Tsarfaty 2008; Goldberg et al. 2009; Goldberg and Elhadad 2011).</S>
			<S sid ="591" ssid = "13">This is equivalent to reassigning the i and j indices to represent character positions instead of word numbers.</S>
			<S sid ="592" ssid = "14">When the yields of the gold standard and the predicted trees are the same, this is equivalent to the standard evaluation measure using the (i, j, L) triplets of word indices and a label, and it will produce the same precision, recall, and F1 as above.</S>
			<S sid ="593" ssid = "15">Effect of external lexicon.</S>
			<S sid ="594" ssid = "16">We start by evaluating the effect of extending the parser ’s lexical model with an external lexicon, as described in Section 6.1.</S>
			<S sid ="595" ssid = "17">The rare-word threshold is set to 100.</S>
			<S sid ="596" ssid = "18">We use the morphological analyzer described in Section 2.3.3.</S>
			<S sid ="597" ssid = "19">We test two conditions: UNIFORM, in which the P(Text |w) distribution is uniform over all the 24 This assumes unary-chains do not contain cycles..</S>
			<S sid ="598" ssid = "20">147 Table 4 Dev-set results when incorporating an external lexicon.</S>
			<S sid ="599" ssid = "21">Set ting Ext Le xic on /Pr ob s F1 (4 cy cle s) F1 (5 cyc les ) Seg Ora cle N O N E 8 3 . 1 3 8 3 . 3 9 Pip elin e N O N E 7 5 . 9 8 7 6 . 6 5 Seg Ora cle U N I F O R M 8 4 . 9 2 8 4 . 5 6 Pip elin e U N I F O R M 7 7 . 5 3 7 7 . 3 5 Seg Ora cle H M M B A S E D 8 6 . 1 7 8 5 . 7 9 Pip elin e H M M B A S E D 7 8 . 7 5 7 8 . 7 8 analyses suggested by the morphological analyzer for the word, and HMM-BASED in which the P(Text |w) distribution is based on pseudo-counts from the final round of EM– HMM training of the semi-supervised POS tagger described in Section 2.3.4.</S>
			<S sid ="600" ssid = "22">Results are presented in Table 4.</S>
			<S sid ="601" ssid = "23">Incorporating the external lexicon helps both in the case where the correct segmentation is assumed to be known, as well as in the pipeline case where the segmentation is automatically induced by a sequential tagger.</S>
			<S sid ="602" ssid = "24">Incorporating the semi-supervised lexical probabilities learned over large unannotated corpora (HMM-BASED) further improves the results, up to 86.1 F1 for the gold-segmentation case and 78.7 F1 for the pipeline case.</S>
			<S sid ="603" ssid = "25">The pipeline model still lags behind the gold-segmentation case, indicating that the correct segmentation is very informative for the parser.</S>
			<S sid ="604" ssid = "26">Joint segmentation and parsing.</S>
			<S sid ="605" ssid = "27">Having established that the external lexicon can be effectively incorporated into the parser, we turn to evaluate the method for joint segmentation and parsing.</S>
			<S sid ="606" ssid = "28">We follow the same conditions as before (UNIFORM and HMM-BASED lexical probabilities), but in this set of experiments the parser is allowed to choose its preferred segmentation using the lattice-parsing methodology presented in Section 7.2.</S>
			<S sid ="607" ssid = "29">The lattice is constructed according to the analyses licensed by the morphological analyzer.</S>
			<S sid ="608" ssid = "30">Table 5 lists the results.</S>
			<S sid ="609" ssid = "31">Lattice parsing is effective, leading to an improvement of about 2–3 F1 points over the pipeline model.</S>
			<S sid ="610" ssid = "32">Agreement filter.</S>
			<S sid ="611" ssid = "33">We now turn to add the agreement filtering on top of the lexicon- enhanced models.</S>
			<S sid ="612" ssid = "34">In this setting, the model outputs its 100-best trees for each sentence, agreement features are propagated, and agreement violations are checked as described Table 5 Dev-set results when using lattice parsing on top of an external lexicon/analyzer.</S>
			<S sid ="613" ssid = "35">Set ting Ext Le xic on/ Pr ob s F1 (4 cyc les ) F1 (5 cyc les ) Pip elin e U N I F O R M 7 7 . 5 3 7 7 . 3 5 Latt ice (Joi nt) U N I F O R M 8 0 . 3 5 8 0 . 3 1 Pip elin e H M M B A S E D 7 8 . 7 5 7 8 . 7 8 Latt ice (Joi nt) H M M B A S E D 8 0 . 9 1 8 0 . 4 6 148 Table 6 Dev-set results of using the agreement-filter on top of the lexicon-enhanced parser (starting from gold segmentation).</S>
			<S sid ="614" ssid = "36">Set ting Ext Le xic on/ Pr ob s F1 (4 cyc les ) F1 (5 cyc les ) No Agr eem ent U N I F O R M 8 4 . 9 2 8 4 . 5 6 Agr ee me nt as Filt er U N I F O R M 8 5 . 3 0 8 4 . 5 2 No Agr eem ent H M M B A S E D 8 6 . 1 7 8 5 . 7 9 Agr ee me nt as Filt er H M M B A S E D 8 6 . 5 5 8 6 . 2 5 in Section 12, and the first tree that does not contain any agreement violation is returned as the final parse for the sentence (or the first-best tree in case that all of the output trees contain an agreement violation).</S>
			<S sid ="615" ssid = "37">Table 6 lists the results when agreement filtering is performed on top of parses based on gold segmentation, and Table 7 lists the results when agreement filtering is performed on top of a lattice-based parsing model that does not assume gold segmentation is available.</S>
			<S sid ="616" ssid = "38">Discussion of agreement filter results.</S>
			<S sid ="617" ssid = "39">Although the agreement filter does not hurt the parser performance, the benefits from it are very small.</S>
			<S sid ="618" ssid = "40">To understand why that is the case, we analyzed the 1-best parses produced by the 5-cycles-trained grammar on the gold-segmented development set (these conditions corresponds to the last column of the third row in Table 6).</S>
			<S sid ="619" ssid = "41">The analysis revealed the following reasons for the low impact of the agreement filter: (1) The grammar is strong enough to produce fairly accurate structures, which have very few agreement mistakes to begin with, and (2) fixing an agreement mistake does not necessarily mean fixing the entire parse—in some cases it is very easy for the parser to fix the agreement mistake and still produce an incorrect parse for other parts of the structure.</S>
			<S sid ="620" ssid = "42">The 1-best trees of the 480 sentences of the development set contain 22,500 parse- tree nodes.</S>
			<S sid ="621" ssid = "43">Of these 22,500 nodes, 2,368 nodes triggered a gender-agreement check: about 10% of the parsing decisions could benefit from gender agreement.</S>
			<S sid ="622" ssid = "44">Of the 2,368 relevant nodes, however, 130 nodes involved conjunctions or possessives, and were outside of the scope of our agreement verification rules.</S>
			<S sid ="623" ssid = "45">Of the remaining 2,238 parse- tree nodes, 2,204 passed the agreement check, and only 34 nodes (1.5% of the relevant nodes, and 0.15% of the total number of nodes) were flagged as gender-agreement violations.</S>
			<S sid ="624" ssid = "46">Similarly for number agreement, 2,244 nodes triggered an agreement check, of which 2,131 nodes could be handled by our system.</S>
			<S sid ="625" ssid = "47">Of these relevant nodes, 2,109 nodes passed the gender-agreement check, and only 23 nodes (1.07% of relevant nodes, Table 7 Dev-set results of using the agreement-filter on top of the lexicon-enhanced lattice parser (parser does both segmentation and parsing).</S>
			<S sid ="626" ssid = "48">Set ting Ext Le xic on/ Pr ob s F1 (4 cyc les ) F1 (5 cyc les ) No Agr eem ent U N I F O R M 8 0 . 3 5 8 0 . 3 1 Agr ee me nt as Filt er U N I F O R M 8 0 . 5 5 8 0 . 7 4 No Agr eem ent H M M B A S E D 8 0 . 9 1 8 0 . 4 6 Agr ee me nt as Filt er H M M B A S E D 8 1 . 0 4 8 0 . 7 2 149 Table 8 Numbers of parse-tree nodes in the 1-best parses of the development set that triggered gender or number agreement checks, and the results of these checks.</S>
			<S sid ="627" ssid = "49">Gender Agreement Number Agreement Trig gere d agr eem ent che ck 2 , 3 6 8 2 , 2 4 4 Co uld be han dle d by the syst em 2 , 2 3 8 2 , 1 3 1 No agr eem ent viol atio n 2 , 2 0 4 2 , 1 0 9 Agr ee me nt viol atio n 3 4 2 3 and 0.1% of the total nodes) were flagged as agreement violations.</S>
			<S sid ="628" ssid = "50">The numbers are summarized in Table 8.</S>
			<S sid ="629" ssid = "51">It is clear that the vast majority of the parser decisions are compatible with the agreement constraints.</S>
			<S sid ="630" ssid = "52">Turning to inspect the cases in which the agreement filter caught an agreement violation, we note that the agreement filter marked 51 of the 480 development sentences as having an agreement violation in the 1-best parse—about 10% of the sentences could potentially benefit from the agreement filter.</S>
			<S sid ="631" ssid = "53">For 38 of the 51 agreement violations, the agreement violation was fixed in the tree suggested in the 100-best list.</S>
			<S sid ="632" ssid = "54">We manually inspected these 51 parse trees, and highlight some the trends we observed.</S>
			<S sid ="633" ssid = "55">In the 13 cases in which the 100-best list did not contain a fix to the agreement violation, the cause was usually that the 1-best parse had many mistakes that were not related to the agreement violation, and diversity in the 100-best list reflected fixes to these mistakes without affecting the agreement violation.</S>
			<S sid ="634" ssid = "56">Another cause of error was an erroneous agreement mistake due to an omission in the lexicon.</S>
			<S sid ="635" ssid = "57">Of the 38 fixable agreement violations, 25 were local to a noun-phrase, 10 were cases of subject–verb agreement, and the remaining three were either corner-cases or harder to categorize.</S>
			<S sid ="636" ssid = "58">The subject– verb agreement violations were handled almost exclusively by keeping the structure mostly intact and changing the NPSUBJ label to some other closely related label that does not require verb agreement, usually NP.</S>
			<S sid ="637" ssid = "59">This is a good strategy for fixing subject-less sentences (about half of the cases), but it is only a partial fix in case the subject should be assigned to a different NP (which does not happen in practice) or in case a more drastic structural change to the parse-structure is needed.</S>
			<S sid ="638" ssid = "60">In one of the 10 cases, the subject–verb agreement mistake indeed resulted in a structural change that improved the overall parse quality.</S>
			<S sid ="639" ssid = "61">The NP internal agreement violations include many cases of noun-compound attachments, and some cases involving coordination.</S>
			<S sid ="640" ssid = "62">The corrections to the agreement violation were mostly local, and usually resulted in correct structure, but sometimes introduced new errors.</S>
			<S sid ="641" ssid = "63">Figure 5 presents some examples of the different cases.</S>
			<S sid ="642" ssid = "64">Our overall impression is that for NP internal mistakes the agreement-filtering method was mostly doing the right thing.</S>
			<S sid ="643" ssid = "65">To conclude, the agreement filter is useful in overcoming some errors and providing better parses, especially with respect to noun-compound construct-state constructions.</S>
			<S sid ="644" ssid = "66">Due to the limited number of parsing mistakes involving agreement violations, however, and because of the local nature of the agreement-violation mistakes, the total effect of the agreement filter on the final parsing score is small.</S>
	</SECTION>
	<SECTION title="The Final Model. " number = "10">
			<S sid ="645" ssid = "1">Finally, we evaluate the best performing model on the test set.</S>
			<S sid ="646" ssid = "2">Table 9 presents the results of parsing the test set while incorporating the external lexicon and using the 150 Figure 5 NP agreement violations that were caught by the agreement filter system.</S>
			<S sid ="647" ssid = "3">(a) Noun-compound case that was correctly handled.</S>
			<S sid ="648" ssid = "4">(b) Case involving conjunction that was correctly handled.</S>
			<S sid ="649" ssid = "5">(c) A case where fixing the agreement violation introduces a PP-attachment mistake.</S>
			<S sid ="650" ssid = "6">Table 9 Test-set results of the best-performing models.</S>
			<S sid ="651" ssid = "7">Set ting M o d e l F1 (4 cyc les ) Gol d Seg me ntat ion H MM Ba se d Ext ern al Le xic on 8 5 . 6 7 + A g r e e m e n t 8 5 . 7 0 Lattice par sing H MM Ba se d Ext ern al Le xic on 7 6 . 8 7 + A g r e e m e n t 7 6 . 9 5 151 HMM-based probabilities, for a grammar trained for four split-merge iterations.</S>
			<S sid ="652" ssid = "8">This grammar is applied both to the gold-segmentation case and to the realistic case where segmentation and parsing are performed jointly using lattice-parsing.</S>
			<S sid ="653" ssid = "9">We also test the effectiveness of the agreement-filter in both situations.</S>
			<S sid ="654" ssid = "10">Agreement information does not hurt performance, but contributes very little to the final accuracy—additionally on the test sentences, the parser makes very few agreement mistakes to begin with.</S>
			<S sid ="655" ssid = "11">Consistent with previous reports (Tsarfaty 2010), the test set is somewhat harder than the development set.</S>
			<S sid ="656" ssid = "12">With gold-segmentation, the models achieve accuracies of 85.70% F1 . In the realistic scenario in which the segmentation is induced by the parser, the accuracies are around 76.9% F1 . We verified that the HMM-based lexical probabilities also outperform the Uniform probabilities on the test set (the F1 scores when using uniform lexical probabilities are 84.06 and 76.30 for the gold and induced segmentations, respectively).</S>
			<S sid ="657" ssid = "13">These are the best reported results for parsing the test-set of the Hebrew Treebank.</S>
	</SECTION>
	<SECTION title="Related Work in Parsing of Morphologically Rich Languages. " number = "11">
			<S sid ="658" ssid = "1">Coping with unknown words.</S>
			<S sid ="659" ssid = "2">Several papers show that the handling of unknown words is a major component to be considered when adapting a parser to a new language.</S>
			<S sid ="660" ssid = "3">For example, the work in Attia et al.</S>
			<S sid ="661" ssid = "4">(2010) uses language-specific unknown-word signatures for several languages based on various indicative prefixes and suffixes, and Huang and Harper (2009) suggest a Chinese-specific model based on the geometric average of the emission probabilities of the individual characters in the rare or unknown word.</S>
			<S sid ="662" ssid = "5">Another method of coping with lexical sparsity is word clustering.</S>
			<S sid ="663" ssid = "6">In Candito and Crabbe´ (2009), the authors demonstrate that replacing words by a combination of a morphological signature and a word-cluster (based on the linear context of a word in a large unannotated corpus) improves parsing performance for French.</S>
			<S sid ="664" ssid = "7">The technique provides more reliable estimates for in-vocabulary words (a given cluster appears more frequently than the actual word form), and it also increases the known vocabulary: Unknown words may share a cluster with known words.</S>
			<S sid ="665" ssid = "8">Arabic.</S>
			<S sid ="666" ssid = "9">Arabic is similar to Hebrew in the challenges it presents for automatic parsing.</S>
			<S sid ="667" ssid = "10">Most early work on constituency parsing of Arabic focused on straightforward adaptations of Bikel’s parser to Arabic, with little empirical success.</S>
			<S sid ="668" ssid = "11">Attia et al.</S>
			<S sid ="669" ssid = "12">(2010) show that parsing accuracies of around 81% F1 can be achieved for Arabic (assuming gold word segmentation) by using a PCFG-LA parser with Arabic-specific unknown- word signatures.</S>
			<S sid ="670" ssid = "13">Recently, Green and Manning (2010) report on an extensive set of experiments with several kinds of tree annotations and refinements, and report parsing accuracies of 79% F1 using the Stanford-parser and 82% F1 using the PCFG-LA BerkeleyParser, both when assuming gold word segmentation.</S>
			<S sid ="671" ssid = "14">The work of Green and Manning also explored the use of lattice-parsing as suggested in Section 7 of this article, as well as earlier in Goldberg and Tsarfaty (2008) and Cohen and Smith (2007), and report promising results for joint segmentation and parsing of Arabic (an F1 score of 76% for sentences of up to 70 words).</S>
			<S sid ="672" ssid = "15">The best reported results for parsing Arabic when the gold word segmentation is not known, however, are obtained using a pipeline model in which a tagger and word-segmenter is applied prior to a manually state-split constituency parser, resulting in an F-score of 79% F1 (for sentences of up to 70 words) (Green and Manning 2010).</S>
			<S sid ="673" ssid = "16">152 Hebrew and relational-realizational parsing.</S>
			<S sid ="674" ssid = "17">Some related work deals directly with constituency parsing of Modern Hebrew.</S>
			<S sid ="675" ssid = "18">The work of Tsarfaty and Sima’an (2007) experiments with grammar refinement for Hebrew, and shows that annotating definiteness and accusativity of constituents, together with parent annotation, improves parsing accuracy when gold word segmentation is available.</S>
			<S sid ="676" ssid = "19">The Relational Realizational (RR) line of work presented in Tsarfaty et al.</S>
			<S sid ="677" ssid = "20">(Tsarfaty and Sima’an 2008; Tsarfaty, Sima’an, and Scha 2009; Tsarfaty and Sima’an 2010; Tsarfaty 2010) handles the constituent-order variation in Hebrew by presenting a separation between the form and function aspects of the grammar.</S>
			<S sid ="678" ssid = "21">Briefly, whereas plain treebank derived grammars have rules such as S → NP VP PP NP PP that are applied in a single step, the RR approach suggests a generative model in which the generation of flat clausal structures is decomposed into three distinct steps.</S>
			<S sid ="679" ssid = "22">First, in the projection step, a non-terminal generates the kinds of its children without specifying their form or the order between them, using rules of the form S → {OBJ,SBJ,PRED,COM,Adjunct}@S. Second, in the configuration step, an order is chosen based on a separate ordering distribution, using rules of the form {OBJ,SBJ,PRED,COM,Adjunct}@S → SBJ@S PRED@S Adj@S OBJ@S COM@S. Third, in the realization step, each functional element receives a specific form, using rules of the form SBJ@S → NP or Adj@S → PP.</S>
			<S sid ="680" ssid = "23">The realization rules can encode syntactic properties that are required by the grammar for the given function—for example, a rule such as OBJ@S → NPdef,acc captures the requirement that definite objects in Hebrew must be marked for accusativity using the תא marker, and the rest if the generative process will generate the object NP according to this specified constraint.</S>
			<S sid ="681" ssid = "24">This kind of linguistically motivated separation of form and function is shown to produce models with fewer parameters and result in better parsing accuracies than plain (or head- driven) PCFGs derived from the same trees.</S>
			<S sid ="682" ssid = "25">The relational-realizational model can accommodate agreement information.</S>
			<S sid ="683" ssid = "26">It is shown in Tsarfaty and Sima’an (2010) that, given gold-standard POS tags that include the gender and number information for individual words, RR models enriched with gender and number agreement information can provide Modern Hebrew parsing ac- curacies of 84% F1 for sentences of up to 40 words, the highest reported number for Modern Hebrew parsing based on gold POS tags and word-segmentation by the time of its publication.</S>
			<S sid ="684" ssid = "27">Although the RR framework is well motivated linguistically and appealing aesthetically, in the current work we chose to rely on the extreme markovization employed by the PCFG-LA BerkeleyParser in order to cope with the constituent order variation, and to model agreement as an external filter that is orthogonal to the grammar.</S>
			<S sid ="685" ssid = "28">The approach taken in this article provides state-of-the-art results for Hebrew constituency parsing.</S>
			<S sid ="686" ssid = "29">We leave the question of integrating the RR approach with the approach presented here to future work.</S>
	</SECTION>
	<SECTION title="Conclusions. " number = "12">
			<S sid ="687" ssid = "1">We presented experiments on Hebrew Constituency Parsing based on the PCFG-LA methodology of Petrov et al.</S>
			<S sid ="688" ssid = "2">(2006).</S>
			<S sid ="689" ssid = "3">The PCFG-LA model performs well out-of-the-box, especially when the gold POS tags are available to the parser.</S>
			<S sid ="690" ssid = "4">It is possible to improve the learned grammar, however, by specifying some manual state-splits, specifically 153 distinguishing between modal, finite, and infinitive verbs, and explicit marking of subject-NPs.</S>
			<S sid ="691" ssid = "5">Parsing accuracies drop considerably when the gold POS tags are not available, and drop even further when using non-gold segmentation.</S>
			<S sid ="692" ssid = "6">A large part of the drop when the gold POS tags are not available is due to the large percentage of lexical events that are unseen or seen only a few times in the training set.</S>
			<S sid ="693" ssid = "7">This drop can be mitigated by extending the lexical coverage of the parser using an external lexical resource such as a wide-coverage morphological analyzer for mapping lexical items to their possible POS tags.</S>
			<S sid ="694" ssid = "8">The POS-tagging schemes assumed by the treebank and the morphological analyzer need not be compatible with each other: We present a method for bridging the POS tags differences between the two resources.</S>
			<S sid ="695" ssid = "9">The morphological analyzer does not provide lexical probabilities.</S>
			<S sid ="696" ssid = "10">Parsing accuracies can be further improved by using lexical probabilities which are derived in a semi-supervised fashion based on the morphological analyzer and a large corpus of unannotated text.</S>
			<S sid ="697" ssid = "11">The correct token-segmentation is very important for achieving high-quality parses, and when the gold segmentation is not available, parsing results drop considerably.</S>
			<S sid ="698" ssid = "12">It is better to let the parser induce its preferred segmentation in interaction with the parsing process rather than to use a segmentation based on an external sequence model in a pipeline fashion.</S>
			<S sid ="699" ssid = "13">The joint induction of both the syntactic structure and the token- segmentation can be performed by representing the possible segmentations in lattice structure, and using lattice parsing.</S>
			<S sid ="700" ssid = "14">Joint parsing and segmentation is shown to outperform the pipeline approach.</S>
			<S sid ="701" ssid = "15">The parsing accuracies with non-gold segmentation are still far below the accuracies when the gold-segmentation is assumed to be known, however, and accurate parsing with non-gold segmentation remains a challenging open research problem.</S>
			<S sid ="702" ssid = "16">The learned PCFG-LA grammar is not capable of modeling agreement information.</S>
			<S sid ="703" ssid = "17">We considered methods of using morphological agreement information to improve parsing accuracy.</S>
			<S sid ="704" ssid = "18">We propose modeling agreement information as a filtering process that is orthogonal to the grammar used for parsing.</S>
			<S sid ="705" ssid = "19">The approach works in the sense that, in contrast to other methods of using agreement information, it does not degrade parsing accuracy and even improves it slightly.</S>
			<S sid ="706" ssid = "20">The benefit from the agreement filtering is small, however: With the strong grammar induced by the PCFG-LA training procedure, the parser makes very few agreement mistakes to begin with.</S>
			<S sid ="707" ssid = "21">Modeling morphological agreement is probably more useful in syntactic generation than in syntactic parsing.</S>
			<S sid ="708" ssid = "22">We expect the filtering approach we propose to be proven useful for tasks involving syntactic generation, such as target-side-syntax machine translation into a morphologically rich language.</S>
			<S sid ="709" ssid = "23">Overall, we presented four enhancements to the PCFG-LA mechanism in order to adapt it to parsing Hebrew: the introduction of manual, linguistically motivated state-splits; extending the lexical coverage of the parser using an external morphological analyzer; performing segmentation and parsing jointly using a lattice parser; and incorporating agreement information in a filtering framework.</S>
			<S sid ="710" ssid = "24">Together, these enhancements result in the best published results for Hebrew Constituency Parsing to date.</S>
			<S sid ="711" ssid = "25">Appendix A: The Hebrew Agreement Filter Hebrew syntax requires agreement in gender, number, and person.</S>
			<S sid ="712" ssid = "26">The implementation considers only the gender and number features, which are the most common.</S>
			<S sid ="713" ssid = "27">Each of 154 the features can take one of five values Masculine, Feminine, Both, Unknown, and NA for Gender, and Singular, Plural, Both, Unknown and NA for Number.</S>
			<S sid ="714" ssid = "28">Masculine, Feminine, Singular, and Plural are self-explanatory, and are assigned when the feature value is obvious.</S>
			<S sid ="715" ssid = "29">NA means that the feature is irrelevant for the given constituent (adverbs and PPs do not carry gender or number features).</S>
			<S sid ="716" ssid = "30">Both and Unknown are assigned when we are uncertain about the corresponding feature value.</S>
			<S sid ="717" ssid = "31">Both and Unknown are identical in the sense that they leave the feature value unspecified, and have the same effect on the filtering process.</S>
			<S sid ="718" ssid = "32">From a practical perspective they could be collapsed into the same category.</S>
			<S sid ="719" ssid = "33">We chose to maintain the distinction between the two cases because they have slightly different semantics.</S>
			<S sid ="720" ssid = "34">Both indicates that both options are possible (for example, the form תודלי is ambiguous between the plural girls and the singular childhood, and the titular רד, Dr. can refer both to males and females), whereas Unknown means that the feature value could not be computed due to a limitation of the model (for example, there is no clear rule as to the gender of a conjunction which coordinates masculine and feminine NPs, and we are currently unable to accurately infer the gender and number associated with certain complex quantifiers such as בור (most).</S>
			<S sid ="721" ssid = "35">Compare: ראשנ התיכה בור, הלכאנ הגועה בור, הגועה בור לכאנ (‘most of the classfem stayedmasc , most of the cakefem was eatenfem , most of the cakefem was eatenmasc ’).</S>
			<S sid ="722" ssid = "36">Feature values are said to agree if they are compatible with each other.</S>
			<S sid ="723" ssid = "37">Feminine is compatible with NA, Both, and Unknown but not with Masculine.</S>
			<S sid ="724" ssid = "38">Similarly, Singular is compatible with NA, Both, and Unknown, but not with Plural.</S>
			<S sid ="725" ssid = "39">Agreement cases.</S>
			<S sid ="726" ssid = "40">The system is designed to handle the following cases of morphological agreement: NP level agreement between nouns and adjectives.</S>
			<S sid ="727" ssid = "41">םילודג םיקורי םיחופת זגרא (‘box-ofSg applesPl greenPl bigPl ’) , לודג םיקורי םיחופת זגרא (‘box-ofSg applesPl greenPl bigSg ’) S level agreement between subject and verbs.</S>
			<S sid ="728" ssid = "42">ךלה םידליה דחא (‘[one-of the-kids]Sg walkedSg ’) Predicative agreement between the subject, ADJP, and copular element.</S>
			<S sid ="729" ssid = "43">םכח אוה (‘he [is] smartmasc ’), המיהדמ התיה איה (‘she was amazing/fem ’), but not with nouns למס התיה איה (‘she was a-symbolmasc ’).</S>
			<S sid ="730" ssid = "44">Agreement between the Verb in a relativized SBAR and the realization of the Null- subject in the external NP.</S>
			<S sid ="731" ssid = "45">אשונב הנד ש הדעווה (‘the-committeefem which [*] discussedfem the-matter ’) Morphological feature propagation.</S>
			<S sid ="732" ssid = "46">The first step of determining agreement is propagating the relevant features from the leaves up to the constituent level.</S>
			<S sid ="733" ssid = "47">The procedure begins by assigning each leaf gender and number features.</S>
			<S sid ="734" ssid = "48">These are assigned based either on the TB tag assigned for the word if training on gold POS tags, or on the morphological analyzer entries for the given word (in most cases the number and gender features are easy to predict, even in cases where the core POS is not clear.</S>
			<S sid ="735" ssid = "49">In the relatively rare cases where the analyzer contains both a feminine and masculine (alt.</S>
			<S sid ="736" ssid = "50">singular and plural) analyses, feature value is marked as Both).</S>
			<S sid ="737" ssid = "51">155 Table A.1 Gender and number percolation rules.</S>
			<S sid ="738" ssid = "52">FC = first child with non-NA gender/number.</S>
			<S sid ="739" ssid = "53">Rules for each constituent type are applied in order, until a condition holds.</S>
			<S sid ="740" ssid = "54">Rules for gender and number are applied independently of each other.</S>
			<S sid ="741" ssid = "55">Constituent Condition Feature Values S B A R S B A R h a s R E L a n d S c h i l d r e n o t h e r w i s e S.f ea tu re s N A P R E D P P R E D P P R E D P h a s A D J P c h i l d h a s A G R c h i l d a n d n o N P c h i l d o t h e r w i s e A D J P . f e a t u r e s A G R . f e a t u r e s N A S S S h a s V P c h i l d a n d n o N P S u b j c h i l d h a s V B c h i l d a n d n o N P S u b j c h i l d o t h e r w i s e V P . f e a t u r e s V B . f e a t u r e s N A NN PG a l w a y s U N P N P N P N P N P N P N P N P h a s N N T c h i l d h a s C D T a n d N P c h i l d r e n i s a c o n j u n c t i o n h a s a “ c h i l d f i r s t c h i l d i s N P , s e c o n d i s P O S h a s I N c h i l d h a s c h i l d w i t h n o n N A g e n / n u m o t h e r w i s e N N T . f e a t u r e s C D T . n u m b e r N P . g e n d e r g e n d e r = U n u m b e r = P l u r a l U NP .fe atu res F C . g e n d e r n u m b e r = U F C . g e n d e r F C . n u m b e r N A A D J P A D J P A D J P h a s J J T c h i l d h a s c h i l d w i t h n o n N A g e n / n u m o t h e r w i s e JJT.</S>
			<S sid ="742" ssid = "56">fea tur es FC .ge nd er FC .nu mb er N A V P V P V P V P h a s V B c h i l d h a s V B M o d a l c h i l d h a s V P c h i l d o t h e r w i s e VB .fe atu res VB M od al.f eat ur es VP .fe atu res N A oth er a l w a y s N A After each leaf is assigned feature values, the features are propagated up the tree according to a set of rules such as the following (the complete set of rules is given in Table A.1): • If the constituent is an NP and has a Construct-noun child, it is assigned the gender of the Construct-noun.</S>
			<S sid ="743" ssid = "57">• If the constituent is a coordinated NP (has a CC child), set its number feature to plural.</S>
			<S sid ="744" ssid = "58">• If the constituent is an S and it has VP child but no NP-Subject child, take the gender from the VP.</S>
			<S sid ="745" ssid = "59">Agreement rules.</S>
			<S sid ="746" ssid = "60">Once the features are propagated from the leaves to a constituent, agreement is verified at the constituent level according to the following rules: NP agreement rules: • Agreement for coordinated NPs and Possessive NPs is not checked.</S>
			<S sid ="747" ssid = "61">156 • If NP has an SBAR child, all the children up to the SBAR whose type is nominal or adjectival must agree in gender and number.</S>
			<S sid ="748" ssid = "62">• If NP has an ADJP child, all the children up to the ADJP whose type is nominal or adjectival must agree in gender and number.</S>
			<S sid ="749" ssid = "63">NP NP NP JJFem,Sg NP JJFem,Sg NNTFem,Sg NP הלודג NNTFem,Sg NP הלודג (a) תספוק NNMs,Pl םיחופת JJMs,Pl םיקורי (b) תספוק NNMs,Pl םיחופת JJMs,Pl םיקורי NPFem,Sg NPFem,Sg NPFem,Sg JJFem,Sg NPFem,Sg JJFem,Sg NNTFem,Sg NPMs,Pl הלודג NNTFem,Sg NPMs,Pl הלודג תספוק (c) Figure A.1 NNMs,Pl םיחופת JJMs,Pl םיקורי (d) תספוק NNMs,Pl םיחופת JJMs,Pl םיקורי Agreement annotation and validation example: correct tree.</S>
			<S sid ="750" ssid = "64">The sentence words translate to box-of apples green big, literally, a big box of green apples.</S>
			<S sid ="751" ssid = "65">NP NP NNTFem,Sg NP NNTFem,Sg NP תספוק NP JJFem,Sg תספוק NP JJFem,S g (a) NNMs,Pl םיחופת JJMs,Pl םיקורי הלודג (b) NNMs,Pl םיחופת JJMs,Pl םיקורי הלודג NPFem,Sg NPFem,Sg NNTFem,Sg NPMs,Pl NNTFem,Sg NPMs,Pl תספוק NPMs,Pl JJFem,Sg תספוק NPMs,Pl JJFem,Sg (c) Figure A.2 NNMs,Pl םיחופת JJMs,Pl םיקורי הלודג (d) NNMs,Pl םיחופת JJMs,Pl םיקורי הלודג Agreement annotation and validation example: incorrect tree, agreement violation.</S>
			<S sid ="752" ssid = "66">box-of apples green big, literally, a big box of green apples, though the parse tree suggests the interpretation a box of big green apples.</S>
			<S sid ="753" ssid = "67">157 S agreement rule: • All children of S with type in {NP-Subject, VP, VB, AUX, PREDP} must agree in their gender and number features.</S>
			<S sid ="754" ssid = "68">ADJP agreement rule: • All children of ADJP with type in {NP, NP-Subject, NN, JJ, ADJP} must agree in their gender and number features.</S>
			<S sid ="755" ssid = "69">An example.</S>
			<S sid ="756" ssid = "70">Consider the tree in Figure A.1a.</S>
			<S sid ="757" ssid = "71">In the first stage (Figure A.1b), agreement features are propagated according to the rules in Table A.1, resulting in the annotated tree in Figure A.1c.</S>
			<S sid ="758" ssid = "72">Agreement is then validated in Figure A.1d (nodes in which an agreement rule applied and passed are marked in green).</S>
			<S sid ="759" ssid = "73">In contrast, the tree in Figure A.2a has an agreement mistake.</S>
			<S sid ="760" ssid = "74">As before, the agreement features are propagated according to the rules (Figure A.2b) resulting in Figure A.2c.</S>
			<S sid ="761" ssid = "75">Agreement validation fails at Figure A.2d (the node in which agreement validation was applied and failed is marked in red).</S>
	</SECTION>
</PAPER>
