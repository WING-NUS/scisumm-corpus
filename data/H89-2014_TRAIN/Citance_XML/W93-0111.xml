<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Bootstrap methods (unsupervised classification) that generate word classes without requiring pretagging have had notable success in the last few years.</S>
		<S sid ="2" ssid = "2">The methods described here strengthen these approaches and produce excellentword classes from a 200,000 word corpus.</S>
		<S sid ="3" ssid = "3">The method uses mutual information measures plus positional information from the words in the immediate context of a target word to compute similarities.</S>
		<S sid ="4" ssid = "4">Using the similarities, classes are built using hierarchical agglomerative clustering.</S>
		<S sid ="5" ssid = "5">At the leaves of the classification tree, words are grouped by syntactic and semantic similarity.</S>
		<S sid ="6" ssid = "6">Further up the tree,the classes are primarily syntactic.</S>
		<S sid ="7" ssid = "7">Once the initialclasses are found, they can be used to classifyambiguous words, i.e., part-of-speech tagging.</S>
		<S sid ="8" ssid = "8">This is done by expanding each context word of a target instance into a tightly defined class of similar words, a simsct.</S>
		<S sid ="9" ssid = "9">The use of simsets is shown to increase the tagging accuracy from 83% to 92% for the forms &amp;quot;cloned&amp;quot; and &amp;quot;deduced&amp;quot;.</S>
	</ABSTRACT>
	<SECTION title="INTRODUCTION" number = "1">
			<S sid ="10" ssid = "10">The identification of the syntactic class and the discovery of semantic information for words not contained in any online dictionary or thesaurus is an important and challenging * This material is based upon work supported by the National Science Foundation under Grant No.</S>
			<S sid ="11" ssid = "11">In our approach, we take into account both the relative positions of the nearby context words as well as the m u t u a l information (Church &amp; Hanks, 1990) associated with the occurrence of a particular context word.</S>
			<S sid ="12" ssid = "12">The similarities computed from these measures of the context contain information about both syntactic and s e m a n t i c relations.</S>
			<S sid ="13" ssid = "13">For example, high similarity values are obtained for the two s e m a n t i c a l l y similar nouns, DIR8814522.</S>
			<S sid ="14" ssid = "14">117 &amp;quot;diameter&amp;quot; and &amp;quot;length&amp;quot;,as well as for the two adjectives &amp;quot;nonmotile&amp;quot; and &amp;quot;nonchemotactic&amp;quot;.</S>
			<S sid ="15" ssid = "15">W e demonstrate the technique on three problems, all using a 200,000 word corpus composed of 1700 abstracts from a specialized field of biology: #1: Generating the full classificationtree for the 1,000 most frequent words (covering 80% of all word occurrences).</S>
			<S sid ="16" ssid = "16">#2: The classification of 138 occurrences of the -ed forms, &amp;quot;cloned&amp;quot; and &amp;quot;deduced&amp;quot; into four syntactic categories, including improvements by using expanded context information derived in #1.</S>
			<S sid ="17" ssid = "17">#3: The classificationof 100 words that only occur once in the entire corpus (hapax legomena), again using expanded contexts.</S>
			<S sid ="18" ssid = "18">The results described below were obtained using no pretagging or online dictionary,but the results compare favorably with methods that do.</S>
			<S sid ="19" ssid = "19">The results are discussed in terms of the semantic fields they delineate, the accuracy of the classificationsand the nature of the errors that occur.</S>
			<S sid ="20" ssid = "20">The results make it clear that this new technology is very promising and should be pursued vigorously.</S>
			<S sid ="21" ssid = "21">The power of the approach appears to result from using a focused corpus, using detailed positional information, using mutual information measures and using a clustering method that updates the detailed context information when each new cluster is formed.</S>
			<S sid ="22" ssid = "22">Our approach was inspired by the fascinating results achieved by Finch and Chater at Edinburgh and the methods they used (Finch &amp; Chater, 1992).</S>
			<S sid ="23" ssid = "23">THE CORPUS -- TECHNICAL, FOCUSED AND SMALL In the Biological Knowledge Laboratory we are pursuing a number of projects to analyze, store and retrieve biologicalresearch papers, including working with full text and graphics (Futrelle, Kakadiaris, Alexander, Carriero, Nikolakis &amp; Futrelle, 1992; Gauch &amp; Futrelle, 1993).</S>
			<S sid ="24" ssid = "24">The work is focused on the biological field of bacterial chemotaxis.</S>
			<S sid ="25" ssid = "25">A biologist has selected approximately 1,700 documents representing all the work done in this field since its inception in 1965.</S>
			<S sid ="26" ssid = "26">Our study uses the titlesfor all these documents plus all the abstracts available for them.</S>
			<S sid ="27" ssid = "27">The resulting corpus contains 227,408 words with 13,309 distinct word forms, including 5,833 words of frequency 1.</S>
			<S sid ="28" ssid = "28">There are 1,686 titles plus 8,530 sentences in the corpus.</S>
			<S sid ="29" ssid = "29">The sentence identification algorithm requires two factors -- contiguous punctuation C.&amp;quot;, &amp;quot;!&amp;quot;,or &amp;quot;?&amp;quot;) and capitalization of the following token.</S>
			<S sid ="30" ssid = "30">To eliminate abbreviations, the token prior to the punctuation must not be a single capital letter and the capitalized token after the punctuation m a y not itselfbe followed by a contiguous &amp;quot;.&amp;quot;.</S>
			<S sid ="31" ssid = "31">An example of a sentence from the corpus is, &amp;quot;$pre2$ $prel$ one of the open reading frames was translated into a protein with $pct$ amino acid identity to S. typhimurium Flil and $pct$ identity to the beta subunit of E. coli A T P synthase $posl$ $pos2$&amp;quot; The positional items $pre... and $pos... have been added to furnish explicitcontext for sentence initial and sentence final constituents.</S>
			<S sid ="32" ssid = "32">Numbers have been converted to three forms corresponding to integers,reals and percentages C$pct$ in the example above).</S>
			<S sid ="33" ssid = "33">The machine-readable version of the corpus uses double quoted items to ease processing by Lisp, our language of choice.</S>
			<S sid ="34" ssid = "34">The terminology we will use for describing words is as follows: • Target word : A word to be classified.</S>
			<S sid ="35" ssid = "35">Context words : Appearing within some distance of a target word, &amp;quot;The big ~ cat 9.a the mat...&amp;quot;.</S>
			<S sid ="36" ssid = "36">• Word class: Any defined set of word forms or labeled instances.</S>
			<S sid ="37" ssid = "37">Simset: A word class in which each item, an expansion word, has a similarity greater than some chosen cutoffto a single base word.</S>
			<S sid ="38" ssid = "38">• Labeled instances : Forms such as &amp;quot;cloned48&amp;quot; or &amp;quot;cloned73VBN&amp;quot;, that 118 would replace an occurrence of &amp;quot;cloned&amp;quot;.</S>
			<S sid ="39" ssid = "39">DESCRIBING QUANTIFYING In these experiments, the context of a target word is described by the preceding two context words and the following two context words, Figure 1.</S>
			<S sid ="40" ssid = "40">Each position is represented by a 150 element vector corresponding to the occurrence of the 150 highest frequency words in the corpus, giving a 600-dimensional vector describing the four-word context.</S>
			<S sid ="41" ssid = "41">Initially, the counts from all instances of a target word form w are summed so t h a t the entry in the corresponding context word position in the vector is the sum of the occurrences of that context word in t h a t position for the corresponding target word form; it is the joint frequency of the context word.</S>
			<S sid ="42" ssid = "42">For example, if the word the i m m e d i a t e l y precedes 10 occurrences of the word gene in the corpus then the element corresponding to the in the - 1 C context vector of gene is set to 10.</S>
			<S sid ="43" ssid = "43">Subsequently, a 600-dimensional vector of mutual information values, MI, is computed from the frequencies as follows, AND WORD Ml(cw)= log 2 [ . NZ~ 1 + ].j ] This expresses the mutual information value for the context word c appearing with the target word w. The mutual information is large whenever a context word appears at a m u c h h i g h e r f r e q u e n c y , f c w , in the Word to be classified with context: -2C 119 neighborhood of a target word than would be predicted from the overall frequencies in the corpus, fc and fw.</S>
			<S sid ="44" ssid = "44">The formula adds 1 to the frequency ratio, so that a 0 (zero) occurrence corresponds to 0 m u t u a l information.</S>
			<S sid ="45" ssid = "45">A possibly better strategy (Church, Gale, Hanks &amp; Hindle, 1991) is capable of generatingnegative m u t u a l information for the non occurrence or low-frequency occurrence of a very high-frequency word and has the form, Ml(cw) = log 2 • fN(~ . +I)&apos;~ ~ / [ I#.</S>
			<S sid ="46" ssid = "46">J In any case, some smoothing is necessary to prevent the m u t u a l information from diverging when fcw= O. SIMILARITY, CLUSTERING AND CLASSIFICATION SPACE IN WORD When the mutual information vectors are computed for a number of words, they can be compared to see which words have similar contexts.</S>
			<S sid ="47" ssid = "47">The comparison we chose is the inner product, or cosine measure, which can vary between -1.0 and +1.0 (Myaeng &amp; Li, 1992).</S>
			<S sid ="48" ssid = "48">Once this similarity is computed for all word pairs in a set, various techniques can be used to identify classes of similar words.</S>
			<S sid ="49" ssid = "49">The method we chose is hierarchical agglomerative clustering (Jain &amp; Dubes, 1988).</S>
			<S sid ="50" ssid = "50">The two words with the highest similarity are first joined into a two-word cluster.</S>
			<S sid ="51" ssid = "51">-1C W +1C +2C F i g u r e 1.</S>
			<S sid ="52" ssid = "52">The 600-dimensional context vector around a target word W. Each subvecter describes the frequency and mutual information of the occurrences of the 150 highest frequency words, HFC, in the corpus.</S>
			<S sid ="53" ssid = "53">CONTI~TS A mutual information vector for the cluster is computed and the cluster and remaining words are again compared, choosing the most similar to join, and so on.</S>
			<S sid ="54" ssid = "54">(To compute the new mutual information vector, the context frequencies in the vectors for the two words or clusters joined at each step are summed, element-wise.)</S>
			<S sid ="55" ssid = "55">In this way, a binary tree is constructed with words at the leaves leading to a single root covering all words.</S>
			<S sid ="56" ssid = "56">Each cluster, a node in the binary tree, is described by an integer denoting its position in the sequence of cluster formation, the total number of words, the similarity of the two children that make it up, and its m e m b e r words.</S>
			<S sid ="57" ssid = "57">Here, for example, are the first 15 clusters from the analysis described in Experiment #I in the next section, (0 2 0.73926157 is was) (1 2 0.6988309 were are) (Z 4 0.708@31 ( i s was) (were are)) (3 2 0.65726656 found shown) (4 Z 0.6216794 the a) (5 Z 0.5913143 s mM) (6 Z 0.59088105 coli typhimurium) (7 2 0.586728 galactose ribose) (8 2 0.58630705 method procedure) (9 2 0.58404166 K-12 K12) (10 2 0.5833811 required necessary) (11 3 0.5793458 rain (s raM)) (12 2 0.5750035 isolated constructed) (13 3 0.56909233 (found shown) used) (14 2 0.$6750214 c e l l s strains) (I5 3 0.5652546 mutants ( c e l l s strains)) In this sample it is clear that clusters are sometimes formed by the pairing of two individual words, sometimes by pairing one word and a previous cluster, and sometimes by combining two already formed clusters.</S>
			<S sid ="58" ssid = "58">In normal tagging, a word is viewed as a member of one of a small number of classes.</S>
			<S sid ="59" ssid = "59">In the classificationapproach we are using, there can be thousands of classes, from pairs of words up to the root node which contains all words in a single class.</S>
			<S sid ="60" ssid = "60">Thus, every class generated is viewed extensionally, it is a structured collection of occurrences in the corpus, with their attendant frequencies and contexts.</S>
			<S sid ="61" ssid = "61">The classes so formed will reflect the particularword use in the corpus they are derived from.</S>
			<S sid ="62" ssid = "62">EXPERIMENT #1: CLASSIFICATION OF THE 1,000 HIGHEST FREQUENCY WORDS The first experiment classified the 1,000 highest frequency words in the corpus, producing 999 clusters (0998) during the process.</S>
			<S sid ="63" ssid = "63">$pre... and Spas... words were included in the context set, but not in the target set.</S>
			<S sid ="64" ssid = "64">Near the leaves, words clustered by syntax (part of speech) and by semantics.</S>
			<S sid ="65" ssid = "65">Later, larger clusters tended to contain words of the same syntactic class, but with less semantic homogeneity.</S>
			<S sid ="66" ssid = "66">In each example below, the words listedare the entire contents of the cluster mentioned.</S>
			<S sid ="67" ssid = "67">The most striking property of the clusters produced was the classificationof words into coherent semantic fields.</S>
			<S sid ="68" ssid = "68">Grefenstette has pointed out (Grefenstette, 1992) that the Deese antonyms, such as &amp;quot;large&amp;quot; and &amp;quot;small&amp;quot; or &amp;quot;hot&amp;quot; and &amp;quot;cold&amp;quot; show up commonly in these analyses.</S>
			<S sid ="69" ssid = "69">Our methods discovered entire graded fields, rather than just pairs of opposites.</S>
			<S sid ="70" ssid = "70">The following example shows a cluster of seventeen adjectives describing comparative quantity terms, cluster 756, similarity 0.28, decreased, effective, few, greater, high, higher, increased, large, less, low, lower, more, much, no, normal, reduced, short Note t h a t pairs such as &amp;quot;high&amp;quot; and &amp;quot;higher&amp;quot; and &amp;quot;low&amp;quot; and &amp;quot;lower&amp;quot; appear.</S>
			<S sid ="71" ssid = "71">&amp;quot;No&amp;quot;, meaning &amp;quot;none&amp;quot; in this collection, is located at one extreme.</S>
			<S sid ="72" ssid = "72">The somewhat marginal item, &amp;quot;effective&amp;quot;, entered the cluster late, at cluster 704.</S>
			<S sid ="73" ssid = "73">It appears in collocations, such as &amp;quot;as.</S>
			<S sid ="74" ssid = "74">effective as&amp;quot; and &amp;quot;effective than&amp;quot;, in which the other terms also appear.</S>
			<S sid ="75" ssid = "75">Comparing the cluster to Roger&apos;s (Berrey, 1962) we find that all the items are in the Roget category Comparative Quantity except for &amp;quot;effective&amp;quot; and &amp;quot;no&amp;quot;.</S>
			<S sid ="76" ssid = "76">The cluster item, &amp;quot;large&amp;quot; is not in this Roget category but the category does include &amp;quot;big&amp;quot;, &amp;quot;huge&amp;quot; and &amp;quot;vast&amp;quot;, so the omission is clearly an error in Roget&apos;s.</S>
			<S sid ="77" ssid = "77">With 120 this correction, 88% (15/17) of the items are in the single Roget category.</S>
			<S sid ="78" ssid = "78">The classification of technical terms from genetics and biochemistry is of particular interest, because many of these terms do not appear in available dictionaries or thesauri.</S>
			<S sid ="79" ssid = "79">Cluster 374, similarity 0.37, contains these 18 items, che, cheA, cheB, cheR, cheY, cheZ, double, fla, flaA, t a B , flaE, H2, hag, mot, motB, tar, trg, tsr All of these are abbreviations for specific bacterial mutations, except for &amp;quot;double&amp;quot;.</S>
			<S sid ="80" ssid = "80">Its appearance drives home the point that the classification depends entirely on usage.</S>
			<S sid ="81" ssid = "81">20 of the 30 occurrences of &amp;quot;double&amp;quot; precede the words &amp;quot;mutant&amp;quot; or &amp;quot;mutants&amp;quot;, as do most of the othermutation terms in this cluster.</S>
			<S sid ="82" ssid = "82">Cluster 240, similarity 0.4 contains these termS, microscopy, electrophoresis, chromatography Each of these is a noun describing a common technique used in experiments in this domain.</S>
			<S sid ="83" ssid = "83">The standard Linnean nomenclature of Genus followed by species, such as Escherichia coli, is reflected by cluster 414, which contains 22 species names, and cluster 510, which contains 9 genus n a m e s . In scientific research, the determination of causal factors and the discovery of essential elements is a major goal.</S>
			<S sid ="84" ssid = "84">Here are six concepts in this semantic field comprising cluster 183, similarity 0.43, required, necessary, involved, responsible, essential, important These terms are used almost interchangeably in our corpus, b u t they don&apos;t f a r e as well in R o g e t &apos; s b e c a u s e of anthropocentric attachments to concepts such as fame, duty and legal liability.</S>
			<S sid ="85" ssid = "85">Discussion of Experiment #1 Given the limited context and modest sized corpus, the classification algorithm is bound to make mistakes, though a study of the text concordance will always tell us why the algorithm failed in any specific case.</S>
			<S sid ="86" ssid = "86">For example, as the similarity drops to 0.24 at cluster 824 we see the adverb triple &amp;quot;greatly&amp;quot;, &amp;quot;rapidly&amp;quot; and &amp;quot;almost&amp;quot;.</S>
			<S sid ="87" ssid = "87">This is still acceptable, but by cluster 836 (similarity 0.24) we see the triple, &amp;quot;them&amp;quot;, &amp;quot;ring&amp;quot;, &amp;quot;rings&amp;quot;.</S>
			<S sid ="88" ssid = "88">At the end there is only a single cluster, 998, which must include all words.</S>
			<S sid ="89" ssid = "89">It comes together stubbornly with a negative similarity of-0.51.</S>
			<S sid ="90" ssid = "90">One problem encountered in this work was that the later, larger clusters have less coherence than we would hope for, identifying an i m p o r t a n t research issue.</S>
			<S sid ="91" ssid = "91">Experiment #1 took 20 hours to run on a Symbolics XL1200.</S>
			<S sid ="92" ssid = "92">A fundamental problem is to devise decision procedures that will tell us which classes are semantically or syntactically homogeneous; procedures that tell us where to cut the tree.</S>
			<S sid ="93" ssid = "93">The examples shown earlier broke down soon after, when words or clusters which in our judgment were weakly related began to be added.</S>
			<S sid ="94" ssid = "94">We are exploring the numerous methods to refine clusters once formed as well as methods to validate clusters for homogeneity (Jain &amp; Dubes, 1988).</S>
			<S sid ="95" ssid = "95">There are also resampling methods to validate clusters formed by top-down partitioning methods (Jain &amp; Moreau, 1987).</S>
			<S sid ="96" ssid = "96">All of these methods are computationally demanding b u t they can result in criteria for when to stop clustering.</S>
			<S sid ="97" ssid = "97">On the other hand, we mustn&apos;t assume that word relations are so simple that we can legitimately insist on finding neatly s e p a r a t e d clusters.</S>
			<S sid ="98" ssid = "98">Word relations m a y simply be too complex and graded for this ever to occur.</S>
			<S sid ="99" ssid = "99">The semantic fields we discovered were not confined to synonyms.</S>
			<S sid ="100" ssid = "100">To understand why this is the case, consider the sentences, &amp;quot;The t e m p e r a t u r e is higher today.&amp;quot; and, &amp;quot;The temperature is lower today.&amp;quot; There is no way to tell from the syntax which word to expect.</S>
			<S sid ="101" ssid = "101">The choice is dependent on the situation in the world; it represents data from the world.</S>
			<S sid ="102" ssid = "102">The 121 utterances are informative for just that reason.</S>
			<S sid ="103" ssid = "103">Taking this reasoning a step further, information theory would suggest that for two contrasting words to be maximally informative, they should appear about equally often in discourse.</S>
			<S sid ="104" ssid = "104">This is born out in our corpus (fhigher=58, i~ower=46) and for the Brown corpus (fhigher=147, fiower=110).</S>
			<S sid ="105" ssid = "105">The same relations are found for m a n y other contrasting pairs, with some bias towards &amp;quot;positive&amp;quot; terms.</S>
			<S sid ="106" ssid = "106">The most extreme &amp;quot;positive&amp;quot; bias in our corpus is f p o s s i b l e = 8 8 , fimpossible=0; &amp;quot;never say never&amp;quot; seems to be the catchphrase here - - highly appropriate for the field of biology.</S>
			<S sid ="107" ssid = "107">Some of the chemical term clusters that were generated are interesting because they contain class terms such as &amp;quot;sugar&amp;quot; and &amp;quot;ion&amp;quot; along with specific members of the classes (hyponyms), such as &amp;quot;maltose&amp;quot; and &amp;quot;Na +&apos;&apos;.</S>
			<S sid ="108" ssid = "108">Comparing these in our K W I C concordance suggests that there m a y be methodical techniques for identifying some of these generalization hierarchies using machine learning (supervised classification)(Futrelle &amp; Gauch, 1993).</S>
			<S sid ="109" ssid = "109">For another discussion of attempts to generate generalization hierarchies, see (Myaeng &amp; Li, 1992).</S>
			<S sid ="110" ssid = "110">As a corpus grows and new words appear, one way to classify them is to find their similarity to the N words for which context vectors have already been computed.</S>
			<S sid ="111" ssid = "111">This requires N comparisons.</S>
			<S sid ="112" ssid = "112">A more efficient method which would probably give the same result would be to successively compare the word to clusters in the tree, starting at the root.</S>
			<S sid ="113" ssid = "113">At each node, the child which is most similar to the unclassified word is followed.</S>
			<S sid ="114" ssid = "114">This is a logarithmic search technique for finding the best matching class which takes only O(log2N) steps.</S>
			<S sid ="115" ssid = "115">In such an approach, the hierarchical cluster is being used as a decision tree, which have been much studied in the machine learning literature (Quinlan, 1993).</S>
			<S sid ="116" ssid = "116">This is an alternate view of the classification approach as the unsupervised learning of a decision tree.</S>
			<S sid ="117" ssid = "117">EXPERIMENT #2: DISAMBIGUATION OF -ED FORMS The following experiment is interesting because it shows a specific use for the similarity computations.</S>
			<S sid ="118" ssid = "118">They are used here to i n c r e a s e t h e a c c u r a c y of t e r m disambiguation which means selecting the best tag or class for a potentially ambiguous word.</S>
			<S sid ="119" ssid = "119">Again, this is a bootstrap method; no prior tagging is needed to construct the classes.</S>
			<S sid ="120" ssid = "120">But if we do identify the tags for a few items by hand or by using a hand-tagged reference corpus, the tags for all the other items in a cluster can be assumed equal to the known items.</S>
			<S sid ="121" ssid = "121">The passive voice is used almost exclusively in the corpus, with some use of the editorial &amp;quot;We&amp;quot;.</S>
			<S sid ="122" ssid = "122">This results in a profusion of participles such as &amp;quot;detected&amp;quot;, &amp;quot;sequenced&amp;quot; and &amp;quot;identified&amp;quot;.</S>
			<S sid ="123" ssid = "123">But such -ed forms can also be simple past tense forms or adjectives.</S>
			<S sid ="124" ssid = "124">In addition, we identified their use in a postmodifying participle clause such as, &amp;quot;... the value ~ from this measurement.&amp;quot; Each one of the 88 instances of &amp;quot;cloned&amp;quot; and the 50 instances of &amp;quot;deduced&amp;quot; was hand tagged and given a unique ID. Then clustering was applied to the resulting collection, giving the result shown in Figure 2A.</S>
			<S sid ="125" ssid = "125">Experiments #2 and #3 took about 15 minutes each to run.</S>
			<S sid ="126" ssid = "126">The resultant clusters are somewhat complex.</S>
			<S sid ="127" ssid = "127">There are four tags and we have shown the top four clusters, but two of the clusters contain adjectives exclusively.</S>
			<S sid ="128" ssid = "128">The past participle and postmodifier occur together in the same cluster.</S>
			<S sid ="129" ssid = "129">(We studied the children of cluster 4, hoping to find better separation, but they are no better.</S>
			<S sid ="130" ssid = "130">) The scoring metric we chose was to associate each cluster with the items that were in the majority in the node and score all other items as errors.</S>
			<S sid ="131" ssid = "131">This is a good approximation to a situation in which a &amp;quot;gold standard&amp;quot; is available to classify the clusters by independent means, such as comparing the clusters to items from a pretagged reference corpus.</S>
			<S sid ="132" ssid = "132">122 ,I 2 I i ] ,,VBO] ° JI JJ = Adjective VBD = Verb, past tense VBN : Verb, past participle VBNP = Participle in postmodifying clause 46 VBN.</S>
			<S sid ="133" ssid = "133">13 VBNP.</S>
			<S sid ="134" ssid = "134">1 VBD.</S>
			<S sid ="135" ssid = "135">1 JJ.</S>
			<S sid ="136" ssid = "136">Figure 2A.</S>
			<S sid ="137" ssid = "137">Clustering of 88 occurrence of&amp;quot;cloned&amp;quot; and 50 occurrences of&amp;quot;deduced&amp;quot; into four syntactic categories.</S>
			<S sid ="138" ssid = "138">The abbreviations, such as &amp;quot;JJ&amp;quot;, are based on (Francis &amp; Kucera, 1982).</S>
			<S sid ="139" ssid = "139">There is a strong admixture of adjectives in cluster 2 and all the postmodifiers are confounded with the past participles in cluster 4.</S>
			<S sid ="140" ssid = "140">The total number of errors (minority classes in a cluster) is 23 for a success rate of(13823)/138 = 83%.</S>
			<S sid ="141" ssid = "141">All minority m e m b e r s of a cluster are counted as errors.</S>
			<S sid ="142" ssid = "142">This leads to the 83% error rate quoted in the figure caption.</S>
			<S sid ="143" ssid = "143">o f a base word a n d a s e q u e n c e o f expansion words.</S>
			<S sid ="144" ssid = "144">The results shown in Figure 2 A can be improved as follows.</S>
			<S sid ="145" ssid = "145">Because we are dealing with single occurrences, only one element, or possibly zero, in each of the four context word vectors is filled, with frequency 1.</S>
			<S sid ="146" ssid = "146">The other 149 elements have frequency (and mutual information) 0.0.</S>
			<S sid ="147" ssid = "147">These sparse vectors will therefore have littleor no overlap with vectors from other occurrences.</S>
			<S sid ="148" ssid = "148">In order to try to improve the classification, we expanded the context values in an effort to produce more overlap, using the following strategy: W e proceed as ifthe corpus is far larger so that in addition to the actual context words already seen, there are m a n y occurrences of highly similar words in the same positions.</S>
			<S sid ="149" ssid = "149">For each nonzero context in each set of 150, we expand it to an ordered class of similar words in the 150, picking words above a fixed similarity threshold (0.3 for the experiments reported here).</S>
			<S sid ="150" ssid = "150">Such a class is called a simset, made up As an example of the expansion of context words via simsets, suppose that the occurrence of the frequency 1 word &amp;quot;cheAcheB&amp;quot; is immediately preceded by &amp;quot;few&amp;quot; and the occurrence of the frequency 1 word &amp;quot;CheA/CheB&amp;quot; is immediately preceded by &amp;quot;less&amp;quot;.</S>
			<S sid ="151" ssid = "151">The -I C context vectors for each will have l&apos;s in different positions so there will be no overlap between them.</S>
			<S sid ="152" ssid = "152">If we expanded &amp;quot;few&amp;quot; into a large enough simset, the set would eventually contain, &amp;quot;less&amp;quot;, and vice-versa.</S>
			<S sid ="153" ssid = "153">Barring that, each simset might contain a distinct c o m m o n word such as &amp;quot;decreased&amp;quot;.</S>
			<S sid ="154" ssid = "154">In either case, there would n o w be some overlap in the context vectors so that the similar use of &amp;quot;cheAcheB&amp;quot; and &amp;quot;CheA/CheB&amp;quot; could be detected.</S>
			<S sid ="155" ssid = "155">The apparent frequency of each expansion word is based on its corpus frequency relative to the corpus frequency of the word being expanded.</S>
			<S sid ="156" ssid = "156">To expand a single context word instance ci appearing with frequency fik in the context of 1 or more occurrences of center word wk, choose all cj such that cj e {set of high-frequency context words} and the 123 I similarity S(ci,cj)_&gt;St, a threshold value.</S>
			<S sid ="157" ssid = "157">Set the apparent frequency of each expansion word cj to fjk = S(ci,cj)xfikx fj / fi , where fi and fj are the corpus frequencies of ci and cj.</S>
			<S sid ="158" ssid = "158">Normalize the total frequency of the context word plus the apparent frequencies of the expansion words to fik.</S>
			<S sid ="159" ssid = "159">For the example being discussed here, fik = 1, St=0.3 and the average number of expansion words was 6.</S>
			<S sid ="160" ssid = "160">Recomputing the classification of the -ed forms with the expanded context words results in the improved classification shown in Figure 2B.</S>
			<S sid ="161" ssid = "161">The number of classification errors is halved, yielding a success rate of 92%.</S>
			<S sid ="162" ssid = "162">This is comparable in performance to many stochastic tagging algorithms.</S>
			<S sid ="163" ssid = "163">Discussion o f Experiment #2This analysis is very similar to part-of speech tagging.</S>
			<S sid ="164" ssid = "164">The simsets of only 6 items are far smaller than the part-of-speech categories conventionally used.</S>
			<S sid ="165" ssid = "165">But since we use high frequency words, they represent a substantial portion of the instances.</S>
			<S sid ="166" ssid = "166">Also, they have higher specificity than, say, Verb.</S>
			<S sid ="167" ssid = "167">M a n y taggers work sequentially and depend on the left context.</S>
			<S sid ="168" ssid = "168">But some words are best 11 1 VBNP.</S>
			<S sid ="169" ssid = "169">I 124 classified by their right context.</S>
			<S sid ="170" ssid = "170">We supply both.</S>
			<S sid ="171" ssid = "171">Clearly this small experiment did not reach the accuracy of the very best taggers, b u t it performed well.</S>
			<S sid ="172" ssid = "172">This experiment has major ramifications for the future.</S>
			<S sid ="173" ssid = "173">The initial classifications found merged all identical word forms together, both as targets and contexts.</S>
			<S sid ="174" ssid = "174">But disambiguation techniques such as those in Experiment #2 can be used to differentially t a g word occurrences with some degree of accuracy.</S>
			<S sid ="175" ssid = "175">These newly classified items can in turn be used as new target and context items (if their frequencies are adequate) and the analysis can be repeated.</S>
			<S sid ="176" ssid = "176">Iterating the method in this way should be able to refine the classes until a fixed point is reached at which no further improvement in classification occurs.</S>
			<S sid ="177" ssid = "177">The major challenge in using this approach will be to keep it computationally tractable.</S>
			<S sid ="178" ssid = "178">This approach is similar in spirit to the iterative computational approaches of the Hidden Markov Models (Kupiec, 1989; Kupiec, 1992; Rabiner, 1989), though our zeroth order solution begins quite close to the desired result, so it should converge very close to a global optimum.</S>
			<S sid ="179" ssid = "179">l l VBNP 4 JJ.</S>
			<S sid ="180" ssid = "180">31 &apos;I 9 VBD 46 VBN.</S>
			<S sid ="181" ssid = "181">3 JJ 1 JJ.</S>
			<S sid ="182" ssid = "182">1 VBD.</S>
			<S sid ="183" ssid = "183">1 VBNP.</S>
			<S sid ="184" ssid = "184">Figure 2B.</S>
			<S sid ="185" ssid = "185">Clustering of&amp;quot;cloned&amp;quot; and &amp;quot;deduced&amp;quot; after expansion of the context words.</S>
			<S sid ="186" ssid = "186">The postmodifying form, not isolated before, is fairlywell isolated in its own subclass.</S>
			<S sid ="187" ssid = "187">The total number of errors is reduced from 23 to 11, for a success rate of 92%.</S>
			<S sid ="188" ssid = "188">21 EXPERIMENT #3: CLASSIFICATION OF SINGLE WORD OCCURRENCES When classifying multiple instances of a single word form as we did in Experiment #2, there are numerous collocations that aid the classification.</S>
			<S sid ="189" ssid = "189">For example, 16 of the 50 occurrences of the word &amp;quot;deduced&amp;quot; occur in the phrase, &amp;quot;of the ~ amino acid sequence&amp;quot;.</S>
			<S sid ="190" ssid = "190">But with words of frequency 1, we cannot rely on such similarities.</S>
			<S sid ="191" ssid = "191">Nevertheless, we experimented with classifying 100 words of corpus f r e q u e n c y 1 with and w i t h o u t expanding the context words.</S>
			<S sid ="192" ssid = "192">Though hand scoring the results is difficult, we estimate t h a t there were 8 reasonable pairs found initially and 26 pairs when expansion was used.</S>
			<S sid ="193" ssid = "193">Examples of words t h a t paired well without expansion are &amp;quot;overlaps&amp;quot; and &amp;quot;flank&amp;quot; (due to a preceding &amp;quot;which&amp;quot;) and &amp;quot;malB&amp;quot; and &amp;quot;cheAcheB&amp;quot; (due to the context &amp;quot;...the [malB, cheAcheB] region...&amp;quot;).</S>
			<S sid ="194" ssid = "194">A f t e r expansion, pairs such as &amp;quot;setting&amp;quot;, &amp;quot;resetting&amp;quot; appeared (due in part to the expansion of the preceding &amp;quot;as&amp;quot; and &amp;quot;to&amp;quot; context words into simsets which both included &amp;quot;with&amp;quot;, &amp;quot;in&amp;quot; and &amp;quot;by&amp;quot;).</S>
			<S sid ="195" ssid = "195">D i s c u s s i o n of E x p e r i m e n t #3.</S>
			<S sid ="196" ssid = "196">The amount of information available about frequency 1 words can vary from a lot to nothing at all, and most frequently tends to the latter, viz., &amp;quot;John and Mary looked at the blork.&amp;quot; N e v e r t h e l e s s , such words are prominent, 44% of our corpus&apos; vocabulary.</S>
			<S sid ="197" ssid = "197">About half Of them are nontechnical and can therefore be analyzed from other corpora or online dictionaries.</S>
			<S sid ="198" ssid = "198">Word morphology and Latinate morphology in particular, can be helpful.</S>
			<S sid ="199" ssid = "199">Online chemical d a t a b a s e s , s u p p l e m e n t e d with rules for chemical nomenclature will clarify additional items, e . g . , &amp;quot; 2 - e p o x y p r o p y l p h o s p h o n i c &amp;quot; or &amp;quot;phosphoglucomutasedeflcient&amp;quot;.</S>
			<S sid ="200" ssid = "200">Furthermore, there are naming conventions for genetic strains and m u t a n t s which aid recognition.</S>
			<S sid ="201" ssid = "201">The combination of all these methods should lead to a reasonable accuracy in the classification of frequency 1 words.</S>
			<S sid ="202" ssid = "202">FURTHER DISCUSSION AND FUTURE DIRECTIONS Our corpus of 220,000 words is much smaller than ones of 40 million words (Finch &amp; Chater, 1992) and certainly of 360 million (Brown, Della Pietra, deSousa, Lai &amp; Mercer, 1992).</S>
			<S sid ="203" ssid = "203">But judging by the results we have presented, especially for the full 1,000 word clustering, our corpus appears to make up in specificity for what it lacks in size.</S>
			<S sid ="204" ssid = "204">Extending this work beyond abstracts to full papers will be challenging because our corpus requires SGML markup to deal with Greek characters, superscripts and subscripts, etc.</S>
			<S sid ="205" ssid = "205">(Futrelle, Dunn, Ellis &amp; Pescitelli, 1991).</S>
			<S sid ="206" ssid = "206">We have over 500,000 words from the bacterial chemotaxis research papers carefully marked up by hand in this way.</S>
			<S sid ="207" ssid = "207">The characterization of context can obviously be e x t e n d e d to more context positions or words, and extensions of our w o r d - r o o t e d e x p a n s i o n t e c h n i q u e s are potentially very powerful, combining broad coverage with specificity in a &amp;quot;tunable&amp;quot; way.</S>
			<S sid ="208" ssid = "208">Morphology can be added to the context vectors by using the ingenious suggestion of Brill to collect high-frequency tri-letter word endings (Brill &amp; Marcus, 1992).</S>
			<S sid ="209" ssid = "209">One of the more subtle problems of the context specification is that it uses summed frequencies, so it may fail to retain important correlations.</S>
			<S sid ="210" ssid = "210">Thus if only AB or CD sequences occurred, or only AD or CB sequences, they would lead to the same (summed) context vector.</S>
			<S sid ="211" ssid = "211">The only correlations faithfully retained are those with the target word.</S>
			<S sid ="212" ssid = "212">Characterizing context n-grams could help work around this problem, b u t is a nontrivial task.</S>
	</SECTION>
	<SECTION title="ACKNOWLEDGMENTS">
</PAPER>
