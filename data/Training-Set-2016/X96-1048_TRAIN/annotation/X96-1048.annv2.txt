Citance Number: 1 | Reference Article:  X96-1048.txt | Citing Article:  A97-1028.txt | Citation Marker Offset:  [] | Citation Marker:  Sundheim ... 1995b | Citation Offset:  [] | Citation Text:  Named Entity evaluation began as a part of recent Message Understanding Conferences (MUC), whose objective was to standardize the evaluation of IE tasks (Sundheim, 1995b). | Reference Offset:  ['357'] | Reference Text:  In addition, there are plans to put evaluations on line, with public access, starting with the NE evaluation; this is intended to make the NE task familiar to new sites and to give them a convenient and low-pressure way to try their hand at following a standardized test procedure | Discourse Facet:  Implication_Citation | Annotator:  Muthu Kumar Chandrasekaran, NUS |


Citance Number: 2 | Reference Article:  X96-1048.txt | Citing Article:  E12-2021.txt | Citation Marker Offset:  [] | Citation Marker:  Sundheim ... 1996 | Citation Offset:  [] | Citation Text:  These base annotations can be connected by binary relations  either directed or undirected  which can be congured for e.g. simple relation extraction, or verb frame annotation (Figure 1 middle and bottom). n-ary associations of annotations are also supported, allowing the annotation of event structures such as those targeted in the MUC (Sundheim, 1996), | Reference Offset:  ['15'] | Reference Text:  Scenario Template (ST) -- Drawing evidence from anywhere in the text, extract prespecified event information, and relate the event information to the particular organization and person entities involved in the event. | Discourse Facet:  Method_Citation | Annotator:  Muthu Kumar Chandrasekaran, NUS |


Citance Number: 3 | Reference Article:  X96-1048.txt | Citing Article:  J00-4003.txt | Citation Marker Offset:  [] | Citation Marker:  Sundheim 1995 | Citation Offset:  [] | Citation Text:  This so-called named entity recognition task has received considerable attention recently ... and was one of the tasks evaluated in the Sixth and Seventh Message Understanding Conferences. In MUC-6, 15 different systems participated in the competition (Sundheim 1995) | Reference Offset:  ['4'] | Reference Text:  The Named Entity and Coreference tasks entailed Standard Generalized Markup Language (SGML) annotation of texts and were being conducted for the first time. | Discourse Facet:  Method_Citation | Annotator:  Muthu Kumar Chandrasekaran, NUS |


Citance Number: 4 | Reference Article:  X96-1048.txt | Citing Article:  J00-4003.txt | Citation Marker Offset:  [] | Citation Marker:  Sundheim 1995 | Citation Offset:  [] | Citation Text:  As a result, quantitative evaluation is n o w commonplace in areas of language engineering such as parsing, and quantitative evaluation techniques are being proposed for semantic ... interpretation as well, for example, at the Sixth and Seventh Message Understanding Conferences (MUC-6 and MUC-7) (Sundheim 1995; Chinchor 1997), which also included evaluations of systems on the so-called coreference task, a subtask of which is the resolution of definite descriptions. | Reference Offset:  ['4', '113'] | Reference Text:  The Named Entity and Coreference tasks entailed Standard Generalized Markup Language (SGML) annotation of texts and were being conducted for the first time. ... In the middle of the spectrum are definite descriptions and pronouns whose choice of referent is constrained by such factors as structural relations and discourse focus | Discourse Facet:  Method_Citation | Annotator:  Muthu Kumar Chandrasekaran, NUS |


Citance Number: 5 | Reference Article:  X96-1048.txt | Citing Article:  J00-4003.txt | Citation Marker Offset:  [] | Citation Marker:  Sundheim 1995 | Citation Offset:  [] | Citation Text:  Previous attempts to annotate anaphoric relations had resulted in very low agreement levels; for example, in the coreference annotation experiments for MUC-6 (Sundheim 1995), relations other than ... identity were dropped due to difficulties in annotating them | Reference Offset:  ['129', '132', '133'] | Reference Text:  In the middle of the effort of preparing the test data for the formal evaluation, an interannotator variability test was conducted ... There was a large number of factors that contributed to the 20% disagreement, including overlooking coreferential NPs, using different interpretations of vague portions of the guidelines, and making different subjective decisions when the text of an article was ambiguous, sloppy, etc. Most human errors pertained to definite descriptions and bare nominals, not to names and pronouns | Discourse Facet:  Results_Citation | Annotator:  Muthu Kumar Chandrasekaran, NUS |


Citance Number: 6 | Reference Article:  X96-1048.txt | Citing Article:  W97-1307.txt | Citation Marker Offset:  [] | Citation Marker:  Sundheim ... 1995 | Citation Offset:  [] | Citation Text:  This algorithm was first implemented for the MUC-6 FASTUS system (Appelt et al., 1995), and produced one of the top scores (a recall of 59% and precision of 72%) in the MUC-6 Coreference Task, which evaluated systems&apos; ability to recognize coreference among noun phrases (Sundheim, 1995) | Reference Offset:  ['13'] | Reference Text:  Coreference (CO) -- Insert SGML tags into the text to link strings that represent coreferring noun phrases | Discourse Facet:  Method_Citation | Annotator:  Muthu Kumar Chandrasekaran, NUS |


Citance Number: 7 | Reference Article:  X96-1048.txt | Citing Article:  P06-1059.txt | Citation Marker Offset:  [] | Citation Marker:  Sundheim ... 1995 | Citation Offset:  [] | Citation Text:  whereas the best performance at MUC-6, in which systems tried to identify general named entities such as person or organization names, was an accuracy of 95% (Sundheim, 1995). | Reference Offset:  ['54'] | Reference Text:  When the outputs are scored in quot;key-to-responsequot; mode, as though one annotator's output represented the &quot;key&quot; and the other the &quot;response,&quot; the humans achieved an overall F-measure of 96.68 and a corresponding error per response fill (ERR) score of 6% | Discourse Facet:  Results_Citation | Annotator:  Muthu Kumar Chandrasekaran, NUS |


Citance Number: 8 | Reference Article:  X96-1048.txt | Citing Article:  C04-1126.txt | Citation Marker Offset:  [] | Citation Marker:  Sundheim ... 1995 | Citation Offset:  [] | Citation Text:  The MUC organisers provided strict guidelines about what constituted a succession event and how the templates should be lled which the annotators sometimes found dicult to interpret (Sundheim, 1995). Interannotator agreement was measured on 30 texts which were examined by two annotators. It was found to be 83% when one annotators templates were assumed to be correct and compared with the other | Reference Offset:  ['27', '28', '31', '227'] | Reference Text:  Testing was conducted using Wall Street Journal texts provided by the Linguistic Data Consortium. The articles used in the evaluation were drawn from a corpus of approximately 58,000 articles spanning the period of January 1993 through June 1994. ... The training set and test set each consisted of 100 articles and were drawn from the corpus using a text retrieval system called Managing Gigabytes, whose retrieval engine is based on a context-vector model, producing a ranked list of hits according to degree of match with a keyword search query ... Of the 100 texts in the test set, 54 were relevant to the management succession scenario, including six that were only marginally relevant | Discourse Facet:  Method_Citation | Annotator:  Muthu Kumar Chandrasekaran, NUS |


Citance Number: 9 | Reference Article:  X96-1048.txt | Citing Article:  C04-1126.txt | Citation Marker Offset:  [] | Citation Marker:  Sundheim ... 1995 | Citation Offset:  [] | Citation Text:  The test corpus consists of 100 Wall Street Journal documents from the period January 1993 to June 1994, 54 of which contained management succession events (Sundheim, 1995). The format used to represent events in the MUC-6 corpus is now described. | Reference Offset:  ['224'] | Reference Text:  Human performance was measured in terms of interannotator variability on only 30 texts in the test set and showed agreement to be approximately 83%, when one annotator's templates were treated as the &quot;key&quot; and the other annotator&apos;s templates were treated as the &quot;response.&quot; | Discourse Facet:  Results_Citation | Annotator:  Muthu Kumar Chandrasekaran, NUS |


Citance Number: 10 | Reference Article:  X96-1048.txt | Citing Article:  W99-0612.txt | Citation Marker Offset:  [] | Citation Marker:  Sundheim 1995 | Citation Offset:  [] | Citation Text:  It is important to mention that the F-measure for the human performance on this task is about 96%, (Sundheim 1995) | Reference Offset:  ['54', '57'] | Reference Text:  When the outputs are scored in &amp;quot;key-to-response&amp;quot; mode, as though one annotator&apos;s output represented the &amp;quot;key&amp;quot; and the other the &amp;quot;response,&amp;quot; the humans achieved an overall F-measure of 96.68 and a corresponding error per response fill (ERR) score of 6% ... SRA &amp;quot;fast&amp;quot; configuration 95.66 | Discourse Facet:  Results_Citation | Annotator:  Muthu Kumar Chandrasekaran, NUS |


Citance Number: 11 | Reference Article:  X96-1048.txt | Citing Article:  E99-1001.txt | Citation Marker Offset:  [] | Citation Marker: Sundheim 1995: 16 | Citation Offset:  [] | Citation Text:  In an article on the Named Entity recognition competition (part of MUC-6) Sundheim (1995) remarks that &quot;common organization names, first names of people and location names can be handled by recourse to list lookup, although there are drawbacks&quot; (Sundheim 1995: 16) | Reference Offset:  ['72'] | Reference Text:  Common organization names, first names of people, and location names can be handled by recourse to list lookup, although there are drawbacks: some names may be on more than one list, the lists will not be ... complete and may not match the name as it is realized in the text (e.g., may not cover the needed abbreviated form of an organization name, may not cover the complete person name), etc | Discourse Facet:  Method_Citation | Annotator:  Muthu Kumar Chandrasekaran, NUS |


Citance Number: 12 | Reference Article:  X96-1048.txt | Citing Article:  M98-1003.txt | Citation Marker Offset:  [] | Citation Marker:  Sundheim 1995 | Citation Offset:  [] | Citation Text:  The organizers of MUC-6 did not attempt to compare the difficulty of the MUC-6 task to the previous MUC tasks saying that &amp;quot;the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed&amp;quot; [Sundheim 1995] | Reference Offset:  ['245', '246'] | Reference Text:  No analysis has been done of the relative difficulty of the MUC-6 ST task compared to previous extraction evaluation tasks. The one month limitation on development in preparation for MUC-6 would be difficult to factor into the computation, and even without that additional factor, the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed | Discourse Facet:  Implication_Citation | Annotator:  Muthu Kumar Chandrasekaran, NUS |


