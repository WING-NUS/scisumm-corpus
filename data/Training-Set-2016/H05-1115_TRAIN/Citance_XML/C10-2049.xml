<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">We introduce the novel problem of automatic related work summarization.</S>
		<S sid ="2" ssid = "2">Given multiple articles (e.g., conference/journal papers) as input, a related work sum- marization system creates a topic-biased summary of related work specific to the target paper.</S>
		<S sid ="3" ssid = "3">Our prototype Related Work Summarization system, ReWoS, takes in set of keywords arranged in a hierarchical fashion that describes a target paper’s topics, to drive the creation of an extractive summary using two different strategies for locating appropriate sentences for general topics as well as detailed ones.</S>
		<S sid ="4" ssid = "4">Our initial results show an improvement over generic multi-document summarization baselines in a human evaluation.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="5" ssid = "5">In many fields, a scholar needs to show an understanding of the context of his problem and relate his work to prior community knowledge.</S>
			<S sid ="6" ssid = "6">A related work section is often the vehicle for this purpose; it contextualizes the scholar’s contributions and helps readers understand the critical aspects of the previous works that current work addresses.</S>
			<S sid ="7" ssid = "7">Creating such a summary requires the author to position his own work within the contextual research to showcase the advantages of his method.</S>
			<S sid ="8" ssid = "8">We envision an NLP application that assists in creating a related work summary.</S>
			<S sid ="9" ssid = "9">We propose this related work summarization task as a challenge summarization problem that takes as input a target scientific document for which a related work section needs to be drafted.</S>
			<S sid ="10" ssid = "10">The output goal is to create a related work section that finds the relevant related works and contextually describes them in relationship to the scientific document at hand.</S>
			<S sid ="11" ssid = "11">We dissect the full challenge as bringing together work of disparate interests; 1) in finding relevant documents; 2) in identifying the salient aspects of these documents in relation to the current work worth summarizing; and 3) in generating the final topic-biased summary.</S>
			<S sid ="12" ssid = "12">While it is clear that current NLP technology does not let us build a complete solution for this task, we believe that tackling the individual components will help bring us towards an eventual solution.</S>
			<S sid ="13" ssid = "13">In fact, existing works in the NLP and recommendation systems communities have already begun work that fits towards the completion of the first two tasks.</S>
			<S sid ="14" ssid = "14">Citation prediction (Nallapati et al., 2008) is a growing research area that has aimed both at predicting citation growth over time within a community and at individual paper citation patterns.</S>
			<S sid ="15" ssid = "15">This year, an automatic keyphrase extraction task from scientific articles was first fielded in SemEval2, partially addressing Task 11.</S>
			<S sid ="16" ssid = "16">Also, automatic survey generation (Moham-.</S>
			<S sid ="17" ssid = "17">mad et al., 2009) is becoming a growing field within the summarization community.</S>
			<S sid ="18" ssid = "18">However, to date, we have not yet seen any work that examines topic-biased summarization of multiple scientific articles.</S>
			<S sid ="19" ssid = "19">For these reasons, we focus on Task 3 here – the creation of a related work section, given a structured input of the topics for summary..</S>
			<S sid ="20" ssid = "20">The remaining contributions of our paper to the automatic summarization community.</S>
			<S sid ="21" ssid = "21">In its full form, it is a topic-biased, multi-document 1http://semeval2.fbk.eu/semeval2.php 427 Coling 2010: Poster Volume, pages 427–435, Beijing, August 2010 consists of work towards this goal: • We conduct a study of the argumentative patterns used in related work sections, to describe the plausible summarization tactics for their creation in Section 3.</S>
			<S sid ="22" ssid = "22">• We describe our approach to generate an extractive related work summary given an input topic hierarchy tree, using two separate strategies to differentiate between summarizing shallow internal nodes from deep detailed leaf nodes of the topic tree in Section 4.</S>
	</SECTION>
	<SECTION title="Related Work. " number = "2">
			<S sid ="23" ssid = "1">Fully automated related work summarization is significantly different from traditional summarization.</S>
			<S sid ="24" ssid = "2">While there are no existing studies on this specific problem, there are closely related endeavors.</S>
			<S sid ="25" ssid = "3">The iOPENER2 project works towards automated creation of technical surveys, given a research topic (Mohammad et al., 2009).</S>
			<S sid ="26" ssid = "4">Standard generic multi-document summarization algorithms were applied to generate technical surveys.</S>
			<S sid ="27" ssid = "5">They showed that citation information was effective in the generation process.</S>
			<S sid ="28" ssid = "6">This was also validated earlier in (Nakov et al., 2004), which showed that the citing sentences in other papers can give a useful description of a target work.</S>
			<S sid ="29" ssid = "7">Other studies focus mainly on single-document scientific article summarization.</S>
			<S sid ="30" ssid = "8">The pioneers of automated summarization (Luhn, 1958; Baxen- dale, 1958; Edmundson, 1969) had envisioned their approaches being used for the automatic creation of scientific summaries.</S>
			<S sid ="31" ssid = "9">They examined various features specific to scientific texts (e.g., frequency-based, sentence position, or rhetorical clues features) which were proven effective for domain-specific summarization tasks.</S>
			<S sid ="32" ssid = "10">Further, Mei and Zhai (2008) and Qazvinian and Radev (2008) utilized citation information in creating summaries for a single scientific article in computational linguistics domain.</S>
			<S sid ="33" ssid = "11">Also, Schwartz and Hearst (2006) also utilized the citation sentences to summarize the key concepts and entities in bioscience texts, and argued that citation sentences may contain informative contributions of a paper that complement its original abstract.</S>
			<S sid ="34" ssid = "12">2http://clair.si.umich.edu/clair/iopener/ These works all center on the role of citations and their contexts in creating a summary, using citation information to rank content for extraction.</S>
			<S sid ="35" ssid = "13">However, they did not study the rhetorical structure of the intended summaries, targeting more on deriving useful content.</S>
			<S sid ="36" ssid = "14">For working along this vein, we turn to studies on the rhetorical structure of scientific articles.</S>
			<S sid ="37" ssid = "15">Perhaps the most relevant is work by (Teufel, 1999; Teufel and Moens, 2002) who defined and studied argumentative zoning of texts, especially ones in computational linguistics.</S>
			<S sid ="38" ssid = "16">While they studied the structure of an entire article, it is clear from their studies that a related work section would contain general background knowledge (BACKGROUND zone) as well as specific information credited to others (OTHER and BASIS zones).</S>
			<S sid ="39" ssid = "17">This vein of work has been followed by many, including Teufel et al.</S>
			<S sid ="40" ssid = "18">(2009; Angrosh et al.</S>
			<S sid ="41" ssid = "19">(2010).</S>
	</SECTION>
	<SECTION title="Structure of Related Work Section. " number = "3">
			<S sid ="42" ssid = "1">We first extend the work on rhetorical analysis, concentrating on related work sections.</S>
			<S sid ="43" ssid = "2">By studying examples in detail, we gain insight on how to approach related work summarization.</S>
			<S sid ="44" ssid = "3">We focus on a concrete related work example for illustration, an excerpt of which is shown in Figure 1a.</S>
			<S sid ="45" ssid = "4">Focusing on the argumentative progression of the text, we note the flow through different topics is hierarchical and can be represented as a topic tree as in Figure 1b.</S>
			<S sid ="46" ssid = "5">This summary provides background knowledge for a paper on text classification, which is the root of the topic tree (node 1; lines 1–5).</S>
			<S sid ="47" ssid = "6">Two topics (“feature selection” and “machine learning”) are then presented in parallel (nodes 2 &amp; 3; lines 5–8 &amp; 9–15), where specific details on relevant works are selected to describe two topics.</S>
			<S sid ="48" ssid = "7">These two topics are implicitly understood as subtopics of a more general topic, namely “monolingual text classification” (node 4; lines 16–17).</S>
			<S sid ="49" ssid = "8">The authors use the monolingual topic to contrast it with the subsequent subtopic “multilingual text classification” (node 5; lines 18–21).</S>
			<S sid ="50" ssid = "9">This topic is described by elaborating its details through two sub- topics: “bilingual text classification” and “cross- lingual text classification” (nodes 6 &amp; 7; lines 22– 25 &amp; 25– 39) where again, various example works line line 1 (a) (b) (c) Figure 1: a) A related work section extracted from (Wu and Oard, 2008); b) An associated topic hierarchy tree of a); c) An associated topic tree, annotated with key words/phrases.</S>
			<S sid ="51" ssid = "10">are described and cited.</S>
			<S sid ="52" ssid = "11">The authors then conclude by contrasting their proposed approach with the introduced relevant approaches (lines 40–42).</S>
			<S sid ="53" ssid = "12">This summary illustrates three important points.</S>
			<S sid ="54" ssid = "13">First, the topic tree is an essential input to the summarization process.</S>
			<S sid ="55" ssid = "14">The topic tree can be thought of as a high-level rhetorical structure for which a process then attaches content.</S>
			<S sid ="56" ssid = "15">While it is certainly nontrivial to build such a tree, modifications to hierarchical topic modeling (M. et al., 2004) or keyphrase extraction algorithms (Witten et al., 1999) we believe can be used to induce a suitable form.</S>
			<S sid ="57" ssid = "16">A resulting topic hierarchy from such a process would provide an associated set of key words or phrases that would describe the node, as shown in Figure 1c.</S>
			<S sid ="58" ssid = "17">Second, while summaries can be structured in many ways, they can be viewed as moves along the topic hierarchy tree.</S>
			<S sid ="59" ssid = "18">In the example, nodes 2 and 3 are discussed before their parent, as the parent node (node 4) serves as a useful contrast to introduce its sibling (node 5).</S>
			<S sid ="60" ssid = "19">We find variants of depth-first traversal common, but breadth-first traversals of nodes with multiple descendants are more rare.</S>
			<S sid ="61" ssid = "20">They may be structured this way to ease the reader’s burden on memory and attention.</S>
			<S sid ="62" ssid = "21">This is in line with other summary genres where information is ordered by high-level logical considerations that place macro level constraints (Barzilay et al., 2002).</S>
			<S sid ="63" ssid = "22">Third, there is a clear distinction between sentences that describe a general topic and those that describe work in detail.</S>
			<S sid ="64" ssid = "23">Generic topics are often represented by background information, which is not tied to a particular prior work.</S>
			<S sid ="65" ssid = "24">These include definitions or descriptions of a topic’s purpose.</S>
			<S sid ="66" ssid = "25">In contrast, detailed information forms the bulk of the summary and often describes key related work that is attributable to specific authors.</S>
			<S sid ="67" ssid = "26">Recently, Jaidka et al.</S>
			<S sid ="68" ssid = "27">(2010) also present the beginnings of a corpus study of related work sections, where they differentiate integrative and descriptive strategies in presenting discourse work.</S>
			<S sid ="69" ssid = "28">We see our differentiation between general and detailed topics as a natural parallel to their notion of integrative and descriptive strategies.</S>
			<S sid ="70" ssid = "29">To introspect on these findings further, we created a related work data set (called RWSData3), which includes 20 articles from well-respected venues in NLP and IR, namely SIGIR, ACL, NAACL, EMNLP and COLING.</S>
			<S sid ="71" ssid = "30">We extracted the related work sections directly from those research articles as well as references the sections cited.</S>
			<S sid ="72" ssid = "31">References to books and Ph.D. theses were removed, as their verbosity would change the problem drastically (Mihalcea and Ceylan, 2007).</S>
			<S sid ="73" ssid = "32">Since we view each related work summary as a topic-biased summary originating from a topic hierarchy tree, annotation of such topical information for our data set is necessary.</S>
			<S sid ="74" ssid = "33">Each article’s data consists of the reference related work summary, the collection of the input research articles 3To be made available at http://wing.comp.nus.</S>
			<S sid ="75" ssid = "34">edu.sg/downloads/rwsdata.</S>
			<S sid ="76" ssid = "35">SbL−RW WbL−RW No−RAs SbL−RA WbL−RA TS TD average 17.9 522.4 10.9 2386.0 51739.6 3.3 1.8 stdev 7.9 216.5 5.6 1306.7 26682.3 1.7 0.6 min 6 179 2 348 8580 1 1 max 40 922 26 5549 112267 7 3 Table 1: The demographics of RWSData.</S>
			<S sid ="77" ssid = "36">No, RW, RA, SbL, WbL, TS, and TD are labeled as (N)umber (o)f, (R)elated (W)orks, (R)eferenced (A)rticles, (S)entence-(b)ased (L)ength of, (W)ord- (b)ased (L)ength of, (T)ree (S)ize, and (T)ree (D)epth, respectively.</S>
			<S sid ="78" ssid = "37">that were referenced and a manually-constructed topic descriptions in a hierarchical fashion (topic tree).</S>
			<S sid ="79" ssid = "38">More details on the demographics of RWS- Data are shown in Table 1.</S>
			<S sid ="80" ssid = "39">RWSData summaries average 17.9 sentences, 522 words in length, citing an average of 10.9 articles.</S>
			<S sid ="81" ssid = "40">While hierarchical, the topic trees are simple, averaging 3.3 topic nodes in size and average depth of 1.8.</S>
			<S sid ="82" ssid = "41">Their simplicity furthers our claim that automated methods would be able to create such trees.</S>
	</SECTION>
	<SECTION title="ReWoS: Paired General and Specific. " number = "4">
			<S sid ="83" ssid = "1">Summarization Figure 2: The ReWoS architecture.</S>
			<S sid ="84" ssid = "2">Decision edges labeled as True, False and Relevant.</S>
			<S sid ="85" ssid = "3">Inspired by the above observations, we propose a novel strategy for related work summarization with respect to a given topic tree.</S>
			<S sid ="86" ssid = "4">Note that while the construction of the topic tree is central to the process, we consider this outside the scope of the current work (see §1); our investigation focuses on how such input could be utilized to construct a reasonable topic-biased related work summary.</S>
			<S sid ="87" ssid = "5">We posit that sentences within a related work section come about by means of two separate processes – a process that gives general background information and another that describes specific author contributions.</S>
			<S sid ="88" ssid = "6">A key realization in our work is that these processes are easily mapped to the topic tree topologically: general content is described in tree-internal nodes, whereas leaf nodes contribute detailed specifics.</S>
			<S sid ="89" ssid = "7">In our approach, these two processes are independent, and combined to construct the final summary.</S>
			<S sid ="90" ssid = "8">We have implemented our idea in ReWoS (Related Work Summarizer), whose general architecture is shown in Figure 2.</S>
			<S sid ="91" ssid = "9">ReWoS is a largely heuristic system, featuring both a General Content Summarization (GCSum) and a Specific Content Summarization (SCSum) modules, prefixed by preprocessing.</S>
			<S sid ="92" ssid = "10">A natural language template generation system fills out the end of the summary.</S>
			<S sid ="93" ssid = "11">ReWoS first applies a set of preprocessing steps (shown in the top of Figure 2).</S>
			<S sid ="94" ssid = "12">Input sentences (i.e., the set of sentences from each related/cited article) first removes sentences that are too short (&lt; 7 tokens) or too long (&gt; 80 tokens), ones that use future tense (possibly future work), and example and navigation sentences.</S>
			<S sid ="95" ssid = "13">This last category is filtered out by checking for the presence of a cue phrase among a lexical pattern database: e.g., “in the section”, “figure x shows”, “for instance”.</S>
			<S sid ="96" ssid = "14">Lowercasing and stemming are also performed.</S>
			<S sid ="97" ssid = "15">We then direct sentences to either GCSum or SCSum based on whether it describes the author’s own work or not, similar in spirit and execution to (Teufel et al., 2009).</S>
			<S sid ="98" ssid = "16">If sentence contains indicative pronouns or cue phrases (e.g., “we”, “this ap proach”), the sentence is deemed to describe own work and is directed to SCSum; otherwise the sentence is directed to the GCSum workflow.</S>
			<S sid ="99" ssid = "17">4.1 (G)eneral (C)ontent (Sum)marization GCSum extracts sentences containing useful background information on the topics of the internal node in focus.</S>
			<S sid ="100" ssid = "18">Since general content sentences do not specifically describe work done by the authors, we only take sentences that do not have the author-as-agent as input for GCSum.</S>
			<S sid ="101" ssid = "19">We divide such general content sentences into two groups: indicative and informative.</S>
			<S sid ="102" ssid = "20">Infor verb phrases (i.e., “based on”, “make use of” and 23 other patterns) are used as the main verb.</S>
			<S sid ="103" ssid = "21">Otherwise, GCSum checks for the presence of at least one citation – general sentences may list a set of citations as examples.</S>
			<S sid ="104" ssid = "22">If both the cue verb and citation checks fail, the sentence is dropped.</S>
			<S sid ="105" ssid = "23">GCSum’s topic relevance computation ranks remaining sentences based on keyword content.</S>
			<S sid ="106" ssid = "24">We state that the topic of an internal node is affected by its surrounding nodes – ancestor, descendants and siblings.</S>
			<S sid ="107" ssid = "25">Based on this idea, the score of a sentence is computed in a discriminative way using the following linear combination: mative sentences give detail on a specific aspect scoreS → scoreQA Q QR of the problem.</S>
			<S sid ="108" ssid = "26">They often give definitions, purpose or application of the topic (“Text classifica S + scoreS − scoreS (1) where scoreS is the final relevance score, and QA Q QR tion is a task that assigns a certain number of predefined labels for a given text.”).</S>
			<S sid ="109" ssid = "27">In contrast, indicative sentences are simpler, inserted to make the topic transition explicit and rhetorically sound (“Many previous studies have studied monolingual text classification.”).</S>
			<S sid ="110" ssid = "28">Indicative sentences can be easily generated by templates, as the primary information that is transmitted is the identity of the topic itself.</S>
			<S sid ="111" ssid = "29">Informative sentences, on the other hand, are better extracted from the source articles themselves, rescoreS , scoreS , and scoreS are the compo nent scores of the sentence S with respect to the ancestor, current or other remaining nodes.</S>
			<S sid ="112" ssid = "30">We give positive credit to a sentence that contains keywords from an ancestor node, but penalize sentences with keywords from other topics (as such sentences would be better descriptors for those other topics).</S>
			<S sid ="113" ssid = "31">Component relevance scores are calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring a specific strategy.</S>
			<S sid ="114" ssid = "32">As informative sen tences contain more content, our strategy with scoreQ = rel(S, Q) Q′ rel(S, Q′) S Q (2) GCSum is to attempt to locate informative sentences to describe the internal nodes, failing which GCSum falls back to using predefined templates to generate an indicative placeholder.</S>
			<S sid ="115" ssid = "33">= w∈Q log(tfw + 1) × log(tfw + 1) × isfw Nor m where rel(S, Q) is the relevance of S with respect to topic Q, Norm is a normalization factor of rel(S, Q) over all input sentences, tf S and tf Q w w To implement GCSum’s informative extractor, we use a set of heuristics in a decision cascade to first filter inappropriate sentences (as shown on the RHS of Figure 2).</S>
			<S sid ="116" ssid = "34">Remaining candidates (if any) are then ranked by relevance and the top n are selected for the summary.</S>
			<S sid ="117" ssid = "35">The heuristic cascade’s purpose is to ensure sentences fit the syntactic structure of commonly- observed informative sentences.</S>
			<S sid ="118" ssid = "36">A useful sentence should discuss the topic directly, so GCSum first checks the subject of each candidate sentence, filtering sentences whose subject do not contain at least one topic keyword.</S>
			<S sid ="119" ssid = "37">We observed that background sentences often feature specific verbs or citations.</S>
			<S sid ="120" ssid = "38">GCSum thus also checks whether stock are the term frequencies of token w within S or sentences that discuss topic Q, respectively.</S>
			<S sid ="121" ssid = "39">isfw is the inverse sentence frequency of w. 4.2 (S)pecific (C)ontent (Sum)marization SCSum aims to extract sentences that contain detailed information about a specific author’s work that is relevant to the input leaf node’s topic from the set of sentences that exhibit the author-as- agent.</S>
			<S sid ="122" ssid = "40">SCSum starts by computing the topic relevance of each candidate sentence as shown in Equation (3).</S>
			<S sid ="123" ssid = "41">This process is identical to the step in GCSum, except that the term scoreQR in Equa tion (1) is replaced by scoreQS , which is the relevance of S with respect to its sibling nodes.</S>
			<S sid ="124" ssid = "42">We hypothesize that given a leaf node, sibling node topics may have an even more pronounced negative effect than other remaining nodes in the topic tree.</S>
			<S sid ="125" ssid = "43">Weighting.</S>
			<S sid ="126" ssid = "44">The score of a candidate content sentence is computed from topic relevance computation (SCSum) that includes contributions for keywords present in the current, ancestor and sibling nodes.</S>
			<S sid ="127" ssid = "45">We observe that the presence of one or scoreS → scoreQA + scoreQ − scoreQS (3) S S S Context Modeling.</S>
			<S sid ="128" ssid = "46">We note that single sentences occasionally do not contain enough contexts to clearly express the idea mentioned in original articles.</S>
			<S sid ="129" ssid = "47">In fact, an agent-based sentence often introduces a concept but pertinent details are often described later.</S>
			<S sid ="130" ssid = "48">Extracting just the agent-based sentence may incompletely describe a concept and more of current, ancestor and sibling nodes may affect the final score from the computation.</S>
			<S sid ="131" ssid = "49">Thus, to partially address this, we add a new weighting coefficient for the score computed from the topic relevance computation (SCSum) (Equation (3)) as follows: score∗ = wQA,Q,QS × scoreS (4 ) S S lead to false inferences.</S>
			<S sid ="132" ssid = "50">Consider the examplein Figure 3.</S>
			<S sid ="133" ssid = "51">Here Sentences 05 are an contigu where: wQA,Q,QS is a weighting coefficient that ous extract of a source article being summarized, S takes on fering values based on the presence where Sentence 0 is an identified agent-based sentence.</S>
			<S sid ="134" ssid = "52">Sentence 6 shows a related work section sentence from a citing article that describes the original article.</S>
			<S sid ="135" ssid = "53">It is clear that the citing description is composed of information taken not only from the agent-based sentence but its context in the following sentences as well.</S>
			<S sid ="136" ssid = "54">This observation Figure 3: A context modeling example.</S>
			<S sid ="137" ssid = "55">motivates us to choose nearby sentences within a contextual window after the agent-based sentence to represent the topic.</S>
			<S sid ="138" ssid = "56">We set the contextual window to 5 and extract a maximum of 2 additional sentences.</S>
			<S sid ="139" ssid = "57">These additions are chosen based on their relevance scores to the topic, using Equation (3).</S>
			<S sid ="140" ssid = "58">Sentences with nonzero scores are then added as contexts of the anchor agent-based sentence, otherwise they are excluded.</S>
			<S sid ="141" ssid = "59">As a result, some topics may contain only a single sentence, but others may be described by additional contextual sentences.</S>
			<S sid ="142" ssid = "60">dif of keywords in the sentence.</S>
			<S sid ="143" ssid = "61">Q, QA, and QS denote keywords from current, ancestor and sibling nodes.</S>
			<S sid ="144" ssid = "62">If the sentence contains keywords from other sibling nodes, we assign a penalty of 0.1.</S>
			<S sid ="145" ssid = "63">Otherwise, we assign a weight of 1.0, 0.5, or 0.25, based on whether keywords are present from both the ancestor node and current node, just the current node or just the ancestor node.</S>
			<S sid ="146" ssid = "64">To build the final summary, ReWoS selects the top scoring sentence and iteratively adds the next most highly ranked sentence, until the n sentence budget is reached.</S>
			<S sid ="147" ssid = "65">We use SimRank (Li et al., 2008) to remove the next sentence to be added, if it is too similar to the sentences already in the summary.</S>
			<S sid ="148" ssid = "66">4.3 Generation.</S>
			<S sid ="149" ssid = "67">ReWoS generates its summaries by using depth- first traversal to order the topic nodes, as in RWS- Data we observed this to be the most prevalent discourse pattern.</S>
			<S sid ="150" ssid = "68">It calls GCSum and SCSum to summarize individual nodes, distributing the total sentence budget equally among nodes.</S>
			<S sid ="151" ssid = "69">ReWoS post-processes sentences to improve fluency where possible.</S>
			<S sid ="152" ssid = "70">We first replace agentive forms with a citation to the articles (e.g., “we” → “(Wu and Oard, 2008)”).</S>
			<S sid ="153" ssid = "71">ReWoS also replaces found abbreviations with their corresponding long forms, by connecting abbreviation with their expansions by utilizing dependency relation output from the Stanford dependency parser.</S>
			<S sid ="154" ssid = "72">System ROUGE Recall Scores Human Evaluation Scores ROUGE-1 ROUGE-2 ROUGE-S4 ROUGE-SU4 Correctness Novelty Fluency Usefulness LEAD 0.501 0.096 0.116 0.181 3.027 2.764 3.082 2.745 MEAD 0.663 0.178 0.211 0.287 3.009 3.109 2.591 2.700 ReWoS−WCM ReWoS−CM 0.584 0.698 0.127 0.183 0.154 0.218 0.227 0.298 3.618 3.691 3.391 3.618 3.391 2.955 3.609 3.573 Table 2: Evaluation results for ReWoS variants and baselines.</S>
	</SECTION>
	<SECTION title="Evaluation. " number = "5">
			<S sid ="155" ssid = "1">We wish to assess the quality of ReWoS, comparing to state-of-the-art generic summarization systems.</S>
			<S sid ="156" ssid = "2">We first detail our baseline systems used for performance comparison, and defined evaluation measures specific to related work summary evaluation.</S>
			<S sid ="157" ssid = "3">In our evaluation, we use our manually- compiled RWSData data set.</S>
			<S sid ="158" ssid = "4">We benchmark ReWoS against two baseline systems: LEAD and MEAD.</S>
			<S sid ="159" ssid = "5">The LEAD baseline represents each of the cited article with an equal number of sentences.</S>
			<S sid ="160" ssid = "6">The first n sentences are drawn from the article, meaning that the title and abstract are usually extracted.</S>
			<S sid ="161" ssid = "7">The order of the article leads used in the resulting summary was determined by the order of articles to be processed.</S>
			<S sid ="162" ssid = "8">MEAD is a well-documented baseline extractive multi-document summarizer, developed in (Radev et al., 2004).</S>
			<S sid ="163" ssid = "9">MEAD offers a set of different features that can be parameterized to create resulting summaries.</S>
			<S sid ="164" ssid = "10">We conducted an internal tuning of MEAD to maximize its performance on the RWSData.</S>
			<S sid ="165" ssid = "11">The optimal configuation uses just two tuned features of centroid and cosine similarity.</S>
			<S sid ="166" ssid = "12">Note that the MEAD baseline does use the topic tree keywords in computing cosine similarity score.</S>
			<S sid ="167" ssid = "13">Our ReWoS system is the only system that leverages the topic tree structure which is central to our approach.</S>
			<S sid ="168" ssid = "14">In our experiments, we used MEAD toolkit4 to produce the summaries for LEAD and MEAD baseline systems.</S>
			<S sid ="169" ssid = "15">Automatic evaluation was performed with ROUGE (Lin, 2004), a widely used and recognized automated summarization evaluation method.</S>
			<S sid ="170" ssid = "16">We employed a number of ROUGE variants, which have been proven to correlate with human judgments in multi-document summarization (Lin, 2004).</S>
			<S sid ="171" ssid = "17">However, given the small size of our summarization dataset, we can only draw notional 4http://www.summarization.com/mead/ evidence from such an evaluation; it is not possible to find statistically significant conclusion from our evaluation.</S>
			<S sid ="172" ssid = "18">To partially address this, we also conducted a human evaluation to assess more fine-grained qualities of our system.</S>
			<S sid ="173" ssid = "19">We asked 11 human judges to follow an evaluation guideline that we prepared, to evaluate the summary quality, consisting of the following evaluation measures: • Correctness: Is the summary content actually relevant to the hierarchical topics given?</S>
			<S sid ="174" ssid = "20">• Novelty: Does the summary introduce novel information that is significant in comparison with the human created summary?</S>
			<S sid ="175" ssid = "21">• Fluency: Does the summary’s exposition flow well, in terms of syntax as well as discourse?</S>
			<S sid ="176" ssid = "22">• Usefulness: Is the summary useful in supporting the researchers to quickly grasp the related works given hierarchical topics?</S>
			<S sid ="177" ssid = "23">Each judge was asked to grade the four summaries according to the measures on a 5-point scale of 1 (very poor) to 5 (very good).</S>
			<S sid ="178" ssid = "24">Summaries 1 and 2 come from LEAD-based and MEAD systems, respectively.</S>
			<S sid ="179" ssid = "25">Summaries 3 and 4 come from our proposed ReWoS systems, without (ReWoS−WCM) and with (ReWoS−CM) the context modeling in SCSum.</S>
			<S sid ="180" ssid = "26">All summarizers were set to yield a summary with the same length (1% of the original relevant articles, measured in sentences).</S>
			<S sid ="181" ssid = "27">Due to limited time, only 10 out of 20 evaluation sets were assessed by the evaluators.</S>
			<S sid ="182" ssid = "28">Each set was graded at least 3 times by 3 different evaluators; evaluators did not know the identities of the systems, which were randomized for each set examined.</S>
	</SECTION>
	<SECTION title="Results. " number = "6">
			<S sid ="183" ssid = "1">ROUGE results are summarized in Table 2.</S>
			<S sid ="184" ssid = "2">Surprisingly, the MEAD baseline system outperforms both LEAD baseline and ReWoS– WCM (without context modeling).</S>
			<S sid ="185" ssid = "3">Only ReWoS–CM (with context modeling) is significantly better than others, in terms of all ROUGE variants.</S>
			<S sid ="186" ssid = "4">Here are some possible reasons to explain this.</S>
			<S sid ="187" ssid = "5">First, ROUGE evaluation seems to work unreasonably when dealing with verbose summaries, often produced by MEAD.</S>
			<S sid ="188" ssid = "6">Second, related work summaries are multi-topic summaries of multi-article references.</S>
			<S sid ="189" ssid = "7">This may cause miscalculation from overlapping n-grams that occur across multiple topics or references.</S>
			<S sid ="190" ssid = "8">Since automatic evaluation with ROUGE does not allow much introspection, we turn to our human evaluation.</S>
			<S sid ="191" ssid = "9">Results are also summarized in Table 2.</S>
			<S sid ="192" ssid = "10">They show that both ReWoS–WCM and ReWoS–CM perform significantly better than baselines in terms of correctness, novelty, and usefulness.</S>
			<S sid ="193" ssid = "11">This is because our system utilized features developed specifically for related work sum- marization.</S>
			<S sid ="194" ssid = "12">Also, our proposed systems compare favorably with LEAD, showing that necessary information is not only located in titles or abstracts, but also in relevant portions of the research article body.</S>
			<S sid ="195" ssid = "13">ReWoS–CM (with context modeling) performed equivalent to ReWoS–WCM (without it) in terms of correctness and usefulness.</S>
			<S sid ="196" ssid = "14">For novelty, ReWoS–CM is better than ReWoS–WCM.</S>
			<S sid ="197" ssid = "15">It proved that the proposed component of context moding is useful in providing new information that is necessary for the related work summaries.</S>
			<S sid ="198" ssid = "16">For fluency, only ReWoS–CM is better than baseline systems.</S>
			<S sid ="199" ssid = "17">This is a negative result, but is not surprising because the summaries from the ReWoS–CM which uses context modeling seems to be longer than others.</S>
			<S sid ="200" ssid = "18">It makes the summaries quite hard to digest; some evaluators told us that they preferred the shorter summaries.</S>
			<S sid ="201" ssid = "19">A future extension is that using information fusion techniques to fuse the contextual sentences with its anchor agentive sentence.</S>
			<S sid ="202" ssid = "20">A detailed error analysis of the results revealed that there are three main types of errors produced by our systems.</S>
			<S sid ="203" ssid = "21">The first issue is in calculating topic relevance.</S>
			<S sid ="204" ssid = "22">In the context of related work summarization, our heuristics-based strategies for sentence extraction cannot capture fully this issue.</S>
			<S sid ="205" ssid = "23">Some sentences that have high relevant scores to topics are not actually semantically rele vant to the topics.</S>
			<S sid ="206" ssid = "24">The second issue of anaphoric expression is more addressable.</S>
			<S sid ="207" ssid = "25">Some extracted sentences still contain anaphoric expression (e.g., “they”, “these”, “such”, . . .</S>
			<S sid ="208" ssid = "26">), making final generated summaries incoherent.</S>
			<S sid ="209" ssid = "27">The third issue is paraphrasing, where substituted paraphrases replace the original words and phrases in the source articles.</S>
	</SECTION>
	<SECTION title="Conclusion and Future Work. " number = "7">
			<S sid ="210" ssid = "1">According to the best of our knowledge, automated related work summarization has not been studied before.</S>
			<S sid ="211" ssid = "2">In this paper, we have taken the initial steps towards solving this problem, by dividing the task into general and specific summarization processes.</S>
			<S sid ="212" ssid = "3">Our initial results show an improvement over generic multi document summarization baselines in human evaluation.</S>
			<S sid ="213" ssid = "4">However, our work shows that there is much room for additional improvement, for which we have outlined a few challenges.</S>
			<S sid ="214" ssid = "5">A shortcoming of our current work is that we assume that a topic hierarchy tree is given as input.</S>
			<S sid ="215" ssid = "6">We feel that this is an acceptable limitation because we feel existing techniques will be able to create such input, and that the topic trees used in this study were quite simple.</S>
			<S sid ="216" ssid = "7">We plan to validate this by generating these topic trees automatically in our future work.</S>
			<S sid ="217" ssid = "8">Exploring related work summarization comes at a timely moment, as scholars now have access to a preponderous amount of scholarly literature.</S>
			<S sid ="218" ssid = "9">Automated assistance in interpreting and organizing scholarly work will help build future applications for integration with digital libraries and reference management tools.</S>
	</SECTION>
</PAPER>
