Proceedings of the...
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 81–84,New York, June 2006. c©2006 Association for Computational Linguistics
Initial Study on Automatic Identification of Speaker Role in Broadcast NewsSpeech
Yang LiuUniversity of Texas at Dallas, Richardson, TX
yangl@hlt.utdallas.edu
Abstract
Identifying a speaker’s role (anchor, reporter,or guest speaker) is important for findingthe structural information in broadcast newsspeech. We present an HMM-based approachand a maximum entropy model for speakerrole labeling using Mandarin broadcast newsspeech. The algorithms achieve classificationaccuracy of about 80% (compared to the base-line of around 50%) using the human tran-scriptions and manually labeled speaker turns.We found that the maximum entropy modelperforms slightly better than the HMM, andthat the combination of them outperforms anymodel alone. The impact of the contextual roleinformation is also examined in this study.
1 IntroductionMore effective information access is beneficial to dealwith the increasing amount of broadcast news speech.Many attempts have been made in the past decade to buildnews browser, spoken document retrieval system, andsummarization or question answering system to effec-tively handle the large volume of news broadcast speech(e.g., the recent DARPA GALE program). Structural in-formation, such as story segmentation or speaker cluster-ing, is critical for all of these applications. In this paper,we investigate automatic identification of the speakers’roles in broadcast news speech. A speaker’s role (suchas anchor, reporter or journalist, interviewee, or somesoundbites) can provide useful structural information ofbroadcast news. For example, anchors appear through theentire program and generally introduce news stories. Re-porters typically report a specific news story, in whichthere may be other guest speakers. The transition be-tween anchors and reporters is usually a good indicatorof story structure. Speaker role information was shownto be useful for summarizing broadcast news (Maskeyand Hirschberg, 2003). Anchor information has also beenused for video segmentation, such as the systems in theTRECVID evaluations.1
1See http://www-nlpir.nist.gov/projects/trecvid/ for more in-formation on video retrieval evaluations.
In this paper, we develop algorithms for speaker roleidentification in broadcast news speech. Human tran-scription and manual speaker turn labels are used in thisinitial study. The task is then to classify each speaker’sturn as anchor, reporter, or other. We use about 170hours of speech for training and testing. Two approachesare evaluated, an HMM and a maximum entropy classi-fier. Our methods achieve about 80% accuracy for thethree-way classification task, compared to around 50%when every speaker is labeled with the majority class la-bel, i.e., anchor.2
The rest of the paper is organized as follows. Relatedwork is introduced in Section 2. We describe our ap-proaches in Section 3. Experimental setup and results arepresented in Section 4. Summary and future work appearin Section 5.
2 Related WorkThe most related previous work is (Barzilay et al., 2000),in which Barzilay et al. used BoosTexter and the max-imum entropy model to classify each speaker’s role inan English broadcast news corpus. Three classes areused, anchor, journalist, and guest speaker, which arevery similar to the role categories in our study. Lexicalfeatures (key words), context features, duration, and ex-plicit speaker introduction are used as features. For thethree-way classification task, they reported accuracy ofabout 80% compared to the chance of 35%. They have in-vestigated using both the reference transcripts and speechrecognition output. Our study differs from theirs in thatwe use one generative modeling approach (HMM), aswell as the conditional maximum entropy method. Wealso evaluate the contextual role information for classifi-cation. In addition, our experiments are conducted usinga different language, Mandarin broadcast news. Theremay be inherent difference across languages and newssources.
Another task related to our study is anchor segmen-tation. Huang et al. (Huang et al., 1999) used a recog-nition model for a particular anchor and a backgroundmodel to identify anchor segments. They reported verypromising results for the task of determining whether
2Even though this is a baseline (or chance performance), itis not very meaningful since there is no information provided inthis output.
81
or not a particular anchor is talking. However, thismethod is not generalizable to multiple anchors, nor isit to reporters or other guest speakers. Speaker roledetection is also related to speaker segmentation andclustering (also called speaker diarization), which was abenchmark test in the NIST Rich Transcription evalua-tions in the past few years (for example, NIST RT-04Fhttp://www.nist.gov/speech/tests/rt/rt2004/fall/). Most ofthe speaker diarization systems only use acoustic infor-mation; however, in recent studies textual sources havealso been utilized to help improve speaker clustering re-sults, such as (Canseco et al., 2005). The goal of speakerdiarization is to identify speaker change and group thesame speakers together. It is different from our task sincewe determine the role of a speaker rather than speakeridentity. In this initial study, instead of using automaticspeaker segmentation and clustering results, we use themanual speaker segments but without any speaker iden-tity information.
3 Speaker Role Identification Approaches3.1 Hidden Markov Model (HMM)
anchor
reporterother
Sentence 1Sentence 2Sentence 3
…………
Sentence 1Sentence 2
…………
Sentence 1Sentence 2Sentence 3
…………
Figure 1: A graphical representation of the HMM ap-proach for speaker role labeling. This is a simple firstorder HMM.
The HMM has been widely used in many tagging prob-lems. Stolcke et al. (Stolcke et al., 2000) used it for dialogact classification, where each utterance (or dialog act) isused as the observation. In speaker role detection, the ob-servation is composed of a much longer word sequence,i.e., the entire speech from one speaker. Figure 1 showsthe graphical representation of the HMM for speaker roleidentification, in which the states are the speaker roles,and the observation associated with a state consists of theutterances from a speaker. The most likely role sequenceRˆ is:
Rˆ = argmaxR
P (R|O) = argmaxR
P (O|R)P (R), (1)
where O is the observation sequence, in which Oi corre-sponds to one speaker turn. If we assume what a speakersays is only dependent on his or her role, then:
P (O|R) =?i
P (Oi|Ri). (2)
From the labeled training set, we train a languagemodel (LM), which provides the transition probabilitiesin the HMM, i.e., the P (R) term in Equation (1). The vo-cabulary in this role LM (or role grammar) consists of dif-ferent role tags. All the sentences belonging to the samerole are put together to train a role specific word-based N-gram LM. During testing, to obtain the observation prob-abilities in the HMM, P (Oi|Ri), each role specific LMis used to calculate the perplexity of those sentences cor-responding to a test speaker turn.
The graph in Figure 1 is a first-order HMM, in whichthe role state is only dependent on the previous state.In order to capture longer dependency relationship, weused a 6-gram LM for the role LM. For each role spe-cific word-based LM, 4-gram is used with Kneser-Neysmoothing. There is a weighting factor when combin-ing the state transitions and the observation probabilitieswith the best weights tuned on the development set (6 forthe transition probabilities in our experiments). In addi-tion, in stead of using Viterbi decoding, we used forward-backward decoding in order to find the most likely roletag for each segment. Finally we may use only a subsetof the sentences in a speaker’s turn, which are possiblymore discriminative to determine the speaker’s role. TheLM training and testing and HMM decoding are imple-mented using the SRILM toolkit (Stolcke, 2002).3.2 Maximum Entropy (Maxent) ClassifierA Maxent model estimates the conditional probability:
P (Ri|O) = 1Z?(O)
exp(?k
?kgk(Ri, O)), (3)
where Z?(O) is the normalization term, functionsgk(Ri, O) are indicator functions weighted by ?, and k isused to indicate different ‘features’. The weights (?) areobtained to maximize the conditional likelihood of thetraining data, or in other words, maximize the entropywhile satisfying all the constraints. Gaussian smoothing(variance=1) is used to avoid overfitting. In our experi-ments we used an existing Maxent toolkit (available fromhttp://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html).
The following features are used in the Maxent model:• bigram and trigram of the words in the first and the
last sentence of the current speaker turn
• bigram and trigram of the words in the last sentenceof the previous turn
82
• bigram and trigram of the words in the first sentenceof the following turn
Our hypothesis is that the first and the last sentence froma speaker’s turn are more indicative of the speaker’s role(e.g., self introduction and closing). Similarly the lastsentence from the previous speaker segment and the firstsentence of the following speaker turn also capture thespeaker transition information. Even though sentencesfrom the other speakers are included as features, the Max-ent model makes a decision for each test speaker turn in-dividually without considering the other segments. Theimpact of the contextual role tags will be evaluated in ourexperiments.
4 Experiments4.1 Experimental SetupWe used the TDT4 Mandarin broadcast news data in thisstudy. The data set consists of about 170 hours (336shows) of news speech from different sources. In theoriginal transcripts provided by LDC, stories are seg-mented; however, speaker information (segmentation oridentity) is not provided. Using the reference transcriptsand the audio files, we manually labeled the data withspeaker turns and the role tag for each turn.3 Speakersegmentation is generally very reliable; however, the roleannotation is ambiguous in some cases. The interanno-tator agreement will be evaluated in our future work. Inthis initial study, we just treat the data as noisy data.
We preprocessed the transcriptions by removing somebad codes and also did text normalization. We used punc-tuation (period, question mark, and exclamation) avail-able from the transcriptions (though not very accurate)to generate sentences, and a left-to-right longest wordmatch approach to segment sentences into words. Thesewords/sentences are then used for feature extraction inthe Maxent model, and LM training and perplexity cal-culation in the HMM as described in Section 3. Notethat the word segmentation approach we used may notbe the-state-of-art, which might have some effect on ourexperiments.
10-fold cross validation is used in our experiments.The entire data set is split into ten subsets. Each timeone subset is used as the test set, another one is used asthe development set, and the rest are used for training.The average number of segments (i.e., speaker turns) inthe ten subsets is 1591, among which 50.8% are anchors.Parameters (e.g., weighting factor) are tuned based on theaverage performance over the ten development sets, andthe same weights are applied to all the splits during test-ing.
3The labeling guideline can be found fromhttp://www.hlt.utdallas.edu/˜yangl/spkr-label/. It was modifiedbased on the annotation manual used for English at ColumbiaUniversity (available from http://www1.cs.columbia.edu/˜smaskey/labeling/Labeling Manual v 2 1.pdf).
4.2 ResultsA HMM and Maxent: Table 1 shows the role iden-
tification results using the HMM and the Maxentmodel, including the overall classification accuracyand the precision/recall rate (%) for each role. Theseresults are the average over the 10 test sets.
HMM Maxentprecision recall precision recall
anchor 78.03 87.33 80.29 87.23reporter 78.54 66.42 73.34 77.01
other 83.05 68.19 89.52 41.30Accuracy (%) 77.18 77.42
Table 1: Automatic role labeling results (%) using theHMM and Maxent classifiers.
From Table 1 we find that the overall classificationperformance is similar when using the HMM andthe Maxent model; however, their error patterns arequite different. For example, the Maxent model isbetter than the HMM at identifying “reporter” role,but worse at identifying “other” speakers (see the re-call rate shown in the table). In the HMM, we onlyused the first and the last sentence in a speaker’sturn, which are more indicative of the speaker’s role.We observed significant performance degradation,that is, 74.68% when using all the sentences forLM training and perplexity calculation, comparedto 77.18% as shown in the table using a subset ofa speaker’s speech. Note that the sentences used inthe HMM and Maxent models are the same; how-ever, the Maxent does not use any contextual roletags (which we will examine next), although it doesinclude some words from the previous and the fol-lowing speaker segments in its feature set.
B Contextual role information: In order to investi-gate how important the role sequence is, we con-ducted two experiments for the Maxent model. Inthe first experiment, for each segment, the referencerole tag of the previous and the following segmentsand the combination of them are included as featuresfor model training and testing (a “cheating” exper-iment). In the second experiment, a two-step ap-proach is employed. Following the HMM and Max-ent experiments (i.e., results as shown in Table 1),Viterbi decoding is performed using the posteriorprobabilities from the Maxent model and the tran-sition probabilities from the role LM as in the HMM(with weight 0.3). The average performance over theten test sets is shown in Table 2 for these two exper-iments. For comparison, we also present the decod-ing results of the HMM with and without using se-quence information (i.e., the transition probabilitiesin the HMM). Additionally, the system combination
83
results of the HMM and Maxent are presented in thetable, with more discussion on this later. We observefrom Table 2 that adding contextual role informa-tion improves performance. Including the two refer-ence role tags yields significant gain in the Maxentmodel, even though some sentences from the previ-ous and the following segments are already includedas features. The HMM suffers more than the Max-ent classifier when role sequence information is notused during decoding, since that is the only contex-tual information used in the HMM, unlike the Max-ent model, which uses features extracted from theneighboring speaker turns.
Accuracy (%)0: Maxent (as in Table 1) 77.421: Maxent + 2 reference tags 80.902: Maxent + sequence decoding 78.593: HMM (as in Table 1) 77.184: HMM w/o sequence 73.30Maxent (0) + HMM (3) 79.74Maxent (2) + HMM (3) 81.97
Table 2: Impact of role sequence information on theHMM and Maxent classifiers. The combination resultsof the HMM and Maxent are also provided.
C System combination: For system combination, weused two different Maxent results: with and with-out the Viterbi sequence decoding, corresponding toexperiments (0) and (2) as shown in Table 2 respec-tively. When combining the HMM and Maxent, i.e.,the last two rows in Table 2, the posterior probabili-ties from them are linearly weighted (weight 0.6 forthe Maxent in the upper one, and 0.7 for the Max-ent in the bottom one). The combination of the twoapproaches yields better performance than any sin-gle model in the two cases. We also investigatedother system combination approaches. For example,a decision tree or SVM that builds a 3-way super-classifier using the posterior probabilities from theHMM and Maxent. However, so far we have notfound any gain from more complicated system com-bination than a simple linear interpolation. We willstudy this in our future work.
5 Summary and Future WorkIn this paper we have reported an initial study of speakerrole identification in Mandarin broadcast news speech us-ing the HMM and Maxent tagging approaches. We findthat the conditional Maxent generally performs slightlybetter than the HMM, and that their combination out-performs each model alone. The HMM and the Max-ent model show differences in identifying different roles.The impact of contextual role information is also exam-
ined for the two approaches, and a significant gain is ob-served when contextual information is modeled. We findthat the beginning and the end sentences in a speaker’sturn are good cues for role identification. The overallclassification performance in this study is similar to thatreported in (Barzilay et al., 2000); however, the chanceperformance is quite different (35% in that study). It isnot clear yet whether it is because of the difference acrossthe two corpora or languages.
The Maxent model provides a convenient way to in-corporate various knowledge sources. We will investi-gate other features to improve the classification results,such as name information, acoustic or prosodic features,and speaker clustering results (considering that the samespeaker typically has the same role tag). We plan toexamine the effect of using speech recognition output,as well as automatic speaker segmentation and cluster-ing results. Analysis of difference news sources mayalso reveal some interesting findings. Since our workinghypothesis is that speaker role information is importantto find structure in broadcast news, we will investigatewhether and how speaker role relates to downstream lan-guage processing applications, such as summarization orquestion answering.
AcknowledgmentThe author thanks Julia Hirschberg and Sameer Maskey atColumbia University and Mari Ostendorf at the University ofWashington for the useful discussions, and Mei-Yuh Hwangfor helping with Mandarin word segmentation and text normal-ization. This material is based upon work supported by theDefense Advanced Research Projects Agency (DARPA) underContract No. HR0011-06-C-0023. Any opinions, findings andconclusions or recommendations expressed in this material arethose of the author(s) and do not necessarily reflect the views ofDARPA.
ReferencesR. Barzilay, M. Collins, J. Hirschberg, and S. Whittaker. 2000.
The rules behind roles: Identifying speaker role in radiobroadcasts. In Proc. of AAAI.
L. Canseco, L. Lamel, and J Gauvain. 2005. A comparativestudy using manual and automatic transcription for diariza-tion. In Proc. of ASRU.
Q. Huang, Z. Liu, A. Rosenberg, D. Gibbon, and B. Shahraray.1999. Automated generation of news content hierarchy byintegrating audio, video, and text information. In Proc. ofICASSP, pages 3025–3028.
S. Maskey and J. Hirschberg. 2003. Automatic summarizationof broadcast news using structural features. In Eurospeech.
A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates,D. Jurasky, P. Taylor, R. Martin, C.V. Ess-Dykema, andM. Meteer. 2000. Dialogue act modeling for automatic tag-ging and recognition of conversational speech. Computa-tional Linguistics, 26(3):339–373.
A. Stolcke. 2002. SRILM – An extensible language modelingtoolkit. In Proc. of ICSLP, pages 901–904.
84
81 




