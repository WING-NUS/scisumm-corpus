Impact of ASR N-Best Information on Bayesian Dialogue Act Recognition
Proceedings of the SIGDIAL 2013 Conference, pages 314–318,Metz, France, 22-24 August 2013. c©2013 Association for Computational Linguistics
Impact of ASR N-Best Information on Bayesian Dialogue Act Recognition
Heriberto Cuaya´huitl, Nina Dethlefs, Helen Hastie, Oliver LemonSchool of Mathematical and Computer Sciences,
Heriot-Watt University, Edinburgh, UK{h.cuayahuitl,n.s.dethlefs,h.hastie,o.lemon}@hw.ac.uk
AbstractA challenge in dialogue act recognitionis the mapping from noisy user inputs todialogue acts. In this paper we describean approach for re-ranking dialogue acthypotheses based on Bayesian classifiersthat incorporate dialogue history and Au-tomatic Speech Recognition (ASR) N-bestinformation. We report results based onthe Let’s Go dialogue corpora that show(1) that including ASR N-best informationresults in improved dialogue act recogni-tion performance (+7% accuracy), and (2)that competitive results can be obtainedfrom as early as the first system dialogueact, reducing the need to wait for subse-quent system dialogue acts.
1 Introduction
The primary challenge of a Dialogue Act Recog-niser (DAR) is to find the correct mapping be-tween a noisy user input and its true dialogueact. In standard “slot-filling” dialogue sys-tems a dialogue act is generally represented asDialogueActType(attribute-value pairs), see Sec-tion 3. While a substantial body of research hasinvestigated different types of models and meth-ods for dialogue act recognition in spoken dia-logue systems (see Section 2), here we focus onre-ranking the outputs of an existing DAR for eval-uation purposes. In practice the re-ranker shouldbe part of the DAR itself. We propose to use mul-tiple Bayesian classifiers to re-rank an initial setof dialogue act hypotheses based on informationfrom the dialogue history as well as ASR N-bestlists. In particular the latter type of informationhelps us to learn mappings between dialogue actsand common mis-recognitions. We present exper-imental results based on the Let’s Go dialogue cor-pora which indicate that re-ranking hypotheses us-ing ASR N-best information can lead to improved
recognition. In addition, we compare the recogni-tion accuracy over time and find that high accuracycan be obtained with as little context as one systemdialogue act, so that there is often no need to takea larger context into account.
2 Related Work
Approaches to dialogue act recognition from spo-ken input have explored a wide range of meth-ods. (Stolcke et al., 2000) use HMMs for dialoguemodelling, where sequences of observations cor-respond to sequences of dialogue act types. Theyalso explore the performance with decision treesand neural networks and report their highest ac-curacy at 65% on the Switchboard corpus. (Zim-mermann et al., 2005) also use HMMs in a jointsegmentation and classification model. (Grau etal., 2004) use a combination of Naive Bayes andn-grams with different smoothing methods. Theirbest models achieve an accuracy of 66% on En-glish Switchboard data and 89% on a Spanish cor-pus. (Sridhar et al., 2009; Wright et al., 1999)both use a maximum entropy classifier with n-grams to classify dialogue acts using prosodic fea-tures. (Sridhar et al., 2009) report an accuracy ofup to 74% on Switchboard data and (Wright et al.,1999) report an accuracy of 69% on the DCIEMMaptask Corpus. (Bohus and Rudnicky, 2006)maintain an N-best list of slot values using logis-tic regression. (Surendran and Levow, 2006) usea combination of linear support vector machines(SVMs) and HMMs. They report an accuracy of65.5% on the HCRC MapTask corpus and con-clude that SVMs are well suited for sparse text anddense acoustic features. (Gamba¨ck et al., 2011)use SVMs within an active learning framework.They show that while passive learning achieves anaccuracy of 77.8% on Switchboard data, the ac-tive learner achieves up to 80.7%. (Henderson etal., 2012) use SVMs for dialogue act recognitionfrom ASR word confusion networks.
314
Speech 
Recognizer
Dialogue Act 
RecognizerDialogue Act 
Re-Ranker
n-best 
list
n-best 
list
n-best 
list
(e.g. Let's Go parser)
speech
scored re-scored
  Following 
Components
Figure 1: Pipeline architecture for dialogue act recognition and re-ranking component. Here, the inputis a list of dialogue acts with confidence scores, and the output is the same list of dialogue acts but withrecomputed confidence scores. A dialogue act is represented as DialogueActType(attribute-value pairs).
Several authors have presented evidence infavour of Bayesian methods. (Keizer and op denAkker, 2007) have shown that Bayesian DARscan outperform baseline classifiers such as deci-sion trees. More generally, (Ng and Jordan, 2001)show that generative classifiers (e.g. Naive Bayes)reach their asymptotic error faster than discrimina-tive ones. As a consequence, generative classifiersare less data intensive than discriminative ones.
In addition, several authors have investigateddialogue belief tracking. While our approachis related to belief tracking, we focus here onspoken language understanding under uncertaintyrather than estimating user goals. (Williams, 2007;Thomson et al., 2008) use approximate inferenceto improve the scalability of Bayes nets for be-lief tracking and (Lison, 2012) presents work onimproving their scalability through abstraction.(Mehta et al., 2010) model user intentions throughthe use of probabilistic ontology trees.
Bayes nets have also been applied to otherdialogue-related tasks, such as surface realisa-tion within dialogue (Dethlefs and Cuaya´huitl,2011) or multi-modal dialogue act recognition(Cuaya´huitl and Kruijff-Korbayova´, 2011). In thefollowing, we will explore a dialogue act recogni-tion technique based on multiple Bayesian classi-fiers and show that re-ranking with ASR N-best in-formation can improve recognition performance.
3 Re-Ranking Dialogue Acts UsingMultiple Bayesian Networks
Figure 1 shows an illustration of our dialogue actre-ranker within a pipeline architecture. Here, pro-cessing begins with the user’s speech being inter-preted by a speech recogniser, which produces afirst N-best list of hypotheses. These hypothesesare subsequently passed on and interpreted by adialogue act recogniser, which in our case is rep-resented by the Let’s Go parser. The parser pro-duces a first set of dialogue act hypotheses, basedon which our re-ranker becomes active. A full
dialogue act in our scenario consists of three el-ements: dialogue act types, attributes (or slots),and slot values. An example dialogue act is in-form(from=Pittsburgh Downtown). The dialogueact re-ranker thus receives a list of hypothesesin the specified form (triples) from its precedingmodule (a DAR or in our case the Let’s Go parser)and its task is to generate confidence scores thatapproximate true label (i.e. the dialogue act reallyspoken by a user) as closely as possible.
We address this task by using multiple Bayesianclassifiers: one for classifying a dialogue act type,one for classifying a set of slots, and the rest forclassifying slot values. The use of multiple classi-fiers is beneficial for scalability purposes; for ex-ample, assuming 10 dialogue act types, 10 slots,10 values per slot, and no other dialogue con-text results in a joint distribution of 1011 parame-ters. Since a typical dialogue system is required tomodel even larger joint distributions, our adoptedapproach is to factorize them into multiple inde-pendent Bayesian networks (with combined out-puts). A multiple classifier system is a power-ful solution to complex classification problems in-volving a large set of inputs and outputs. Thisapproach not only decreases training time but hasalso been shown to increase the performance ofclassification (Tax et al., 2000).
A Bayesian Network (BN) models a joint prob-ability distribution over a set of random variablesand their dependencies, see (Bishop, 2006) foran introduction to BNs. Our motivation for us-ing multiple BNs is to incorporate a fairly rich di-alogue context in terms of what the system anduser said at lexical and semantic levels. In con-trast, using a single BN for all slots with rich di-alogue context faces scalability issues, especiallyfor slots with large numbers of domain values,and is therefore not an attractive option. Wedenote our set of Bayesian classifiers as ? ={?dat, ?att, ..., ?val(i)}, where BN ?dat is used torank dialogue act types, BN ?att is used to rankattributes, and the other BNs (?val(i)) are used to
315
rank values for each slot i. The score of a userdialogue act (< d, a, v >) is computed as:
P (d, a, v) =1
Z
?P (d|pad)P (a|paa)P (v|pav),
where d is a dialogue act type, a is an attribute(or slot), v is a slot value, pax is a parent randomvariable, andZ is a normalising constant. This im-plies that the score of a dialogue act is the productof probabilities of dialogue act type and slot-valuepairs. For dialogue acts including multiple slot-value pairs, the product above can be extended ac-cordingly. The best and highest ranked hypothesis(from space H) can be obtained according to:
< d, a, v >*= arg max<d,a,v>?H
P (d, a, v).
In the following, we describe our experimentalsetting. Here, the structure and parameters of ourclassifiers will be estimated from a corpus of spo-ken dialogues, and we will use the equations abovefor re-ranking user dialogue acts. Finally, we re-port results comparing Bayesian classifiers thatmake use of ASR N-best information and dialoguecontext against Bayesian classifiers that make pre-dictions based on the dialogue context alone.
4 Experiments and Results
4.1 DataOur experiments are based on the Let’s Go corpus(Raux et al., 2005). Let’s Go contains recorded in-teractions between a spoken dialogue system andhuman users who make enquiries about the busschedule in Pittsburgh. Dialogues are driven bysystem-initiative and query the user sequentiallyfor five slots: an optional bus route, a departureplace, a destination, a desired travel date, and adesired travel time. Each slot needs to be explic-itly (or implicity) confirmed by the user. Our anal-yses are based on a subset of this data set contain-ing 779 dialogues with 7275 turns, collected in theSummer of 2010. From these dialogues, we used70% for training our classifiers and the rest fortesting (with 100 random splits). Briefly, this dataset contains 12 system dialogue act types1, 11 userdialogue act types2, and 5 main slots with varia-tions3. The number of slot values ranges between
1ack, cant help, example, expl conf, go back, hello,impl conf, more buses, request, restart, schedule, sorry.
2affirm, bye, go back, inform, negate, next bus, prevbus,repeat, restart, silence, tellchoices.
3date.absday, date.abmonth, date.day, date.relweek, from,route, time.ampm, time.arriveleave, time.hour, time.minute,time.rel, to.
*
Figure 2: Bayesian network for probabilistic rea-soning of locations (variable “from desc”), whichincorporates ASR N-best information in the vari-able“from desc nbest” and dialogue history in-formation in the remaining random variables.
102 and 103 so that the combination of all possi-ble dialogue act types, attributes and values leadsto large amounts of triplets. While the majorityof user inputs contain one user dialogue act, theaverage number of system dialogue acts per turnis 4.2. Note that for the user dialogue act types,we also model silence explicitly. This is often notconsidered in dialogue act recognisers: since theASR will always try to recognise something outof any input (even background noise), typical dia-logue act recognisers will then try to map the ASRoutput onto a semantic interpretation.
4.2 Bayesian NetworksWe trained our Bayesian networks in a supervisedlearning manner and used 43 discrete features (orrandom variables) plus a class label (also discrete).The feature set is described by three main subsets:25 system-utterance-level binary features4 derivedfrom the system dialogue act(s) in the last turn; 17user-utterance-level binary features5 derived from(a) what the user heard prior to the current turn,or (b) what keywords the system recognised in its
4System utterance features: heardAck, heardCantHelp,heardExample, heardExplConf, heardGoBackDAT, heard-Hello, heardImplConf, heardMoreBuses, heardRequest,heardRestartDAT, heardSchedule, heardSorry, heardDate,heardFrom, heardRoute, heardTime, heardTo, heardNext,heardPrevious, heardGoBack, heardChoices, heardRestart,heardRepeat, heardDontKnow, lastSystemDialActType.
5User utterance features: hasRoute, hasFrom, hasTo, has-Date, hasTime, hasYes, hasNo, hasNext, hasPrevious, has-GoBack, hasChoices, hasRestart, hasRepeat, hasDontKnow,hasBye, hasNothing, duration in secs. (values=0,1,2,3,4,>5).
316
list of speech recognition hypotheses; and 1 word-level non-binary feature (* nbest) correspondingto the slot values in the ASR N-best lists.
Figure 2 shows the Bayes net corresponding tothe classifier used to rank location names. Therandom variable from desc is the class label, therandom variable from desc nbest (marked with anasterisk) incorporates slot values from the ASRN-best lists, and the remaining variables modeldialogue history context. The structure of ourBayesian classifiers were derived from the K2 al-gorithm6, and their parameters were derived frommaximum likelihood estimation. In addition, weperformed probabilistic inference using the Junc-tion tree algorithm7. Based on these data andtools, we trained 14 Bayesian classifiers: one forscoring dialogue act types, one for scoring at-tributes (slots), and the rest for scoring slot values.
4.3 Experimental ResultsWe compared 7 different dialogue act recognisersin terms of classification accuracy. The compar-ison was made against gold standard data froma human-labelled corpus. (Semi-Random) is arecogniser choosing a random dialogue act fromthe Let’s Go N-best parsing hypotheses. (Inci) isour proposed approach considering a context of isystem dialogue acts, and (Ceiling) is a recogniserchoosing the correct dialogue act from the Let’sGo N-best parsing hypotheses. The latter was usedas a gold standard from manual annotations, whichreflects the proportion of correct labels in the N-best parsing hypotheses.
We also assessed the impact of ASR N-best in-formation on probabilistic inference. To this end,we compared Bayes nets with a focus on the ran-dom variable “* nbest”, which in one case con-tains induced distributions from data and in theother case contains an equal distribution of slotvalues. Our hypothesis is that the former settingwill lead to better performance.
Figure 3 shows the classification accuracy ofour dialogue act recognisers. The first point to no-tice is that the incorporation of ASR N-best infor-mation makes an important difference. The per-formance of recogniser IncK (K being the num-ber of system dialogue acts) is 66.9% withoutASR N-best information and 73.9% with ASR N-best information (the difference is significant8 at
6www.cs.waikato.ac.nz/ml/weka/7www.cs.cmu.edu/˜javabayes/Home/8Based on a two-sided Wilcoxon Signed-Rank test.
Semi-Random Inc0 Inc1 Inc2 Inc3 IncK Ceiling40
45
50
55
60
65
70
75
80
85
90
Dialogue Act Recogniser
Clas
sifica
tion 
Accu
racy
 (%)
 
 
Without ASR N-Best InformationWith ASR N-Best Information
Figure 3: Bayesian dialogue act recognisers show-ing the impact of ASR N-best information.
p < 0.05). The latter represents a substantial im-provement over the semi-random baseline (62.9%)and Lets Go dialogue act recognizer (69%), bothsignificant at p < 0.05. A second point to notice isthat the differences between Inci (? i>0) recognis-ers were not significant. We can say that the use ofone system dialogue act as context is as competi-tive as using a larger set of system dialogue acts.This suggests that dialogue act recognition carriedout at early stages (e.g. after the first dialogue act)in an utterance does not degrade recognition per-formance. The effect is possibly domain-specificand generalisations remain to be investigated.
Generally, we were able to observe that morethan half of the errors made by the Bayesian clas-sifiers were due to noise in the environment andcaused by the users themselves, which interferedwith ASR results. Detecting when users do notconvey dialogue acts to the system is therefore stilla standing challenge for dialogue act recognition.
5 Conclusion and Future Work
We have described a re-ranking approach for userdialogue act recognition. Multiple Bayesian clas-sifiers are used to rank dialogue acts from a set ofdialogue history features and ASR N-best infor-mation. Applying our approach to the Let’s Godata we found the following: (1) that includingASR N-best information results in improved di-alogue act recognition performance; and (2) thatcompetitive results can be obtained from as earlyas the first system dialogue act, reducing the needto include subsequent ones.
Future work includes: (a) a comparison of our
317
Bayesian classifiers with other probabilistic mod-els and forms of training (for example by us-ing semi-supervised learning), (b) training dia-logue act recognisers in different (multi-modal andmulti-task) domains, and (c) dealing with randomvariables that contain very large domain values.
6 Acknowledgements
This research was funded by the EC FP7 pro-gramme under grant agreement no. 287615 (PAR-LANCE) and no. 270019 (SPACEBOOK).
Sample Re-Ranked User Inputs
User input: “forty six d”N-Best List of Dialogue Acts Let’s Go Score Bayesian Scoreinform(route=46a) 3.33E-4 1.9236763E-6inform(route=46b) 1.0E-6 1.5243509E-16inform(route=46d) 0.096107 7.030841E-4inform(route=46k) 0.843685 4.9941495E-10silence() NA 0
User input: “um jefferson hills to mckeesport”N-Best List of Dialogue Acts Let’s Go Score Bayesian Scoreinform(from=mill street) 7.8E-4 3.5998527E-16inform(from=mission street) 0.015577 3.5998527E-16inform(from=osceola street) 0.0037 3.5998527E-16inform(from=robinson township) 0.007292 3.5998527E-16inform(from=sheraden station) 0.001815 3.1346254E-8inform(from=brushton) 2.45E-4 3.5998527E-16inform(from=jefferson) 0.128727 0.0054255757inform(from=mckeesport) 0.31030 2.6209198E-4silence() NA 0
References[Bishop2006] Christopher M. Bishop. 2006. Pattern Recog-
nition and Machine Learning (Information Science andStatistics). Springer-Verlag New York, Inc., Secaucus, NJ,USA.
[Bohus and Rudnicky2006] D. Bohus and A. Rudnicky.2006. A k hypotheses + other” belief updating model. InAAAI Workshop on Statistical and Empirical Approachesto Spoken Dialogue Systems.
[Cuaya´huitl and Kruijff-Korbayova´2011] H. Cuaya´huitl andI. Kruijff-Korbayova´. 2011. Learning human-robot di-alogue policies combining speech and visual beliefs. InIWSDS, pages 133–140.
[Dethlefs and Cuaya´huitl2011] Nina Dethlefs and HeribertoCuaya´huitl. 2011. Combining Hierarchical Reinforce-ment Learning and Bayesian Networks for Natural Lan-guage Generation in Situated Dialogue. In ENLG, Nancy,France.
[Gamba¨ck et al.2011] Bjo¨rn Gamba¨ck, Fredrik Olsson, andOscar Ta¨ckstro¨m. 2011. Active Learning for DialogueAct Classification. In INTERSPEECH, pages 1329–1332.
[Grau et al.2004] Sergio Grau, Emilio Sanchis, Maria JoseCastro, and David Vilar. 2004. Dialogue Act Classifi-cation Using a Bayesian Approach. In SPECOM.
[Henderson et al.2012] Matthew Henderson, Milica Gasic,Blaise Thomson, Pirros Tsiakoulis, Kai Yu, and SteveYoung. 2012. Discriminative spoken language under-standing using word confusion networks. In SLT, pages176–181.
[Keizer and op den Akker2007] Simon Keizer and Rieksop den Akker. 2007. Dialogue Act Recognition UnderUncertainty Using Bayesian Networks. Natural LanguageEngineering, 13(4):287–316.
[Lison2012] Pierre Lison. 2012. Probabilistic dialogue mod-els with prior domain knowledge. In SIGDIAL Confer-ence, pages 179–188.
[Mehta et al.2010] Neville Mehta, Rakesh Gupta, AntoineRaux, Deepak Ramachandran, and Stefan Krawczyk.2010. Probabilistic ontology trees for belief tracking indialog systems. In SIGDIAL Conference, pages 37–46.
[Ng and Jordan2001] Andrew Y. Ng and Michael I. Jordan.2001. On discriminative vs. generative classifiers: A com-parison of logistic regression and naive bayes. In NIPS,pages 841–848.
[Raux et al.2005] Antoine Raux, Brian Langner, Dan Bohus,Alan W. Black, and Maxine Eskenazi. 2005. Let’sgo public! Taking a Spoken Dialog System to the RealWorld. In INTERSPEECH, pages 885–888.
[Sridhar et al.2009] Vivek Kumar Rangarajan Sridhar, Srini-vas Bangalore, and Shrikanth Narayanan. 2009. Com-bining Lexical, Syntactic and Prosodic Cues for ImprovedOnline Dialog Act Tagging. Computer Speech & Lan-guage, 23(4):407–422.
[Stolcke et al.2000] Andreas Stolcke, Klaus Ries, Noah Coc-caro, Elizabeth Shriberg, Rebecca A. Bates, Daniel Juraf-sky, Paul Taylor, Rachel Martin, Carol Van Ess-Dykema,and Marie Meteer. 2000. Dialog Act Modeling for Auto-matic Tagging and Recognition of Conversational Speech.Computational Linguistics, 26(3):339–373.
[Surendran and Levow2006] Dinoj Surendran and Gina-AnneLevow. 2006. Dialog Act Tagging with Support Vec-tor Machines and Hidden Markov Models. In INTER-SPEECH.
[Tax et al.2000] David M. Tax, Martijn van Breukelen,Robert P. Duin, and Josef Kittler. 2000. Combining mul-tiple classifiers by averaging or by multiplying? PatternRecognition, 33(9):1475–1485, September.
[Thomson et al.2008] Blaise Thomson, Jost Schatzmann, andSteve Young. 2008. Bayesian update of dialogue state forrobust dialogue systems. In ICASSP, pages 4937–4940.
[Williams2007] Jason D. Williams. 2007. Using particle fil-ters to track dialogue state. In ASRU, pages 502–507.
[Wright et al.1999] H. Wright, Massimo Poesio, and StephenIsard. 1999. Using high level dialogue information fordialogue act recognition using prosodic features. In Pro-ceedings of an ESCA Tutorial and Research Workshop onDialogue and Prosody, pages 139–143, Eindhoven, TheNetherlands.
[Zimmermann et al.2005] Matthias Zimmermann, Yang Liu,Elizabeth Shriberg, and Andreas Stolcke. 2005. TowardJoint Segmentation and Classification of Dialog Acts inMultiparty Meetings. In MLMI, pages 187–193.
318
