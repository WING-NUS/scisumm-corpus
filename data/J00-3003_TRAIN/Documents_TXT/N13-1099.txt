Improving the Quality of Minority Class Identification in Dialog Act Tagging
Proceedings of NAACL-HLT 2013, pages 802–807,Atlanta, Georgia, 9–14 June 2013. c©2013 Association for Computational Linguistics
Improving the Quality of Minority Class Identification in Dialog Act Tagging
Adinoyi Omuya10gen
New York, NY, USAwisdom@10gen.com
Vinodkumar PrabhakaranCS, Columbia University
New York, NY, USAvinod@cs.columbia.edu
Owen RambowCCLS, Columbia University
New York, NY, USArambow@ccls.columbia.edu
Abstract
We present a method of improving the perfor-mance of dialog act tagging in identifying mi-nority classes by using per-class feature opti-mization and a method of choosing the classbased not on confidence, but on a cascade ofclassifiers. We show that it gives a minor-ity class F-measure error reduction of 22.8%,while also reducing the error for other classesand the overall error by about 10%.
1 Introduction
In this paper, we discuss dialog act tagging, thetask of assigning a dialog act to an utterance, wherea dialog act (DA) is a high-level categorization ofthe pragmatic meaning of the utterance. Our data isemail. Our starting point is the tagger described in(Hu et al., 2009), which uses a standard multi-classclassifier based on support vector machines (SVMs).While the performance of this system is pretty goodas measured by accuracy, it performs badly on theDA REQUEST-ACTION, which is a rare class. Multi-class SVMs are typically implemented as a set ofSVMs, one per class, with the overall choice of classbeing determined by the SVM with the highest con-fidence (“one-against-all”). Multi-class SVMs aretypically packaged as a single system, whose innerworkings are ignored by the NLP researcher. In thispaper we show that, for our problem of DA classi-fication, we can boost the performance of the rareclasses (while maintaining the overall performance)by performing feature optimization separately foreach individual classifier. But we also show that we
can achieve an all-around error reduction by alter-ing the method by which the multi-class classifiercombines the individual SVMs. This new methodof combination is a simple cascade: we run the in-dividual classifiers in ascending order of frequencyof the classes in the training corpus; the first classi-fier to classify the data point positively determinesthe choice of the overall classifier. If no classifierclassifies the data point positively, we use the usualconfidence-based method. This new method obtainsa 22.8% error reduction for the minority class, andaround 10% error reduction for the other classes andfor the overall classifier.
This paper is structured as follows. We start outby discussing related work (Section 2). We thenpresent our data in Section 3, and in Section 4 wepresent the experiments with our systems and the re-sults. We report the results of an extrinsic evaluationin Section 5, and conclude.
2 Related Work
Dialog act (DA) annotations and tagging, inspiredby the speech act theory of Austin (1975) and Searle(1976), have been used in the NLP community to un-derstand and model dialog. Initial work was done onspoken interactions (see for example (Stolcke et al.,2000)). Recently, studies have explored dialog acttagging in written interactions such as emails (Co-hen et al., 2004), forums (Kim et al., 2006; Kim etal., 2010b), instant messaging (Kim et al., 2010a)and Twitter (Zhang et al., 2012). Most DA taggingsystems for written interactions use a message/postlevel tagging scheme, and allow multiple tags foreach message/post. In such a tagging scheme, indi-
802
vidual binary classifiers for each tag are independentof one another. However, recent studies have foundmerit in segmenting each message into functionalunits and assigning a single DA to each segment (Huet al., 2009). Our work falls in this paradigm (wechoose a single DA for smaller textual units). Webuild on the work by (Hu et al., 2009); we improvetheir dialog act predicting performance on minorityclasses using per-class feature optimization.
3 Data
In this study, we use the email corpus presented in(Hu et al., 2009), which is manually annotated forDA tags. The corpus contains 122 email threadswith a total of 360 messages and 20,740 word to-kens. This set of email threads is chosen from a ver-sion of the Enron email corpus with some missingmessages restored from other emails in which theywere quoted (Yeh and Harnly, 2006; Agarwal et al.,2012). Most emails are concerned with exchanginginformation, scheduling meetings, or solving prob-lems, but there are also purely social emails.
Dialog Act Tag Count (%)REQUEST-ACTION (R-A) 35 (2.5%)REQUEST-INFORMATION (R-I) 151 (10.7%)CONVENTIONAL (CONV) 357 (25.4%)INFORM (INF) 853 (60.7%)Total # of DFUs 1406
Table 1: Annotation statistics
Each message in the thread is segmented into Di-alog Functional Units (DFUs). A DFU is a con-tiguous span within an email message which hasa coherent communicative intention. Each DFUis assigned a single DA label which is one of thefollowing: REQUEST-ACTION (R-A), REQUEST-INFORMATION (R-I), CONVENTIONAL (CONV)and INFORM (INF). There are three other DA labels— INFORM-OFFLINE, COMMIT, and NODA for nodialog act — which occurred 5 or fewer times in thecorpus. We ignore these DA labels in this paper. Thecorpus also contains links between the DFUs, but wedo not use those annotations in this study. Table 1presents the distribution of DA labels in our corpus.We now describe each of the DAs we consider in ourexperiments.
In a REQUEST-ACTION, the writer signalsher desire that the reader perform some non-communicative act, i.e., an act that cannot in itselfbe part of the dialogue. For example, a writer canask the reader to write a report or make coffee.
In a REQUEST-INFORMATION, the writer signalsher desire that the reader perform a specific com-municative act, namely that he provide information(either facts or opinion).
In an INFORM, the writer conveys information, ormore precisely, the writer signals that her desire thatthe reader adopt a certain belief. It covers many dif-ferent types of information that can be conveyed in-cluding answers to questions, beliefs (committed ornot), attitudes, and elaborations on prior DAs.
A CONVENTIONAL dialog act does not signal anyspecific communicative intention on the part of thewriter, but rather it helps structure and thus facilitatethe communication. Examples include greetings, in-troductions, expressions of gratitude, etc.
4 System
We developed four systems for our experiments: abaseline (BAS) system which is close to the systemdescribed in (Hu et al., 2009), and three variants ofour novel divide and conquer (DAC) system. Fea-tures used in both systems are extracted as explainedin Section 4.2. Section 4.3 describes the baselinesystem, the basic DAC system, and two variationsof the DAC system.
4.1 Experimental FrameworkIn all our experiments, we use linear kernel Sup-port Vector Machines (SVM). However, across thesystems, there are differences in how we use them.Our framework was built with the ClearTK toolkit(Ogren et al., 2008) with its wrapper for SVMLight(Joachims, 1999). The ClearTK wrapper internallyshifts the prediction threshold based on posteriorprobabilistic scores calculated using the algorithmof Lin et al. (2007). We report results from 5-foldcross validation performed on the entire corpus.
4.2 Feature EngineeringIn developing our system, we classified our featuresinto three categories: lexical, verbal and message-
803
level. Lexical features consists of n-grams of words,n-grams of POS tags, mixed n-grams of closed classwords and POS tags (Prabhakaran et al., 2012), aswell as a small set of specialized features — Start-POS/Lemma (POS tag and lemma of the first word),LastPOS/Lemma (POS tag and lemma of the lastword), MDCount (number of modal verbs in theDFU) and QuestionMark (is there a question markin the DFU). We used the POS tags produced by theOpenNLP POS tagger. Verbal features capture theposition and identity of the first verb in the DFU. Fi-nally, message-level features capture aspects of thelocation of the DFU in the message and of the mes-sage in the thread (relative position and size). Inoptimizing each system, we first performed an ex-haustive search across all combinations of featureswithin each category. For the lexical n-gram fea-tures we varied the n-gram window from 1 to 5. Thisstep gave us the best performing feature combinationwithin each category. In a second step, we found thebest combination of categories, using the previouslydetermined features for each category. In this pa-per, we do not report best performing feature setsfor each configuration, due to lack of space.
4.3 Experiments
Baseline (BAS) System This system uses theClearTK built-in one-versus-all multiclass SVM inprediction. Internally, the multi-class SVM buildsa set of binary classifiers, one for each dialog act.For a given test instance, the classifier that obtainsthe highest probability score determines the overallprediction. We performed feature optimization onthe whole multiclass classifier (as described in Sec-tion 4.2), i.e., the same set of features was availableto all component classifiers. We optimized for sys-tem accuracy. Table 2 shows results using this sys-tem. In this and all tables, we give the performanceof the system on the four DAs, using precision, re-call, and F-measure. The DAs are listed in ascend-ing order of frequency in the corpus (least frequentDA first). We also give an overall accuracy evalua-tion. As we can see, detecting REQUEST-ACTION ismuch harder than detecting the other DAs.
Basic Divide and Conquer (DAC) System Likethe BAS system, the DAC system also builds a bi-nary classifier for each dialog act separately, and the
Prec. Rec. F-meas.R-A 57.9 31.4 40.7R-I 91.5 78.2 84.3CONV 92.0 95.8 93.8INF 91.6 95.1 93.3Accuracy 91.3
Table 2: Results for baseline (BAS) system (standardmulticlass SVM)
component classifier with highest probability scoredetermines the overall prediction. The crucial dif-ference in the DAC system is that the feature opti-mization is performed for each component classifierseparately. Each component classifier is optimizedfor F-measure. Table 3 shows results using this sys-tem.
Prec. Recall F-meas. ERR-A 66.7 40.0 50.0 15.6R-I 91.5 78.2 84.3 0.0CONV 93.9 94.1 94.0 2.6INF 91.4 96.1 93.7 5.7Accuracy 91.7 4.9
Table 3: Results for basic DAC system (per-class featureoptimization followed by maximum confidence basedchoice); “ER” refers to error reduction in percent overstandard multiclass SVM (Table 2)
Minority Preference (DACMP) System This sys-tem is exactly the same as the basic DAC systemexcept for one crucial difference: overall classifica-tion is biased towards a specified minority class. Ifthe minority class binary classifier predicts true, thissystem chooses the minority class as the predictedclass. In cases where the minority class classifierpredicts false, it backs off to the basic DAC systemafter removing the minority class classifier from theconfidence tally. Table 4 shows our results usingREQUEST-ACTION as the minority class.
Cascading Minority Preference (DACCMP) SystemThis system is similar to the Minority PreferenceSystem; however, instead of a single supplied mi-nority class, the system accepts an ordered list ofclasses. The classifier then works, in order, throughthis list; whenever any classifier in the list predicts
804
Prec. Recall F-meas. ERR-A 66.7 45.7 54.2 22.8R-I 91.5 78.2 84.3 0.0CONV 93.9 94.1 94.0 2.6INF 91.6 96.0 93.8 6.5Accuracy 91.8 5.7
Table 4: Results for minority-preference DAC system —DACMP (first consult REQUEST-ACTION tagger, then de-fault to choice by maximum confidence); “ER” refers toerror reduction in percent over standard multiclass SVM(Table 2)
true, for a given instance, it then assigns this classas the predicted class. The subsequent classifiers inthe list are not run. If all classifiers predict false, weback off to the basic DAC system, i.e., the compo-nent classifier with highest probability score deter-mines the overall prediction. We ordered the list ofclasses in the ascending order of their frequencies inthe training data. This ordering is driven by the ob-servation that the less frequent classes are also hardto predict correctly. Table 5 shows our results usingthe ordered list: (REQUEST-ACTION, REQUEST-INFORMATION, CONVENTIONAL, INFORM).
Prec. Recall F-meas. ERR-A 66.7 45.7 54.2 22.8R-I 91.0 80.8 85.6 8.4CONV 93.7 95.3 94.5 10.1INF 92.4 95.8 94.0 10.0Accuracy 92.2 10.6
Table 5: Results for cascading minority-preference DACsystem — DACCMP (consult classifiers in reverse orderof frequency of class); “ER” refers to error reduction inpercent over standard multiclass SVM (Table 2)
4.4 DiscussionAs shown in Table 3, the basic DAC system obtaineda 15.6% F-measure error reduction for the minor-ity class REQUEST-ACTION over the BAS system.It also improves performance of two other classes— CONVENTIONAL and INFORM, and obtaines a4.9% error reduction on overall accuracy. Recallhere that the only difference between the DAC sys-tem and the BAS system is the per-class feature op-timization and therefore this must be the reason for
this boost in performance. When we turn to DACMP,we see that the performance on the minority classREQUEST-ACTION is further enhanced, with an F-measure error reduction of 22.8%; the overall ac-curacy improves slightly with an error reduction of5.7%. Finally, DACCMP further improves the perfor-mance. Since the method of choosing the minor-ity class REQUEST-ACTION does not change overDACMP, the F-measure error reduction remains thesame. However, now all three other classes also im-prove their performance, and we obtain a 10.6% er-ror reduction on overall accuracy over the baselinesystem.
Following (Guyon et al., 2002), we performed apost-hoc analysis by inspecting the feature weightsof the best performing models created for each in-dividual classifier in the DAC system. Table 6 listssome interesting features chosen during feature opti-mization for the individual SVMs. We selected themfrom the top 25 features in terms of absolute valueof feature weights.
Some features help distinguish different DA cat-egories. For example, the feature QuestionMarkis the feature with the highest negative weight forINFORM, but has the highest positive weight forREQUEST-INFORMATION. Features like fyi and pe-riod (.) have high positive weights for INFORMand high negative weights for CONVENTIONAL.Some other features are important only for certainclasses. For e.g., please and VB NN are importantfor REQUEST-ACTION, but not so for other classes.Overall, the most discriminating features for bothINFORM and CONVENTIONAL are mostly wordngrams, while those for REQUEST-ACTION andREQUEST-INFORMATION are mostly POS ngrams.This shows why our approach of per-class featureoptimization is important to boost the classificationperformance.
Another interesting observation is that the leastfrequent category, REQUEST-ACTION, has the leaststrong indicators (as measured by feature weights).Presumably this is because there is much less train-ing data for this class. This explains why our cascad-ing classifiers approach giving priority to the leastfrequent categories worked better than a simple con-fidence based approach, since the simple approachdrowns out the less confident classifiers.
805
REQUEST-ACTION REQUEST-INFORMATION CONVENTIONAL INFORMplease (0.9) QuestionMark (6.6) StartPOS NNP (2.7) QuestionMark (-3.0)VB NN (0.7) BOS PRP (-1.2) thanks (2.3) thanks (-2.2)you VB (0.3) WRB (1.0) . (-2.0) . (2.2)PRP (-0.3) PRP VBP (-0.9) fyi (-2.0) fyi (1.9)MD PRP VB (0.3) BOS MD (0.8) , (0.9) you (-1.0)will (-0.2) BOS DT (-0.7) QuestionMark (-0.8) can you (-0.9)
Table 6: Post-hoc analysis on the models built by the DAC system: some of the top features with correspondingfeature weights in parentheses, for each individual tagger. (POS tags are capitalized; BOS stands for Beginning OfSentence)
5 Extrinsic Evaluation
In this section, we perform an extrinsic evaluationfor the dialog act tagger presented in Section 4 byapplying it to the task of identifying Overt Displaysof Power (ODP) in emails, proposed by Prabhakaranet al. (2012). The task is to identify utterances wherethe linguistic form introduces additional constraintson its responses, beyond those introduced by thegeneral dialog act. The dialog act features werefound to be useful and the best performing systemobtained an F-measure of 65.8 using gold dialogact tags. For our extrinsic evaluation, we retrainedthe ODP tagger using dialog act tags predicted byour BAS and DACCMP systems instead of gold dia-log acts. ODP tagger uses the same dataset as oursfor training. In the cross validation step, we madesure that the test folds for ODP were excluded fromtraining the taggers to obtain DA tags. At each ODPcross validation step, we trained a BAS or DACCMPtagger using ODP’s training folds for that step andused tags produced by that tagger for both trainingand testing the ODP tagger for that step. Table 7 liststhe results obtained.
Prec. Rec. F-meas.No-DA 55.7 45.4 50.0Gold-DA 75.8 58.1 65.8BAS-DA 60.6 46.5 52.6DACCMP-DA 67.2 45.4 54.2
Table 7: Results for ODP system using various sourcesof DA tags
Using BAS tagged DA, the F-measure of ODPsystem reduced by 13.2 points to 52.6 from usinggold dialog acts (F=65.8). Using DACCMP, the F-
measure improved over BAS by 1.6 points to 54.2.This constitutes an error reduction of 12.1%, tak-ing the system using gold DA tags as the reference.This improvement is noteworthy, given the fact thatthe overall error reduction obtained by DACCMP overBAS in the DA tagging was around 10.6%. Also, theDACCMP-based ODP system obtained an error reduc-tion of about 26.6% over a system that does not usethe DA features at all (F=50.0).
6 Conclusion
We presented a method of improving the perfor-mance of dialog act tagging in identifying minorityclasses by using per-class feature optimization andchoosing the class based on a cascade of classifiers.We showed that it gives a minority class F-measureerror reduction of 22.8% while also reducing the er-ror on other classes and the overall error by around10%. We also presented an extrinsic evaluation ofthis technique on detecting Overt Displays of Powerin dialog, where we achieve an error reduction of12.1% over using the standard multiclass SVM togenerate dialog act tags.
Acknowledgements
This work is supported, in part, by the Johns Hop-kins Human Language Technology Center of Ex-cellence. Any opinions, findings, and conclusionsor recommendations expressed in this material arethose of the authors and do not necessarily reflect theviews of the sponsor. While working on this project,the first author Adinoyi Omuya was affiliated withthe Center for Computational Learning Systems atColumbia University. We thank several anonymousreviewers for their constructive feedback.
806
References
Apoorv Agarwal, Adinoyi Omuya, Aaron Harnly, andOwen Rambow. 2012. A Comprehensive Gold Stan-dard for the Enron Organizational Hierarchy. In Pro-ceedings of the 50th Annual Meeting of the Associa-tion for Computational Linguistics (Volume 2: ShortPapers), pages 161–165, Jeju Island, Korea, July. As-sociation for Computational Linguistics.
J. L. Austin. 1975. How to Do Things with Words. Har-vard University Press, Cambridge, Mass.
William W. Cohen, Vitor R. Carvalho, and Tom M.Mitchell. 2004. Learning to Classify Email into“Speech Acts” . In Dekang Lin and Dekai Wu, ed-itors, Proceedings of EMNLP 2004, pages 309–316,Barcelona, Spain, July. Association for ComputationalLinguistics.
Isabelle Guyon, Jason Weston, Stephen Barnhill, andVladimir Vapnik. 2002. Gene Selection for CancerClassification using Support Vector Machines. Mach.Learn., 46:389–422, March.
Jun Hu, Rebecca Passonneau, and Owen Rambow. 2009.Contrasting the Interaction Structure of an Email anda Telephone Corpus: A Machine Learning Approachto Annotation of Dialogue Function Units. In Pro-ceedings of the SIGDIAL 2009 Conference, London,UK, September. Association for Computational Lin-guistics.
Thorsten Joachims. 1999. Making Large-Scale SVMLearning Practical. In Bernhard Scho¨lkopf, Christo-pher J.C. Burges, and A. Smola, editors, Advancesin Kernel Methods - Support Vector Learning, Cam-bridge, MA, USA. MIT Press.
J. Kim, G. Chern, D. Feng, E. Shaw, and E. Hovy.2006. Mining and Assessing Discussions on the WebThrough Speech Act Analysis. In Proceedings of theWorkshop on Web Content Mining with Human Lan-guage Technologies at the 5th International SemanticWeb Conference.
S.N. Kim, L. Cavedon, and T. Baldwin. 2010a. Classify-ing Dialogue Acts in One-on-one Live Chats. In Pro-ceedings of the 2010 Conference on Empirical Meth-ods in Natural Language Processing, pages 862–871.Association for Computational Linguistics.
S.N. Kim, L. Wang, and T. Baldwin. 2010b. Taggingand Linking Web Forum Posts. In Proceedings ofthe Fourteenth Conference on Computational NaturalLanguage Learning, pages 192–202. Association forComputational Linguistics.
Hsuan-Tien Lin, Chih-Jen Lin, and Ruby C. Weng. 2007.A Note on Platt’s Probabilistic Outputs for SupportVector Machines. Mach. Learn., 68:267–276, Octo-ber.
Philip V. Ogren, Philipp G. Wetzler, and Steven Bethard.2008. ClearTK: A UIMA toolkit for statistical naturallanguage processing. In Towards Enhanced Interoper-ability for Large HLT Systems: UIMA for NLP work-shop at Language Resources and Evaluation Confer-ence (LREC).
Vinodkumar Prabhakaran, Owen Rambow, and MonaDiab. 2012. Predicting Overt Display of Power inWritten Dialogs. In Human Language Technologies:The 2012 Annual Conference of the North AmericanChapter of the Association for Computational Linguis-tics, Montreal, Canada, June. Association for Compu-tational Linguistics.
J.R. Searle. 1976. A Classification of Illocutionary Acts.Language in society, 5(01):1–23.
A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates,D. Jurafsky, P. Taylor, R. Martin, C.V. Ess-Dykema,and M. Meteer. 2000. Dialogue Act Modeling forAutomatic Tagging and Recognition of ConversationalSpeech. Computational linguistics, 26(3):339–373.
J.Y. Yeh and A. Harnly. 2006. Email Thread ReassemblyUsing Similarity Matching. In Third Conference onEmail and Anti-Spam (CEAS), pages 27–28.
R. Zhang, D. Gao, and W. Li. 2012. Towards ScalableSpeech Act Recognition in Twitter: Tackling Insuffi-cient Training Data. EACL 2012, page 18.
807
