Dialogue Act Recognition using Reweighted Speaker Adaptation
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 118–125,Seoul, South Korea, 5-6 July 2012. c©2012 Association for Computational Linguistics
Dialogue Act Recognition using Reweighted Speaker Adaptation
Congkai SunInstitute for Creative
Technologies12015 Waterfront Drive
Playa Vista, CA 90094-2536csun@ict.usc.edu
Louis-Philippe MorencyInstitute for Creative
Technologies12015 Waterfront Drive
Playa Vista, CA 90094-2536morency@ict.usc.edu
Abstract
In this work we study the effectiveness ofspeaker adaptation for dialogue act recogni-tion in multiparty meetings. First, we analyzeidiosyncracy in dialogue verbal acts by quali-tatively studying the differences and conflictsamong speakers and by quantitively compar-ing speaker-specific models. Based on theseobservations, we propose a new approach fordialogue act recognition based on reweighteddomain adaptation which effectively balancethe influence of speaker specific and otherspeakers’ data. Our experiments on a real-world meeting dataset show that with evenonly 200 speaker-specific annotated dialogueacts, the performances on dialogue act recog-nition are significantly improved when com-pared to several baseline algorithms. To ourknowledge, this work is the first 1 to tackle thispromising research direction of speaker adap-tation for dialogue act recogntion.
1 Introduction
By representing a higher level intention of utterancesduring human conversation, dialogue act labels arebeing used to enrich the information provided byspoken words (Stolcke et al., 2000). Dialogue actrecognition is a preliminary step towards deep dia-logue understanding. It plays a key role in the de-sign of dialogue systems. Besides, Fernandez et al.(2008) find certain dialogue acts are important cuesfor detecting decisions in Multi-party dialogue. In
1This paper is an extended version of a poster presented atSemDial 2011, with new experiments and deeper analysis.
Ranganath et al. (2009), dialogue acts are used asimportant features for flirt detection.
Automatic dialogue act recognition is still an ac-tive research topic. The conventional approach is totrain one generic classifier using a large corpus ofannotated utterances. One aspect that makes it sochallenging is that people can express the same idea(or speech act) using a very different set of spokenwords. Even more, people can mean different thingswith the exact same spoken words. These idiosyn-cratic differences in dialogue acts make the learningof generic classifiers extremely challenging. Luck-ily, in many applications such as face-to-face meet-ings or tele-immersion, we have access to archivesof previous interactions with the same participants.From these archives, a small subset of spoken utter-ances can be efficiently annotated. As we will latershow in our experiments, even a small number of an-notated utterances can make a significant difference.
In this paper, we propose a new approach fordialogue act recognition based on reweighted do-main adaptation which effectively balance the influ-ence of speaker specific and other speakers’ data.By treating each speaker as one domain, we pointout the connection between training speaker spe-cific dialogue act classifier and supervised domainadaptation problem. We analyze idiosyncracy indialogue verbal acts by qualitatively studying thedifferences and conflicts among speakers and byquantitively comparing speaker-specific models. Wepresent an extensive set of experiments studying theeffect of speaker adaptation on dialogue act recogn-tion in multi-party meetings using the ICSI-MRDAdataset (Shriberg, 2004).
118
The following section presents related work on di-alogue act recognition and domain adaptation. Sec-tion 3 describes the ICSI-MRDA (Shriberg, 2004)dataset which is used in all our experiments. Sec-tion 4 analyze idiosyncracy in dialogue acts, bothqualitatively and quantitatively. Section 5 ex-plains our reweighting-based speaker adaptation al-gorithm. Section 6 contains all experiments to provethe applicability of speaker adaptation to dialogueact recognition. Finally, inspired by the promisingresults, Section 8 describes some future directions.
2 Previous Work
Automatic dialogue act recognition has been an im-portant problem in the past decades. Different dia-logue act labeling standards and datasets have beenprovided, including Switchboard-DAMSL (Stolckeet al., 2000), ICSI-MRDA (Shriberg, 2004) andAMI (Carletta, 2007). Stolcke et al (2000) is oneof the first work using machine learning technique(HMM) to automatically segment and recognize di-alogue acts. Rangarajan et al. (2009) demonstratedwell-designed prosodic n-gram features are veryhelpful for Dialogue Act recognition in MaximumEntropy model. And Ang et al (2005) exploredjoint segmentation and dialogue act classification forspeech from ICSI.
Domain adaptation is a popular problem in natu-ral language processing community due to the spar-sity of labeled data. Jiang (Jiang, 2007) breaksthe analysis of domain adaptation problem into dis-tributional differences in instances and classifica-tion functions between source and target data. InDaume’s work (2007) several domain adaptation al-gorithms are described. Our speaker adaptation al-gorithm is inspired by the reweighting-based adap-tation algorithm introduced in this paper.
Recently, dialogue act adaptation has been gettinga lot of attention. Tur et al. (2006) successfully useSwitchboard-DAMSL to help dialogue act recogni-tion in ICSI-MRDA. Promising results have beenobtained by using a regression model to combine themodel weights obtained by training on Switchboard-DAMSL and ICSI-MRDA respectively. Followingthe work by Tur et al. (2006), Guz et al. (2009) fur-ther studied the effectiveness of dialogue act domainadaptation in cascaded dialogue act segmentation
and recognition system, their results prove adapta-tion in the intermediate step (segmentation) are alsovery helpful for the final output (recognition). Jeonget al (2009) use semi-supervised boosting algorithmto leverage labeled data from Switchboard-DAMSLand ICSI-MRDA to help dialogue act recognition inemail and forums. Margolis et.al (2010) use a struc-tural correspondence learning technique to adapt di-alogue act recognition on automatic translated Span-ish genre with the help of Switchboard-DAMSL andICSI-MRDA. Kolar et al. (2007) explores the dif-ference among speakers for dialogue act segmenta-tion in ICSI-MRDA dataset. Similar to the approachtaken in Tur et al. (2006), adaptation is performedthrough the combination of generic speaker inde-pendent Language Model and other speakers’ Lan-guage Model. Significant improvements have beenobtained for most of the selected speakers.
All these previous papers focused on adapting di-alogue act models between domains and did notaddress the person-specific adaptation. The onlyexception was Kolar et al. (2007) who exploredspeaker-specific dialogue act segmentation. To ourknowledge, this paper is the first work to analyze theeffectiveness of speaker adaptation for dialogue actrecognition.
3 ICSI-MRDA Corpus
Different Dialogue Act labeling standards anddatasets have been provided in recent years, in-cluding Switchboard-DAMSL (Stolcke et al., 2000),ICSI-MRDA (Shriberg, 2004) and AMI (Carletta,2007). ICSI-MRDA is the dataset for our exper-iments because many of its meetings contain thesame speakers, thus making it more suitable for ourspeaker adaptation study. The tagset in ICSI-MRDAis adapted from DAMSL standard (damsl, 1997) byallowing multiple tags per dialogue act. Each dia-logue act in ICSI-MRDA has one general tag andmultiple specific tags.
ICSI-MRDA consists of 75 meetings, eachroughly an hour long. There are five categories ofmeetings (three of which we are actively using inour experiments) : Bed is about the discussion ofnatural language processing and neural theories oflanguage, Bmr is for the discussion on ICSI meetingcorpus, Bro is on speech recognition topics and Bns
119
ID Tag Type Nb. Meetings Nb. DAs1 mn015 Bed 15 62282 me010 Bed 11 53093 me013 Bmr 25 97534 mn017 Bmr 15 40595 fe016 Bmr 18 55006 me018 Bro 20 42637 me013 Bro 22 11928
Table 1: The 7 speakers from ICSI-MRDA dataset usedin our experiments. The table lists: the Speaker ID, orig-inal speaker tag, the type of meeting selected for thisspeaker, the number of meetings this speaker participatedand the total number of dialogue acts by this speaker.
is about network and architecture. The last categoryis varies which contains all other topics.
From these 75 meetings, there are 53 uniquespeakers in total, and an average of about 6 speakersper meeting. 7 speakers2 having more than 4, 000dialogue acts are selected for our adaptation experi-ments. Table 1 shows the details of our 7 selectedspeakers. From the word transcriptions, we cre-ated an extended list of linguistic features per ut-terance. From the 7 selected speakers, we com-puted 14653 unigram features, 158884 bigram fea-tures and 400025 trigram features.
Following the work of Shriberg et al. (2004), weuse the 5 general tags in our experiments:
• Disruption indicates the current Dialogue Actis interrupted.
• Back Channel are utterances which are notmade directly by a speaker as a response anddo not function in a way that elicits a responseeither.
• Floor Mechanism are dialogue acts for grab-bing or maintaining the floor.
• Question is for eliciting listener feed back.• And finally, unless an utterance is completely
indecipherable or else can be further describedby a general tag, then its default status is State-ment.
Our dataset consisted of 47040 dialogue acts. Thedistribution of Dialogue Act is shown in Table 2.
2speaker me013 is split into me013-Bmr and me013-Bro toavoid the difference introduced by meeting types.
Tag proportionDisruption 14.73%
Back Channel 10.20%Floor Mechanism 12.40%
Question 7.20%Statement 55.46%
Table 2: Distribution of dialogue acts in our dataset.
4 Idiosyncrasy in Dialogue Acts
Our goal is to create a dialogue act recognition al-gorithm that can adapt to specific speakers. Someimportant questions must be studied before creat-ing such algorithm. The first obvious one is: dospeakers really differ in their choice of words andassociated dialogue acts? Do we really see a vari-ability on how people express their dialogue in-tent? If the answers are yes, then we will expectthat learning a dialogue act recognizer from speaker-specific utterances should always outperform a rec-ognizer learned from someone else data. Section 4.1presents a comparative experiment addressing thesequestions.
To better understand the results from this com-parative experiment, we also performed a quali-tative analysis presented in Section 4.2 where welook more closely at the differences between speak-ers. These two qualitative and quantitative analysisare building block for our adaptation algorithm pre-sented in Section 5.
4.1 Speaker-Specific Recognizers
An important assumption when performing speakeradaptation (or more generally domain adaptation)is that data coming from the same speaker shouldbe similar than data coming from another person.In other words, a recognizer trained on a speakershould perform better (when tested on the same per-son) than a recognizer trained on another speaker.We designed an experiment to test this hypothesis.
We learned 7 speaker-specific recognizers, onefor each speaker (see Table 1). We then tested allthese recognizers on new utterances from the same7 speakers. We looked the recognition performancewhen (1) the recognizer was trained on the sameperson and (2) when the recognizer was trained ona different person. This experiments quantitatively
120
Figure 1: Effect of same-speaker data on dialogue actrecognition. We compare two approaches: (1) when arecognizer is trained on the same person and tested onnew utterances from the same person, and (2) when therecognizer was trained on another speaker (same test set).We vary the amount of training data to be 200, 500,1000, 1500 and 2000 dialogue acts. In all cases, usingspeaker-specific recognizer outperforms recognizer fromother speakers.
analyze the the difference among speakers. The ex-perimental methodology used in this experiment isthe same as the other experiments described in thispaper (see Section 6). We use the Maximum En-tropy model(MaxEnt) for all dialogue act recogniz-ers (Ratnaparkhi, 1996). Please refer to Section 6.2for more details about the experimental methodol-ogy.
Figure 1 compares the average performanceswhen testing on the same speaker or on some otherspeaker. We vary the number of training data foreach speaker to be 200, 500, 1000, 1500 and 2000dialogue acts. For all five cases, the recognizerstrained on the same speaker outperforms the aver-age performance when using a recognizer from another person. Thus speaker specific dialogue actsadaptation fits the assumption of domain adaptationproblems.
4.2 Speakers Differences
To better understand the problem, we look moreclosely at the differences among speakers and theiruse of dialogue acts. We analyze the probleminduced by speaker idiosyncrasy in dialogue acts.During our qualitative analysis of the ICSI-MRDAdataset, we identified three major differences ex-plaining the performances observed in the previous
sections: dialogue act conflicts, word distributionand dialogue act label distribution. We describethese three differences with some examples:
Conflicts: These differences happen when twospeakers intended to express different meaningswhile speaking the exact same utterance. To exam-plify these conflicts, we computed mutual informa-tion between a specific utterance and all dialogue actlabels. We find interesting examples where for ex-emple the word right is the most important cue fordialogue act question when spoken by me013-Bmr,while right is also an important cue for dialogue actback-channel for speaker me010-Bed. These exam-ples suggest that conflicts exist among speakers andsimply trying to learn one generic model may notbe able to handle these conflicts. The generic modelwill learn what most people mean with this utter-ance, which may be the wrong prediction for ourspecific speaker.
Word distribution: People have their own vocab-ulary. Although many words are the same, how of-ten one person use each word will vary. Although wemay not have direct conflict here, the problem canalso be serious. The learning algorithm may mis-leadingly focus on optimizing the weights for certainwords which are not important(e.g., words that oc-cur more often in other speakers’ dialogue acts thanhis/her own) while under-estimating the importantwords for this speaker. This observation suggeststhat our adaptation should take into account worddistribution.
Label Distribution: Another interesting observa-tion is to look at the distribution of dialogue act la-bels for different speakers. Table 2 shows the aver-age distribution over all 7 speakers. When lookingmore closely at each speaker, we find some interest-ing differences. For example, speaker 1 made state-ments 61% of the time while speaker 4 made 49% ofthe time. While this difference may not look signif-icant, these changes can definitely affect the recog-nition performance. So the adaptation model shouldalso take into account the dialogue act label distri-bution.
5 Reweighted Speaker Adaptation
Based on the observations described in the previoussections, we implement a simple reweighting-based
121
domain adaptation algorithm mentioned in (Daume,2007) based on Maximum Entropy model (MaxEnt)(Ratnaparkhi, 1996). MaxEnt model is a popularand efficient discriminative model which can effec-tively accommodate large numbers of features. Allthe unigram, bigram and trigram features are usedas input to the maxEnt model, the output is the di-alogue act label. MaxEnt model maximizes the logconditional likelihood of all samples:
Loss =
N?1
log(p(yn|xn)) (1)
where N is the number of samples for the trainingdata. xn represents the feature of the nth sample andyn is the label. The conditional likelihood is definedas
p(y|x) = exp(?i
?ifi(x, y))/Z(x) (2)
where Z(x) is the normalization factor and fi(x, y)are the n-gram features described in Section 3.
When applied to our problem of speaker adapta-tion, the reweighting adaptation model can be for-mally defined as
Loss = wS?
n=1
log(p(yn|xn))+O?
m=1
log(p(ym|xm))(3)
where S is the number of labeled speaker-specificdialogue acts, O is the number for other speakers’labeled dialogue acts. For each speaker, we trainone speaker-specific classifier by varying the distri-bution of training data. We reweight the importanceof speaker specific dialogue acts versus other speak-ers’ labeled dialogue acts in the training data. Theoptimal weight parameter w is automatically esti-mated through validation.
It is worth mentioning a specific instance of thereweighting adaptation algorithm. When w is set to1, the reweighting adaptation algorithm is equivalentto simply training a MaxEnt model by putting thespeaker-specific and generic data samples togetheras training data. In our experiments, we will com-pare the reweighting adaptation approach with thissimpler approach, referred as constant adaptation.
6 Experiments
Our goal is to get one model specifically adaptedfor each speaker. We first describes 4 different ap-proaches to be compared in the experiments, andsection 6.2 explains our experimental methodology.
6.1 4 ApproachesIn these experiments, we compare our approach,called reweighted adaptation, with three moreconventional approaches: speaker-specific only,Generic and Constant adaptation.
• Speaker Specific Only For this approach, wetrain the dialogue act recognizer using trainingsentences from the same speaker used duringtesting.
• Generic In this case, we train the dialogue actrecognizer using utterances from all speakersother than the speaker used during testing.
• Constant Adaptation For this approach, wetrain the dialogue act recognizer using allspeakers, including the speaker who will laterbe used for testing. All utterances have thesame weight in this case.
• Reweighted Adaptation This is our proposedapproach. As described in Section 5, we trainour dialogue act recognizer using all speakersbut reweight the utterances from the speakerwho will later be used for testing.
6.2 MethodologyIn all the following experiments we use MaxEntmodels as defined in Section 5. L2 regularizationis used for MaxEnt to avoid overfitting. The optimalregularization parameter was automatically selectedduring validation. The following regularization pa-rameters were used: 0.01, 0.1, 1, 10 , 100, 1000 and0 (no regularization). All the unigram, bigram andtrigram features are used in the maxEnt model. Thelabels are the five dialogue act tags described in Sec-tion 3.
All experiments were performed using hold-outtesting and hold-out validation. Both validationand test sets consisted of 1000 dialogue acts. Thetraining sets contained only utterances from meet-ings that were not in the validation set of test set.
122
Train Data 200 500 1000 1500 2000Speaker-specific
Only 64.07 65.99 68.51 69.99 71.06Constant
adaptation model 76.81 76.96 77.00 77.23 77.53Our reweighted
adaptation model 78.17 78.29 78.67 78.74 78.47
Table 3: Average results among all 7 speakers when trainwith different combinations of speaker specific data andother speakers’ data. The number of speaker specific datais varied from 200, 500, 1000, 1500 to 2000.
In many of our experiments, we analyzed the ef-fect of training set size on the recognition perfor-mance. The speaker-specific data size varied from200, 500, 1000, 1500 and 2000 dialogue acts respec-tively. When training our reweighting adaptation al-gorithm described in Section 5, we used the follow-ing weights: 10, 30, 50, 75, and 100. The optimalweight factor was selected automatically during val-idation.
7 Results
In this section we present our approaches to studythe importance of speaker adaptation for dialogueact recognition. All following results are calculatedbased on the overall tag accuracies. We designedthree series of experiments for this study:
• Generic Recognizer (Section 7.1)• Sparsity in speaker-specific data (Section 7.2)• Effectiveness of Constant Adaptation (Sec-
tion 7.3)
• Performance of the reweighting algorithm(Section 7.4)
7.1 Generic RecognizerThe first result we get is on average, for each speakerwhen we use all other speaker’s data for training,then test on speaker- specific test data. The perfor-mance of this generic recognizer is 76.76% is thebaseline we try to improve when adding speaker-specific data into consideration. 3
3The performance of our generic model is comparable to theresults from Ang et al (2005) when you take into considerationthat we used only 47,040 dialogue acts in our experiments (i.e.,dialogue acts from our 7 speakers) which is a small fractioncompared with Ang et al (2005) .
7.2 Sparsity of speaker-specific data
A second result is the performance when only us-ing speaker-specific data. The row Speaker SpecificOnly in Table 3 shows the average results amongall speakers when for each speaker, we train us-ing only data from the same speaker. The numberof speaker-specific training data we tried are 200,500, 1000, 1500, and 2000 respectively. Even with2000 speaker-specific dialogue acts for training, thebest accuracy is 71.06% which is lower than 76.76%when using generic recognizer. Given the challengein getting 2000 speaker-specific annotated dialogueacts, we are looking at a different approach wherewe need less speaker-specific data.
7.3 Results of Constant Adaptation
The most straightforward way to combine otherspeakers’ data is to directly add them with speaker-specific data as train. We refer to this approachas constant adaptation. The row Constant Adap-tation in Table 3 shows the average results amongall speakers when for each speaker, we combinethe speaker-specific data directly with the all otherspeaker’s data. In our experiments, we varied theamount of speaker-specific data included to be 200,500, 1000, 1500, and 2000 respectively. For all7 speakers, the performance can always been im-proved by including speaker-specific data with allother speakers’ data for training. Furthermore, themore speaker specific data added, the better perfor-mance we get.
7.4 Results of Reweighting Algorithm
Finally, in this section we describe the results fora simple adaptation algorithm based on reweight-ing, as described in Section 5. Following the samemethodology as previous experiments, we vary theamount of speaker-specific data to be 200, 500,1000, 1500 and 2000. The best reweighting factor isselected through validation on speaker-specific val-idation data described in section 6.2. The results ofall 7 speakers from Reweighting algorithm when wevary the amount of speaker-specific data are shownin Figure 3.
We analyze the influence of the weighting factoron our speaker adaptation by plotting the recogni-tion performance for different weights. Figure 4 il-
123
Figure 2: The average results among all 7 speakers whentrain with different combinations of speaker specific dataand other speakers’ data are displayed. In both Constantadaptation and Reweighted adaptation models the num-ber of speaker specific data are varied from 200, 500,1000, 1500 to 2000. In Generic model, only all otherspeakers’ data are used for training data.
Figure 3: Reweighting algorithm for all 7 IndividualSpeakers when varying the amount of training data to be0, 200, 500, 1000, 1500 and 2000.
lustrates the influence of the weight factor on threespeaker adaptation cases: None, 500 and 2000. Inthis case, None represent the Constant Adaptation.We observe the following trend: with more speaker-specific data, the optimal reweighting factor is alsolower. This confirms that our reweighting algorithmfinds the right balance between speaker-specific dataand generic data.
Figure 2 and the row Reweighted Adaptationfrom Table 3 shows the effectiveness of reweight-ing algorithm. Results shows that even this sim-ple algorithm can efficiently balance the influenceof speaker specific data and other speakers’ data and
0 20 40 60 80 1000.765
0.77
0.775
0.78
0.785
0.79
 
 
None5002000
Figure 4: Average results of Reweighting among all 7speakers when the amount of speaker specific data is 0,500, 2000
give significantly improved results. And most sur-prisingly, even with only 200 speaker specific datathe reweighting algorithm can give very promisingresults.
8 Conclusion
In this work we analyze the effectiveness of speakeradaptation for dialogue act recognition. A simplereweighting algorithm is shown to give promisingimprovement on several baseline algorithms evenwith only 200 speaker-specific dialogue acts. Thispaper is a first step toward automatic adaptation fordialogue act recognition. Inspired by the promisingresults from the simple reweighting algorithm, weplan to evaluate other domain adaptation techniquessuch as Daume’s feature-based approach (2007). Itwill also be interesting to consider the unlabeleddata from each speaker when performing dialogueact recognition.
Acknowledgments
This material is based upon work supported bythe National Science Foundation under Grant No.1118018 and the U.S. Army Research, Develop-ment, and Engineering Command (RDECOM). Thecontent does not necessarily reflect the position orthe policy of the Government, and no official en-dorsement should be inferred.
124
References
Jeremy Ang, Yang Liu, Elizabeth Shriberg 2005. Au-tomatic Dialog Act Segmentation and Classification inMultiparty Meetings. ICASSP.
Jean Carletta. 2007. Unleashing the killer corpus: expe-riences in creating the multi-everything AMI MeetingCorpus. Language Resources and Evaluation, 41(2):181-190
Mark Core and James Allen. 1997. Working Notes:AAAI Fall Symposium. HLT-NAACL SIGDIAL Work-shop.
Hal Daume´ III. 2007. Frustratingly Easy Domain Adap-tation. Proceedings of the 45th Annual Meeting of theAssociation for Computational Linguistics.
Raquel Fernandez, Matthew Frampton, Patrick Ehlen,Matthew Purver and Stanley Peters. 2008. Mod-elling and Detecting Decisions in Multi-Party Dia-logue. Proceedings of the 9th SIGdial Workshop onDiscourse and Dialogue.
Umit Guz, Gokhan Tur, Dilek Hakkani-Tur, and Se-bastien Cuendet. 2009. Cascaded model adaptationfor dialog act segmentation and tagging. ComputerSpeech & Language, 24(2):289–306.
Minwoo Jeong, Chin-Yew Lin and Gary Lee. 2009.Semi-supervised speech act recognition in emails andforums. The 2009 Conference on Empirical Methodson Natural Language Processing.
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-ing for domain adaptation in NLP. Proceedings of the45th Annual Meeting of the Association for Computa-tional Linguistics.
Jachym Kolar, Yang Liu, and Elizabeth Shriberg. 2007.Speaker Adaptation of Language Models for Auto-matic Dialog Act Segmentation of Meetings. Inter-speech, 339–373.
Anna Margolis, Karen Livescu, Mari Ostendorf. 2010.Semi-supervised domain adaptation for automatic di-alog act tagging. ACL 2010 Workshop on DomainAdaptation for Natural Language Processing.
Rajesh Ranganath, Dan Jurafsky, and Dan McFarland.2009. It’s Not You, it’s Me: Detecting Flirting and itsMisperception in Speed-Dates. The 2009 Conferenceon Empirical Methods on Natural Language Process-ing.
Vivek Rangarajan, Srinivas Bangaloreb and ShrikanthNarayanana. 2009. Combining lexical, syntactic andprosodic cues for improved online dialog act tagging.Computer Speech and Language, 23(4): 407-422
Adwait Ratnaparkhi. 1996. A maximum entropy modelfor part-of-speech tagging. In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing.
Elizabeth Shriberg, Raj Dhillon, Sonali Bhagat, JeremyAng and Hannah Carvey. 2004. The ICSI MeetingRecorder Dialog Act (MRDA) Corpus. HLT-NAACLSIGDIAL Workshop.
Andreas Stolcke, Klaus Ries, Noah Coccaro, ElizabethShriberg, Rebecca Bates, Daniel Jurafsky, Paul Tay-lor, Rachel Martin, Carol V. Ess-dykema and MarieMeteer. 2000. Dialogue Act Modeling for AutomaticTagging and Recognition of Conversational Speech.Computational Linguistics, 26:339-373.
Gokhan Tur, Umit Guz and Dilek Hakkani-Tur. 2006.Model Adaptation For Dialogue Act Tagging. SpokenLanguage Technology Workshop.
125
