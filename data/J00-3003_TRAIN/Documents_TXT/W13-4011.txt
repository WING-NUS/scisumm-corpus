A quantitative view of feedback lexical markers in conversational French
Proceedings of the SIGDIAL 2013 Conference, pages 87–91,Metz, France, 22-24 August 2013. c©2013 Association for Computational Linguistics
A quantitative view of feedback lexical markers in conversational French
Laurent Pre´vot Brigitte BigiAix Marseille Universite´ & CNRS
Laboratoire Parole et LangageAix-en-Provence (France)
firstname.lastname@lpl-aix.fr
Roxane Bertrand
Abstract
This paper presents a quantitative descrip-tion of the lexical items used for linguis-tic feedback in the Corpus of InteractionalData (CID). The paper includes the rawfigures for feedback lexical item as wellas more detailed figures concerning inter-individual variability. This effort is a firststep before a broader analysis includingmore discourse situations and featuringcommunicative function annotation.
Index Terms: Feedback, Backchannel, Corpus,French Language
1 Objectives
Conversational feedback is mostly performedthrough short utterances such as yeah, mh, okaynot produced by the main speaker but by one ofthe other participants of a conversation. Such ut-terances are among the most frequent in conver-sational data (Stolcke et al., 2000). They alsohave been described in psycho-linguistic modelsof communication as a crucial communicative toolfor achieving coordination or alignment in dia-logue (Clark, 1996).
The general objective of the project (ANRCoFee: Conversational Feedback)1(Pre´vot andBertrand, 2012) in which this work takes placeis to propose a fine grained model of theform/function relationship concerning feedbackbehaviors in conversation. The present study isfirst exploration aiming at knowing better the dis-tribution of these items in one of our corpus. Moreprecisely, we would to verify how much inter-individual variability we will face in further studyand whether we can identify a structure in thisvariability (e.g speaker profiles). Second, we tried
1See the project website: http://cofee.hypotheses.org
to check there some strong trends in terms of evo-lution of use of these items in the course of theconversation. This later point was not conclusiveand is not developed in this paper.
Some data-intensive works exist for English(Gravano et al., 2012), Japanese (Kamiya et al.,2010; Misu et al., 2011) or Swedish (Allwood etal., 1992; Cerrato, 2007; Neiberg et al., 2013) butnot on many other languages such as French forexample. On French, the work of (Muller andPre´vot, 2003; Muller and Pre´vot, 2009) concerneda smaller scale (A hour corpus) and very specifictask. (Bertrand et al., 2007) was focussed on thefeedback inviting cues and also on a smaller scale(2 × 15 minutes). They showed that particularpitch contours and discursive markers play a sys-tematic role as inviting-cues both for vocal andgestural back-channels.
The paper is structured as follow. Section 2presents the conversational corpus used for thisstudy, then section 3 presents how this corpus hasbeen processed. Section 4 is related to general fig-ures for the feedback lexical items, followed bymore detailed information about inter-individualvariability (section 5).
2 The corpus
The Corpus of Interactional Data (CID) (Bertrandet al., 2008; Blache et al., 2009)2 is an audio-videorecording of 8 hours of spontaneous French dia-logues, 1 hour of recording per session. Each di-alogue involved two participants of the same gen-der. One of the following two topics of conver-sation was suggested to participants: conflicts intheir professional environment or unusual situa-tions in which participants may have found them-selves. It features a nearly free conversationalstyle with only a single theme proposed to the par-ticipants at the beginning of the experiment. This
2http://www.sldr.org/sldr000027/en
87
corpus is fully transcribed and forced-aligned atphone level. Moreover, it has been annotated withvarious linguistic information (Prosodic Phrasing,Discourse units, Syntactic tags, ...) (Blache et al.,2010) which will allow us later to take advantageof these levels of analysis.
Numerous studies have been carried out in pre-pared speech. However, conversational speechrefers to a more informal activity, in which par-ticipants have constantly to manage and negotiateturn-taking, topic changes (among other things)without any preparation. As a consequence, nu-merous phenomena appear such as hesitations, re-peats, backchannels, etc. Phonetic phenomenasuch as non-standard elision, reduction phenom-ena, truncated words, and more generally, non-standard pronunciations are also very frequent.All these phenomena can impact on the phoneti-zation, then on alignment.
3 Processing the corpus
The transcription process is done following spe-cific conventions derived from that of the GARS(Blanche-Benveniste and Jeanjean, 1987). Theresult is what we call an enriched orthographictranscription (EOT), from which two derived tran-scriptions are generated automatically : the stan-dard orthographic transcription (the list of ortho-graphic tokens) and a specific transcription fromwhich the phonetic tokens are obtained to beused by the grapheme-phoneme converter. Fromthe phoneme sequence and the audio signal, thealigner outputs for each phoneme its time localiza-tion. This corpus has been processed with severalaligners. The first and main one (Brun et al., 2004)is HMM-based, it uses a set of 10 macro-classesof vowel (7 oral and 3 nasal), 2 semi-vowels and15 consonants. Finally, from the time alignedphoneme sequence plus the EOT, the orthographictokens is time-aligned.
The alignment for this paper is another ver-sion that has been carried out using SPPAS3 (Bigi,2012). SPPAS is a tool to produce automatic anno-tations which include utterance, word, syllabic andphonemic segmentations from a recorded speechsound and its transcription.
Alignment of items of the list given in (1) werethen manually verified. Largest errors were cor-rected to obtain reliable alignments.
DM prononciations are the standard ones except3http://www.lpl-aix.fr/~bigi/sppas/
for a few cases. There are only two items withnon standard cases that are over 2 occurrences:sampa: m.w.e.) that is an hybrid between mhand ouais, and sampa w.a.l.a, a reduction ofv.w.a.l.a voila`.
The extraction themselves have been realizedby the authors with a Python script and all thestatistical analyses and plots have been producedwith R statistical analysis tool.
4 Descriptive statistics for the lexicalmarkers used in feedback
All the lexical items of the list given in (1) wereautomatically extracted and categorized into twocategories: (i) Isolated items are items or sequenceof items surrounded by pauses of at least 200 msand not including any extra material than the itemsof this list ; (ii) Initial items (or sequence items)are located in front of some other items (but thereis no other material within the sequence). Mostof these items also occur in final or even sur-rounded positions but we did not consider thesecases since they do are not clearly related to feed-back. More precisely surrounded items are mostlyconsisting in breaks of disfluencies or genuinelyintegrated construction (e.g j’e´tais d’accord aveclui / I agreed with him). Final ones can play arole in eliciting feedback or sometimes bring somekind of closure at the end of the utterance (whathas been described as Pivot Ending in (Gravano etal., 2012)).
(1) ah (ah), bon (well), ben (well), euh (err,uh), mh (mh), ouais (yeah), oui (yes), non(no), d’accord (agreed), OK (okay), voila`(that’s it, right)
Strictly speaking, the list (1) is not exhaustive.However, other items are already in the thin partof the distribution’s tail. Moreover, some of theitems such as euh / err are not necessarily relatedto feedback. However, by crossing lexical valueswith position we expect to get close enough thefull set of tokens involved in feedback. For exam-ple, initial euh not followed by a feedback relateditem will not be included in the final dataset. Thisis also an objective of the present work to identifythese situations.
The different markers exhibit very different fig-ures with regard to their location as it can be seenin 1. While some are specialized in isolated feed-back such as the continuer mh which is most of the
88
time backchanneled, others are found at the begin-ning of utterances such as euh, ah. The later makessense since euh is also a filled pause.
ah 
ah_
ouai
s 
ah_
oui 
ben 
bon 
dacc
ord 
eu
h 
mh 
mh_
mh 
no
n 
ou
ais
 
ou
ais
_oua
is 
ou
i 
voila
 
0
100
200
300
400
500
initialisolated
Figure 1: Distribution of isolated vs. initial posi-tion for the most frequent lexical items
In total 197 different combinations of the ba-sic markers were identified. The most frequentare the simple repetitions of items such as ouais(up to nine times) or mh. There are also morecomplex structures as exhibited in (2) that seemto mix two kinds of items: base ones and mod-ifiers (ah, euh). The base ones seem by defaultto carry general purpose communicative functionsas described in (Bunt, 2009; Bunt, 2012) while theothers can also be produced alone but are generallydealing specific dimension such as turn-taking, at-titude expression or time management.
(2) a. ah ouais d’accord ok (ah yeah rightokay)
b. voila` oui non (that’s it yes no)
With regard to duration, the data is rather messyconcerning the very long items. There are extremelengthening on these units. Aside that and the filleruh that exhibit a wide spread, the other items arenot produced with huge variations. Monosyllabicremain well centered around 150-250 ms while di-syllabic and repeated items are distributed in the250-500 ms range. This is important for our nextstep in which automatic acoustic analysis of theseitems will be performed.
l
l
ll
l
l
lll
l
l
l
l
lllll
ll
ll
l
l
l
l
l l
l
ll
l
l
l
ll
ll
l
ll
l
l
l
l
l
lll
l
l
l
lll
ll
l
lll
l
ll
l
ll
ll
lllll
l
l
lll
lll
l
l
l
l
lll
llllllll
ll
lll
l
llllllllll
l
l
l
l
lllllll
lll
l
l
l
lll
ll
l
ll
l
l
l
ll
l
l
ll
ah 
ah_
ouai
s 
ah_
oui 
ben 
bon 
dacc
ord 
eu
h 
mh 
mh_
mh 
no
n 
ou
ais
 
ou
ais
_oua
is 
ou
i 
voila
 
0.0
0.5
1.0
1.5
2.0
Figure 2: Duration (in seconds) of each lexicaltype
5 Inter-individual variability
Inter-individual variation is a big issue on the wayto the generalizability. We would like to under-stand some of the feedback producing profiles.Our intuitions coming from familiarity of the datais that there are strong variation but they corre-spond to a few different speaking styles. In fu-ture work, we would like to see in a second stepwhether we can identify and characterize thesestyles.
l
150
200
250
300
350
400
450
Figure 3: Number of feedback items per speaker
Figure 3 illustrates the total figures of feedbackper speaker. As expected variation is huge, from132 to 425 but with in fact with few outliers witha nice batch of speaker in the 200 - 300 range.The wider spread of the distribution in the highrange comes from two factors. First of all, thereare participants producing a high quantity of feed-
89
back items. They produce a massive amount oflight backchannels (mh, ouais) compared to low-quantity feedback producers. The later also pro-duce feedback during the long pauses of the mainspeaker but they produce much less overlappingbackchannels. This should be double checkedwith a specific measure (adding overlapping as afactor). However, a second effect seems importantfor at least one speaker (the outlier): the amountto time holding the floor. In fact the speaker pro-ducing the most feedback did so because she wasrarely the main speaker.
In order to get a global idea of the different usesof these items, Figure 5 represents the proportionof each item per speaker. As expected, the varia-tion is important but one can spot some tendencies.For examples for the most frequent items, the rankseems to preserved across speakers.
CID_
AB 
CID_
AC 
CID_
AG 
CID_
AP 
CID_
BX 
CID_
CM 
CID_
EB 
CID_
IM 
CID_
LJ 
CID_
LL 
CID_
MB 
CID_
MG
 CI
D_M
L CI
D_NH
 CI
D_SR
 CI
D_YM
 
voilaouiouais_ouaisouaisnonmh_mhmheuhdaccordbonbenah_ouiah_ouaisah
0.0
0.2
0.4
0.6
0.8
1.0
Figure 4: Distribution of the lexical items
Based on their feedback profile (proportion ofuse of each items as illustrated in Figure 5), weattempted to cluster the participants as showed in5. While the lower parts of the dendrogram arehard to interpret the higher part matches well withthe impression acquired by listening to the corpus(no backchannels and rather formal feedback vs.lots of backchannels and very colloquial style).
6 Current and Future Work
About this first batch of analyses, we will com-plete the analysis of the evolution during the con-versation. More precisely, we will go at theindividual level looking for time-based changes
AB
AG
IM ML
YM
BX
AP EB
LJ NH
CM
SR
AC MB
LL MG
0.1
0.2
0.3
0.4
0.5
Dendrogram of  diana(x = distSpForm)
diana (*, "NA")distSpForm
Hei
ght
Figure 5: Dendrogram of the participants clusterbased on their feedback profile
in their profiles as well as looking at the pairsfor tracking potential convergence effect either interms of distribution of lexical marker types or intheir duration.
In parallel to this work, we are launching in-dependent prosodic and kinesic analyses of theforms, as well as a discourse analysis of the func-tions. Moreover the work is being extended byadding two corpora in the study in order to allowfor a better situation generalisability: A FrenchMapTask; and a third corpus consisting in a lesscooperative situation. The idea is later to bringtogether the observations from the different levelsin order to propose a multidimensional model forfeedback in French dialogues.
Those are steps toward more extensive studiesin the spirit of (Gravano et al., 2012) or (Neiberget al., 2013) on French language and in which wehope to address more directly the issue of dis-course situation generalisability.
Acknowledgment
This work has been realized with the support ofthe ANR (Grant Number: ANR-12-JCJC-JSH2-006-01) and exploited aligned data produced inthe framework of the ANR project (Grant NumberANR-08-BLAN-0239). We would like to thank allthe members of these two projects.
90
ReferencesJ. Allwood, J. Nivre, and E. Ahlsen. 1992. On the se-
mantics and pragmatics of linguistic feedback. Jour-nal of Semantics, 9.
R. Bertrand, G. Ferre´, P. Blache, R. Espesser, andS. Rauzy. 2007. Backchannels revisited from a mul-timodal perspective. In Proceedings of Auditory-visual Speech Processing. Citeseer.
R. Bertrand, P. Blache, R. Espesser, G. Ferre´, C. Me-unier, B. Priego-Valverde, and S. Rauzy. 2008.Le cid-corpus of interactional data-annotation et ex-ploitation multimodale de parole conversationnelle.Traitement Automatique des Langues, 49(3):1–30.
B. Bigi. 2012. SPPAS: a tool for the phonetic segmen-tation of speech. In Language Resource and Evalu-ation Conference, pages 1748–1755, ISBN 978–2–9517408–7–7, Istanbul (Turkey).
P. Blache, R. Bertrand, and G. Ferre´. 2009. Creatingand exploiting multimodal annotated corpora: thetoma project. Multimodal corpora, pages 38–53.
P. Blache, R. Bertrand, B. Bigi, E. Bruno, E. Cela,R. Espesser, G. Ferre´, M. Guardiola, D. Hirst,E. Muriasco, J.-C. Martin, C. Meunier, M.-A. Morel,I. Nesterenko, P. Nocera, B. Palaud, L. Pre´vot,B. Priego-Valverde, J. Seinturier, N. Tan, M. Tel-lier, and S. Rauzy. 2010. Multimodal annotationof conversational data. In Proceedings of LinguisticAnnotation Workshop.
C. Blanche-Benveniste and C. Jeanjean. 1987. Lefranc¸ais parle´. Edition et transcription. Paris, Di-dier Erudition.
A. Brun, C. Cerisara, D. Fohr, I. Illina, D. Langlois,O. Mella, and K. Smai¨li. 2004. Ants: le syste`me detranscription automatique du loria. In Actes des XXVJourne´es d’Etudes sur la Parole, Fe`s, Morocco.
H. Bunt. 2009. Multifunctionality and multidimen-sional dialogue act annotation. In Proceedings ofDiaHolmia, SEMDIAL.
H. Bunt. 2012. The semantics of feedback. In16th Workshop on the Semantics and Pragmatics ofDialogue (SEMDIAL 2012), pages 118–127, Paris(France).
L. Cerrato. 2007. Investigating Communicative Feed-back Phenomena across Languages and Modalities.Ph.D. thesis.
H.H. Clark. 1996. Using language. Cambridge: Cam-bridge University Press.
A. Gravano, J. Hirschberg, and S?. Ben?us?. 2012. Af-firmative cue words in task-oriented dialogue. Com-putational Linguistics, 38(1):1–39.
Y. Kamiya, T. Ohno, and S. Matsubara. 2010. Coher-ent back-channel feedback tagging of in-car spokendialogue corpus. In Proceedings of the 11th AnnualMeeting of the Special Interest Group on Discourseand Dialogue, pages 205–208. Association for Com-putational Linguistics.
T. Misu, E. Mizukami, Y. Shiga, S. Kawamoto,H. Kawai, and S. Nakamura. 2011. Toward con-struction of spoken dialogue system that evokesusers’ spontaneous backchannels. In Proceedingsof the SIGDIAL 2011 Conference, pages 259–265.Association for Computational Linguistics.
P. Muller and L. Pre´vot. 2003. An empirical studyof acknowledgement structures. In Proceedings odDiabruck, 7th workshop on semantics and pragmat-ics of dialogue, Saarbrucken.
P. Muller and L. Pre´vot. 2009. Grounding informationin route explanation dialogues. In Spatial Languageand Dialogue. Oxford University Press.
D. Neiberg, G. Salvi, and J. Gustafson. 2013. Semi-supervised methods for exploring the acoustics ofsimple productive feedback. Speech Communica-tion.
L. Pre´vot and R. Bertrand. 2012. Cofee-toward a mul-tidimensional analysis of conversational feedback,the case of french language. In Proceedings of theWorkshop on Feedback Behaviors. (poster).
A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates,D. Jurafsky, P. Taylor, R. Martin, C.V. Ess-Dykema,and M. Meteer. 2000. Dialogue act modeling forautomatic tagging and recognition of conversationalspeech. Computational linguistics, 26(3):339–373.
91
