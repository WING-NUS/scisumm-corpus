    <S sid="144" ssid="7">For comparison, the completely unsupervised feature-HMM baseline accuracy on the universal POS tags for English is 79.4%, and goes up to 88.7% with a treebank dictionary.</S>
    <S sid="44" ssid="10">Because all English vertices are going to be labeled, we do not need to disambiguate them by embedding them in trigrams.</S
    <S sid="16" ssid="12">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (&#167;3), and then use graph label propagation to project syntactic information from English to the foreign language (&#167;4).</S>
<S sid="110" ssid="10">We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.</S>
    <S sid="115" ssid="15">The taggers were trained on datasets labeled with the universal tags.</S>
<S sid="158" ssid="1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
<S sid="23" ssid="19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
<S sid="24" ssid="1">The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.</S>
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
    <S sid="23" ssid="19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
<S sid="56" ssid="22">To define a similarity function between the English and the foreign vertices, we rely on high-confidence word alignments.</
<S sid="161" ssid="4">Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.</S>