Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P08-1028,D08-1094,0,2008,0,"Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p+ a as a function f operating on four components: c= f (p, a, R, K) (3) R is the relation holding between p and a, and Kadditional knowledge","Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p+ a as a function f operating on four components: c= f (p, a, R, K) (3) R is the relation holding between p and a, and K additional knowledge","'62','57','51','63'","<S sid=""62"" ssid=""10"">Another simplification concerns K which can be ignored so as to explore what can be achieved in the absence of additional knowledge.</S><S sid=""57"" ssid=""5"">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S><S sid=""51"" ssid=""24"">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S sid=""63"" ssid=""11"">This reduces the class of models to: However, this still leaves the particular form of the function f unspecified.</S>",
4,P08-1028,P14-1060,0,2008,0,"While works such asthe SDSM model suffer from the problem of sparsity in composing structures beyond bi grams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstetteand Sadrzadeh (2011) are restricted by significant model biases in representing semantic com position by generic algebraic operations","While works such as the SDSM model suffer from the problem of sparsity in composing structures beyond bigrams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstette and Sadrzadeh (2011) are restricted by significant model biases in representing semantic com position by generic algebraic operations","'87','91','138','1'","<S sid=""87"" ssid=""35"">Combining the multiplicative model with an additive model, which does not suffer from this problem, could mitigate this problem: pi = &#945;ui +&#946;vi +&#947;uivi (11) where &#945;, &#946;, and &#947; are weighting constants.</S><S sid=""91"" ssid=""4"">This change in the verb&#8217;s sense is equated to a shift in its position in semantic space.</S><S sid=""138"" ssid=""51"">Model Parameters Irrespectively of their form, all composition models discussed here are based on a semantic space for representing the meanings of individual words.</S><S sid=""1"" ssid=""1"">This paper proposes a framework for representing the meaning of phrases and sentences in vector space.</S>",
6,P08-1028,P10-1097,0,2008,0,"Mitchell and Lapata (2008), henceforth M& amp; L, propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression","Mitchell and Lapata (2008), henceforth M& amp; L, propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression","'21','20','11','28'","<S sid=""21"" ssid=""17"">Central in these models is the notion of compositionality &#8212; the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.</S><S sid=""20"" ssid=""16"">Computational models of semantics which use symbolic logic representations (Montague, 1974) can account naturally for the meaning of phrases or sentences.</S><S sid=""11"" ssid=""7"">Despite their widespread use, vector-based models are typically directed at representing words in isolation and methods for constructing representations for phrases or sentences have received little attention in the literature.</S><S sid=""28"" ssid=""1"">The problem of vector composition has received some attention in the connectionist literature, particularly in response to criticisms of the ability of connectionist representations to handle complex structures (Fodor and Pylyshyn, 1988).</S>",
7,P08-1028,P10-1097,0,2008,0,"Interestingly, Mitchell and Lapata (2008) came to the same result in a different setting","Interestingly, Mitchell and Lapata (2008) came to the same result in a different setting","'15','104','72','45'","<S sid=""15"" ssid=""11"">Sentences (1-a) and (1-b) contain exactly the same set of words but their meaning is entirely different.</S><S sid=""104"" ssid=""17"">Verbs and nouns that were attested less than fifty times in the BNC were removed as they would result in unreliable vectors.</S><S sid=""72"" ssid=""20"">As a result of the assumption of symmetry, both these models are &#8216;bag of words&#8217; models and word order insensitive.</S><S sid=""45"" ssid=""18"">Vector addition does not increase the dimensionality of the resulting vector.</S>",
8,P08-1028,D11-1094,0,2008,0,"And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to combine the constituent vectors","And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to combine the constituent vectors","'21','2','182','25'","<S sid=""21"" ssid=""17"">Central in these models is the notion of compositionality &#8212; the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.</S><S sid=""2"" ssid=""2"">Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions.</S><S sid=""182"" ssid=""16"">The lowest correlation (p = 0.04) is observed for the simple additive model which is not significantly different from the non-compositional baseline model.</S><S sid=""25"" ssid=""21"">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>",
9,P08-1028,W11-0131,0,2008,0,"Mitchell and Lapata (2008) provide a general framework for semantic vector composition: p= f (u, v, R, K) (1) 295 where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base, and p is a resulting composed vector (or tensor)","Mitchell and Lapata (2008) provide a general framework for semantic vector composition: p= f (u, v, R, K) (1) 295 where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base, and p is a resulting composed vector (or tensor)","'189','0','57','53'","<S sid=""189"" ssid=""1"">In this paper we presented a general framework for vector-based semantic composition.</S><S sid=""0"">Vector-based Models of Semantic Composition</S><S sid=""57"" ssid=""5"">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S><S sid=""53"" ssid=""1"">We formulate semantic composition as a function of two vectors, u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word&#8217;s vector typically represents its co-occurrence with neighboring words.</S>",
10,P08-1028,W11-0131,0,2008,0,"As Mitchell and Lapata (2008) did, let us temporarily suspend discussion on what semantics populate our vectors for now","As Mitchell and Lapata (2008) did, let us temporarily suspend discussion on what semantics populate our vectors for now","'0','57','20','138'","<S sid=""0"">Vector-based Models of Semantic Composition</S><S sid=""57"" ssid=""5"">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S><S sid=""20"" ssid=""16"">Computational models of semantics which use symbolic logic representations (Montague, 1974) can account naturally for the meaning of phrases or sentences.</S><S sid=""138"" ssid=""51"">Model Parameters Irrespectively of their form, all composition models discussed here are based on a semantic space for representing the meanings of individual words.</S>",
11,P08-1028,P13-2083,0,2008,0,"Mitchell and Lapata (2008) propose a framework to define the composition c= f (a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition","Mitchell and Lapata (2008) propose a framework to define the composition c= f (a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition","'57','51','59','62'","<S sid=""57"" ssid=""5"">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S><S sid=""51"" ssid=""24"">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S sid=""59"" ssid=""7"">It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence, via the argument R, on syntax.</S><S sid=""62"" ssid=""10"">Another simplification concerns K which can be ignored so as to explore what can be achieved in the absence of additional knowledge.</S>",
12,P08-1028,P13-2083,0,"Mitchell and Lapata, 2008",0,"As our final set of baselines, we extend two simple techniques proposed by (Mitchell and Lapata, 2008) that use element-wise addition and multiplication operators to perform composition","As our final set of baselines, we extend two simple techniques proposed by (Mitchell and Lapata, 2008) that use element-wise addition and multiplication operators to perform composition","'190','161','51','36'","<S sid=""190"" ssid=""2"">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S><S sid=""161"" ssid=""74"">Evaluation Methodology We evaluated the proposed composition models in two ways.</S><S sid=""51"" ssid=""24"">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S sid=""36"" ssid=""9"">To overcome this problem, other techniques have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components.</S>",
13,P08-1028,P10-1021,0,2008,0,"Althoughthis model has been shown to successfully simulate single and multiple-word priming (McDonald and Brew 2004), it failed to predict processing costs in the Embra eye-tracking corpus (McDonald and Shillcock 2003) .In this work we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008)","Although this model has been shown to successfully simulate single and multiple-word priming (McDonald and Brew 2004), it failed to predict processing costs in the Embra eye-tracking corpus (McDonald and Shillcock 2003). In this work we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008)","'10','29','68','52'","<S sid=""10"" ssid=""6"">Moreover, the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald, 2000) and word association norms (Denhire and Lemaire, 2004).</S><S sid=""29"" ssid=""2"">While neural networks can readily represent single distinct objects, in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.</S><S sid=""68"" ssid=""16"">Although the composition model in (5) is commonly used in the literature, from a linguistic perspective, the model in (6) is more appealing.</S><S sid=""52"" ssid=""25"">Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S>",
14,P08-1028,P10-1021,0,2008,0,"Assuming that h is a linear function of the Cartesian product of u and v allows to specify additive models which are by far the most common method of vector combination in the literature: hi =ui+ vi (3) Alternatively, we can assume that h is a linear function of the tensor product of u and v, and thus derive models based on multiplication: hi =ui? vi (4) Mitchell and Lapata (2008) show that several additive and multiplicative models can be formulated under this framework, including the well known tensor products (Smolensky 1990) and circular convolution (Plate 1995)","Mitchell and Lapata (2008) show that several additive and multiplicative models can be formulated under this framework, including the well known tensor products (Smolensky 1990) and circular convolution (Plate 1995)","'35','40','38','190'","<S sid=""35"" ssid=""8"">The tensor product u &#174; v is a matrix whose components are all the possible products uivj of the components of vectors u and v. A major difficulty with tensor products is their dimensionality which is higher than the dimensionality of the original vectors (precisely, the tensor product has dimensionality m x n).</S><S sid=""40"" ssid=""13"">Noisy versions of the original vectors can be recovered by means of circular correlation which is the approximate inverse of circular convolution.</S><S sid=""38"" ssid=""11"">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S><S sid=""190"" ssid=""2"">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>",
15,P08-1028,W11-0115,0,2008,0,"Mitchell and Lapata (2008) introduce a whole family of models of compositionality based on vector addition and point wise-multiplication (and a weighted combination of both), evaluated on a sentence similarity task inspired by Kintsch (2001)","Mitchell and Lapata (2008) introduce a whole family of models of compositionality based on vector addition and point wise-multiplication (and a weighted combination of both), evaluated on a sentence similarity task inspired by Kintsch (2001)","'88','168','190','3'","<S sid=""88"" ssid=""1"">We evaluated the models presented in Section 3 on a sentence similarity task initially proposed by Kintsch (2001).</S><S sid=""168"" ssid=""2"">These included three additive models, i.e., simple addition (equation (5), Add), weighted addition (equation (7), WeightAdd), and Kintsch&#8217;s (2001) model (equation (10), Kintsch), a multiplicative model (equation (6), Multiply), and also a model which combines multiplication with addition (equation (11), Combined).</S><S sid=""190"" ssid=""2"">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S><S sid=""3"" ssid=""3"">Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.</S>",
16,P08-1028,W11-0115,0,2008,0,"The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression","The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression","'161','155','183','2'","<S sid=""161"" ssid=""74"">Evaluation Methodology We evaluated the proposed composition models in two ways.</S><S sid=""155"" ssid=""68"">Secondly, we optimized the weightings in the combined model (11) with a similar grid search over its three parameters.</S><S sid=""183"" ssid=""17"">The weighted additive model (p = 0.09) is not significantly different from the baseline either or Kintsch (2001) (p = 0.09).</S><S sid=""2"" ssid=""2"">Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions.</S>",
17,P08-1028,W11-0115,0,2008,0,"For example, Mitchell and Lapata (2008) use their models to approximate the human ratings in their sentence similarity dataset","For example, Mitchell and Lapata (2008) use their models to approximate the human ratings in their sentence similarity dataset","'180','128','162','163'","<S sid=""180"" ssid=""14"">As can be seen, all models are significantly correlated with the human ratings.</S><S sid=""128"" ssid=""41"">First, we examined whether participants gave high ratings to high similarity sentence pairs and low ratings to low similarity ones.</S><S sid=""162"" ssid=""75"">First, we used the models to estimate the cosine similarity between the reference sentence and its landmarks.</S><S sid=""163"" ssid=""76"">We expect better models to yield a pattern of similarity scores like those observed in the human ratings (see Figure 2).</S>",
18,P08-1028,W11-1310,0,2008,0,We use other WSM settings following Mitchell and Lapata (2008),We use other WSM settings following Mitchell and Lapata (2008),"'7','100','140','34'","<S sid=""7"" ssid=""3"">A variety of NLP tasks have made good use of vector-based models.</S><S sid=""100"" ssid=""13"">In the following we describe our data collection procedure and give details on how our composition models were constructed and evaluated.</S><S sid=""140"" ssid=""53"">Following previous work (Bullinaria and Levy, 2007), we optimized its parameters on a word-based semantic similarity task.</S><S sid=""34"" ssid=""7"">Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.</S>",
19,P08-1028,W11-1310,0,2008,0,Mitchell and Lapata (2008) observed that a simple multiplication function modelled compositionality better than addition,Mitchell and Lapata (2008) observed that a simple multiplication function modelled compositionality better than addition,"'165','190','182','163'","<S sid=""165"" ssid=""78"">Again, better models should correlate better with the experimental data.</S><S sid=""190"" ssid=""2"">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S><S sid=""182"" ssid=""16"">The lowest correlation (p = 0.04) is observed for the simple additive model which is not significantly different from the non-compositional baseline model.</S><S sid=""163"" ssid=""76"">We expect better models to yield a pattern of similarity scores like those observed in the human ratings (see Figure 2).</S>",
20,P08-1028,W11-1310,0,"Mitchell and Lapata, 2008",0,"We use the compositionality functions, simple addition and simple multiplication to build compositional vectors Vwr1+wr2 and Vwr1 ?wr2. These are as described in (Mitchell and Lapata, 2008)","We use the compositionality functions, simple addition and simple multiplication to build compositional vectors Vwr1+wr2 and Vwr1 ?wr2. These are as described in (Mitchell and Lapata, 2008)","'190','174','182','168'","<S sid=""190"" ssid=""2"">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S><S sid=""174"" ssid=""8"">The simple additive model fails to distinguish between High and Low Similarity items.</S><S sid=""182"" ssid=""16"">The lowest correlation (p = 0.04) is observed for the simple additive model which is not significantly different from the non-compositional baseline model.</S><S sid=""168"" ssid=""2"">These included three additive models, i.e., simple addition (equation (5), Add), weighted addition (equation (7), WeightAdd), and Kintsch&#8217;s (2001) model (equation (10), Kintsch), a multiplicative model (equation (6), Multiply), and also a model which combines multiplication with addition (equation (11), Combined).</S>",
