Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,W06-3114,W06-3120,0,"Koehn and Monz, 2006",0,"The official results were slightly better because a lowercase evaluation was used, see (Koehn and Monz, 2006)","The official results were slightly better because a lowercase evaluation was used, see (Koehn and Monz, 2006)","'124','35','70','8'","<S sid=""124"" ssid=""17"">More judgements would have enabled us to make better distinctions, but it is not clear what the upper limit is.</S><S sid=""35"" ssid=""1"">For the automatic evaluation, we used BLEU, since it is the most established metric in the field.</S><S sid=""70"" ssid=""9"">See Figure 3 for a screenshot of the evaluation tool.</S><S sid=""8"" ssid=""1"">The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.</S>",
2,W06-3114,D07-1092,0,"Koehn and Monz, 2006",0,"We are further focusing on the shared task of the workshop on Statistical Machine Translation, which took place last year (Koehn and Monz, 2006) and consisted in translating Spanish, German, and French texts from and to English","We are further focusing on the shared task of the workshop on Statistical Machine Translation, which took place last year (Koehn and Monz, 2006) and consisted in translating Spanish, German, and French texts from and to English","'6','151','8','29'","<S sid=""6"" ssid=""4"">English was again paired with German, French, and Spanish.</S><S sid=""151"" ssid=""44"">The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphological rich languages, as demonstrated by the results for English-German and English-French.</S><S sid=""8"" ssid=""1"">The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.</S><S sid=""29"" ssid=""22"">About half of the participants of last year&#8217;s shared task participated again.</S>",
3,W06-3114,C08-1074,0,"Koehn and Monz, 2006",0,"For our training and test data we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference","For our training and test data we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference","'9','16','126','8'","<S sid=""9"" ssid=""2"">Training and testing is based on the Europarl corpus.</S><S sid=""16"" ssid=""9"">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000, which is excluded from the training data.</S><S sid=""126"" ssid=""19"">The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences out-ofdomain test data.</S><S sid=""8"" ssid=""1"">The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.</S>",
4,W06-3114,W07-0718,0,"Koehn and Monz, 2006",0,"The results of last year? s workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006)","The results of last year's workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006)","'90','26','78','171'","<S sid=""90"" ssid=""6"">Another way to view the judgements is that they are less quality judgements of machine translation systems per se, but rankings of machine translation systems.</S><S sid=""26"" ssid=""19"">Most of these groups follow a phrase-based statistical approach to machine translation.</S><S sid=""78"" ssid=""17"">Judges where excluded from assessing the quality of MT systems that were submitted by their institution.</S><S sid=""171"" ssid=""2"">While many systems had similar performance, the results offer interesting insights, especially about the relative performance of statistical and rule-based systems.</S>",
5,W06-3114,P07-1083,0,"Koehn and Monz, 2006",0,"For the bi text-based annotation, we use publicly available word alignments from the Europarl corpus, automatically generated by GIZA++ for FrenchEnglish (Fr), Spanish-English (Es) and GermanEnglish (De) (Koehn and Monz, 2006)","For the bi text-based annotation, we use publicly available word alignments from the Europarl corpus, automatically generated by GIZA++ for FrenchEnglish (Fr), Spanish-English (Es) and German-English (De) (Koehn and Monz, 2006)","'6','9','5','150'","<S sid=""6"" ssid=""4"">English was again paired with German, French, and Spanish.</S><S sid=""9"" ssid=""2"">Training and testing is based on the Europarl corpus.</S><S sid=""5"" ssid=""3"">&#8226; We evaluated translation from English, in addition to into English.</S><S sid=""150"" ssid=""43"">For some language pairs (such as GermanEnglish) system performance is more divergent than for others (such as English-French), at least as measured by BLEU.</S>",
6,W06-3114,W07-0738,0,2006,0,"Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MTquality indicator","Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MTquality indicator","'39','140','35','4'","<S sid=""39"" ssid=""5"">However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong.</S><S sid=""140"" ssid=""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S><S sid=""35"" ssid=""1"">For the automatic evaluation, we used BLEU, since it is the most established metric in the field.</S><S sid=""4"" ssid=""2"">This revealed interesting clues about the properties of automatic and manual scoring.</S>",
7,W06-3114,W07-0738,0,2006,0,"For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001)","For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001)","'39','140','36','103'","<S sid=""39"" ssid=""5"">However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong.</S><S sid=""140"" ssid=""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S><S sid=""36"" ssid=""2"">The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.</S><S sid=""103"" ssid=""19"">Given a set of n sentences, we can compute the sample mean x&#65533; and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x&#8722;d, x+df can be computed by d = 1.96 &#183;&#65533;n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>",
8,W06-3114,W07-0738,0,2006,0,Wepresenta comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006) (see Section 3),We present a comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006) (see Section 3),"'39','140','82','50'","<S sid=""39"" ssid=""5"">However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong.</S><S sid=""140"" ssid=""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S><S sid=""82"" ssid=""21"">This decreases the statistical significance of our results compared to those studies.</S><S sid=""50"" ssid=""16"">Following this method, we repeatedly &#8212; say, 1000 times &#8212; sample sets of sentences from the output of each system, measure their BLEU score, and use these 1000 BLEU scores as basis for estimating a confidence interval.</S>",
9,W06-3114,W07-0738,0,2006,0,Weanalyze some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006),We analyze some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006),"'39','140','7','10'","<S sid=""39"" ssid=""5"">However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong.</S><S sid=""140"" ssid=""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S><S sid=""7"" ssid=""5"">We dropped, however, one of the languages, Finnish, partly to keep the number of tracks manageable, partly because we assumed that it would be hard to find enough Finnish speakers for the manual evaluation.</S><S sid=""10"" ssid=""3"">Figure 1 provides some statistics about this corpus.</S>",
10,W06-3114,D07-1030,0,"Koehn and Monz, 2006",0,"We use the same method described in (Koehn and Monz, 2006) to perform the significance test","We use the same method described in (Koehn and Monz, 2006) to perform the significance test","'49','52','113','102'","<S sid=""49"" ssid=""15"">Hence, we use the bootstrap resampling method described by Koehn (2004).</S><S sid=""52"" ssid=""18"">Pairwise comparison: We can use the same method to assess the statistical significance of one system outperforming another.</S><S sid=""113"" ssid=""6"">The confidence intervals are computed by bootstrap resampling for BLEU, and by standard significance testing for the manual scores, as described earlier in the paper.</S><S sid=""102"" ssid=""18"">Confidence Interval: To estimate confidence intervals for the average mean scores for the systems, we use standard significance testing.</S>",
11,W06-3114,D07-1030,0,"Koehn and Monz, 2016",0,"We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006)","We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006)","'123','130','40','118'","<S sid=""123"" ssid=""16"">For the manual scoring, we can distinguish only half of the systems, both in terms of fluency and adequacy.</S><S sid=""130"" ssid=""23"">This is demonstrated by average scores over all systems, in terms of BLEU, fluency and adequacy, as displayed in Figure 5.</S><S sid=""40"" ssid=""6"">They demonstrated this with the comparison of statistical systems against (a) manually post-edited MT output, and (b) a rule-based commercial system.</S><S sid=""118"" ssid=""11"">At first glance, we quickly recognize that many systems are scored very similar, both in terms of manual judgement and BLEU.</S>",
12,W06-3114,W08-0406,0,"Koehn and Monz, 2017",0,"The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002)","The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002)","'14','34','26','38'","<S sid=""14"" ssid=""7"">There is twice as much language modelling data, since training data for the machine translation system is filtered against sentences of length larger than 40 words.</S><S sid=""34"" ssid=""27"">For more on the participating systems, please refer to the respective system description in the proceedings of the workshop.</S><S sid=""26"" ssid=""19"">Most of these groups follow a phrase-based statistical approach to machine translation.</S><S sid=""38"" ssid=""4"">The BLEU score has been shown to correlate well with human judgement, when statistical machine translation systems are compared (Doddington, 2002; Przybocki, 2004; Li, 2005).</S>",
13,W06-3114,W11-1002,0,2006,0,"Callison-Burch et al (2006 )andKoehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality","Callison-Burch et al (2006) and Koehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality","'39','140','62','38'","<S sid=""39"" ssid=""5"">However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong.</S><S sid=""140"" ssid=""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S><S sid=""62"" ssid=""1"">While automatic measures are an invaluable tool for the day-to-day development of machine translation systems, they are only a imperfect substitute for human assessment of translation quality, or as the acronym BLEU puts it, a bilingual evaluation understudy.</S><S sid=""38"" ssid=""4"">The BLEU score has been shown to correlate well with human judgement, when statistical machine translation systems are compared (Doddington, 2002; Przybocki, 2004; Li, 2005).</S>",
14,W06-3114,D07-1091,0,"Koehn and Monz, 2006",0,"The English? German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006)","The English German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006)","'5','9','126','21'","<S sid=""5"" ssid=""3"">&#8226; We evaluated translation from English, in addition to into English.</S><S sid=""9"" ssid=""2"">Training and testing is based on the Europarl corpus.</S><S sid=""126"" ssid=""19"">The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences out-ofdomain test data.</S><S sid=""21"" ssid=""14"">The out-of-domain test set differs from the Europarl data in various ways.</S>",
15,W06-3114,D07-1091,0,"Koehn and Monz, 2006",0,"We report results on the development test set, which is also the out-of-domain test set of the WMT06 workshop shared task (Koehn and Monz, 2006)","We report results on the development test set, which is also the out-of-domain test set of the WMT06 workshop shared task (Koehn and Monz, 2006)","'20','58','21','126'","<S sid=""20"" ssid=""13"">For statistics on this test set, refer to Figure 1.</S><S sid=""58"" ssid=""24"">We divide up each test set into blocks of 20 sentences (100 blocks for the in-domain test set, 53 blocks for the out-of-domain test set), check for each block, if one system has a higher BLEU score than the other, and then use the sign test.</S><S sid=""21"" ssid=""14"">The out-of-domain test set differs from the Europarl data in various ways.</S><S sid=""126"" ssid=""19"">The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences out-ofdomain test data.</S>",
16,W06-3114,P07-1108,0,"Koehn and Monz, 2006",0,"A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Ma chine Translation (Koehn and Monz, 2006)","A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Machine Translation (Koehn and Monz, 2006)","'0','90','26','8'","<S sid=""0"">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid=""90"" ssid=""6"">Another way to view the judgements is that they are less quality judgements of machine translation systems per se, but rankings of machine translation systems.</S><S sid=""26"" ssid=""19"">Most of these groups follow a phrase-based statistical approach to machine translation.</S><S sid=""8"" ssid=""1"">The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.</S>",
18,W06-3114,E12-3010,0,"Koehn and Monz, 2006",0,"For the same reason, human evaluation metrics based on adequacy and fluency were not suitable either (Koehn and Monz, 2006)","For the same reason, human evaluation metrics based on adequacy and fluency were not suitable either (Koehn and Monz, 2006)","'175','63','68','64'","<S sid=""175"" ssid=""6"">Replacing this with an ranked evaluation seems to be more suitable.</S><S sid=""63"" ssid=""2"">Many human evaluation metrics have been proposed.</S><S sid=""68"" ssid=""7"">We asked participants to each judge 200&#8211;300 sentences in terms of fluency and adequacy, the most commonly used manual evaluation metrics.</S><S sid=""64"" ssid=""3"">Also, the argument has been made that machine translation performance should be evaluated via task-based evaluation metrics, i.e. how much it assists performing a useful task, such as supporting human translators or aiding the analysis of texts.</S>",
19,W06-3114,W09-0402,0,"Koehn and Monz, 2006",0,"The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008)","The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008)","'39','140','6','132'","<S sid=""39"" ssid=""5"">However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong.</S><S sid=""140"" ssid=""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S><S sid=""6"" ssid=""4"">English was again paired with German, French, and Spanish.</S><S sid=""132"" ssid=""25"">It is well know that language pairs such as EnglishGerman pose more challenges to machine translation systems than language pairs such as FrenchEnglish.</S>",
